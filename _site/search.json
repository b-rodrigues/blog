[
  {
    "objectID": "posts/2025/01/github_pages_setup_with_quarto.html",
    "href": "posts/2025/01/github_pages_setup_with_quarto.html",
    "title": "github pages setup for this website",
    "section": "",
    "text": "Desired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\n\n1 + 1"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Bruno Rodrigues and hold a PhD in Economics from the University of Strasbourg.\n\n&lt;img src=\"/assets/img/profile.jpg\"/&gt;\n\nI’m currently employed as a statistician for the Ministry of Research and Higher Education in Luxembourg. Before that I was senior data scientist and then manager in the data science team at PwC Luxembourg, and before that I was a research assistant at STATEC Research.\nMy hobbies are boxing, lifting weights, cycling, cooking and reading or listening to audiobooks, which is more compatible with the life of a young father. I started this blog to share my enthusiasm for statistics. My blog posts are reshared on R-bloggers and RWeekly. I also enjoy learning about the R programming language and sharing my knowledge. That’s why I made this blog and write ebooks. I also have a youtube channel, where I show some tips and tricks with R, or rant about stuff."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Econometrics and Free Software",
    "section": "",
    "text": "Welcome to my blog where I talk about R, Nix, Econometrics and Data Science. If you enjoy reading what I write, you might enjoy my books or want to follow me on Mastodon or Twitter or Bluesky. If you are 40+, click here instead. I also make videos on youtube.\n\n\n\n\n\n\n2025-01\n\n\n\ngithub pages setup for this website\n\n\n\n2024-12\n\n\n\nhuhu\n\n\n\n2024-11\n\n\n\nNovember blog post\n\n\n\n2017-03\n\n\n\nLesser known dplyr tricks\n\n\n\n2017-02\n\n\n\nHow to use jailbreakr\n\n\n\n2017-01\n\n\n\nMy free book has a cover!\n\n\n\n2016-12\n\n\n\nFunctional programming and unit testing for data munging with R available on Leanpub\n\n\nWork on lists of datasets instead of individual datasets by using functional programming\n\n\n\n2016-11\n\n\n\nI’ve started writing a ‘book’: Functional programming and unit testing for data munging with R\n\n\n\n2016-07\n\n\n\nData frame columns as arguments to dplyr functions\n\n\nMerge a list of datasets together\n\n\nRead a lot of datasets at once with R\n\n\n\n2016-03\n\n\n\nCareful with tryCatch\n\n\nUnit testing with R\n\n\n\n2015-11\n\n\n\nBootstrapping standard errors for difference-in-differences estimation with R\n\n\n\n2015-05\n\n\n\nUpdate to Introduction to programming econometrics with R\n\n\n\n2015-02\n\n\n\nExport R output to a file\n\n\n\n2015-01\n\n\n\nIntroduction to programming econometrics with R\n\n\n\n2014-11\n\n\n\nR, R with Atlas, R with OpenBLAS and Revolution R Open: which is fastest?\n\n\n\n2014-04\n\n\n\nObject Oriented Programming with R: An example with a Cournot duopoly\n\n\n\n2013-12\n\n\n\nSimulated Maximum Likelihood with R\n\n\nUsing R as a Computer Algebra System with Ryacas\n\n\n\n2013-11\n\n\n\nNonlinear Gmm with R - Example with a logistic regression\n\n\n\n2013-01\n\n\n\nMethod of Simulated Moments with R\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/11/haha.html",
    "href": "posts/2024/11/haha.html",
    "title": "November blog post",
    "section": "",
    "text": "Undesired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\n\n1 + 1"
  },
  {
    "objectID": "posts/2024/12/huhu.html",
    "href": "posts/2024/12/huhu.html",
    "title": "huhu",
    "section": "",
    "text": "Undesired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\n\n1 + 1"
  },
  {
    "objectID": "posts/2013/01/2013-01-16-simulated-maximum-likelihood-with-r.html",
    "href": "posts/2013/01/2013-01-16-simulated-maximum-likelihood-with-r.html",
    "title": "Simulated Maximum Likelihood with R",
    "section": "",
    "text": "This document details section 12.4.5. Unobserved Heterogeneity Example from Cameron and Trivedi’s book - MICROECONOMETRICS: Methods and Applications. The original source code giving the results from table 12.2 are available from the authors' site here and written for Stata. This is an attempt to translate the code to R. I’d like to thank Reddit user anonemouse2010 for his advice which helped me write the function.\n\n\nConsult the original source code if you want to read the authors' comments. If you want the R source code without all the commentaries, grab it here. This is not guaranteed to work, nor to be correct. It could set your pet on fire and/or eat your first born. Use at your own risk. I may, or may not, expand this example. Corrections, constructive criticism are welcome.\n\n\nThe model is \\(y=\\theta+u+\\varepsilon\\) where \\(\\theta\\) is a scalar parameter equal to 1. \\(u\\) is extreme value type 1 (Gumbel distribution), \\(\\varepsilon \\leadsto \\mathbb{N}(0,1)\\). For more details, consult the book.\n\n\nImport the data\n\n\nYou can consult the original source code to see how the authors simulated the data. To get the same results, and verify that I didn't make mistakes I prefer importing their data directly from their website.\n\n\ndata &lt;- read.table(\"http://cameron.econ.ucdavis.edu/mmabook/mma12p2mslmsm.asc\")\nu &lt;- data[, 1]\ne &lt;- data[, 2]\ny &lt;- data[, 3]\nnumobs &lt;- length(u)\nsimreps &lt;- 10000\n\n\nSimulation\n\n\nIn the code below, the following likelihood function:\n\\[\\log{\\hat{L}_N(\\theta)} = \\dfrac{1}{N} \\sum_{i=1}^N\\log{\\big( \\dfrac{1}{S}\\sum_{s=1}^S \\dfrac{1}{\\sqrt{2\\pi}} \\exp \\{ -(-y_i-\\theta-u_i^s)^2/2 \\}\\big)}\\]\nwhich can be found on page 397 is programmed using the function sapply.\n\n\ndenssim &lt;- function(theta) {\n    loglik &lt;- mean(sapply(y, function(y) log(mean((1/sqrt(2 * pi)) * exp(-(y - theta + log(-log(runif(simreps))))^2/2)))))\n    return(-loglik)\n}\n\n\nThis likelihood is then maximized:\n\n\nsystem.time(res &lt;- optim(0.1, denssim, method = \"BFGS\", control = list(maxit = simreps)))\n\n   user  system elapsed \n  27.89    0.55   28.47 \n\n\n\nConvergence is achieved pretty rapidly, to\n\n\nres\n\n$par\n[1] 1.321366\n\n$value\n[1] 1.924474\n\n$counts\nfunction gradient \n     151       14 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\nwhich is close to the true value of the parameter 1 (which was used to generate the data).\n\n\nLet's try again with another parameter value, for example ( ). We have to generate y again:\n\n\ny2 &lt;- 2.5 + u + e\n\n\nand slightly modify the likelihood:\n\n\ndenssim2 &lt;- function(theta) {\n  loglik &lt;- mean(\n    sapply(\n      y2,\n      function(y2) log(mean((1/sqrt(2 * pi)) * exp(-(y2 - theta + log(-log(runif(simreps))))^2/2)))))\n  return(-loglik)\n}\n\n\nwhich can then be maximized:\n\n\nsystem.time(res2 &lt;- optim(0.1, denssim2, method = \"BFGS\", control = list(maxit = simreps)))\n\n   user  system elapsed \n  17.41    0.31   17.84 \n\n\n\nThe value that maximizes the likelihood is:\n\n\nres2\n\n$par\n[1] 2.662668\n\n$value\n[1] 1.922372\n\n$counts\nfunction gradient \n      81       14 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\nwhich is close to the true value of the parameter 2.5 (which was used to generate the data)."
  },
  {
    "objectID": "blog/2025/01/github_pages_setup_with_quarto.html",
    "href": "blog/2025/01/github_pages_setup_with_quarto.html",
    "title": "github pages setup for this website",
    "section": "",
    "text": "Desired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\n\n1 + 1"
  },
  {
    "objectID": "blog/2024/11/haha.html",
    "href": "blog/2024/11/haha.html",
    "title": "November blog post",
    "section": "",
    "text": "Undesired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\n\n1 + 1"
  },
  {
    "objectID": "blog/2013/01/2013-01-16-simulated-maximum-likelihood-with-r.html",
    "href": "blog/2013/01/2013-01-16-simulated-maximum-likelihood-with-r.html",
    "title": "Simulated Maximum Likelihood with R",
    "section": "",
    "text": "This document details section 12.4.5. Unobserved Heterogeneity Example from Cameron and Trivedi’s book - MICROECONOMETRICS: Methods and Applications. The original source code giving the results from table 12.2 are available from the authors' site here and written for Stata. This is an attempt to translate the code to R. I’d like to thank Reddit user anonemouse2010 for his advice which helped me write the function.\n\n\nConsult the original source code if you want to read the authors' comments. If you want the R source code without all the commentaries, grab it here. This is not guaranteed to work, nor to be correct. It could set your pet on fire and/or eat your first born. Use at your own risk. I may, or may not, expand this example. Corrections, constructive criticism are welcome.\n\n\nThe model is \\(y=\\theta+u+\\varepsilon\\) where \\(\\theta\\) is a scalar parameter equal to 1. \\(u\\) is extreme value type 1 (Gumbel distribution), \\(\\varepsilon \\leadsto \\mathbb{N}(0,1)\\). For more details, consult the book.\n\n\nImport the data\n\n\nYou can consult the original source code to see how the authors simulated the data. To get the same results, and verify that I didn't make mistakes I prefer importing their data directly from their website.\n\n\ndata &lt;- read.table(\"http://cameron.econ.ucdavis.edu/mmabook/mma12p2mslmsm.asc\")\nu &lt;- data[, 1]\ne &lt;- data[, 2]\ny &lt;- data[, 3]\nnumobs &lt;- length(u)\nsimreps &lt;- 10000\n\n\nSimulation\n\n\nIn the code below, the following likelihood function:\n\\[\\log{\\hat{L}_N(\\theta)} = \\dfrac{1}{N} \\sum_{i=1}^N\\log{\\big( \\dfrac{1}{S}\\sum_{s=1}^S \\dfrac{1}{\\sqrt{2\\pi}} \\exp \\{ -(-y_i-\\theta-u_i^s)^2/2 \\}\\big)}\\]\nwhich can be found on page 397 is programmed using the function sapply.\n\n\ndenssim &lt;- function(theta) {\n    loglik &lt;- mean(sapply(y, function(y) log(mean((1/sqrt(2 * pi)) * exp(-(y - theta + log(-log(runif(simreps))))^2/2)))))\n    return(-loglik)\n}\n\n\nThis likelihood is then maximized:\n\n\nsystem.time(res &lt;- optim(0.1, denssim, method = \"BFGS\", control = list(maxit = simreps)))\n\n   user  system elapsed \n  17.05    0.45   17.51 \n\n\n\nConvergence is achieved pretty rapidly, to\n\n\nres\n\n$par\n[1] 1.064092\n\n$value\n[1] 1.923841\n\n$counts\nfunction gradient \n     107        9 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\nwhich is close to the true value of the parameter 1 (which was used to generate the data).\n\n\nLet's try again with another parameter value, for example ( ). We have to generate y again:\n\n\ny2 &lt;- 2.5 + u + e\n\n\nand slightly modify the likelihood:\n\n\ndenssim2 &lt;- function(theta) {\n  loglik &lt;- mean(\n    sapply(\n      y2,\n      function(y2) log(mean((1/sqrt(2 * pi)) * exp(-(y2 - theta + log(-log(runif(simreps))))^2/2)))))\n  return(-loglik)\n}\n\n\nwhich can then be maximized:\n\n\nsystem.time(res2 &lt;- optim(0.1, denssim2, method = \"BFGS\", control = list(maxit = simreps)))\n\n   user  system elapsed \n   7.69    0.10    7.94 \n\n\n\nThe value that maximizes the likelihood is:\n\n\nres2\n\n$par\n[1] 0.1\n\n$value\n[1] 3.014571\n\n$counts\nfunction gradient \n      50        3 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\nwhich is close to the true value of the parameter 2.5 (which was used to generate the data)."
  },
  {
    "objectID": "blog/2024/12/huhu.html",
    "href": "blog/2024/12/huhu.html",
    "title": "huhu",
    "section": "",
    "text": "Undesired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\n\n1 + 1"
  },
  {
    "objectID": "blog/2024-12-09-huhu.html",
    "href": "blog/2024-12-09-huhu.html",
    "title": "huhu",
    "section": "",
    "text": "Undesired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\n\n1 + 1"
  },
  {
    "objectID": "blog/2013-01-16-simulated-maximum-likelihood-with-r.html",
    "href": "blog/2013-01-16-simulated-maximum-likelihood-with-r.html",
    "title": "Simulated Maximum Likelihood with R",
    "section": "",
    "text": "This document details section 12.4.5. Unobserved Heterogeneity Example from Cameron and Trivedi’s book - MICROECONOMETRICS: Methods and Applications. The original source code giving the results from table 12.2 are available from the authors' site here and written for Stata. This is an attempt to translate the code to R. I’d like to thank Reddit user anonemouse2010 for his advice which helped me write the function.\n\n\nConsult the original source code if you want to read the authors' comments. If you want the R source code without all the commentaries, grab it here. This is not guaranteed to work, nor to be correct. It could set your pet on fire and/or eat your first born. Use at your own risk. I may, or may not, expand this example. Corrections, constructive criticism are welcome.\n\n\nThe model is \\(y=\\theta+u+\\varepsilon\\) where \\(\\theta\\) is a scalar parameter equal to 1. \\(u\\) is extreme value type 1 (Gumbel distribution), \\(\\varepsilon \\leadsto \\mathbb{N}(0,1)\\). For more details, consult the book.\n\n\nImport the data\n\n\nYou can consult the original source code to see how the authors simulated the data. To get the same results, and verify that I didn't make mistakes I prefer importing their data directly from their website.\n\n\ndata &lt;- read.table(\"http://cameron.econ.ucdavis.edu/mmabook/mma12p2mslmsm.asc\")\nu &lt;- data[, 1]\ne &lt;- data[, 2]\ny &lt;- data[, 3]\nnumobs &lt;- length(u)\nsimreps &lt;- 10000\n\n\nSimulation\n\n\nIn the code below, the following likelihood function:\n\\[\\log{\\hat{L}_N(\\theta)} = \\dfrac{1}{N} \\sum_{i=1}^N\\log{\\big( \\dfrac{1}{S}\\sum_{s=1}^S \\dfrac{1}{\\sqrt{2\\pi}} \\exp \\{ -(-y_i-\\theta-u_i^s)^2/2 \\}\\big)}\\]\nwhich can be found on page 397 is programmed using the function sapply.\n\n\ndenssim &lt;- function(theta) {\n    loglik &lt;- mean(sapply(y, function(y) log(mean((1/sqrt(2 * pi)) * exp(-(y - theta + log(-log(runif(simreps))))^2/2)))))\n    return(-loglik)\n}\n\n\nThis likelihood is then maximized:\n\n\nsystem.time(res &lt;- optim(0.1, denssim, method = \"BFGS\", control = list(maxit = simreps)))\n\n   user  system elapsed \n   7.42    0.12    7.55 \n\n\n\nConvergence is achieved pretty rapidly, to\n\n\nres\n\n$par\n[1] 1.110683\n\n$value\n[1] 1.922757\n\n$counts\nfunction gradient \n      40        7 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\nwhich is close to the true value of the parameter 1 (which was used to generate the data).\n\n\nLet's try again with another parameter value, for example ( ). We have to generate y again:\n\n\ny2 &lt;- 2.5 + u + e\n\n\nand slightly modify the likelihood:\n\n\ndenssim2 &lt;- function(theta) {\n  loglik &lt;- mean(\n    sapply(\n      y2,\n      function(y2) log(mean((1/sqrt(2 * pi)) * exp(-(y2 - theta + log(-log(runif(simreps))))^2/2)))))\n  return(-loglik)\n}\n\n\nwhich can then be maximized:\n\n\nsystem.time(res2 &lt;- optim(0.1, denssim2, method = \"BFGS\", control = list(maxit = simreps)))\n\n   user  system elapsed \n   6.20    0.13    6.32 \n\n\n\nThe value that maximizes the likelihood is:\n\n\nres2\n\n$par\n[1] 0.1774183\n\n$value\n[1] 2.959929\n\n$counts\nfunction gradient \n      39        4 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\nwhich is close to the true value of the parameter 2.5 (which was used to generate the data)."
  },
  {
    "objectID": "blog/2024-11-09-haha.html",
    "href": "blog/2024-11-09-haha.html",
    "title": "November blog post",
    "section": "",
    "text": "Undesired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\n\n1 + 1"
  },
  {
    "objectID": "blog/2025-01-09-github_pages_setup_with_quarto.html",
    "href": "blog/2025-01-09-github_pages_setup_with_quarto.html",
    "title": "github pages setup for this website",
    "section": "",
    "text": "Desired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\n\n1 + 1"
  },
  {
    "objectID": "blog/2013-01-29-method-of-simulated-moments-with-r.html",
    "href": "blog/2013-01-29-method-of-simulated-moments-with-r.html",
    "title": "Method of Simulated Moments with R",
    "section": "",
    "text": "This document details section 12.5.6. Unobserved Heterogeneity Example. The original source code giving the results from table 12.3 are available from the authors' site here and written for Stata. This is an attempt to translate the code to R.\n\n\nConsult the original source code if you want to read the authors' comments. If you want the R source code without all the commentaries, grab it here. This is not guaranteed to work, nor to be correct. It could set your pet on fire and/or eat your first born. Use at your own risk. I may, or may not, expand this example. Corrections, constructive criticism are welcome.\n\n\nThe model is the same as the one described here, so I won't go into details. The moment condition used is $ E[(y_i--u_i)]=0 $, so we can replace the expectation operator by the empirical mean:\n\n\n\\[\\dfrac{1}{N} \\sum_{i=1}^N(y_i - \\theta - E[u_i])=0\\]\n\n\nSupposing that $E[] $ is unknown, we can instead use the method of simulated moments for \\(\\theta\\) defined by:\n\n\n\\[\\dfrac{1}{N} \\sum_{i=1}^N(y_i - \\theta - \\dfrac{1}{S} \\sum_{s=1}^S u_i^s)=0\\]\n\n\nImport the data\n\n\nYou can consult the original source code to see how the authors simulated the data. To get the same results, and verify that I didn't make mistakes I prefer importing their data directly from their website.\n\n\ndata &lt;- read.table(\"http://cameron.econ.ucdavis.edu/mmabook/mma12p2mslmsm.asc\")\nu &lt;- data[, 1]\ne &lt;- data[, 2]\ny &lt;- data[, 3]\nnumobs &lt;- length(u)\nsimreps &lt;- 10000\n\n\nSimulation\n\n\nIn the code below, we simulate the equation defined above:\n\n\nusim &lt;- -log(-log(runif(simreps)))\nesim &lt;- rnorm(simreps, 0, 1)\n\nisim &lt;- 0\nwhile (isim &lt; simreps) {\n\n    usim = usim - log(-log(runif(simreps)))\n    esim = esim + rnorm(simreps, 0, 1)\n\n    isim = isim + 1\n\n}\n\nusimbar = usim/simreps\nesimbar = esim/simreps\n\ntheta = y - usimbar - esimbar\n\ntheta_msm &lt;- mean(theta)\napprox_sterror &lt;- sd(theta)/sqrt(simreps)\n\n\nThese steps yield the following results:\n\n\ntheta_msm\n\n[1] 1.187865\n\n\nand\n\napprox_sterror\n\n[1] 0.01676098"
  },
  {
    "objectID": "posts/2024-12-09-huhu.html",
    "href": "posts/2024-12-09-huhu.html",
    "title": "huhu",
    "section": "",
    "text": "Undesired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\n\n1 + 1"
  },
  {
    "objectID": "posts/2025-01-09-github_pages_setup_with_quarto.html",
    "href": "posts/2025-01-09-github_pages_setup_with_quarto.html",
    "title": "github pages setup for this website",
    "section": "",
    "text": "Desired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\n\n1 + 1"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks, presentations, workshops…",
    "section": "",
    "text": "BUILDING REPRODUCIBLE ANALYTICAL PIPELINES WITH R, DOCKER AND NIX: online talk for RLadies Rome, on the 29th of June 2024. Repository, Slides and video.\nRix: reproducible environments with Nix: talk I gave for the online part of the 2024 edition of useR! in Salzburg. Video\nRIX: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk and tutorial I gave for the 2024 edition of the Rencontres R in Vannes, Brittany, France. Slides and tutorial. No video available.\nRix: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk I gave on the 7th of June, 2024 for the INED (Institute Nationale d’Études Démographiques, National institute for demographic studies). Repository, Slides and Video.\nReproducible research with Nix and rix, a talk I gave at the DIPF, Leibniz Institute for Research and Information in Education, in LOS! (Leibniz Open Science!), the event series of the Leibniz Strategy Forum on Open Science. The workshop was held online, on May 16, 2024. Repository, Slides and video of the talk\n“rix: An R package for reproducible dev environments with Nix (FOSDEM 2024)”, a talk I gave at FOSDEM 2024 on the 4th of February 2024 in the Nix and NixOS dev room. Video here.\n“Building reproducible analytical pipelines”, Workshop I gave for the “R User Group Tunis” on December 9th, 2023. Video here. Slides and code here.\n“Porquê usar monads (programação funcional)? Uma ilustração com o pacote {chronicler}” (Why use monads (functional programming)? An illustration with the {chronicler} package), Talk I gave (online) at the INE (Instituto Nacional de Estatística), on the 7th of June 2023. Slides in Portuguese.\n“La reproductibilité avec R, ou pourquoi celle-ci est située sur un continuum” (Reproducibility with R, or why it lies on a continuum). Talk I gave at the Rencontres R 2023 in Avignon, France, on the 26th of June 2023. Slides in French, Video with English subtitles.\n“Building reproducible analytical pipelines”, Workshop I gave for “Workshops for Ukraine” on June 29th, 2023. You can pull the Docker image with the slides and code here.\n“Building reproducible analytical pipelines”, Talk I gave for ReproducibiliTea UCL on the 19th of July, 2023. Slides and Video. You can pull the Docker image with the slides and code here."
  },
  {
    "objectID": "talks.html#talks-presentation-workshops",
    "href": "talks.html#talks-presentation-workshops",
    "title": "Talks",
    "section": "",
    "text": "BUILDING REPRODUCIBLE ANALYTICAL PIPELINES WITH R, DOCKER AND NIX: online talk for RLadies Rome, on the 29th of June 2024. Repository, Slides and video.\nRix: reproducible environments with Nix: talk I gave for the online part of the 2024 edition of useR! in Salzburg. Video\nRIX: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk and tutorial I gave for the 2024 edition of the Rencontres R in Vannes, Brittany, France. Slides and tutorial. No video available.\nRix: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk I gave on the 7th of June, 2024 for the INED (Institute Nationale d’Études Démographiques, National institute for demographic studies). Repository, Slides and Video.\nReproducible research with Nix and rix, a talk I gave at the DIPF, Leibniz Institute for Research and Information in Education, in LOS! (Leibniz Open Science!), the event series of the Leibniz Strategy Forum on Open Science. The workshop was held online, on May 16, 2024. Repository, Slides and video of the talk\n“rix: An R package for reproducible dev environments with Nix (FOSDEM 2024)”, a talk I gave at FOSDEM 2024 on the 4th of February 2024 in the Nix and NixOS dev room. Video here.\n“Building reproducible analytical pipelines”, Workshop I gave for the “R User Group Tunis” on December 9th, 2023. Video here. Slides and code here.\n“Porquê usar monads (programação funcional)? Uma ilustração com o pacote {chronicler}” (Why use monads (functional programming)? An illustration with the {chronicler} package), Talk I gave (online) at the INE (Instituto Nacional de Estatística), on the 7th of June 2023. Slides in Portuguese.\n“La reproductibilité avec R, ou pourquoi celle-ci est située sur un continuum” (Reproducibility with R, or why it lies on a continuum). Talk I gave at the Rencontres R 2023 in Avignon, France, on the 26th of June 2023. Slides in French, Video with English subtitles.\n“Building reproducible analytical pipelines”, Workshop I gave for “Workshops for Ukraine” on June 29th, 2023. You can pull the Docker image with the slides and code here.\n“Building reproducible analytical pipelines”, Talk I gave for ReproducibiliTea UCL on the 19th of July, 2023. Slides and Video. You can pull the Docker image with the slides and code here."
  },
  {
    "objectID": "talks.html#interviews-podcasts",
    "href": "talks.html#interviews-podcasts",
    "title": "Talks, presentations, workshops…",
    "section": "Interviews, podcasts…",
    "text": "Interviews, podcasts…\n\nFrench Data Workers Podcast #1 Large Scale Testing : Contenir la covid-19 avec des dépistages ciblés\nFrench Bruno Rodrigues défend une approche basée sur la reproductibilité de la data science au Luxembourg\nEnglish Leanpub Frontmatter Podcast #263"
  },
  {
    "objectID": "talks.html#talks",
    "href": "talks.html#talks",
    "title": "Talks, presentations, workshops…",
    "section": "",
    "text": "BUILDING REPRODUCIBLE ANALYTICAL PIPELINES WITH R, DOCKER AND NIX: online talk for RLadies Rome, on the 29th of June 2024. Repository, Slides and video.\nRix: reproducible environments with Nix: talk I gave for the online part of the 2024 edition of useR! in Salzburg. Video\nRIX: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk and tutorial I gave for the 2024 edition of the Rencontres R in Vannes, Brittany, France. Slides and tutorial. No video available.\nRix: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk I gave on the 7th of June, 2024 for the INED (Institute Nationale d’Études Démographiques, National institute for demographic studies). Repository, Slides and Video.\nReproducible research with Nix and rix, a talk I gave at the DIPF, Leibniz Institute for Research and Information in Education, in LOS! (Leibniz Open Science!), the event series of the Leibniz Strategy Forum on Open Science. The workshop was held online, on May 16, 2024. Repository, Slides and video of the talk\n“rix: An R package for reproducible dev environments with Nix (FOSDEM 2024)”, a talk I gave at FOSDEM 2024 on the 4th of February 2024 in the Nix and NixOS dev room. Video here.\n“Building reproducible analytical pipelines”, Workshop I gave for the “R User Group Tunis” on December 9th, 2023. Video here. Slides and code here.\n“Porquê usar monads (programação funcional)? Uma ilustração com o pacote {chronicler}” (Why use monads (functional programming)? An illustration with the {chronicler} package), Talk I gave (online) at the INE (Instituto Nacional de Estatística), on the 7th of June 2023. Slides in Portuguese.\n“La reproductibilité avec R, ou pourquoi celle-ci est située sur un continuum” (Reproducibility with R, or why it lies on a continuum). Talk I gave at the Rencontres R 2023 in Avignon, France, on the 26th of June 2023. Slides in French, Video with English subtitles.\n“Building reproducible analytical pipelines”, Workshop I gave for “Workshops for Ukraine” on June 29th, 2023. You can pull the Docker image with the slides and code here.\n“Building reproducible analytical pipelines”, Talk I gave for ReproducibiliTea UCL on the 19th of July, 2023. Slides and Video. You can pull the Docker image with the slides and code here."
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "Build reproducible analytical pipelines that will output consistent, high-quality analyses using the R programming language, Git, Github and Docker. Learn about functional and literate programming to keep your code concise, easier to test, easier to share and easily understandable by others.\n\n\n\n\nRead it for free: https://raps-with-r.dev/\n\n\nBuy the DRM free Epub or PDF: https://leanpub.com/raps-with-r/\n\n\nBuy a physical copy on Amazon: https://www.amazon.com/Building-reproducible-analytical-pipelines-R/dp/B0C87H6MGF\n\n\n\n\n\n\nThis book focuses on teaching you how to use R in a modern way. Why modern? Because you will learn about the tidyverse collection of packages, which enable you to write efficient, legible code. You will also learn about functional programming, some statistical methods, writing your own packages… By the end of the book, you will have all the knowledge and tools on hand to use R for your day-to-day data science tasks!\n\n\n\n\nRead it for free: https://modern-rstats.eu/\n\n\n\nBuy the DRM free Epub or PDF (very out of date, update coming Q4-2025): https://leanpub.com/modern_tidyverse/\n\n\nBuy a physical copy (coming one day)"
  },
  {
    "objectID": "books.html#building-reproducible-analytical-pipelines-with-r",
    "href": "books.html#building-reproducible-analytical-pipelines-with-r",
    "title": "Books",
    "section": "",
    "text": "Build reproducible analytical pipelines that will output consistent, high-quality analyses using the R programming language, Git, Github and Docker. Learn about functional and literate programming to keep your code concise, easier to test, easier to share and easily understandable by others.\n\n\n\n\nRead it for free: https://raps-with-r.dev/\n\n\nBuy the DRM free Epub or PDF: https://leanpub.com/raps-with-r/\n\n\nBuy a physical copy on Amazon: https://www.amazon.com/Building-reproducible-analytical-pipelines-R/dp/B0C87H6MGF\n\n\n\n\n\n\nThis book focuses on teaching you how to use R in a modern way. Why modern? Because you will learn about the tidyverse collection of packages, which enable you to write efficient, legible code. You will also learn about functional programming, some statistical methods, writing your own packages… By the end of the book, you will have all the knowledge and tools on hand to use R for your day-to-day data science tasks!\n\n\n\n\nRead it for free: https://modern-rstats.eu/\n\n\n\nBuy the DRM free Epub or PDF (very out of date, update coming Q4-2025): https://leanpub.com/modern_tidyverse/\n\n\nBuy a physical copy (coming one day)"
  },
  {
    "objectID": "posts/2013-01-16-simulated-maximum-likelihood-with-r.html",
    "href": "posts/2013-01-16-simulated-maximum-likelihood-with-r.html",
    "title": "Simulated Maximum Likelihood with R",
    "section": "",
    "text": "This document details section 12.4.5. Unobserved Heterogeneity Example from Cameron and Trivedi’s book - MICROECONOMETRICS: Methods and Applications. The original source code giving the results from table 12.2 are available from the authors' site here and written for Stata. This is an attempt to translate the code to R. I’d like to thank Reddit user anonemouse2010 for his advice which helped me write the function.\n\n\nConsult the original source code if you want to read the authors' comments. If you want the R source code without all the commentaries, grab it here. This is not guaranteed to work, nor to be correct. It could set your pet on fire and/or eat your first born. Use at your own risk. I may, or may not, expand this example. Corrections, constructive criticism are welcome.\n\n\nThe model is \\(y=\\theta+u+\\varepsilon\\) where \\(\\theta\\) is a scalar parameter equal to 1. \\(u\\) is extreme value type 1 (Gumbel distribution), \\(\\varepsilon \\leadsto \\mathbb{N}(0,1)\\). For more details, consult the book.\n\n\nImport the data\n\n\nYou can consult the original source code to see how the authors simulated the data. To get the same results, and verify that I didn't make mistakes I prefer importing their data directly from their website.\n\n\ndata &lt;- read.table(\"http://cameron.econ.ucdavis.edu/mmabook/mma12p2mslmsm.asc\")\nu &lt;- data[, 1]\ne &lt;- data[, 2]\ny &lt;- data[, 3]\nnumobs &lt;- length(u)\nsimreps &lt;- 10000\n\n\nSimulation\n\n\nIn the code below, the following likelihood function:\n\\[\\log{\\hat{L}_N(\\theta)} = \\dfrac{1}{N} \\sum_{i=1}^N\\log{\\big( \\dfrac{1}{S}\\sum_{s=1}^S \\dfrac{1}{\\sqrt{2\\pi}} \\exp \\{ -(-y_i-\\theta-u_i^s)^2/2 \\}\\big)}\\]\nwhich can be found on page 397 is programmed using the function sapply.\n\n\ndenssim &lt;- function(theta) {\n    loglik &lt;- mean(sapply(y, function(y) log(mean((1/sqrt(2 * pi)) * exp(-(y - theta + log(-log(runif(simreps))))^2/2)))))\n    return(-loglik)\n}\n\n\nThis likelihood is then maximized:\n\n\nsystem.time(res &lt;- optim(0.1, denssim, method = \"BFGS\", control = list(maxit = simreps)))\n\n   user  system elapsed \n  1.460   0.197   1.657 \n\n\n\nConvergence is achieved pretty rapidly, to\n\n\nres\n\n$par\n[1] 1.193006\n\n$value\n[1] 1.921685\n\n$counts\nfunction gradient \n      58       10 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\nwhich is close to the true value of the parameter 1 (which was used to generate the data).\n\n\nLet's try again with another parameter value, for example ( ). We have to generate y again:\n\n\ny2 &lt;- 2.5 + u + e\n\n\nand slightly modify the likelihood:\n\n\ndenssim2 &lt;- function(theta) {\n  loglik &lt;- mean(\n    sapply(\n      y2,\n      function(y2) log(mean((1/sqrt(2 * pi)) * exp(-(y2 - theta + log(-log(runif(simreps))))^2/2)))))\n  return(-loglik)\n}\n\n\nwhich can then be maximized:\n\n\nsystem.time(res2 &lt;- optim(0.1, denssim2, method = \"BFGS\", control = list(maxit = simreps)))\n\n   user  system elapsed \n  1.321   0.000   1.322 \n\n\n\nThe value that maximizes the likelihood is:\n\n\nres2\n\n$par\n[1] 2.73753\n\n$value\n[1] 1.92257\n\n$counts\nfunction gradient \n      49       10 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\nwhich is close to the true value of the parameter 2.5 (which was used to generate the data)."
  },
  {
    "objectID": "posts/2013-01-29-method-of-simulated-moments-with-r.html",
    "href": "posts/2013-01-29-method-of-simulated-moments-with-r.html",
    "title": "Method of Simulated Moments with R",
    "section": "",
    "text": "This document details section 12.5.6. Unobserved Heterogeneity Example. The original source code giving the results from table 12.3 are available from the authors' site here and written for Stata. This is an attempt to translate the code to R.\n\n\nConsult the original source code if you want to read the authors' comments. If you want the R source code without all the commentaries, grab it here. This is not guaranteed to work, nor to be correct. It could set your pet on fire and/or eat your first born. Use at your own risk. I may, or may not, expand this example. Corrections, constructive criticism are welcome.\n\n\nThe model is the same as the one described here, so I won't go into details. The moment condition used is \\(E[(y_i-\\theta-u_i)]=0\\), so we can replace the expectation operator by the empirical mean:\n\n\n\\[\\dfrac{1}{N} \\sum_{i=1}^N(y_i - \\theta - E[u_i])=0\\]\n\n\nSupposing that \\(E[\\overline{u}]\\) is unknown, we can instead use the method of simulated moments for \\(\\theta\\) defined by:\n\n\n\\[\\dfrac{1}{N} \\sum_{i=1}^N(y_i - \\theta - \\dfrac{1}{S} \\sum_{s=1}^S u_i^s)=0\\]\n\n\nImport the data\n\n\nYou can consult the original source code to see how the authors simulated the data. To get the same results, and verify that I didn't make mistakes I prefer importing their data directly from their website.\n\n\ndata &lt;- read.table(\"http://cameron.econ.ucdavis.edu/mmabook/mma12p2mslmsm.asc\")\nu &lt;- data[, 1]\ne &lt;- data[, 2]\ny &lt;- data[, 3]\nnumobs &lt;- length(u)\nsimreps &lt;- 10000\n\n\nSimulation\n\n\nIn the code below, we simulate the equation defined above:\n\n\nusim &lt;- -log(-log(runif(simreps)))\nesim &lt;- rnorm(simreps, 0, 1)\n\nisim &lt;- 0\nwhile (isim &lt; simreps) {\n\n    usim = usim - log(-log(runif(simreps)))\n    esim = esim + rnorm(simreps, 0, 1)\n\n    isim = isim + 1\n\n}\n\nusimbar = usim/simreps\nesimbar = esim/simreps\n\ntheta = y - usimbar - esimbar\n\ntheta_msm &lt;- mean(theta)\napprox_sterror &lt;- sd(theta)/sqrt(simreps)\n\n\nThese steps yield the following results:\n\n\ntheta_msm\n\n[1] 1.187978\n\n\nand\n\napprox_sterror\n\n[1] 0.01676286"
  },
  {
    "objectID": "posts/2013-11-07-gmm-with-rmd.html",
    "href": "posts/2013-11-07-gmm-with-rmd.html",
    "title": "Nonlinear Gmm with R - Example with a logistic regression",
    "section": "",
    "text": "In this post, I will explain how you can use the R gmm package to estimate a non-linear model, and more specifically a logit model. For my research, I have to estimate Euler equations using the Generalized Method of Moments. I contacted Pierre Chaussé, the creator of the gmm library for help, since I was having some difficulties. I am very grateful for his help (without him, I'd still probably be trying to estimate my model!).\n\n\nTheoretical background, motivation and data set\n\n\nI will not dwell in the theory too much, because you can find everything you need here. I think it’s more interesting to try to understand why someone would use the Generalized Method of Moments instead of maximization of the log-likelihood. Well, in some cases, getting the log-likelihood can be quite complicated, as can be the case for arbitrary, non-linear models (for example if you want to estimate the parameters of a very non-linear utility function). Also, moment conditions can sometimes be readily available, so using GMM instead of MLE is trivial. And finally, GMM is… well, a very general method: every popular estimator can be obtained as a special case of the GMM estimator, which makes it quite useful.\n\n\nAnother question that I think is important to answer is: why this post? Well, because that’s exactly the kind of post I would have loved to have found 2 months ago, when I was beginning to work with the GMM. Most posts I found presented the gmm package with very simple and trivial examples, which weren’t very helpful. The example presented below is not very complicated per se, but much more closer to a real-world problem than most stuff that is out there. At least, I hope you will find it useful!\n\n\nFor illustration purposes, I'll use data from Marno Verbeek's A guide to modern Econometrics, used in the illustration on page 197. You can download the data from the book's companion page here under the section Data sets or from the Ecdat package in R, which I’ll be using.\n\n\nImplementation in R\n\n\nI don't estimate the exact same model, but only use a subset of the variables available in the data set. Keep in mind that this post is just for illustration purposes.\n\n\nFirst load the gmm package and load the data set:\n\n\nlibrary(gmm)\nlibrary(Ecdat)\ndata(\"Benefits\")\n\nBenefits &lt;- transform(\n  Benefits,\n  age2 = age**2,\n  rr2 = rr**2\n  )\n\n\nWe can then estimate a logit model with the glm() function:\n\n\nnative &lt;- glm(ui ~ age + age2 + dkids + dykids + head + male + married + rr + rr2,\n              data = Benefits,\n              family = binomial(link = \"logit\"),\n              na.action = na.pass)\n\nsummary(native)\n\n## \n## Call:\n## glm(formula = y ~ age + age2 + dkids + dykids + head + male + \n##     married + rr + rr2, family = binomial(link = \"logit\"), na.action = na.pass)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -1.889  -1.379   0.788   0.896   1.237  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)   \n## (Intercept) -1.00534    0.56330   -1.78   0.0743 . \n## age          0.04909    0.02300    2.13   0.0328 * \n## age2        -0.00308    0.00293   -1.05   0.2924   \n## dkids       -0.10922    0.08374   -1.30   0.1921   \n## dykids       0.20355    0.09490    2.14   0.0320 * \n## head        -0.21534    0.07941   -2.71   0.0067 **\n## male        -0.05988    0.08456   -0.71   0.4788   \n## married      0.23354    0.07656    3.05   0.0023 **\n## rr           3.48590    1.81789    1.92   0.0552 . \n## rr2         -5.00129    2.27591   -2.20   0.0280 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 6086.1  on 4876  degrees of freedom\n## Residual deviance: 5983.9  on 4867  degrees of freedom\n## AIC: 6004\n## \n## Number of Fisher Scoring iterations: 4\n\n\nNow comes the interesting part: how can you estimate such a non-linear model with the gmm() function from the gmm package?\n\n\nFor every estimation with the Generalized Method of Moments, you will need valid moment conditions. It turns out that in the case of the logit model, this moment condition is quite simple:\n\n\\[E[X' * (Y-\\Lambda(X'\\theta))] = 0\\]\n\nwhere ( () ) is the logistic function. Let's translate this condition into code. First, we need the logistic function:\n\n\nlogistic &lt;- function(theta, data) {\n    return(1/(1 + exp(-data %*% theta)))\n}\n\n\nand let's also define a new data frame, to make our life easier with the moment conditions (don’t forget to add a column of ones to the matrix, hence the 1 after y):\n\n\ndat &lt;- data.matrix(with(Benefits,\n                        cbind(ui, 1, age, age2, dkids,\n                              dykids, head, sex,\n                              married, rr, rr2)))\n\n\nand now the moment condition itself:\n\n\nmoments &lt;- function(theta, data) {\n  y &lt;- as.numeric(data[, 1])\n  x &lt;- data.matrix(data[, 2:11])\n  m &lt;- x * as.vector((y - logistic(theta, x)))\n  return(cbind(m))\n}\n\n\nThe moment condition(s) are given by a function which returns a matrix with as many columns as moment conditions (same number of columns as parameters for just-identified models).\n\n\nTo use the gmm() function to estimate our model, we need to specify some initial values to get the maximization routine going. One neat trick is simply to use the coefficients of a linear regression; I found it to work well in a lot of situations:\n\n\ninit &lt;- (lm(ui ~ age + age2 + dkids + dykids + head + sex + married + rr + rr2,\n            data = Benefits))$coefficients\n\n\nAnd finally, we have everything to use gmm():\n\n\nmy_gmm &lt;- gmm(moments, x = dat, t0 = init, type = \"iterative\", crit = 1e-25, wmatrix = \"optimal\", method = \"Nelder-Mead\", control = list(reltol = 1e-25, maxit = 20000))\n\nsummary(my_gmm)\n\n\nPlease, notice the options crit=1e-25,method=“Nelder-Mead”,control=list(reltol=1e-25,maxit=20000): these options mean that the Nelder-Mead algorithm is used, and to specify further options to the Nelder-Mead algorithm, the control option is used. This is very important, as Pierre Chaussé explained to me: non-linear optimization is an art, and most of the time the default options won't cut it and will give you false results. To add insult to injury, the Generalized Method of Moments itself is very capricious and you will also have to play around with different initial values to get good results. As you can see, the Convergence code equals 10, which is a code specific to the Nelder-Mead method which indicates «degeneracy of the Nelder–Mead simplex.» . I’m not sure if this is a bad thing though, but other methods can give you better results. I’d suggest you try always different maximization routines with different starting values to see if your estimations are robust. Here, the results are very similar to what we obtained with the built-in function glm() so we can stop here.\n\n\nShould you notice any error whatsoever, do not hesitate to tell me."
  },
  {
    "objectID": "posts/2013-12-31-r-cas.html",
    "href": "posts/2013-12-31-r-cas.html",
    "title": "Using R as a Computer Algebra System with Ryacas",
    "section": "",
    "text": "R is used to perform statistical analysis and doesn't focus on symbolic maths. But it is sometimes useful to let the computer derive a function for you (and have the analytic expression of said derivative), but maybe you don't want to leave your comfy R shell. It is possible to turn R into a full-fledged computer algebra system. CASs are tools that perform symbolic operations, such as getting the expression of the derivative of a user-defined (and thus completely arbitrary) function. Popular CASs include the proprietary Mathematica and Maple. There exists a lot of CASs under a Free Software license, Maxima (based on the very old Macsyma), Yacas, Xcas… In this post I will focus on Yacas and the Ryacas libarary. There is also the possibility to use the rSympy library that uses the Sympy Python library, which has a lot more features than Yacas. However, depending on your operating system installation can be tricky as it also requires rJava as a dependency.\n\n\nEven though Ryacas is quite nice to have, there are some issues though. For example, let's say you want the first derivative of a certain function f. If you use Ryacas to get it, the returned object won't be a function. There is a way to “extract” the text from the returned object and make a function out of it. But there are still other issues; I'll discuss them later.\n\n\nInstallation\n\n\nInstallation should be rather painless. On Linux you need to install Yacas first, which should be available in the major distros' repositories. Then you can install Ryacas from within the R shell. On Windows, you need to run these three commands (don't bother installing Yacas first):\n\n\ninstall.packages(Ryacas)\nlibrary(Ryacas)\nyacasInstall()\n\n\nYou can find more information on the project's page.\n\n\nExample session\n\n\nFirst, you must load Ryacas and define symbols that you will use in your functions.\n\n\nlibrary(Ryacas)\n\n## Loading required package: Ryacas Loading required package: XML\n\n\nx &lt;- Sym(\"x\")\n\n\nYou can then define your fonctions:\n\n\nmy_func &lt;- function(x) {\n  return(x/(x^2 + 3))\n}\n\n\nAnd you can get the derivative for instance:\n\n\nmy_deriv &lt;- yacas(deriv(my_func(x), x))\n\n## [1] \"Starting Yacas!\"\n\n\nIf you check the class of my_deriv, you'll see that it is of class yacas, which is not very useful. Let's «convert» it to a function:\n\n\nmy_deriv2 &lt;- function(x) {\n  eval(parse(text = my_deriv$YacasForm))\n}\n\n\nWe can then evaluate it. A lot of different operations are possible. But there are some problems.\n\n\nIssues with Ryacas\n\n\nYou can't use elements of a vector as parameters of your function, i.e.:\n\n\ntheta &lt;- Sym(\"theta\")\nfunc &lt;- function(x) {\n  return(theta[1] * x + theta[2])\n}\n\n\nLet's integrate this\nFunc &lt;- yacas(Integrate(func(x), x)) \n\n\nreturns (x^2theta)/2+NAx; which is not quite what we want…there is a workaround however. Define your functions like this:\n\n\na &lt;- Sym(\"a\")\nb &lt;- Sym(\"b\")\nfunc2 &lt;- function(x) {\n  return(a * x + b)\n}\n\n# Let&#39;s integrate this\nFunc2 &lt;- yacas(Integrate(func2(x), x))\n\n\nwe get the expected result: (x^2a)/2+bx;. Now replace a and b by the thetas:\n\n\nFunc2 &lt;- gsub(\"a\", \"theta[1]\", Func2$YacasForm)\nFunc2 &lt;- gsub(\"b\", \"theta[2]\", Func2)\n\n\nNow we have what we want:\n\n\nFunc2\n\n## [1] \"(x^2*theta[1])/2+theta[2]*x;\"\n\n\nYou can then copy-paste this result into a function.\n\n\nAnother problem is if you use built-in functions that are different between R and Yacas. For example:\n\n\nmy_log &lt;- function(x) {\n    return(sin(log(2 + x)))\n}\n\n\nNow try to differentiate it:\n\n\ndmy_log &lt;- yacas(deriv(my_log(x), x))\n\n\nyou get: Cos(Ln(x+2))/(x+2);. The problem with this, is that R doesn't recognize Cos as the cosine (which is cos in R) and the same goes for Ln. These are valid Yacas functions, but that is not the case in R. So you'll have to use gsub to replace these functions and then copy paste the end result into a function.\n\n\nConclusion\n\n\nWhile it has some flaws, Ryacas can be quite useful if you need to derive or integrate complicated expressions that you then want to use in R. Using some of the tricks I showed here, you should be able to overcome some of its shortcomings. If installation of rJava and thus rSympy becomes easier, I'll probably also do a short blog-post about it, as it has more features than Ryacas."
  },
  {
    "objectID": "posts/2014-04-23-r-s4-rootfinding.html",
    "href": "posts/2014-04-23-r-s4-rootfinding.html",
    "title": "Object Oriented Programming with R: An example with a Cournot duopoly",
    "section": "",
    "text": "I started reading Applied Computational Economics & Finance by Mario J. Miranda and Paul L. Fackler. It is a very interesting book that I recommend to every one of my colleagues. The only issue I have with this book, is that the programming language they use is Matlab, which is proprietary. While there is a free as in freedom implementation of the Matlab language, namely Octave, I still prefer using R. In this post, I will illustrate one example the authors present in the book with R, using the package rootSolve. rootSolve implements Newtonian algorithms to find roots of functions; to specify the functions for which I want the roots, I use R's Object-Oriented Programming (OOP) capabilities to build a model that returns two functions. This is optional, but I found that it was a good example to illustrate OOP, even though simpler solutions exist, one of which was proposed by reddit user TheDrownedKraken (whom I thank) and will be presented at the end of the article.\n\n\nTheoretical background\n\n\nThe example is taken from Miranda's and Fackler's book, on page 35. The authors present a Cournot duopoly model. In a Cournot duopoly model, two firms compete against each other by quantities. Both produce a certain quantity of an homogenous good, and take the quantity produce by their rival as given.\n\n\nThe inverse demand of the good is :\n\n\\[P(q) = q^{-\\dfrac{1}{\\eta}}\\]\n\nthe cost function for firm i is:\n\n\\[C_i(q_i) = P(q_1+q_2)*q_i - C_i(q_i)\\]\n\nand the profit for firm i:\n\n\\[\\pi_i(q1,q2) = P(q_1+q_2)q_i - C_i(q_i)\\]\n\nThe optimality condition for firm i is thus:\n\n\\[\\dfrac{\\partial \\pi_i}{\\partial q_i} = (q1+q2)^{-\\dfrac{1}{\\eta}} - \\dfrac{1}{\\eta} (q_1+q_2)^{\\dfrac{-1}{\\eta-1}}q_i - c_iq_i=0.\\]\n\nImplementation in R\n\n\nIf we want to find the optimal quantities (q_1) and (q_2) we need to program the optimality condition and we could also use the jacobian of the optimality condition. The jacobian is generally useful to speed up the root finding routines. This is were OOP is useful. First let's create a new class, called Model:\n\n\nsetClass(Class = \"Model\", slots = list(OptimCond = \"function\", JacobiOptimCond = \"function\"))\n\n\nThis new class has two slots, which here are functions (in general slots are properties of your class); we need the model to return the optimality condition and the jacobian of the optimality condition.\n\n\nNow we can create a function which will return these two functions for certain values of the parameters, c and  of the model:\n\n\nmy_mod &lt;- function(eta, c) {\n    e = -1/eta\n\n    OptimCond &lt;- function(q) {\n        return(sum(q)^e + e * sum(q)^(e - 1) * q - diag(c) %*% q)\n    }\n\n    JacobiOptimCond &lt;- function(q) {\n      return(\n        (e * sum(q)^e) * array(1, c(2, 2)) +\n        (e * sum(q)^(e - 1)) * diag(1, 2) +\n        (e - 1) * e * sum(q)^(e - 2) * q * c(1, 1) - diag(c)\n      )\n    }\n\n    return(new(\"Model\", OptimCond = OptimCond, JacobiOptimCond = JacobiOptimCond))\n\n}\n\n\nThe function my_mod takes two parameters, eta and c and returns two functions, the optimality condition and the jacobian of the optimality condition. Both are now accessible via my_mod(eta=1.6,c = c(0.6,0.8))@OptimCond and my_mod(eta=1.6,c = c(0.6,0.8))@JacobiOptimCond respectively (and by specifying values for eta and c).\n\n\nNow, we can use the rootSolve package to get the optimal values (q_1) and (q_2)\n\n\nlibrary(\"rootSolve\")\n\nmultiroot(f = my_mod(eta = 1.6, c = c(0.6, 0.8))@OptimCond,\n          start = c(1, 1),\n          maxiter = 100,\n          jacfunc = my_mod(eta = 1.6, c = c(0.6, 0.8))@JacobiOptimCond)\n\n## $root\n## [1] 0.8396 0.6888\n## \n## $f.root\n##            [,1]\n## [1,] -2.220e-09\n## [2,]  9.928e-09\n## \n## $iter\n## [1] 4\n## \n## $estim.precis\n## [1] 6.074e-09\n\n\nAfter 4 iterations, we get that  and  are equal to 0.84 and 0.69 respectively, which are the same values as in the book!\n\n\nSuggestion by Reddit user, TheDrownedKraken\n\n\nI posted this blog post on the rstats subbreddit on www.reddit.com. I got a very useful comment by reddit member TheDrownedKraken which suggested the following approach, which doesn't need a new class to be build. I thank him for this. Here is his suggestion:\n\n\ngenerator &lt;- function(eta, a) {\n    e = -1/eta\n\n    OptimCond &lt;- function(q) {\n        return(sum(q)^e + e * sum(q)^(e - 1) * q - diag(a) %*% q)\n    }\n\n    JacobiOptimCond &lt;- function(q) {\n      return(\n        (e * sum(q)^e) * array(1, c(2, 2)) +\n        (e * sum(q)^(e - 1)) * diag(1, 2) +\n        (e - 1) * e * sum(q)^(e - 2) * q * c(1, 1) - diag(a)\n      )\n    }\n\n    return(list(OptimCond = OptimCond, JacobiOptimCond = JacobiOptimCond))\n\n}\n\nf.s &lt;- generator(eta = 1.6, a = c(0.6, 0.8))\n\nmultiroot(f = f.s$OptimCond, start = c(1, 1), maxiter = 100, jacfunc = f.s$JacobiOptimCond)\n\n## $root\n## [1] 0.8396 0.6888\n## \n## $f.root\n##            [,1]\n## [1,] -2.220e-09\n## [2,]  9.928e-09\n## \n## $iter\n## [1] 4\n## \n## $estim.precis\n## [1] 6.074e-09"
  },
  {
    "objectID": "posts/2014-11-11-benchmarks-r-blas-atlas-rro.html",
    "href": "posts/2014-11-11-benchmarks-r-blas-atlas-rro.html",
    "title": "R, R with Atlas, R with OpenBLAS and Revolution R Open: which is fastest?",
    "section": "",
    "text": "In this short post, I benchmark different “versions” of R. I compare the execution speeds of R, R linked against OpenBLAS, R linked against ATLAS and Revolution R Open. Revolution R Open is a new open source version of R made by Revolution Analytics. It is linked against MKL and should offer huge speed improvements over vanilla R. Also, it uses every cores of your computer by default, without any change whatsoever to your code.\n\n\nTL;DR: Revolution R Open is the fastest of all the benchmarked versions (with R linked against OpenBLAS and ATLAS just behind), and easier to setup.\n\n\nSetup\n\n\nI benchmarked these different versions of R using R-benchmark-25.R that you can download here. This benchmark file was created by Simon Urbanek.\n\n\nI ran the benchmarks on my OpenSUSE 13.2 computer with a Pentium Dual-Core CPU E6500@2.93GHz with 4GB of Ram. It's outdated, but it's still quite fast for most of my numerical computation needs. I installed “vanilla” R from the official OpenSUSE repositories which is currently at version 3.1.2.\n\n\nThen, I downloaded OpenBLAS and ATLAS also from the official OpenSUSE repositories and made R use these libraries instead of its own implementation of BLAS. The way I did that is a bit hacky, but works: first, go to /usr/lib64/R/lib and backup libRblas.so (rename it to libRblas.soBackup for instance). Then link /usr/lib64/libopenblas.so.0 to /usr/lib64/R/lib/libRblas, and that's it, R will use OpenBLAS. For ATLAS, you can do it in the same fashion, but you'll find the library in /usr/lib64/atlas/. These paths should be the same for any GNU/Linux distribution. For other operating systems, I'm sure you can find where these libraries are with Google.\n\n\nThe last version I benchmarked was Revolution R Open. This is a new version of R released by Revolution Analytics. Revolution Analytics had their own version of R, called Revolution R, for quite some time now. They decided to release a completely free as in freedom and free as in free beer version of this product which they now renamed Revolution R Open. You can download Revolution R Open here. You can have both “vanilla” R and Revolution R Open installed on your system.\n\n\nResults\n\n\nI ran the R-benchmark-25.R 6 times for every version but will only discuss the 4 best runs.\n\n\n\n\n\nR version\n\n\nFastest run\n\n\nSlowest run\n\n\nMean Run\n\n\n\n\nVanilla R\n\n\n63.65\n\n\n66.21\n\n\n64.61\n\n\n\n\nOpenBLAS R\n\n\n15.63\n\n\n18.96\n\n\n16.94\n\n\n\n\nATLAS R\n\n\n16.92\n\n\n21.57\n\n\n18.24\n\n\n\n\nRRO\n\n\n14.96\n\n\n16.08\n\n\n15.49\n\n\n\n\nAs you can read from the table above, Revolution R Open was the fastest of the four versions, but not significantly faster than BLAS or ATLAS R. However, RRO uses all the available cores by default, so if your code relies on a lot matrix algebra, RRO might be actually a lot more faster than OpenBLAS and ATLAS R. Another advantage of RRO is that it is very easy to install, and also works with Rstudio and is compatible with every R package to existence. “Vanilla” R is much slower than the other three versions, more than 3 times as slow!\n\n\nConclusion\n\n\nWith other benchmarks, you could get other results, but I don’t think that “vanilla” R could beat any of the other three versions. Whatever your choice, I recommend not using plain, “vanilla” R. The other options are much faster than standard R, and don't require much work to set up. I'd personally recommend Revolution R Open, as it is free software and compatible with CRAN packages and Rstudio."
  },
  {
    "objectID": "posts/2015-01-12-introduction-to-programming-econometrics-with-r.html",
    "href": "posts/2015-01-12-introduction-to-programming-econometrics-with-r.html",
    "title": "Introduction to programming econometrics with R",
    "section": "",
    "text": "This semester, I’ll be teaching an introduction to applied econometrics with R, so I’ve decided to write a very small book called “Introduction to programming Econometrics with R”. This is primarily intended for bachelor students and the focus is not much on econometric theory, but more on how to implement econometric theory into computer code, using the R programming language. It’s very basic and doesn’t cover any advanced topics in econometrics and is intended for people with 0 previous programming knowledge. It is still very rough around the edges, and it’s missing the last chapter about reproducible research, and the references, but I think it’s time to put it out there; someone else than my students may find it useful. The book’s probably full of typos and mistakes, so don’t hesitate to drop me an e-mail if you find something fishy: contact@brodrigues.co\nAlso there might be some sections at the beginning that only concern my students. Just ignore that.\nGet it here: download\n\nUpdate (2017-01-22)\nYou might find the book useful as it is now, but I never had a chance to finish it. I might get back to it once I’ll have more time, and port it to bookdown."
  },
  {
    "objectID": "posts/2015-02-22-export-r-output-to-file.html",
    "href": "posts/2015-02-22-export-r-output-to-file.html",
    "title": "Export R output to a file",
    "section": "",
    "text": "Sometimes it is useful to export the output of a long-running R command. For example, you might want to run a time consuming regression just before leaving work on Friday night, but would like to get the output saved inside your Dropbox folder to take a look at the results before going back to work on Monday.\n\n\nThis can be achieved very easily using capture.output() and cat() like so:\n\nout &lt;- capture.output(summary(my_very_time_consuming_regression))\n\ncat(\"My title\", out, file=\"summary_of_my_very_time_consuming_regression.txt\", sep=\"\\n\", append=TRUE)\n\nmy_very_time_consuming_regression is an object of class lm for example. I save the output of summary(my_very_time_consuming_regression) as text using capture.output and save it in a variable called out. Finally, I save out to a file called summary_of_my_very_time_consuming_regression.txt with the first sentence being My title (you can put anything there). The file summary_of_my_very_time_consuming_regression.txt doesn’t have to already exist in your working directory. The option sep=\"\" is important or else the whole output will be written in a single line. Finally, append=TRUE makes sure your file won’t be overwritten; additional output will be appended to the file, which can be nice if you want to compare different versions of your model."
  },
  {
    "objectID": "posts/2015-05-03-update-introduction-r-programming.html",
    "href": "posts/2015-05-03-update-introduction-r-programming.html",
    "title": "Update to Introduction to programming econometrics with R",
    "section": "",
    "text": "This semester I taught a course on applied econometrics with the R programming language. For this, I created a document that I gave to my students and shared online. This is the kind of document I would have liked to read when I first started using R. I already had some programming experience in C and Pascal but this is not necessarily the case for everyone that is confronted to R when they start learning about econometrics.\nThis is why the beginning of the document focuses more on general programming knowledge and techniques, and then only on econometrics. People online seemed to like the document, as I’ve received some positive comments by David Smith from Revolution R (read his blog post about the document here) and Dave Giles which links to David’s blog post here. People on twitter have also retweeted David’s and Dave’s tweets to their blog posts and I’ve received some requests by people to send them the PDF by email (because they live in places where Dropbox is not accessible unfortunately).\nThe document is still a work in progress (and will probably remain so for a long time), but I’ve added some new sections about reproducible research and thought that this update could warrant a new blog post.\nFor now, only linear models are reviewed, but I think I’ll start adding some chapters about non-linear models soonish. The goal for these notes, however, is not to re-invent the wheel: there are lots of good books about econometrics with R out there and packages that estimate a very wide range of models. What I want for these notes, is to focus more on the programming knowledge an econometrician needs, in a very broad and general sense. I want my students to understand that R is a true programming language, and that they need to use every feature offered by such a language, and not think of R as a black box into which you only type pre-programmed commands, but also be able to program their own routines.\nAlso, I’ve made it possible to create the PDF using a Makefile. This may be useful for people that do not have access to Dropbox, but are familiar with git.\nYou can compile the book in two ways: first download the whole repository:\ngit clone git@bitbucket.org:b-rodrigues/programmingeconometrics.git\nand then, with Rstudio, open the file appliedEconometrics.Rnw and knit it. Another solution is to use the Makefile. Just type:\nmake\ninside a terminal (should work on GNU+Linux and OSX systems) and it should compile the document. You may get some message about “additional information” for some R packages. When these come up, just press Q on your keyboard to continue the compilation process.\nGet the notes here.\nAs always, if you have questions or suggestions, do not hesitate to send me an email and I sure hope you’ll find these notes useful!"
  },
  {
    "objectID": "posts/2016-12-21-work-on-lists-of-datasets-instead-of-individual-datasets-by-using-functional-programming.html",
    "href": "posts/2016-12-21-work-on-lists-of-datasets-instead-of-individual-datasets-by-using-functional-programming.html",
    "title": "Work on lists of datasets instead of individual datasets by using functional programming",
    "section": "",
    "text": "Analyzing a lot of datasets can be tedious. In my work, I often have to compute descriptive statistics, or plot some graphs for some variables for a lot of datasets. The variables in question have the same name accross the datasets but are measured for different years. As an example, imagine you have this situation:\n\n\ndata2000 &lt;- mtcars\ndata2001 &lt;- mtcars\n\n\nFor the sake of argument, imagine that data2000 is data from a survey conducted in the year 2000 and data2001 is the same survey but conducted in the year 2001. For illustration purposes, I use the mtcars dataset, but I could have used any other example. In these sort of situations, the variables are named the same in both datasets. Now if I want to check the summary statistics of a variable, I might do it by running:\n\n\nsummary(data2000$cyl)\nsummary(data2001$cyl)\n\n\nbut this can get quite tedious, especially if instead of only having two years of data, you have 20 years. Another possibility is to merge both datasets and then check the summary statistics of the variable of interest. But this might require a lot of preprocessing, and sometimes you really just want to do a quick check, or some dirty graphs. So you might be tempted to write a loop, which would require to put these two datasets in some kind of structure, such as a list:\n\n\nlist_data &lt;- list(\"data2000\" = data2000, \"data2001\" = data2001)\n\nfor (i in 1:2){\n    print(summary(list_data[[i]]$cyl))\n}\n\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000 \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000\n\nBut this also might get tedious, especially if you want to do this for a lot of different variables, and want to use different functions than summary().\n\n\nAnother, simpler way of doing this, is to use purrr::map() or lapply(). But there is a catch though: how do we specify the column we want to work on? Let’s try some things out:\n\n\nlibrary(purrr)\n\nmap(list_data, summary(cyl))\n\nError in summary(cyl) : object 'cyl' not found\n\nMaybe this will work:\n\n\nmap(list_data, summary, cyl)\n\n## $data2000\n     mpg             cyl             disp             hp       \nMin.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \nMedian :19.20   Median :6.000   Median :196.3   Median :123.0  \nMean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \nMax.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n     drat             wt             qsec             vs        \nMin.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \nMedian :3.695   Median :3.325   Median :17.71   Median :0.0000  \nMean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \nMax.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n      am              gear            carb      \nMin.   :0.0000   Min.   :3.000   Min.   :1.000  \n1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \nMedian :0.0000   Median :4.000   Median :2.000  \nMean   :0.4062   Mean   :3.688   Mean   :2.812  \n3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \nMax.   :1.0000   Max.   :5.000   Max.   :8.000  \n\ndata2001\n     mpg             cyl             disp             hp       \nMin.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \nMedian :19.20   Median :6.000   Median :196.3   Median :123.0  \nMean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \nMax.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n     drat             wt             qsec             vs        \nMin.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \nMedian :3.695   Median :3.325   Median :17.71   Median :0.0000  \nMean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \nMax.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n      am              gear            carb      \nMin.   :0.0000   Min.   :3.000   Min.   :1.000  \n1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \nMedian :0.0000   Median :4.000   Median :2.000  \nMean   :0.4062   Mean   :3.688   Mean   :2.812  \n3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \nMax.   :1.0000   Max.   :5.000   Max.   :8.000\n\nNot quite! You get the summary statistics of every variable, cyl simply gets ignored. This might be ok in our small toy example, but if you have dozens of datasets with hundreds of variables, the output becomes unreadable. The solution is to use an anonymous functions:\n\n\nmap(list_data, (function(x) summary(x$cyl)))\n\n## $data2000\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000 \n\n$data2001\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000\n\nThis is, in my opinion, much more readable than a loop, and the output of this is another list, so it’s easy to save it:\n\n\nsummary_cyl &lt;- map(list_data, (function(x) summary(x$cyl)))\nstr(summary_cyl)\n\n## List of 2\n$ data2000:Classes 'summaryDefault', 'table'  Named num [1:6] 4 4 6 6.19 8 ...\n .. ..- attr(*, \"names\")= chr [1:6] \"Min.\" \"1st Qu.\" \"Median\" \"Mean\" ...\n$ data2001:Classes 'summaryDefault', 'table'  Named num [1:6] 4 4 6 6.19 8 ...\n .. ..- attr(*, \"names\")= chr [1:6] \"Min.\" \"1st Qu.\" \"Median\" \"Mean\" ...\n\nWith the loop, you would need to “allocate” an empty list that you would fill at each iteration.\n\n\nSo this is already nice, but wouldn’t it be nicer to simply have to type:\n\n\nsummary(list_data$cyl)\n\n\nand have the summary of variable cyl for each dataset in the list? Well it is possible with the following function I wrote to make my life easier:\n\n\nto_map &lt;- function(func){\n  function(list, column, ...){\n    if(missing(column)){\n        res &lt;- purrr::map(list, (function(x) func(x, ...)))\n      } else {\n        res &lt;- purrr::map(list, (function(x) func(x[column], ...)))\n             }\n    res\n  }\n}\n\n\nBy following this chapter of Hadley Wickham’s book, Advanced R, I was able to write this function. What does it do? It basically generalizes a function to work on a list of datasets instead of just on a dataset. So for example, in the case of summary():\n\n\nsummarymap &lt;- to_map(summary)\n\nsummarymap(list_data, \"cyl\")\n\n$data2000\n     cyl       \nMin.   :4.000  \n1st Qu.:4.000  \nMedian :6.000  \nMean   :6.188  \n3rd Qu.:8.000  \nMax.   :8.000  \n\n$data2001\n     cyl       \nMin.   :4.000  \n1st Qu.:4.000  \nMedian :6.000  \nMean   :6.188  \n3rd Qu.:8.000  \nMax.   :8.000\n\nSo now everytime I want to have summary statistics for a variable, I just need to use summarymap():\n\n\nsummarymap(list_data, \"mpg\")\n\n## $data2000\n      mpg       \n Min.   :10.40  \n 1st Qu.:15.43  \n Median :19.20  \n Mean   :20.09  \n 3rd Qu.:22.80  \n Max.   :33.90  \n\n$data2001\n      mpg       \n Min.   :10.40  \n 1st Qu.:15.43  \n Median :19.20  \n Mean   :20.09  \n 3rd Qu.:22.80  \n Max.   :33.90\n\nIf I want the summary statistics for every variable, I simply omit the column name:\n\n\nsummarymap(list_data)\n\n$data2000\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n$data2001\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000\n\nI can use any function:\n\n\ntablemap &lt;- to_map(table)\n\ntablemap(list_data, \"cyl\")\n\n## $data2000\n\n 4  6  8 \n11  7 14 \n\n$data2001\n\n 4  6  8 \n11  7 14\n\ntablemap(list_data, \"mpg\")\n\n## $data2000\n\n10.4 13.3 14.3 14.7   15 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7 19.2 19.7 \n   2    1    1    1    1    2    1    1    1    1    1    1    1    2    1 \n  21 21.4 21.5 22.8 24.4   26 27.3 30.4 32.4 33.9 \n   2    2    1    2    1    1    1    2    1    1 \n\n$data2001\n\n10.4 13.3 14.3 14.7   15 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7 19.2 19.7 \n   2    1    1    1    1    2    1    1    1    1    1    1    1    2    1 \n  21 21.4 21.5 22.8 24.4   26 27.3 30.4 32.4 33.9 \n   2    2    1    2    1    1    1    2    1    1\n\nI hope you will find this little function useful, and as usual, for any comments just drop me an email by clicking the red enveloppe in the top right corner or tweet me."
  },
  {
    "objectID": "posts/2016-07-30-merge-a-list-of-datasets-together.html",
    "href": "posts/2016-07-30-merge-a-list-of-datasets-together.html",
    "title": "Merge a list of datasets together",
    "section": "",
    "text": "Last week I showed how to read a lot of datasets at once with R, and this week I’ll continue from there and show a very simple function that uses this list of read datasets and merges them all together.\n\n\nFirst we’ll use read_list() to read all the datasets at once (for more details read last week’s post):\n\nlibrary(\"readr\")\nlibrary(\"tibble\")\n\ndata_files &lt;- list.files(pattern = \".csv\")\n\nprint(data_files)\n## [1] \"data_1.csv\" \"data_2.csv\" \"data_3.csv\"\nlist_of_data_sets &lt;- read_list(data_files, read_csv)\n\nglimpse(list_of_data_sets)\n## List of 3\n##  $ data_1:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,018930679\" \"0,8748013128\" \"0,1025635934\" \"0,6246140983\" ...\n##   ..$ col2: chr [1:19] \"0,0377725807\" \"0,5959457638\" \"0,4429121533\" \"0,558387159\" ...\n##   ..$ col3: chr [1:19] \"0,6241767189\" \"0,031324594\" \"0,2238059868\" \"0,2773350732\" ...\n##  $ data_2:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9098418493\" \"0,1127788509\" \"0,5818891392\" \"0,1011773532\" ...\n##   ..$ col2: chr [1:19] \"0,7455905887\" \"0,4015039612\" \"0,6625796605\" \"0,029955339\" ...\n##   ..$ col3: chr [1:19] \"0,327232932\" \"0,2784035673\" \"0,8092386735\" \"0,1216045306\" ...\n##  $ data_3:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9236124896\" \"0,6303271761\" \"0,6413583054\" \"0,5573887416\" ...\n##   ..$ col2: chr [1:19] \"0,2114708388\" \"0,6984538266\" \"0,0469865249\" \"0,9271510226\" ...\n##   ..$ col3: chr [1:19] \"0,4941919971\" \"0,7391538511\" \"0,3876723797\" \"0,2815014394\" ...\n\nYou see that all these datasets have the same column names. We can now merge them using this simple function:\n\nmulti_join &lt;- function(list_of_loaded_data, join_func, ...){\n\n    require(\"dplyr\")\n\n    output &lt;- Reduce(function(x, y) {join_func(x, y, ...)}, list_of_loaded_data)\n\n    return(output)\n}\n\nThis function uses Reduce(). Reduce() is a very important function that can be found in all functional programming languages. What does Reduce() do? Let’s take a look at the following example:\n\nReduce(`+`, c(1, 2, 3, 4, 5))\n## [1] 15\n\nReduce() has several arguments, but you need to specify at least two: a function, here + and a list, here c(1, 2, 3, 4, 5). The next code block shows what Reduce() basically does:\n\n0 + c(1, 2, 3, 4, 5)\n0 + 1 + c(2, 3, 4, 5)\n0 + 1 + 2 + c(3, 4, 5)\n0 + 1 + 2 + 3 + c(4, 5)\n0 + 1 + 2 + 3 + 4 + c(5)\n0 + 1 + 2 + 3 + 4 + 5\n\n0 had to be added as in “init”. You can also specify this “init” to Reduce():\n\nReduce(`+`, c(1, 2, 3, 4, 5), init = 20)\n## [1] 35\n\nSo what multi_join() does, is the same operation as in the example above, but where the function is a user supplied join or merge function, and the list of datasets is the one read with read_list().\n\n\nLet’s see what happens when we use multi_join() on our list:\n\nmerged_data &lt;- multi_join(list_of_data_sets, full_join)\nclass(merged_data)\n## [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\nglimpse(merged_data)\n## Observations: 57\n## Variables: 3\n## $ col1 &lt;chr&gt; \"0,018930679\", \"0,8748013128\", \"0,1025635934\", \"0,6246140...\n## $ col2 &lt;chr&gt; \"0,0377725807\", \"0,5959457638\", \"0,4429121533\", \"0,558387...\n## $ col3 &lt;chr&gt; \"0,6241767189\", \"0,031324594\", \"0,2238059868\", \"0,2773350...\n\nYou should make sure that all the data frames have the same column names but you can also join data frames with different column names if you give the argument by to the join function. This is possible thanks to … that allows you to pass further argument to join_func().\n\n\nThis function was inspired by the one found on the blog Coffee and Econometrics in the Morning."
  },
  {
    "objectID": "posts/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions.html",
    "href": "posts/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions.html",
    "title": "Data frame columns as arguments to dplyr functions",
    "section": "",
    "text": "Suppose that you would like to create a function which does a series of computations on a data frame. You would like to pass a column as this function’s argument. Something like:\n\ndata(cars)\nconvertToKmh &lt;- function(dataset, col_name){\n  dataset$col_name &lt;- dataset$speed * 1.609344\n  return(dataset)\n}\n\nThis example is obviously not very interesting (you don’t need a function for this), but it will illustrate the point. You would like to append a column called speed_in_kmh with the speed in kilometers per hour to this dataset, but this is what happens:\n\nhead(convertToKmh(cars, \"speed_in_kmh\"))\n##   speed dist  col_name\n1     4    2  6.437376\n2     4   10  6.437376\n3     7    4 11.265408\n4     7   22 11.265408\n5     8   16 12.874752\n6     9   10 14.484096\n\nYour column is not called speed_in_kmh but col_name! It turns out that there is a very simple solution:\n\nconvertToKmh &lt;- function(dataset, col\\_name){\n  dataset[col_name] &lt;- dataset$speed * 1.609344\n  return(dataset)\n}\n\nhead(convertToKmh(cars, \"speed\\_in\\_kmh\"))\n##   speed dist speed\\_in\\_kmh\n1     4    2     6.437376\n2     4   10     6.437376\n3     7    4    11.265408\n4     7   22    11.265408\n5     8   16    12.874752\n6     9   10    14.484096\n\nYou can access columns with [] instead of $.\n\n\nBut sometimes you want to do more complex things and for example have a function that groups by a variable and then computes new variables, filters by another and so on. You would like to avoid having to hard code these variables in your function, because then why write a function and of course you would like to use dplyr to do it.\n\n\nI often use dplyr functions in my functions. For illustration purposes, consider this very simple function:\n\nsimpleFunction &lt;- function(dataset, col_name){\n  require(\"dplyr\")\n  dataset %&gt;% \n    group\\_by(col\\_name) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\nsimpleFunction(cars, \"dist\")\n\nThis function takes a dataset as an argument, as well as a column name. However, this does not work. You get this error:\n\nError: unknown variable to group by : col_name \n\nThe variable col_name is passed to simpleFunction() as a string, but group_by() requires a variable name. So why not try to convert col_name to a name?\n\nsimpleFunction &lt;- function(dataset, col_name){\n  require(\"dplyr\")\n  col\\_name &lt;- as.name(col_name)\n  dataset %&gt;% \n    group\\_by(col\\_name) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\nsimpleFunction(cars, \"dist\")\n\nYou get the same error as before:\n\nError: unknown variable to group by : col_name \n\nSo how can you pass a column name to group_by()? Well, there is another version of group_by() called group_by_() that uses standard evaluation. You can learn more about it here. Let’s take a look at what happens when we use group_by_():\n\nsimpleFunction &lt;- function(dataset, col_name){\n  require(\"dplyr\")\n  dataset %&gt;% \n    group\\_by\\_(col_name) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\nsimpleFunction(cars, \"dist\")\nA tibble: 35 x 2\n dist mean\\_speed\n&lt;dbl&gt;      &lt;dbl&gt;\n1      2        4.0\n2      4        7.0\n3     10        6.5\n4     14       12.0\n5     16        8.0\n6     17       11.0\n7     18       10.0\n8     20       13.5\n9     22        7.0\n10    24       12.0\n ... with 25 more rows\n\nWe can even use a formula instead of a string:\n\nsimpleFunction(cars, ~dist)\n A tibble: 35 x 2\n    dist mean_speed\n   &lt;dbl&gt;      &lt;dbl&gt;\n1      2        4.0\n2      4        7.0\n3     10        6.5\n4     14       12.0\n5     16        8.0\n6     17       11.0\n7     18       10.0\n8     20       13.5\n9     22        7.0\n10    24       12.0\n... with 25 more rows\n\nWhat if you want to pass column names and constants, for example to filter without hardcoding anything?\n\n\nTrying to do it naively will only yield pain and despair:\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  require(\"dplyr\")\n  dataset %&gt;% \n    filter\\_(col\\_name == value) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n&gt; simpleFunction(cars, \"dist\", 10)\n\n  mean_speed\n1        NaN\n\n&gt; simpleFunction(cars, dist, 10)\n\n Error in col_name == value : \n  comparison (1) is possible only for atomic and list types \n  \n&gt; simpleFunction(cars, ~dist, 10)\n\n  mean_speed\n1        NaN\n\n\nTo solve this issue, we need to know a little bit about two concepts, lazy evaluation and non-standard evaluation. I recommend you read the following document from Hadley Wickham’s book Advanced R as well as the part on lazy evaluation here.\n\n\nA nice package called lazyeval can help us out. We would like to make R understand that the column name is not col_name but the string inside it \"dist\", and now we would like to use filter() for dist equal to 10.\n\n\nIn the lazyeval package, you’ll find the function interp(). interp() allows you to\n\n\n\nbuild an expression up from a mixture of constants and variables.\n\n\n\nTake a look at this example:\n\nlibrary(lazyeval)\ninterp(~x+y, x = 2)\n## ~2 + y\n\nWhat you get back is this nice formula that you can then use within functions. To see why this is useful, let’s look at the above example again, and make it work using interp():\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  require(\"dplyr\")\n  require(\"lazyeval\")\n  filter\\_criteria &lt;- interp(~y == x, .values=list(y = as.name(col_name), x = value))\n  dataset %&gt;% \n    filter\\_(filter_criteria) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(cars, \"dist\", 10)\n  mean\\_speed\n1        6.5\n\nAnd now it works! For some reason, you have to pass the column name as a string though.\n\n\nSources: apart from the documents above, the following stackoverflow threads helped me out quite a lot: In R: pass column name as argument and use it in function with dplyr::mutate() and lazyeval::interp() and Non-standard evaluation (NSE) in dplyr’s filter_ & pulling data from MySQL."
  },
  {
    "objectID": "posts/2016-03-31-unit-testing-with-r.html",
    "href": "posts/2016-03-31-unit-testing-with-r.html",
    "title": "Unit testing with R",
    "section": "",
    "text": "I've been introduced to unit testing while working with colleagues on quite a big project for which we use Python.\n\n\nAt first I was a bit skeptical about the need of writing unit tests, but now I must admit that I am seduced by the idea and by the huge time savings it allows. Naturally, I was wondering if the same could be achieved with R, and was quite happy to find out that it also possible to write unit tests in R using a package called testthat.\n\n\nUnit tests (Not to be confused with unit root tests for time series) are small functions that test your code and help you make sure everything is alright. I'm going to show how the testthat packages works with a very trivial example, that might not do justice to the idea of unit testing. But you'll hopefully see why writing unit tests is not a waste of your time, especially if your project gets very complex (if you're writing a package for example).\n\n\nFirst, you'll need to download and install testthat. Some dependencies will also be installed.\n\n\nNow, you'll need a function to test. Let's suppose you've written a function that returns the nth Fibonacci number:\n\nFibonacci &lt;- function(n){\n    a &lt;- 0\n    b &lt;- 1\n    for (i in 1:n){\n        temp &lt;- b\n        b &lt;- a\n        a &lt;- a + temp\n    }\n    return(a)\n}\n\n\nYou then save this function in a file, let's call it fibo.R. What you'll probably do once you've written this function, is to try it out:\n\nFibonacci(5)\n\n## [1] 5\n\n\nYou'll see that the function returns the right result and continue programming. The idea behind unit testing is write a bunch of functions that you can run after you make changes to your code, just to check that everything is still running as it should.\n\n\nLet's create a script called test_fibo.R and write the following code in it:\n\ntest_that(\"Test Fibo(15)\",{\n  phi &lt;- (1 + sqrt(5))/2\n  psi &lt;- (1 - sqrt(5))/2\n  expect_equal(Fibonacci(15), (phi**15 - psi**15)/sqrt(5))\n})\n\n\nThe code above uses Binet's formula, a closed form formula that gives the nth Fibonacci number and compares it our implementation of the algorithm. If you didn't know about Binet's formula, you could simply compute some numbers by hand and compare them to what your function returns, for example. The function expect_equal is a function from the package testthat and does exactly what it tells. We expect the result of our implementation to be equal to the result of Binet's Formula. The file test_fibo.R can contain as many tests as you need. Also, the file that contains the tests must start with the string test, so that testthat knows with files it has to run.\n\n\nNow, we're almost done, create yet another script, let's call it run_tests.R and write the following code in it:\n\nlibrary(testthat) \n\nsource(\"path/to/fibo.R\")\n\ntest_results &lt;- test_dir(\"path/to/tests\", reporter=\"summary\")\n\n\nAfter running these lines, and if everything goes well, you should see a message like this:\n\n&gt; library(testthat)\n&gt; source(\"path/to/fibo.R\")\n&gt; test_results &lt;- test_dir(\"path/to/tests\", reporter=\"summary\")\n\n.\nYour tests are dandy! \n\n\nNotice the small . over the message? This means that one test was run successfully. You'll get one dot per successful test. If you take a look at test_results you'll see this:\n\n&gt; test_results\n         file context          test nb failed skipped error  user system  real\n1 test_fibo.R         Test Fibo(15)  1      0   FALSE FALSE 0.004      0 0.006\n\n\nYou'll see each file and each function inside the files that were tested, and also whether the test was skipped, failed etc. This may seem overkill for such a simple function, but imagine that you write dozens of functions that get more and more complex over time. You might have to change a lot of lines because as time goes by you add new functionality, but don't want to break what was working. Running your unit tests each time you make changes can help you pinpoint regressions in your code. Unit tests can also help you start with your code. It can happen that sometimes you don't know exactly how to start; well you could start by writing a unit test that returns the result you want to have and then try to write the code to make that unit test pass. This is called test-driven development.\n\n\nI hope that this post motivated you to write unit tests and make you a better R programmer!"
  },
  {
    "objectID": "posts/2015-11-11-bootstrapping-did-with-r.html",
    "href": "posts/2015-11-11-bootstrapping-did-with-r.html",
    "title": "Bootstrapping standard errors for difference-in-differences estimation with R",
    "section": "",
    "text": "I’m currently working on a paper (with my colleague Vincent Vergnat who is also a Phd candidate at BETA) where I want to estimate the causal impact of the birth of a child on hourly and daily wages as well as yearly worked hours. For this we are using non-parametric difference-in-differences (henceforth DiD) and thus have to bootstrap the standard errors. In this post, I show how this is possible using the function boot.\n\n\nFor this we are going to replicate the example from Wooldridge’s Econometric Analysis of Cross Section and Panel Data and more specifically the example on page 415. You can download the data for R here. The question we are going to try to answer is how much does the price of housing decrease due to the presence of an incinerator in the neighborhood?\n\n\nFirst put the data in a folder and set the correct working directory and load the boot library.\n\nlibrary(boot)\nsetwd(\"/home/path/to/data/kiel data/\")\nload(\"kielmc.RData\")\n\nNow you need to write a function that takes the data as an argument, as well as an indices argument. This argument is used by the boot function to select samples. This function should return the statistic you’re interested in, in our case, the DiD estimate.\n\nrun_DiD &lt;- function(my_data, indices){\n    d &lt;- my_data[indices,]\n    return(\n        mean(d$rprice[d$year==1981 & d$nearinc==1]) - \n        mean(d$rprice[d$year==1981 & d$nearinc==0]) - \n        (mean(d$rprice[d$year==1978 & d$nearinc==1]) - \n        mean(d$rprice[d$year==1978 & d$nearinc==0]))\n    )\n}\n\nYou’re almost done! To bootstrap your DiD estimate you just need to use the boot function. If you have cpu with multiple cores (which you should, single core machines are quite outdated by now) you can even parallelize the bootstrapping.\n\nboot_est &lt;- boot(data, run_DiD, R=1000, parallel=\"multicore\", ncpus = 2)\n\nNow you should just take a look at your estimates:\n\nboot_est\n \nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = data, statistic = run_DiD, R = 1000, parallel = \"multicore\", \n ncpus = 2)\n\n\nBootstrap Statistics :\n original    bias    std. error\nt1* -11863.9 -553.3393    8580.435\n\nThese results are very similar to the ones in the book, only the standard error is higher.\n\n\nYou can get confidence intervals like this:\n\nquantile(boot_est$t, c(0.025, 0.975))\n##       2.5%      97.5% \n## -30186.397   3456.133\n\nor a t-statistic:\n\nboot_est$t0/sd(boot_est$t)\n## [1] -1.382669\n\nOr the density of the replications:\n\nplot(density(boot_est$t))\n\n&lt;img src=\"/img/density_did.png\" width=\"670\" height=\"450\" /&gt;&lt;/a&gt;\n\n\nJust as in the book, we find that the DiD estimate is not significant to the 5% level."
  },
  {
    "objectID": "posts/2016-06-21-careful-with-trycatch.html",
    "href": "posts/2016-06-21-careful-with-trycatch.html",
    "title": "Careful with tryCatch",
    "section": "",
    "text": "tryCatch is one of the functions that allows the users to handle errors in a simple way. With it, you can do things like: if(error), then(do this).\n\n\nTake the following example:\n\nsqrt(\"a\")\nError in sqrt(\"a\") : non-numeric argument to mathematical function\n\nNow maybe you’d want something to happen when such an error happens. You can achieve that with tryCatch:\n\ntryCatch(sqrt(\"a\"), error=function(e) print(\"You can't take the square root of a character, silly!\"))\n## [1] \"You can't take the square root of a character, silly!\"\n\nWhy am I interested in tryCatch?\n\n\nI am currently working with dates, specifically birthdays of people in my data sets. For a given mother, the birthday of her child is given in three distinct columns: a column for the child’s birth year, birth month and birth day respectively. I’ve wanted to put everything in a single column and convert the birthday to unix time (I have a very good reason to do that, but I won’t bore you with the details).\n\n\nLet’s create some data:\n\nmother &lt;- as.data.frame(list(month=12, day=1, year=1988))\n\nIn my data, there’s a lot more columns of course, such as the mother’s wage, education level, etc, but for illustration purposes, this is all that’s needed.\n\n\nNow, to create this birthday column:\n\nmother$birth1 &lt;- as.POSIXct(paste0(as.character(mother$year), \n                                   \"-\", as.character(mother$month), \n                                   \"-\", as.character(mother$day)), \n                            origin=\"1970-01-01\")\n\nand to convert it to unix time:\n\nmother$birth1 &lt;- as.numeric(as.POSIXct(paste0(as.character(mother$year), \n                                              \"-\", as.character(mother$month), \n                                              \"-\", as.character(mother$day)),\n                                       origin=\"1970-01-01\"))\n\nprint(mother)\n##   month day year    birth1\n## 1    12   1 1988 596934000\n\nNow let’s see what happens in this other example here:\n\nmother2 &lt;- as.data.frame(list(month=2, day=30, year=1988))\n\nmother2$birth1 &lt;- as.POSIXct(paste0(as.character(mother2$year), \n                                    \"-\", as.character(mother2$month), \n                                    \"-\", as.character(mother2$day)), \n                             origin=\"1970-01-01\")\n\nThis is what happens:\n\nError in as.POSIXlt.character(x, tz, ...) : \n  character string is not in a standard unambiguous format\n\nThis error is to be expected; there is no 30th of February! It turns out that in some rare cases, weird dates like this exist in my data. Probably some encoding errors. Not a problem I thought, I could use tryCatch and return NA in the case of an error.\n\nmother2 &lt;- as.data.frame(list(month=2, day=30, year=1988))\n\nmother2$birth1 &lt;- tryCatch(as.POSIXct(paste0(as.character(mother2$year), \n                                    \"-\", as.character(mother2$month), \n                                    \"-\", as.character(mother2$day)), \n                             origin=\"1970-01-01\"), error=function(e) NA)\n\nprint(mother2)\n##   month day year birth1\n## 1     2  30 1988     NA\n\nPretty great, right? Well, no. Take a look at what happens in this case:\n\nmother &lt;- as.data.frame(list(month=c(12, 2), day=c(1, 30), year=c(1988, 1987)))\nprint(mother)\n##   month day year\n## 1    12   1 1988\n## 2     2  30 1987\n\nWe’d expect to have a correct date for the first mother and an NA for the second. However, this is what happens\n\nmother$birth1 &lt;- tryCatch(as.POSIXct(paste0(as.character(mother$year), \n                                    \"-\", as.character(mother$month), \n                                    \"-\", as.character(mother$day)), \n                             origin=\"1970-01-01\"), error=function(e) NA)\n\nprint(mother)\n##   month day year birth1\n## 1    12   1 1988     NA\n## 2     2  30 1987     NA\n\nAs you can see, we now have an NA for both mothers! That’s actually to be expected. Indeed, this little example illustrates it well:\n\nsqrt(c(4, 9, \"haha\"))\nError in sqrt(c(4, 9, \"haha\")) : \n  non-numeric argument to mathematical function\n\nBut you’d like to have this:\n\n[1]  2  3 NA\n\nSo you could make the same mistake as myself and use tryCatch:\n\ntryCatch(sqrt(c(4, 9, \"haha\")), error=function(e) NA)\n## [1] NA\n\nBut you only get NA in return. That’s actually completely normal, but it took me off-guard and I spent quite some time to figure out what was happening. Especially because I had written unit tests to test my function create_birthdays() that was doing the above computations and all tests were passing! The problem was that in my tests, I only had a single individual, so for a wrong date, having NA for this individual was expected behaviour. But in a panel, only some individuals have a weird date like the 30th of February, but because of those, the whole column was filled with NA’s! What I’m doing now is trying to either remove these weird birthdays (there are mothers whose children were born on the 99-99-9999. Documentation is lacking, but this probably means missing value), or tyring to figure out how to only get NA’s for the “weird” dates. I guess that the answer lies with dplyr’s group_by() and mutate() to compute this birthdays for each individual separately."
  },
  {
    "objectID": "posts/2016-07-26-read-a-lot-of-datasets-at-once-with-r.html",
    "href": "posts/2016-07-26-read-a-lot-of-datasets-at-once-with-r.html",
    "title": "Read a lot of datasets at once with R",
    "section": "",
    "text": "I often have to read a lot of datasets at once using R. So I’ve wrote the following function to solve this issue:\n\nread_list &lt;- function(list_of_datasets, read_func){\n\n        read_and_assign &lt;- function(dataset, read_func){\n                dataset_name &lt;- as.name(dataset)\n                dataset_name &lt;- read_func(dataset)\n        }\n\n        # invisible is used to suppress the unneeded output\n        output &lt;- invisible(\n                sapply(list_of_datasets,\n                           read_and_assign, read_func = read_func, simplify = FALSE, USE.NAMES = TRUE))\n\n        # Remove the extension at the end of the data set names\n        names_of_datasets &lt;- c(unlist(strsplit(list_of_datasets, \"[.]\"))[c(T, F)])\n        names(output) &lt;- names_of_datasets\n        return(output)\n}\n\nYou need to supply a list of datasets as well as the function to read the datasets to read_list. So for example to read in .csv files, you could use read.csv() (or read_csv() from the readr package, which I prefer to use), or read_dta() from the package haven for STATA files, and so on.\n\n\nNow imagine you have some data in your working directory. First start by saving the name of the datasets in a variable:\n\ndata_files &lt;- list.files(pattern = \".csv\")\n\nprint(data_files)\n## [1] \"data_1.csv\" \"data_2.csv\" \"data_3.csv\"\n\nNow you can read all the data sets and save them in a list with read_list():\n\nlibrary(\"readr\")\nlibrary(\"tibble\")\n\nlist_of_data_sets &lt;- read_list(data_files, read_csv)\n\n\nglimpse(list_of_data_sets)\n## List of 3\n##  $ data_1:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,018930679\" \"0,8748013128\" \"0,1025635934\" \"0,6246140983\" ...\n##   ..$ col2: chr [1:19] \"0,0377725807\" \"0,5959457638\" \"0,4429121533\" \"0,558387159\" ...\n##   ..$ col3: chr [1:19] \"0,6241767189\" \"0,031324594\" \"0,2238059868\" \"0,2773350732\" ...\n##  $ data_2:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9098418493\" \"0,1127788509\" \"0,5818891392\" \"0,1011773532\" ...\n##   ..$ col2: chr [1:19] \"0,7455905887\" \"0,4015039612\" \"0,6625796605\" \"0,029955339\" ...\n##   ..$ col3: chr [1:19] \"0,327232932\" \"0,2784035673\" \"0,8092386735\" \"0,1216045306\" ...\n##  $ data_3:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9236124896\" \"0,6303271761\" \"0,6413583054\" \"0,5573887416\" ...\n##   ..$ col2: chr [1:19] \"0,2114708388\" \"0,6984538266\" \"0,0469865249\" \"0,9271510226\" ...\n##   ..$ col3: chr [1:19] \"0,4941919971\" \"0,7391538511\" \"0,3876723797\" \"0,2815014394\" ...\n\nIf you prefer not to have the datasets in a list, but rather import them into the global environment, you can change the above function like so:\n\nread_list &lt;- function(list_of_datasets, read_func){\n\n        read_and_assign &lt;- function(dataset, read_func){\n                assign(dataset, read_func(dataset), envir = .GlobalEnv)\n        }\n\n        # invisible is used to suppress the unneeded output\n        output &lt;- invisible(\n                sapply(list_of_datasets,\n                           read_and_assign, read_func = read_func, simplify = FALSE, USE.NAMES = TRUE))\n\n}\n\nBut I personnally don’t like this second option, but I put it here for completeness."
  },
  {
    "objectID": "posts/2016-11-04-ive-started-writing-a-book-functional-programming-and-unit-testing-for-data-munging-with-r.html",
    "href": "posts/2016-11-04-ive-started-writing-a-book-functional-programming-and-unit-testing-for-data-munging-with-r.html",
    "title": "I’ve started writing a ‘book’: Functional programming and unit testing for data munging with R",
    "section": "",
    "text": "I have started writing a ‘book’ using the awesome bookdown package. In the book I explain and show why using functional programming and putting your functions in your own packages is the way to go when you want to clean, prepare and transform large data sets. It makes testing and documenting your code easier. You don’t need to think about managing paths either. The book is far from complete, but I plan on working on it steadily. For now, you can read an intro to functional programming, unit testing and creating your own packages that will hold your code. I also show you can write documentation for your functions. I am also looking for feedback; so if you have any suggestions, do not hesitate to shoot me an email or a tweet! You can read the book by clicking here."
  },
  {
    "objectID": "posts/2016-12-24-functional-programming-and-unit-testing-for-data-munging-with-r-available-on-leanpub.html",
    "href": "posts/2016-12-24-functional-programming-and-unit-testing-for-data-munging-with-r-available-on-leanpub.html",
    "title": "Functional programming and unit testing for data munging with R available on Leanpub",
    "section": "",
    "text": "The book I’ve been working on these pasts months (you can read about it here, and read it for free here) is now available on Leanpub! You can grab a copy and read it on your ebook reader or on your computer, and what’s even better is that it is available for free (but you can also decide to buy it if you really like it). Here is the link on Leanpub.\nIn the book, I show you the basics of functional programming, unit testing and package development for the R programming language. The end goal is to make your data tidy in a reproducible way!\nJust a heads up: as the book is right now, the formatting is not perfect and images are missing. This is because I use bookdown to write the book and convert it to Leanpub’s markdown flavour is not trivial. I will find a solution to automate the conversion from bookdown’s version to Leanpub’s markdown and try to keep both versions in sync. Of course, once the book will be finished, the version on Leanpub and on my website are going to be completely identical. If you want to read it on your computer offline, you can also download a pdf from the book’s website, by clicking on the pdf icon in the top left corner. Do not hesitate to send me an email if you want to give me feedback (just click on the red envelope in the top right corner) or tweet me @brodriguesco."
  },
  {
    "objectID": "posts/2017-01-07-my-free-book-has-a-cover.html",
    "href": "posts/2017-01-07-my-free-book-has-a-cover.html",
    "title": "My free book has a cover!",
    "section": "",
    "text": "I’m currently writing a book as a hobby. It’s titled Functional programming and unit testing for data munging with R and you can get it for free here. You can also read it online for free on my webpage What’s the book about?\nHere’s the teaser text:\n\nLearn the basics of functional programming, unit testing and package development for the R programming language in order to make your data tidy!\n\nThe book now has a beautiful cover thanks to @putosaure. Putosaure is a Paris based graphic designer who also reviews video games. He is also a very good friend of mine and I am very happy he made this beautiful cover for my book:\n\n\n\nIn it, we see a guy holding a shield with the Greek letter lambda, which also happens to be the letter to designate functional programming. I’ve added the title with the Komika Title font.\nConsider this cover in beta, it’ll probably evolve some more. But I couldn’t wait to use it!\nI love it. Hope you’ll love it too!"
  },
  {
    "objectID": "assets/img/2017-01-07-my-free-book-has-a-cover.html",
    "href": "assets/img/2017-01-07-my-free-book-has-a-cover.html",
    "title": "My free book has a cover!",
    "section": "",
    "text": "I’m currently writing a book as a hobby. It’s titled Functional programming and unit testing for data munging with R and you can get it for free here. You can also read it online for free on my webpage What’s the book about?\nHere’s the teaser text:\n\nLearn the basics of functional programming, unit testing and package development for the R programming language in order to make your data tidy!\n\nThe book now has a beautiful cover thanks to @putosaure. Putosaure is a Paris based graphic designer who also reviews video games. He is also a very good friend of mine and I am very happy he made this beautiful cover for my book:\n\nIn it, we see a guy holding a shield with the Greek letter lambda, which also happens to be the letter to designate functional programming. I’ve added the title with the Komika Title font.\nConsider this cover in beta, it’ll probably evolve some more. But I couldn’t wait to use it!\nI love it. Hope you’ll love it too!"
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html",
    "title": "How to use jailbreakr",
    "section": "",
    "text": "The jailbreakr package is probably one of the most interesting packages I came across recently. This package makes it possible to extract messy data from spreadsheets. What is meant by messy? I am sure you already had to deal with spreadsheets that contained little tables inside a single sheet for example. As far as I know, there is no simple way of extracting these tables without having to fiddle around a lot. This is now over with jailbreakr. Well not entirely, because jailbreakr is still in development, but it works well already. If you want to know more about the planned features, you can watch the following video by Jenny Bryan, one of the package’s authors."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html#what-is-jailbreakr",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html#what-is-jailbreakr",
    "title": "How to use jailbreakr",
    "section": "",
    "text": "The jailbreakr package is probably one of the most interesting packages I came across recently. This package makes it possible to extract messy data from spreadsheets. What is meant by messy? I am sure you already had to deal with spreadsheets that contained little tables inside a single sheet for example. As far as I know, there is no simple way of extracting these tables without having to fiddle around a lot. This is now over with jailbreakr. Well not entirely, because jailbreakr is still in development, but it works well already. If you want to know more about the planned features, you can watch the following video by Jenny Bryan, one of the package’s authors."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html#installation-and-data",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html#installation-and-data",
    "title": "How to use jailbreakr",
    "section": "\nInstallation and data\n",
    "text": "Installation and data\n\n\nYou will have to install the package from Github, as it is not on CRAN yet. Here is the Github link. To install the package, just run the following commands in an R console:\n\ndevtools::install_github(c(\"hadley/xml2\",\n                           \"rsheets/linen\",\n                           \"rsheets/cellranger\",\n                           \"rsheets/rexcel\",\n                           \"rsheets/jailbreakr\"))\n\nIf you get the following error:\n\ndevtools::install_github(\"hadley/xml2\")\nDownloading GitHub repo hadley/xml2@master\nfrom URL https://api.github.com/repos/hadley/xml2/zipball/master\nError in system(full, intern = quiet, ignore.stderr = quiet, ...) :\n    error in running command\n\nand if you’re on a GNU+Linux distribution try to run the following command:\n\noptions(unzip = \"internal\")\n\nand then run github_install() again.\n\n\nAs you can see, you need some other packages to make it work. Now we are going to get some data. We are going to download some time series from the European Commission, data I had to deal with recently. Download the data by clicking here and look for the spreadsheet titled Investment_total_factors_nace2.xlsx. The data we are interested in is on the second sheet, named TOT. You cannot import this sheet easily into R because there are four tables on the same sheet. Let us use jailbreakr to get these tables out of the sheet and into nice, tidy, data frames."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html#jailbreakr-to-the-rescue",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html#jailbreakr-to-the-rescue",
    "title": "How to use jailbreakr",
    "section": "\njailbreakr to the rescue\n",
    "text": "jailbreakr to the rescue\n\n\nThe first step is to read the data in. For this, we are going to use the rexcel package, which is also part of the rsheets organization on Github that was set up by Jenny Brian and Rich Fitzjohn, the authors of these packages. rexcel imports the sheet you want but not in a way that is immediately useful to you. It just gets the sheet into R, which makes it then possible to use jailbreakr’s magic on it. First, let’s import the packages we need:\n\nlibrary(\"rexcel\")\nlibrary(\"jailbreakr\")\n\nWe need to check which sheet to import. There are two sheets, and we want to import the one called TOT, the second one. But is it really the second one? I have noticed that sometimes, there are hidden sheets which makes importing the one you want impossible. So first, let use use another package, readxl and its function excel_sheets() to make sure we are extracting the sheet we really need:\n\nsheets &lt;- readxl::excel_sheets(path_to_data)\n\ntot_sheet &lt;- which(sheets == \"TOT\")\n\nprint(tot_sheet)\n## [1] 3\n\nAs you can see, the sheet we want is not the second, but the third! Let us import this sheet into R now (this might take more time than you think; on my computer it takes around 10 seconds):\n\nmy_sheet &lt;- rexcel_read(path_to_data, sheet = tot_sheet)\n\nNow we can start using jailbreakr. The function split_sheet() is the one that splits the sheet into little tables:\n\ntables &lt;- split_sheet(my_sheet)\nstr(tables)\n## List of 4\n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 34 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list \n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 33 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list \n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 32 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list \n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 33 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list\n\ntables is actually a list containing worksheet_view objects. Take a look at the dim attribute: you see the dimensions of the tables there. When I started using jailbreakr I was stuck here. I was looking for the function that would extract the data frames and could not find it. Then I watched the video and I understood what I had to do: a worksheet_view object has a values() method that does the extraction for you. This is a bit unusual in R (it made me feel like I was using Python); maybe in future versions this values() method will become a separate function of its own in the package. What happens when we use values()?\n\nlibrary(\"purrr\")\nlist_of_data &lt;-  map(tables, (function(x)(x$values())))\nmap(list_of_data, head)\n## [[1]]\n##      [,1]     [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]\n## [1,] \"TOT\"    NA      NA    NA    NA    NA    NA    NA    NA    NA   \n## [2,] \"DEMAND\" 33603   33969 34334 34699 35064 35430 35795 36160 36525\n## [3,] \"FDEMT\"  \"FDEMN\" NA    NA    NA    NA    NA    NA    NA    NA   \n## [4,] \"EU\"     \":\"     16.9  -1.4  20.2  34.5  31.4  37.5  39    37.3 \n## [5,] \"EA\"     \":\"     15.5  -13.1 14.8  30.9  25.1  35.2  39.2  37.1 \n## [6,] \"BE\"     \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   42.3  43.1 \n##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n## [1,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [2,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543\n## [3,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [4,] 39.2  27.5  20.6  21.4  29.8  26.4  32.5  47.1  19    -1.3  23.5 \n## [5,] 39.5  25.3  18.2  18.9  27.4  23    28.2  46.1  12.3  -9.3  19.3 \n## [6,] 45.8  42.2  42.9  43.8  45.8  47.4  49.1  50.9  48.2  46.9  46.3 \n##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] \n## [1,] NA    NA    NA    NA    NA    NA    NA    \n## [2,] 40908 41274 41639 42004 42369 42735 43100 \n## [3,] NA    NA    NA    NA    NA    NA    NA    \n## [4,] 29    22    21.1  25.6  31.8  22.9  \"30.7\"\n## [5,] 26.2  18.6  15.7  21.7  28.8  17.3  26.6  \n## [6,] 46.8  47.1  48.2  50.1  49.2  34.5  34.4  \n## \n## [[2]]\n##      [,1]        [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]\n## [1,] \"FINANCIAL\" 33603   33969 34334 34699 35064 35430 35795 36160 36525\n## [2,] \"FFINT\"     \"FFINN\" NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] \"EU\"        \":\"     -5.1  -6.2  2.7   6.7   9     14.4  13.9  14   \n## [4,] \"EA\"        \":\"     -8.8  -13.5 -3.4  2.6   5.7   12.5  13.2  13.1 \n## [5,] \"BE\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   21.5  22.4 \n## [6,] \"BG\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"  \n##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n## [1,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543\n## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] 16.4  9.4   7.4   8.1   12.4  8.4   13.6  23.4  4.1   -4    10.9 \n## [4,] 16.5  8     6.8   5.1   9.9   4.8   8.4   24.3  -2.8  -10.5 9.3  \n## [5,] 20.9  22.3  32.2  33.5  33.8  34.8  35    34.5  37.2  33.5  32.7 \n## [6,] \":\"   \":\"   20.8  24    27.1  28.3  33.4  37.5  37.7  26.6  30.4 \n##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] \n## [1,] 40908 41274 41639 42004 42369 42735 43100 \n## [2,] NA    NA    NA    NA    NA    NA    NA    \n## [3,] 12.4  10.2  8.8   13.4  17.4  6.2   \"12.3\"\n## [4,] 9     7.2   5     11    13.1  -1    6.5   \n## [5,] 31.5  32.3  33    31.7  32.2  19.9  20.5  \n## [6,] 33.8  35.6  36    41.5  41.6  44.2  43.8  \n## \n## [[3]]\n##      [,1]        [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]\n## [1,] \"TECHNICAL\" 33603   33969 34334 34699 35064 35430 35795 36160 36525\n## [2,] \"FTECT\"     \"FTECN\" NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] \"EU\"        \":\"     39.2  37.6  38.3  40    40.7  42.8  43.5  43.8 \n## [4,] \"EA\"        \":\"     39.7  36.2  37.5  41.2  40    44    44.8  44.9 \n## [5,] \"BE\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   58.8  58.5 \n## [6,] \"BG\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"  \n##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n## [1,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543\n## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] 37    31.1  27.2  30.9  30.4  30.3  27.4  40.5  25.8  23.1  27.4 \n## [4,] 37    30.3  27.4  31    29.9  29.7  24.8  41    23.4  19.5  26.4 \n## [5,] 58.3  58.4  57.7  59.2  59.6  59.4  60.2  59.5  60.5  57.9  56.3 \n## [6,] \":\"   \":\"   17.3  17.5  21.1  21.5  25.3  28.2  26.1  21    25.3 \n##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] \n## [1,] 40908 41274 41639 42004 42369 42735 43100 \n## [2,] NA    NA    NA    NA    NA    NA    NA    \n## [3,] 28.9  26.3  31.3  32.1  32.1  30.2  \"34.6\"\n## [4,] 28.5  25.9  32.1  32.4  33.1  30.2  36    \n## [5,] 56.7  57.7  57.9  58.6  59.1  13.1  13.1  \n## [6,] 24.6  26.8  30.4  31.9  34.1  34.8  33.7  \n## \n## [[4]]\n##      [,1]    [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10] [,11]\n## [1,] \"OTHER\" 33603   33969 34334 34699 35064 35430 35795 36160 36525 36891\n## [2,] \"FOTHT\" \"FOTHN\" NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] \"EU\"    \":\"     2.9   -0.5  3.9   3.9   1     4.1   4.7   7     7.2  \n## [4,] \"EA\"    \":\"     2.3   -4.9  1.4   1.3   -2.4  1.1   3.2   5.8   7    \n## [5,] \"BE\"    \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   14    14.9  15.9 \n## [6,] \"BG\"    \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"  \n##      [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]\n## [1,] 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543 40908\n## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] -1.5  6.2   8.1   7.6   1.4   2.4   13.7  -1.9  -3.2  1.1   1.1  \n## [4,] -3.7  5.5   7.1   7.2   -2.2  0.4   15.5  -4.6  -8.4  0.3   -3.3 \n## [5,] 16.3  22.8  23.1  22.4  24.5  25.3  25.5  26.6  26.6  24.7  24.6 \n## [6,] \":\"   -2.3  -0.8  2.4   2.9   3.5   4.8   5.5   2.2   3.3   3.2  \n##      [,23] [,24] [,25] [,26] [,27] [,28]\n## [1,] 41274 41639 42004 42369 42735 43100\n## [2,] NA    NA    NA    NA    NA    NA   \n## [3,] -1.6  0.9   2.7   1.9   -3.3  \"2.1\"\n## [4,] -2.3  0.6   2.5   2.1   -5.4  1.7  \n## [5,] 26.4  25.9  25    25.3  4.7   5.2  \n## [6,] 5.9   7     8.2   9.6   9.4   9.1\n\nWe are getting really close to something useful! Now we can get the first table and do some basic cleaning to have a tidy dataset:\n\ndataset1 &lt;- list_of_data[[1]]\n\ndataset1 &lt;- dataset1[-c(1:3), ]\ndataset1[dataset1 == \":\"] &lt;- NA\ncolnames(dataset1) &lt;- c(\"country\", seq(from = 1991, to = 2017))\n\nhead(dataset1)\n##      country 1991 1992 1993  1994 1995 1996 1997 1998 1999 2000 2001 2002\n## [1,] \"EU\"    NA   16.9 -1.4  20.2 34.5 31.4 37.5 39   37.3 39.2 27.5 20.6\n## [2,] \"EA\"    NA   15.5 -13.1 14.8 30.9 25.1 35.2 39.2 37.1 39.5 25.3 18.2\n## [3,] \"BE\"    NA   NA   NA    NA   NA   NA   NA   42.3 43.1 45.8 42.2 42.9\n## [4,] \"BG\"    NA   NA   NA    NA   NA   NA   NA   NA   NA   NA   NA   39.6\n## [5,] \"CZ\"    NA   NA   NA    NA   NA   NA   NA   NA   NA   NA   NA   54.9\n## [6,] \"DK\"    49.5 45   50    59.5 62.5 55.5 60.5 57.5 56   61.5 57.5 59.5\n##      2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016\n## [1,] 21.4 29.8 26.4 32.5 47.1 19   -1.3 23.5 29   22   21.1 25.6 31.8 22.9\n## [2,] 18.9 27.4 23   28.2 46.1 12.3 -9.3 19.3 26.2 18.6 15.7 21.7 28.8 17.3\n## [3,] 43.8 45.8 47.4 49.1 50.9 48.2 46.9 46.3 46.8 47.1 48.2 50.1 49.2 34.5\n## [4,] 43   42.8 45.5 49.1 52.6 50.7 39.5 45.5 47.4 45.6 50.5 51.4 49.9 53.2\n## [5,] 37   48.5 67.9 66.4 66.8 69.3 64.7 61   56   47.5 53   53.5 67.5 58  \n## [6,] 53.5 50   59   64   63   56   33.5 57   47   48   52   45.5 40.5 36.5\n##      2017  \n## [1,] \"30.7\"\n## [2,] 26.6  \n## [3,] 34.4  \n## [4,] 52.8  \n## [5,] 59.5  \n## [6,] 37.5\n\nEt voilà! We went from a messy spreadsheet to a tidy dataset in a matter of minutes. Even though this package is still in early development and not all the features that are planned are available, the basics are there and can save you a lot of pain!"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html",
    "href": "posts/2017-03-08-lesser_known_tricks.html",
    "title": "Lesser known dplyr tricks",
    "section": "",
    "text": "In this blog post I share some lesser-known (at least I believe they are) tricks that use mainly functions from dplyr."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#removing-unneeded-columns",
    "href": "posts/2017-03-08-lesser_known_tricks.html#removing-unneeded-columns",
    "title": "Lesser known dplyr tricks",
    "section": "\nRemoving unneeded columns\n",
    "text": "Removing unneeded columns\n\n\nDid you know that you can use - in front of a column name to remove it from a data frame?\n\nmtcars %&gt;% \n    select(-disp) %&gt;% \n    head()\n##                    mpg cyl  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#re-ordering-columns",
    "href": "posts/2017-03-08-lesser_known_tricks.html#re-ordering-columns",
    "title": "Lesser known dplyr tricks",
    "section": "\nRe-ordering columns\n",
    "text": "Re-ordering columns\n\n\nStill using select(), it is easy te re-order columns in your data frame:\n\nmtcars %&gt;% \n    select(cyl, disp, hp, everything()) %&gt;% \n    head()\n##                   cyl disp  hp  mpg drat    wt  qsec vs am gear carb\n## Mazda RX4           6  160 110 21.0 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag       6  160 110 21.0 3.90 2.875 17.02  0  1    4    4\n## Datsun 710          4  108  93 22.8 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive      6  258 110 21.4 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout   8  360 175 18.7 3.15 3.440 17.02  0  0    3    2\n## Valiant             6  225 105 18.1 2.76 3.460 20.22  1  0    3    1\n\nAs its name implies everything() simply means all the other columns."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#renaming-columns-with-rename",
    "href": "posts/2017-03-08-lesser_known_tricks.html#renaming-columns-with-rename",
    "title": "Lesser known dplyr tricks",
    "section": "\nRenaming columns with rename()\n",
    "text": "Renaming columns with rename()\n\nmtcars &lt;- rename(mtcars, spam_mpg = mpg)\nmtcars &lt;- rename(mtcars, spam_disp = disp)\nmtcars &lt;- rename(mtcars, spam_hp = hp)\n\nhead(mtcars)\n##                   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am\n## Mazda RX4             21.0   6       160     110 3.90 2.620 16.46  0  1\n## Mazda RX4 Wag         21.0   6       160     110 3.90 2.875 17.02  0  1\n## Datsun 710            22.8   4       108      93 3.85 2.320 18.61  1  1\n## Hornet 4 Drive        21.4   6       258     110 3.08 3.215 19.44  1  0\n## Hornet Sportabout     18.7   8       360     175 3.15 3.440 17.02  0  0\n## Valiant               18.1   6       225     105 2.76 3.460 20.22  1  0\n##                   gear carb\n## Mazda RX4            4    4\n## Mazda RX4 Wag        4    4\n## Datsun 710           4    1\n## Hornet 4 Drive       3    1\n## Hornet Sportabout    3    2\n## Valiant              3    1"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#selecting-columns-with-a-regexp",
    "href": "posts/2017-03-08-lesser_known_tricks.html#selecting-columns-with-a-regexp",
    "title": "Lesser known dplyr tricks",
    "section": "\nSelecting columns with a regexp\n",
    "text": "Selecting columns with a regexp\n\n\nIt is easy to select the columns that start with “spam” with some helper functions:\n\nmtcars %&gt;% \n    select(contains(\"spam\")) %&gt;% \n    head()\n##                   spam_mpg spam_disp spam_hp\n## Mazda RX4             21.0       160     110\n## Mazda RX4 Wag         21.0       160     110\n## Datsun 710            22.8       108      93\n## Hornet 4 Drive        21.4       258     110\n## Hornet Sportabout     18.7       360     175\n## Valiant               18.1       225     105\n\ntake also a look at starts_with(), ends_with(), contains(), matches(), num_range(), one_of() and everything()."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#create-new-columns-with-mutate-and-if_else",
    "href": "posts/2017-03-08-lesser_known_tricks.html#create-new-columns-with-mutate-and-if_else",
    "title": "Lesser known dplyr tricks",
    "section": "\nCreate new columns with mutate() and if_else()\n",
    "text": "Create new columns with mutate() and if_else()\n\nmtcars %&gt;% \n    mutate(vs_new = if_else(\n        vs == 1, \n        \"one\", \n        \"zero\", \n        NA_character_)) %&gt;% \n    head()\n##   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am gear carb vs_new\n## 1     21.0   6       160     110 3.90 2.620 16.46  0  1    4    4   zero\n## 2     21.0   6       160     110 3.90 2.875 17.02  0  1    4    4   zero\n## 3     22.8   4       108      93 3.85 2.320 18.61  1  1    4    1    one\n## 4     21.4   6       258     110 3.08 3.215 19.44  1  0    3    1    one\n## 5     18.7   8       360     175 3.15 3.440 17.02  0  0    3    2   zero\n## 6     18.1   6       225     105 2.76 3.460 20.22  1  0    3    1    one\n\nYou might want to create a new variable conditionally on several values of another column:\n\nmtcars %&gt;% \n    mutate(carb_new = case_when(.$carb == 1 ~ \"one\",\n                                .$carb == 2 ~ \"two\",\n                                .$carb == 4 ~ \"four\",\n                                 TRUE ~ \"other\")) %&gt;% \n    head(15)\n##    spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am gear carb\n## 1      21.0   6     160.0     110 3.90 2.620 16.46  0  1    4    4\n## 2      21.0   6     160.0     110 3.90 2.875 17.02  0  1    4    4\n## 3      22.8   4     108.0      93 3.85 2.320 18.61  1  1    4    1\n## 4      21.4   6     258.0     110 3.08 3.215 19.44  1  0    3    1\n## 5      18.7   8     360.0     175 3.15 3.440 17.02  0  0    3    2\n## 6      18.1   6     225.0     105 2.76 3.460 20.22  1  0    3    1\n## 7      14.3   8     360.0     245 3.21 3.570 15.84  0  0    3    4\n## 8      24.4   4     146.7      62 3.69 3.190 20.00  1  0    4    2\n## 9      22.8   4     140.8      95 3.92 3.150 22.90  1  0    4    2\n## 10     19.2   6     167.6     123 3.92 3.440 18.30  1  0    4    4\n## 11     17.8   6     167.6     123 3.92 3.440 18.90  1  0    4    4\n## 12     16.4   8     275.8     180 3.07 4.070 17.40  0  0    3    3\n## 13     17.3   8     275.8     180 3.07 3.730 17.60  0  0    3    3\n## 14     15.2   8     275.8     180 3.07 3.780 18.00  0  0    3    3\n## 15     10.4   8     472.0     205 2.93 5.250 17.98  0  0    3    4\n##    carb_new\n## 1      four\n## 2      four\n## 3       one\n## 4       one\n## 5       two\n## 6       one\n## 7      four\n## 8       two\n## 9       two\n## 10     four\n## 11     four\n## 12    other\n## 13    other\n## 14    other\n## 15     four\n\nMind the .\\(&lt;/code&gt; before the variable &lt;code&gt;carb&lt;/code&gt;. There is a &lt;a href=\"https://github.com/hadley/dplyr/issues/1965\"&gt;github issue&lt;/a&gt;\nabout this, and it is already fixed in the development version of &lt;code&gt;dplyr&lt;/code&gt;, which means that in the next version\nof &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;case_when()&lt;/code&gt; will work as any other specialized &lt;code&gt;dplyr&lt;/code&gt; function inside &lt;code&gt;mutate()&lt;/code&gt;.&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"apply-a-function-to-certain-columns-only-by-rows\" class=\"section level2\"&gt;\n&lt;h2&gt;Apply a function to certain columns only, by rows&lt;/h2&gt;\n&lt;pre class=\"r\"&gt;&lt;code&gt;mtcars %&gt;%\n    select(am, gear, carb) %&gt;%\n    purrr::by_row(sum, .collate = &quot;cols&quot;, .to = &quot;sum_am_gear_carb&quot;) -&gt; mtcars2\nhead(mtcars2)&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;For this, I had to use &lt;code&gt;purrr&lt;/code&gt;’s &lt;code&gt;by_row()&lt;/code&gt; function. You can then add this column to your original data frame:&lt;/p&gt;\n&lt;pre class=\"r\"&gt;&lt;code&gt;mtcars &lt;- cbind(mtcars, &quot;sum_am_gear_carb&quot; = mtcars2\\)sum_am_gear_carb) head(mtcars)\n\n##                   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am\n## Mazda RX4             21.0   6       160     110 3.90 2.620 16.46  0  1\n## Mazda RX4 Wag         21.0   6       160     110 3.90 2.875 17.02  0  1\n## Datsun 710            22.8   4       108      93 3.85 2.320 18.61  1  1\n## Hornet 4 Drive        21.4   6       258     110 3.08 3.215 19.44  1  0\n## Hornet Sportabout     18.7   8       360     175 3.15 3.440 17.02  0  0\n## Valiant               18.1   6       225     105 2.76 3.460 20.22  1  0\n##                   gear carb sum_am_gear_carb\n## Mazda RX4            4    4                9\n## Mazda RX4 Wag        4    4                9\n## Datsun 710           4    1                6\n## Hornet 4 Drive       3    1                4\n## Hornet Sportabout    3    2                5\n## Valiant              3    1                4"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#use-do-to-do-any-arbitrary-operation",
    "href": "posts/2017-03-08-lesser_known_tricks.html#use-do-to-do-any-arbitrary-operation",
    "title": "Lesser known dplyr tricks",
    "section": "\nUse do() to do any arbitrary operation\n",
    "text": "Use do() to do any arbitrary operation\n\nmtcars %&gt;% \n    group_by(cyl) %&gt;% \n    do(models = lm(spam_mpg ~ drat + wt, data = .)) %&gt;% \n    broom::tidy(models)\n## # A tibble: 9 x 6\n## # Groups:   cyl [3]\n##     cyl term        estimate std.error statistic p.value\n##   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1     4 (Intercept)   33.2      17.1       1.94  0.0877 \n## 2     4 drat           1.32      3.45      0.384 0.711  \n## 3     4 wt            -5.24      2.22     -2.37  0.0456 \n## 4     6 (Intercept)   30.7       7.51      4.08  0.0151 \n## 5     6 drat          -0.444     1.17     -0.378 0.725  \n## 6     6 wt            -2.99      1.57     -1.91  0.129  \n## 7     8 (Intercept)   29.7       7.09      4.18  0.00153\n## 8     8 drat          -1.47      1.63     -0.903 0.386  \n## 9     8 wt            -2.45      0.799    -3.07  0.0107\n\ndo() is useful when you want to use any R function (user defined functions work too!) with dplyr functions. First I grouped the observations by cyl and then ran a linear model for each group. Then I converted the output to a tidy data frame using broom::tidy()."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#using-dplyr-functions-inside-your-own-functions",
    "href": "posts/2017-03-08-lesser_known_tricks.html#using-dplyr-functions-inside-your-own-functions",
    "title": "Lesser known dplyr tricks",
    "section": "\nUsing dplyr functions inside your own functions\n",
    "text": "Using dplyr functions inside your own functions\n\nextract_vars &lt;- function(data, some_string){\n    \n  data %&gt;%\n    select_(lazyeval::interp(~contains(some_string))) -&gt; data\n    \n  return(data)\n}\n\nextract_vars(mtcars, \"spam\")\n##                     spam_mpg spam_disp spam_hp\n## Mazda RX4               21.0     160.0     110\n## Mazda RX4 Wag           21.0     160.0     110\n## Datsun 710              22.8     108.0      93\n## Hornet 4 Drive          21.4     258.0     110\n## Hornet Sportabout       18.7     360.0     175\n## Valiant                 18.1     225.0     105\n## Duster 360              14.3     360.0     245\n## Merc 240D               24.4     146.7      62\n## Merc 230                22.8     140.8      95\n## Merc 280                19.2     167.6     123\n## Merc 280C               17.8     167.6     123\n## Merc 450SE              16.4     275.8     180\n## Merc 450SL              17.3     275.8     180\n## Merc 450SLC             15.2     275.8     180\n## Cadillac Fleetwood      10.4     472.0     205\n## Lincoln Continental     10.4     460.0     215\n## Chrysler Imperial       14.7     440.0     230\n## Fiat 128                32.4      78.7      66\n## Honda Civic             30.4      75.7      52\n## Toyota Corolla          33.9      71.1      65\n## Toyota Corona           21.5     120.1      97\n## Dodge Challenger        15.5     318.0     150\n## AMC Javelin             15.2     304.0     150\n## Camaro Z28              13.3     350.0     245\n## Pontiac Firebird        19.2     400.0     175\n## Fiat X1-9               27.3      79.0      66\n## Porsche 914-2           26.0     120.3      91\n## Lotus Europa            30.4      95.1     113\n## Ford Pantera L          15.8     351.0     264\n## Ferrari Dino            19.7     145.0     175\n## Maserati Bora           15.0     301.0     335\n## Volvo 142E              21.4     121.0     109\n\nAbout this last point, you can read more about it here.\n\n\nHope you liked this small list of tricks!"
  }
]