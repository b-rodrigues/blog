[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Econometrics and Free Software",
    "section": "",
    "text": "Welcome to my blog where I talk about R, Nix, Econometrics and Data Science. If you enjoy reading what I write, you might enjoy my books or want to follow me on Mastodon or Twitter or Bluesky. If you are 40+, click here instead. I also make videos on youtube.\n\n\n\n\n\n\n2025-01\n\n\n\ngithub pages setup for this website\n\n\n\n2024-12\n\n\n\nhuhu\n\n\n\n2024-11\n\n\n\nNovember blog post\n\n\n\n2018-01\n\n\n\nIt’s lists all the way down\n\n\nIt’s lists all the way down, part 2: We need to go deeper\n\n\nMapping a list of functions to a list of datasets with a list of columns as arguments\n\n\n\n2017-12\n\n\n\nBuilding formulae\n\n\nTeaching the tidyverse to beginners\n\n\n\n2017-11\n\n\n\nPeace of mind with purrr\n\n\n\n2017-10\n\n\n\nEasy peasy STATA-like marginal effects with R\n\n\n\n2017-08\n\n\n\nWhy I find tidyeval useful\n\n\n\n2017-07\n\n\n\nLesser known dplyr 0.7* tricks\n\n\ntidyr::spread() and dplyr::rename_at() in action\n\n\n\n2017-03\n\n\n\nIntroducing brotools\n\n\nLesser known dplyr tricks\n\n\nLesser known purrr tricks\n\n\nMake ggplot2 purrr\n\n\n\n2017-02\n\n\n\nHow to use jailbreakr\n\n\n\n2017-01\n\n\n\nMy free book has a cover!\n\n\n\n2016-12\n\n\n\nFunctional programming and unit testing for data munging with R available on Leanpub\n\n\nWork on lists of datasets instead of individual datasets by using functional programming\n\n\n\n2016-11\n\n\n\nI’ve started writing a ‘book’: Functional programming and unit testing for data munging with R\n\n\n\n2016-07\n\n\n\nData frame columns as arguments to dplyr functions\n\n\nMerge a list of datasets together\n\n\nRead a lot of datasets at once with R\n\n\n\n2016-03\n\n\n\nCareful with tryCatch\n\n\nUnit testing with R\n\n\n\n2015-11\n\n\n\nBootstrapping standard errors for difference-in-differences estimation with R\n\n\n\n2015-05\n\n\n\nUpdate to Introduction to programming econometrics with R\n\n\n\n2015-02\n\n\n\nExport R output to a file\n\n\n\n2015-01\n\n\n\nIntroduction to programming econometrics with R\n\n\n\n2014-11\n\n\n\nR, R with Atlas, R with OpenBLAS and Revolution R Open: which is fastest?\n\n\n\n2014-04\n\n\n\nObject Oriented Programming with R: An example with a Cournot duopoly\n\n\n\n2013-12\n\n\n\nSimulated Maximum Likelihood with R\n\n\nUsing R as a Computer Algebra System with Ryacas\n\n\n\n2013-11\n\n\n\nNonlinear Gmm with R - Example with a logistic regression\n\n\n\n2013-01\n\n\n\nMethod of Simulated Moments with R\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2016-12-24-functional-programming-and-unit-testing-for-data-munging-with-r-available-on-leanpub.html",
    "href": "posts/2016-12-24-functional-programming-and-unit-testing-for-data-munging-with-r-available-on-leanpub.html",
    "title": "Functional programming and unit testing for data munging with R available on Leanpub",
    "section": "",
    "text": "The book I’ve been working on these pasts months (you can read about it here, and read it for free here) is now available on Leanpub! You can grab a copy and read it on your ebook reader or on your computer, and what’s even better is that it is available for free (but you can also decide to buy it if you really like it). Here is the link on Leanpub.\nIn the book, I show you the basics of functional programming, unit testing and package development for the R programming language. The end goal is to make your data tidy in a reproducible way!\nJust a heads up: as the book is right now, the formatting is not perfect and images are missing. This is because I use bookdown to write the book and convert it to Leanpub’s markdown flavour is not trivial. I will find a solution to automate the conversion from bookdown’s version to Leanpub’s markdown and try to keep both versions in sync. Of course, once the book will be finished, the version on Leanpub and on my website are going to be completely identical. If you want to read it on your computer offline, you can also download a pdf from the book’s website, by clicking on the pdf icon in the top left corner. Do not hesitate to send me an email if you want to give me feedback (just click on the red envelope in the top right corner) or tweet me @brodriguesco."
  },
  {
    "objectID": "posts/2017-08-27-why_tidyeval.html",
    "href": "posts/2017-08-27-why_tidyeval.html",
    "title": "Why I find tidyeval useful",
    "section": "",
    "text": "First thing’s first: maybe you shouldn’t care about tidyeval. Maybe you don’t need it. If you exclusively work interactively, I don’t think that learning about tidyeval is important. I can only speak for me, and explain to you why I personally find tidyeval useful.\n\n\nI wanted to write this blog post after reading this twitter thread and specifically this question.\n\n\nMara Averick then wrote this blogpost linking to 6 other blog posts that give some tidyeval examples. Reading them, plus the Programming with dplyr vignette should help you get started with tidyeval.\n\n\nBut maybe now you know how to use it, but not why and when you should use it… Basically, whenever you want to write a function that looks something like this:\n\nmy_function(my_data, one_column_inside_data)\n\nis when you want to use the power of tidyeval.\n\n\nI work at STATEC, Luxembourg’s national institute of statistics. I work on all kinds of different projects, and when data gets updated (for example because a new round of data collection for some survey finished), I run my own scripts on the fresh data to make the data nice and tidy for analysis. Because surveys get updated, sometimes column names change a little bit, and this can cause some issues.\n\n\nVery recently, a dataset I work with got updated. Data collection was finished, so I just loaded my hombrewed package written for this project, changed the path from last year’s script to this year’s fresh data path, ran the code, and watched as the folders got populated with new ggplot2 graphs and LaTeX tables with descriptive statistics and regression results. This is then used to generate this year’s report. However, by looking at the graphs, I noticed something weird; some graphs were showing some very strange patterns. It turns out that one column got its name changed, and also one of its values got changed too.\n\n\nLast year, this column, let’s call it spam, had values 1 for good and 0 for bad. This year the column is called Spam and the values are 1 and 2. When I found out that this was the source of the problem, I just had to change the arguments of my functions from\n\ngenerate_spam_plot(dataset = data2016, column = spam, value = 1)\ngenerate_spam_plot(dataset = data2016, column = spam, value = 0)\n\nto\n\ngenerate_spam_plot(dataset = data2017, column = Spam, value = 1)\ngenerate_spam_plot(dataset = data2017, column = Spam, value = 2)\n\nwithout needing to change anything else. This is why I use tidyeval; without it, writing a function such as genereta_spam_plot would not be easy. It would be possible, but not easy.\n\n\nIf you want to know more about tidyeval and working programmatically with R, I shamelessly invite you to read a book I’ve been working on: https://b-rodrigues.github.io/fput/ It’s still a WIP, but maybe you’ll find it useful. I plan on finishing it by the end of the year, but there’s already some content to keep you busy!"
  },
  {
    "objectID": "posts/2017-03-27-introducing_brotools.html",
    "href": "posts/2017-03-27-introducing_brotools.html",
    "title": "Introducing brotools",
    "section": "",
    "text": "I’m happy to announce my first R package, called brotools. This is a package that contains functions that are specific to my needs but that you might find also useful. I blogged about some of these functions, so if you follow my blog you might already be familiar with some of them. It is not on CRAN and might very well never be. The code is hosted on bitbucket and you can install the package with\n\ndevtools::install_bitbucket(\"b-rodrigues/brotools\")\n\nHope you’ll find the brotools useful!"
  },
  {
    "objectID": "posts/2017-03-29-make-ggplot2-purrr.html",
    "href": "posts/2017-03-29-make-ggplot2-purrr.html",
    "title": "Make ggplot2 purrr",
    "section": "",
    "text": "Update: I’ve included another way of saving a separate plot by group in this article, as pointed out by @monitus. Actually, this is the preferred solution; using dplyr::do() is deprecated, according to Hadley Wickham himself.\n\n\nI’ll be honest: the title is a bit misleading. I will not use purrr that much in this blog post. Actually, I will use one single purrr function, at the very end. I use dplyr much more. However Make ggplot2 purrr sounds better than Make ggplot dplyr or whatever the verb for dplyr would be.\n\n\nAlso, this blog post was inspired by a stackoverflow question and in particular one of the answers. So I don’t bring anything new to the table, but I found this stackoverflow answer so useful and so underrated (only 16 upvotes as I’m writing this!) that I wanted to write something about it.\n\n\nBasically the idea of this blog post is to show how to create graphs using ggplot2, but by grouping by a factor variable beforehand. To illustrate this idea, let’s use the data from the Penn World Tables 9.0. The easiest way to get this data is to install the package called pwt9 with:\n\ninstall.packages(\"pwt9\")\n\nand then load the data with:\n\ndata(\"pwt9.0\")\n\nNow, let’s load the needed packages. I am also using ggthemes which makes themeing your ggplots very easy. I’ll be making Tufte-style plots.\n\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(pwt9)\n\nFirst let’s select a list of countries:\n\ncountry_list &lt;- c(\"France\", \"Germany\", \"United States of America\", \"Luxembourg\", \"Switzerland\", \"Greece\")\n\nsmall_pwt &lt;- pwt9.0 %&gt;%\n  filter(country %in% country_list)\n\nLet’s us also order the countries in the data frame as I have written them in country_list:\n\nsmall_pwt &lt;- small_pwt %&gt;%\n  mutate(country = factor(country, levels = country_list, ordered = TRUE))\n\nYou might be wondering why this is important. At the end of the article, we are going to save the plots to disk. If we do not re-order the countries inside the data frame as in country_list, the name of the files will not correspond to the correct plots!\n\n\nUpdate: While this can still be interesting to know, especially if you want to order the bars of a barplot made with ggplot2, I included a suggestion by @expersso that does not require your data to be ordered!\n\n\nNow when you want to plot the same variable by countries, say avh (Average annual hours worked by persons engaged), the usual way to do this is with one of facet_wrap() or facet_grid():\n\nggplot(data = small_pwt) + theme_tufte() +\n  geom_line(aes(y = avh, x = year)) +\n  facet_wrap(~country)\n\n\n\nggplot(data = small_pwt) + theme_tufte() +\n  geom_line(aes(y = avh, x = year)) +\n  facet_grid(country~.)\n\n\n\n\nAs you can see, for this particular example, facet_grid() is not very useful, but do notice its argument, country~., which is different from facet_wrap()’s argument. This way, I get the graphs stacked horizontally. If I had used facet_grid(~country) the graphs would be side by side and completely unreadable.\n\n\nNow, let’s go to the meat of this post: what if you would like to have one single graph for each country? You’d probably think of using dplyr::group_by() to form the groups and then the graphs. This is the way to go, but you also have to use dplyr::do(). This is because as far as I understand, ggplot2 is not dplyr-aware, and using an arbitrary function with groups is only possible with dplyr::do().\n\n\nUpdate: As explained in the intro above, I also added the solution that uses tidyr::nest():\n\n# Ancient, deprecated way of doing this\nplots &lt;- small_pwt %&gt;%\n  group_by(country) %&gt;%\n  do(plot = ggplot(data = .) + theme_tufte() +\n       geom_line(aes(y = avh, x = year)) +\n       ggtitle(unique(.$country)) +\n       ylab(\"Year\") +\n       xlab(\"Average annual hours worked by persons engaged\"))\n\nAnd this is the approach that uses tidyr::nest():\n\n# Preferred approach\nplots &lt;- small_pwt %&gt;%\n  group_by(country) %&gt;%\n  nest() %&gt;%\n  mutate(plot = map2(data, country, ~ggplot(data = .x) + theme_tufte() +\n       geom_line(aes(y = avh, x = year)) +\n       ggtitle(.y) +\n       ylab(\"Year\") +\n       xlab(\"Average annual hours worked by persons engaged\")))\n\nIf you know dplyr at least a little bit, the above lines should be easy for you to understand. But notice how we get the title of the graphs, with ggtitle(unique(.$country)), which was actually the point of the stackoverflow question.\n\n\nUpdate: The modern version uses tidyr::nest(). Its documentation tells us:\n\n\nThere are many possible ways one could choose to nest columns inside a data frame. nest() creates a list of data frames containing all the nested variables: this seems to be the most useful form in practice. Let’s take a closer look at what it does exactly:\n\nsmall_pwt %&gt;%\n  group_by(country) %&gt;%\n  nest() %&gt;%\n  head()\n## # A tibble: 6 x 2\n##   country                  data              \n##   &lt;ord&gt;                    &lt;list&gt;            \n## 1 Switzerland              &lt;tibble [65 × 46]&gt;\n## 2 Germany                  &lt;tibble [65 × 46]&gt;\n## 3 France                   &lt;tibble [65 × 46]&gt;\n## 4 Greece                   &lt;tibble [65 × 46]&gt;\n## 5 Luxembourg               &lt;tibble [65 × 46]&gt;\n## 6 United States of America &lt;tibble [65 × 46]&gt;\n\nThis is why I love lists in R; we get a tibble where each element of the column data is itself a tibble. We can now apply any function that we know works on lists.\n\n\nWhat might be surprising though, is the object that is created by this code. Let’s take a look at plots:\n\nprint(plots)\n## # A tibble: 6 x 3\n##   country                  data               plot    \n##   &lt;ord&gt;                    &lt;list&gt;             &lt;list&gt;  \n## 1 Switzerland              &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 2 Germany                  &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 3 France                   &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 4 Greece                   &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 5 Luxembourg               &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 6 United States of America &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n\nAs dplyr::do()’s documentation tells us, the return values get stored inside a list. And this is exactly what we get back; a list of plots! Lists are a very flexible and useful class, and you cannot spell list without purrr (at least not when you’re a neRd).\n\n\nHere are the final lines that use purrr::map2() to save all these plots at once inside your working directory:\n\n\nUpdate: I have changed the code below which does not require your data frame to be ordered according to the variable country_list.\n\n# file_names &lt;- paste0(country_list, \".pdf\")\n\nmap2(paste0(plots$country, \".pdf\"), plots$plot, ggsave)\n\nAs I said before, if you do not re-order the countries inside the data frame, the names of the files and the plots will not match. Try running all the code without re-ordering, you’ll see!\n\n\nI hope you found this post useful. You can follow me on twitter for blog updates.\n\n\nUpdate: Many thanks to the readers of this article and for their useful suggestions. I love the R community; everyday I learn something new and useful!"
  },
  {
    "objectID": "posts/2017-12-17-teaching_tidyverse.html",
    "href": "posts/2017-12-17-teaching_tidyverse.html",
    "title": "Teaching the tidyverse to beginners",
    "section": "",
    "text": "End October I tweeted this:\n\n\n\nwill teach #rstats soon again but this time following @drob 's suggestion of the tidyverse first as laid out here: https://t.co/js8SsUs8Nv\n\n— Bruno Rodrigues (@brodriguesco) October 24, 2017\n\n\n\nand it generated some discussion. Some people believe that this is the right approach, and some others think that one should first present base R and then show how the tidyverse complements it. This year, I taught three classes; a 12-hour class to colleagues that work with me, a 15-hour class to master’s students and 3 hours again to some of my colleagues. Each time, I decided to focus on the tidyverse(almost) entirely, and must say that I am not disappointed with the results!\n\n\nThe 12 hour class was divided in two 6 hours days. It was a bit intense, especially the last 3 hours that took place Friday afternoon. The crowd was composed of some economists that had experience with STATA, some others that were mostly using Excel and finally some colleagues from the IT department that sometimes need to dig into some data themselves. Apart from 2 people, all the other never had any experience with R.\n\n\nWe went from 0 to being able to do the plot below after the end of the first day (so 6 hours in). Keep in mind that practically none of them even had opened RStudio before. I show the code so you can see the progress made in just a few hours:\n\nlibrary(Ecdat)\nlibrary(tidyverse)\nlibrary(ggthemes)\ndata(Bwages)\nbwages = Bwages %&gt;%\n  mutate(educ_level = case_when(educ == 1 ~ \"Primary School\",\n                                educ == 2 ~ \"High School\",\n                                educ == 3 ~ \"Some university\",\n                                educ == 4 ~ \"Master's degree\",\n                                educ == 5 ~ \"Doctoral degree\"))\n\nggplot(bwages) +\n  geom_smooth(aes(exper, wage, colour = educ_level)) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n## `geom_smooth()` using method = 'loess'\n\n\n\n\nOf course some of them needed some help here and there, and I also gave them hints (for example I told them about case_when() and try to use it inside mutate() instead of nested ifs) but it was mostly due to lack of experience and because they hadn’t had the time to fully digest R’s syntax which was for most people involved completely new.\n\n\nOn the second day I showed purrr::map() and purrr::reduce() and overall it went quite well too. I even showed list-columns, and this is where I started losing some of them; I did not insist too much on it though, only wanted to show them the flexibility of data.frame objects. Some of them were quite impressed by list-columns! Then I started showing (for and while) loops and writing functions. I even showed them tidyeval and again, it went relatively well. Once they had the opportunity to play a bit around with it, I think it clicked (plus they have lots of code examples to go back too).\n\n\nAt the end, people seemed to have enjoyed the course, but told me that Friday was heavy; indeed it was, but I feel that it was mostly because 12 hours spread on 2 days is not the best format for this type of material, but we all had time constraints.\n\n\nThe 15 hour Master’s course was spread over 4 days, and covered basically the same. I just used the last 3 hours to show the students some basic functions for model estimation (linear, count, logit/probit and survival models). Again, the students were quite impressed by how easily they could get descriptive statistics by first grouping by some variables. Through their questions, I even got to show them scoped versions of dplyr verbs, such as select_if() and summarise_at(). I was expecting to lose them there, but actually most of them got these scoped versions quite fast. These students already had some experience with R though, but none with the tidyverse.\n\n\nFinally the 3 hour course was perhaps the most interesting; I only had 100% total beginners. Some just knew R by name and had never heard/seen/opened RStudio (with the exception of one person)! I did not show them any loops, function definitions and no plots. I only showed them how RStudio looked and worked, what were (and how to install) packages (as well as the CRAN Task Views) and then how to import data with rio and do descriptive statistics only with dplyr. They were really interested and quite impressed by rio (“what do you mean I can use the same code for importing any dataset, in any format?”) but also by the simplicity of dplyr.\n\n\nIn all the courses, I did show the $ primitive to refer to columns inside a data.frame. First I showed them lists which is where I introduced $. Then it was easy to explain to them why it was the same for a column inside a data.frame; a data.frame is simply a list! This is also the distinction I made from the previous years; I simply mentioned (and showed really quickly) matrices and focused almost entirely on lists. Most participants, if not all, had learned to program statistics by thinking about linear algebra and matrices. Nothing wrong with that, but I feel that R really shines when you focus on lists and on how to work with them.\n\n\nOverall as the teacher, I think that focusing on the tidyverse might be a very good strategy. I might have to do some adjustments here and there for the future courses, but my hunch is that the difficulties that some participants had were not necessarily due to the tidyverse but simply to lack of time to digest what was shown, as well as a total lack of experience with R. I do not think that these participants would have better understood a more traditional, base, matrix-oriented course."
  },
  {
    "objectID": "posts/2015-05-03-update-introduction-r-programming.html",
    "href": "posts/2015-05-03-update-introduction-r-programming.html",
    "title": "Update to Introduction to programming econometrics with R",
    "section": "",
    "text": "This semester I taught a course on applied econometrics with the R programming language. For this, I created a document that I gave to my students and shared online. This is the kind of document I would have liked to read when I first started using R. I already had some programming experience in C and Pascal but this is not necessarily the case for everyone that is confronted to R when they start learning about econometrics.\nThis is why the beginning of the document focuses more on general programming knowledge and techniques, and then only on econometrics. People online seemed to like the document, as I’ve received some positive comments by David Smith from Revolution R (read his blog post about the document here) and Dave Giles which links to David’s blog post here. People on twitter have also retweeted David’s and Dave’s tweets to their blog posts and I’ve received some requests by people to send them the PDF by email (because they live in places where Dropbox is not accessible unfortunately).\nThe document is still a work in progress (and will probably remain so for a long time), but I’ve added some new sections about reproducible research and thought that this update could warrant a new blog post.\nFor now, only linear models are reviewed, but I think I’ll start adding some chapters about non-linear models soonish. The goal for these notes, however, is not to re-invent the wheel: there are lots of good books about econometrics with R out there and packages that estimate a very wide range of models. What I want for these notes, is to focus more on the programming knowledge an econometrician needs, in a very broad and general sense. I want my students to understand that R is a true programming language, and that they need to use every feature offered by such a language, and not think of R as a black box into which you only type pre-programmed commands, but also be able to program their own routines.\nAlso, I’ve made it possible to create the PDF using a Makefile. This may be useful for people that do not have access to Dropbox, but are familiar with git.\nYou can compile the book in two ways: first download the whole repository:\ngit clone git@bitbucket.org:b-rodrigues/programmingeconometrics.git\nand then, with Rstudio, open the file appliedEconometrics.Rnw and knit it. Another solution is to use the Makefile. Just type:\nmake\ninside a terminal (should work on GNU+Linux and OSX systems) and it should compile the document. You may get some message about “additional information” for some R packages. When these come up, just press Q on your keyboard to continue the compilation process.\nGet the notes here.\nAs always, if you have questions or suggestions, do not hesitate to send me an email and I sure hope you’ll find these notes useful!"
  },
  {
    "objectID": "posts/2017-07-02-dplyr-0-70-tutorial.html",
    "href": "posts/2017-07-02-dplyr-0-70-tutorial.html",
    "title": "Lesser known dplyr 0.7* tricks",
    "section": "",
    "text": "This blog post is an update to an older one I wrote in March. In the post from March, dplyr was at version 0.50, but since then a major update introduced some changes that make some of the tips in that post obsolete. So here I revisit the blog post from March by using dplyr 0.70."
  },
  {
    "objectID": "posts/2017-07-02-dplyr-0-70-tutorial.html#create-new-columns-with-mutate-and-case_when",
    "href": "posts/2017-07-02-dplyr-0-70-tutorial.html#create-new-columns-with-mutate-and-case_when",
    "title": "Lesser known dplyr 0.7* tricks",
    "section": "\nCreate new columns with mutate() and case_when()\n",
    "text": "Create new columns with mutate() and case_when()\n\n\nThe basic things such as selecting columns, renaming them, filtering, etc did not change with this new version. What did change however is creating new columns using case_when(). First, load dplyr and the mtcars dataset:\n\nlibrary(\"dplyr\")\ndata(mtcars)\n\nThis was how it was done in version 0.50 (notice the ’.\\(’ symbol before the variable ‘carb’):&lt;/p&gt;\n&lt;pre class=\"r\"&gt;&lt;code&gt;mtcars %&gt;%\n    mutate(carb_new = case_when(.\\)carb == 1 ~ \"one\", .\\(carb == 2 ~ &quot;two&quot;,\n                                .\\)carb == 4 ~ \"four\", TRUE ~ \"other\")) %&gt;% head(5)\n\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb carb_new\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4     four\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4     four\n## 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1      one\n## 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1      one\n## 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2      two\n\nThis has been simplified to:\n\nmtcars %&gt;%\n    mutate(carb_new = case_when(carb == 1 ~ \"one\",\n                                carb == 2 ~ \"two\",\n                                carb == 4 ~ \"four\",\n                                TRUE ~ \"other\")) %&gt;%\n    head(5)\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb carb_new\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4     four\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4     four\n## 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1      one\n## 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1      one\n## 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2      two\n\nNo need for .$ anymore."
  },
  {
    "objectID": "posts/2017-07-02-dplyr-0-70-tutorial.html#apply-a-function-to-certain-columns-only-by-rows-with-purrrlyr",
    "href": "posts/2017-07-02-dplyr-0-70-tutorial.html#apply-a-function-to-certain-columns-only-by-rows-with-purrrlyr",
    "title": "Lesser known dplyr 0.7* tricks",
    "section": "\nApply a function to certain columns only, by rows, with purrrlyr\n",
    "text": "Apply a function to certain columns only, by rows, with purrrlyr\n\n\ndplyr wasn’t the only package to get an overhaul, purrr also got the same treatment.\n\n\nIn the past, I applied a function to certains columns like this:\n\nmtcars %&gt;%\n    select(am, gear, carb) %&gt;%\n    purrr::by_row(sum, .collate = \"cols\", .to = \"sum_am_gear_carb\") -&gt; mtcars2\nhead(mtcars2)\n\nNow, by_row() does not exist in purrr anymore, but instead a new package called purrrlyr was introduced with functions that don’t really fit inside purrr nor dplyr:\n\nmtcars %&gt;%\n    select(am, gear, carb) %&gt;%\n    purrrlyr::by_row(sum, .collate = \"cols\", .to = \"sum_am_gear_carb\") -&gt; mtcars2\nhead(mtcars2)\n## # A tibble: 6 x 4\n##      am  gear  carb sum_am_gear_carb\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;\n## 1     1     4     4                9\n## 2     1     4     4                9\n## 3     1     4     1                6\n## 4     0     3     1                4\n## 5     0     3     2                5\n## 6     0     3     1                4\n\nThink of purrrlyr as purrrs and dplyrs love child."
  },
  {
    "objectID": "posts/2017-07-02-dplyr-0-70-tutorial.html#using-dplyr-functions-inside-your-own-functions-or-what-is-tidyeval",
    "href": "posts/2017-07-02-dplyr-0-70-tutorial.html#using-dplyr-functions-inside-your-own-functions-or-what-is-tidyeval",
    "title": "Lesser known dplyr 0.7* tricks",
    "section": "\nUsing dplyr functions inside your own functions, or what is tidyeval\n",
    "text": "Using dplyr functions inside your own functions, or what is tidyeval\n\n\nProgramming with dplyr has been simplified a lot. Before version 0.70, one needed to use dplyr in conjuction with lazyeval to use dplyr functions inside one’s own fuctions. It was not always very easy, especially if you mixed columns and values inside your functions. Here’s the example from the March blog post:\n\nextract_vars &lt;- function(data, some_string){\n\n  data %&gt;%\n    select_(lazyeval::interp(~contains(some_string))) -&gt; data\n\n  return(data)\n}\n\nextract_vars(mtcars, \"spam\")\n\nMore examples are available in this other blog post.\n\n\nI will revisit them now with dplyr’s new tidyeval syntax. I’d recommend you read the Tidy evaluation vignette here. This vignette is part of the rlang package, which gets used under the hood by dplyr for all your programming needs. Here is the function I called simpleFunction(), written with the old dplyr syntax:\n\nsimpleFunction &lt;- function(dataset, col_name){\n  dataset %&gt;%\n    group_by_(col_name) %&gt;%\n    summarise(mean_mpg = mean(mpg)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, \"cyl\")\n## # A tibble: 3 x 2\n##     cyl mean_mpg\n##   &lt;dbl&gt;    &lt;dbl&gt;\n## 1     4     26.7\n## 2     6     19.7\n## 3     8     15.1\n\nWith the new synax, it must be rewritten a little bit:\n\nsimpleFunction &lt;- function(dataset, col_name){\n  col_name &lt;- enquo(col_name)\n  dataset %&gt;%\n    group_by(!!col_name) %&gt;%\n    summarise(mean_mpg = mean(mpg)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, cyl)\n## # A tibble: 3 x 2\n##     cyl mean_mpg\n##   &lt;dbl&gt;    &lt;dbl&gt;\n## 1     4     26.7\n## 2     6     19.7\n## 3     8     15.1\n\nWhat has changed? Forget the underscore versions of the usual functions such as select_(), group_by_(), etc. Now, you must quote the column name using enquo() (or just quo() if working interactively, outside a function), which returns a quosure. This quosure can then be evaluated using !! in front of the quosure and inside the usual dplyr functions.\n\n\nLet’s look at another example:\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  filter_criteria &lt;- lazyeval::interp(~y == x, .values=list(y = as.name(col_name), x = value))\n  dataset %&gt;%\n    filter_(filter_criteria) %&gt;%\n    summarise(mean_cyl = mean(cyl)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, \"am\", 1)\n##   mean_cyl\n## 1 5.076923\n\nAs you can see, it’s a bit more complicated, as you needed to use lazyeval::interp() to make it work. With the improved dplyr, here’s how it’s done:\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  col_name &lt;- enquo(col_name)\n  dataset %&gt;%\n    filter((!!col_name) == value) %&gt;%\n    summarise(mean_cyl = mean(cyl)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, am, 1)\n##   mean_cyl\n## 1 5.076923\n\nMuch, much easier! There is something that you must pay attention to though. Notice that I’ve written:\n\nfilter((!!col_name) == value)\n\nand not:\n\nfilter(!!col_name == value)\n\nI have enclosed !!col_name inside parentheses. I struggled with this, but thanks to help from @dmi3k and @_lionelhenry I was able to understand what was happening (isn’t the #rstats community on twitter great?).\n\n\nOne last thing: let’s make this function a bit more general. I hard-coded the variable cyl inside the body of the function, but maybe you’d like the mean of another variable? Easy:\n\nsimpleFunction &lt;- function(dataset, group_col, mean_col, value){\n  group_col &lt;- enquo(group_col)\n  mean_col &lt;- enquo(mean_col)\n  dataset %&gt;%\n    filter((!!group_col) == value) %&gt;%\n    summarise(mean((!!mean_col))) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, am, cyl, 1)\n##   mean(cyl)\n## 1  5.076923\n\n«That’s very nice Bruno, but mean((cyl)) in the output looks ugly as sin» you might think, and you’d be right. It is possible to set the name of the column in the output using := instead of =:\n\nsimpleFunction &lt;- function(dataset, group_col, mean_col, value){\n  group_col &lt;- enquo(group_col)\n  mean_col &lt;- enquo(mean_col)\n  mean_name &lt;- paste0(\"mean_\", mean_col)[2]\n  dataset %&gt;%\n    filter((!!group_col) == value) %&gt;%\n    summarise(!!mean_name := mean((!!mean_col))) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, am, cyl, 1)\n##   mean_cyl\n## 1 5.076923\n\nTo get the name of the column I added this line:\n\nmean_name &lt;- paste0(\"mean_\", mean_col)[2]\n\nTo see what it does, try the following inside an R interpreter (remember to us quo() instead of enquo() outside functions!):\n\npaste0(\"mean_\", quo(cyl))\n## [1] \"mean_~\"   \"mean_cyl\"\n\nenquo() quotes the input, and with paste0() it gets converted to a string that can be used as a column name. However, the ~ is in the way and the output of paste0() is a vector of two strings: the correct name is contained in the second element, hence the [2]. There might be a more elegant way of doing that, but for now this has been working well for me.\n\n\nThat was it folks! I do recommend you read the Programming with dplyr vignette here as well as other blog posts, such as the one recommended to me by @dmi3k here.\n\n\nHave fun with dplyr 0.70!"
  },
  {
    "objectID": "posts/2017-11-14-peace_r.html",
    "href": "posts/2017-11-14-peace_r.html",
    "title": "Peace of mind with purrr",
    "section": "",
    "text": "I think what I enjoy the most about functional programming is the peace of mind that comes with it. With functional programming, there’s a lot of stuff you don’t need to think about. You can write functions that are general enough so that they solve a variety of problems. For example, imagine for a second that R does not have the sum() function anymore. If you want to compute the sum of, say, the first 100 integers, you could write a loop that would do that for you:\n\nnumbers = 0\n\nfor (i in 1:100){\n  numbers = numbers + i\n}\n\nprint(numbers)\n\n[1] 5050\n\n\nThe problem with this approach, is that you cannot reuse any of the code there, even if you put it inside a function. For instance, what if you want to merge 4 datasets together? You would need something like this:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata(mtcars)\n\nmtcars1 = mtcars %&gt;%\n  mutate(id = \"1\")\n\nmtcars2 = mtcars %&gt;%\n  mutate(id = \"2\")\n\nmtcars3 = mtcars %&gt;%\n  mutate(id = \"3\")\n\nmtcars4 = mtcars %&gt;%\n  mutate(id = \"4\")\n\ndatasets = list(mtcars1, mtcars2, mtcars3, mtcars4)\n\ntemp = datasets[[1]]\n\nfor(i in 1:3){\n  temp = full_join(temp, datasets[[i+1]])\n}\n\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\n\n\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\n\nglimpse(temp)\n\nRows: 128\nColumns: 12\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n$ id   &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n\n\nOf course, the logic is very similar as before, but you need to think carefully about the structure holding your elements (which can be numbers, datasets, characters, etc…) as well as be careful about indexing correctly… and depending on the type of objects you are working on, you might need to tweak the code further.\nHow would a functional programming approach make this easier? Of course, you could use purrr::reduce() to solve these problems. However, since I assumed that sum() does not exist, I will also assume that purrr::reduce() does not exist either and write my own, clumsy implementation. Here’s the code:\n\nmy_reduce = function(a_list, a_func, init = NULL, ...){\n\n  if(is.null(init)){\n    init = `[[`(a_list, 1)\n    a_list = tail(a_list, -1)\n  }\n\n  car = `[[`(a_list, 1)\n  cdr = tail(a_list, -1)\n  init = a_func(init, car, ...)\n\n  if(length(cdr) != 0){\n    my_reduce(cdr, a_func, init, ...)\n  }\n  else {\n    init\n  }\n}\n\nThis can look much more complicated than before, but the idea is quite simple; if you know about recursive functions (recursive functions are functions that call themselves). I won’t explain how the function works, because it is not the main point of the article (but if you’re curious, I encourage you to play around with it). The point is that now, I can do the following:\n\nmy_reduce(list(1,2,3,4,5), `+`)\n\n[1] 15\n\nmy_reduce(datasets, full_join) %&gt;% glimpse\n\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\n\n\nRows: 128\nColumns: 12\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n$ id   &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n\n\nBut since my_reduce() is very general, I can even do this:\n\nmy_reduce(list(1, 2, 3, 4, \"5\"), paste)\n\n[1] \"1 2 3 4 5\"\n\n\nOf course, paste() is vectorized, so you could just as well do paste(1, 2, 3, 4, 5), but again, I want to insist on the fact that writing functions, even if they look a bit complicated, can save you a huge amount of time in the long run.\nBecause I know that my function is quite general, I can be confident that it will work in a lot of different situations; as long as the a_func argument is a binary operator that combines the elements inside a_list, it’s going to work. And I don’t need to think about indexing, about having temporary variables or thinking about the structure that will hold my results."
  },
  {
    "objectID": "posts/2017-10-26-margins_r.html",
    "href": "posts/2017-10-26-margins_r.html",
    "title": "Easy peasy STATA-like marginal effects with R",
    "section": "",
    "text": "Model interpretation is essential in the social sciences. If one wants to know the effect of variable x on the dependent variable y, marginal effects are an easy way to get the answer. STATA includes a margins command that has been ported to R by Thomas J. Leeper of the London School of Economics and Political Science. You can find the source code of the package on github. In this short blog post, I demo some of the functionality of margins.\n\n\nFirst, let’s load some packages:\n\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(broom)\nlibrary(margins)\nlibrary(Ecdat)\n\nAs an example, we are going to use the Participation data from the Ecdat package:\n\ndata(Participation)\n?Participation\nLabor Force Participation\n\nDescription\n\na cross-section\n\nnumber of observations : 872\n\nobservation : individuals\n\ncountry : Switzerland\n\nUsage\n\ndata(Participation)\nFormat\n\nA dataframe containing :\n\nlfp\nlabour force participation ?\n\nlnnlinc\nthe log of nonlabour income\n\nage\nage in years divided by 10\n\neduc\nyears of formal education\n\nnyc\nthe number of young children (younger than 7)\n\nnoc\nnumber of older children\n\nforeign\nforeigner ?\n\nSource\n\nGerfin, Michael (1996) “Parametric and semiparametric estimation of the binary response”, Journal of Applied Econometrics, 11(3), 321-340.\n\nReferences\n\nDavidson, R. and James G. MacKinnon (2004) Econometric Theory and Methods, New York, Oxford University Press, http://www.econ.queensu.ca/ETM/, chapter 11.\n\nJournal of Applied Econometrics data archive : http://qed.econ.queensu.ca/jae/.\n\nThe variable of interest is lfp: whether the individual participates in the labour force or not. To know which variables are relevant in the decision to participate in the labour force, one could estimate a logit model, using glm().\n\nlogit_participation = glm(lfp ~ ., data = Participation, family = \"binomial\")\n\nNow that we ran the regression, we can take a look at the results. I like to use broom::tidy() to look at the results of regressions, as tidy() returns a nice data.frame, but you could use summary() if you’re only interested in reading the output:\n\ntidy(logit_participation)\n##          term    estimate  std.error  statistic      p.value\n## 1 (Intercept) 10.37434616 2.16685216  4.7877499 1.686617e-06\n## 2     lnnlinc -0.81504064 0.20550116 -3.9661122 7.305449e-05\n## 3         age -0.51032975 0.09051783 -5.6378920 1.721444e-08\n## 4        educ  0.03172803 0.02903580  1.0927211 2.745163e-01\n## 5         nyc -1.33072362 0.18017027 -7.3859224 1.514000e-13\n## 6         noc -0.02198573 0.07376636 -0.2980454 7.656685e-01\n## 7  foreignyes  1.31040497 0.19975784  6.5599678 5.381941e-11\n\nFrom the results above, one can only interpret the sign of the coefficients. To know how much a variable influences the labour force participation, one has to use margins():\n\neffects_logit_participation = margins(logit_participation) \n\nprint(effects_logit_participation)\n## Average marginal effects\n## glm(formula = lfp ~ ., family = \"binomial\", data = Participation)\n##  lnnlinc     age     educ     nyc       noc foreignyes\n##  -0.1699 -0.1064 0.006616 -0.2775 -0.004584     0.2834\n\nUsing summary() on the object returned by margins() provides more details:\n\nsummary(effects_logit_participation)\n##      factor     AME     SE       z      p   lower   upper\n##         age -0.1064 0.0176 -6.0494 0.0000 -0.1409 -0.0719\n##        educ  0.0066 0.0060  1.0955 0.2733 -0.0052  0.0185\n##  foreignyes  0.2834 0.0399  7.1102 0.0000  0.2053  0.3615\n##     lnnlinc -0.1699 0.0415 -4.0994 0.0000 -0.2512 -0.0887\n##         noc -0.0046 0.0154 -0.2981 0.7656 -0.0347  0.0256\n##         nyc -0.2775 0.0333 -8.3433 0.0000 -0.3426 -0.2123\n\nAnd it is also possible to plot the effects with base graphics:\n\nplot(effects_logit_participation)\n\n\n\n\nThis uses the basic R plotting capabilities, which is useful because it is a simple call to the function plot() but if you’ve been using ggplot2 and want this graph to have the same look as the others made with ggplot2 you first need to save the summary in a variable. Let’s overwrite this effects_logit_participation variable with its summary:\n\neffects_logit_participation = summary(effects_logit_participation)\n\nAnd now it is possible to use ggplot2 to create the same plot:\n\nggplot(data = effects_logit_participation) +\n  geom_point(aes(factor, AME)) +\n  geom_errorbar(aes(x = factor, ymin = lower, ymax = upper)) +\n  geom_hline(yintercept = 0) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45))\n\n\n\n\nSo an infinitesimal increase, in say, non-labour income (lnnlinc) of 0.001 is associated with a decrease of the probability of labour force participation by 0.001*17 percentage points.\n\n\nYou can also extract the marginal effects of a single variable, with dydx():\n\nhead(dydx(Participation, logit_participation, \"lnnlinc\"))\n##   dydx_lnnlinc\n## 1  -0.15667764\n## 2  -0.20014487\n## 3  -0.18495109\n## 4  -0.05377262\n## 5  -0.18710476\n## 6  -0.19586986\n\nWhich makes it possible to extract the effects for a list of individuals that you can create yourself:\n\nmy_subjects = tribble(\n    ~lfp,  ~lnnlinc, ~age, ~educ, ~nyc, ~noc, ~foreign,\n    \"yes\",   10.780,  7.0,     4,    1,    1,    \"yes\",\n     \"no\",     1.30,  9.0,     1,    4,    1,    \"yes\"\n)\n\ndydx(my_subjects, logit_participation, \"lnnlinc\")\n##   dydx_lnnlinc\n## 1  -0.09228119\n## 2  -0.17953451\n\nI used the tribble() function from the tibble package to create this test data set, row by row. Then, using dydx(), I get the marginal effect of variable lnnlinc for these two individuals. No doubt that this package will be a huge help convincing more social scientists to try out R and make a potential transition from STATA easier."
  },
  {
    "objectID": "posts/2016-07-26-read-a-lot-of-datasets-at-once-with-r.html",
    "href": "posts/2016-07-26-read-a-lot-of-datasets-at-once-with-r.html",
    "title": "Read a lot of datasets at once with R",
    "section": "",
    "text": "I often have to read a lot of datasets at once using R. So I’ve wrote the following function to solve this issue:\n\nread_list &lt;- function(list_of_datasets, read_func){\n\n        read_and_assign &lt;- function(dataset, read_func){\n                dataset_name &lt;- as.name(dataset)\n                dataset_name &lt;- read_func(dataset)\n        }\n\n        # invisible is used to suppress the unneeded output\n        output &lt;- invisible(\n                sapply(list_of_datasets,\n                           read_and_assign, read_func = read_func, simplify = FALSE, USE.NAMES = TRUE))\n\n        # Remove the extension at the end of the data set names\n        names_of_datasets &lt;- c(unlist(strsplit(list_of_datasets, \"[.]\"))[c(T, F)])\n        names(output) &lt;- names_of_datasets\n        return(output)\n}\n\nYou need to supply a list of datasets as well as the function to read the datasets to read_list. So for example to read in .csv files, you could use read.csv() (or read_csv() from the readr package, which I prefer to use), or read_dta() from the package haven for STATA files, and so on.\n\n\nNow imagine you have some data in your working directory. First start by saving the name of the datasets in a variable:\n\ndata_files &lt;- list.files(pattern = \".csv\")\n\nprint(data_files)\n## [1] \"data_1.csv\" \"data_2.csv\" \"data_3.csv\"\n\nNow you can read all the data sets and save them in a list with read_list():\n\nlibrary(\"readr\")\nlibrary(\"tibble\")\n\nlist_of_data_sets &lt;- read_list(data_files, read_csv)\n\n\nglimpse(list_of_data_sets)\n## List of 3\n##  $ data_1:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,018930679\" \"0,8748013128\" \"0,1025635934\" \"0,6246140983\" ...\n##   ..$ col2: chr [1:19] \"0,0377725807\" \"0,5959457638\" \"0,4429121533\" \"0,558387159\" ...\n##   ..$ col3: chr [1:19] \"0,6241767189\" \"0,031324594\" \"0,2238059868\" \"0,2773350732\" ...\n##  $ data_2:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9098418493\" \"0,1127788509\" \"0,5818891392\" \"0,1011773532\" ...\n##   ..$ col2: chr [1:19] \"0,7455905887\" \"0,4015039612\" \"0,6625796605\" \"0,029955339\" ...\n##   ..$ col3: chr [1:19] \"0,327232932\" \"0,2784035673\" \"0,8092386735\" \"0,1216045306\" ...\n##  $ data_3:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9236124896\" \"0,6303271761\" \"0,6413583054\" \"0,5573887416\" ...\n##   ..$ col2: chr [1:19] \"0,2114708388\" \"0,6984538266\" \"0,0469865249\" \"0,9271510226\" ...\n##   ..$ col3: chr [1:19] \"0,4941919971\" \"0,7391538511\" \"0,3876723797\" \"0,2815014394\" ...\n\nIf you prefer not to have the datasets in a list, but rather import them into the global environment, you can change the above function like so:\n\nread_list &lt;- function(list_of_datasets, read_func){\n\n        read_and_assign &lt;- function(dataset, read_func){\n                assign(dataset, read_func(dataset), envir = .GlobalEnv)\n        }\n\n        # invisible is used to suppress the unneeded output\n        output &lt;- invisible(\n                sapply(list_of_datasets,\n                           read_and_assign, read_func = read_func, simplify = FALSE, USE.NAMES = TRUE))\n\n}\n\nBut I personnally don’t like this second option, but I put it here for completeness."
  },
  {
    "objectID": "posts/2016-12-21-work-on-lists-of-datasets-instead-of-individual-datasets-by-using-functional-programming.html",
    "href": "posts/2016-12-21-work-on-lists-of-datasets-instead-of-individual-datasets-by-using-functional-programming.html",
    "title": "Work on lists of datasets instead of individual datasets by using functional programming",
    "section": "",
    "text": "Analyzing a lot of datasets can be tedious. In my work, I often have to compute descriptive statistics, or plot some graphs for some variables for a lot of datasets. The variables in question have the same name accross the datasets but are measured for different years. As an example, imagine you have this situation:\n\n\ndata2000 &lt;- mtcars\ndata2001 &lt;- mtcars\n\n\nFor the sake of argument, imagine that data2000 is data from a survey conducted in the year 2000 and data2001 is the same survey but conducted in the year 2001. For illustration purposes, I use the mtcars dataset, but I could have used any other example. In these sort of situations, the variables are named the same in both datasets. Now if I want to check the summary statistics of a variable, I might do it by running:\n\n\nsummary(data2000$cyl)\nsummary(data2001$cyl)\n\n\nbut this can get quite tedious, especially if instead of only having two years of data, you have 20 years. Another possibility is to merge both datasets and then check the summary statistics of the variable of interest. But this might require a lot of preprocessing, and sometimes you really just want to do a quick check, or some dirty graphs. So you might be tempted to write a loop, which would require to put these two datasets in some kind of structure, such as a list:\n\n\nlist_data &lt;- list(\"data2000\" = data2000, \"data2001\" = data2001)\n\nfor (i in 1:2){\n    print(summary(list_data[[i]]$cyl))\n}\n\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000 \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000\n\nBut this also might get tedious, especially if you want to do this for a lot of different variables, and want to use different functions than summary().\n\n\nAnother, simpler way of doing this, is to use purrr::map() or lapply(). But there is a catch though: how do we specify the column we want to work on? Let’s try some things out:\n\n\nlibrary(purrr)\n\nmap(list_data, summary(cyl))\n\nError in summary(cyl) : object 'cyl' not found\n\nMaybe this will work:\n\n\nmap(list_data, summary, cyl)\n\n## $data2000\n     mpg             cyl             disp             hp       \nMin.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \nMedian :19.20   Median :6.000   Median :196.3   Median :123.0  \nMean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \nMax.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n     drat             wt             qsec             vs        \nMin.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \nMedian :3.695   Median :3.325   Median :17.71   Median :0.0000  \nMean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \nMax.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n      am              gear            carb      \nMin.   :0.0000   Min.   :3.000   Min.   :1.000  \n1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \nMedian :0.0000   Median :4.000   Median :2.000  \nMean   :0.4062   Mean   :3.688   Mean   :2.812  \n3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \nMax.   :1.0000   Max.   :5.000   Max.   :8.000  \n\ndata2001\n     mpg             cyl             disp             hp       \nMin.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \nMedian :19.20   Median :6.000   Median :196.3   Median :123.0  \nMean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \nMax.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n     drat             wt             qsec             vs        \nMin.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \nMedian :3.695   Median :3.325   Median :17.71   Median :0.0000  \nMean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \nMax.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n      am              gear            carb      \nMin.   :0.0000   Min.   :3.000   Min.   :1.000  \n1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \nMedian :0.0000   Median :4.000   Median :2.000  \nMean   :0.4062   Mean   :3.688   Mean   :2.812  \n3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \nMax.   :1.0000   Max.   :5.000   Max.   :8.000\n\nNot quite! You get the summary statistics of every variable, cyl simply gets ignored. This might be ok in our small toy example, but if you have dozens of datasets with hundreds of variables, the output becomes unreadable. The solution is to use an anonymous functions:\n\n\nmap(list_data, (function(x) summary(x$cyl)))\n\n## $data2000\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000 \n\n$data2001\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000\n\nThis is, in my opinion, much more readable than a loop, and the output of this is another list, so it’s easy to save it:\n\n\nsummary_cyl &lt;- map(list_data, (function(x) summary(x$cyl)))\nstr(summary_cyl)\n\n## List of 2\n$ data2000:Classes 'summaryDefault', 'table'  Named num [1:6] 4 4 6 6.19 8 ...\n .. ..- attr(*, \"names\")= chr [1:6] \"Min.\" \"1st Qu.\" \"Median\" \"Mean\" ...\n$ data2001:Classes 'summaryDefault', 'table'  Named num [1:6] 4 4 6 6.19 8 ...\n .. ..- attr(*, \"names\")= chr [1:6] \"Min.\" \"1st Qu.\" \"Median\" \"Mean\" ...\n\nWith the loop, you would need to “allocate” an empty list that you would fill at each iteration.\n\n\nSo this is already nice, but wouldn’t it be nicer to simply have to type:\n\n\nsummary(list_data$cyl)\n\n\nand have the summary of variable cyl for each dataset in the list? Well it is possible with the following function I wrote to make my life easier:\n\n\nto_map &lt;- function(func){\n  function(list, column, ...){\n    if(missing(column)){\n        res &lt;- purrr::map(list, (function(x) func(x, ...)))\n      } else {\n        res &lt;- purrr::map(list, (function(x) func(x[column], ...)))\n             }\n    res\n  }\n}\n\n\nBy following this chapter of Hadley Wickham’s book, Advanced R, I was able to write this function. What does it do? It basically generalizes a function to work on a list of datasets instead of just on a dataset. So for example, in the case of summary():\n\n\nsummarymap &lt;- to_map(summary)\n\nsummarymap(list_data, \"cyl\")\n\n$data2000\n     cyl       \nMin.   :4.000  \n1st Qu.:4.000  \nMedian :6.000  \nMean   :6.188  \n3rd Qu.:8.000  \nMax.   :8.000  \n\n$data2001\n     cyl       \nMin.   :4.000  \n1st Qu.:4.000  \nMedian :6.000  \nMean   :6.188  \n3rd Qu.:8.000  \nMax.   :8.000\n\nSo now everytime I want to have summary statistics for a variable, I just need to use summarymap():\n\n\nsummarymap(list_data, \"mpg\")\n\n## $data2000\n      mpg       \n Min.   :10.40  \n 1st Qu.:15.43  \n Median :19.20  \n Mean   :20.09  \n 3rd Qu.:22.80  \n Max.   :33.90  \n\n$data2001\n      mpg       \n Min.   :10.40  \n 1st Qu.:15.43  \n Median :19.20  \n Mean   :20.09  \n 3rd Qu.:22.80  \n Max.   :33.90\n\nIf I want the summary statistics for every variable, I simply omit the column name:\n\n\nsummarymap(list_data)\n\n$data2000\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n$data2001\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000\n\nI can use any function:\n\n\ntablemap &lt;- to_map(table)\n\ntablemap(list_data, \"cyl\")\n\n## $data2000\n\n 4  6  8 \n11  7 14 \n\n$data2001\n\n 4  6  8 \n11  7 14\n\ntablemap(list_data, \"mpg\")\n\n## $data2000\n\n10.4 13.3 14.3 14.7   15 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7 19.2 19.7 \n   2    1    1    1    1    2    1    1    1    1    1    1    1    2    1 \n  21 21.4 21.5 22.8 24.4   26 27.3 30.4 32.4 33.9 \n   2    2    1    2    1    1    1    2    1    1 \n\n$data2001\n\n10.4 13.3 14.3 14.7   15 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7 19.2 19.7 \n   2    1    1    1    1    2    1    1    1    1    1    1    1    2    1 \n  21 21.4 21.5 22.8 24.4   26 27.3 30.4 32.4 33.9 \n   2    2    1    2    1    1    1    2    1    1\n\nI hope you will find this little function useful, and as usual, for any comments just drop me an email by clicking the red enveloppe in the top right corner or tweet me."
  },
  {
    "objectID": "posts/2016-11-04-ive-started-writing-a-book-functional-programming-and-unit-testing-for-data-munging-with-r.html",
    "href": "posts/2016-11-04-ive-started-writing-a-book-functional-programming-and-unit-testing-for-data-munging-with-r.html",
    "title": "I’ve started writing a ‘book’: Functional programming and unit testing for data munging with R",
    "section": "",
    "text": "I have started writing a ‘book’ using the awesome bookdown package. In the book I explain and show why using functional programming and putting your functions in your own packages is the way to go when you want to clean, prepare and transform large data sets. It makes testing and documenting your code easier. You don’t need to think about managing paths either. The book is far from complete, but I plan on working on it steadily. For now, you can read an intro to functional programming, unit testing and creating your own packages that will hold your code. I also show you can write documentation for your functions. I am also looking for feedback; so if you have any suggestions, do not hesitate to shoot me an email or a tweet! You can read the book by clicking here."
  },
  {
    "objectID": "posts/2017-01-07-my-free-book-has-a-cover.html",
    "href": "posts/2017-01-07-my-free-book-has-a-cover.html",
    "title": "My free book has a cover!",
    "section": "",
    "text": "I’m currently writing a book as a hobby. It’s titled Functional programming and unit testing for data munging with R and you can get it for free here. You can also read it online for free on my webpage What’s the book about?\nHere’s the teaser text:\n\nLearn the basics of functional programming, unit testing and package development for the R programming language in order to make your data tidy!\n\nThe book now has a beautiful cover thanks to @putosaure. Putosaure is a Paris based graphic designer who also reviews video games. He is also a very good friend of mine and I am very happy he made this beautiful cover for my book:\n\n\n\nIn it, we see a guy holding a shield with the Greek letter lambda, which also happens to be the letter to designate functional programming. I’ve added the title with the Komika Title font.\nConsider this cover in beta, it’ll probably evolve some more. But I couldn’t wait to use it!\nI love it. Hope you’ll love it too!"
  },
  {
    "objectID": "posts/2017-12-27-build_formulae.html",
    "href": "posts/2017-12-27-build_formulae.html",
    "title": "Building formulae",
    "section": "",
    "text": "This Stackoverflow question made me think about how to build formulae. For example, you might want to programmatically build linear model formulae and then map these models on data. For example, suppose the following (output suppressed):\n\ndata(mtcars)\n\nlm(mpg ~ hp, data = mtcars)\nlm(mpg ~I(hp^2), data = mtcars)\nlm(mpg ~I(hp^3), data = mtcars)\nlm(mpg ~I(hp^4), data = mtcars)\nlm(mpg ~I(hp^5), data = mtcars)\nlm(mpg ~I(hp^6), data = mtcars)\n\nTo avoid doing this, one can write a function that builds the formulae:\n\ncreate_form = function(power){\n  rhs = substitute(I(hp^pow), list(pow=power))\n  rlang::new_formula(quote(mpg), rhs)\n}\n\nIf you are not familiar with substitute(), try the following to understand what it does:\n\nsubstitute(y ~ x, list(x = 1))\n## y ~ 1\n\nThen using rlang::new_formula() I build a formula by providing the left hand side, which is quote(mpg) here, and the right hand side, which I built using substitute(). Now I can create a list of formulae:\n\nlibrary(tidyverse)\n\nlist_formulae = map(seq(1, 6), create_form)\n\nstr(list_formulae)\n## List of 6\n##  $ :Class 'formula'  language mpg ~ I(hp^1L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605f897ca0&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^2L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605f891418&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^3L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605da76098&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^4L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605da6a600&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^5L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605da68980&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^6L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605da66d38&gt;\n\nAs you can see, power got replaced by 1, 2, 3,… and each element of the list is a nice formula. Exactly what lm() needs. So now it’s easy to map lm() to this list of formulae:\n\ndata(mtcars)\n\nmap(list_formulae, lm, data = mtcars)\n## [[1]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^1)  \n##    30.09886     -0.06823  \n## \n## \n## [[2]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^2)  \n##  24.3887252   -0.0001649  \n## \n## \n## [[3]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^3)  \n##   2.242e+01   -4.312e-07  \n## \n## \n## [[4]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^4)  \n##   2.147e+01   -1.106e-09  \n## \n## \n## [[5]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^5)  \n##   2.098e+01   -2.801e-12  \n## \n## \n## [[6]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^6)  \n##   2.070e+01   -7.139e-15\n\nThis is still a new topic for me there might be more elegant ways to do that, using tidyeval to remove the hardcoding of the columns in create_form(). I might continue exploring this."
  },
  {
    "objectID": "posts/2014-11-11-benchmarks-r-blas-atlas-rro.html",
    "href": "posts/2014-11-11-benchmarks-r-blas-atlas-rro.html",
    "title": "R, R with Atlas, R with OpenBLAS and Revolution R Open: which is fastest?",
    "section": "",
    "text": "In this short post, I benchmark different “versions” of R. I compare the execution speeds of R, R linked against OpenBLAS, R linked against ATLAS and Revolution R Open. Revolution R Open is a new open source version of R made by Revolution Analytics. It is linked against MKL and should offer huge speed improvements over vanilla R. Also, it uses every cores of your computer by default, without any change whatsoever to your code.\n\n\nTL;DR: Revolution R Open is the fastest of all the benchmarked versions (with R linked against OpenBLAS and ATLAS just behind), and easier to setup.\n\n\nSetup\n\n\nI benchmarked these different versions of R using R-benchmark-25.R that you can download here. This benchmark file was created by Simon Urbanek.\n\n\nI ran the benchmarks on my OpenSUSE 13.2 computer with a Pentium Dual-Core CPU E6500@2.93GHz with 4GB of Ram. It's outdated, but it's still quite fast for most of my numerical computation needs. I installed “vanilla” R from the official OpenSUSE repositories which is currently at version 3.1.2.\n\n\nThen, I downloaded OpenBLAS and ATLAS also from the official OpenSUSE repositories and made R use these libraries instead of its own implementation of BLAS. The way I did that is a bit hacky, but works: first, go to /usr/lib64/R/lib and backup libRblas.so (rename it to libRblas.soBackup for instance). Then link /usr/lib64/libopenblas.so.0 to /usr/lib64/R/lib/libRblas, and that's it, R will use OpenBLAS. For ATLAS, you can do it in the same fashion, but you'll find the library in /usr/lib64/atlas/. These paths should be the same for any GNU/Linux distribution. For other operating systems, I'm sure you can find where these libraries are with Google.\n\n\nThe last version I benchmarked was Revolution R Open. This is a new version of R released by Revolution Analytics. Revolution Analytics had their own version of R, called Revolution R, for quite some time now. They decided to release a completely free as in freedom and free as in free beer version of this product which they now renamed Revolution R Open. You can download Revolution R Open here. You can have both “vanilla” R and Revolution R Open installed on your system.\n\n\nResults\n\n\nI ran the R-benchmark-25.R 6 times for every version but will only discuss the 4 best runs.\n\n\n\n\n\nR version\n\n\nFastest run\n\n\nSlowest run\n\n\nMean Run\n\n\n\n\nVanilla R\n\n\n63.65\n\n\n66.21\n\n\n64.61\n\n\n\n\nOpenBLAS R\n\n\n15.63\n\n\n18.96\n\n\n16.94\n\n\n\n\nATLAS R\n\n\n16.92\n\n\n21.57\n\n\n18.24\n\n\n\n\nRRO\n\n\n14.96\n\n\n16.08\n\n\n15.49\n\n\n\n\nAs you can read from the table above, Revolution R Open was the fastest of the four versions, but not significantly faster than BLAS or ATLAS R. However, RRO uses all the available cores by default, so if your code relies on a lot matrix algebra, RRO might be actually a lot more faster than OpenBLAS and ATLAS R. Another advantage of RRO is that it is very easy to install, and also works with Rstudio and is compatible with every R package to existence. “Vanilla” R is much slower than the other three versions, more than 3 times as slow!\n\n\nConclusion\n\n\nWith other benchmarks, you could get other results, but I don’t think that “vanilla” R could beat any of the other three versions. Whatever your choice, I recommend not using plain, “vanilla” R. The other options are much faster than standard R, and don't require much work to set up. I'd personally recommend Revolution R Open, as it is free software and compatible with CRAN packages and Rstudio."
  },
  {
    "objectID": "posts/2016-07-30-merge-a-list-of-datasets-together.html",
    "href": "posts/2016-07-30-merge-a-list-of-datasets-together.html",
    "title": "Merge a list of datasets together",
    "section": "",
    "text": "Last week I showed how to read a lot of datasets at once with R, and this week I’ll continue from there and show a very simple function that uses this list of read datasets and merges them all together.\n\n\nFirst we’ll use read_list() to read all the datasets at once (for more details read last week’s post):\n\nlibrary(\"readr\")\nlibrary(\"tibble\")\n\ndata_files &lt;- list.files(pattern = \".csv\")\n\nprint(data_files)\n## [1] \"data_1.csv\" \"data_2.csv\" \"data_3.csv\"\nlist_of_data_sets &lt;- read_list(data_files, read_csv)\n\nglimpse(list_of_data_sets)\n## List of 3\n##  $ data_1:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,018930679\" \"0,8748013128\" \"0,1025635934\" \"0,6246140983\" ...\n##   ..$ col2: chr [1:19] \"0,0377725807\" \"0,5959457638\" \"0,4429121533\" \"0,558387159\" ...\n##   ..$ col3: chr [1:19] \"0,6241767189\" \"0,031324594\" \"0,2238059868\" \"0,2773350732\" ...\n##  $ data_2:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9098418493\" \"0,1127788509\" \"0,5818891392\" \"0,1011773532\" ...\n##   ..$ col2: chr [1:19] \"0,7455905887\" \"0,4015039612\" \"0,6625796605\" \"0,029955339\" ...\n##   ..$ col3: chr [1:19] \"0,327232932\" \"0,2784035673\" \"0,8092386735\" \"0,1216045306\" ...\n##  $ data_3:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9236124896\" \"0,6303271761\" \"0,6413583054\" \"0,5573887416\" ...\n##   ..$ col2: chr [1:19] \"0,2114708388\" \"0,6984538266\" \"0,0469865249\" \"0,9271510226\" ...\n##   ..$ col3: chr [1:19] \"0,4941919971\" \"0,7391538511\" \"0,3876723797\" \"0,2815014394\" ...\n\nYou see that all these datasets have the same column names. We can now merge them using this simple function:\n\nmulti_join &lt;- function(list_of_loaded_data, join_func, ...){\n\n    require(\"dplyr\")\n\n    output &lt;- Reduce(function(x, y) {join_func(x, y, ...)}, list_of_loaded_data)\n\n    return(output)\n}\n\nThis function uses Reduce(). Reduce() is a very important function that can be found in all functional programming languages. What does Reduce() do? Let’s take a look at the following example:\n\nReduce(`+`, c(1, 2, 3, 4, 5))\n## [1] 15\n\nReduce() has several arguments, but you need to specify at least two: a function, here + and a list, here c(1, 2, 3, 4, 5). The next code block shows what Reduce() basically does:\n\n0 + c(1, 2, 3, 4, 5)\n0 + 1 + c(2, 3, 4, 5)\n0 + 1 + 2 + c(3, 4, 5)\n0 + 1 + 2 + 3 + c(4, 5)\n0 + 1 + 2 + 3 + 4 + c(5)\n0 + 1 + 2 + 3 + 4 + 5\n\n0 had to be added as in “init”. You can also specify this “init” to Reduce():\n\nReduce(`+`, c(1, 2, 3, 4, 5), init = 20)\n## [1] 35\n\nSo what multi_join() does, is the same operation as in the example above, but where the function is a user supplied join or merge function, and the list of datasets is the one read with read_list().\n\n\nLet’s see what happens when we use multi_join() on our list:\n\nmerged_data &lt;- multi_join(list_of_data_sets, full_join)\nclass(merged_data)\n## [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\nglimpse(merged_data)\n## Observations: 57\n## Variables: 3\n## $ col1 &lt;chr&gt; \"0,018930679\", \"0,8748013128\", \"0,1025635934\", \"0,6246140...\n## $ col2 &lt;chr&gt; \"0,0377725807\", \"0,5959457638\", \"0,4429121533\", \"0,558387...\n## $ col3 &lt;chr&gt; \"0,6241767189\", \"0,031324594\", \"0,2238059868\", \"0,2773350...\n\nYou should make sure that all the data frames have the same column names but you can also join data frames with different column names if you give the argument by to the join function. This is possible thanks to … that allows you to pass further argument to join_func().\n\n\nThis function was inspired by the one found on the blog Coffee and Econometrics in the Morning."
  },
  {
    "objectID": "posts/2018-01-03-lists_all_the_way.html",
    "href": "posts/2018-01-03-lists_all_the_way.html",
    "title": "It’s lists all the way down",
    "section": "",
    "text": "There’s a part 2 to this post: read it here.\n\n\nToday, I had the opportunity to help someone over at the R for Data Science Slack group (read more about this group here) and I thought that the question asked could make for an interesting blog post, so here it is!\n\n\nDisclaimer: the way I’m doing things here is totally not optimal, but I want to illustrate how to map functions over nested lists. But I show the optimal way at the end, so for the people that are familiar with purrr don’t get mad at me.\n\n\nSuppose you have to do certain data transformation tasks on a data frame, and you write a nice function that does that for you:\n\nlibrary(tidyverse)\ndata(mtcars)\n\nnice_function = function(df, param1, param2){\n  df = df %&gt;%\n    filter(cyl == param1, am == param2) %&gt;%\n    mutate(result = mpg * param1 * (2 - param2))\n\n  return(df)\n}\n\nnice_function(mtcars, 4, 0)\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\n\nThis might seem like a silly function and not a nice function, but it will illustrate the point I want to make (and the question that was asked) very well. This function is completely useless, but bear with me. Now, suppose that you want to do these operations for each value of cyl and am (of course you can do that without using nice_function()…). First, you might want to fix the value of am to 0, and then loop over the values of cyl. But as I have explained in this other blog post I prefer using the map() functions included in purrr. For example:\n\nvalues_cyl = c(4, 6, 8)\n\n(result = map(values_cyl, nice_function, df = mtcars, param2 = 0))\n## [[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\n## \n## [[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## \n## [[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n\nWhat you get here is a list for each value in values_cyl; so one list for 4, one for 6 and one for 8. Suppose now that you are feeling adventurous, and want to loop over the values of am too:\n\nvalues_am = c(0, 1)\n\nSo first, we need to map a function to each element of values_am. But which function? Well, for given value of am, our problem is the same as before; we need to map nice_function() to each value of cyl. So, that’s what we’re going to do:\n\n(result = map(values_am, ~map(values_cyl, nice_function, df = mtcars, param2 = .)))\n## [[1]]\n## [[1]][[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\n## \n## [[1]][[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## \n## [[1]][[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## \n## \n## [[2]]\n## [[2]][[1]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## \n## [[2]][[2]]\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2\n## \n## [[2]][[3]]\n##    mpg cyl disp  hp drat   wt qsec vs am gear carb result\n## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4\n## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0\n\nWe now have a list of size 2 (for each value of am) where each element is itself a list of size 3 (for each value of cyl) where each element is a data frame. Are you still with me? Also, notice that the second map is given as a formula (notice the ~ in front of the second map). This creates an anonymous function, where the parameter is given by the . (think of the . as being the x in f(x)). So the . is the stand-in for the values contained inside values_am.\n\n\nThe people that are familiar with the map() functions must be fuming right now; there is a way to avoid this nested hell. I will talk about it soon, but first I want to play around with this list of lists.\n\n\nIf you have a list of data frames, you can bind their rows together with reduce(list_of_dfs, rbind). You would like to this here, but because your lists of data frames are contained inside another list… you guessed it, you have to map over it!\n\n(result2 = map(result, ~reduce(., rbind)))\n## [[1]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## \n## [[2]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 2  32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 3  30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 4  33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 5  27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 6  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 7  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 8  21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 9  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 10 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 11 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 12 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 13 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nHere again, I pass reduce() as a formula to map() to create an anonymous function. Again, the . is used as the stand-in for each element contained in result; a list of data frames, where reduce(., rbind) knows what to do. Now that we have this we can use reduce() with rbind() again to get a single data frame:\n\n(result3 = reduce(result2, rbind))\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## 20 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 21 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 22 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 23 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 24 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 25 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 26 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 27 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 28 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 29 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 31 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 32 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nOf course, since reduce(list_of_dfs, rbind) is such a common operation, you could have simply used dplyr::bind_rows, which does exactly this:\n\n(result2 = map(result, bind_rows))\n## [[1]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## \n## [[2]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 2  32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 3  30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 4  33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 5  27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 6  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 7  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 8  21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 9  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 10 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 11 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 12 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 13 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nand then:\n\n(result3 = bind_rows(result2))\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## 20 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 21 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 22 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 23 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 24 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 25 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 26 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 27 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 28 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 29 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 31 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 32 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nOf course, things are even simpler: you can avoid this deeply nested monstrosity by using map_df() instead of map()! map_df() works just like map() but return a data frame (hence the _df in the name) instead of a list:\n\n(result_df = map_df(values_am, ~map_df(values_cyl, nice_function, df = mtcars, param2 = .)))\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## 20 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 21 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 22 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 23 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 24 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 25 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 26 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 27 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 28 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 29 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 31 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 32 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nIf you look at the source code of map_df() you see that dplyr::bind_rows gets called at the end:\n\nmap_df\n## function (.x, .f, ..., .id = NULL) \n## {\n##     if (!is_installed(\"dplyr\")) {\n##         abort(\"`map_df()` requires dplyr\")\n##     }\n##     .f &lt;- as_mapper(.f, ...)\n##     res &lt;- map(.x, .f, ...)\n##     dplyr::bind_rows(res, .id = .id)\n## }\n## &lt;bytecode: 0x55dad486e6a0&gt;\n## &lt;environment: namespace:purrr&gt;\n\nSo moral of the story? There are a lot of variants of the common purrr::map() functions (as well as of dplyr verbs, such as filter_at, select_if, etc…) and learning about them can save you from a lot of pain! However, if you need to apply a function to nested lists this is still possible; you just have to think about the structure of the nested list for a bit. There is also another function that you might want to study, modify_depth() which solves related issues but I will end the blog post here. I might talk about it in a future blog post.\n\n\nAlso, if you want to learn more about R and the tidyverse, do read the link I posted in the introduction of the post and join the R4ds slack group! There are a lot of very nice people there that want to help you get better with your R-fu. Also, this is where I got the inspiration to write this blog post and I am thankful to the people there for the discussions; I feel comfortable with R, but I still learn new tips and tricks every day!\n\n\nIf you enjoy these blog posts, you can follow me on twitter. And happy new yeaR!"
  },
  {
    "objectID": "posts/2018-01-19-mapping_functions_with_any_cols.html",
    "href": "posts/2018-01-19-mapping_functions_with_any_cols.html",
    "title": "Mapping a list of functions to a list of datasets with a list of columns as arguments",
    "section": "",
    "text": "This week I had the opportunity to teach R at my workplace, again. This course was the “advanced R” course, and unlike the one I taught at the end of last year, I had one more day (so 3 days in total) where I could show my colleagues the joys of the tidyverse and R.\n\n\nTo finish the section on programming with R, which was the very last section of the whole 3 day course I wanted to blow their minds; I had already shown them packages from the tidyverse in the previous days, such as dplyr, purrr and stringr, among others. I taught them how to use ggplot2, broom and modelr. They also liked janitor and rio very much. I noticed that it took them a bit more time and effort for them to digest purrr::map() and purrr::reduce(), but they all seemed to see how powerful these functions were. To finish on a very high note, I showed them the ultimate purrr::map() use case.\n\n\nConsider the following; imagine you have a situation where you are working on a list of datasets. These datasets might be the same, but for different years, or for different countries, or they might be completely different datasets entirely. If you used rio::import_list() to read them into R, you will have them in a nice list. Let’s consider the following list as an example:\n\nlibrary(tidyverse)\ndata(mtcars)\ndata(iris)\n\ndata_list = list(mtcars, iris)\n\nI made the choice to have completely different datasets. Now, I would like to map some functions to the columns of these datasets. If I only worked on one, for example on mtcars, I would do something like:\n\nmy_summarise_f = function(dataset, cols, funcs){\n  dataset %&gt;%\n    summarise_at(vars(!!!cols), funs(!!!funcs))\n}\n\nAnd then I would use my function like so:\n\nmtcars %&gt;%\n  my_summarise_f(quos(mpg, drat, hp), quos(mean, sd, max))\n##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max\n## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93\n##   hp_max\n## 1    335\n\nmy_summarise_f() takes a dataset, a list of columns and a list of functions as arguments and uses tidy evaluation to apply mean(), sd(), and max() to the columns mpg, drat and hp of mtcars. That’s pretty useful, but not useful enough! Now I want to apply this to the list of datasets I defined above. For this, let’s define the list of columns I want to work on:\n\ncols_mtcars = quos(mpg, drat, hp)\ncols_iris = quos(Sepal.Length, Sepal.Width)\n\ncols_list = list(cols_mtcars, cols_iris)\n\nNow, let’s use some purrr magic to apply the functions I want to the columns I have defined in list_cols:\n\nmap2(data_list,\n     cols_list,\n     my_summarise_f, funcs = quos(mean, sd, max))\n## [[1]]\n##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max\n## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93\n##   hp_max\n## 1    335\n## \n## [[2]]\n##   Sepal.Length_mean Sepal.Width_mean Sepal.Length_sd Sepal.Width_sd\n## 1          5.843333         3.057333       0.8280661      0.4358663\n##   Sepal.Length_max Sepal.Width_max\n## 1              7.9             4.4\n\nThat’s pretty useful, but not useful enough! I want to also use different functions to different datasets!\n\n\nWell, let’s define a list of functions then:\n\nfuncs_mtcars = quos(mean, sd, max)\nfuncs_iris = quos(median, min)\n\nfuncs_list = list(funcs_mtcars, funcs_iris)\n\nBecause there is no map3(), we need to use pmap():\n\npmap(\n  list(\n    dataset = data_list,\n    cols = cols_list,\n    funcs = funcs_list\n  ),\n  my_summarise_f)\n## [[1]]\n##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max\n## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93\n##   hp_max\n## 1    335\n## \n## [[2]]\n##   Sepal.Length_median Sepal.Width_median Sepal.Length_min Sepal.Width_min\n## 1                 5.8                  3              4.3               2\n\nNow I’m satisfied! Let me tell you, this blew their minds 😄!\n\n\nTo be able to use things like that, I told them to always solve a problem for a single example, and from there, try to generalize their solution using functional programming tools found in purrr.\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Bruno Rodrigues and hold a PhD in Economics from the University of Strasbourg.\n\n\n\nI’m currently employed as a statistician for the Ministry of Research and Higher Education in Luxembourg. Before that I was senior data scientist and then manager in the data science team at PwC Luxembourg, and before that I was a research assistant at STATEC Research.\nMy hobbies are boxing, lifting weights, cycling, cooking and reading or listening to audiobooks, which is more compatible with the life of a young father. I started this blog to share my enthusiasm for statistics. My blog posts are reshared on R-bloggers and RWeekly. I also enjoy learning about the R programming language and sharing my knowledge. That’s why I made this blog and write ebooks. I also have a youtube channel, where I show some tips and tricks with R, or rant about stuff."
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "Build reproducible analytical pipelines that will output consistent, high-quality analyses using the R programming language, Git, Github and Docker. Learn about functional and literate programming to keep your code concise, easier to test, easier to share and easily understandable by others.\n\n\n\n\nRead it for free: https://raps-with-r.dev/\n\n\nBuy the DRM free Epub or PDF: https://leanpub.com/raps-with-r/\n\n\nBuy a physical copy on Amazon: https://www.amazon.com/Building-reproducible-analytical-pipelines-R/dp/B0C87H6MGF\n\n\n\n\n\n\nThis book focuses on teaching you how to use R in a modern way. Why modern? Because you will learn about the tidyverse collection of packages, which enable you to write efficient, legible code. You will also learn about functional programming, some statistical methods, writing your own packages… By the end of the book, you will have all the knowledge and tools on hand to use R for your day-to-day data science tasks!\n\n\n\n\nRead it for free: https://modern-rstats.eu/\n\n\n\nBuy the DRM free Epub or PDF (very out of date, update coming Q4-2025): https://leanpub.com/modern_tidyverse/\n\n\nBuy a physical copy (coming one day)"
  },
  {
    "objectID": "books.html#building-reproducible-analytical-pipelines-with-r",
    "href": "books.html#building-reproducible-analytical-pipelines-with-r",
    "title": "Books",
    "section": "",
    "text": "Build reproducible analytical pipelines that will output consistent, high-quality analyses using the R programming language, Git, Github and Docker. Learn about functional and literate programming to keep your code concise, easier to test, easier to share and easily understandable by others.\n\n\n\n\nRead it for free: https://raps-with-r.dev/\n\n\nBuy the DRM free Epub or PDF: https://leanpub.com/raps-with-r/\n\n\nBuy a physical copy on Amazon: https://www.amazon.com/Building-reproducible-analytical-pipelines-R/dp/B0C87H6MGF\n\n\n\n\n\n\nThis book focuses on teaching you how to use R in a modern way. Why modern? Because you will learn about the tidyverse collection of packages, which enable you to write efficient, legible code. You will also learn about functional programming, some statistical methods, writing your own packages… By the end of the book, you will have all the knowledge and tools on hand to use R for your day-to-day data science tasks!\n\n\n\n\nRead it for free: https://modern-rstats.eu/\n\n\n\nBuy the DRM free Epub or PDF (very out of date, update coming Q4-2025): https://leanpub.com/modern_tidyverse/\n\n\nBuy a physical copy (coming one day)"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks, presentations, workshops…",
    "section": "",
    "text": "BUILDING REPRODUCIBLE ANALYTICAL PIPELINES WITH R, DOCKER AND NIX: online talk for RLadies Rome, on the 29th of June 2024. Repository, Slides and video.\nRix: reproducible environments with Nix: talk I gave for the online part of the 2024 edition of useR! in Salzburg. Video\nRIX: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk and tutorial I gave for the 2024 edition of the Rencontres R in Vannes, Brittany, France. Slides and tutorial. No video available.\nRix: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk I gave on the 7th of June, 2024 for the INED (Institute Nationale d’Études Démographiques, National institute for demographic studies). Repository, Slides and Video.\nReproducible research with Nix and rix, a talk I gave at the DIPF, Leibniz Institute for Research and Information in Education, in LOS! (Leibniz Open Science!), the event series of the Leibniz Strategy Forum on Open Science. The workshop was held online, on May 16, 2024. Repository, Slides and video of the talk\n“rix: An R package for reproducible dev environments with Nix (FOSDEM 2024)”, a talk I gave at FOSDEM 2024 on the 4th of February 2024 in the Nix and NixOS dev room. Video here.\n“Building reproducible analytical pipelines”, Workshop I gave for the “R User Group Tunis” on December 9th, 2023. Video here. Slides and code here.\n“Porquê usar monads (programação funcional)? Uma ilustração com o pacote {chronicler}” (Why use monads (functional programming)? An illustration with the {chronicler} package), Talk I gave (online) at the INE (Instituto Nacional de Estatística), on the 7th of June 2023. Slides in Portuguese.\n“La reproductibilité avec R, ou pourquoi celle-ci est située sur un continuum” (Reproducibility with R, or why it lies on a continuum). Talk I gave at the Rencontres R 2023 in Avignon, France, on the 26th of June 2023. Slides in French, Video with English subtitles.\n“Building reproducible analytical pipelines”, Workshop I gave for “Workshops for Ukraine” on June 29th, 2023. You can pull the Docker image with the slides and code here.\n“Building reproducible analytical pipelines”, Talk I gave for ReproducibiliTea UCL on the 19th of July, 2023. Slides and Video. You can pull the Docker image with the slides and code here."
  },
  {
    "objectID": "talks.html#talks",
    "href": "talks.html#talks",
    "title": "Talks, presentations, workshops…",
    "section": "",
    "text": "BUILDING REPRODUCIBLE ANALYTICAL PIPELINES WITH R, DOCKER AND NIX: online talk for RLadies Rome, on the 29th of June 2024. Repository, Slides and video.\nRix: reproducible environments with Nix: talk I gave for the online part of the 2024 edition of useR! in Salzburg. Video\nRIX: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk and tutorial I gave for the 2024 edition of the Rencontres R in Vannes, Brittany, France. Slides and tutorial. No video available.\nRix: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk I gave on the 7th of June, 2024 for the INED (Institute Nationale d’Études Démographiques, National institute for demographic studies). Repository, Slides and Video.\nReproducible research with Nix and rix, a talk I gave at the DIPF, Leibniz Institute for Research and Information in Education, in LOS! (Leibniz Open Science!), the event series of the Leibniz Strategy Forum on Open Science. The workshop was held online, on May 16, 2024. Repository, Slides and video of the talk\n“rix: An R package for reproducible dev environments with Nix (FOSDEM 2024)”, a talk I gave at FOSDEM 2024 on the 4th of February 2024 in the Nix and NixOS dev room. Video here.\n“Building reproducible analytical pipelines”, Workshop I gave for the “R User Group Tunis” on December 9th, 2023. Video here. Slides and code here.\n“Porquê usar monads (programação funcional)? Uma ilustração com o pacote {chronicler}” (Why use monads (functional programming)? An illustration with the {chronicler} package), Talk I gave (online) at the INE (Instituto Nacional de Estatística), on the 7th of June 2023. Slides in Portuguese.\n“La reproductibilité avec R, ou pourquoi celle-ci est située sur un continuum” (Reproducibility with R, or why it lies on a continuum). Talk I gave at the Rencontres R 2023 in Avignon, France, on the 26th of June 2023. Slides in French, Video with English subtitles.\n“Building reproducible analytical pipelines”, Workshop I gave for “Workshops for Ukraine” on June 29th, 2023. You can pull the Docker image with the slides and code here.\n“Building reproducible analytical pipelines”, Talk I gave for ReproducibiliTea UCL on the 19th of July, 2023. Slides and Video. You can pull the Docker image with the slides and code here."
  },
  {
    "objectID": "talks.html#interviews-podcasts",
    "href": "talks.html#interviews-podcasts",
    "title": "Talks, presentations, workshops…",
    "section": "Interviews, podcasts…",
    "text": "Interviews, podcasts…\n\nFrench Data Workers Podcast #1 Large Scale Testing : Contenir la covid-19 avec des dépistages ciblés\nFrench Bruno Rodrigues défend une approche basée sur la reproductibilité de la data science au Luxembourg\nEnglish Leanpub Frontmatter Podcast #263"
  },
  {
    "objectID": "posts/2018-01-05-lists_all_the_way2.html",
    "href": "posts/2018-01-05-lists_all_the_way2.html",
    "title": "It’s lists all the way down, part 2: We need to go deeper",
    "section": "",
    "text": "Shortly after my previous blog post, I saw this tweet on my timeline:\n\n\n\nThe purrr resolution for 2018 - learn at least one purrr function per week - is officially launched with encouragement and inspiration from @statwonk and @hadleywickham. We start with modify_depth: https://t.co/dCMnSHP7Pl. Please join to learn and share. #rstats\n\n— Isabella R. Ghement (@IsabellaGhement) January 3, 2018\n\n\n\nThis is a great initiative, and a big coincidence, as I just had blogged about nested lists and how to map over them. I also said this in my previous blog post:\n\n\n\nThere is also another function that you might want to study, modify_depth() which solves related issues but I will end the blog post here. I might talk about it in a future blog post.\n\n\n\nAnd so after I got this reply from @IsabellaGhement:\n\n\n\nBruno, I would love it if you would chime in with an explicit contrast between nested map calls (which I personally find a bit clunky) and alternatives. In other words, present solutions side-by-side and highlight pros and cons. That would be very useful! 🤗\n\n— Isabella R. Ghement (@IsabellaGhement) January 4, 2018\n\n\n\nWhat else was I supposed to do than blog about purrr::modify_depth()?\n\n\nBear in mind that I was not really familiar with this function before writing my last blog post; and even then, I decided to keep it for another blog post, which is this one. Which came much faster than what I had originally planned. So I might have missed some functionality; if that’s the case don’t hesitate to tweet me an example or send me an email! (bruno at brodrigues dot co)\n\n\nSo what is this blog post about? It’s about lists, nested lists, and some things that you can do with them. Let’s use the same example as in my last post:\n\nlibrary(tidyverse)\ndata(mtcars)\n\nnice_function = function(df, param1, param2){\n  df = df %&gt;%\n    filter(cyl == param1, am == param2) %&gt;%\n    mutate(result = mpg * param1 * (2 - param2))\n\n  return(df)\n}\n\nnice_function(mtcars, 4, 0)\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\nvalues_cyl = c(4, 6, 8)\n\nvalues_am = c(0, 1)\n\nNow that we’re here, we would like to apply nice_function() to each element of values_cyl and values_am. In essence, loop over these values. But because loops are not really easy to manipulate, (as explained, in part, here) I use the map* family of functions included in purrr (When I teach R, I only show loops in the advanced topics chapter of my notes). So let’s “loop” over values_cyl and values_am with map() (and not map_df(); there is a reason for this, bear with me):\n\n(result = map(values_am, ~map(values_cyl, nice_function, df = mtcars, param2 = .)))\n## [[1]]\n## [[1]][[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\n## \n## [[1]][[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## \n## [[1]][[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## \n## \n## [[2]]\n## [[2]][[1]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## \n## [[2]][[2]]\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2\n## \n## [[2]][[3]]\n##    mpg cyl disp  hp drat   wt qsec vs am gear carb result\n## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4\n## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0\n\nUntil now, nothing new compared to my previous post (so if you have a hard time to follow what I’m doing here, go read it here).\n\n\nAs far as I know, there is no way, in this example, to avoid this nested map call. However, suppose now that you want to apply a function to each single data frame contained in the list result. Of course, here, you could simply use bind_rows() to have a single data frame and then apply your function to it. But suppose that you want to keep this list structure; at the end, I will give an example of why you might want that, using another purrr function, walk() and Thomas’ J. Leeper brilliant rio package.\n\n\nSo suppose you want to use this function here:\n\ndouble_col = function(dataset, col){\n  col = enquo(col)\n  col_name = paste0(\"double_\", quo_name(col))\n  dataset %&gt;%\n    mutate(!!col_name := 2*(!!col))\n}\n\nto double the values of a column of a dataset. It uses tidyeval’s enquo(), quo_name() and !!() functions to make it work with tidyverse functions such as mutate(). You can use it like this:\n\ndouble_col(mtcars, hp)\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb double_hp\n## 1  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4       220\n## 2  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4       220\n## 3  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1       186\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1       220\n## 5  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2       350\n## 6  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1       210\n## 7  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4       490\n## 8  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2       124\n## 9  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2       190\n## 10 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4       246\n## 11 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4       246\n## 12 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3       360\n## 13 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3       360\n## 14 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3       360\n## 15 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4       410\n## 16 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4       430\n## 17 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4       460\n## 18 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1       132\n## 19 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2       104\n## 20 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1       130\n## 21 21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1       194\n## 22 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2       300\n## 23 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2       300\n## 24 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4       490\n## 25 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2       350\n## 26 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1       132\n## 27 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2       182\n## 28 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2       226\n## 29 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4       528\n## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6       350\n## 31 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8       670\n## 32 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2       218\n\nNice, but you want to use this function on all of the data frames contained in your result list. You can use a nested map() as before:\n\nmap(result, ~map(., .f = double_col, col = disp))\n## [[1]]\n## [[1]][[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2\n## \n## [[1]][[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8       516.0\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2       450.0\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4       335.2\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6       335.2\n## \n## [[1]][[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2       720.0\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8       720.0\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4       551.6\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8       551.6\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2       551.6\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4       944.0\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4       920.0\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2       880.0\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0       636.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2       608.0\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8       700.0\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2       800.0\n## \n## \n## [[2]]\n## [[2]][[1]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2       216.0\n## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6       157.4\n## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6       151.4\n## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6       142.2\n## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2       158.0\n## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0       240.6\n## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6       190.2\n## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6       242.0\n## \n## [[2]][[2]]\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0         320\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0         320\n## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2         290\n## \n## [[2]][[3]]\n##    mpg cyl disp  hp drat   wt qsec vs am gear carb result double_disp\n## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4         702\n## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0         602\n\nbut there’s an easier solution, which is using modify_depth():\n\n(result = modify_depth(result, .depth = 2, double_col, col = disp))\n## [[1]]\n## [[1]][[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2\n## \n## [[1]][[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8       516.0\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2       450.0\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4       335.2\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6       335.2\n## \n## [[1]][[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2       720.0\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8       720.0\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4       551.6\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8       551.6\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2       551.6\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4       944.0\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4       920.0\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2       880.0\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0       636.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2       608.0\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8       700.0\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2       800.0\n## \n## \n## [[2]]\n## [[2]][[1]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2       216.0\n## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6       157.4\n## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6       151.4\n## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6       142.2\n## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2       158.0\n## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0       240.6\n## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6       190.2\n## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6       242.0\n## \n## [[2]][[2]]\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0         320\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0         320\n## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2         290\n## \n## [[2]][[3]]\n##    mpg cyl disp  hp drat   wt qsec vs am gear carb result double_disp\n## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4         702\n## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0         602\n\nSo how does it work? modify_depth() needs a list and a .depth argument, which corresponds to where you you want to apply your function. The following lines of code might help you understand:\n\n# Depth of 1:\n\nresult[[1]]\n## [[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2\n## \n## [[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8       516.0\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2       450.0\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4       335.2\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6       335.2\n## \n## [[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2       720.0\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8       720.0\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4       551.6\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8       551.6\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2       551.6\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4       944.0\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4       920.0\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2       880.0\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0       636.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2       608.0\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8       700.0\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2       800.0\n\nIn this example, a depth of 1 corresponds to a list of three data frame. Can you use your function double_col() on a list of three data frames? No, because the domain of double_col() is the set of data frames, not the set of lists of data frames. So you need to go deeper:\n\n# Depth of 2:\n\nresult[[1]][[1]] # or try result[[1]][[2]] or result[[1]][[3]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2\n\nAt the depth of 2, you’re dealing with data frames! So you can use your function double_col(). With a depth of 2, one might not see the added value of modify_depth() over nested map calls, but if you have to go even deeper, nested map calls are very confusing and verbose.\n\n\nNow for the last part; why doing all this, and not simply bind all the rows, apply double_col() and call it a day? Well, suppose that there is a reason you have these data frames inside lists; for example, the first element, i.e., result[[1]] might be data for, say, Portugal, for 3 different years. result[[2]] however, is data for France, for the same years. Suppose also that you have to give this data, after having worked on it, to a colleague (or to another institution) in the Excel format; one Excel workbook per country, one sheet per year. This example might seem contrived, but I have been confronted to this exact situation very often. Well, if you bind all the rows together, how are you going to save the data in the workbooks like you are required to?\n\n\nWell, thanks to rio, one line of code is enough:\n\nlibrary(rio)\n\nwalk2(result, list(\"portugal.xlsx\", \"france.xlsx\"), export)\n\nI know what you’re thinking; Bruno, that’s two lines of code!. Yes, but I had to load rio. Also, walk() (and walk2()) are basically the same as map(), but you use walk() over map() when you are only interested in the side effect of the function you are applying over your list; here, export() which is rio’s function to write data to disk. The side effect of this function is… writing data to disk! You could have used map2() just the same, but I wanted to show you walk2() (however, you cannot replace map() by walk() in most cases; try it and see what happens).\n\n\nHere’s what it looks like:\n\n\n\n\n\nI have two Excel workbooks, (one per list), where each sheet is a data frame!\n\n\nIf you enjoy these blog posts, you can follow me on twitter."
  },
  {
    "objectID": "posts/2016-03-31-unit-testing-with-r.html",
    "href": "posts/2016-03-31-unit-testing-with-r.html",
    "title": "Unit testing with R",
    "section": "",
    "text": "I've been introduced to unit testing while working with colleagues on quite a big project for which we use Python.\n\n\nAt first I was a bit skeptical about the need of writing unit tests, but now I must admit that I am seduced by the idea and by the huge time savings it allows. Naturally, I was wondering if the same could be achieved with R, and was quite happy to find out that it also possible to write unit tests in R using a package called testthat.\n\n\nUnit tests (Not to be confused with unit root tests for time series) are small functions that test your code and help you make sure everything is alright. I'm going to show how the testthat packages works with a very trivial example, that might not do justice to the idea of unit testing. But you'll hopefully see why writing unit tests is not a waste of your time, especially if your project gets very complex (if you're writing a package for example).\n\n\nFirst, you'll need to download and install testthat. Some dependencies will also be installed.\n\n\nNow, you'll need a function to test. Let's suppose you've written a function that returns the nth Fibonacci number:\n\nFibonacci &lt;- function(n){\n    a &lt;- 0\n    b &lt;- 1\n    for (i in 1:n){\n        temp &lt;- b\n        b &lt;- a\n        a &lt;- a + temp\n    }\n    return(a)\n}\n\n\nYou then save this function in a file, let's call it fibo.R. What you'll probably do once you've written this function, is to try it out:\n\nFibonacci(5)\n\n## [1] 5\n\n\nYou'll see that the function returns the right result and continue programming. The idea behind unit testing is write a bunch of functions that you can run after you make changes to your code, just to check that everything is still running as it should.\n\n\nLet's create a script called test_fibo.R and write the following code in it:\n\ntest_that(\"Test Fibo(15)\",{\n  phi &lt;- (1 + sqrt(5))/2\n  psi &lt;- (1 - sqrt(5))/2\n  expect_equal(Fibonacci(15), (phi**15 - psi**15)/sqrt(5))\n})\n\n\nThe code above uses Binet's formula, a closed form formula that gives the nth Fibonacci number and compares it our implementation of the algorithm. If you didn't know about Binet's formula, you could simply compute some numbers by hand and compare them to what your function returns, for example. The function expect_equal is a function from the package testthat and does exactly what it tells. We expect the result of our implementation to be equal to the result of Binet's Formula. The file test_fibo.R can contain as many tests as you need. Also, the file that contains the tests must start with the string test, so that testthat knows with files it has to run.\n\n\nNow, we're almost done, create yet another script, let's call it run_tests.R and write the following code in it:\n\nlibrary(testthat) \n\nsource(\"path/to/fibo.R\")\n\ntest_results &lt;- test_dir(\"path/to/tests\", reporter=\"summary\")\n\n\nAfter running these lines, and if everything goes well, you should see a message like this:\n\n&gt; library(testthat)\n&gt; source(\"path/to/fibo.R\")\n&gt; test_results &lt;- test_dir(\"path/to/tests\", reporter=\"summary\")\n\n.\nYour tests are dandy! \n\n\nNotice the small . over the message? This means that one test was run successfully. You'll get one dot per successful test. If you take a look at test_results you'll see this:\n\n&gt; test_results\n         file context          test nb failed skipped error  user system  real\n1 test_fibo.R         Test Fibo(15)  1      0   FALSE FALSE 0.004      0 0.006\n\n\nYou'll see each file and each function inside the files that were tested, and also whether the test was skipped, failed etc. This may seem overkill for such a simple function, but imagine that you write dozens of functions that get more and more complex over time. You might have to change a lot of lines because as time goes by you add new functionality, but don't want to break what was working. Running your unit tests each time you make changes can help you pinpoint regressions in your code. Unit tests can also help you start with your code. It can happen that sometimes you don't know exactly how to start; well you could start by writing a unit test that returns the result you want to have and then try to write the code to make that unit test pass. This is called test-driven development.\n\n\nI hope that this post motivated you to write unit tests and make you a better R programmer!"
  },
  {
    "objectID": "posts/2016-06-21-careful-with-trycatch.html",
    "href": "posts/2016-06-21-careful-with-trycatch.html",
    "title": "Careful with tryCatch",
    "section": "",
    "text": "tryCatch is one of the functions that allows the users to handle errors in a simple way. With it, you can do things like: if(error), then(do this).\n\n\nTake the following example:\n\nsqrt(\"a\")\nError in sqrt(\"a\") : non-numeric argument to mathematical function\n\nNow maybe you’d want something to happen when such an error happens. You can achieve that with tryCatch:\n\ntryCatch(sqrt(\"a\"), error=function(e) print(\"You can't take the square root of a character, silly!\"))\n## [1] \"You can't take the square root of a character, silly!\"\n\nWhy am I interested in tryCatch?\n\n\nI am currently working with dates, specifically birthdays of people in my data sets. For a given mother, the birthday of her child is given in three distinct columns: a column for the child’s birth year, birth month and birth day respectively. I’ve wanted to put everything in a single column and convert the birthday to unix time (I have a very good reason to do that, but I won’t bore you with the details).\n\n\nLet’s create some data:\n\nmother &lt;- as.data.frame(list(month=12, day=1, year=1988))\n\nIn my data, there’s a lot more columns of course, such as the mother’s wage, education level, etc, but for illustration purposes, this is all that’s needed.\n\n\nNow, to create this birthday column:\n\nmother$birth1 &lt;- as.POSIXct(paste0(as.character(mother$year), \n                                   \"-\", as.character(mother$month), \n                                   \"-\", as.character(mother$day)), \n                            origin=\"1970-01-01\")\n\nand to convert it to unix time:\n\nmother$birth1 &lt;- as.numeric(as.POSIXct(paste0(as.character(mother$year), \n                                              \"-\", as.character(mother$month), \n                                              \"-\", as.character(mother$day)),\n                                       origin=\"1970-01-01\"))\n\nprint(mother)\n##   month day year    birth1\n## 1    12   1 1988 596934000\n\nNow let’s see what happens in this other example here:\n\nmother2 &lt;- as.data.frame(list(month=2, day=30, year=1988))\n\nmother2$birth1 &lt;- as.POSIXct(paste0(as.character(mother2$year), \n                                    \"-\", as.character(mother2$month), \n                                    \"-\", as.character(mother2$day)), \n                             origin=\"1970-01-01\")\n\nThis is what happens:\n\nError in as.POSIXlt.character(x, tz, ...) : \n  character string is not in a standard unambiguous format\n\nThis error is to be expected; there is no 30th of February! It turns out that in some rare cases, weird dates like this exist in my data. Probably some encoding errors. Not a problem I thought, I could use tryCatch and return NA in the case of an error.\n\nmother2 &lt;- as.data.frame(list(month=2, day=30, year=1988))\n\nmother2$birth1 &lt;- tryCatch(as.POSIXct(paste0(as.character(mother2$year), \n                                    \"-\", as.character(mother2$month), \n                                    \"-\", as.character(mother2$day)), \n                             origin=\"1970-01-01\"), error=function(e) NA)\n\nprint(mother2)\n##   month day year birth1\n## 1     2  30 1988     NA\n\nPretty great, right? Well, no. Take a look at what happens in this case:\n\nmother &lt;- as.data.frame(list(month=c(12, 2), day=c(1, 30), year=c(1988, 1987)))\nprint(mother)\n##   month day year\n## 1    12   1 1988\n## 2     2  30 1987\n\nWe’d expect to have a correct date for the first mother and an NA for the second. However, this is what happens\n\nmother$birth1 &lt;- tryCatch(as.POSIXct(paste0(as.character(mother$year), \n                                    \"-\", as.character(mother$month), \n                                    \"-\", as.character(mother$day)), \n                             origin=\"1970-01-01\"), error=function(e) NA)\n\nprint(mother)\n##   month day year birth1\n## 1    12   1 1988     NA\n## 2     2  30 1987     NA\n\nAs you can see, we now have an NA for both mothers! That’s actually to be expected. Indeed, this little example illustrates it well:\n\nsqrt(c(4, 9, \"haha\"))\nError in sqrt(c(4, 9, \"haha\")) : \n  non-numeric argument to mathematical function\n\nBut you’d like to have this:\n\n[1]  2  3 NA\n\nSo you could make the same mistake as myself and use tryCatch:\n\ntryCatch(sqrt(c(4, 9, \"haha\")), error=function(e) NA)\n## [1] NA\n\nBut you only get NA in return. That’s actually completely normal, but it took me off-guard and I spent quite some time to figure out what was happening. Especially because I had written unit tests to test my function create_birthdays() that was doing the above computations and all tests were passing! The problem was that in my tests, I only had a single individual, so for a wrong date, having NA for this individual was expected behaviour. But in a panel, only some individuals have a weird date like the 30th of February, but because of those, the whole column was filled with NA’s! What I’m doing now is trying to either remove these weird birthdays (there are mothers whose children were born on the 99-99-9999. Documentation is lacking, but this probably means missing value), or tyring to figure out how to only get NA’s for the “weird” dates. I guess that the answer lies with dplyr’s group_by() and mutate() to compute this birthdays for each individual separately."
  },
  {
    "objectID": "posts/2014-04-23-r-s4-rootfinding.html",
    "href": "posts/2014-04-23-r-s4-rootfinding.html",
    "title": "Object Oriented Programming with R: An example with a Cournot duopoly",
    "section": "",
    "text": "I started reading Applied Computational Economics & Finance by Mario J. Miranda and Paul L. Fackler. It is a very interesting book that I recommend to every one of my colleagues. The only issue I have with this book, is that the programming language they use is Matlab, which is proprietary. While there is a free as in freedom implementation of the Matlab language, namely Octave, I still prefer using R. In this post, I will illustrate one example the authors present in the book with R, using the package rootSolve. rootSolve implements Newtonian algorithms to find roots of functions; to specify the functions for which I want the roots, I use R's Object-Oriented Programming (OOP) capabilities to build a model that returns two functions. This is optional, but I found that it was a good example to illustrate OOP, even though simpler solutions exist, one of which was proposed by reddit user TheDrownedKraken (whom I thank) and will be presented at the end of the article.\n\n\nTheoretical background\n\n\nThe example is taken from Miranda's and Fackler's book, on page 35. The authors present a Cournot duopoly model. In a Cournot duopoly model, two firms compete against each other by quantities. Both produce a certain quantity of an homogenous good, and take the quantity produce by their rival as given.\n\n\nThe inverse demand of the good is :\n\n\\[P(q) = q^{-\\dfrac{1}{\\eta}}\\]\n\nthe cost function for firm i is:\n\n\\[C_i(q_i) = P(q_1+q_2)*q_i - C_i(q_i)\\]\n\nand the profit for firm i:\n\n\\[\\pi_i(q1,q2) = P(q_1+q_2)q_i - C_i(q_i)\\]\n\nThe optimality condition for firm i is thus:\n\n\\[\\dfrac{\\partial \\pi_i}{\\partial q_i} = (q1+q2)^{-\\dfrac{1}{\\eta}} - \\dfrac{1}{\\eta} (q_1+q_2)^{\\dfrac{-1}{\\eta-1}}q_i - c_iq_i=0.\\]\n\nImplementation in R\n\n\nIf we want to find the optimal quantities (q_1) and (q_2) we need to program the optimality condition and we could also use the jacobian of the optimality condition. The jacobian is generally useful to speed up the root finding routines. This is were OOP is useful. First let's create a new class, called Model:\n\n\nsetClass(Class = \"Model\", slots = list(OptimCond = \"function\", JacobiOptimCond = \"function\"))\n\n\nThis new class has two slots, which here are functions (in general slots are properties of your class); we need the model to return the optimality condition and the jacobian of the optimality condition.\n\n\nNow we can create a function which will return these two functions for certain values of the parameters, c and  of the model:\n\n\nmy_mod &lt;- function(eta, c) {\n    e = -1/eta\n\n    OptimCond &lt;- function(q) {\n        return(sum(q)^e + e * sum(q)^(e - 1) * q - diag(c) %*% q)\n    }\n\n    JacobiOptimCond &lt;- function(q) {\n      return(\n        (e * sum(q)^e) * array(1, c(2, 2)) +\n        (e * sum(q)^(e - 1)) * diag(1, 2) +\n        (e - 1) * e * sum(q)^(e - 2) * q * c(1, 1) - diag(c)\n      )\n    }\n\n    return(new(\"Model\", OptimCond = OptimCond, JacobiOptimCond = JacobiOptimCond))\n\n}\n\n\nThe function my_mod takes two parameters, eta and c and returns two functions, the optimality condition and the jacobian of the optimality condition. Both are now accessible via my_mod(eta=1.6,c = c(0.6,0.8))@OptimCond and my_mod(eta=1.6,c = c(0.6,0.8))@JacobiOptimCond respectively (and by specifying values for eta and c).\n\n\nNow, we can use the rootSolve package to get the optimal values (q_1) and (q_2)\n\n\nlibrary(\"rootSolve\")\n\nmultiroot(f = my_mod(eta = 1.6, c = c(0.6, 0.8))@OptimCond,\n          start = c(1, 1),\n          maxiter = 100,\n          jacfunc = my_mod(eta = 1.6, c = c(0.6, 0.8))@JacobiOptimCond)\n\n## $root\n## [1] 0.8396 0.6888\n## \n## $f.root\n##            [,1]\n## [1,] -2.220e-09\n## [2,]  9.928e-09\n## \n## $iter\n## [1] 4\n## \n## $estim.precis\n## [1] 6.074e-09\n\n\nAfter 4 iterations, we get that  and  are equal to 0.84 and 0.69 respectively, which are the same values as in the book!\n\n\nSuggestion by Reddit user, TheDrownedKraken\n\n\nI posted this blog post on the rstats subbreddit on www.reddit.com. I got a very useful comment by reddit member TheDrownedKraken which suggested the following approach, which doesn't need a new class to be build. I thank him for this. Here is his suggestion:\n\n\ngenerator &lt;- function(eta, a) {\n    e = -1/eta\n\n    OptimCond &lt;- function(q) {\n        return(sum(q)^e + e * sum(q)^(e - 1) * q - diag(a) %*% q)\n    }\n\n    JacobiOptimCond &lt;- function(q) {\n      return(\n        (e * sum(q)^e) * array(1, c(2, 2)) +\n        (e * sum(q)^(e - 1)) * diag(1, 2) +\n        (e - 1) * e * sum(q)^(e - 2) * q * c(1, 1) - diag(a)\n      )\n    }\n\n    return(list(OptimCond = OptimCond, JacobiOptimCond = JacobiOptimCond))\n\n}\n\nf.s &lt;- generator(eta = 1.6, a = c(0.6, 0.8))\n\nmultiroot(f = f.s$OptimCond, start = c(1, 1), maxiter = 100, jacfunc = f.s$JacobiOptimCond)\n\n## $root\n## [1] 0.8396 0.6888\n## \n## $f.root\n##            [,1]\n## [1,] -2.220e-09\n## [2,]  9.928e-09\n## \n## $iter\n## [1] 4\n## \n## $estim.precis\n## [1] 6.074e-09"
  },
  {
    "objectID": "posts/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions.html",
    "href": "posts/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions.html",
    "title": "Data frame columns as arguments to dplyr functions",
    "section": "",
    "text": "Suppose that you would like to create a function which does a series of computations on a data frame. You would like to pass a column as this function’s argument. Something like:\n\ndata(cars)\nconvertToKmh &lt;- function(dataset, col_name){\n  dataset$col_name &lt;- dataset$speed * 1.609344\n  return(dataset)\n}\n\nThis example is obviously not very interesting (you don’t need a function for this), but it will illustrate the point. You would like to append a column called speed_in_kmh with the speed in kilometers per hour to this dataset, but this is what happens:\n\nhead(convertToKmh(cars, \"speed_in_kmh\"))\n##   speed dist  col_name\n1     4    2  6.437376\n2     4   10  6.437376\n3     7    4 11.265408\n4     7   22 11.265408\n5     8   16 12.874752\n6     9   10 14.484096\n\nYour column is not called speed_in_kmh but col_name! It turns out that there is a very simple solution:\n\nconvertToKmh &lt;- function(dataset, col\\_name){\n  dataset[col_name] &lt;- dataset$speed * 1.609344\n  return(dataset)\n}\n\nhead(convertToKmh(cars, \"speed\\_in\\_kmh\"))\n##   speed dist speed\\_in\\_kmh\n1     4    2     6.437376\n2     4   10     6.437376\n3     7    4    11.265408\n4     7   22    11.265408\n5     8   16    12.874752\n6     9   10    14.484096\n\nYou can access columns with [] instead of $.\n\n\nBut sometimes you want to do more complex things and for example have a function that groups by a variable and then computes new variables, filters by another and so on. You would like to avoid having to hard code these variables in your function, because then why write a function and of course you would like to use dplyr to do it.\n\n\nI often use dplyr functions in my functions. For illustration purposes, consider this very simple function:\n\nsimpleFunction &lt;- function(dataset, col_name){\n  require(\"dplyr\")\n  dataset %&gt;% \n    group\\_by(col\\_name) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\nsimpleFunction(cars, \"dist\")\n\nThis function takes a dataset as an argument, as well as a column name. However, this does not work. You get this error:\n\nError: unknown variable to group by : col_name \n\nThe variable col_name is passed to simpleFunction() as a string, but group_by() requires a variable name. So why not try to convert col_name to a name?\n\nsimpleFunction &lt;- function(dataset, col_name){\n  require(\"dplyr\")\n  col\\_name &lt;- as.name(col_name)\n  dataset %&gt;% \n    group\\_by(col\\_name) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\nsimpleFunction(cars, \"dist\")\n\nYou get the same error as before:\n\nError: unknown variable to group by : col_name \n\nSo how can you pass a column name to group_by()? Well, there is another version of group_by() called group_by_() that uses standard evaluation. You can learn more about it here. Let’s take a look at what happens when we use group_by_():\n\nsimpleFunction &lt;- function(dataset, col_name){\n  require(\"dplyr\")\n  dataset %&gt;% \n    group\\_by\\_(col_name) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\nsimpleFunction(cars, \"dist\")\nA tibble: 35 x 2\n dist mean\\_speed\n&lt;dbl&gt;      &lt;dbl&gt;\n1      2        4.0\n2      4        7.0\n3     10        6.5\n4     14       12.0\n5     16        8.0\n6     17       11.0\n7     18       10.0\n8     20       13.5\n9     22        7.0\n10    24       12.0\n ... with 25 more rows\n\nWe can even use a formula instead of a string:\n\nsimpleFunction(cars, ~dist)\n A tibble: 35 x 2\n    dist mean_speed\n   &lt;dbl&gt;      &lt;dbl&gt;\n1      2        4.0\n2      4        7.0\n3     10        6.5\n4     14       12.0\n5     16        8.0\n6     17       11.0\n7     18       10.0\n8     20       13.5\n9     22        7.0\n10    24       12.0\n... with 25 more rows\n\nWhat if you want to pass column names and constants, for example to filter without hardcoding anything?\n\n\nTrying to do it naively will only yield pain and despair:\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  require(\"dplyr\")\n  dataset %&gt;% \n    filter\\_(col\\_name == value) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n&gt; simpleFunction(cars, \"dist\", 10)\n\n  mean_speed\n1        NaN\n\n&gt; simpleFunction(cars, dist, 10)\n\n Error in col_name == value : \n  comparison (1) is possible only for atomic and list types \n  \n&gt; simpleFunction(cars, ~dist, 10)\n\n  mean_speed\n1        NaN\n\n\nTo solve this issue, we need to know a little bit about two concepts, lazy evaluation and non-standard evaluation. I recommend you read the following document from Hadley Wickham’s book Advanced R as well as the part on lazy evaluation here.\n\n\nA nice package called lazyeval can help us out. We would like to make R understand that the column name is not col_name but the string inside it \"dist\", and now we would like to use filter() for dist equal to 10.\n\n\nIn the lazyeval package, you’ll find the function interp(). interp() allows you to\n\n\n\nbuild an expression up from a mixture of constants and variables.\n\n\n\nTake a look at this example:\n\nlibrary(lazyeval)\ninterp(~x+y, x = 2)\n## ~2 + y\n\nWhat you get back is this nice formula that you can then use within functions. To see why this is useful, let’s look at the above example again, and make it work using interp():\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  require(\"dplyr\")\n  require(\"lazyeval\")\n  filter\\_criteria &lt;- interp(~y == x, .values=list(y = as.name(col_name), x = value))\n  dataset %&gt;% \n    filter\\_(filter_criteria) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(cars, \"dist\", 10)\n  mean\\_speed\n1        6.5\n\nAnd now it works! For some reason, you have to pass the column name as a string though.\n\n\nSources: apart from the documents above, the following stackoverflow threads helped me out quite a lot: In R: pass column name as argument and use it in function with dplyr::mutate() and lazyeval::interp() and Non-standard evaluation (NSE) in dplyr’s filter_ & pulling data from MySQL."
  },
  {
    "objectID": "posts/2025-01-09-github_pages_setup_with_quarto.html",
    "href": "posts/2025-01-09-github_pages_setup_with_quarto.html",
    "title": "github pages setup for this website",
    "section": "",
    "text": "Desired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\n\n1 + 1"
  },
  {
    "objectID": "posts/2015-11-11-bootstrapping-did-with-r.html",
    "href": "posts/2015-11-11-bootstrapping-did-with-r.html",
    "title": "Bootstrapping standard errors for difference-in-differences estimation with R",
    "section": "",
    "text": "I’m currently working on a paper (with my colleague Vincent Vergnat who is also a Phd candidate at BETA) where I want to estimate the causal impact of the birth of a child on hourly and daily wages as well as yearly worked hours. For this we are using non-parametric difference-in-differences (henceforth DiD) and thus have to bootstrap the standard errors. In this post, I show how this is possible using the function boot.\n\n\nFor this we are going to replicate the example from Wooldridge’s Econometric Analysis of Cross Section and Panel Data and more specifically the example on page 415. You can download the data for R here. The question we are going to try to answer is how much does the price of housing decrease due to the presence of an incinerator in the neighborhood?\n\n\nFirst put the data in a folder and set the correct working directory and load the boot library.\n\nlibrary(boot)\nsetwd(\"/home/path/to/data/kiel data/\")\nload(\"kielmc.RData\")\n\nNow you need to write a function that takes the data as an argument, as well as an indices argument. This argument is used by the boot function to select samples. This function should return the statistic you’re interested in, in our case, the DiD estimate.\n\nrun_DiD &lt;- function(my_data, indices){\n    d &lt;- my_data[indices,]\n    return(\n        mean(d$rprice[d$year==1981 & d$nearinc==1]) - \n        mean(d$rprice[d$year==1981 & d$nearinc==0]) - \n        (mean(d$rprice[d$year==1978 & d$nearinc==1]) - \n        mean(d$rprice[d$year==1978 & d$nearinc==0]))\n    )\n}\n\nYou’re almost done! To bootstrap your DiD estimate you just need to use the boot function. If you have cpu with multiple cores (which you should, single core machines are quite outdated by now) you can even parallelize the bootstrapping.\n\nboot_est &lt;- boot(data, run_DiD, R=1000, parallel=\"multicore\", ncpus = 2)\n\nNow you should just take a look at your estimates:\n\nboot_est\n \nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = data, statistic = run_DiD, R = 1000, parallel = \"multicore\", \n ncpus = 2)\n\n\nBootstrap Statistics :\n original    bias    std. error\nt1* -11863.9 -553.3393    8580.435\n\nThese results are very similar to the ones in the book, only the standard error is higher.\n\n\nYou can get confidence intervals like this:\n\nquantile(boot_est$t, c(0.025, 0.975))\n##       2.5%      97.5% \n## -30186.397   3456.133\n\nor a t-statistic:\n\nboot_est$t0/sd(boot_est$t)\n## [1] -1.382669\n\nOr the density of the replications:\n\nplot(density(boot_est$t))\n\n&lt;img src=\"/img/density_did.png\" width=\"670\" height=\"450\" /&gt;&lt;/a&gt;\n\n\nJust as in the book, we find that the DiD estimate is not significant to the 5% level."
  },
  {
    "objectID": "posts/2017-07-27-spread_rename_at.html",
    "href": "posts/2017-07-27-spread_rename_at.html",
    "title": "tidyr::spread() and dplyr::rename_at() in action",
    "section": "",
    "text": "I was recently confronted to a situation that required going from a long dataset to a wide dataset, but with a small twist: there were two datasets, which I had to merge into one. You might wonder what kinda crappy twist that is, right? Well, let’s take a look at the data:\n\ndata1; data2\n## # A tibble: 20 x 4\n##    country date       variable_1       value\n##    &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;            &lt;int&gt;\n##  1 lu      01/01/2005 maybe               22\n##  2 lu      01/07/2005 maybe               13\n##  3 lu      01/01/2006 maybe               40\n##  4 lu      01/07/2006 maybe               25\n##  5 lu      01/01/2005 totally_agree       42\n##  6 lu      01/07/2005 totally_agree       17\n##  7 lu      01/01/2006 totally_agree       25\n##  8 lu      01/07/2006 totally_agree       16\n##  9 lu      01/01/2005 totally_disagree    39\n## 10 lu      01/07/2005 totally_disagree    17\n## 11 lu      01/01/2006 totally_disagree    23\n## 12 lu      01/07/2006 totally_disagree    21\n## 13 lu      01/01/2005 kinda_disagree      69\n## 14 lu      01/07/2005 kinda_disagree      12\n## 15 lu      01/01/2006 kinda_disagree      10\n## 16 lu      01/07/2006 kinda_disagree       9\n## 17 lu      01/01/2005 kinda_agree         38\n## 18 lu      01/07/2005 kinda_agree         31\n## 19 lu      01/01/2006 kinda_agree         19\n## 20 lu      01/07/2006 kinda_agree         12\n## # A tibble: 20 x 4\n##    country date       variable_2       value\n##    &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;            &lt;int&gt;\n##  1 lu      01/01/2005 kinda_agree         22\n##  2 lu      01/07/2005 kinda_agree         13\n##  3 lu      01/01/2006 kinda_agree         40\n##  4 lu      01/07/2006 kinda_agree         25\n##  5 lu      01/01/2005 totally_agree       42\n##  6 lu      01/07/2005 totally_agree       17\n##  7 lu      01/01/2006 totally_agree       25\n##  8 lu      01/07/2006 totally_agree       16\n##  9 lu      01/01/2005 totally_disagree    39\n## 10 lu      01/07/2005 totally_disagree    17\n## 11 lu      01/01/2006 totally_disagree    23\n## 12 lu      01/07/2006 totally_disagree    21\n## 13 lu      01/01/2005 maybe               69\n## 14 lu      01/07/2005 maybe               12\n## 15 lu      01/01/2006 maybe               10\n## 16 lu      01/07/2006 maybe                9\n## 17 lu      01/01/2005 kinda_disagree      38\n## 18 lu      01/07/2005 kinda_disagree      31\n## 19 lu      01/01/2006 kinda_disagree      19\n## 20 lu      01/07/2006 kinda_disagree      12\n\nAs explained in Hadley (2014), this is how you should keep your data… But for a particular purpose, I had to transform these datasets. What I was asked to do was to merge these into a single wide data frame. Doing this for one dataset is easy:\n\ndata1 %&gt;%\n  spread(variable_1, value)\n## # A tibble: 4 x 7\n##   country date       kinda_agree kinda_disagree maybe totally_agree\n##   &lt;chr&gt;   &lt;chr&gt;            &lt;int&gt;          &lt;int&gt; &lt;int&gt;         &lt;int&gt;\n## 1 lu      01/01/2005          38             69    22            42\n## 2 lu      01/01/2006          19             10    40            25\n## 3 lu      01/07/2005          31             12    13            17\n## 4 lu      01/07/2006          12              9    25            16\n## # ... with 1 more variable: totally_disagree &lt;int&gt;\n\nBut because data1 and data2 have the same levels for variable_1 and variable_2, this would not work. So the solution I found online, in this SO thread was to use tidyr::spread() with dplyr::rename_at() like this:\n\ndata1 &lt;- data1 %&gt;%\n  spread(variable_1, value) %&gt;%\n  rename_at(vars(-country, -date), funs(paste0(\"variable1:\", .)))\n\nglimpse(data1)\n## Observations: 4\n## Variables: 7\n## $ country                      &lt;chr&gt; \"lu\", \"lu\", \"lu\", \"lu\"\n## $ date                         &lt;chr&gt; \"01/01/2005\", \"01/01/2006\", \"01/0...\n## $ `variable1:kinda_agree`      &lt;int&gt; 38, 19, 31, 12\n## $ `variable1:kinda_disagree`   &lt;int&gt; 69, 10, 12, 9\n## $ `variable1:maybe`            &lt;int&gt; 22, 40, 13, 25\n## $ `variable1:totally_agree`    &lt;int&gt; 42, 25, 17, 16\n## $ `variable1:totally_disagree` &lt;int&gt; 39, 23, 17, 21\ndata2 &lt;- data2 %&gt;%\n  spread(variable_2, value) %&gt;%\n  rename_at(vars(-country, -date), funs(paste0(\"variable2:\", .)))\n\nglimpse(data2)\n## Observations: 4\n## Variables: 7\n## $ country                      &lt;chr&gt; \"lu\", \"lu\", \"lu\", \"lu\"\n## $ date                         &lt;chr&gt; \"01/01/2005\", \"01/01/2006\", \"01/0...\n## $ `variable2:kinda_agree`      &lt;int&gt; 22, 40, 13, 25\n## $ `variable2:kinda_disagree`   &lt;int&gt; 38, 19, 31, 12\n## $ `variable2:maybe`            &lt;int&gt; 69, 10, 12, 9\n## $ `variable2:totally_agree`    &lt;int&gt; 42, 25, 17, 16\n## $ `variable2:totally_disagree` &lt;int&gt; 39, 23, 17, 21\n\nrename_at() needs variables which you pass to vars(), a helper function to select variables, and a function that will do the renaming, passed to funs(). The function I use is simply paste0(), which pastes a string, for example “variable1:” with the name of the columns, given by the single ‘.’, a dummy argument. Now these datasets can be merged:\n\ndata1 %&gt;%\n  full_join(data2) %&gt;%\n  glimpse()\n## Joining, by = c(\"country\", \"date\")\n## Observations: 4\n## Variables: 12\n## $ country                      &lt;chr&gt; \"lu\", \"lu\", \"lu\", \"lu\"\n## $ date                         &lt;chr&gt; \"01/01/2005\", \"01/01/2006\", \"01/0...\n## $ `variable1:kinda_agree`      &lt;int&gt; 38, 19, 31, 12\n## $ `variable1:kinda_disagree`   &lt;int&gt; 69, 10, 12, 9\n## $ `variable1:maybe`            &lt;int&gt; 22, 40, 13, 25\n## $ `variable1:totally_agree`    &lt;int&gt; 42, 25, 17, 16\n## $ `variable1:totally_disagree` &lt;int&gt; 39, 23, 17, 21\n## $ `variable2:kinda_agree`      &lt;int&gt; 22, 40, 13, 25\n## $ `variable2:kinda_disagree`   &lt;int&gt; 38, 19, 31, 12\n## $ `variable2:maybe`            &lt;int&gt; 69, 10, 12, 9\n## $ `variable2:totally_agree`    &lt;int&gt; 42, 25, 17, 16\n## $ `variable2:totally_disagree` &lt;int&gt; 39, 23, 17, 21\n\nHope this post helps you understand the difference between long and wide datasets better, as well as dplyr::rename_at()!"
  },
  {
    "objectID": "posts/2013-11-07-gmm-with-rmd.html",
    "href": "posts/2013-11-07-gmm-with-rmd.html",
    "title": "Nonlinear Gmm with R - Example with a logistic regression",
    "section": "",
    "text": "In this post, I will explain how you can use the R gmm package to estimate a non-linear model, and more specifically a logit model. For my research, I have to estimate Euler equations using the Generalized Method of Moments. I contacted Pierre Chaussé, the creator of the gmm library for help, since I was having some difficulties. I am very grateful for his help (without him, I'd still probably be trying to estimate my model!).\n\n\nTheoretical background, motivation and data set\n\n\nI will not dwell in the theory too much, because you can find everything you need here. I think it’s more interesting to try to understand why someone would use the Generalized Method of Moments instead of maximization of the log-likelihood. Well, in some cases, getting the log-likelihood can be quite complicated, as can be the case for arbitrary, non-linear models (for example if you want to estimate the parameters of a very non-linear utility function). Also, moment conditions can sometimes be readily available, so using GMM instead of MLE is trivial. And finally, GMM is… well, a very general method: every popular estimator can be obtained as a special case of the GMM estimator, which makes it quite useful.\n\n\nAnother question that I think is important to answer is: why this post? Well, because that’s exactly the kind of post I would have loved to have found 2 months ago, when I was beginning to work with the GMM. Most posts I found presented the gmm package with very simple and trivial examples, which weren’t very helpful. The example presented below is not very complicated per se, but much more closer to a real-world problem than most stuff that is out there. At least, I hope you will find it useful!\n\n\nFor illustration purposes, I'll use data from Marno Verbeek's A guide to modern Econometrics, used in the illustration on page 197. You can download the data from the book's companion page here under the section Data sets or from the Ecdat package in R, which I’ll be using.\n\n\nImplementation in R\n\n\nI don't estimate the exact same model, but only use a subset of the variables available in the data set. Keep in mind that this post is just for illustration purposes.\n\n\nFirst load the gmm package and load the data set:\n\n\nlibrary(gmm)\nlibrary(Ecdat)\ndata(\"Benefits\")\n\nBenefits &lt;- transform(\n  Benefits,\n  age2 = age**2,\n  rr2 = rr**2\n  )\n\n\nWe can then estimate a logit model with the glm() function:\n\n\nnative &lt;- glm(ui ~ age + age2 + dkids + dykids + head + male + married + rr + rr2,\n              data = Benefits,\n              family = binomial(link = \"logit\"),\n              na.action = na.pass)\n\nsummary(native)\n\n## \n## Call:\n## glm(formula = y ~ age + age2 + dkids + dykids + head + male + \n##     married + rr + rr2, family = binomial(link = \"logit\"), na.action = na.pass)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -1.889  -1.379   0.788   0.896   1.237  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)   \n## (Intercept) -1.00534    0.56330   -1.78   0.0743 . \n## age          0.04909    0.02300    2.13   0.0328 * \n## age2        -0.00308    0.00293   -1.05   0.2924   \n## dkids       -0.10922    0.08374   -1.30   0.1921   \n## dykids       0.20355    0.09490    2.14   0.0320 * \n## head        -0.21534    0.07941   -2.71   0.0067 **\n## male        -0.05988    0.08456   -0.71   0.4788   \n## married      0.23354    0.07656    3.05   0.0023 **\n## rr           3.48590    1.81789    1.92   0.0552 . \n## rr2         -5.00129    2.27591   -2.20   0.0280 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 6086.1  on 4876  degrees of freedom\n## Residual deviance: 5983.9  on 4867  degrees of freedom\n## AIC: 6004\n## \n## Number of Fisher Scoring iterations: 4\n\n\nNow comes the interesting part: how can you estimate such a non-linear model with the gmm() function from the gmm package?\n\n\nFor every estimation with the Generalized Method of Moments, you will need valid moment conditions. It turns out that in the case of the logit model, this moment condition is quite simple:\n\n\\[E[X' * (Y-\\Lambda(X'\\theta))] = 0\\]\n\nwhere ( () ) is the logistic function. Let's translate this condition into code. First, we need the logistic function:\n\n\nlogistic &lt;- function(theta, data) {\n    return(1/(1 + exp(-data %*% theta)))\n}\n\n\nand let's also define a new data frame, to make our life easier with the moment conditions (don’t forget to add a column of ones to the matrix, hence the 1 after y):\n\n\ndat &lt;- data.matrix(with(Benefits,\n                        cbind(ui, 1, age, age2, dkids,\n                              dykids, head, sex,\n                              married, rr, rr2)))\n\n\nand now the moment condition itself:\n\n\nmoments &lt;- function(theta, data) {\n  y &lt;- as.numeric(data[, 1])\n  x &lt;- data.matrix(data[, 2:11])\n  m &lt;- x * as.vector((y - logistic(theta, x)))\n  return(cbind(m))\n}\n\n\nThe moment condition(s) are given by a function which returns a matrix with as many columns as moment conditions (same number of columns as parameters for just-identified models).\n\n\nTo use the gmm() function to estimate our model, we need to specify some initial values to get the maximization routine going. One neat trick is simply to use the coefficients of a linear regression; I found it to work well in a lot of situations:\n\n\ninit &lt;- (lm(ui ~ age + age2 + dkids + dykids + head + sex + married + rr + rr2,\n            data = Benefits))$coefficients\n\n\nAnd finally, we have everything to use gmm():\n\n\nmy_gmm &lt;- gmm(moments, x = dat, t0 = init, type = \"iterative\", crit = 1e-25, wmatrix = \"optimal\", method = \"Nelder-Mead\", control = list(reltol = 1e-25, maxit = 20000))\n\nsummary(my_gmm)\n\n\nPlease, notice the options crit=1e-25,method=“Nelder-Mead”,control=list(reltol=1e-25,maxit=20000): these options mean that the Nelder-Mead algorithm is used, and to specify further options to the Nelder-Mead algorithm, the control option is used. This is very important, as Pierre Chaussé explained to me: non-linear optimization is an art, and most of the time the default options won't cut it and will give you false results. To add insult to injury, the Generalized Method of Moments itself is very capricious and you will also have to play around with different initial values to get good results. As you can see, the Convergence code equals 10, which is a code specific to the Nelder-Mead method which indicates «degeneracy of the Nelder–Mead simplex.» . I’m not sure if this is a bad thing though, but other methods can give you better results. I’d suggest you try always different maximization routines with different starting values to see if your estimations are robust. Here, the results are very similar to what we obtained with the built-in function glm() so we can stop here.\n\n\nShould you notice any error whatsoever, do not hesitate to tell me."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html",
    "title": "How to use jailbreakr",
    "section": "",
    "text": "The jailbreakr package is probably one of the most interesting packages I came across recently. This package makes it possible to extract messy data from spreadsheets. What is meant by messy? I am sure you already had to deal with spreadsheets that contained little tables inside a single sheet for example. As far as I know, there is no simple way of extracting these tables without having to fiddle around a lot. This is now over with jailbreakr. Well not entirely, because jailbreakr is still in development, but it works well already. If you want to know more about the planned features, you can watch the following video by Jenny Bryan, one of the package’s authors."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html#what-is-jailbreakr",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html#what-is-jailbreakr",
    "title": "How to use jailbreakr",
    "section": "",
    "text": "The jailbreakr package is probably one of the most interesting packages I came across recently. This package makes it possible to extract messy data from spreadsheets. What is meant by messy? I am sure you already had to deal with spreadsheets that contained little tables inside a single sheet for example. As far as I know, there is no simple way of extracting these tables without having to fiddle around a lot. This is now over with jailbreakr. Well not entirely, because jailbreakr is still in development, but it works well already. If you want to know more about the planned features, you can watch the following video by Jenny Bryan, one of the package’s authors."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html#installation-and-data",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html#installation-and-data",
    "title": "How to use jailbreakr",
    "section": "\nInstallation and data\n",
    "text": "Installation and data\n\n\nYou will have to install the package from Github, as it is not on CRAN yet. Here is the Github link. To install the package, just run the following commands in an R console:\n\ndevtools::install_github(c(\"hadley/xml2\",\n                           \"rsheets/linen\",\n                           \"rsheets/cellranger\",\n                           \"rsheets/rexcel\",\n                           \"rsheets/jailbreakr\"))\n\nIf you get the following error:\n\ndevtools::install_github(\"hadley/xml2\")\nDownloading GitHub repo hadley/xml2@master\nfrom URL https://api.github.com/repos/hadley/xml2/zipball/master\nError in system(full, intern = quiet, ignore.stderr = quiet, ...) :\n    error in running command\n\nand if you’re on a GNU+Linux distribution try to run the following command:\n\noptions(unzip = \"internal\")\n\nand then run github_install() again.\n\n\nAs you can see, you need some other packages to make it work. Now we are going to get some data. We are going to download some time series from the European Commission, data I had to deal with recently. Download the data by clicking here and look for the spreadsheet titled Investment_total_factors_nace2.xlsx. The data we are interested in is on the second sheet, named TOT. You cannot import this sheet easily into R because there are four tables on the same sheet. Let us use jailbreakr to get these tables out of the sheet and into nice, tidy, data frames."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html#jailbreakr-to-the-rescue",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html#jailbreakr-to-the-rescue",
    "title": "How to use jailbreakr",
    "section": "\njailbreakr to the rescue\n",
    "text": "jailbreakr to the rescue\n\n\nThe first step is to read the data in. For this, we are going to use the rexcel package, which is also part of the rsheets organization on Github that was set up by Jenny Brian and Rich Fitzjohn, the authors of these packages. rexcel imports the sheet you want but not in a way that is immediately useful to you. It just gets the sheet into R, which makes it then possible to use jailbreakr’s magic on it. First, let’s import the packages we need:\n\nlibrary(\"rexcel\")\nlibrary(\"jailbreakr\")\n\nWe need to check which sheet to import. There are two sheets, and we want to import the one called TOT, the second one. But is it really the second one? I have noticed that sometimes, there are hidden sheets which makes importing the one you want impossible. So first, let use use another package, readxl and its function excel_sheets() to make sure we are extracting the sheet we really need:\n\nsheets &lt;- readxl::excel_sheets(path_to_data)\n\ntot_sheet &lt;- which(sheets == \"TOT\")\n\nprint(tot_sheet)\n## [1] 3\n\nAs you can see, the sheet we want is not the second, but the third! Let us import this sheet into R now (this might take more time than you think; on my computer it takes around 10 seconds):\n\nmy_sheet &lt;- rexcel_read(path_to_data, sheet = tot_sheet)\n\nNow we can start using jailbreakr. The function split_sheet() is the one that splits the sheet into little tables:\n\ntables &lt;- split_sheet(my_sheet)\nstr(tables)\n## List of 4\n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 34 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list \n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 33 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list \n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 32 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list \n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 33 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list\n\ntables is actually a list containing worksheet_view objects. Take a look at the dim attribute: you see the dimensions of the tables there. When I started using jailbreakr I was stuck here. I was looking for the function that would extract the data frames and could not find it. Then I watched the video and I understood what I had to do: a worksheet_view object has a values() method that does the extraction for you. This is a bit unusual in R (it made me feel like I was using Python); maybe in future versions this values() method will become a separate function of its own in the package. What happens when we use values()?\n\nlibrary(\"purrr\")\nlist_of_data &lt;-  map(tables, (function(x)(x$values())))\nmap(list_of_data, head)\n## [[1]]\n##      [,1]     [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]\n## [1,] \"TOT\"    NA      NA    NA    NA    NA    NA    NA    NA    NA   \n## [2,] \"DEMAND\" 33603   33969 34334 34699 35064 35430 35795 36160 36525\n## [3,] \"FDEMT\"  \"FDEMN\" NA    NA    NA    NA    NA    NA    NA    NA   \n## [4,] \"EU\"     \":\"     16.9  -1.4  20.2  34.5  31.4  37.5  39    37.3 \n## [5,] \"EA\"     \":\"     15.5  -13.1 14.8  30.9  25.1  35.2  39.2  37.1 \n## [6,] \"BE\"     \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   42.3  43.1 \n##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n## [1,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [2,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543\n## [3,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [4,] 39.2  27.5  20.6  21.4  29.8  26.4  32.5  47.1  19    -1.3  23.5 \n## [5,] 39.5  25.3  18.2  18.9  27.4  23    28.2  46.1  12.3  -9.3  19.3 \n## [6,] 45.8  42.2  42.9  43.8  45.8  47.4  49.1  50.9  48.2  46.9  46.3 \n##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] \n## [1,] NA    NA    NA    NA    NA    NA    NA    \n## [2,] 40908 41274 41639 42004 42369 42735 43100 \n## [3,] NA    NA    NA    NA    NA    NA    NA    \n## [4,] 29    22    21.1  25.6  31.8  22.9  \"30.7\"\n## [5,] 26.2  18.6  15.7  21.7  28.8  17.3  26.6  \n## [6,] 46.8  47.1  48.2  50.1  49.2  34.5  34.4  \n## \n## [[2]]\n##      [,1]        [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]\n## [1,] \"FINANCIAL\" 33603   33969 34334 34699 35064 35430 35795 36160 36525\n## [2,] \"FFINT\"     \"FFINN\" NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] \"EU\"        \":\"     -5.1  -6.2  2.7   6.7   9     14.4  13.9  14   \n## [4,] \"EA\"        \":\"     -8.8  -13.5 -3.4  2.6   5.7   12.5  13.2  13.1 \n## [5,] \"BE\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   21.5  22.4 \n## [6,] \"BG\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"  \n##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n## [1,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543\n## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] 16.4  9.4   7.4   8.1   12.4  8.4   13.6  23.4  4.1   -4    10.9 \n## [4,] 16.5  8     6.8   5.1   9.9   4.8   8.4   24.3  -2.8  -10.5 9.3  \n## [5,] 20.9  22.3  32.2  33.5  33.8  34.8  35    34.5  37.2  33.5  32.7 \n## [6,] \":\"   \":\"   20.8  24    27.1  28.3  33.4  37.5  37.7  26.6  30.4 \n##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] \n## [1,] 40908 41274 41639 42004 42369 42735 43100 \n## [2,] NA    NA    NA    NA    NA    NA    NA    \n## [3,] 12.4  10.2  8.8   13.4  17.4  6.2   \"12.3\"\n## [4,] 9     7.2   5     11    13.1  -1    6.5   \n## [5,] 31.5  32.3  33    31.7  32.2  19.9  20.5  \n## [6,] 33.8  35.6  36    41.5  41.6  44.2  43.8  \n## \n## [[3]]\n##      [,1]        [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]\n## [1,] \"TECHNICAL\" 33603   33969 34334 34699 35064 35430 35795 36160 36525\n## [2,] \"FTECT\"     \"FTECN\" NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] \"EU\"        \":\"     39.2  37.6  38.3  40    40.7  42.8  43.5  43.8 \n## [4,] \"EA\"        \":\"     39.7  36.2  37.5  41.2  40    44    44.8  44.9 \n## [5,] \"BE\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   58.8  58.5 \n## [6,] \"BG\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"  \n##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n## [1,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543\n## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] 37    31.1  27.2  30.9  30.4  30.3  27.4  40.5  25.8  23.1  27.4 \n## [4,] 37    30.3  27.4  31    29.9  29.7  24.8  41    23.4  19.5  26.4 \n## [5,] 58.3  58.4  57.7  59.2  59.6  59.4  60.2  59.5  60.5  57.9  56.3 \n## [6,] \":\"   \":\"   17.3  17.5  21.1  21.5  25.3  28.2  26.1  21    25.3 \n##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] \n## [1,] 40908 41274 41639 42004 42369 42735 43100 \n## [2,] NA    NA    NA    NA    NA    NA    NA    \n## [3,] 28.9  26.3  31.3  32.1  32.1  30.2  \"34.6\"\n## [4,] 28.5  25.9  32.1  32.4  33.1  30.2  36    \n## [5,] 56.7  57.7  57.9  58.6  59.1  13.1  13.1  \n## [6,] 24.6  26.8  30.4  31.9  34.1  34.8  33.7  \n## \n## [[4]]\n##      [,1]    [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10] [,11]\n## [1,] \"OTHER\" 33603   33969 34334 34699 35064 35430 35795 36160 36525 36891\n## [2,] \"FOTHT\" \"FOTHN\" NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] \"EU\"    \":\"     2.9   -0.5  3.9   3.9   1     4.1   4.7   7     7.2  \n## [4,] \"EA\"    \":\"     2.3   -4.9  1.4   1.3   -2.4  1.1   3.2   5.8   7    \n## [5,] \"BE\"    \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   14    14.9  15.9 \n## [6,] \"BG\"    \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"  \n##      [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]\n## [1,] 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543 40908\n## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] -1.5  6.2   8.1   7.6   1.4   2.4   13.7  -1.9  -3.2  1.1   1.1  \n## [4,] -3.7  5.5   7.1   7.2   -2.2  0.4   15.5  -4.6  -8.4  0.3   -3.3 \n## [5,] 16.3  22.8  23.1  22.4  24.5  25.3  25.5  26.6  26.6  24.7  24.6 \n## [6,] \":\"   -2.3  -0.8  2.4   2.9   3.5   4.8   5.5   2.2   3.3   3.2  \n##      [,23] [,24] [,25] [,26] [,27] [,28]\n## [1,] 41274 41639 42004 42369 42735 43100\n## [2,] NA    NA    NA    NA    NA    NA   \n## [3,] -1.6  0.9   2.7   1.9   -3.3  \"2.1\"\n## [4,] -2.3  0.6   2.5   2.1   -5.4  1.7  \n## [5,] 26.4  25.9  25    25.3  4.7   5.2  \n## [6,] 5.9   7     8.2   9.6   9.4   9.1\n\nWe are getting really close to something useful! Now we can get the first table and do some basic cleaning to have a tidy dataset:\n\ndataset1 &lt;- list_of_data[[1]]\n\ndataset1 &lt;- dataset1[-c(1:3), ]\ndataset1[dataset1 == \":\"] &lt;- NA\ncolnames(dataset1) &lt;- c(\"country\", seq(from = 1991, to = 2017))\n\nhead(dataset1)\n##      country 1991 1992 1993  1994 1995 1996 1997 1998 1999 2000 2001 2002\n## [1,] \"EU\"    NA   16.9 -1.4  20.2 34.5 31.4 37.5 39   37.3 39.2 27.5 20.6\n## [2,] \"EA\"    NA   15.5 -13.1 14.8 30.9 25.1 35.2 39.2 37.1 39.5 25.3 18.2\n## [3,] \"BE\"    NA   NA   NA    NA   NA   NA   NA   42.3 43.1 45.8 42.2 42.9\n## [4,] \"BG\"    NA   NA   NA    NA   NA   NA   NA   NA   NA   NA   NA   39.6\n## [5,] \"CZ\"    NA   NA   NA    NA   NA   NA   NA   NA   NA   NA   NA   54.9\n## [6,] \"DK\"    49.5 45   50    59.5 62.5 55.5 60.5 57.5 56   61.5 57.5 59.5\n##      2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016\n## [1,] 21.4 29.8 26.4 32.5 47.1 19   -1.3 23.5 29   22   21.1 25.6 31.8 22.9\n## [2,] 18.9 27.4 23   28.2 46.1 12.3 -9.3 19.3 26.2 18.6 15.7 21.7 28.8 17.3\n## [3,] 43.8 45.8 47.4 49.1 50.9 48.2 46.9 46.3 46.8 47.1 48.2 50.1 49.2 34.5\n## [4,] 43   42.8 45.5 49.1 52.6 50.7 39.5 45.5 47.4 45.6 50.5 51.4 49.9 53.2\n## [5,] 37   48.5 67.9 66.4 66.8 69.3 64.7 61   56   47.5 53   53.5 67.5 58  \n## [6,] 53.5 50   59   64   63   56   33.5 57   47   48   52   45.5 40.5 36.5\n##      2017  \n## [1,] \"30.7\"\n## [2,] 26.6  \n## [3,] 34.4  \n## [4,] 52.8  \n## [5,] 59.5  \n## [6,] 37.5\n\nEt voilà! We went from a messy spreadsheet to a tidy dataset in a matter of minutes. Even though this package is still in early development and not all the features that are planned are available, the basics are there and can save you a lot of pain!"
  },
  {
    "objectID": "posts/2015-01-12-introduction-to-programming-econometrics-with-r.html",
    "href": "posts/2015-01-12-introduction-to-programming-econometrics-with-r.html",
    "title": "Introduction to programming econometrics with R",
    "section": "",
    "text": "This semester, I’ll be teaching an introduction to applied econometrics with R, so I’ve decided to write a very small book called “Introduction to programming Econometrics with R”. This is primarily intended for bachelor students and the focus is not much on econometric theory, but more on how to implement econometric theory into computer code, using the R programming language. It’s very basic and doesn’t cover any advanced topics in econometrics and is intended for people with 0 previous programming knowledge. It is still very rough around the edges, and it’s missing the last chapter about reproducible research, and the references, but I think it’s time to put it out there; someone else than my students may find it useful. The book’s probably full of typos and mistakes, so don’t hesitate to drop me an e-mail if you find something fishy: contact@brodrigues.co\nAlso there might be some sections at the beginning that only concern my students. Just ignore that.\nGet it here: download\n\nUpdate (2017-01-22)\nYou might find the book useful as it is now, but I never had a chance to finish it. I might get back to it once I’ll have more time, and port it to bookdown."
  },
  {
    "objectID": "posts/2013-01-16-simulated-maximum-likelihood-with-r.html",
    "href": "posts/2013-01-16-simulated-maximum-likelihood-with-r.html",
    "title": "Simulated Maximum Likelihood with R",
    "section": "",
    "text": "This document details section 12.4.5. Unobserved Heterogeneity Example from Cameron and Trivedi’s book - MICROECONOMETRICS: Methods and Applications. The original source code giving the results from table 12.2 are available from the authors' site here and written for Stata. This is an attempt to translate the code to R. I’d like to thank Reddit user anonemouse2010 for his advice which helped me write the function.\n\n\nConsult the original source code if you want to read the authors' comments. If you want the R source code without all the commentaries, grab it here. This is not guaranteed to work, nor to be correct. It could set your pet on fire and/or eat your first born. Use at your own risk. I may, or may not, expand this example. Corrections, constructive criticism are welcome.\n\n\nThe model is \\(y=\\theta+u+\\varepsilon\\) where \\(\\theta\\) is a scalar parameter equal to 1. \\(u\\) is extreme value type 1 (Gumbel distribution), \\(\\varepsilon \\leadsto \\mathbb{N}(0,1)\\). For more details, consult the book.\n\n\nImport the data\n\n\nYou can consult the original source code to see how the authors simulated the data. To get the same results, and verify that I didn't make mistakes I prefer importing their data directly from their website.\n\n\ndata &lt;- read.table(\"http://cameron.econ.ucdavis.edu/mmabook/mma12p2mslmsm.asc\")\nu &lt;- data[, 1]\ne &lt;- data[, 2]\ny &lt;- data[, 3]\nnumobs &lt;- length(u)\nsimreps &lt;- 10000\n\n\nSimulation\n\n\nIn the code below, the following likelihood function:\n\\[\\log{\\hat{L}_N(\\theta)} = \\dfrac{1}{N} \\sum_{i=1}^N\\log{\\big( \\dfrac{1}{S}\\sum_{s=1}^S \\dfrac{1}{\\sqrt{2\\pi}} \\exp \\{ -(-y_i-\\theta-u_i^s)^2/2 \\}\\big)}\\]\nwhich can be found on page 397 is programmed using the function sapply.\n\n\ndenssim &lt;- function(theta) {\n    loglik &lt;- mean(sapply(y, function(y) log(mean((1/sqrt(2 * pi)) * exp(-(y - theta + log(-log(runif(simreps))))^2/2)))))\n    return(-loglik)\n}\n\n\nThis likelihood is then maximized:\n\n\nsystem.time(res &lt;- optim(0.1, denssim, method = \"BFGS\", control = list(maxit = simreps)))\n\n   user  system elapsed \n  1.460   0.197   1.657 \n\n\n\nConvergence is achieved pretty rapidly, to\n\n\nres\n\n$par\n[1] 1.193006\n\n$value\n[1] 1.921685\n\n$counts\nfunction gradient \n      58       10 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\nwhich is close to the true value of the parameter 1 (which was used to generate the data).\n\n\nLet's try again with another parameter value, for example ( ). We have to generate y again:\n\n\ny2 &lt;- 2.5 + u + e\n\n\nand slightly modify the likelihood:\n\n\ndenssim2 &lt;- function(theta) {\n  loglik &lt;- mean(\n    sapply(\n      y2,\n      function(y2) log(mean((1/sqrt(2 * pi)) * exp(-(y2 - theta + log(-log(runif(simreps))))^2/2)))))\n  return(-loglik)\n}\n\n\nwhich can then be maximized:\n\n\nsystem.time(res2 &lt;- optim(0.1, denssim2, method = \"BFGS\", control = list(maxit = simreps)))\n\n   user  system elapsed \n  1.321   0.000   1.322 \n\n\n\nThe value that maximizes the likelihood is:\n\n\nres2\n\n$par\n[1] 2.73753\n\n$value\n[1] 1.92257\n\n$counts\nfunction gradient \n      49       10 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\nwhich is close to the true value of the parameter 2.5 (which was used to generate the data)."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html",
    "href": "posts/2017-03-24-lesser_known_purrr.html",
    "title": "Lesser known purrr tricks",
    "section": "",
    "text": "purrr is a package that extends R’s functional programming capabilities. It brings a lot of new stuff to the table and in this post I show you some of the most useful (at least to me) functions included in purrr."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#getting-rid-of-loops-with-map",
    "href": "posts/2017-03-24-lesser_known_purrr.html#getting-rid-of-loops-with-map",
    "title": "Lesser known purrr tricks",
    "section": "\nGetting rid of loops with map()\n",
    "text": "Getting rid of loops with map()\n\nlibrary(purrr)\n\nnumbers &lt;- list(11, 12, 13, 14)\n\nmap_dbl(numbers, sqrt)\n## [1] 3.316625 3.464102 3.605551 3.741657\n\nYou might wonder why this might be preferred to a for loop? It’s a lot less verbose, and you do not need to initialise any kind of structure to hold the result. If you google “create empty list in R” you will see that this is very common. However, with the map() family of functions, there is no need for an initial structure. map_dbl() returns an atomic list of real numbers, but if you use map() you will get a list back. Try them all out!"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#map-conditionally",
    "href": "posts/2017-03-24-lesser_known_purrr.html#map-conditionally",
    "title": "Lesser known purrr tricks",
    "section": "\nMap conditionally\n",
    "text": "Map conditionally\n\n\n\nmap_if()\n\n# Create a helper function that returns TRUE if a number is even\nis_even &lt;- function(x){\n  !as.logical(x %% 2)\n}\n\nmap_if(numbers, is_even, sqrt)\n## [[1]]\n## [1] 11\n## \n## [[2]]\n## [1] 3.464102\n## \n## [[3]]\n## [1] 13\n## \n## [[4]]\n## [1] 3.741657\n\n\n\nmap_at()\n\nmap_at(numbers, c(1,3), sqrt)\n## [[1]]\n## [1] 3.316625\n## \n## [[2]]\n## [1] 12\n## \n## [[3]]\n## [1] 3.605551\n## \n## [[4]]\n## [1] 14\n\nmap_if() and map_at() have a further argument than map(); in the case of map_if(), a predicate function ( a function that returns TRUE or FALSE) and a vector of positions for map_at(). This allows you to map your function only when certain conditions are met, which is also something that a lot of people google for."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#map-a-function-with-multiple-arguments",
    "href": "posts/2017-03-24-lesser_known_purrr.html#map-a-function-with-multiple-arguments",
    "title": "Lesser known purrr tricks",
    "section": "\nMap a function with multiple arguments\n",
    "text": "Map a function with multiple arguments\n\nnumbers2 &lt;- list(1, 2, 3, 4)\n\nmap2(numbers, numbers2, `+`)\n## [[1]]\n## [1] 12\n## \n## [[2]]\n## [1] 14\n## \n## [[3]]\n## [1] 16\n## \n## [[4]]\n## [1] 18\n\nYou can map two lists to a function which takes two arguments using map_2(). You can even map an arbitrary number of lists to any function using pmap().\n\n\nBy the way, try this in: +(1,3) and see what happens."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#dont-stop-execution-of-your-function-if-something-goes-wrong",
    "href": "posts/2017-03-24-lesser_known_purrr.html#dont-stop-execution-of-your-function-if-something-goes-wrong",
    "title": "Lesser known purrr tricks",
    "section": "\nDon’t stop execution of your function if something goes wrong\n",
    "text": "Don’t stop execution of your function if something goes wrong\n\npossible_sqrt &lt;- possibly(sqrt, otherwise = NA_real_)\n\nnumbers_with_error &lt;- list(1, 2, 3, \"spam\", 4)\n\nmap(numbers_with_error, possible_sqrt)\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 1.414214\n## \n## [[3]]\n## [1] 1.732051\n## \n## [[4]]\n## [1] NA\n## \n## [[5]]\n## [1] 2\n\nAnother very common issue is to keep running your loop even when something goes wrong. In most cases the loop simply stops at the error, but you would like it to continue and see where it failed. Try to google “skip error in a loop” or some variation of it and you’ll see that a lot of people really just want that. This is possible by combining map() and possibly(). Most solutions involve the use of tryCatch() which I personally do not find very easy to use."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#dont-stop-execution-of-your-function-if-something-goes-wrong-and-capture-the-error",
    "href": "posts/2017-03-24-lesser_known_purrr.html#dont-stop-execution-of-your-function-if-something-goes-wrong-and-capture-the-error",
    "title": "Lesser known purrr tricks",
    "section": "\nDon’t stop execution of your function if something goes wrong and capture the error\n",
    "text": "Don’t stop execution of your function if something goes wrong and capture the error\n\nsafe_sqrt &lt;- safely(sqrt, otherwise = NA_real_)\n\nmap(numbers_with_error, safe_sqrt)\n## [[1]]\n## [[1]]$result\n## [1] 1\n## \n## [[1]]$error\n## NULL\n## \n## \n## [[2]]\n## [[2]]$result\n## [1] 1.414214\n## \n## [[2]]$error\n## NULL\n## \n## \n## [[3]]\n## [[3]]$result\n## [1] 1.732051\n## \n## [[3]]$error\n## NULL\n## \n## \n## [[4]]\n## [[4]]$result\n## [1] NA\n## \n## [[4]]$error\n## \n\nsafely() is very similar to possibly() but it returns a list of lists. An element is thus a list of the result and the accompagnying error message. If there is no error, the error component is NULL if there is an error, it returns the error message."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#transpose-a-list",
    "href": "posts/2017-03-24-lesser_known_purrr.html#transpose-a-list",
    "title": "Lesser known purrr tricks",
    "section": "\nTranspose a list\n",
    "text": "Transpose a list\n\nsafe_result_list &lt;- map(numbers_with_error, safe_sqrt)\n\ntranspose(safe_result_list)\n## $result\n## $result[[1]]\n## [1] 1\n## \n## $result[[2]]\n## [1] 1.414214\n## \n## $result[[3]]\n## [1] 1.732051\n## \n## $result[[4]]\n## [1] NA\n## \n## $result[[5]]\n## [1] 2\n## \n## \n## $error\n## $error[[1]]\n## NULL\n## \n## $error[[2]]\n## NULL\n## \n## $error[[3]]\n## NULL\n## \n## $error[[4]]\n## \n\nHere we transposed the above list. This means that we still have a list of lists, but where the first list holds all the results (which you can then access with safe_result_list$result) and the second list holds all the errors (which you can access with safe_result_list$error). This can be quite useful!"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#apply-a-function-to-a-lower-depth-of-a-list",
    "href": "posts/2017-03-24-lesser_known_purrr.html#apply-a-function-to-a-lower-depth-of-a-list",
    "title": "Lesser known purrr tricks",
    "section": "\nApply a function to a lower depth of a list\n",
    "text": "Apply a function to a lower depth of a list\n\ntransposed_list &lt;- transpose(safe_result_list)\n\ntransposed_list %&gt;%\n    at_depth(2, is_null)\n## Warning: at_depth() is deprecated, please use `modify_depth()` instead\n## $result\n## $result[[1]]\n## [1] FALSE\n## \n## $result[[2]]\n## [1] FALSE\n## \n## $result[[3]]\n## [1] FALSE\n## \n## $result[[4]]\n## [1] FALSE\n## \n## $result[[5]]\n## [1] FALSE\n## \n## \n## $error\n## $error[[1]]\n## [1] TRUE\n## \n## $error[[2]]\n## [1] TRUE\n## \n## $error[[3]]\n## [1] TRUE\n## \n## $error[[4]]\n## [1] FALSE\n## \n## $error[[5]]\n## [1] TRUE\n\nSometimes working with lists of lists can be tricky, especially when we want to apply a function to the sub-lists. This is easily done with at_depth()!"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#set-names-of-list-elements",
    "href": "posts/2017-03-24-lesser_known_purrr.html#set-names-of-list-elements",
    "title": "Lesser known purrr tricks",
    "section": "\nSet names of list elements\n",
    "text": "Set names of list elements\n\nname_element &lt;- c(\"sqrt()\", \"ok?\")\n\nset_names(transposed_list, name_element)\n## $`sqrt()`\n## $`sqrt()`[[1]]\n## [1] 1\n## \n## $`sqrt()`[[2]]\n## [1] 1.414214\n## \n## $`sqrt()`[[3]]\n## [1] 1.732051\n## \n## $`sqrt()`[[4]]\n## [1] NA\n## \n## $`sqrt()`[[5]]\n## [1] 2\n## \n## \n## $`ok?`\n## $`ok?`[[1]]\n## NULL\n## \n## $`ok?`[[2]]\n## NULL\n## \n## $`ok?`[[3]]\n## NULL\n## \n## $`ok?`[[4]]\n##"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#reduce-a-list-to-a-single-value",
    "href": "posts/2017-03-24-lesser_known_purrr.html#reduce-a-list-to-a-single-value",
    "title": "Lesser known purrr tricks",
    "section": "\nReduce a list to a single value\n",
    "text": "Reduce a list to a single value\n\nreduce(numbers, `*`)\n## [1] 24024\n\nreduce() applies the function * iteratively to the list of numbers. There’s also accumulate():\n\naccumulate(numbers, `*`)\n## [1]    11   132  1716 24024\n\nwhich keeps the intermediary results.\n\n\nThis function is very general, and you can reduce anything:\n\n\nMatrices:\n\nmat1 &lt;- matrix(rnorm(10), nrow = 2)\nmat2 &lt;- matrix(rnorm(10), nrow = 2)\nmat3 &lt;- matrix(rnorm(10), nrow = 2)\nlist_mat &lt;- list(mat1, mat2, mat3)\n\nreduce(list_mat, `+`)\n##             [,1]       [,2]       [,3]     [,4]      [,5]\n## [1,] -2.48530177  1.0110049  0.4450388 1.280802 1.3413979\n## [2,]  0.07596679 -0.6872268 -0.6579242 1.615237 0.8231933\n\neven data frames:\n\ndf1 &lt;- as.data.frame(mat1)\ndf2 &lt;- as.data.frame(mat2)\ndf3 &lt;- as.data.frame(mat3)\n\nlist_df &lt;- list(df1, df2, df3)\n\nreduce(list_df, dplyr::full_join)\n## Joining, by = c(\"V1\", \"V2\", \"V3\", \"V4\", \"V5\")\n## Joining, by = c(\"V1\", \"V2\", \"V3\", \"V4\", \"V5\")\n##           V1         V2          V3          V4         V5\n## 1 -0.6264538 -0.8356286  0.32950777  0.48742905  0.5757814\n## 2  0.1836433  1.5952808 -0.82046838  0.73832471 -0.3053884\n## 3 -0.8969145  1.5878453 -0.08025176  0.70795473  1.9844739\n## 4  0.1848492 -1.1303757  0.13242028 -0.23969802 -0.1387870\n## 5 -0.9619334  0.2587882  0.19578283  0.08541773 -1.2188574\n## 6 -0.2925257 -1.1521319  0.03012394  1.11661021  1.2673687\n\nHope you enjoyed this list of useful functions! If you enjoy the content of my blog, you can follow me on twitter."
  },
  {
    "objectID": "posts/2024-12-09-huhu.html",
    "href": "posts/2024-12-09-huhu.html",
    "title": "huhu",
    "section": "",
    "text": "Undesired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\n\n1 + 1"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html",
    "href": "posts/2017-03-08-lesser_known_tricks.html",
    "title": "Lesser known dplyr tricks",
    "section": "",
    "text": "In this blog post I share some lesser-known (at least I believe they are) tricks that use mainly functions from dplyr."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#removing-unneeded-columns",
    "href": "posts/2017-03-08-lesser_known_tricks.html#removing-unneeded-columns",
    "title": "Lesser known dplyr tricks",
    "section": "\nRemoving unneeded columns\n",
    "text": "Removing unneeded columns\n\n\nDid you know that you can use - in front of a column name to remove it from a data frame?\n\nmtcars %&gt;% \n    select(-disp) %&gt;% \n    head()\n##                    mpg cyl  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#re-ordering-columns",
    "href": "posts/2017-03-08-lesser_known_tricks.html#re-ordering-columns",
    "title": "Lesser known dplyr tricks",
    "section": "\nRe-ordering columns\n",
    "text": "Re-ordering columns\n\n\nStill using select(), it is easy te re-order columns in your data frame:\n\nmtcars %&gt;% \n    select(cyl, disp, hp, everything()) %&gt;% \n    head()\n##                   cyl disp  hp  mpg drat    wt  qsec vs am gear carb\n## Mazda RX4           6  160 110 21.0 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag       6  160 110 21.0 3.90 2.875 17.02  0  1    4    4\n## Datsun 710          4  108  93 22.8 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive      6  258 110 21.4 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout   8  360 175 18.7 3.15 3.440 17.02  0  0    3    2\n## Valiant             6  225 105 18.1 2.76 3.460 20.22  1  0    3    1\n\nAs its name implies everything() simply means all the other columns."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#renaming-columns-with-rename",
    "href": "posts/2017-03-08-lesser_known_tricks.html#renaming-columns-with-rename",
    "title": "Lesser known dplyr tricks",
    "section": "\nRenaming columns with rename()\n",
    "text": "Renaming columns with rename()\n\nmtcars &lt;- rename(mtcars, spam_mpg = mpg)\nmtcars &lt;- rename(mtcars, spam_disp = disp)\nmtcars &lt;- rename(mtcars, spam_hp = hp)\n\nhead(mtcars)\n##                   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am\n## Mazda RX4             21.0   6       160     110 3.90 2.620 16.46  0  1\n## Mazda RX4 Wag         21.0   6       160     110 3.90 2.875 17.02  0  1\n## Datsun 710            22.8   4       108      93 3.85 2.320 18.61  1  1\n## Hornet 4 Drive        21.4   6       258     110 3.08 3.215 19.44  1  0\n## Hornet Sportabout     18.7   8       360     175 3.15 3.440 17.02  0  0\n## Valiant               18.1   6       225     105 2.76 3.460 20.22  1  0\n##                   gear carb\n## Mazda RX4            4    4\n## Mazda RX4 Wag        4    4\n## Datsun 710           4    1\n## Hornet 4 Drive       3    1\n## Hornet Sportabout    3    2\n## Valiant              3    1"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#selecting-columns-with-a-regexp",
    "href": "posts/2017-03-08-lesser_known_tricks.html#selecting-columns-with-a-regexp",
    "title": "Lesser known dplyr tricks",
    "section": "\nSelecting columns with a regexp\n",
    "text": "Selecting columns with a regexp\n\n\nIt is easy to select the columns that start with “spam” with some helper functions:\n\nmtcars %&gt;% \n    select(contains(\"spam\")) %&gt;% \n    head()\n##                   spam_mpg spam_disp spam_hp\n## Mazda RX4             21.0       160     110\n## Mazda RX4 Wag         21.0       160     110\n## Datsun 710            22.8       108      93\n## Hornet 4 Drive        21.4       258     110\n## Hornet Sportabout     18.7       360     175\n## Valiant               18.1       225     105\n\ntake also a look at starts_with(), ends_with(), contains(), matches(), num_range(), one_of() and everything()."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#create-new-columns-with-mutate-and-if_else",
    "href": "posts/2017-03-08-lesser_known_tricks.html#create-new-columns-with-mutate-and-if_else",
    "title": "Lesser known dplyr tricks",
    "section": "\nCreate new columns with mutate() and if_else()\n",
    "text": "Create new columns with mutate() and if_else()\n\nmtcars %&gt;% \n    mutate(vs_new = if_else(\n        vs == 1, \n        \"one\", \n        \"zero\", \n        NA_character_)) %&gt;% \n    head()\n##   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am gear carb vs_new\n## 1     21.0   6       160     110 3.90 2.620 16.46  0  1    4    4   zero\n## 2     21.0   6       160     110 3.90 2.875 17.02  0  1    4    4   zero\n## 3     22.8   4       108      93 3.85 2.320 18.61  1  1    4    1    one\n## 4     21.4   6       258     110 3.08 3.215 19.44  1  0    3    1    one\n## 5     18.7   8       360     175 3.15 3.440 17.02  0  0    3    2   zero\n## 6     18.1   6       225     105 2.76 3.460 20.22  1  0    3    1    one\n\nYou might want to create a new variable conditionally on several values of another column:\n\nmtcars %&gt;% \n    mutate(carb_new = case_when(.$carb == 1 ~ \"one\",\n                                .$carb == 2 ~ \"two\",\n                                .$carb == 4 ~ \"four\",\n                                 TRUE ~ \"other\")) %&gt;% \n    head(15)\n##    spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am gear carb\n## 1      21.0   6     160.0     110 3.90 2.620 16.46  0  1    4    4\n## 2      21.0   6     160.0     110 3.90 2.875 17.02  0  1    4    4\n## 3      22.8   4     108.0      93 3.85 2.320 18.61  1  1    4    1\n## 4      21.4   6     258.0     110 3.08 3.215 19.44  1  0    3    1\n## 5      18.7   8     360.0     175 3.15 3.440 17.02  0  0    3    2\n## 6      18.1   6     225.0     105 2.76 3.460 20.22  1  0    3    1\n## 7      14.3   8     360.0     245 3.21 3.570 15.84  0  0    3    4\n## 8      24.4   4     146.7      62 3.69 3.190 20.00  1  0    4    2\n## 9      22.8   4     140.8      95 3.92 3.150 22.90  1  0    4    2\n## 10     19.2   6     167.6     123 3.92 3.440 18.30  1  0    4    4\n## 11     17.8   6     167.6     123 3.92 3.440 18.90  1  0    4    4\n## 12     16.4   8     275.8     180 3.07 4.070 17.40  0  0    3    3\n## 13     17.3   8     275.8     180 3.07 3.730 17.60  0  0    3    3\n## 14     15.2   8     275.8     180 3.07 3.780 18.00  0  0    3    3\n## 15     10.4   8     472.0     205 2.93 5.250 17.98  0  0    3    4\n##    carb_new\n## 1      four\n## 2      four\n## 3       one\n## 4       one\n## 5       two\n## 6       one\n## 7      four\n## 8       two\n## 9       two\n## 10     four\n## 11     four\n## 12    other\n## 13    other\n## 14    other\n## 15     four\n\nMind the .$ before the variable carb. There is a github issue about this, and it is already fixed in the development version of dplyr, which means that in the next version of dplyr, case_when() will work as any other specialized dplyr function inside mutate()."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#apply-a-function-to-certain-columns-only-by-rows",
    "href": "posts/2017-03-08-lesser_known_tricks.html#apply-a-function-to-certain-columns-only-by-rows",
    "title": "Lesser known dplyr tricks",
    "section": "\nApply a function to certain columns only, by rows\n",
    "text": "Apply a function to certain columns only, by rows\n\nmtcars %&gt;%\n    select(am, gear, carb) %&gt;%\n    purrr::by_row(sum, .collate = \"cols\", .to = \"sum_am_gear_carb\") -&gt; mtcars2\nhead(mtcars2)\n\nFor this, I had to use purrr’s by_row() function. You can then add this column to your original data frame:\n\nmtcars &lt;- cbind(mtcars, \"sum_am_gear_carb\" = mtcars2$sum_am_gear_carb)\nhead(mtcars)\n##                   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am\n## Mazda RX4             21.0   6       160     110 3.90 2.620 16.46  0  1\n## Mazda RX4 Wag         21.0   6       160     110 3.90 2.875 17.02  0  1\n## Datsun 710            22.8   4       108      93 3.85 2.320 18.61  1  1\n## Hornet 4 Drive        21.4   6       258     110 3.08 3.215 19.44  1  0\n## Hornet Sportabout     18.7   8       360     175 3.15 3.440 17.02  0  0\n## Valiant               18.1   6       225     105 2.76 3.460 20.22  1  0\n##                   gear carb sum_am_gear_carb\n## Mazda RX4            4    4                9\n## Mazda RX4 Wag        4    4                9\n## Datsun 710           4    1                6\n## Hornet 4 Drive       3    1                4\n## Hornet Sportabout    3    2                5\n## Valiant              3    1                4"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#use-do-to-do-any-arbitrary-operation",
    "href": "posts/2017-03-08-lesser_known_tricks.html#use-do-to-do-any-arbitrary-operation",
    "title": "Lesser known dplyr tricks",
    "section": "\nUse do() to do any arbitrary operation\n",
    "text": "Use do() to do any arbitrary operation\n\nmtcars %&gt;% \n    group_by(cyl) %&gt;% \n    do(models = lm(spam_mpg ~ drat + wt, data = .)) %&gt;% \n    broom::tidy(models)\n## # A tibble: 9 x 6\n## # Groups:   cyl [3]\n##     cyl term        estimate std.error statistic p.value\n##   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1     4 (Intercept)   33.2      17.1       1.94  0.0877 \n## 2     4 drat           1.32      3.45      0.384 0.711  \n## 3     4 wt            -5.24      2.22     -2.37  0.0456 \n## 4     6 (Intercept)   30.7       7.51      4.08  0.0151 \n## 5     6 drat          -0.444     1.17     -0.378 0.725  \n## 6     6 wt            -2.99      1.57     -1.91  0.129  \n## 7     8 (Intercept)   29.7       7.09      4.18  0.00153\n## 8     8 drat          -1.47      1.63     -0.903 0.386  \n## 9     8 wt            -2.45      0.799    -3.07  0.0107\n\ndo() is useful when you want to use any R function (user defined functions work too!) with dplyr functions. First I grouped the observations by cyl and then ran a linear model for each group. Then I converted the output to a tidy data frame using broom::tidy()."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#using-dplyr-functions-inside-your-own-functions",
    "href": "posts/2017-03-08-lesser_known_tricks.html#using-dplyr-functions-inside-your-own-functions",
    "title": "Lesser known dplyr tricks",
    "section": "\nUsing dplyr functions inside your own functions\n",
    "text": "Using dplyr functions inside your own functions\n\nextract_vars &lt;- function(data, some_string){\n    \n  data %&gt;%\n    select_(lazyeval::interp(~contains(some_string))) -&gt; data\n    \n  return(data)\n}\n\nextract_vars(mtcars, \"spam\")\n##                     spam_mpg spam_disp spam_hp\n## Mazda RX4               21.0     160.0     110\n## Mazda RX4 Wag           21.0     160.0     110\n## Datsun 710              22.8     108.0      93\n## Hornet 4 Drive          21.4     258.0     110\n## Hornet Sportabout       18.7     360.0     175\n## Valiant                 18.1     225.0     105\n## Duster 360              14.3     360.0     245\n## Merc 240D               24.4     146.7      62\n## Merc 230                22.8     140.8      95\n## Merc 280                19.2     167.6     123\n## Merc 280C               17.8     167.6     123\n## Merc 450SE              16.4     275.8     180\n## Merc 450SL              17.3     275.8     180\n## Merc 450SLC             15.2     275.8     180\n## Cadillac Fleetwood      10.4     472.0     205\n## Lincoln Continental     10.4     460.0     215\n## Chrysler Imperial       14.7     440.0     230\n## Fiat 128                32.4      78.7      66\n## Honda Civic             30.4      75.7      52\n## Toyota Corolla          33.9      71.1      65\n## Toyota Corona           21.5     120.1      97\n## Dodge Challenger        15.5     318.0     150\n## AMC Javelin             15.2     304.0     150\n## Camaro Z28              13.3     350.0     245\n## Pontiac Firebird        19.2     400.0     175\n## Fiat X1-9               27.3      79.0      66\n## Porsche 914-2           26.0     120.3      91\n## Lotus Europa            30.4      95.1     113\n## Ford Pantera L          15.8     351.0     264\n## Ferrari Dino            19.7     145.0     175\n## Maserati Bora           15.0     301.0     335\n## Volvo 142E              21.4     121.0     109\n\nAbout this last point, you can read more about it here.\n\n\nHope you liked this small list of tricks!"
  },
  {
    "objectID": "posts/2013-01-29-method-of-simulated-moments-with-r.html",
    "href": "posts/2013-01-29-method-of-simulated-moments-with-r.html",
    "title": "Method of Simulated Moments with R",
    "section": "",
    "text": "This document details section 12.5.6. Unobserved Heterogeneity Example. The original source code giving the results from table 12.3 are available from the authors' site here and written for Stata. This is an attempt to translate the code to R.\n\n\nConsult the original source code if you want to read the authors' comments. If you want the R source code without all the commentaries, grab it here. This is not guaranteed to work, nor to be correct. It could set your pet on fire and/or eat your first born. Use at your own risk. I may, or may not, expand this example. Corrections, constructive criticism are welcome.\n\n\nThe model is the same as the one described here, so I won't go into details. The moment condition used is \\(E[(y_i-\\theta-u_i)]=0\\), so we can replace the expectation operator by the empirical mean:\n\n\n\\[\\dfrac{1}{N} \\sum_{i=1}^N(y_i - \\theta - E[u_i])=0\\]\n\n\nSupposing that \\(E[\\overline{u}]\\) is unknown, we can instead use the method of simulated moments for \\(\\theta\\) defined by:\n\n\n\\[\\dfrac{1}{N} \\sum_{i=1}^N(y_i - \\theta - \\dfrac{1}{S} \\sum_{s=1}^S u_i^s)=0\\]\n\n\nImport the data\n\n\nYou can consult the original source code to see how the authors simulated the data. To get the same results, and verify that I didn't make mistakes I prefer importing their data directly from their website.\n\n\ndata &lt;- read.table(\"http://cameron.econ.ucdavis.edu/mmabook/mma12p2mslmsm.asc\")\nu &lt;- data[, 1]\ne &lt;- data[, 2]\ny &lt;- data[, 3]\nnumobs &lt;- length(u)\nsimreps &lt;- 10000\n\n\nSimulation\n\n\nIn the code below, we simulate the equation defined above:\n\n\nusim &lt;- -log(-log(runif(simreps)))\nesim &lt;- rnorm(simreps, 0, 1)\n\nisim &lt;- 0\nwhile (isim &lt; simreps) {\n\n    usim = usim - log(-log(runif(simreps)))\n    esim = esim + rnorm(simreps, 0, 1)\n\n    isim = isim + 1\n\n}\n\nusimbar = usim/simreps\nesimbar = esim/simreps\n\ntheta = y - usimbar - esimbar\n\ntheta_msm &lt;- mean(theta)\napprox_sterror &lt;- sd(theta)/sqrt(simreps)\n\n\nThese steps yield the following results:\n\n\ntheta_msm\n\n[1] 1.187978\n\n\nand\n\napprox_sterror\n\n[1] 0.01676286"
  },
  {
    "objectID": "posts/2024-11-09-haha.html",
    "href": "posts/2024-11-09-haha.html",
    "title": "November blog post",
    "section": "",
    "text": "Undesired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\n\n1 + 1"
  },
  {
    "objectID": "posts/2015-02-22-export-r-output-to-file.html",
    "href": "posts/2015-02-22-export-r-output-to-file.html",
    "title": "Export R output to a file",
    "section": "",
    "text": "Sometimes it is useful to export the output of a long-running R command. For example, you might want to run a time consuming regression just before leaving work on Friday night, but would like to get the output saved inside your Dropbox folder to take a look at the results before going back to work on Monday.\n\n\nThis can be achieved very easily using capture.output() and cat() like so:\n\nout &lt;- capture.output(summary(my_very_time_consuming_regression))\n\ncat(\"My title\", out, file=\"summary_of_my_very_time_consuming_regression.txt\", sep=\"\\n\", append=TRUE)\n\nmy_very_time_consuming_regression is an object of class lm for example. I save the output of summary(my_very_time_consuming_regression) as text using capture.output and save it in a variable called out. Finally, I save out to a file called summary_of_my_very_time_consuming_regression.txt with the first sentence being My title (you can put anything there). The file summary_of_my_very_time_consuming_regression.txt doesn’t have to already exist in your working directory. The option sep=\"\" is important or else the whole output will be written in a single line. Finally, append=TRUE makes sure your file won’t be overwritten; additional output will be appended to the file, which can be nice if you want to compare different versions of your model."
  },
  {
    "objectID": "posts/2013-12-31-r-cas.html",
    "href": "posts/2013-12-31-r-cas.html",
    "title": "Using R as a Computer Algebra System with Ryacas",
    "section": "",
    "text": "R is used to perform statistical analysis and doesn't focus on symbolic maths. But it is sometimes useful to let the computer derive a function for you (and have the analytic expression of said derivative), but maybe you don't want to leave your comfy R shell. It is possible to turn R into a full-fledged computer algebra system. CASs are tools that perform symbolic operations, such as getting the expression of the derivative of a user-defined (and thus completely arbitrary) function. Popular CASs include the proprietary Mathematica and Maple. There exists a lot of CASs under a Free Software license, Maxima (based on the very old Macsyma), Yacas, Xcas… In this post I will focus on Yacas and the Ryacas libarary. There is also the possibility to use the rSympy library that uses the Sympy Python library, which has a lot more features than Yacas. However, depending on your operating system installation can be tricky as it also requires rJava as a dependency.\n\n\nEven though Ryacas is quite nice to have, there are some issues though. For example, let's say you want the first derivative of a certain function f. If you use Ryacas to get it, the returned object won't be a function. There is a way to “extract” the text from the returned object and make a function out of it. But there are still other issues; I'll discuss them later.\n\n\nInstallation\n\n\nInstallation should be rather painless. On Linux you need to install Yacas first, which should be available in the major distros' repositories. Then you can install Ryacas from within the R shell. On Windows, you need to run these three commands (don't bother installing Yacas first):\n\n\ninstall.packages(Ryacas)\nlibrary(Ryacas)\nyacasInstall()\n\n\nYou can find more information on the project's page.\n\n\nExample session\n\n\nFirst, you must load Ryacas and define symbols that you will use in your functions.\n\n\nlibrary(Ryacas)\n\n## Loading required package: Ryacas Loading required package: XML\n\n\nx &lt;- Sym(\"x\")\n\n\nYou can then define your fonctions:\n\n\nmy_func &lt;- function(x) {\n  return(x/(x^2 + 3))\n}\n\n\nAnd you can get the derivative for instance:\n\n\nmy_deriv &lt;- yacas(deriv(my_func(x), x))\n\n## [1] \"Starting Yacas!\"\n\n\nIf you check the class of my_deriv, you'll see that it is of class yacas, which is not very useful. Let's «convert» it to a function:\n\n\nmy_deriv2 &lt;- function(x) {\n  eval(parse(text = my_deriv$YacasForm))\n}\n\n\nWe can then evaluate it. A lot of different operations are possible. But there are some problems.\n\n\nIssues with Ryacas\n\n\nYou can't use elements of a vector as parameters of your function, i.e.:\n\n\ntheta &lt;- Sym(\"theta\")\nfunc &lt;- function(x) {\n  return(theta[1] * x + theta[2])\n}\n\n\nLet's integrate this\nFunc &lt;- yacas(Integrate(func(x), x)) \n\n\nreturns (x^2theta)/2+NAx; which is not quite what we want…there is a workaround however. Define your functions like this:\n\n\na &lt;- Sym(\"a\")\nb &lt;- Sym(\"b\")\nfunc2 &lt;- function(x) {\n  return(a * x + b)\n}\n\n# Let&#39;s integrate this\nFunc2 &lt;- yacas(Integrate(func2(x), x))\n\n\nwe get the expected result: (x^2a)/2+bx;. Now replace a and b by the thetas:\n\n\nFunc2 &lt;- gsub(\"a\", \"theta[1]\", Func2$YacasForm)\nFunc2 &lt;- gsub(\"b\", \"theta[2]\", Func2)\n\n\nNow we have what we want:\n\n\nFunc2\n\n## [1] \"(x^2*theta[1])/2+theta[2]*x;\"\n\n\nYou can then copy-paste this result into a function.\n\n\nAnother problem is if you use built-in functions that are different between R and Yacas. For example:\n\n\nmy_log &lt;- function(x) {\n    return(sin(log(2 + x)))\n}\n\n\nNow try to differentiate it:\n\n\ndmy_log &lt;- yacas(deriv(my_log(x), x))\n\n\nyou get: Cos(Ln(x+2))/(x+2);. The problem with this, is that R doesn't recognize Cos as the cosine (which is cos in R) and the same goes for Ln. These are valid Yacas functions, but that is not the case in R. So you'll have to use gsub to replace these functions and then copy paste the end result into a function.\n\n\nConclusion\n\n\nWhile it has some flaws, Ryacas can be quite useful if you need to derive or integrate complicated expressions that you then want to use in R. Using some of the tricks I showed here, you should be able to overcome some of its shortcomings. If installation of rJava and thus rSympy becomes easier, I'll probably also do a short blog-post about it, as it has more features than Ryacas."
  }
]