[
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks, presentations, workshops…",
    "section": "",
    "text": "What’s up with reproducibility in R?: talk I gave for the Data and Analytics Learning seminars from the Bank of England on the 29th of January 2025. Repository with code and slides.\nBUILDING REPRODUCIBLE ANALYTICAL PIPELINES WITH R, DOCKER AND NIX: online talk for RLadies Rome, on the 29th of June 2024. Repository, Slides and video.\nRix: reproducible environments with Nix: talk I gave for the online part of the 2024 edition of useR! in Salzburg. Video\nRIX: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk and tutorial I gave for the 2024 edition of the Rencontres R in Vannes, Brittany, France. Slides and tutorial. No video available.\nRix: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk I gave on the 7th of June, 2024 for the INED (Institute Nationale d’Études Démographiques, National institute for demographic studies). Repository, Slides and Video.\nReproducible research with Nix and rix, a talk I gave at the DIPF, Leibniz Institute for Research and Information in Education, in LOS! (Leibniz Open Science!), the event series of the Leibniz Strategy Forum on Open Science. The workshop was held online, on May 16, 2024. Repository, Slides and video of the talk\n“rix: An R package for reproducible dev environments with Nix (FOSDEM 2024)”, a talk I gave at FOSDEM 2024 on the 4th of February 2024 in the Nix and NixOS dev room. Video here.\n“Building reproducible analytical pipelines”, Workshop I gave for the “R User Group Tunis” on December 9th, 2023. Video here. Slides and code here.\n“Porquê usar monads (programação funcional)? Uma ilustração com o pacote {chronicler}” (Why use monads (functional programming)? An illustration with the {chronicler} package), Talk I gave (online) at the INE (Instituto Nacional de Estatística), on the 7th of June 2023. Slides in Portuguese.\n“La reproductibilité avec R, ou pourquoi celle-ci est située sur un continuum” (Reproducibility with R, or why it lies on a continuum). Talk I gave at the Rencontres R 2023 in Avignon, France, on the 26th of June 2023. Slides in French, Video with English subtitles.\n“Building reproducible analytical pipelines”, Workshop I gave for “Workshops for Ukraine” on June 29th, 2023. You can pull the Docker image with the slides and code here.\n“Building reproducible analytical pipelines”, Talk I gave for ReproducibiliTea UCL on the 19th of July, 2023. Slides and Video. You can pull the Docker image with the slides and code here."
  },
  {
    "objectID": "talks.html#talks",
    "href": "talks.html#talks",
    "title": "Talks, presentations, workshops…",
    "section": "",
    "text": "What’s up with reproducibility in R?: talk I gave for the Data and Analytics Learning seminars from the Bank of England on the 29th of January 2025. Repository with code and slides.\nBUILDING REPRODUCIBLE ANALYTICAL PIPELINES WITH R, DOCKER AND NIX: online talk for RLadies Rome, on the 29th of June 2024. Repository, Slides and video.\nRix: reproducible environments with Nix: talk I gave for the online part of the 2024 edition of useR! in Salzburg. Video\nRIX: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk and tutorial I gave for the 2024 edition of the Rencontres R in Vannes, Brittany, France. Slides and tutorial. No video available.\nRix: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk I gave on the 7th of June, 2024 for the INED (Institute Nationale d’Études Démographiques, National institute for demographic studies). Repository, Slides and Video.\nReproducible research with Nix and rix, a talk I gave at the DIPF, Leibniz Institute for Research and Information in Education, in LOS! (Leibniz Open Science!), the event series of the Leibniz Strategy Forum on Open Science. The workshop was held online, on May 16, 2024. Repository, Slides and video of the talk\n“rix: An R package for reproducible dev environments with Nix (FOSDEM 2024)”, a talk I gave at FOSDEM 2024 on the 4th of February 2024 in the Nix and NixOS dev room. Video here.\n“Building reproducible analytical pipelines”, Workshop I gave for the “R User Group Tunis” on December 9th, 2023. Video here. Slides and code here.\n“Porquê usar monads (programação funcional)? Uma ilustração com o pacote {chronicler}” (Why use monads (functional programming)? An illustration with the {chronicler} package), Talk I gave (online) at the INE (Instituto Nacional de Estatística), on the 7th of June 2023. Slides in Portuguese.\n“La reproductibilité avec R, ou pourquoi celle-ci est située sur un continuum” (Reproducibility with R, or why it lies on a continuum). Talk I gave at the Rencontres R 2023 in Avignon, France, on the 26th of June 2023. Slides in French, Video with English subtitles.\n“Building reproducible analytical pipelines”, Workshop I gave for “Workshops for Ukraine” on June 29th, 2023. You can pull the Docker image with the slides and code here.\n“Building reproducible analytical pipelines”, Talk I gave for ReproducibiliTea UCL on the 19th of July, 2023. Slides and Video. You can pull the Docker image with the slides and code here."
  },
  {
    "objectID": "talks.html#interviews-podcasts",
    "href": "talks.html#interviews-podcasts",
    "title": "Talks, presentations, workshops…",
    "section": "Interviews, podcasts…",
    "text": "Interviews, podcasts…\n\nFrench Data Workers Podcast #1 Large Scale Testing : Contenir la covid-19 avec des dépistages ciblés\nFrench Bruno Rodrigues défend une approche basée sur la reproductibilité de la data science au Luxembourg\nEnglish Leanpub Frontmatter Podcast #263"
  },
  {
    "objectID": "blog/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions/index.html",
    "href": "blog/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions/index.html",
    "title": "Data frame columns as arguments to dplyr functions",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2023-07-30-nix_for_r_part3/index.html",
    "href": "blog/2023-07-30-nix_for_r_part3/index.html",
    "title": "Reproducible data science with Nix, part 3 – frictionless {plumber} api deployments with Nix",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-10-05-ggplot2_purrr_officer/index.html",
    "href": "blog/2018-10-05-ggplot2_purrr_officer/index.html",
    "title": "Exporting editable plots from R to Powerpoint: making ggplot2 purrr with officer",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2017-12-17-teaching_tidyverse/index.html",
    "href": "blog/2017-12-17-teaching_tidyverse/index.html",
    "title": "Teaching the tidyverse to beginners",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-04-04-chron_post/index.html",
    "href": "blog/2022-04-04-chron_post/index.html",
    "title": "The {chronicler} package, an implementation of the logger monad in R",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2021-06-04-own_knit_server/index.html",
    "href": "blog/2021-06-04-own_knit_server/index.html",
    "title": "Building your own knitr compile farm on your Raspberry Pi with {plumber}",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2023-03-18-docxtractr/index.html",
    "href": "blog/2023-03-18-docxtractr/index.html",
    "title": "Automating checks of handcrafted Word tables with {docxtractr}",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-07-19-statmatch/index.html",
    "href": "blog/2019-07-19-statmatch/index.html",
    "title": "Statistical matching, or when one single data source is not enough",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-05-18-cran_0_2_0/index.html",
    "href": "blog/2022-05-18-cran_0_2_0/index.html",
    "title": "chronicler is now available on CRAN",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-03-03-sparklyr_h2o_rsparkling/index.html",
    "href": "blog/2018-03-03-sparklyr_h2o_rsparkling/index.html",
    "title": "Getting {sparklyr}, {h2o}, {rsparkling} to work together and some fun with bash",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2017-01-07-my-free-book-has-a-cover/index.html",
    "href": "blog/2017-01-07-my-free-book-has-a-cover/index.html",
    "title": "My free book has a cover!",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2023-07-19-nix_for_r_part2/index.html",
    "href": "blog/2023-07-19-nix_for_r_part2/index.html",
    "title": "Reproducible data science with Nix, part 2 – running {targets} pipelines with Nix",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-11-01-nethack/index.html",
    "href": "blog/2018-11-01-nethack/index.html",
    "title": "From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2017-03-08-lesser_known_tricks/index.html",
    "href": "blog/2017-03-08-lesser_known_tricks/index.html",
    "title": "Lesser known dplyr tricks",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-10-29-mkusb_minp/index.html",
    "href": "blog/2022-10-29-mkusb_minp/index.html",
    "title": "A Linux Live USB as a statistical programming dev environment",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-08-14-lpm/index.html",
    "href": "blog/2019-08-14-lpm/index.html",
    "title": "Using linear models with binary dependent variables, a simulation study",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-02-10-stringr_package/index.html",
    "href": "blog/2019-02-10-stringr_package/index.html",
    "title": "Manipulating strings with the {stringr} package",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2017-07-27-spread_rename_at/index.html",
    "href": "blog/2017-07-27-spread_rename_at/index.html",
    "title": "tidyr::spread() and dplyr::rename_at() in action",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2017-11-14-peace_r/index.html",
    "href": "blog/2017-11-14-peace_r/index.html",
    "title": "Peace of mind with purrr",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-11-19-raps/index.html",
    "href": "blog/2022-11-19-raps/index.html",
    "title": "Reproducibility with Docker and Github Actions for the average R enjoyer",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-07-23-grepl_vs_stringi/index.html",
    "href": "blog/2022-07-23-grepl_vs_stringi/index.html",
    "title": "What’s the fastest way to search and replace strings in a data frame?",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2023-10-05-repro_overview/index.html",
    "href": "blog/2023-10-05-repro_overview/index.html",
    "title": "An overview of what’s out there for reproducibility with R",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-09-03-disk_frame/index.html",
    "href": "blog/2019-09-03-disk_frame/index.html",
    "title": "{disk.frame} is epic",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2023-06-29-book_quarto/index.html",
    "href": "blog/2023-06-29-book_quarto/index.html",
    "title": "How to self-publish a technical book on Leanpub and Amazon using Quarto",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-02-18-loudly/index.html",
    "href": "blog/2022-02-18-loudly/index.html",
    "title": "Add logging to your functions using my newest package {loud}",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-11-02-mice_exp/index.html",
    "href": "blog/2019-11-02-mice_exp/index.html",
    "title": "Multiple data imputation and explainability",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-03-12-keep_trying/index.html",
    "href": "blog/2018-03-12-keep_trying/index.html",
    "title": "Keep trying that api call with purrr::possibly()",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2017-10-26-margins_r/index.html",
    "href": "blog/2017-10-26-margins_r/index.html",
    "title": "Easy peasy STATA-like marginal effects with R",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2020-12-30-year_review/index.html",
    "href": "blog/2020-12-30-year_review/index.html",
    "title": "A year in review",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-04-15-announcing_pmice/index.html",
    "href": "blog/2018-04-15-announcing_pmice/index.html",
    "title": "{pmice}, an experimental package for missing data imputation in parallel using {mice} and {furrr}",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2016-06-21-careful-with-trycatch/index.html",
    "href": "blog/2016-06-21-careful-with-trycatch/index.html",
    "title": "Careful with tryCatch",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2023-07-13-nix_for_r_part1/index.html",
    "href": "blog/2023-07-13-nix_for_r_part1/index.html",
    "title": "Reproducible data science with Nix, part 1 – what is Nix",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2024-04-04-nix_for_r_part_11/index.html",
    "href": "blog/2024-04-04-nix_for_r_part_11/index.html",
    "title": "Reproducible data science with Nix, part 11 – build and cache binaries with Github Actions and Cachix",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2021-09-04-quest_fast/index.html",
    "href": "blog/2021-09-04-quest_fast/index.html",
    "title": "The quest for fast(er?) row-oriented workflows",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-11-10-nethack_analysis_part2/index.html",
    "href": "blog/2018-11-10-nethack_analysis_part2/index.html",
    "title": "Analyzing NetHack data, part 2: What players kill the most",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-12-30-reticulate/index.html",
    "href": "blog/2018-12-30-reticulate/index.html",
    "title": "R or Python? Why not both? Using Anaconda Python within R with {reticulate}",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-04-07-historical_newspaper_scraping_tesseract/index.html",
    "href": "blog/2019-04-07-historical_newspaper_scraping_tesseract/index.html",
    "title": "Historical newspaper scraping with {tesseract} and R",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2020-04-27-nace_explorer/index.html",
    "href": "blog/2020-04-27-nace_explorer/index.html",
    "title": "Exploring NACE codes",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2023-01-12-repro_R/index.html",
    "href": "blog/2023-01-12-repro_R/index.html",
    "title": "MRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-06-10-scraping_pdfs/index.html",
    "href": "blog/2018-06-10-scraping_pdfs/index.html",
    "title": "Getting data from pdfs using the pdftools package",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-07-08-rob_stderr/index.html",
    "href": "blog/2018-07-08-rob_stderr/index.html",
    "title": "Dealing with heteroskedasticity; regression with robust standard errors using R",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2020-09-20-shiny_raspberry/index.html",
    "href": "blog/2020-09-20-shiny_raspberry/index.html",
    "title": "The Raspberry Pi 4B as a shiny server",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2013-12-31-r-cas/index.html",
    "href": "blog/2013-12-31-r-cas/index.html",
    "title": "Using R as a Computer Algebra System with Ryacas",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2023-12-19-nix_for_r_part_8/index.html",
    "href": "blog/2023-12-19-nix_for_r_part_8/index.html",
    "title": "Reproducible data science with Nix, part 8 – nixpkgs, a tale of the magic of free and open source software and a call for charity",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2023-09-15-nix_for_r_part5/index.html",
    "href": "blog/2023-09-15-nix_for_r_part5/index.html",
    "title": "Reproducible data science with Nix, part 5 – Reproducible literate programming with Nix and Quarto",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-11-16-rgenoud_arima/index.html",
    "href": "blog/2018-11-16-rgenoud_arima/index.html",
    "title": "Using a genetic algorithm for the hyperparameter optimization of a SARIMA model",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-05-18-xml2/index.html",
    "href": "blog/2019-05-18-xml2/index.html",
    "title": "For posterity: install {xml2} on GNU/Linux distros",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2021-07-30-worth_weight/index.html",
    "href": "blog/2021-07-30-worth_weight/index.html",
    "title": "Is it worth the weight?",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2020-02-23-synthpop/index.html",
    "href": "blog/2020-02-23-synthpop/index.html",
    "title": "Synthetic micro-datasets: a promising middle ground between data privacy and data analysis",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2013-01-16-simulated-maximum-likelihood-with-r/index.html",
    "href": "blog/2013-01-16-simulated-maximum-likelihood-with-r/index.html",
    "title": "Simulated Maximum Likelihood with R",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2020-12-05-poorman_translate/index.html",
    "href": "blog/2020-12-05-poorman_translate/index.html",
    "title": "Poorman’s automated translation with R and Google Sheets using {googlesheets4}",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2015-01-12-introduction-to-programming-econometrics-with-r/index.html",
    "href": "blog/2015-01-12-introduction-to-programming-econometrics-with-r/index.html",
    "title": "Introduction to programming econometrics with R",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-01-31-newspapers_shiny_app/index.html",
    "href": "blog/2019-01-31-newspapers_shiny_app/index.html",
    "title": "Using Data Science to read 10 years of Luxembourguish newspapers from the 19th century",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-03-12-purely/index.html",
    "href": "blog/2022-03-12-purely/index.html",
    "title": "Capture errors, warnings and messages",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-09-15-time_use/index.html",
    "href": "blog/2018-09-15-time_use/index.html",
    "title": "How Luxembourguish residents spend their time: a small {flexdashboard} demo using the Time use survey data",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-04-14-playing_with_furrr/index.html",
    "href": "blog/2018-04-14-playing_with_furrr/index.html",
    "title": "Imputing missing values in parallel using {furrr}",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2021-02-20-covid_paper/index.html",
    "href": "blog/2021-02-20-covid_paper/index.html",
    "title": "R makes it too easy to write papers",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2020-09-27-golemDemo/index.html",
    "href": "blog/2020-09-27-golemDemo/index.html",
    "title": "Building apps with {shinipsum} and {golem}",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-11-30-pipelines-as/index.html",
    "href": "blog/2022-11-30-pipelines-as/index.html",
    "title": "Functional programming explains why containerization is needed for reproducibility",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-08-17-modern_R/index.html",
    "href": "blog/2019-08-17-modern_R/index.html",
    "title": "Modern R with the tidyverse is available on Leanpub",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2014-11-11-benchmarks-r-blas-atlas-rro/index.html",
    "href": "blog/2014-11-11-benchmarks-r-blas-atlas-rro/index.html",
    "title": "R, R with Atlas, R with OpenBLAS and Revolution R Open: which is fastest?",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2024-02-02-nix_for_r_part_9/index.html",
    "href": "blog/2024-02-02-nix_for_r_part_9/index.html",
    "title": "Reproducible data science with Nix, part 9 – rix is looking for testers!",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2024-02-29-nix_for_r_part_10/index.html",
    "href": "blog/2024-02-29-nix_for_r_part_10/index.html",
    "title": "Reproducible data science with Nix, part 10 – contributing to nixpkgs",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2020-02-08-harold_part2/index.html",
    "href": "blog/2020-02-08-harold_part2/index.html",
    "title": "Dynamic discrete choice models, reinforcement learning and Harold, part 2",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-10-31-optim_shiny/index.html",
    "href": "blog/2022-10-31-optim_shiny/index.html",
    "title": "How to deal with annoying medium sized data inside a Shiny app",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2020-03-08-tidymodels/index.html",
    "href": "blog/2020-03-08-tidymodels/index.html",
    "title": "Machine learning with {tidymodels}",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-12-15-lubridate_africa/index.html",
    "href": "blog/2018-12-15-lubridate_africa/index.html",
    "title": "Manipulate dates easily with {lubridate}",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-07-01-tidy_ive/index.html",
    "href": "blog/2018-07-01-tidy_ive/index.html",
    "title": "Missing data imputation and instrumental variables regression: the tidy approach",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-11-14-luxairport/index.html",
    "href": "blog/2018-11-14-luxairport/index.html",
    "title": "Easy time-series prediction with R: a tutorial with air traffic data from Lux Airport",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-05-15-self_doc_ggplot/index.html",
    "href": "blog/2022-05-15-self_doc_ggplot/index.html",
    "title": "Self-documenting {ggplot}s thanks to the power of monads!",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2017-03-24-lesser_known_purrr/index.html",
    "href": "blog/2017-03-24-lesser_known_purrr/index.html",
    "title": "Lesser known purrr tricks",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-10-05-parallel_maxlik/index.html",
    "href": "blog/2019-10-05-parallel_maxlik/index.html",
    "title": "Split-apply-combine for Maximum Likelihood Estimation of a linear model",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2020-12-12-ethics_statistics/index.html",
    "href": "blog/2020-12-12-ethics_statistics/index.html",
    "title": "(Half) Lies, (half) truths and (half) statistics",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2015-02-22-export-r-output-to-file/index.html",
    "href": "blog/2015-02-22-export-r-output-to-file/index.html",
    "title": "Export R output to a file",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-11-16-open_source_repro/index.html",
    "href": "blog/2022-11-16-open_source_repro/index.html",
    "title": "Open source is a hard requirement for reproducibility",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2020-04-12-basic_ggplot2/index.html",
    "href": "blog/2020-04-12-basic_ggplot2/index.html",
    "title": "How to basic: bar plots",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-11-06-explainability_econometrics/index.html",
    "href": "blog/2019-11-06-explainability_econometrics/index.html",
    "title": "Intrumental variable regression and machine learning",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2016-03-31-unit-testing-with-r/index.html",
    "href": "blog/2016-03-31-unit-testing-with-r/index.html",
    "title": "Unit testing with R",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2020-09-05-tidytable/index.html",
    "href": "blog/2020-09-05-tidytable/index.html",
    "title": "Gotta go fast with “{tidytable}”",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Econometrics and Free Software",
    "section": "",
    "text": "Welcome to my blog where I talk about R, Nix, Econometrics and Data Science. If you enjoy reading what I write, you might enjoy my books or want to follow me on Mastodon or Twitter or Bluesky. If you are 40+, click here instead. I also make videos on youtube.\n\n\n\n\n\n\n2025\n\n\n\nWhy we forked nixpkgs\n\n\nUsing options() to inject a function’s internal variable for reproducible testing\n\n\nNew year, new blog\n\n\n\n2024\n\n\n\nReproducible data science with Nix, part 9 – rix is looking for testers!\n\n\nReproducible data science with Nix, part 13 – {rix} is on CRAN!\n\n\nReproducible data science with Nix, part 12 – Nix as a polyglot build automation tool for data science\n\n\nReproducible data science with Nix, part 11 – build and cache binaries with Github Actions and Cachix\n\n\nReproducible data science with Nix, part 10 – contributing to nixpkgs\n\n\n\n2023\n\n\n\nZSA Voyager review\n\n\nWhy you should consider working on a dockerized development environment\n\n\nWhat I’ve learned making an .epub Ebook with Quarto\n\n\nSoftware engineering techniques that non-programmers who write a lot of code can benefit from — the DRY WIT approach\n\n\nReproducible data science with Nix, part 8 – nixpkgs, a tale of the magic of free and open source software and a call for charity\n\n\nReproducible data science with Nix, part 7 – Building a Quarto book using Nix on Github Actions\n\n\nReproducible data science with Nix, part 6 – CI/CD has never been easier\n\n\nReproducible data science with Nix, part 5 – Reproducible literate programming with Nix and Quarto\n\n\nReproducible data science with Nix, part 4 – So long, {renv} and Docker, and thanks for all the fish\n\n\nReproducible data science with Nix, part 3 – frictionless {plumber} api deployments with Nix\n\n\nReproducible data science with Nix, part 2 – running {targets} pipelines with Nix\n\n\nReproducible data science with Nix, part 1 – what is Nix\n\n\nMRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?\n\n\nI’ve been blogging for 10 years\n\n\nHow to self-publish a technical book on Leanpub and Amazon using Quarto\n\n\nAutomating checks of handcrafted Word tables with {docxtractr}\n\n\nAn overview of what’s out there for reproducibility with R\n\n\n\n2022\n\n\n\nchronicler is now available on CRAN\n\n\nWhy you should(n’t) care about Monads if you’re an R programmer\n\n\nWhy and how to use JS in your Shiny app\n\n\nWhat’s the fastest way to search and replace strings in a data frame?\n\n\nThe {chronicler} package, an implementation of the logger monad in R\n\n\nSome learnings from functional programming you can use to write safer programs\n\n\nSelf-documenting {ggplot}s thanks to the power of monads!\n\n\nReproducibility with Docker and Github Actions for the average R enjoyer\n\n\nR, its license and my take on it\n\n\nR will always be arcane to those who do not make a serious effort to learn it…\n\n\nOpen source is a hard requirement for reproducibility\n\n\nHow to deal with annoying medium sized data inside a Shiny app\n\n\nGet packages that introduce unique syntax adopted less?\n\n\nFunctional programming explains why containerization is needed for reproducibility\n\n\nCode longevity of the R programming language\n\n\nCapture errors, warnings and messages\n\n\nAdd logging to your functions using my newest package {loud}\n\n\nA Linux Live USB as a statistical programming dev environment\n\n\n\n2021\n\n\n\nUsing explainability methods to understand (some part) of the spread of COVID-19 in a landlocked country\n\n\nThe quest for fast(er?) row-oriented workflows\n\n\nThe link between keyboard layouts and typing speed - Data collection phase\n\n\nSpeedrunning row-oriented workflows\n\n\nServer(shiny)-less dashboards with R, {htmlwidgets} and {crosstalk}\n\n\nR makes it too easy to write papers\n\n\nIs it worth the weight?\n\n\nHow to write code that returns (Rmarkdown) code\n\n\nHow to treat as many files as fit on your hard disk without loops (sorta) nor running out of memory all the while being as lazy as possible\n\n\nHow to draw a map of arbitrary contiguous regions, or visualizing the spread of COVID-19 in the Greater Region\n\n\nDealing with non-representative samples with post-stratification\n\n\nBuilding your own knitr compile farm on your Raspberry Pi with {plumber}\n\n\n\n2020\n\n\n\nWhat would a keyboard optimised for Luxembourguish look like?\n\n\nThe Raspberry Pi 4B as a shiny server\n\n\nSynthetic micro-datasets: a promising middle ground between data privacy and data analysis\n\n\nPoorman’s automated translation with R and Google Sheets using {googlesheets4}\n\n\nNo excuse not to be a Bayesian anymore\n\n\nMachine learning with {tidymodels}\n\n\nIt’s time to retire the “data scientist” label\n\n\nHow to basic: bar plots\n\n\nGraphical User Interfaces were a mistake but you can still make things right\n\n\nGotta go fast with “{tidytable}”\n\n\nExploring NACE codes\n\n\nExplainbility of {tidymodels} models with {iml}\n\n\nDynamic discrete choice models, reinforcement learning and Harold, part 2\n\n\nDynamic discrete choice models, reinforcement learning and Harold, part 1\n\n\nBuilding apps with {shinipsum} and {golem}\n\n\nA year in review\n\n\n(Half) Lies, (half) truths and (half) statistics\n\n\n\n2019\n\n\n\n{disk.frame} is epic\n\n\nUsing linear models with binary dependent variables, a simulation study\n\n\nUsing cosine similarity to find matching documents: a tutorial using Seneca’s letters to his friend Lucilius\n\n\nUsing Data Science to read 10 years of Luxembourguish newspapers from the 19th century\n\n\nThe never-ending editor war (?)\n\n\nStatistical matching, or when one single data source is not enough\n\n\nSplit-apply-combine for Maximum Likelihood Estimation of a linear model\n\n\nPivoting data frames just got easier thanks to pivot_wide() and pivot_long()\n\n\nMultiple data imputation and explainability\n\n\nModern R with the tidyverse is available on Leanpub\n\n\nManipulating strings with the {stringr} package\n\n\nMaking sense of the METS and ALTO XML standards\n\n\nLooking into 19th century ads from a Luxembourguish newspaper with R\n\n\nIntrumental variable regression and machine learning\n\n\nIntermittent demand, Croston and Die Hard\n\n\nHistorical newspaper scraping with {tesseract} and R\n\n\nGet text from pdfs or images using OCR: a tutorial with {tesseract} and {magick}\n\n\nFor posterity: install {xml2} on GNU/Linux distros\n\n\nFast food, causality and R packages, part 2\n\n\nFast food, causality and R packages, part 1\n\n\nCurly-Curly, the successor of Bang-Bang\n\n\nCluster multiple time series using K-means\n\n\nClassification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2\n\n\nClassification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 1\n\n\nBuilding a shiny app to explore historical newspapers: a step-by-step guide\n\n\n\n2018\n\n\n\n{pmice}, an experimental package for missing data imputation in parallel using {mice} and {furrr}\n\n\nWhat hyper-parameters are, and what to do with them; an illustration with ridge regression\n\n\nUsing the tidyverse for more than data manipulation: estimating pi with Monte Carlo methods\n\n\nUsing a genetic algorithm for the hyperparameter optimization of a SARIMA model\n\n\nThe year of the GNU+Linux desktop is upon us: using user ratings of Steam Play compatibility to play around with regex and the tidyverse\n\n\nThe best way to visit Luxembourguish castles is doing data science + combinatorial optimization\n\n\nSome fun with {gganimate}\n\n\nSearching for the optimal hyper-parameters of an ARIMA model in parallel: the tidy gridsearch approach\n\n\nR or Python? Why not both? Using Anaconda Python within R with {reticulate}\n\n\nPredicting job search by training a random forest on an unbalanced dataset\n\n\nObjects types and some useful R functions for beginners\n\n\nMissing data imputation and instrumental variables regression: the tidy approach\n\n\nMaps with pie charts on top of each administrative division: an example with Luxembourg’s elections data\n\n\nMapping a list of functions to a list of datasets with a list of columns as arguments\n\n\nManipulate dates easily with {lubridate}\n\n\nKeep trying that api call with purrr::possibly()\n\n\nIt’s lists all the way down, part 2: We need to go deeper\n\n\nIt’s lists all the way down\n\n\nImputing missing values in parallel using {furrr}\n\n\nImporting 30GB of data into R with sparklyr\n\n\nHow Luxembourguish residents spend their time: a small {flexdashboard} demo using the Time use survey data\n\n\nGoing from a human readable Excel file to a machine-readable csv with {tidyxl}\n\n\nGetting {sparklyr}, {h2o}, {rsparkling} to work together and some fun with bash\n\n\nGetting the data from the Luxembourguish elections out of Excel\n\n\nGetting data from pdfs using the pdftools package\n\n\nGet basic summary statistics for all the variables in a data frame\n\n\nFrom webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack\n\n\nForecasting my weight with R\n\n\nExporting editable plots from R to Powerpoint: making ggplot2 purrr with officer\n\n\nEasy time-series prediction with R: a tutorial with air traffic data from Lux Airport\n\n\nDealing with heteroskedasticity; regression with robust standard errors using R\n\n\nAnalyzing NetHack data, part 2: What players kill the most\n\n\nAnalyzing NetHack data, part 1: What kills the players\n\n\nA tutorial on tidy cross-validation with R\n\n\n\n2017\n\n\n\ntidyr::spread() and dplyr::rename_at() in action\n\n\nWhy I find tidyeval useful\n\n\nTeaching the tidyverse to beginners\n\n\nPeace of mind with purrr\n\n\nMy free book has a cover!\n\n\nMake ggplot2 purrr\n\n\nLesser known purrr tricks\n\n\nLesser known dplyr tricks\n\n\nLesser known dplyr 0.7* tricks\n\n\nIntroducing brotools\n\n\nHow to use jailbreakr\n\n\nEasy peasy STATA-like marginal effects with R\n\n\nBuilding formulae\n\n\n\n2016\n\n\n\nWork on lists of datasets instead of individual datasets by using functional programming\n\n\nUnit testing with R\n\n\nRead a lot of datasets at once with R\n\n\nMerge a list of datasets together\n\n\nI’ve started writing a ‘book’: Functional programming and unit testing for data munging with R\n\n\nFunctional programming and unit testing for data munging with R available on Leanpub\n\n\nData frame columns as arguments to dplyr functions\n\n\nCareful with tryCatch\n\n\n\n2015\n\n\n\nUpdate to Introduction to programming econometrics with R\n\n\nIntroduction to programming econometrics with R\n\n\nExport R output to a file\n\n\nBootstrapping standard errors for difference-in-differences estimation with R\n\n\n\n2014\n\n\n\nR, R with Atlas, R with OpenBLAS and Revolution R Open: which is fastest?\n\n\nObject Oriented Programming with R: An example with a Cournot duopoly\n\n\n\n2013\n\n\n\nUsing R as a Computer Algebra System with Ryacas\n\n\nSimulated Maximum Likelihood with R\n\n\nNonlinear Gmm with R - Example with a logistic regression\n\n\nMethod of Simulated Moments with R\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-09-20-nix_for_r_part6.html",
    "href": "posts/2023-09-20-nix_for_r_part6.html",
    "title": "Reproducible data science with Nix, part 6 – CI/CD has never been easier",
    "section": "",
    "text": "Warning: I highly recommend you read this blog post first, which will explain how to run a pipeline inside Nix in detail. This blog post will assume that you’ve read that one, and it would also help if you’re familiar with Github Actions, if not, read this other blog post of mine as well\n\n\nThis is getting ridiculous. The meme that I’m using as a header for this blog post perfectly summaries how I feel.\n\n\nThis will be a short blog post, because Nix makes things so easy that there’s not much to say. I wanted to try how I could use Nix on Github Actions to run a reproducible pipeline. This pipeline downloads some data, prepares it, and fits a machine learning model. It is code that I had laying around from an old video on the now deprecated {drake} package, {targets} predecessor.\n\n\nYou can find the pipeline here and you can also take a look at the same pipeline but which uses Docker here for comparison purposes.\n\n\nWhat I wanted to achieve was the following: I wanted to set up a reproducible environment with Nix on my computer, work on my pipeline locally, and then have it run on Github Actions as well. But I wanted my pipeline to run exactly on the same environment as the one I was using to develop it. In a world without Nix, this means using a mix of {renv} (or {groundhog} or {rang}) and a Docker image that ships the right version of R. I would then need to write a Github Actions workflow file that builds that Docker image, then runs it and saves the outputs as artifacts. Also, in practice that image would not be exactly the same as my local environment: I would have the same version of R and R packages, but every other system-level dependency would be a different version unless I use that Dockerized environment to develop locally, something I suggested you should do merely 4 months ago (oooh, how blind was I!).\n\n\nWith Nix, not only can I take care of the version of R and R packages with one single tool but also every underlying system-level dependency gets handled by Nix. So if I use a package that requires, say, Java, or GDAL, or any other of these usual suspects that make installing their R bindings so tricky, Nix will handle this for me without any intervention on my part. I can also use this environment to develop locally, and then, once I’m done working locally, exactly this environment, exactly every bit of that environment, will get rebuilt and used to run my code on Github Actions.\n\n\nSo this is the repository where you can find the code. There’s a {targets} script that defines the pipeline and a functions/ folder with some code that I wrote for said pipeline. What’s unfamiliar to you (unless you’ve been reading my Nix adventures since the beginning) is the default.nix file:\n\nlet\n pkgs = import (fetchTarball \"https://github.com/NixOS/nixpkgs/archive/976fa3369d722e76f37c77493d99829540d43845.tar.gz\") {};\n rpkgs = builtins.attrValues {\n  inherit (pkgs.rPackages) tidymodels vetiver targets xgboost;\n};\n system_packages = builtins.attrValues {\n  inherit (pkgs) R;\n};\nin\n pkgs.mkShell {\n  buildInputs = [  rpkgs system_packages  ];\n }\n\nThis few lines of code define an environment that pulls packages from revision 976fa3369d722e76f37c77493d99829540d43845 of nixpkgs. It installs the packages {tidymodels}, {vetiver}, {targets} and {xgboost} (actually, I’m not using {vetiver} for this yet, so it could even be removed). Then it also installs R. Because we’re using that specific revision of Nix, exactly the same packages (and their dependencies) will get installed, regardless of when we build this environment. I want to insist that this file is 12 lines long and it defines a complete environment. The equivalent Dockerfile is much longer, and not even completely reproducible, and I would have needed external tools like {renv} (or use the Posit CRAN mirror dated snapshots) as you can check out here.\n\n\nLet’s now turn our attention to the workflow file:\n\nname: train_model\n\non:\n  push:\n    branches: [main]\n\njobs:\n  targets:\n    runs-on: ubuntu-latest\n    env:\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n    steps:\n\n      - uses: actions/checkout@v3\n\n      - name: Install Nix\n        uses: DeterminateSystems/nix-installer-action@main\n        with:\n          logger: pretty\n          log-directives: nix_installer=trace\n          backtrace: full\n\n      - name: Nix cache\n        uses: DeterminateSystems/magic-nix-cache-action@main\n\n      - name: Build development environment\n        run: |\n          nix-build\n\n      - name: Check if previous runs exists\n        id: runs-exist\n        run: git ls-remote --exit-code --heads origin targets-runs\n        continue-on-error: true\n\n      - name: Checkout previous run\n        if: steps.runs-exist.outcome == 'success'\n        uses: actions/checkout@v2\n        with:\n          ref: targets-runs\n          fetch-depth: 1\n          path: .targets-runs\n\n      - name: Restore output files from the previous run\n        if: steps.runs-exist.outcome == 'success'\n        run: |\n          nix-shell default.nix --run \"Rscript -e 'for (dest in scan(\\\".targets-runs/.targets-files\\\", what = character())) {\n            source &lt;- file.path(\\\".targets-runs\\\", dest)\n            if (!file.exists(dirname(dest))) dir.create(dirname(dest), recursive = TRUE)\n            if (file.exists(source)) file.rename(source, dest)\n          }'\"\n\n      - name: Run model\n        run: |\n          nix-shell default.nix --run \"Rscript -e 'targets::tar_make()'\"\n\n      - name: Identify files that the targets pipeline produced\n        run: git ls-files -mo --exclude=renv &gt; .targets-files\n\n      - name: Create the runs branch if it does not already exist\n        if: steps.runs-exist.outcome != 'success'\n        run: git checkout --orphan targets-runs\n\n      - name: Put the worktree in the runs branch if the latter already exists\n        if: steps.runs-exist.outcome == 'success'\n        run: |\n          rm -r .git\n          mv .targets-runs/.git .\n          rm -r .targets-runs\n\n      - name: Upload latest run\n        run: |\n          git config --local user.name \"GitHub Actions\"\n          git config --local user.email \"actions@github.com\"\n          rm -r .gitignore .github/workflows\n          git add --all -- ':!renv'\n          for file in $(git ls-files -mo --exclude=renv)\n          do\n            git add --force $file\n          done\n          git commit -am \"Run pipeline\"\n          git push origin targets-runs\n\n      - name: Prepare failure artifact\n        if: failure()\n        run: rm -rf .git .github .targets-files .targets-runs\n\n      - name: Post failure artifact\n        if: failure()\n        uses: actions/upload-artifact@main\n        with:\n          name: ${{ runner.os }}-r${{ matrix.config.r }}-results\n          path: .\n\nThe workflow file above is heavily inspired from the one you get when you run targets::tar_github_actions(). Running this puts the following file on the root of your {targets} project. This file is a Github Actions workflow file, which means that each time you push your code on Github, the pipeline will run in the cloud. However it needs you to use {renv} with the project so that the right packages get installed. You’ll also see a step called Install Linux dependencies which you will have to adapt to your project.\n\n\nAll of this can be skipped when using Nix. All that must be done is installing Nix itself, using the nix-installer-action from Determinate Systems, then using the magic-nix-cache-action which caches the downloaded packages so we don’t need to wait for the environment to build each time we push (unless we changed the environment of course) and that’s about it. We then build the environment on Github Actions using nix-build and then run the pipeline using nix-shell default.nix –run “Rscript -e ‘targets::tar_make()’”. All the other steps are copied almost verbatim from the linked file above and make sure that the computed targets only get recomputed if I edit anything that impacts them, and also that they get pushed into a branch called targets-runs. I say copied almost verbatim because some steps must run inside R, so we need to specify that we want to use the R that is available through the Nix environment we just built.\n\n\nNow, each time we push, the following happens:\n\n\n\nif we didn’t change anything to default.nix, the environment gets retrieved from the cache. If we did change something, then environment gets rebuilt (or rather, only the parts that need to be rebuilt, the rest will still get retrieved from the cache)\n\n\nif we didn’t change anything to the _targets.R pipeline itself, then every target will get skipped. If not, only the targets that need to get recomputed will get recomputed.\n\n\n\nOne last thing that I didn’t mention: on line 9 you’ll see this:\n\nruns-on: ubuntu-latest\n\nthis means that the Github Actions will run on the latest available version of Ubuntu, which is obviously not fixed. When the next LTS gets released in April 2024, this pipeline will be running on Ubuntu 24.04 instead of the current LTS, version 22.04. This is not good practice because we don’t want the underlying operating system to be changing, because this could have an impact on the reproducibility of our pipeline. But with Nix, this does not matter. Remember that we are using a specific revision of nixpkgs for our pipeline, so the exact same version of not only R and R packages gets installed, but every underlying piece of software that needs to be available will be installed as well. We could be running this in 50 years on Ubuntu LTS 74.04 and it would still install the same stuff and run the same code and produce exactly the same results.\n\n\nThis is really bonkers.\n\n\nNix is an incredibly powerful tool. I’ve been exploring and using it for 3 months now, but if something impresses me more than how useful it is, is how terribly unknown it still is. I hope that this series of blog posts will motivate other people to learn it."
  },
  {
    "objectID": "posts/2022-10-29-mkusb_minp.html",
    "href": "posts/2022-10-29-mkusb_minp.html",
    "title": "A Linux Live USB as a statistical programming dev environment",
    "section": "",
    "text": "This blog post is divided in two parts: in the first part I’ll show you how to create a Linux Live USB with persistent storage that can be used as development environment, and in the second part I’ll show you the easiest way to set up RStudio and R in Ubuntu."
  },
  {
    "objectID": "posts/2022-10-29-mkusb_minp.html#making-your-own-portable-development-environment-based-on-ubuntu-or-debian",
    "href": "posts/2022-10-29-mkusb_minp.html#making-your-own-portable-development-environment-based-on-ubuntu-or-debian",
    "title": "A Linux Live USB as a statistical programming dev environment",
    "section": "\nMaking your own, portable, development environment based on Ubuntu or Debian\n",
    "text": "Making your own, portable, development environment based on Ubuntu or Debian\n\n\nI’m currently teaching a course at the University of Luxembourg, which focuses on setting up reproducible analytical pipelines (if you’re interested, you can find the course notes here).\n\n\nThe problem is that my work laptop runs Windows, and I didn’t want to teach on Windows since I make heavy use of the command line. Plus I don’t have admin rights on this machine, so installing what I needed would have been a pain. I also don’t have a personal laptop, so I use my wife’s laptop. However, the laptop is completely full of pictures of our kids, so I couldn’t install what I needed… This is when I thought about making a persistent live USB with Kubuntu on it (Kubuntu is a variant of Ubuntu with KDE as the desktop manager instead of Gnome) with all the software I needed (R, Quarto, RStudio basically). It works quite well, and was also quite easy to do. But what is a live USB anyways? A live USB is a full Linux installation on a USB stick, that you can use to test different Linux distributions or even to install said distribution on computers.\n\n\nThe first step is to get a USB stick. Those are quite cheap nowadays, but you’ll need at least one with 8GB of space, and ideally USB 3 (you probably can’t find any USB 2 these days anyways). I’ve bought a 32GB one for 10€.\n\n\nThen, we need to install Ubuntu on it. I’ll be using Kubuntu 22.04, which is an LTS release. I would always recommend an LTS release for something like crafting a development environment. So if you’re reading this in the future, and there’s a new LTS (could be 24.04, 26.04, etc), you’d need to get that one.\n\n\nCreating a live USB is quite simple, but the issue if you create a live USB using the standard methods is that whatever you do on it once you’re logged in will get erased after rebooting. A persistent live USB, I’m sure you’ve guessed it, keeps your changes even after rebooting, which means that you basically end up with a portable development environment. Note however that only Ubuntu (and variants) or Debian can be used to create persistent live USBs.\n\n\nYou can create persistent live USB from another Linux distro, Windows or macOS.\n\n\nIf you’re already running Ubuntu on your pc, you might want to take a look at this page. You’ll need to install a tool called mkusb. If you’re not running Ubuntu, but find this tool in your distribution’s package manager, I guess you’re good to go as well. In my case, I’m running opensuse tumbleweed, and could not find this program in the opensuse’s repositories. So I’ve used this guide that shows how to achieve the same thing using a very simple to use shell script which you can get here called mkusb-minp. So in my case, I simply had to stick the USB stick in my computer, find out where it was mounted by running df in bash (in my case it was in /dev/sdd), download Kubuntu’s iso image and run the following in my terminal:\n\nsudo ./mkusb-minp -p kubuntu-22.04.1-desktop-amd64.iso /dev/sdX\n\n(/dev/sdX: replace the X by the right letter, for me it was /dev/sdd)\n\n\nIf you’re using Windows, you can install Rufus to create a persistent live USB.\n\n\nIt would seem that for macOS the process is a bit more involved, but I’ve found this blog post that explains the process.\n\n\nOnce the process is finished, you can boot into your live USB key. For this, you might need to press delete or F2 when your computer starts booting to access the boot menu. You can then choose to boot from your USB device.\n\n\nWait a bit and at some point you should see a prompt asking you if you want to try or install Ubuntu. Choose Try Ubuntu:\n\n\n\n\n\n\n\nAnd then wait some minutes. Yes booting takes some time because you’re loading an entire operating system from a USB stick (hence why it’s a good idea to go with a USB 3 stick). After some time you should see a new window:\n\n\n\n\n\n\n\nOnce again, try Ubuntu, wait a bit, and that’s it you’re inside your dev environment!"
  },
  {
    "objectID": "posts/2022-10-29-mkusb_minp.html#setting-up-r-and-rstudio",
    "href": "posts/2022-10-29-mkusb_minp.html#setting-up-r-and-rstudio",
    "title": "A Linux Live USB as a statistical programming dev environment",
    "section": "\nSetting up R and RStudio\n",
    "text": "Setting up R and RStudio\n\n\nNow that you’re inside your dev environment, you actually need to start adding some tools. Let’s start by adding R. The easiest way that I found is to use the r2u project by Dirk Eddelbuettel. If you’re on Ubuntu 22.04, run this script, as explained in the tutorial. This will add the required repositories that will install binary versions of R packages in mere seconds. The script will also add a repository to install the most recent version of R, so once the script is done running, install R and the {tidyverse} (or any other package) with the following command:\n\nsudo apt install --no-install-recommends r-base r-cran-tidyverse\n\nYou can then install other packages from R using install.packages(“package_name”) as usual, and this will also make use of the r2u repositories.\n\n\nAll that’s missing now is RStudio (if you use RStudio). Surprisingly, when I set up my live USB two weeks ago, the current version of RStudio for Ubuntu would not install. This is apparently fixed with the daily versions which you can get here. But before that, do try to install the stable version. If you’re reading this sometime in the future, maybe the issue I encountered has been fixed. Download RStudio from here, and then double click on the downloaded .deb package. If you see this message:\n\nThe following packages have unmet dependencies:\n rstudio : Depends: libssl1.0.0 but it is not installable or\n                    libssl1.0.2 but it is not installable or\n                    libssl1.1 but it is not installable\n           Recommends: r-base (&gt;= 3.0.1) but it is not going to be installed\nE: Unable to correct problems, you have held broken packages.\n\nthen this means that the problem has not been fixed. In that case, run the following line to repair everything:\n\nsudo apt-get update --fix-missing\n\nThis should put you back into a clean state. So to continue, install a daily build from the link above. Simply click on the Ubuntu 22 button to download the daily. Unfortunately daily builds can be unstable and are usually used for testing purposes. So hopefully Posit will fix this soon.\n\n\nOf course, if you’re using the greatest IDE ever made instead of RStudio, you won’t have this issue.\n\n\nYou can now keep installing things, for example Quarto, or Python, or, or, or… there are no limits, and performance, as you would have noticed is great, because the operating system has access to all the resources from your machine. A persistent live USB is a great solution if you need a portable dev environment and don’t want/can’t use Docker for example."
  },
  {
    "objectID": "posts/2014-04-23-r-s4-rootfinding.html",
    "href": "posts/2014-04-23-r-s4-rootfinding.html",
    "title": "Object Oriented Programming with R: An example with a Cournot duopoly",
    "section": "",
    "text": "I started reading Applied Computational Economics & Finance by Mario J. Miranda and Paul L. Fackler. It is a very interesting book that I recommend to every one of my colleagues. The only issue I have with this book, is that the programming language they use is Matlab, which is proprietary. While there is a free as in freedom implementation of the Matlab language, namely Octave, I still prefer using R. In this post, I will illustrate one example the authors present in the book with R, using the package rootSolve. rootSolve implements Newtonian algorithms to find roots of functions; to specify the functions for which I want the roots, I use R's Object-Oriented Programming (OOP) capabilities to build a model that returns two functions. This is optional, but I found that it was a good example to illustrate OOP, even though simpler solutions exist, one of which was proposed by reddit user TheDrownedKraken (whom I thank) and will be presented at the end of the article.\n\n\nTheoretical background\n\n\nThe example is taken from Miranda's and Fackler's book, on page 35. The authors present a Cournot duopoly model. In a Cournot duopoly model, two firms compete against each other by quantities. Both produce a certain quantity of an homogenous good, and take the quantity produce by their rival as given.\n\n\nThe inverse demand of the good is :\n\n\\[P(q) = q^{-\\dfrac{1}{\\eta}}\\]\n\nthe cost function for firm i is:\n\n\\[C_i(q_i) = P(q_1+q_2)*q_i - C_i(q_i)\\]\n\nand the profit for firm i:\n\n\\[\\pi_i(q1,q2) = P(q_1+q_2)q_i - C_i(q_i)\\]\n\nThe optimality condition for firm i is thus:\n\n\\[\\dfrac{\\partial \\pi_i}{\\partial q_i} = (q1+q2)^{-\\dfrac{1}{\\eta}} - \\dfrac{1}{\\eta} (q_1+q_2)^{\\dfrac{-1}{\\eta-1}}q_i - c_iq_i=0.\\]\n\nImplementation in R\n\n\nIf we want to find the optimal quantities (q_1) and (q_2) we need to program the optimality condition and we could also use the jacobian of the optimality condition. The jacobian is generally useful to speed up the root finding routines. This is were OOP is useful. First let's create a new class, called Model:\n\n\nsetClass(Class = \"Model\", slots = list(OptimCond = \"function\", JacobiOptimCond = \"function\"))\n\n\nThis new class has two slots, which here are functions (in general slots are properties of your class); we need the model to return the optimality condition and the jacobian of the optimality condition.\n\n\nNow we can create a function which will return these two functions for certain values of the parameters, c and  of the model:\n\n\nmy_mod &lt;- function(eta, c) {\n    e = -1/eta\n\n    OptimCond &lt;- function(q) {\n        return(sum(q)^e + e * sum(q)^(e - 1) * q - diag(c) %*% q)\n    }\n\n    JacobiOptimCond &lt;- function(q) {\n      return(\n        (e * sum(q)^e) * array(1, c(2, 2)) +\n        (e * sum(q)^(e - 1)) * diag(1, 2) +\n        (e - 1) * e * sum(q)^(e - 2) * q * c(1, 1) - diag(c)\n      )\n    }\n\n    return(new(\"Model\", OptimCond = OptimCond, JacobiOptimCond = JacobiOptimCond))\n\n}\n\n\nThe function my_mod takes two parameters, eta and c and returns two functions, the optimality condition and the jacobian of the optimality condition. Both are now accessible via my_mod(eta=1.6,c = c(0.6,0.8))@OptimCond and my_mod(eta=1.6,c = c(0.6,0.8))@JacobiOptimCond respectively (and by specifying values for eta and c).\n\n\nNow, we can use the rootSolve package to get the optimal values (q_1) and (q_2)\n\n\nlibrary(\"rootSolve\")\n\nmultiroot(f = my_mod(eta = 1.6, c = c(0.6, 0.8))@OptimCond,\n          start = c(1, 1),\n          maxiter = 100,\n          jacfunc = my_mod(eta = 1.6, c = c(0.6, 0.8))@JacobiOptimCond)\n\n## $root\n## [1] 0.8396 0.6888\n## \n## $f.root\n##            [,1]\n## [1,] -2.220e-09\n## [2,]  9.928e-09\n## \n## $iter\n## [1] 4\n## \n## $estim.precis\n## [1] 6.074e-09\n\n\nAfter 4 iterations, we get that  and  are equal to 0.84 and 0.69 respectively, which are the same values as in the book!\n\n\nSuggestion by Reddit user, TheDrownedKraken\n\n\nI posted this blog post on the rstats subbreddit on www.reddit.com. I got a very useful comment by reddit member TheDrownedKraken which suggested the following approach, which doesn't need a new class to be build. I thank him for this. Here is his suggestion:\n\n\ngenerator &lt;- function(eta, a) {\n    e = -1/eta\n\n    OptimCond &lt;- function(q) {\n        return(sum(q)^e + e * sum(q)^(e - 1) * q - diag(a) %*% q)\n    }\n\n    JacobiOptimCond &lt;- function(q) {\n      return(\n        (e * sum(q)^e) * array(1, c(2, 2)) +\n        (e * sum(q)^(e - 1)) * diag(1, 2) +\n        (e - 1) * e * sum(q)^(e - 2) * q * c(1, 1) - diag(a)\n      )\n    }\n\n    return(list(OptimCond = OptimCond, JacobiOptimCond = JacobiOptimCond))\n\n}\n\nf.s &lt;- generator(eta = 1.6, a = c(0.6, 0.8))\n\nmultiroot(f = f.s$OptimCond, start = c(1, 1), maxiter = 100, jacfunc = f.s$JacobiOptimCond)\n\n## $root\n## [1] 0.8396 0.6888\n## \n## $f.root\n##            [,1]\n## [1,] -2.220e-09\n## [2,]  9.928e-09\n## \n## $iter\n## [1] 4\n## \n## $estim.precis\n## [1] 6.074e-09"
  },
  {
    "objectID": "posts/2021-03-02-no_shiny_dashboard.html",
    "href": "posts/2021-03-02-no_shiny_dashboard.html",
    "title": "Server(shiny)-less dashboards with R, {htmlwidgets} and {crosstalk}",
    "section": "",
    "text": "In this blog post, I want to discuss something that I, personally, have never seen discussed; how to create a “serverless” (or “shinyless” you could say) dashboard using R.\nI made one dashboard like that, which you can find here. This dashboard is running on a simple, standard web server. No Shiny involved!\nThe idea is to create a dashboard with simple tables, graphs, and filters, to communicate results without the need for a Shiny server. The “dashboard” will be a simple html file that only needs a good old web server. Or you could even send the rendered html file per email, and the recipient only needs to open it using a web browser. The shortcoming of that, of course, is that this “dashboard”, which is a simple html file will be static; no computation will be possible (well not quite as you’ll see), so you need to precompute everything that you want to show. It won’t also be possible for the users to enter parameters and then have graphs update automatically. For instance, you cannot let a user choose how many days should be used in a moving average. At best, you can compute three variable, each one with a different number of days, and then let the user choose which of these precomputed variables should be drawn.\nBut the first question is, why would we want, or need, something so limited?\nThe advantage of not needing a Shiny server, is that it makes deployment much easier. If you can “deploy” a dashboard that does not need a Shiny server, this means that you don’t need to set up…, well a server. In an institutional setting, this can literally mean you end up saving weeks, sometimes months, of getting the right green lights and signatures. When I worked as a consultant, deployment was definitely the toughest problem to solve (well, toughest maybe after getting access to the data itself). And also, this solution might not be as limited as you think. While it is true that users cannot compute anything on the fly, it is still possible to do a lot of things, which should in all honesty be enough for most use cases. Most users only want or need a glorified Excel with pivot tables and pivot charts. So we’re giving them that, but in a nicer package: the dashboard can be hosted, and users do not have writing rights. That’s honestly all I need in perhaps 90% of the situations.\nThe solution I’m going to present was in front of me for the longest time; it’s just that I did not put 2 and 2 together. The first part of the solution is {flexdashboard}, which is the framework allowing us to build a dashboard. Dashboards made with {flexdashboard} are simple html files, which can have Shiny elements in them, so for instance an interactive plot that gets generated once the user has entered some input. But these dashboards don’t need to have Shiny elements in them; htmlwidgets are enough. What are htmlwidgets? Take a look at the graph below:\nYou can interact with this visualisation, and it’s 100% running in your web browser. No Shiny involved, even though you can zoom and select different levels in the legend on the top right (try double-clicking on the “0” level for instance). This visualisation was made with the {plotly} package, one of the many htmlwidgets available. My favorite for making such visualisations is {echarts4r} which I’ve used to create the following map (how-to blog post here). htmlwidgets bring JavaScript visualisations (and other goodies) to R, and what’s really cool about them is that they don’t need a Shiny server to run (that’s the whole point of JavaScript, everything runs in the browser). So this means that by combining {flexdashboard} with the right htmlwidgets we can create a simple, yet useful, dashboard that can be deployed as a web page.\nTo illustrate, I’ve made the following dashboard, which shows tables, graphs, and even a pivot table of COVID-19 cases and deaths of the Greater Region (to know more about the Greater Region and why this interests me currently, you can read this).\nSomething else I need to talk about: on the very first tab, you can see a sidebar with some inputs that the user can interact with. For instance, the user can choose which country’s data should appear on the table. It is also possible to filter the positive cases data (not the deaths, but this could be added). This interaction between the sidebar and the table (which was made using {DT}) was made possible using the {crosstalk} package. This package makes it possible to link several htmlwidgets together, but they have to be compatible with {crosstalk}. Unfortunately, at the time of writing, not many htmlwidgets are compatible with {crosstalk} (see here), but I would say that the ones that are compatible still make it possible to create some pretty useful stuff.\nThe only thing you need to do to link htmlwidgets with each other is to convent the dataframe holding your data to a SharedData object:\nWidgets compatible with {crosstalk} can now use this SharedData object instead of the regular dataframe, and this is how you link them: through this SharedData object.\nAnother tab that uses {crosstalk} is the last one, where you can take a look at the weekly positive cases and deaths for the countries of the Greater Regions (but only for the sub-regions of these countries composing the Greater Region). Here, the user can choose whether deaths or positive cases should be shown. The plot updates immediately, and it’s also possible to focus on a single country by double-clicking on it in the legend on the top-right. Again, it’s also possible to focus on a particular month. Here I wanted to use a slicer like on the first table, but on the date. This should work (I’m using exactly that on another dashboard I made), but for some reason here, it would not work. The dashboard would compile without any error message but trying to open the html file on my browser would make the browser hang. So I settled for another type of slicer. Something else that is quite cool; if you choose to focus on the cases, you can hover the mouse over the bars and see how many cases there were in the sub regions in each country. For this, I had to change the default behavior of the popup in the {plotly} visualisation.\nNow comes the cherry on top of this already delicious cake; on the second tab, you can interact with a pivot table! This makes it possible to, for instance, see how many deaths there were in each country, region or sub-region, on a weekly basis. You can even switch from a table to several types of visualisations! This pivot table is made possible using the very nice {rpivotTable} package. This package is honestly nuts. It feels like it shouldn’t work so well, and yet, it does work beautifully. Seriously, play around with it in the dashboard, it’s pure magic.\nOne final note; on the top right of the dashboard you can click on “Source Code” and read the dashboard’s source code. You will notice that I use two functions, tar_load() and tar_read() that can be found in the {targets} package. I will be explaining what that is exactly in a subsequent blog post, or perhaps a video on my youtube channel. You can also see how the inputs in the sidebar work, and how they are linked (through the SharedData object) to the visualisations they control.\nIn any case, I’m quite happy that I found the possibility to develop dashboards without the need of a server, where all the logic is handled client-side by the web browser. I think that this definitely can help many of you that need to communicate results fast to stakeholders without the need to deploy a full server, which can often take quite a long time."
  },
  {
    "objectID": "posts/2021-03-02-no_shiny_dashboard.html#bonus",
    "href": "posts/2021-03-02-no_shiny_dashboard.html#bonus",
    "title": "Server(shiny)-less dashboards with R, {htmlwidgets} and {crosstalk}",
    "section": "\nBonus\n",
    "text": "Bonus\n\n\nBy the way, yesterday I read the most amazing tweet:\n\n\n\nHost on GitHub, like you would a normal repo (incl. pics dir etc.)GH doesn’t render HTML by default… But you just need to change the root of your URL:“github” -&gt; “raw DOT githack”(Also delete the “blob/” bit.)I host all my lectures and seminar slides this way.\n\n— Grant McDermott (@grant_mcdermott) March 2, 2021\n\n\n\nI used this trick to host the dashboard on github!"
  },
  {
    "objectID": "posts/2018-06-10-scraping_pdfs.html",
    "href": "posts/2018-06-10-scraping_pdfs.html",
    "title": "Getting data from pdfs using the pdftools package",
    "section": "",
    "text": "It is often the case that data is trapped inside pdfs, but thankfully there are ways to extract it from the pdfs. A very nice package for this task is pdftools (Github link) and this blog post will describe some basic functionality from that package.\n\n\nFirst, let’s find some pdfs that contain interesting data. For this post, I’m using the diabetes country profiles from the World Health Organization. You can find them here. If you open one of these pdfs, you are going to see this:\n\n\n\n\n\nhttp://www.who.int/diabetes/country-profiles/lux_en.pdf?ua=1\n\n\n\n\nI’m interested in this table here in the middle:\n\n\n\n\n\nhttp://www.who.int/diabetes/country-profiles/lux_en.pdf?ua=1\n\n\n\n\nI want to get the data from different countries, put it all into a nice data frame and make a simple plot.\n\n\nLet’s first start by loading the needed packages:\n\nlibrary(\"pdftools\")\nlibrary(\"glue\")\nlibrary(\"tidyverse\")\n## ── Attaching packages ────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──\n## ✔ ggplot2 2.2.1     ✔ purrr   0.2.5\n## ✔ tibble  1.4.2     ✔ dplyr   0.7.5\n## ✔ tidyr   0.8.1     ✔ stringr 1.3.1\n## ✔ readr   1.1.1     ✔ forcats 0.3.0\n## ── Conflicts ───────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::collapse() masks glue::collapse()\n## ✖ dplyr::filter()   masks stats::filter()\n## ✖ dplyr::lag()      masks stats::lag()\nlibrary(\"ggthemes\")\n\ncountry &lt;- c(\"lux\", \"fra\", \"deu\", \"usa\", \"prt\", \"gbr\")\n\nurl &lt;- \"http://www.who.int/diabetes/country-profiles/{country}_en.pdf?ua=1\"\n\nThe first 4 lines load the needed packages for this exercise: pdftools is the package that I described in the beginning of the post, glue is optional but offers a nice alternative to the paste() and paste0() functions. Take a closer look at the url: you’ll see that I wrote {country}. This is not in the original links; the original links look like this (for example for the USA):\n\n\"http://www.who.int/diabetes/country-profiles/usa_en.pdf?ua=1\"\n\nSo because I’m interested in several countries, I created a vector with the country codes of the countries I’m interested in. Now, using the glue() function, something magical happens:\n\n(urls &lt;- glue(url))\n## http://www.who.int/diabetes/country-profiles/lux_en.pdf?ua=1\n## http://www.who.int/diabetes/country-profiles/fra_en.pdf?ua=1\n## http://www.who.int/diabetes/country-profiles/deu_en.pdf?ua=1\n## http://www.who.int/diabetes/country-profiles/usa_en.pdf?ua=1\n## http://www.who.int/diabetes/country-profiles/prt_en.pdf?ua=1\n## http://www.who.int/diabetes/country-profiles/gbr_en.pdf?ua=1\n\nThis created a vector with all the links where {country} is replaced by each of the codes contained in the variable country.\n\n\nI use the same trick to create the names of the pdfs that I will download:\n\npdf_names &lt;- glue(\"report_{country}.pdf\")\n\nAnd now I can download them:\n\nwalk2(urls, pdf_names, download.file, mode = \"wb\")\n\nwalk2() is a function from the purrr package that is similar to map2(). You could use map2() for this, but walk2() is cleaner here, because dowload.file() is a function with a so-called side effect; it downloads files. map2() is used for functions without side effects.\n\n\nNow, I can finally use the pdf_text() function from the pdftools function to get the text from the pdfs:\n\nraw_text &lt;- map(pdf_names, pdf_text)\n\nraw_text is a list of where each element is the text from one of the pdfs. Let’s take a look:\n\nstr(raw_text)\n## List of 6\n##  $ : chr \"Luxembourg                                                                                                     \"| __truncated__\n##  $ : chr \"France                                                                                                         \"| __truncated__\n##  $ : chr \"Germany                                                                                                        \"| __truncated__\n##  $ : chr \"United States Of America                                                                                       \"| __truncated__\n##  $ : chr \"Portugal                                                                                                       \"| __truncated__\n##  $ : chr \"United Kingdom                                                                                                 \"| __truncated__\n\nLet’s take a look at one of these elements, which is nothing but a very long character:\n\nraw_text[[1]]\n## [1] \"Luxembourg                                                                                                                                          Total population: 567 000\\n                                                                                                                                                         Income group: High\\nMortality\\nNumber of diabetes deaths                                                                     Number of deaths attributable to high blood glucose\\n                                                                     males         females                                                            males       females\\nages 30–69                                                           &lt;100            &lt;100     ages 30–69                                              &lt;100          &lt;100\\nages 70+                                                             &lt;100            &lt;100     ages 70+                                                &lt;100          &lt;100\\nProportional mortality (% of total deaths, all ages)                                          Trends in age-standardized prevalence of diabetes\\n                    Communicable,\\n                   maternal, perinatal              Injuries                                                    35%\\n                    and nutritional                   6%                     Cardiovascular\\n                      conditions                                               diseases\\n                          6%                                                      33%\\n                                                                                                                30%\\n                                                                                                                25%\\n                                                                                              % of population\\n               Other NCDs\\n                  16%                                                                                           20%\\n                                     No data available                                                          15%           No data available\\n              Diabetes                                                                                          10%\\n                 2%\\n                                                                                                                5%\\n                   Respiratory\\n                    diseases\\n                       6%                                                                                       0%\\n                                                           Cancers\\n                                                            31%\\n                                                                                                                                  males     females\\nPrevalence of diabetes and related risk factors\\n                                                                                                                      males               females               total\\nDiabetes                                                                                                              8.3%                 5.3%                 6.8%\\nOverweight                                                                                                            70.7%               51.5%                61.0%\\nObesity                                                                                                               28.3%               21.3%                24.8%\\nPhysical inactivity                                                                                                   28.2%               31.7%                30.0%\\nNational response to diabetes\\nPolicies, guidelines and monitoring\\nOperational policy/strategy/action plan for diabetes                                                                                                ND\\nOperational policy/strategy/action plan to reduce overweight and obesity                                                                            ND\\nOperational policy/strategy/action plan to reduce physical inactivity                                                                               ND\\nEvidence-based national diabetes guidelines/protocols/standards                                                                                     ND\\nStandard criteria for referral of patients from primary care to higher level of care                                                                ND\\nDiabetes registry                                                                                                                                   ND\\nRecent national risk factor survey in which blood glucose was measured                                                                              ND\\nAvailability of medicines, basic technologies and procedures in the public health sector\\nMedicines in primary care facilities                                                          Basic technologies in primary care facilities\\nInsulin                                                                               ND      Blood glucose measurement                                             ND\\nMetformin                                                                             ND      Oral glucose tolerance test                                           ND\\nSulphonylurea                                                                         ND      HbA1c test                                                            ND\\nProcedures                                                                                    Dilated fundus examination                                            ND\\nRetinal photocoagulation                                                              ND      Foot vibration perception by tuning fork                              ND\\nRenal replacement therapy by dialysis                                                 ND      Foot vascular status by Doppler                                       ND\\nRenal replacement therapy by transplantation                                          ND      Urine strips for glucose and ketone measurement                       ND\\nND = country did not respond to country capacity survey\\n〇 = not generally available   ● = generally available\\nWorld Health Organization – Diabetes country profiles, 2016.\\n\"\n\nAs you can see, this is a very long character string with some line breaks (the \"\" character). So first, we need to split this string into a character vector by the \"\" character. Also, it might be difficult to see, but the table starts at the line with the following string: \"Prevalence of diabetes\" and ends with \"National response to diabetes\". Also, we need to get the name of the country from the text and add it as a column. As you can see, a whole lot of operations are needed, so what I do is put all these operations into a function that I will apply to each element of raw_text:\n\nclean_table &lt;- function(table){\n    table &lt;- str_split(table, \"\\n\", simplify = TRUE)\n    country_name &lt;- table[1, 1] %&gt;% \n        stringr::str_squish() %&gt;% \n        stringr::str_extract(\".+?(?=\\\\sTotal)\")\n    table_start &lt;- stringr::str_which(table, \"Prevalence of diabetes\")\n    table_end &lt;- stringr::str_which(table, \"National response to diabetes\")\n    table &lt;- table[1, (table_start +1 ):(table_end - 1)]\n    table &lt;- str_replace_all(table, \"\\\\s{2,}\", \"|\")\n    text_con &lt;- textConnection(table)\n    data_table &lt;- read.csv(text_con, sep = \"|\")\n    colnames(data_table) &lt;- c(\"Condition\", \"Males\", \"Females\", \"Total\")\n    dplyr::mutate(data_table, Country = country_name)\n}\n\nI advise you to go through all these operations and understand what each does. However, I will describe some of the lines, such as this one:\n\nstringr::str_extract(\".+?(?=\\\\sTotal)\")\n\nThis uses a very bizarre looking regular expression: \".+?(?=\\sTotal)\". This extracts everything before a space, followed by the string \"Total\". This is because the first line, the one that contains the name of the country looks like this: \"Luxembourg Total population: 567 000\". So everything before a space followed by the word \"Total\" is the country name. Then there’s these lines:\n\ntable &lt;- str_replace_all(table, \"\\\\s{2,}\", \"|\")\ntext_con &lt;- textConnection(table)\ndata_table &lt;- read.csv(text_con, sep = \"|\")\n\nThe first lines replaces 2 spaces or more (“\\s{2,}”) with \"|\". The reason I do this is because then I can read the table back into R as a data frame by specifying the separator as the “|” character. On the second line, I define table as a text connection, that I can then read back into R using read.csv(). On the second to the last line I change the column names and then I add a column called \"Country\" to the data frame.\n\n\nNow, I can map this useful function to the list of raw text extracted from the pdfs:\n\ndiabetes &lt;- map_df(raw_text, clean_table) %&gt;% \n    gather(Sex, Share, Males, Females, Total) %&gt;% \n    mutate(Share = as.numeric(str_extract(Share, \"\\\\d{1,}\\\\.\\\\d{1,}\")))\n\nI reshape the data with the gather() function (see what the data looks like before and after reshaping). I then convert the \"Share\" column into a numeric (it goes from something that looks like \"12.3 %\" into 12.3) and then I can create a nice plot. But first let’s take a look at the data:\n\ndiabetes\n##              Condition                  Country     Sex Share\n## 1             Diabetes               Luxembourg   Males   8.3\n## 2           Overweight               Luxembourg   Males  70.7\n## 3              Obesity               Luxembourg   Males  28.3\n## 4  Physical inactivity               Luxembourg   Males  28.2\n## 5             Diabetes                   France   Males   9.5\n## 6           Overweight                   France   Males  69.9\n## 7              Obesity                   France   Males  25.3\n## 8  Physical inactivity                   France   Males  21.2\n## 9             Diabetes                  Germany   Males   8.4\n## 10          Overweight                  Germany   Males  67.0\n## 11             Obesity                  Germany   Males  24.1\n## 12 Physical inactivity                  Germany   Males  20.1\n## 13            Diabetes United States Of America   Males   9.8\n## 14          Overweight United States Of America   Males  74.1\n## 15             Obesity United States Of America   Males  33.7\n## 16 Physical inactivity United States Of America   Males  27.6\n## 17            Diabetes                 Portugal   Males  10.7\n## 18          Overweight                 Portugal   Males  65.0\n## 19             Obesity                 Portugal   Males  21.4\n## 20 Physical inactivity                 Portugal   Males  33.5\n## 21            Diabetes           United Kingdom   Males   8.4\n## 22          Overweight           United Kingdom   Males  71.1\n## 23             Obesity           United Kingdom   Males  28.5\n## 24 Physical inactivity           United Kingdom   Males  35.4\n## 25            Diabetes               Luxembourg Females   5.3\n## 26          Overweight               Luxembourg Females  51.5\n## 27             Obesity               Luxembourg Females  21.3\n## 28 Physical inactivity               Luxembourg Females  31.7\n## 29            Diabetes                   France Females   6.6\n## 30          Overweight                   France Females  58.6\n## 31             Obesity                   France Females  26.1\n## 32 Physical inactivity                   France Females  31.2\n## 33            Diabetes                  Germany Females   6.4\n## 34          Overweight                  Germany Females  52.7\n## 35             Obesity                  Germany Females  21.4\n## 36 Physical inactivity                  Germany Females  26.5\n## 37            Diabetes United States Of America Females   8.3\n## 38          Overweight United States Of America Females  65.3\n## 39             Obesity United States Of America Females  36.3\n## 40 Physical inactivity United States Of America Females  42.1\n## 41            Diabetes                 Portugal Females   7.8\n## 42          Overweight                 Portugal Females  55.0\n## 43             Obesity                 Portugal Females  22.8\n## 44 Physical inactivity                 Portugal Females  40.8\n## 45            Diabetes           United Kingdom Females   6.9\n## 46          Overweight           United Kingdom Females  62.4\n## 47             Obesity           United Kingdom Females  31.1\n## 48 Physical inactivity           United Kingdom Females  44.3\n## 49            Diabetes               Luxembourg   Total   6.8\n## 50          Overweight               Luxembourg   Total  61.0\n## 51             Obesity               Luxembourg   Total  24.8\n## 52 Physical inactivity               Luxembourg   Total  30.0\n## 53            Diabetes                   France   Total   8.0\n## 54          Overweight                   France   Total  64.1\n## 55             Obesity                   France   Total  25.7\n## 56 Physical inactivity                   France   Total  26.4\n## 57            Diabetes                  Germany   Total   7.4\n## 58          Overweight                  Germany   Total  59.7\n## 59             Obesity                  Germany   Total  22.7\n## 60 Physical inactivity                  Germany   Total  23.4\n## 61            Diabetes United States Of America   Total   9.1\n## 62          Overweight United States Of America   Total  69.6\n## 63             Obesity United States Of America   Total  35.0\n## 64 Physical inactivity United States Of America   Total  35.0\n## 65            Diabetes                 Portugal   Total   9.2\n## 66          Overweight                 Portugal   Total  59.8\n## 67             Obesity                 Portugal   Total  22.1\n## 68 Physical inactivity                 Portugal   Total  37.3\n## 69            Diabetes           United Kingdom   Total   7.7\n## 70          Overweight           United Kingdom   Total  66.7\n## 71             Obesity           United Kingdom   Total  29.8\n## 72 Physical inactivity           United Kingdom   Total  40.0\n\nNow let’s go for the plot:\n\nggplot(diabetes) + theme_fivethirtyeight() + scale_fill_hc() +\n    geom_bar(aes(y = Share, x = Sex, fill = Country), \n             stat = \"identity\", position = \"dodge\") +\n    facet_wrap(~Condition)\n\n\n\n\nThat was a whole lot of work for such a simple plot!\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2018-04-15-announcing_pmice.html",
    "href": "posts/2018-04-15-announcing_pmice.html",
    "title": "{pmice}, an experimental package for missing data imputation in parallel using {mice} and {furrr}",
    "section": "",
    "text": "Yesterday I wrote this blog post which showed how one could use {furrr} and {mice} to impute missing data in parallel, thus speeding up the process tremendously.\n\n\nTo make using this snippet of code easier, I quickly cobbled together an experimental package called {pmice} that you can install from Github:\n\ndevtools::install_github(\"b-rodrigues/pmice\")\n\nFor now, it returns a list of mids objects and not a mids object like mice::mice() does, but I’ll be working on it. Contributions welcome!\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2018-02-11-census-random_forest.html",
    "href": "posts/2018-02-11-census-random_forest.html",
    "title": "Predicting job search by training a random forest on an unbalanced dataset",
    "section": "",
    "text": "Update 2022: there some literature advising against using techniques to artificially balance a dataset, for example one. Use at your own risks!\nIn this blog post, I am going to train a random forest on census data from the US to predict the probability that someone is looking for a job. To this end, I downloaded the US 1990 census data from the UCI Machine Learning Repository. Having a background in economics, I am always quite interested by such datasets. I downloaded the raw data which is around 820mb uncompressed. You can download it from this folder here.\nBefore training a random forest on it, some preprocessing is needed. First problem: the columns in the data do not have names. Actually, training a random forest on unamed variables is possible, but I like my columns to have names. The names are on a separate file, called USCensus1990raw.attributes.txt. This is how this file looks like:\nThe variable names are always written in upper case and sometimes end with some numbers. Regular expressions will help extract these column names:\nUsing readLines I load this text file into R. Then with stringr::str_extract_all, I can extract the variable names from this text file. The regular expression, 1+(\\d{1,}|[A-Z])\\s+ can seem complicated, but by breaking it up, it’ll be clear:\nThis regular expression matches only the variable names. By using ^ I only limit myself to the uppercase letters at the start of the line, which already removes a lot of unneeded lines from the text. Then, by matching numbers or letters, followed by spaces, I avoid matching strings such as VAR:. There’s probably a shorter way to write this regular expression, but since this one works, I stopped looking for another solution.\nNow that I have a vector called column_names, I can baptize the columns in my dataset:\nI also add a column called caseid to the dataset, but it’s actually not really needed. But it made me look for and find rownames_to_column(), which can be useful:\nNow I select the variables I need. I use dplyr::select() to select the columns I need (actually, I will remove some of these later for the purposes of the blog post, but will continue exploring them. Maybe write a part 2?):\nNow, I convert factor variables to factors and only relevel the race variable:\nSo the variable I want to predict is looking which has 2 levels (I removed the level 0, which stands for NA). I convert all the variables that are supposed to be factors into factors using mutate_at() and then reselect a subsample of the columns. census is now a tibble with 39 columns and 2458285 rows. I will train the forest on a subsample only, because with cross validation it would take forever on the whole dataset.\nI run the training on another script, that I will then run using the Rscript command instead of running it from Spacemacs (yes, I don’t use RStudio at home but Spacemacs + ESS). Here’s the script:\n90% of the individuals in the sample are not looking for a new job. For training purposes, I will only use 50000 observations instead of the whole sample. I’m already thinking about writing another blog post where I show how to use the whole data. But 50000 observations should be more than enough to have a pretty nice model. However, having 90% of observations belonging to a single class can cause problems with the model; the model might predict that everyone should belong to class 2 and in doing so, the model would be 90% accurate! Let’s ignore this for now, but later I am going to tackle this issue with a procedure calleds SMOTE.\nNow, using caret::trainIndex(), I partition the data into a training sample and a testing sample:\nI also save the testing data to disk, because when the training is done I’ll lose my R session (remember, I’ll run the training using Rscript):\nBefore training the model, I’ll change some options; I’ll do 5-fold cross validation that I repeat 5 times. This will further split the training set into training/testing sets which will increase my confidence in the metrics that I get from the training. This will ensure that the best model really is the best, and not a fluke resulting from the splitting of the data that I did beforehand. Then, I will test the best model on the testing data from above:\nA very nice feature from the caret package is the possibility to make the training in parallel. For this, load the doParallel package (which I did above), and then register the number of cores you want to use for training with makeCluster(). You can replace detectCores() by the number of cores you want to use:\nFinally, we can train the model:\nBecause it takes around 1 and a half hours to train, I save the model to disk using saveRDS():\nThe picture below shows all the cores from my computer running and RAM usage being around 20gb during the training process:\nAnd this the results of training the random forest on the unbalanced data:\nIf someone really is looking for a job, the model is able to predict it correctly 92% of the times and 98% of the times if that person is not looking for a job. It’s slightly better than simply saying than no one is looking for a job, which would be right 90% of the times, but not great either.\nTo train to make the model more accurate in predicting class 1, I will resample the training set, but by downsampling class 2 and upsampling class 1. This can be done with the function SMOTE() from the {DMwR} package. However, the testing set should have the same distribution as the population, so I should not apply SMOTE() to the testing set. I will resplit the data, but this time with a 95/5 % percent split; this way I have 5% of the original dataset used for testing, I can use SMOTE() on the 95% remaining training set. Because SMOTEing takes some time, I save the SMOTEd training set using readRDS() for later use:\nThe testing set has 34780 observations and below you can see the distribution of the target variable, looking:\nHere are the results:\nThe balanced accuracy is higher, but unlike what I expected (and hoped), this model is worse in predicting class 1! I will be trying one last thing; since I have a lot of data at my disposal, I will simply sample 25000 observations where the target variable looking equals 1, and then sample another 25000 observations where the target variable equals 2 (without using SMOTE()). Then I’ll simply bind the rows and train the model on that:\nAnd here are the results:\nLooks like it’s not much better than using SMOTE()!\nThere are several ways I could achieve better predictions; tuning the model is one possibility, or perhaps going with another type of model altogether. I will certainly come back to this dataset in future blog posts!\nUsing the best model, let’s take a look at which variables are the most important for predicting job search:\nIt’s also possible to have a plot of the above:\nTo make sense of this, we have to read the description of the features here.\nrlabor3 is the most important variable, and means that the individual is unemployed. rlabor6 means not in the labour force. Then the age of the individual as well as the individual’s income play a role. tmpabsnt is a variable that equals 1 if the individual is temporary absent from work, due to a layoff. All these variables having an influence on the probability of looking for a job make sense, but looks like a very simple model focusing on just a couple of variables would make as good a job as the random forest.\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2018-02-11-census-random_forest.html#footnotes",
    "href": "posts/2018-02-11-census-random_forest.html#footnotes",
    "title": "Predicting job search by training a random forest on an unbalanced dataset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA-Z↩︎\nA-Z↩︎"
  },
  {
    "objectID": "posts/2021-09-05-speedrunning_rows.html",
    "href": "posts/2021-09-05-speedrunning_rows.html",
    "title": "Speedrunning row-oriented workflows",
    "section": "",
    "text": "If you haven’t, you should read this first. This is part two.\n\n\nSpeedrunning is the… hrm… - sport? art? - of playing games from start to finish as fast as possible. Speedrunning requires an insane amount of knowledge of the game being played, as well as an enourmous amount of skill. Also, contrary to what you might think, it is a community effort. Players do speedrun the game alone, and it is a ferocious competition, each one of them aiming for the top spot on the leaderboards. But discovering the strategies that will allow the top players to shave off, sometimes literally, hundredths of seconds from the previous world record require many, many, people from the speedrunning community trying to break the games in new ways, or testing how fast theoretical strategies using computers that play the game perfectly are (these type of speedruns are called TAS, for Tool Assisted Speedrun, and are a very important part of the speedrunning effort).\n\n\nIf you read until here, I commend you dear reader, and thank you for not having already closed the tab. The meat of the post is coming.\n\n\nIf you don’t know anything about speedrunning, I can only urge you to watch this video about the story of the Super Mario Bros. World Records. If you’re more into Doom, then watch this video abut the history of Doom 2 World Records. It really is worth your time, believe me.\n\n\nAnyways, why am I talking about this? What is the point of this blog post? Isn’t this a blog about Econometrics and Free Software (lol)?\n\n\nThe reason I’m talking about speedrunning in video games, is because my previous blog post sparked an interesting discussion on twitter, which very much reminded me of what you’d see in the speedrunning community.\n\n\nJust like in speedrunning, I tried to play a game which consisted in running an arbitrary function over the rows of a data frame, and employed some basic strategies for it. As a reminder, here is the example code with the top two strategies: using apply() and a combination of asplit() and map() (I won’t be showing all the code here, it’s the same as in the previous blog post):\n\nrun_apply &lt;- function(dataset, my_function = my_function){\n\n  dataset %&gt;%\n    mutate(score = apply(., MARGIN = 1, my_function))\n\n}\n\nrun_map &lt;- function(dataset, my_function = my_function){\n  dataset %&gt;%\n    mutate(score = map_dbl(asplit(., 1), .f = my_function))\n}\n\nAlso, just as a reminder, here is the rowwise() approach:\n\nrun_rowwise &lt;- function(dataset, my_function = my_function){\n  dataset %&gt;%\n    rowwise() %&gt;%\n    mutate(score = my_function(c_across(everything()))) %&gt;%\n    ungroup()\n}\n\nThis is, AFAIK, the official tidyverse-approach, but not the fastest. However, while it is slower than the two approaches above, it does have the advantage that you can run the function over the rows, but only by using certain columns instead of all of them. For example, to apply the function over only the columns that start with the letter “c” (and for each row), you could write this:\n\nrun_rowwise &lt;- function(dataset, my_function = my_function){\n  dataset %&gt;%\n    rowwise() %&gt;%\n    mutate(score = my_function(c_across(starts_with(\"c\")))) %&gt;%\n    ungroup()\n}\n\nThis is not possible with the two fast approaches, run_map() and run_apply(). These two approaches do run quite fast, but in the twitter discussion I linked above, many more suggestions were made, and some are likely faster, so let’s see! There’s first an approach using pmap() proposed by both @lgaborini and @JoeWasserman:\n\nrun_pmap &lt;- function(dataset, my_function = my_function){\n  dataset %&gt;%\n    mutate(score = pmap_dbl(., .f = lift_vd(my_function)))\n\n}\n\nI won’t go into the details here of how and why this works. For more details, click here. In any case, this does not run faster that the two approaches listed above. But it does run faster than using rowwise() and also allows for selecting columns over which to run the function:\n\nrun_pmap &lt;- function(dataset, my_function = my_function){\n  dataset %&gt;%\n    mutate(score = pmap_dbl(select(., matches(\".(4|5|6)\")), .f = lift_vd(mean)))\n\n}\n\nrun_pmap(dataset) %&gt;%\n  head\n## # A tibble: 6 × 7\n##       x1     x2     x3    x4    x5     x6 score\n##    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 0.0644 0.789  0.489  0.665 0.664 0.230  0.520\n## 2 0.771  0.209  0.443  0.248 0.756 0.0164 0.340\n## 3 0.342  0.0382 0.619  0.196 0.115 0.783  0.365\n## 4 0.638  0.915  0.0472 0.891 0.346 0.639  0.625\n## 5 0.0366 0.601  0.426  0.421 0.835 0.906  0.721\n## 6 0.0465 0.937  0.260  0.803 0.376 0.330  0.503\n\nSo this is quite useful!\n\n\nThere was another proposal, a pure base approach, by @grant_mcdermott:\n\nrun_pure_base &lt;- function(dataset, my_function = my_function){\n  dataset |&gt;\n    within({score = sapply(asplit(dataset, 1), my_function)})\n}\n\nIt even uses the new, shiny (haha), native pipe, |&gt;! I have not benchmarked this yet, as I’m writing this, so let’s see…\n\n\nFinally, there is also a {data.table} approach, proposed by @thatpinkney:\n\nlibrary(data.table)\n\nrun_dt2 &lt;- function(dataset, my_function = my_function){\n\n  dataset &lt;- as.data.table(dataset)\n  dataset[, rowid := .I]\n  dataset[, \":=\" (score = melt(dataset, id.vars = \"rowid\")[, my_function(value), by = rowid][, V1],\n                  rowid = NULL)]\n\n}\n\nThe problem of this approach, at least to me, is that I do not know {data.table}, which is the reason why I did not include it in the previous blog post. But I have read many times that {data.table} is blazing fast, so I definitely should learn at least some basics!\n\n\nNow is benchmarking time. Let’s see (I’m not considering run_pmap(), because I already benchmarked it before writing this blog post, and know that it runs slower than the run_map() or run_apply()):\n\nlist_datasets &lt;- map(seq(2, 5), ~init_pop(objective_function = my_function,\n                                          pop_size = `^`(10, .x)))\n\n\nrun_benchmarks &lt;- function(dataset, times = 5){\n  microbenchmark::microbenchmark(\n                    run_apply(dataset, my_function = my_function),\n                    run_pure_base(dataset, my_function = my_function),\n                    run_dt2(dataset, my_function = my_function),\n                    run_map(dataset, my_function = my_function),\n                    times = times,\n                    unit = \"s\"\n                  )\n}\nbenchmark_results &lt;- map(list_datasets, run_benchmarks)\n\nbenchmark_data &lt;- map2(.x = benchmark_results, .y = 10^seq(2, 5), .f = ~mutate(tibble(.x), pop_size = .y)) %&gt;%\n  bind_rows() %&gt;%\n  mutate(expr = str_remove_all(expr, \"\\\\(.*\\\\)\")) %&gt;%\n  group_by(expr, pop_size) %&gt;%\n  mutate(time_seconds = time/10^9) %&gt;%\n  summarise(fastest_run = min(time_seconds),\n            average_run = mean(time_seconds),\n            slowest_run = max(time_seconds))\n## `summarise()` has grouped output by 'expr'. You can override using the `.groups` argument.\nbenchmark_data %&gt;%\n  ggplot(aes(y = average_run, x = pop_size)) +\n  geom_ribbon(aes(ymin = fastest_run, ymax = slowest_run, fill = expr), alpha = .6) +\n  geom_line(aes(group = expr, col = expr)) +\n  ylab(\"Seconds\") +\n  xlab(\"Rows in the dataset\") +\n  ggtitle(\"Speed of rowwise operations using different methods\") +\n  theme_blog()\n\n\n\n\nThese are really interesting results! The pure base solution runs almost as fast as the one that uses asplit() and map(). The one that uses apply() only is a close second, but all these strategies get obliterated by the {data.table} solution!\n\n\nSo, what have we learned?\n\n\n\nFirst of all, the #RStats community is really great! I’m really blown away by the interest that my previous blog post generated and by the very interesting discussion that ensued.\n\n\nSecond, if speed is really paramount to solving your problem, you’d probably want to use {data.table}. It does seem to be incredibly fast!\n\n\nThird, and final point, if you need to run rowwise operations, but only over certain columns, use pmap() instead of rowwise() - across() - everything()."
  },
  {
    "objectID": "posts/2020-12-12-ethics_statistics.html",
    "href": "posts/2020-12-12-ethics_statistics.html",
    "title": "(Half) Lies, (half) truths and (half) statistics",
    "section": "",
    "text": "Note: if you’re reading this and images are not showing, visit the original post on my blog. The blog post contains interactive plots which help in understanding the point I’m making.\n\n\nI’ve recently come across this graph (on Twitter) from the Economist:\n\n\n\n\n\nYou can read the article here (archived for posterity). There are many things wrong with this chart. First of all, the economist is fitting a linear regression to some data points, and does not provide anything else to the reader, namely the regression coefficients, their standard errors, and the R². I know already that some readers will disagree, thinking something along the lines of “But Bruno, come on, this is only to show that’s there a negative correlation between GDP per capita and trust in vaccines! The readers don’t need to be bothered with these complex concepts. This is just an illustration, and it’s good enough.”\n\n\nWRONG.\n\n\nLook, I’m all for good enough. That’s very likely going to be my epitaph. But sometimes, you can’t simplify things so much that they’re not only misleading, but lies. In this case here, the relationship between GDP per capita and trust in vaccines, if there is any, is probably highly nonlinear, and very difficult to pinpoint with any degree of accuracy. But before going further, let’s get the data and replicate the graph. I’ll be adding the equation of the regression line as well as the R² to the plot. I won’t comment my code, since the point of this blog post is not to teach you how to do it, but of course, you’re very welcome to reproduce the analysis.\n\n\nYou can download the data here, under “Full dataset for this chart”. You can also grab a csv version here.\n\n\n\nClick to see the code\n\nlibrary(tidyverse)\nlibrary(ggiraph)\n\ndataset &lt;- data.table::fread(\"https://gist.githubusercontent.com/b-rodrigues/388f6309a462c9ccbdf00f32ac9055cb/raw/92962f08f9e23b9a8586045291795f4ab21ad053/wgm2018.csv\")\n\ndataset &lt;- dataset %&gt;%\n  filter(grepl(\"(GDP per capita)|(Q25)\", question_statistic)) %&gt;%\n  mutate(response_type = ifelse(response_type == \"\", \"GDP per capita\", response_type)) %&gt;%\n  filter(grepl(\"(National Total)|(GDP)\", response_type)) %&gt;%\n  mutate(response_type = str_remove(response_type, \"National Total: \")) %&gt;%\n  select(country_name, response = response_type, value = result_percent) %&gt;%\n  mutate(gdp_per_capita = ifelse(grepl(\"GDP\", response), value, NA)) %&gt;% \n  fill(gdp_per_capita, .direction = \"down\") %&gt;%\n  filter(!grepl(\"GDP\", response))  %&gt;%\n  mutate(gdp_per_capita = as.numeric(gdp_per_capita),\n         value = as.numeric(value),\n         l_gdp = log(gdp_per_capita))\nplot_data &lt;- dataset %&gt;%\n  mutate(agree = ifelse(grepl(\" agree$\", response), \"safe\", \"not_safe\")) %&gt;%  \n  group_by(country_name, l_gdp, agree) %&gt;% \n  summarise(value = sum(value)) %&gt;%\n  filter(agree == \"safe\")\n## `summarise()` regrouping output by 'country_name', 'l_gdp' (override with `.groups` argument)\nlin_mod &lt;- lm(value ~ l_gdp, data = plot_data)\n\nlin_mod_coefs &lt;- coefficients(lin_mod)\nlin_mod_se &lt;- sqrt(diag(vcov(lin_mod)))\n\nregression_line_result &lt;- paste0(\"value = \",\n       round(lin_mod_coefs[1], 2),\n       \"[\",\n       round(lin_mod_se[1], 2),\n       \"]\",\n       round(lin_mod_coefs[2], 2),\n       \"[\",\n       round(lin_mod_se[2], 2),\n       \"]\",\n       \"*l_gdp\",\n       \",\\n R2 = \",\n       round(summary(lin_mod)$r.squared, 2))\n\nmy_plot &lt;- plot_data %&gt;%\n  ggplot(aes(y = value, x = l_gdp)) +\n  geom_point_interactive(aes(tooltip = country_name), colour = \"orange\") +\n  #geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  #ggrepel::geom_label_repel(aes(label = country_name)) +\n  geom_text(y = 35, x = 8,\n            label = regression_line_result,\n            colour = \"white\",\n            size = 3) +\n  brotools::theme_blog()\n\n\nIf you look at the code above, you’ll see that I’m doing a bunch of stuff to reproduce the graph. Let’s take a look at it (you can mouse over the points to see the country names over the labels):\n\ngirafe(ggobj = my_plot, width_svg = 8)\n## `geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nSo what’s actually going on? value is the percentage of people, in a country, that believe vaccines are safe. l_gdp is the logarithm of GDP per capita in that same country. Looking at this, many people will conclude that the richer the country, the less people trust vaccines. This is the story the Economist is telling its readers. This is a simple explanation, and it’s backed by numbers and stats, so it must be correct. Right?\n\n\nWRONG.\n\n\nLet’s take a look at the regression equation (standard errors in square brackets):\n\n\n[ = 122.04[9.3] - 4.95[0.98] * ]\n\n\nBoth coefficients are significant at the usual 5% level (the intercept is interesting though, as it implies a value greater than 100 for very low levels of log of GDP). This gives comfort to the person believing the basic story.\n\n\nBut take a look at the R². It’s 0.15. That means that the linear regression will be able to predict up to 15% of the variance in the dependent variable using the log of GDP per capita as a predictor. That already should sound all sorts of alarms in your head (if that scatter plot that looks almost like random noise didn’t already). However, I’m not done yet.\n\n\nWhat if you wanted to do something a little bit more elaborate? For instance, let’s say that you’d like to see if infant mortality plays a role? After all, you could argue that in very poor countries, where people seem to trust vaccines very much, infant mortality is very high. Vaccinating your kid seems like a no-brainer when the alternative is almost certain death from any of the many diseases afflicting children (don’t get me wrong here, vaccinating children against deadly diseases is a no-brainer anywhere on the planet). Maybe people in wealthier countries don’t ascribe low infant mortality to vaccines, but to other things such as access to clean water, good infrastructure etc, and thus tend to downplay the role of vaccines. Who knows. But let’s dig deeper and get some more data.\n\n\nFor this I’m using another data set that gives the infant mortality rate in 2018 for most of the countries from the original analysis. I got that data from the Worldbank, and you can easily download the csv from here.\n\n\nBelow, I’m downloading the data and joining that to my original dataset. Then I’m computing a rank based on the median infant mortality rate. Countries that have an infant mortality rate below the median are classified as “low infant mortality rate” countries and countries that have an infant mortality rate above the median infant mortality rate are classified as “high infant mortality rate” countries. I then redo the same plot as before, but I’m computing one regression line per group of countries.\n\n\n\nClick to see the code\n\ninfant_mortality_rate &lt;- data.table::fread(\"https://gist.githubusercontent.com/b-rodrigues/33f64ce6910e6ec4df9d586eacf335c2/raw/01df8977edd3924a3687f783e7e5a134d5f3fd87/infant_mortality_rate_2018.csv\") %&gt;%\n  janitor::clean_names() %&gt;%\n  select(country_name, imr = x2018_yr2018)\n\nplot_data_simpson &lt;- plot_data %&gt;%\n  ungroup() %&gt;%  \n  left_join(infant_mortality_rate) %&gt;%\n  mutate(imr = as.numeric(imr)) %&gt;%  \n  filter(!is.na(imr)) %&gt;%  \n  mutate(rank = ntile(imr, n = 2))  %&gt;%\n  mutate(rank = ifelse(rank == 2,\n                       \"High infant mortality rate\",\n                       \"Low infant mortality rate\"))\n## Joining, by = \"country_name\"\nmy_plot &lt;- plot_data_simpson %&gt;%\n  ggplot(aes(y = value, x = l_gdp)) +\n  geom_point_interactive(aes(tooltip = country_name, colour = rank)) +\n  geom_smooth(aes(group = rank), method = \"lm\") +\n  brotools::theme_blog()\n\ngirafe(ggobj = my_plot, width_svg = 8)\n## `geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nAll of a sudden, the relationship turns positive for high income countries. This is the famous Simpson’s paradox in action. If you don’t know about Simpson’s paradox, you can read about it here.\n\n\nNow what? Should we stop here? No.\n\n\nLet’s not even consider Simpson’s paradox. Even though the authors never claim to have found any causal mechanism (and the Economist made no such claim, even though they tried hard to find some after the fact explanation to justify their findings), authors of such studies do very often imply that their simple analysis has at the very least some predictive power. We already know that this is bullocks, because the R² is so low. But let’s try something fun; let’s split the dataset into a training set and a testing set, and let’s see if we can accurately predict the points from the test set. Also, I won’t do this once, because, who knows, maybe that one regression we did had some very hard to predict points in the test set, so I’ll do it 100 times, always with new randomly generated training and testing sets. The way I’m evaluating the accuracy of the regression is visually: I’ll be doing a plot like before, where I’m showing the points from the training set, the points from the test set, as well as the predictions. I’ll also be showing the distance between the prediction and the points from the test set.\n\n\n\nClick to see the code to run the 100 regressions.\n\nrun_regression &lt;- function(dataset){\n\n  training_index &lt;- sample(1:nrow(dataset), 120)\n\n  training_set &lt;- dataset[training_index, ]\n\n  testing_set &lt;- dataset[-training_index, ]\n\n  fitted_model &lt;- lm(value ~ l_gdp, data = training_set)\n\n  predicted_points &lt;- predict.lm(fitted_model, newdata = testing_set)\n\n  predicted_points &lt;- cbind(testing_set, \"prediction\" = predicted_points)\n\n  rbind(training_set, predicted_points)\n}\n\nresults &lt;- tribble(~id,\n                   seq(1, 100)) %&gt;%\n  mutate(dataset = list(filter(plot_data, country_name != \"Taiwan\"))) %&gt;%  \n  unnest(cols = c(id)) %&gt;%\n  mutate(regression = map(dataset, run_regression))\n\n\nNow that I ran the 100 regressions, let’s create some visualisations:\n\n\n\nClick to see the code to create the plots.\n\nresults &lt;- results %&gt;%\n  mutate(regression = map(regression,\n                          ~mutate(., type_set = ifelse(is.na(prediction),\n                                                    \"training_set\",\n                                                    \"testing_set\"))))\n\n\nmake_plots &lt;- function(dataset){\nggplot() +\n  geom_point(data = dataset,\n             aes(y = value, x = l_gdp, shape = type_set), size = 5) +\n  geom_smooth(data = dataset,\n              aes(y = value, x = l_gdp),\n              method = \"lm\") +\n  geom_point(data = {dataset %&gt;%\n                      filter(!is.na(prediction)) %&gt;%\n                       pivot_longer(c(value, prediction), names_to = \"values\") %&gt;%\n                       mutate(values = ifelse(values == \"value\",\n                                              \"Actual value\",\n                                              \"Prediction\"))},\n             aes(y = value, x = l_gdp, colour = values, group = country_name)) +\n  geom_path(data = {dataset %&gt;%\n                      filter(!is.na(prediction)) %&gt;%\n                      pivot_longer(c(value, prediction), names_to = \"values\") %&gt;%\n                      mutate(values = ifelse(values == \"value\",\n                                             \"Actual value\",\n                                             \"Prediction\"))},\n               aes(y = value, x = l_gdp, colour = values, group = country_name),\n               arrow = arrow(length = unit(0.03, \"npc\"))) +\n  brotools::theme_blog()\n} \n\nresults &lt;- results %&gt;%\n  mutate(plots = map(regression, make_plots))\n\n\nFinally, let’s take a look at some of them:\n\n\n\nClick to see some plots.\n\nresults$plots[1:3]\n## [[1]]\n## `geom_smooth()` using formula 'y ~ x'\n\n\n\n## \n## [[2]]\n## `geom_smooth()` using formula 'y ~ x'\n\n\n\n## \n## [[3]]\n## `geom_smooth()` using formula 'y ~ x'\n\n\n\nThe red dots are the actual values in the test set (the triangles are the points in the training set). The blue dots are the predictions. See what happens? They all get very close to the regression line. This is of course completely normal; after all, the line is what the model is predicting, so how else could it be? I don’t know if this is exactly what is named “regression towards the mean”, but it does look very much like it. But in general, we speak of regression towards the mean when there’s time involved in whatever you’re studying (for example students that score very well on a first test tend to score worse, on average, on a second test, and vice-versa). But what matters here, is that a regression line cannot even be useful to make any type of prediction.\n\n\nSo where does that leave us? Should we avoid using simple methods like linear regression and only use very complex methods? Should we stop communicating numbers and stats and graphs to the general public? Certainly not. But using the excuse that the general public does not understand complex methods to justify using faulty stats is also not an option. In an article that mentions trust in vaccines, it also seems crucial to give more context; trust in vaccines may be higher on average in poorer countries (and that’s an assumption, the article of the Economist does not allow to conclude that), but distrust is also more extreme.\n\n\nI don’t think I’ve ever seen the general public distrust science and stats so much than during this pandemic. Many scientists made many predictions that of course never materialized, because scientists should not give out single point forecasts. Unfortunately, that’s what they do because that’s how they get people’s attention, and unfortunately, many also confuse science with stats. I think Millenials are very guilty of this. We all were thought critical thinking in school, and now all arguments devolve very quickly to “I have data and models to back my opinions up so my opinions are actually facts, and your data and models are wrong and you’re a terrible human being by the way”. The problem is that having data and models is not a sufficient condition for being right.\n\n\nAs statisticians, we have a responsibility to use the right methods, and make more and better efforts to communicate our results to the general public, even if the methods used are complex. Sometimes there’s simply no simplifying further. Anything else is just charlatanism."
  },
  {
    "objectID": "posts/2019-10-12-cluster_ts.html",
    "href": "posts/2019-10-12-cluster_ts.html",
    "title": "Cluster multiple time series using K-means",
    "section": "",
    "text": "I have been recently confronted to the issue of finding similarities among time-series and though about using k-means to cluster them. To illustrate the method, I’ll be using data from the Penn World Tables, readily available in R (inside the {pwt9} package):\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(pwt9)\nlibrary(brotools)\n\nFirst, of all, let’s only select the needed columns:\n\npwt &lt;- pwt9.0 %&gt;%\nselect(country, year, avh)\n\navh contains the average worked hours for a given country and year. The data looks like this:\n\nhead(pwt)\n##          country year avh\n## ABW-1950   Aruba 1950  NA\n## ABW-1951   Aruba 1951  NA\n## ABW-1952   Aruba 1952  NA\n## ABW-1953   Aruba 1953  NA\n## ABW-1954   Aruba 1954  NA\n## ABW-1955   Aruba 1955  NA\n\nFor each country, there’s yearly data on the avh variable. The goal here is to cluster the different countries by looking at how similar they are on the avh variable. Let’s do some further cleaning. The k-means implementation in R expects a wide data frame (currently my data frame is in the long format) and no missing values. These could potentially be imputed, but I can’t be bothered:\n\npwt_wide &lt;- pwt %&gt;%\n  pivot_wider(names_from = year, values_from = avh)  %&gt;%\n  filter(!is.na(`1950`)) %&gt;%\n  mutate_at(vars(-country), as.numeric)\n\nTo convert my data frame from long to wide, I use the fresh pivot_wider() function, instead of the less intuitive spread() function.\n\n\nWe’re ready to use the k-means algorithm. To know how many clusters I should aim for, I’ll be using the elbow method (if you’re not familiar with this method, click on the image at the very top of this post):\n\nwss &lt;- map_dbl(1:5, ~{kmeans(select(pwt_wide, -country), ., nstart=50,iter.max = 15 )$tot.withinss})\n\nn_clust &lt;- 1:5\n\nelbow_df &lt;- as.data.frame(cbind(\"n_clust\" = n_clust, \"wss\" = wss))\n\n\nggplot(elbow_df) +\ngeom_line(aes(y = wss, x = n_clust), colour = \"#82518c\") +\ntheme_blog()\n\n\n\n\nLooks like 3 clusters is a good choice. Let’s now run the kmeans algorithm:\n\nclusters &lt;- kmeans(select(pwt_wide, -country), centers = 3)\n\nclusters is a list with several interesting items. The item centers contains the “average” time series:\n\n(centers &lt;- rownames_to_column(as.data.frame(clusters$centers), \"cluster\"))\n##   cluster     1950     1951     1952     1953     1954     1955     1956\n## 1       1 2110.440 2101.273 2088.947 2074.273 2066.617 2053.391 2034.926\n## 2       2 2086.509 2088.571 2084.433 2081.939 2078.756 2078.710 2074.175\n## 3       3 2363.600 2350.774 2338.032 2325.375 2319.011 2312.083 2308.483\n##       1957     1958     1959     1960     1961     1962     1963     1964\n## 1 2021.855 2007.221 1995.038 1985.904 1978.024 1971.618 1963.780 1962.983\n## 2 2068.807 2062.021 2063.687 2060.176 2052.070 2044.812 2038.939 2037.488\n## 3 2301.355 2294.556 2287.556 2279.773 2272.899 2262.781 2255.690 2253.431\n##       1965     1966     1967     1968     1969     1970     1971     1972\n## 1 1952.945 1946.961 1928.445 1908.354 1887.624 1872.864 1855.165 1825.759\n## 2 2027.958 2021.615 2015.523 2007.176 2001.289 1981.906 1967.323 1961.269\n## 3 2242.775 2237.216 2228.943 2217.717 2207.037 2190.452 2178.955 2167.124\n##       1973     1974     1975     1976     1977     1978     1979     1980\n## 1 1801.370 1770.484 1737.071 1738.214 1713.395 1693.575 1684.215 1676.703\n## 2 1956.755 1951.066 1933.527 1926.508 1920.668 1911.488 1904.316 1897.103\n## 3 2156.304 2137.286 2125.298 2118.138 2104.382 2089.717 2083.036 2069.678\n##       1981     1982     1983     1984     1985     1986     1987     1988\n## 1 1658.894 1644.019 1636.909 1632.371 1623.901 1615.320 1603.383 1604.331\n## 2 1883.376 1874.730 1867.266 1861.386 1856.947 1849.568 1848.748 1847.690\n## 3 2055.658 2045.501 2041.428 2030.095 2040.210 2033.289 2028.345 2029.290\n##       1989     1990     1991     1992     1993     1994     1995     1996\n## 1 1593.225 1586.975 1573.084 1576.331 1569.725 1567.599 1567.113 1558.274\n## 2 1842.079 1831.907 1823.552 1815.864 1823.824 1830.623 1831.815 1831.648\n## 3 2031.741 2029.786 1991.807 1974.954 1973.737 1975.667 1980.278 1988.728\n##       1997     1998     1999     2000     2001     2002     2003     2004\n## 1 1555.079 1555.071 1557.103 1545.349 1530.207 1514.251 1509.647 1522.389\n## 2 1835.372 1836.030 1839.857 1827.264 1813.477 1781.696 1786.047 1781.858\n## 3 1985.076 1961.219 1966.310 1959.219 1946.954 1940.110 1924.799 1917.130\n##       2005     2006     2007     2008     2009     2010     2011     2012\n## 1 1514.492 1512.872 1515.299 1514.055 1493.875 1499.563 1503.049 1493.862\n## 2 1775.167 1776.759 1773.587 1771.648 1734.559 1736.098 1742.143 1735.396\n## 3 1923.496 1912.956 1902.156 1897.550 1858.657 1861.875 1861.608 1850.802\n##       2013     2014\n## 1 1485.589 1486.991\n## 2 1729.973 1729.543\n## 3 1848.158 1851.829\n\nclusters also contains the cluster item, which tells me to which cluster the different countries belong to. I can easily add this to the original data frame:\n\npwt_wide &lt;- pwt_wide %&gt;% \n  mutate(cluster = clusters$cluster)\n\nNow, let’s prepare the data for visualisation. I have to go back to a long data frame for this:\n\npwt_long &lt;- pwt_wide %&gt;%\n  pivot_longer(cols=c(-country, -cluster), names_to = \"year\", values_to = \"avh\") %&gt;%\n  mutate(year = ymd(paste0(year, \"-01-01\")))\n\ncenters_long &lt;- centers %&gt;%\n  pivot_longer(cols = -cluster, names_to = \"year\", values_to = \"avh\") %&gt;%  \n  mutate(year = ymd(paste0(year, \"-01-01\")))\n\nAnd I can now plot the different time series, by cluster and highlight the “average” time series for each cluster as well (yellow line):\n\nggplot() +\n  geom_line(data = pwt_long, aes(y = avh, x = year, group = country), colour = \"#82518c\") +\n  facet_wrap(~cluster, nrow = 1) + \n  geom_line(data = centers_long, aes(y = avh, x = year, group = cluster), col = \"#b58900\", size = 2) +\n  theme_blog() +\n  labs(title = \"Average hours worked in several countries\", \n       caption = \"The different time series have been clustered using k-means.\n                 Cluster 1: Belgium, Switzerland, Germany, Denmark, France, Luxembourg, Netherlands,\n                 Norway, Sweden.\\nCluster 2: Australia, Colombia, Ireland, Iceland, Japan, Mexico,\n                 Portugal, Turkey.\\nCluster 3: Argentina, Austria, Brazil, Canada, Cyprus, Spain, Finland,\n                 UK, Italy, New Zealand, Peru, USA, Venezuela\") +\n  theme(plot.caption = element_text(colour = \"white\"))"
  },
  {
    "objectID": "posts/2019-04-28-diffindiff_part1.html",
    "href": "posts/2019-04-28-diffindiff_part1.html",
    "title": "Fast food, causality and R packages, part 1",
    "section": "",
    "text": "I am currently working on a package for the R programming language; its initial goal was to simply distribute the data used in the Card and Krueger 1994 paper that you can read here (PDF warning).\n\n\nThe gist of the paper is to try to answer the following question: Do increases in minimum wages reduce employment? According to Card and Krueger’s paper from 1994, no. The authors studied a change in legislation in New Jersey which increased the minimum wage from $4.25 an hour to $5.05 an hour. The neighbourghing state of Pennsylvania did not introduce such an increase. The authors thus used the State of Pennsylvania as a control for the State of New Jersey and studied how the increase in minimum wage impacted the employment in fast food restaurants and found, against what economic theory predicted, an increase and not a decrease in employment. The authors used a method called difference-in-differences to asses the impact of the minimum wage increase.\n\n\nThis result was and still is controversial, with subsequent studies finding subtler results. For instance, showing that there is a reduction in employment following an increase in minimum wage, but only for large restaurants (see Ropponen and Olli, 2011).\n\n\nAnyways, this blog post will discuss how to create a package using to distribute the data. In a future blog post, I will discuss preparing the data to make it available as a demo dataset inside the package, and then writing and documenting functions.\n\n\nThe first step to create a package, is to create a new project:\n\n\n\n\n\nSelect “New Directory”:\n\n\n\n\n\nThen “R package”:\n\n\n\n\n\nand on the window that appears, you can choose the name of the package, as well as already some starting source files:\n\n\n\n\n\nAlso, I’d highly recommend you click on the “Create a git repository” box and use git within your project for reproducibility and sharing your code more easily. If you do not know git, there’s a lot of online resources to get you started. It’s not super difficult, but it does require making some new habits, which can take some time.\n\n\nI called my package {diffindiff}, and clicked on “Create Project”. This opens up a new project with a hello.R script, which gives you some pointers:\n\n# Hello, world!\n#\n# This is an example function named 'hello' \n# which prints 'Hello, world!'.\n#\n# You can learn more about package authoring with RStudio at:\n#\n#   http://r-pkgs.had.co.nz/\n#\n# Some useful keyboard shortcuts for package authoring:\n#\n#   Install Package:           'Ctrl + Shift + B'\n#   Check Package:             'Ctrl + Shift + E'\n#   Test Package:              'Ctrl + Shift + T'\n\nhello &lt;- function() {\n  print(\"Hello, world!\")\n}\n\nNow, to simplify the creation of your package, I highly recommend you use the {usethis} package. {usethis} removes a lot of the pain involved in creating packages.\n\n\nFor instance, want to start by adding a README file? Simply run:\n\nusethis::use_readme_md()\n✔ Setting active project to '/path/to/your/package/diffindiff'\n✔ Writing 'README.md'\n● Modify 'README.md'\n\nThis creates a README.md file in the root directory of your package. Simply change that file, and that’s it.\n\n\nThe next step could be setting up your package to work with {roxygen2}, which is very useful for writing documentation:\n\nusethis::use_roxygen_md()\n✔ Setting Roxygen field in DESCRIPTION to 'list(markdown = TRUE)'\n✔ Setting RoxygenNote field in DESCRIPTION to '6.1.1'\n● Run `devtools::document()`\n\nSee how the output tells you to run devtools::document()? This function will document your package, transforming the comments you write to describe your functions to documentation and managing the NAMESPACE file. Let’s run this function too:\n\ndevtools::document()\nUpdating diffindiff documentation\nFirst time using roxygen2. Upgrading automatically...\nLoading diffindiff\nWarning: The existing 'NAMESPACE' file was not generated by roxygen2, and will not be overwritten.\n\nYou might have a similar message than me, telling you that the NAMESPACE file was not generated by {roxygen2}, and will thus not be overwritten. Simply remove the file and run devtools::document() again:\n\ndevtools::document()\nUpdating diffindiff documentation\nFirst time using roxygen2. Upgrading automatically...\nWriting NAMESPACE\nLoading diffindiff\n\nBut what is actually the NAMESPACE file? This file is quite important, as it details where your package’s functions have to look for in order to use other functions. This means that if your package needs function foo() from package {bar}, it will consistently look for foo() inside {bar} and not confuse it with, say, the foo() function from the {barley} package, even if you load {barley} after {bar} in your interactive session. This can seem confusing now, but in the next blog posts I will detail this, and you will see that it’s not that difficult. Just know that it is an important file, and that you do not have to edit it by hand.\n\n\nNext, I like to run the following:\n\nusethis::use_pipe()\n✔ Adding 'magrittr' to Imports field in DESCRIPTION\n✔ Writing 'R/utils-pipe.R'\n● Run `devtools::document()`\n\nThis makes the now famous %&gt;% function available internally to your package (so you can use it to write the functions that will be included in your package) but also available to the users that will load the package.\n\n\nYour package is still missing a license. If you plan on writing a package for your own personal use, for instance, a collection of functions, there is no need to think about licenses. But if you’re making your package available through CRAN, then you definitely need to think about it. For this package, I’ll be using the MIT license, because the package will distribute data which I do not own (I’ve got permission from Card to re-distribute it) and thus I think it would be better to use a permissive license (I don’t know if the GPL, another license, which is stricter in terms of redistribution, could be used in this case).\n\nusethis::use_mit_license()\n✔ Setting License field in DESCRIPTION to 'MIT + file LICENSE'\n✔ Writing 'LICENSE.md'\n✔ Adding '^LICENSE\\\\.md$' to '.Rbuildignore'\n✔ Writing 'LICENSE'\n\nWe’re almost done setting up the structure of the package. If we forget something though, it’s not an issue, we’ll just have to run the right use_* function later on. Let’s finish by preparing the folder that will contains the script to prepare the data:\n\nusethis::use_data_raw()\n✔ Creating 'data-raw/'\n✔ Adding '^data-raw$' to '.Rbuildignore'\n✔ Writing 'data-raw/DATASET.R'\n● Modify 'data-raw/DATASET.R'\n● Finish the data preparation script in 'data-raw/DATASET.R'\n● Use `usethis::use_data()` to add prepared data to package\n\nThis creates the data-raw folder with the DATASET.R script inside. This is the script that will contain the code to download and prepare datasets that you want to include in your package. This will be the subject of the next blog post.\n\n\nLet’s now finish by documenting the package, and pushing everything to Github:\n\ndevtools::document()\n\nThe following lines will only work if you set up the Github repo:\n\ngit add .\ngit commit -am \"first commit\"\ngit push origin master"
  },
  {
    "objectID": "posts/2017-07-02-dplyr-0-70-tutorial.html",
    "href": "posts/2017-07-02-dplyr-0-70-tutorial.html",
    "title": "Lesser known dplyr 0.7* tricks",
    "section": "",
    "text": "This blog post is an update to an older one I wrote in March. In the post from March, dplyr was at version 0.50, but since then a major update introduced some changes that make some of the tips in that post obsolete. So here I revisit the blog post from March by using dplyr 0.70."
  },
  {
    "objectID": "posts/2017-07-02-dplyr-0-70-tutorial.html#create-new-columns-with-mutate-and-case_when",
    "href": "posts/2017-07-02-dplyr-0-70-tutorial.html#create-new-columns-with-mutate-and-case_when",
    "title": "Lesser known dplyr 0.7* tricks",
    "section": "\nCreate new columns with mutate() and case_when()\n",
    "text": "Create new columns with mutate() and case_when()\n\n\nThe basic things such as selecting columns, renaming them, filtering, etc did not change with this new version. What did change however is creating new columns using case_when(). First, load dplyr and the mtcars dataset:\n\nlibrary(\"dplyr\")\ndata(mtcars)\n\nThis was how it was done in version 0.50 (notice the .$ symbol before the variable carb):\n\nmtcars %&gt;%\n    mutate(carb_new = case_when(.$carb == 1 ~ \"one\",\n                                .$carb == 2 ~ \"two\",\n                                .$carb == 4 ~ \"four\",\n                                 TRUE ~ \"other\")) %&gt;%\n    head(5)\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb carb_new\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4     four\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4     four\n## 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1      one\n## 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1      one\n## 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2      two\n\nThis has been simplified to:\n\nmtcars %&gt;%\n    mutate(carb_new = case_when(carb == 1 ~ \"one\",\n                                carb == 2 ~ \"two\",\n                                carb == 4 ~ \"four\",\n                                TRUE ~ \"other\")) %&gt;%\n    head(5)\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb carb_new\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4     four\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4     four\n## 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1      one\n## 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1      one\n## 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2      two\n\nNo need for .$ anymore."
  },
  {
    "objectID": "posts/2017-07-02-dplyr-0-70-tutorial.html#apply-a-function-to-certain-columns-only-by-rows-with-purrrlyr",
    "href": "posts/2017-07-02-dplyr-0-70-tutorial.html#apply-a-function-to-certain-columns-only-by-rows-with-purrrlyr",
    "title": "Lesser known dplyr 0.7* tricks",
    "section": "\nApply a function to certain columns only, by rows, with purrrlyr\n",
    "text": "Apply a function to certain columns only, by rows, with purrrlyr\n\n\ndplyr wasn’t the only package to get an overhaul, purrr also got the same treatment.\n\n\nIn the past, I applied a function to certains columns like this:\n\nmtcars %&gt;%\n    select(am, gear, carb) %&gt;%\n    purrr::by_row(sum, .collate = \"cols\", .to = \"sum_am_gear_carb\") -&gt; mtcars2\nhead(mtcars2)\n\nNow, by_row() does not exist in purrr anymore, but instead a new package called purrrlyr was introduced with functions that don’t really fit inside purrr nor dplyr:\n\nmtcars %&gt;%\n    select(am, gear, carb) %&gt;%\n    purrrlyr::by_row(sum, .collate = \"cols\", .to = \"sum_am_gear_carb\") -&gt; mtcars2\nhead(mtcars2)\n## # A tibble: 6 x 4\n##      am  gear  carb sum_am_gear_carb\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;\n## 1     1     4     4                9\n## 2     1     4     4                9\n## 3     1     4     1                6\n## 4     0     3     1                4\n## 5     0     3     2                5\n## 6     0     3     1                4\n\nThink of purrrlyr as purrrs and dplyrs love child."
  },
  {
    "objectID": "posts/2017-07-02-dplyr-0-70-tutorial.html#using-dplyr-functions-inside-your-own-functions-or-what-is-tidyeval",
    "href": "posts/2017-07-02-dplyr-0-70-tutorial.html#using-dplyr-functions-inside-your-own-functions-or-what-is-tidyeval",
    "title": "Lesser known dplyr 0.7* tricks",
    "section": "\nUsing dplyr functions inside your own functions, or what is tidyeval\n",
    "text": "Using dplyr functions inside your own functions, or what is tidyeval\n\n\nProgramming with dplyr has been simplified a lot. Before version 0.70, one needed to use dplyr in conjuction with lazyeval to use dplyr functions inside one’s own fuctions. It was not always very easy, especially if you mixed columns and values inside your functions. Here’s the example from the March blog post:\n\nextract_vars &lt;- function(data, some_string){\n\n  data %&gt;%\n    select_(lazyeval::interp(~contains(some_string))) -&gt; data\n\n  return(data)\n}\n\nextract_vars(mtcars, \"spam\")\n\nMore examples are available in this other blog post.\n\n\nI will revisit them now with dplyr’s new tidyeval syntax. I’d recommend you read the Tidy evaluation vignette here. This vignette is part of the rlang package, which gets used under the hood by dplyr for all your programming needs. Here is the function I called simpleFunction(), written with the old dplyr syntax:\n\nsimpleFunction &lt;- function(dataset, col_name){\n  dataset %&gt;%\n    group_by_(col_name) %&gt;%\n    summarise(mean_mpg = mean(mpg)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, \"cyl\")\n## # A tibble: 3 x 2\n##     cyl mean_mpg\n##   &lt;dbl&gt;    &lt;dbl&gt;\n## 1     4     26.7\n## 2     6     19.7\n## 3     8     15.1\n\nWith the new synax, it must be rewritten a little bit:\n\nsimpleFunction &lt;- function(dataset, col_name){\n  col_name &lt;- enquo(col_name)\n  dataset %&gt;%\n    group_by(!!col_name) %&gt;%\n    summarise(mean_mpg = mean(mpg)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, cyl)\n## # A tibble: 3 x 2\n##     cyl mean_mpg\n##   &lt;dbl&gt;    &lt;dbl&gt;\n## 1     4     26.7\n## 2     6     19.7\n## 3     8     15.1\n\nWhat has changed? Forget the underscore versions of the usual functions such as select_(), group_by_(), etc. Now, you must quote the column name using enquo() (or just quo() if working interactively, outside a function), which returns a quosure. This quosure can then be evaluated using !! in front of the quosure and inside the usual dplyr functions.\n\n\nLet’s look at another example:\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  filter_criteria &lt;- lazyeval::interp(~y == x, .values=list(y = as.name(col_name), x = value))\n  dataset %&gt;%\n    filter_(filter_criteria) %&gt;%\n    summarise(mean_cyl = mean(cyl)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, \"am\", 1)\n##   mean_cyl\n## 1 5.076923\n\nAs you can see, it’s a bit more complicated, as you needed to use lazyeval::interp() to make it work. With the improved dplyr, here’s how it’s done:\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  col_name &lt;- enquo(col_name)\n  dataset %&gt;%\n    filter((!!col_name) == value) %&gt;%\n    summarise(mean_cyl = mean(cyl)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, am, 1)\n##   mean_cyl\n## 1 5.076923\n\nMuch, much easier! There is something that you must pay attention to though. Notice that I’ve written:\n\nfilter((!!col_name) == value)\n\nand not:\n\nfilter(!!col_name == value)\n\nI have enclosed !!col_name inside parentheses. I struggled with this, but thanks to help from @dmi3k and @_lionelhenry I was able to understand what was happening (isn’t the #rstats community on twitter great?).\n\n\nOne last thing: let’s make this function a bit more general. I hard-coded the variable cyl inside the body of the function, but maybe you’d like the mean of another variable? Easy:\n\nsimpleFunction &lt;- function(dataset, group_col, mean_col, value){\n  group_col &lt;- enquo(group_col)\n  mean_col &lt;- enquo(mean_col)\n  dataset %&gt;%\n    filter((!!group_col) == value) %&gt;%\n    summarise(mean((!!mean_col))) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, am, cyl, 1)\n##   mean(cyl)\n## 1  5.076923\n\n«That’s very nice Bruno, but mean((cyl)) in the output looks ugly as sin» you might think, and you’d be right. It is possible to set the name of the column in the output using := instead of =:\n\nsimpleFunction &lt;- function(dataset, group_col, mean_col, value){\n  group_col &lt;- enquo(group_col)\n  mean_col &lt;- enquo(mean_col)\n  mean_name &lt;- paste0(\"mean_\", mean_col)[2]\n  dataset %&gt;%\n    filter((!!group_col) == value) %&gt;%\n    summarise(!!mean_name := mean((!!mean_col))) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, am, cyl, 1)\n##   mean_cyl\n## 1 5.076923\n\nTo get the name of the column I added this line:\n\nmean_name &lt;- paste0(\"mean_\", mean_col)[2]\n\nTo see what it does, try the following inside an R interpreter (remember to us quo() instead of enquo() outside functions!):\n\npaste0(\"mean_\", quo(cyl))\n## [1] \"mean_~\"   \"mean_cyl\"\n\nenquo() quotes the input, and with paste0() it gets converted to a string that can be used as a column name. However, the ~ is in the way and the output of paste0() is a vector of two strings: the correct name is contained in the second element, hence the [2]. There might be a more elegant way of doing that, but for now this has been working well for me.\n\n\nThat was it folks! I do recommend you read the Programming with dplyr vignette here as well as other blog posts, such as the one recommended to me by @dmi3k here.\n\n\nHave fun with dplyr 0.70!"
  },
  {
    "objectID": "posts/2023-08-12-nix_for_r_part4.html",
    "href": "posts/2023-08-12-nix_for_r_part4.html",
    "title": "Reproducible data science with Nix, part 4 – So long, {renv} and Docker, and thanks for all the fish",
    "section": "",
    "text": "For this blog post, I also made a youtube video that goes over roughly the same ideas, but the blog post is more detailed as I explain the contents of default.nix files, which I don’t do in the video. Watch the video here.\nThis is the fourth post in a series of posts about Nix. Disclaimer: I’m a super beginner with Nix. So this series of blog posts is more akin to notes that I’m taking while learning than a super detailed Nix tutorial. So if you’re a Nix expert and read something stupid in here, that’s normal. This post is going to focus on R (obviously) but the ideas are applicable to any programming language.\nIf you’ve never heard of Nix, take a look at part 1.\nIn this blog post I will go over many, nitty-gritty details and explain, line by line, what a Nix expression you can use to build an environment for your projects contains. In practice, building such an environment allows you to essentially replace {renv}+Docker, but writing the right expressions to achieve it is not easy. So this blog post will also go over the features of {rix}, an R package by Philipp Baumann and myself.\nLet me also address the click-bait title directly. Yes, the title is click-bait and I got you. I don’t believe that {renv} and Docker are going away any time soon and you should not hesitate to invest the required time to get to know and use these tools (I wrote something by the way). But I am more and more convinced that Nix is an amazing alternative that offers many possibilities, albeit with a high entry cost. By writing {rix}, we aimed at decreasing this entry cost as much as possible. However, more documentation, examples, etc., need to be written and more testing is required. This series of blog posts is a first step to get the word out and get people interested in the package and more broadly in Nix. So if you’re interested or intrigued, don’t hesitate to get in touch!\nThis will be a long and boring post. Unless you really want to know how all of this works go watch the Youtube video, which is more practical instead. I needed to write this down, as it will likely serve as documentation. I’m essentially beta testing it with you, so if you do take the time to read, and even better, to try out the code, please let us know how it went! Was it clear, was it simple, was it useful? Many thanks in advance."
  },
  {
    "objectID": "posts/2023-08-12-nix_for_r_part4.html#part-1-starting-a-new-project-with-nix",
    "href": "posts/2023-08-12-nix_for_r_part4.html#part-1-starting-a-new-project-with-nix",
    "title": "Reproducible data science with Nix, part 4 – So long, {renv} and Docker, and thanks for all the fish",
    "section": "\nPart 1: starting a new project with Nix\n",
    "text": "Part 1: starting a new project with Nix\n\n\nLet’s suppose that you don’t even have R installed on your computer yet. Maybe you bought a new computer, or changed operating system, whatever. Maybe you even have R already, which you installed from the installer that you can download from the R project website. It doesn’t matter, as we are going to install a (somewhat) isolated version of R using Nix for the purposes of this blog post. If you don’t know where to start, it’s simple: first, use the installer from Determinate Systems. This installer will make it easy to install Nix on Linux, macOS or Windows (with WSL2). Once you have Nix installed, you can use it to install R and {rix} to start building reproducible development environments. To help you get started, you can run this line here (as documented in {rix}’s Readme), which will drop you into a Nix shell with R and {rix} available. Run the line inside a terminal (if you’re running Windows, run this in a Linux distribution that you installed for WSL2):\n\nnix-shell --expr \"$(curl -sl https://raw.githubusercontent.com/b-rodrigues/rix/master/inst/extdata/default.nix)\"\n\nThis will take a bit to run, and then you will be inside an R session. This environment is not suited for development, but is only provided as an easy way for you to start using {rix}. Using {rix}, you can now use it to create a more complex environment suited for a project that you would like to start. Let’s start by loading {rix}:\n\nlibrary(rix)\n\nNow you can run the following command to create an environment with the latest version of R and some packages (change the R version and list of packages to suit your needs):\n\npath_default_nix &lt;- \"path/to/my/project\"\n\nrix(r_ver = \"current\",\n    r_pkgs = c(\"dplyr\", \"ggplot2\"),\n    other_pkgs = NULL,\n    git_pkgs = list(package_name = \"housing\",\n                    repo_url = \"https://github.com/rap4all/housing\",\n                    branch_name = \"fusen\",\n                    commit = \"1c860959310b80e67c41f7bbdc3e84cef00df18e\"),\n    ide = \"rstudio\",\n    project_path = path_default_nix,\n    overwrite = TRUE)\n\nRunning the code above will create the following default.nix file in path/to/my/project:\n\n# This file was generated by the {rix} R package on Sat Aug 12 22:18:55 2023\n# with following call:\n# &gt;rix(r_ver = \"cf73a86c35a84de0e2f3ba494327cf6fb51c0dfd\",\n#  &gt; r_pkgs = c(\"dplyr\",\n#  &gt; \"ggplot2\"),\n#  &gt; other_pkgs = NULL,\n#  &gt; git_pkgs = list(package_name = \"housing\",\n#  &gt; repo_url = \"https://github.com/rap4all/housing\",\n#  &gt; branch_name = \"fusen\",\n#  &gt; commit = \"1c860959310b80e67c41f7bbdc3e84cef00df18e\"),\n#  &gt; ide = \"rstudio\",\n#  &gt; project_path = path_default_nix,\n#  &gt; overwrite = TRUE)\n# It uses nixpkgs' revision cf73a86c35a84de0e2f3ba494327cf6fb51c0dfd for reproducibility purposes\n# which will install R as it was as of nixpkgs revision: cf73a86c35a84de0e2f3ba494327cf6fb51c0dfd\n# Report any issues to https://github.com/b-rodrigues/rix\n{ pkgs ? import (fetchTarball \"https://github.com/NixOS/nixpkgs/archive/cf73a86c35a84de0e2f3ba494327cf6fb51c0dfd.tar.gz\") {} }:\n\nwith pkgs;\n\nlet\n  my-r = rWrapper.override {\n    packages = with rPackages; [\n        dplyr\n        ggplot2\n        (buildRPackage {\n          name = \"housing\";\n          src = fetchgit {\n          url = \"https://github.com/rap4all/housing\";\n          branchName = \"fusen\";\n          rev = \"1c860959310b80e67c41f7bbdc3e84cef00df18e\";\n          sha256 = \"sha256-s4KGtfKQ7hL0sfDhGb4BpBpspfefBN6hf+XlslqyEn4=\";\n          };\n          propagatedBuildInputs = [\n            dplyr\n            ggplot2\n            janitor\n            purrr\n            readxl\n            rlang\n            rvest\n            stringr\n            tidyr\n            ];\n          })\n        ];\n    };\n  my-rstudio = rstudioWrapper.override {\n    packages = with rPackages; [\n        dplyr\n        ggplot2\n        (buildRPackage {\n          name = \"housing\";\n          src = fetchgit {\n          url = \"https://github.com/rap4all/housing\";\n          branchName = \"fusen\";\n          rev = \"1c860959310b80e67c41f7bbdc3e84cef00df18e\";\n          sha256 = \"sha256-s4KGtfKQ7hL0sfDhGb4BpBpspfefBN6hf+XlslqyEn4=\";\n          };\n          propagatedBuildInputs = [\n            dplyr\n            ggplot2\n            janitor\n            purrr\n            readxl\n            rlang\n            rvest\n            stringr\n            tidyr\n            ];\n          })\n        ];\n    };\nin\n mkShell {\n   LOCALE_ARCHIVE = \"${glibcLocales}/lib/locale/locale-archive\";\n     buildInputs = [\n        my-r\n        my-rstudio\n      ];\n }\n\nLet’s go through it. The first thing you will notice is that this file is written in a language that you might not know: this language is called Nix as well! So Nix can both refer to the package manager, but also to the programming language. The Nix programming language was designed for creating and composing derivations. A derivation is Nix jargon for a package (not necessarily an R package; any piece of software that you can install through Nix is a package). To know more about the language itself, you can RTFM.\n\n\nLet’s go back to our default.nix file. The first lines state the revision of nixpkgs used that is being used in this expression, as well as which version of R gets installed through it. nixpkgs is Nix’s repository which contains all the software that we will be installing. This is important to understand: since all the expressions that build all the software available through nixpkgs are versioned on Github, it is possible to choose a particular commit, or revision, and use that particular release of nixpkgs. So by judiciously choosing the right commit, it’s possible to install any version of R (well any version until 3.0.2). {rix} takes care of this for you: state the version of R that is needed, and the right revision will be returned (the list of R versions and revisions can be found here).\n\n\nThe call that was used to generate the default.nix file is also saved, but if you look at the argument r_ver, the nixpkgs revision is specified instead of “current”. This is because if you re-run this call but keep r_ver = “current”, another, more recent nixpkgs revision will get used instead, which will break reproducibility. To avoid this, the expression gets changed, so if you re-run it, you’re sure to find the exact same environment.\n\n\nThen comes this line:\n\n{ pkgs ? import (fetchTarball \"https://github.com/NixOS/nixpkgs/archive/cf73a86c35a84de0e2f3ba494327cf6fb51c0dfd.tar.gz\") {} }:\n\nThis actually defines a function with argument pkgs that is optional (hence the ?). All that follows, import (fetchTarball … ) {} is the default value for pkgs if no argument is provided when you run this (which will always be the case). So here, if I call this function without providing any pkgs argument, the release of nixpkgs at that commit will be used. Then comes:\n\nwith pkgs;\n\nlet\n  my-pkgs = rWrapper.override {\n    packages = with rPackages; [\n      dplyr\n      ggplot2\n\nThe with pkgs statement makes all the imported packages available in the scope of the function. So I can write quarto if I want to install Quarto (the program that compiles .qmd files, not the {quarto} R package that provides bindings to it) instead of nixpkgs.quarto. Actually, R also has with(), so you can write this:\n\nwith(mtcars, plot(mpg ~ hp))\n\n\n\n\ninstead of this:\n\nplot(mtcars$mpg ~ mtcars$hp)\n\nThen follows a let … in. This is how a variable gets defined locally, for example, this is a valid Nix statement:\n\nlet x = 1; y = 2; in x + y\n\nwhich will obviously return 3. So here we are defining a series of packages that will ultimately be available in our environment. These packages are named my-pkgs and are a list of R packages. You can see that I use a wrapper called rWrapper which changes certain options to make R installed through Nix work well. This wrapper has a packages attribute which I override using its .override method, and then I redefine packages as a list of R packages. Just like before, I use with rPackages before listing them, which allows me to write dplyr instead of rPackages.dplyr to refer to the {dplyr} packages. R packages that have a . character in their name must be written using _, so if you need {data.table} you’ll need to write data_table (but {rix} does this for you as well, so don’t worry). Then follows the list of R packages available through nixpkgs (which is the entirety of CRAN:\n\npackages = with rPackages; [\n          dplyr\n          ggplot2\n\nEach time you need to add a package, add it here, and rebuild your environment, do not run install.packages(blabla) to install the {blabla} package, because it’s likely not going to work anyways, and it’s not reproducible. Your projects need to be entirely defined as code. This also means that packages that have helper functions that install something, for example tinytex::install_tinytex(), cannot be used anymore. Instead, you will need to install texlive (by putting it in other_pkgs) and rebuild the expression. We plan to write vignettes documenting all these use-cases. For example, my blog is still built using Hugo (and will likely stay like this forever). I’m using a very old version of Hugo to generate it (I don’t want to upgrade and have to deal with potential issues), so I install the right version I need using Nix, instead of using blogdown::install_hugo().\n\n\nThen comes the expression that installs a package from Github:\n\n(buildRPackage {\n  name = \"housing\";\n  src = fetchgit {\n  url = \"https://github.com/rap4all/housing\";\n  branchName = \"fusen\";\n  rev = \"1c860959310b80e67c41f7bbdc3e84cef00df18e\";\n  sha256 = \"sha256-s4KGtfKQ7hL0sfDhGb4BpBpspfefBN6hf+XlslqyEn4=\";\n  };\n  propagatedBuildInputs = [\n    dplyr\n    ggplot2\n    janitor\n    purrr\n    readxl\n    rlang\n    rvest\n    stringr\n    tidyr\n    ];\n})\n\nAs you can see it’s quite a mouthful, but it was generated from this R code only:\n\ngit_pkgs = list(package_name = \"housing\",\n                repo_url = \"https://github.com/rap4all/housing\",\n                branch_name = \"fusen\",\n                commit = \"1c860959310b80e67c41f7bbdc3e84cef00df18e\"),\n\nIf you want to install more than one package, you can also provide a list of lists, for example:\n\ngit_pkgs = list(\n  list(package_name = \"housing\",\n       repo_url = \"https://github.com/rap4all/housing/\",\n       branch_name = \"fusen\",\n       commit = \"1c860959310b80e67c41f7bbdc3e84cef00df18e\"),\n  list(package_name = \"fusen\",\n       repo_url = \"https://github.com/ThinkR-open/fusen\",\n       branch_name = \"main\",\n       commit = \"d617172447d2947efb20ad6a4463742b8a5d79dc\")\n),\n...\n\nand the right expressions will be generated. There’s actually a lot going on here, so let me explain. The first thing is the sha256 field. This field contains a hash that gets generated by Nix, and that must be provided by the user. But users rarely, if ever, know this value, so instead what they do is they try to build the expression without providing it. An error message like this one gets returned:\n\nerror: hash mismatch in fixed-output derivation '/nix/store/449zx4p6x0yijym14q3jslg55kihzw66-housing-1c86095.drv':\n         specified: sha256-AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\n            got:    sha256-s4KGtfKQ7hL0sfDhGb4BpBpspfefBN6hf+XlslqyEn4=\n\nThe sha256 can now get copy-and-pasted into the expression. This approach is called “Trust On First Use”, or TOFU for short. Because this is quite annoying, {rix} provides a “private” function, called get_sri_hash_deps() that generates this hash for you. The issue is that this hash cannot be computed easily if you don’t have Nix installed, and since I don’t want to force users to install Nix to use {rix}, what I did is that I set up a server with Nix installed and a {plumber} api. get_sri_hash_deps() makes a call to that api and gets back the sha256, and also a list of packages (more on this later).\n\n\nYou can try making a call to the api if you have curl installed on your system:\n\ncurl -X GET \"http://git2nixsha.dev:1506/hash?repo_url=https://github.com/rap4all/housing/&branchName=fusen&commit=1c860959310b80e67c41f7bbdc3e84cef00df18e\" -H \"accept: */*\"\n\nThis is what you will get back:\n\n{\n  \"sri_hash\" : [\"sha256-s4KGtfKQ7hL0sfDhGb4BpBpspfefBN6hf+XlslqyEn4=\"],\n  \"deps\"     : [\"dplyr ggplot2 janitor purrr readxl rlang rvest stringr tidyr\"]\n}\n\nThe reason computing sri_hash is not easy is because it gets computed on the folder containing the source code (after having deleted the .git folder in the case of a Github repo) after it was serialised. You are certainly familiar with serialisations such as the ZIP or TAR serialisation (in other words, zipping a folder is “serialising” it). But these serialisation algorithms come with certain shortcomings that I won’t discuss here, but if you’re interested check out section 5.2. The Nix store from Eelco Dolstra’s Phd thesis which you can find here. Instead, a Nix-specific serialisation algorithm was developed, called NAR. So to compute this hash, I either had to implement this serialisation algorithm in R, or write an api that does that for me by using the implementation that ships with Nix. Since I’m not talented enough to implement such an algorithm in R, I went for the api. But who knows, maybe in the future this could be done. There are implementation of this algorithm in other programming languages like Rust, so maybe packaging the Rust binary could be an option.\n\n\nThis gets then further processed by rix(). The second thing that gets returned is a list of packages. These get scraped from the Imports and LinkingTo sections of the DESCRIPTION file from the package and are then provided as the propagatedBuildInputs in the Nix expression. These packages are dependencies that must be available to your package at build and run-time.\n\n\nYou should know that as of today ({rix} commit 15cadf7f) GitHub packages that use the Remotes field (so that have dependencies that are also on Github) are not handled by {rix}, but supporting this is planned. What {rix} supports though is installing packages from the CRAN archives, so you can specify a version of a package and have that installed. For example:\n\nrix(r_ver = \"current\",\n    r_pkgs = c(\"dplyr@0.8.0\", \"ggplot2@3.1.1\"),\n    other_pkgs = NULL,\n    git_pkgs = NULL,\n    ide = \"other\",\n    path = path_default_nix,\n    overwrite = TRUE)\n\nThe difference with the default.nix file from before is that these packages get downloaded off the CRAN archives, so fetchzip() is used to download them instead of fetchgit() (both Nix functions). Here is what the generated Nix code looks like:\n\n(buildRPackage {\n  name = \"dplyr\";\n  src = fetchzip {\n  url = \"https://cran.r-project.org/src/contrib/Archive/dplyr/dplyr_0.8.0.tar.gz\";\n  sha256 = \"sha256-f30raalLd9KoZKZSxeTN71PG6BczXRIiP6g7EZeH09U=\";\n  };\n  propagatedBuildInputs = [\n    assertthat\n    glue\n    magrittr\n    pkgconfig\n    R6\n    Rcpp\n    rlang\n    tibble\n    tidyselect\n    BH\n    plogr\n    Rcpp\n    ];\n})\n(buildRPackage {\n  name = \"ggplot2\";\n  src = fetchzip {\n  url = \"https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_3.1.1.tar.gz\";\n  sha256 = \"sha256-0Qv/5V/XMsFBcGEFy+3IAaBJIscRMTwGong6fiP5Op0=\";\n  };\n  propagatedBuildInputs = [\n    digest\n    gtable\n    lazyeval\n    MASS\n    mgcv\n    plyr\n    reshape2\n    rlang\n    scales\n    tibble\n    viridisLite\n    withr\n    ];\n})\n\nHere’s what this looks like:\n\n\n\n\n\n\n\nThis feature should ideally be used sparingly. If you want to reconstruct an environment as it was around a specific date (for example to run an old project), use the version of R that was current at that time. This will ensure that every package that gets installed is at a version compatible with that version of R, which might not be the case if you need to install a very old version of one particular package. But this feature is quite useful if you want to install a package that is not available on CRAN anymore, but that is archived, like {ZeligChoice}.\n\n\nThen a second list of packages gets defined, this time using the rstudioWrapper wrapper. This is because I specified that I wanted to use RStudio, but RStudio is a bit peculiar. It redefines many paths and so if you have RStudio installed in your system, it won’t be able to “see” the R installed through Nix. So you have to install RStudio through Nix as well (this is not necessary for VS Code nor Emacs, and likely not for other editors as well). However, it is still necessary to provide each package, again, to the rstudioWrapper. This is because the RStudio installed through Nix is also not able to “see” the R installed through Nix as well. But don’t worry, this does not take twice the space, since the packages simply get symlinked.\n\n\nThe last part of the expression uses mkShell which builds a shell with the provided buildInputs (our list of packages). There is also a line to define the location of the locale archive, which should properly configure the locale of the shell (so language, time zone and units):\n\nin\n mkShell {\n   LOCALE_ARCHIVE = \"${glibcLocales}/lib/locale/locale-archive\";\n     buildInputs = [\n        my-r\n        my-rstudio\n      ];\n }\n\nWith this file in hand, we can now build the environment and use it."
  },
  {
    "objectID": "posts/2023-08-12-nix_for_r_part4.html#part-2-using-your-environment",
    "href": "posts/2023-08-12-nix_for_r_part4.html#part-2-using-your-environment",
    "title": "Reproducible data science with Nix, part 4 – So long, {renv} and Docker, and thanks for all the fish",
    "section": "\nPart 2: using your environment\n",
    "text": "Part 2: using your environment\n\n\nSo let’s suppose that you have a default.nix file and you wish to build the environment. To do so, you need to have Nix installed, and, thanks to the contributions of Philipp Baumann, you can use rix::nix_build() to build the environment as well:\n\nnix_build(project_path = path_default_nix, exec_mode = \"blocking\")\n\nIf you prefer, you can use Nix directly as well; navigate to the project folder containing the default.nix file and run the command line tool nix-build that gets installed with Nix:\n\nnix-build\n\nThis will take some time to run, depending on whether cached binary packages can be pulled from https://cache.nixos.org/ or not. Once the build process is done, you should see a file called result next to the default.nix file. You can now drop into the Nix shell by typing this into your operating system’s terminal (after you navigated to the folder containing the default.nix and result files):\n\nnix-shell\n\n(this time, you really have to leave your current R session! But Philipp and myself are thinking about how we could also streamline this part as well…).\n\n\nThe environment that you just built is not an entirely isolated environment: you can still interact with your computer, unlike with Docker. For example, you can still use programs that are installed on your computer. This means that you can run your usual editor as well, but starting it from the Nix shell will make your editor be able to “see” the R installed in that environment. You need to be careful with this, because sometimes this can lead to surprising behavior. For example, if you already have R installed with some packages, these packages could interfere with your Nix environment. There are two ways of dealing with this: you either only use Nix-based environments to work (which would be my primary recommendation, as there can be no interference between different Nix environments), or you call nix-shell –pure instead of just nix-shell. This will ensure that only whatever is available in the environment gets used, but be warned that Nix environments are very, very lean, so you might need to add some tools to have something completely functional.\n\n\nWe can take advantage of the fact that environments are not completely isolated to use our IDEs. For example, if you use VS Code or Emacs, you can use the one that is installed directly on your system, as explained before. As already explained, but to drive the point home, if you’re an RStudio user, you need to specify the ide = “rstudio” argument to rix(), because in the case of RStudio, it needs to be installed by Nix as well (the current available RStudio version installed by Nix is now out of date, but efforts are ongoing to update it). This is because RStudio looks for R runtimes in very specific paths, and these need to be patched to see Nix-provided R versions. Hence the version that gets installed by Nix gets patched so that RStudio is able to find the correct runtimes.\n\n\nOnce you dropped into the shell, simply type rstudio to launch RStudio in that environment (or code if you use VS Code or other if you use Emacs, or any other editor). On Linux, RStudio may fail to launch with this error message:\n\nCould not initialize GLX\nAborted (core dumped)\n\nchange your default.nix file from this:\n\nmkShell {\n  LOCALE_ARCHIVE = \"${glibcLocales}/lib/locale/locale-archive\";\n    buildInputs = [\n       my-r\n       my-rstudio\n     ];\n}\n\nto this:\n\nmkShell {\n  LOCALE_ARCHIVE = \"${glibcLocales}/lib/locale/locale-archive\";\n    buildInputs = [\n       my-r\n       my-rstudio\n     ];\n  shellHook = ''\n    export QT_XCB_GL_INTEGRATION=none\n  '';\n}\n\nwhich should solve the issue, which is related to hardware acceleration as far as I can tell.\n\n\nshellHooks are a nice feature which I haven’t discussed a lot yet (I did so in part 2 of this series, to run a {targets} pipeline each time I dropped into the shell). Whatever goes into the shellHook gets executed as soon as one drops into the Nix shell. I personally have to add the export QT_XCB_GL_INTEGRATION=none line in on virtual machines and on my desktop computer as well, but I’ve had problems in the past with my graphics drivers, and I think it’s related. I’m planning also to add an option to rix() to add this automatically.\n\n\nIf you need to add packages, best is to call rix::rix() again, but this time, provide the nixpkgs revision as the argument to r_ver. Copy and paste the call from the generated default.nix to an R console and rerun it:\n\nrix(r_ver = \"cf73a86c35a84de0e2f3ba494327cf6fb51c0dfd\",\n    r_pkgs = c(\"dplyr\", \"ggplot2\", \"tidyr\", \"quarto\"),\n    other_pkgs = \"quarto\",\n    git_pkgs = list(package_name = \"housing\",\n                    repo_url = \"https://github.com/rap4all/housing\",\n                    branch_name = \"fusen\",\n                    commit = \"1c860959310b80e67c41f7bbdc3e84cef00df18e\"),\n    ide = \"rstudio\",\n    path = path_default_nix,\n    overwrite = TRUE)\n\nIn the call above I’ve added the {tidyr} and {quarto} packages, as well as the quarto command line utility to generate .qmd files. For r_ver I’m this time using the nixpkgs revision from my original default.nix file. This will ensure that my environment stays the same.\n\n\nSo if you have read up until this point, let me first thank you, and secondly humbly ask you to test {rix}! I’m looking for testers, especially on Windows and macOS, and would be really grateful if you could provide some feedback on the package. To report anything, simply open issue here.\n\n\nThanks to Philipp for proof-reading this post."
  },
  {
    "objectID": "posts/2023-01-12-repro_R.html#you-expect-me-to-read-this-long-ass-blog-post",
    "href": "posts/2023-01-12-repro_R.html#you-expect-me-to-read-this-long-ass-blog-post",
    "title": "MRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?",
    "section": "\nYou expect me to read this long ass blog post?\n",
    "text": "You expect me to read this long ass blog post?\n\n\nIf you’re too busy to read this blog post, know that I respect your time. The table below summarizes this blog post:\n\n\n\n\n\n\n\n\n\nNeed\n\n\nSolution\n\n\n\n\n\n\nI want to start a project and make it reproducible.\n\n\n{renv} and Docker\n\n\n\n\nThere’s an old script laying around that I want to run.\n\n\n{groundhog} and Docker\n\n\n\n\nI want to work inside an environment that\n\n\nDocker and the Posit\n\n\n\n\nenables me to run code in a reproducible way.\n\n\nCRAN mirror.\n\n\n\n\n\nBut this table doesn’t show the whole picture, especially the issues with relying so much on Docker. So if you’re interesting in making science and your work reproducible and robust, my advice is that you stop doomscrolling on social media and keep on reading. If at the end of the blog post you think that this was a waste of time, just sent an insult my way, that’s ok.\n\n\nI learnt last week that MRAN is going to get shutdown this year. For those of you that don’t know, MRAN was a CRAN mirror managed by Microsoft. What made MRAN stand out was the fact that Microsoft took daily snapshots of CRAN and it was thus possible to quite easily install old packages using the {checkpoint} package. This was a good thing for reproducibility, and for Windows and macOS, it was even possible to install binary packages, so no need to compile them from source.\n\n\nLast year I had the opportunity to teach a course on building reproducible analytical pipelines at the University of Luxembourg, and made my teaching material available as an ebook that you can find here. I also wrote some blog posts about reproducibility and it looks like I will be continuing this trend for the forseeable future.\n\n\nSo in this blog post I’m going to show what you, as an R user, can do to make your code reproducible now that MRAN is getting shutdown. MRAN is not the only option for reproducibility, and I’m going to present in this blog post everything I know about other options. So if you happen to know of some solution that I don’t discuss here, please leave a comment here. But this blog post is not just a tutorial. I will also discuss what I think is a major risk that is coming and what, maybe, we can do to avoid it."
  },
  {
    "objectID": "posts/2023-01-12-repro_R.html#reproducibility-is-on-a-continuum",
    "href": "posts/2023-01-12-repro_R.html#reproducibility-is-on-a-continuum",
    "title": "MRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?",
    "section": "\nReproducibility is on a continuum\n",
    "text": "Reproducibility is on a continuum\n\n\nReproducibility is on a continuum, and depending on the nature of your project different tools are needed. First, let’s understand what I mean by “reproducibility is on a continuum”. Let’s suppose that you have written a script that produces some output. Here is the list of everything that can influence the output (other than the mathematical algorithm running under the hood):\n\n\n\nVersion of the programming language used;\n\n\nVersions of the packages/libraries of said programming language used;\n\n\nOperating System, and its version;\n\n\nVersions of the underlying system libraries (which often go hand in hand with OS version, but not necessarily).\n\n\nHardware that you run all that software stack on.\n\n\n\nSo by “reproducibility is on a continuum”, what I mean is that you could set up your project in a way that none, one, two, three, four or all of the preceding items are taken into consideration when making your project reproducible.\n\n\nThere is, however, something else to consider. Before, I wrote “let’s suppose you have written a script”, which means that you actually have a script laying around that was written in a particular programming language, and which makes use of a known set of packages/libraries. So for example, if my script uses the following R packages:\n\n\n\ndplyr\n\n\ntidyr\n\n\nggplot2\n\n\n\nI, obviously, know this list and if I want to make my script reproducible, I should take note of the versions of these 3 packages (and potentially of their own dependencies). However, what if you don’t know this set of packages that are used? This happens when you want to set up an environment that is frozen, and then distribute this environment. Developers will then all work on the same base environment, but you cannot possibly list all the packages that are going to be used because you have no idea what the developers will end up using (and remember that future you is included in these developers, and you should always try to be nice to future you).\n\n\nSo these means that we have two scenarios:\n\n\n\nScenario 1: I have a script (or several), and want to make sure that it will always produce the same output;\n\n\nScenario 2: I don’t know what I (or my colleagues) will develop, but we want to use the same environment across the organization to develop and deploy data products.\n\n\n\nTurns out that the solutions to these two scenarios are different, but available in R, even though we won’t soon be able to count on MRAN anymore. HOWEVER! R won’t suffice for this job."
  },
  {
    "objectID": "posts/2023-01-12-repro_R.html#scenario-1-making-a-script-reproducible",
    "href": "posts/2023-01-12-repro_R.html#scenario-1-making-a-script-reproducible",
    "title": "MRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?",
    "section": "\nScenario 1: making a script reproducible\n",
    "text": "Scenario 1: making a script reproducible\n\n\nOk, so let’s suppose that I want to make a script reproducible, or at least increase the probability that others (including future me) will be able to run this script and get the exact same output as I originally did. If you want to minimize the amount of time spent doing it, the minimum thing you should do is use {renv}. {renv} allows you to create a file called renv.lock. You can then distribute this file to others alongside the code of your project, or the paper, data, whatever. Nothing else is required from you; if people have this file, they should be able to set up an environment that would closely mimic the one that was used to get the results originally (but ideally, you’d invest a bit more time to make your script run anywhere, for example by avoiding setting absolute paths that only exist on your machine).\n\n\nLet’s take a look at such an renv.lock file:\n\n\n\nClick me to see lock file\n\n{\n  \"R\": {\n    \"Version\": \"4.2.1\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"http://cran.rstudio.com\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"MASS\": {\n      \"Package\": \"MASS\",\n      \"Version\": \"7.3-58.1\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"762e1804143a332333c054759f89a706\",\n      \"Requirements\": []\n    },\n    \"Matrix\": {\n      \"Package\": \"Matrix\",\n      \"Version\": \"1.5-1\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"539dc0c0c05636812f1080f473d2c177\",\n      \"Requirements\": [\n        \"lattice\"\n      ]\n    },\n    \"R6\": {\n      \"Package\": \"R6\",\n      \"Version\": \"2.5.1\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"470851b6d5d0ac559e9d01bb352b4021\",\n      \"Requirements\": []\n    },\n    \"RColorBrewer\": {\n      \"Package\": \"RColorBrewer\",\n      \"Version\": \"1.1-3\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"45f0398006e83a5b10b72a90663d8d8c\",\n      \"Requirements\": []\n    },\n    \"cli\": {\n      \"Package\": \"cli\",\n      \"Version\": \"3.4.1\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"0d297d01734d2bcea40197bd4971a764\",\n      \"Requirements\": []\n    },\n    \"colorspace\": {\n      \"Package\": \"colorspace\",\n      \"Version\": \"2.0-3\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"bb4341986bc8b914f0f0acf2e4a3f2f7\",\n      \"Requirements\": []\n    },\n    \"digest\": {\n      \"Package\": \"digest\",\n      \"Version\": \"0.6.29\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"cf6b206a045a684728c3267ef7596190\",\n      \"Requirements\": []\n    },\n    \"fansi\": {\n      \"Package\": \"fansi\",\n      \"Version\": \"1.0.3\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"83a8afdbe71839506baa9f90eebad7ec\",\n      \"Requirements\": []\n    },\n    \"farver\": {\n      \"Package\": \"farver\",\n      \"Version\": \"2.1.1\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"8106d78941f34855c440ddb946b8f7a5\",\n      \"Requirements\": []\n    },\n    \"ggplot2\": {\n      \"Package\": \"ggplot2\",\n      \"Version\": \"3.3.6\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"0fb26d0674c82705c6b701d1a61e02ea\",\n      \"Requirements\": [\n        \"MASS\",\n        \"digest\",\n        \"glue\",\n        \"gtable\",\n        \"isoband\",\n        \"mgcv\",\n        \"rlang\",\n        \"scales\",\n        \"tibble\",\n        \"withr\"\n      ]\n    },\n    \"glue\": {\n      \"Package\": \"glue\",\n      \"Version\": \"1.6.2\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"4f2596dfb05dac67b9dc558e5c6fba2e\",\n      \"Requirements\": []\n    },\n    \"gtable\": {\n      \"Package\": \"gtable\",\n      \"Version\": \"0.3.1\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"36b4265fb818f6a342bed217549cd896\",\n      \"Requirements\": []\n    },\n    \"isoband\": {\n      \"Package\": \"isoband\",\n      \"Version\": \"0.2.5\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"7ab57a6de7f48a8dc84910d1eca42883\",\n      \"Requirements\": []\n    },\n    \"labeling\": {\n      \"Package\": \"labeling\",\n      \"Version\": \"0.4.2\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"3d5108641f47470611a32d0bdf357a72\",\n      \"Requirements\": []\n    },\n    \"lattice\": {\n      \"Package\": \"lattice\",\n      \"Version\": \"0.20-45\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"b64cdbb2b340437c4ee047a1f4c4377b\",\n      \"Requirements\": []\n    },\n    \"lifecycle\": {\n      \"Package\": \"lifecycle\",\n      \"Version\": \"1.0.3\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"001cecbeac1cff9301bdc3775ee46a86\",\n      \"Requirements\": [\n        \"cli\",\n        \"glue\",\n        \"rlang\"\n      ]\n    },\n    \"magrittr\": {\n      \"Package\": \"magrittr\",\n      \"Version\": \"2.0.3\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"7ce2733a9826b3aeb1775d56fd305472\",\n      \"Requirements\": []\n    },\n    \"mgcv\": {\n      \"Package\": \"mgcv\",\n      \"Version\": \"1.8-40\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"c6b2fdb18cf68ab613bd564363e1ba0d\",\n      \"Requirements\": [\n        \"Matrix\",\n        \"nlme\"\n      ]\n    },\n    \"munsell\": {\n      \"Package\": \"munsell\",\n      \"Version\": \"0.5.0\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"6dfe8bf774944bd5595785e3229d8771\",\n      \"Requirements\": [\n        \"colorspace\"\n      ]\n    },\n    \"nlme\": {\n      \"Package\": \"nlme\",\n      \"Version\": \"3.1-159\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"4a0b3a68f846cb999ff0e8e519524fbb\",\n      \"Requirements\": [\n        \"lattice\"\n      ]\n    },\n    \"pillar\": {\n      \"Package\": \"pillar\",\n      \"Version\": \"1.8.1\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"f2316df30902c81729ae9de95ad5a608\",\n      \"Requirements\": [\n        \"cli\",\n        \"fansi\",\n        \"glue\",\n        \"lifecycle\",\n        \"rlang\",\n        \"utf8\",\n        \"vctrs\"\n      ]\n    },\n    \"pkgconfig\": {\n      \"Package\": \"pkgconfig\",\n      \"Version\": \"2.0.3\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"01f28d4278f15c76cddbea05899c5d6f\",\n      \"Requirements\": []\n    },\n    \"purrr\": {\n      \"Package\": \"purrr\",\n      \"Version\": \"0.3.5\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"54842a2443c76267152eface28d9e90a\",\n      \"Requirements\": [\n        \"magrittr\",\n        \"rlang\"\n      ]\n    },\n    \"renv\": {\n      \"Package\": \"renv\",\n      \"Version\": \"0.16.0\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"c9e8442ab69bc21c9697ecf856c1e6c7\",\n      \"Requirements\": []\n    },\n    \"rlang\": {\n      \"Package\": \"rlang\",\n      \"Version\": \"1.0.6\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"4ed1f8336c8d52c3e750adcdc57228a7\",\n      \"Requirements\": []\n    },\n    \"scales\": {\n      \"Package\": \"scales\",\n      \"Version\": \"1.2.1\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"906cb23d2f1c5680b8ce439b44c6fa63\",\n      \"Requirements\": [\n        \"R6\",\n        \"RColorBrewer\",\n        \"farver\",\n        \"labeling\",\n        \"lifecycle\",\n        \"munsell\",\n        \"rlang\",\n        \"viridisLite\"\n      ]\n    },\n    \"tibble\": {\n      \"Package\": \"tibble\",\n      \"Version\": \"3.1.8\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"56b6934ef0f8c68225949a8672fe1a8f\",\n      \"Requirements\": [\n        \"fansi\",\n        \"lifecycle\",\n        \"magrittr\",\n        \"pillar\",\n        \"pkgconfig\",\n        \"rlang\",\n        \"vctrs\"\n      ]\n    },\n    \"utf8\": {\n      \"Package\": \"utf8\",\n      \"Version\": \"1.2.2\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"c9c462b759a5cc844ae25b5942654d13\",\n      \"Requirements\": []\n    },\n    \"vctrs\": {\n      \"Package\": \"vctrs\",\n      \"Version\": \"0.5.1\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"970324f6572b4fd81db507b5d4062cb0\",\n      \"Requirements\": [\n        \"cli\",\n        \"glue\",\n        \"lifecycle\",\n        \"rlang\"\n      ]\n    },\n    \"viridisLite\": {\n      \"Package\": \"viridisLite\",\n      \"Version\": \"0.4.1\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"62f4b5da3e08d8e5bcba6cac15603f70\",\n      \"Requirements\": []\n    },\n    \"withr\": {\n      \"Package\": \"withr\",\n      \"Version\": \"2.5.0\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"c0e49a9760983e81e55cdd9be92e7182\",\n      \"Requirements\": []\n    }\n  }\n}\n\n\n\nas you can see this file lists the R version that was used as well as the different libraries for a project. Versions of the libraries are also listed, where they came from (CRAN or Github for example) and these libraries’ requirements as well. So someone who wants to run the original script in a similar environment has all the info needed to do it. {renv} also provides a simple way to install all of this. Simply put the renv.lock file in the same folder as the original script and run renv::restore(), and the right packages with the right versions get automagically installed (and without interfering with your system-wide, already installed library of packages). The “only” difficulty that you might have is installing the right version of R. If this is a recent enough version of R, this shouldn’t be a problem, but installing old software might be difficult. For example installing R version 2.5 might, or might not, be possible depending on your operating system (I don’t like microsoft windows, but generally speaking it is quite easy to install very old software on it, which in the case of Linux distros can be quite difficult. So I guess on windows this could work more easily). Then there’s also the system libraries that your script might need, and it might also be difficult to install these older versions. So that’s why I said that providing the renv.lock file is the bare minimum. But before seeing how we can deal with that, let’s discuss a scenario 1bis, which is the case where you want to run an old script (say, from 5 years ago), but there’s no renv.lock file to be found."
  },
  {
    "objectID": "posts/2023-01-12-repro_R.html#scenario-1bis-old-script-no-renv.lock-file",
    "href": "posts/2023-01-12-repro_R.html#scenario-1bis-old-script-no-renv.lock-file",
    "title": "MRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?",
    "section": "\nScenario 1bis: old script, no renv.lock file\n",
    "text": "Scenario 1bis: old script, no renv.lock file\n\n\nFor these cases I would have used {checkpoint}, but as explained in the intro MRAN is getting shutdown, and with it out of the picture {checkpoint} will cease to work.\n\n\nThe way {checkpoint} worked is that you would simply add the following line at the very top of the script in question:\n\ncheckpoint::checkpoint(\"2018-12-12\")\n\nand this would download the packages needed for the script from that specific date and run your script. Really, really useful. But unfortunately, Microsoft decided that MRAN should get the axe. So what else is there? While researching for this blog post, I learned about {groundhog} which supposedly provides the same functionality. Suppose I have a script from 5 years ago that loads the following libraries:\n\nlibrary(purrr)\nlibrary(ggplot2)\n\nBy rewriting this like so (and installing {groundhog} beforehand):\n\ngroundhog.library(\"\n    library(purrr)\n    library(ggplot2)\",\n    \"2017-10-04\",\n    tolerate.R.version = \"4.2.2\")\n\n{purrr} and {ggplot2} get downloaded as they were on the date I provided. If you want to know what I had to use the “tolerate.R.version” option, this is because if you try to run it without it, you get the following very useful message:\n\n---------------------------------------------------------------------------\n|IMPORTANT.\n|    Groundhog says: you are using R-4.2.2, but the version of R current \n|    for the entered date, '2017-10-04', is R-3.4.x. It is recommended \n|    that you either keep this date and switch to that version of R, or \n|    you keep the version of R you are using but switch the date to \n|    between '2022-04-22' and '2023-01-08'. \n|\n|    You may bypass this R-version check by adding: \n|    `tolerate.R.version='4.2.2'`as an option in your groundhog.library() \n|    call. Please type 'OK' to confirm you have read this message. \n|   &gt;ok\n\nThat’s is pretty neat, as it tells you “hey, getting the right packages is good, but if your R version is not the same, you’re not guaranteed to get the same results back, and this might not even work at all”.\n\n\nSo, here’s what happens when I try to install these packages (on my windows laptop, as most people would do), without installing the right version of R as suggested by {groundhog}:\n\n\n\nClick me to see lock file\n\n+ Will now attempt installing 5 packages from source.\n\ngroundhog says: Installing 'magrittr_1.5', package #1 (from source) out of 5 needed\n&gt; As of 16:12, the best guess is that all 5 packages will install around 16:14\ntrying URL 'https://packagemanager.rstudio.com/all/latest/src/contrib/Archive/magrittr/magrittr_1.5.tar.gz'\nContent type 'application/x-tar' length 200957 bytes (196 KB)\ndownloaded 196 KB\n\n\ngroundhog says: Installing 'rlang_0.1.2', package #2 (from source) out of 5 needed\n&gt; As of 16:12, the best guess is that all 5 packages will install around 16:14\ntrying URL 'https://packagemanager.rstudio.com/all/latest/src/contrib/Archive/rlang/rlang_0.1.2.tar.gz'\nContent type 'application/x-tar' length 200867 bytes (196 KB)\ndownloaded 196 KB\n\n\ngroundhog says: Installing 'Rcpp_0.12.13', package #3 (from source) out of 5 needed\n&gt; As of 16:13, the best guess is that all 5 packages will install around 16:14\ntrying URL 'https://packagemanager.rstudio.com/all/latest/src/contrib/Archive/Rcpp/Rcpp_0.12.13.tar.gz'\nContent type 'application/x-tar' length 3752364 bytes (3.6 MB)\ndownloaded 3.6 MB\n\nWill try again, now showing all installation output.\ntrying URL 'https://packagemanager.rstudio.com/all/latest/src/contrib/Archive/Rcpp/Rcpp_0.12.13.tar.gz'\nContent type 'application/x-tar' length 3752364 bytes (3.6 MB)\ndownloaded 3.6 MB\n\n* installing *source* package 'Rcpp' ...\n** package 'Rcpp' successfully unpacked and MD5 sums checked\nstaged installation is only possible with locking\n** using non-staged installation\n** libs\ng++ -std=gnu++11  -I\"c:/Users/Bruno/AppData/Roaming/R-42~1.2/include\" -DNDEBUG -I../inst/include/    -I\"c:/rtools42/x86_64-w64-mingw32.static.posix/include\"     -O2 -Wall  -mfpmath=sse -msse2 -mstackrealign  -c Date.cpp -o Date.o\nIn file included from ../inst/include/RcppCommon.h:67,\n                 from ../inst/include/Rcpp.h:27,\n                 from Date.cpp:31:\n../inst/include/Rcpp/sprintf.h: In function 'std::string Rcpp::sprintf(const char*, ...)':\n../inst/include/Rcpp/sprintf.h:30:12: warning: unnecessary parentheses in declaration of 'ap' [-Wparentheses]\n   30 |     va_list(ap);\n      |            ^~~~\n../inst/include/Rcpp/sprintf.h:30:12: note: remove parentheses\n   30 |     va_list(ap);\n      |            ^~~~\n      |            -  -\nIn file included from ../inst/include/Rcpp.h:77,\n                 from Date.cpp:31:\n../inst/include/Rcpp/Rmath.h: In function 'double R::pythag(double, double)':\n../inst/include/Rcpp/Rmath.h:222:60: error: '::Rf_pythag' has not been declared; did you mean 'pythag'?\n  222 |     inline double pythag(double a, double b)    { return ::Rf_pythag(a, b); }\n      |                                                            ^~~~~~~~~\n      |                                                            pythag\nmake: *** [c:/Users/Bruno/AppData/Roaming/R-42~1.2/etc/x64/Makeconf:260: Date.o] Error 1\nERROR: compilation failed for package 'Rcpp'\n* removing 'C:/Users/Bruno/Documents/R_groundhog/groundhog_library/R-4.2/Rcpp_0.12.13/Rcpp'\nThe package 'Rcpp_0.12.13' failed to install!\ngroundhog says:\nThe package may have failed to install because you are using R-4.2.2\nwhich is at least one major update after the date you entered '2017-10-04'.\nYou can try using a more recent date in your groundhog.library() command, \nor run it with the same date using 'R-3.4.4'\nInstructions for running older versions of R: \n    http://groundhogr.com/many\n\n----------------   The package purrr_0.2.3 did NOT install.  Read above for details  -----------------\n\nWarning message:\nIn utils::install.packages(url, repos = NULL, lib = snowball$installation.path[k],  :\n  installation of package 'C:/Users/Bruno/AppData/Local/Temp/RtmpyKYXFd/downloaded_packages/Rcpp_0.12.13.tar.gz' had non-zero exit status\n&gt; \n\n\nAs you can see it failed, very likely because I don’t have the right development libraries installed on my windows laptop, due to the version mismatch that {groundhog} complained about. I also tried on my Linux workstation, and got the same outcome. In any case, I want to stress that this is not {groundhog}’s fault, but this due to the fact that I was here only concerned with packages; as I said multiple times now, reproducibility is on an continuum, and you also need to deal with OS and system libraries. So for now, we only got part of the solution.\n\n\nBy the way, you should know 3 more things about {groundhog}:\n\n\n\nthe earliest available date is, in theory, any date. However, according to its author, {groundhog} should work reliably with a date as early as “2015-04-16”. That’s because the oldest R version {groundhog} is compatible with is R 3.2. However, again according to its author, it should be possible to patch {groundhog} to work with any earlier versions of R.\n\n\n{groundhog}’s developer is planning to save the binary packages off MRAN so that {groundhog} will continue offering binary packages once MRAN is out of the picture, which will make installing these packages more reliable.\n\n\nOn Windows and macOS, {groundhog} installs binary packages if they’re available (which basically is always the case, in the example above it was not the case because I was using Posit’s CRAN mirror, and I don’t think they have binary packages for Windows that are that old. But if using another mirror, that should not be a problem). So if you install the right version of R, you’re almost guaranteed that it’s going to work. But, there is always a but, this also depends on hardware now. I’ll explain in the last part of this blog post, so read on."
  },
  {
    "objectID": "posts/2023-01-12-repro_R.html#docker-to-the-rescue",
    "href": "posts/2023-01-12-repro_R.html#docker-to-the-rescue",
    "title": "MRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?",
    "section": "\nDocker to the rescue\n",
    "text": "Docker to the rescue\n\n\nThe full solution in both scenarios involves Docker. If you are totally unfamiliar with Docker, you can imagine that Docker makes it easy to set up a Linux virtual machine and run it. In Docker, you use Dockerfiles (which are configuration files) to define Docker images and you can then run containers (your VMs, if you wish) based on that image. Inside that Dockerfile you can declare which operating system you want to use and what you want to run inside of it. For example, here’s a very simple Dockerfile that prints “Hello from Docker” on the Ubuntu operating system (a popular Linux distribution):\n\nFROM ubuntu:latest\n\nRUN echo \"Hello, World!\"\n\nYou then need to build the image as defined from this Dockerfile. (Don’t try to follow along for now with your own computer; I’ll link to resources below so that you can get started if you’re interested. What matters is that you understand why Docker is needed).\n\n\nBuilding the image can be achieved by running this command where the Dockerfile is located:\n\ndocker build -t hello .\n\nand then run a container from this image:\n\ndocker run --rm -d --name hello_container hello\nSending build context to Docker daemon  2.048kB\nStep 1/2 : FROM ubuntu:latest\n ---&gt; 6b7dfa7e8fdb\nStep 2/2 : RUN echo \"Hello, World!\"\n ---&gt; Running in 5dfbff5463cf\nHello, World!\nRemoving intermediate container 5dfbff5463cf\n ---&gt; c14004cd1801\nSuccessfully built c14004cd1801\nSuccessfully tagged hello:latest\n\nYou should see “Hello, World!” inside your terminal. Ok so this is the very basics. Now why is that useful? It turns out that there’s the so-called Rocker project, and this project provides a collection of Dockerfiles for current, but also older versions of R. So if we go back to our renv.lock file from before, we can see which R version was used (it was R 4.2.1) and define a new Dockerfile that builds upon the Dockerfile from the Rocker project for R version 4.2.1.\n\n\nLet’s first start by writing a very simple script. Suppose that this is the script that we want to make reproducible:\n\nlibrary(purrr)\nlibrary(ggplot2)\n\ndata(mtcars)\n\nmyplot &lt;- ggplot(mtcars) +\n  geom_line(aes(y = hp, x = mpg))\n\nggsave(\"/home/project/output/myplot.pdf\", myplot)\n\nFirst, let’s assume that I did my homework and that the renv.lock file from before is actually the one that was generated at the time this script was written. In that case, you could write a Dockerfile with the correct version of R and use the renv.lock file to install the right packages. This Dockerfile would look like this:\n\nFROM rocker/r-ver:4.2.1\n\nRUN mkdir /home/project\n\nRUN mkdir /home/project/output\n\nCOPY renv.lock /home/project/renv.lock\n\nCOPY script.R /home/project/script.R\n\nRUN R -e \"install.packages('renv')\"\n\nRUN R -e \"setwd('/home/project/');renv::restore(confirm = FALSE)\"\n\nCMD R -e \"source('/home/project/script.R')\"\n\nWe need to put the renv.lock file, as well as the script script.R in the same folder as the Dockerfile, and then build and run the image:\n\n# Build it with\ndocker build -t project .\n\nrun the container (and mount a volume to get the image back – don’t worry if you don’t know what volumes are, I’ll link resources at the end):\n\ndocker run --rm -d --name project_container -v /path/to/your/project/output:/home/project/output:rw project\n\nEven if you’ve never seen a Dockerfile in your life, you likely understand what is going on here: the first line pulls a Docker image that contains R version 4.2.1 pre-installed on Ubuntu. Then, we create a directory to hold our files, we copy said files in the directory, and then run several R commands to install the packages as defined in the renv.lock file and run our script in an environment that not only has the right versions of the packages but also the right version of R. This script then saves the plot in a folder called output/, which we link to a folder also called output/ but on our machine, so that we can then look at the generated plot (this is what -v /path/to/your/project/output:/home/project/output:rw does). Just as this script saves a plot, it could be doing any arbitrarily complex thing, like compiling an Rmarkdown file, running a model, etc, etc.\n\n\nHere’s a short video of this process in action:\n\n\n\n\n\nNow, let’s do the same thing but for our scenario 1bis that relied on {groundhog}. Before writing the Dockerfile down, here’s how you should change the script. Add these lines at the very top:\n\ngroundhog::set.groundhog.folder(\"/home/groundhog_folder\")\n\ngroundhog::groundhog.library(\"\n    library(purrr)\n    library(ggplot2)\",\n    \"2017-10-04\"\n    )\n\ndata(mtcars)\n\nmyplot &lt;- ggplot(mtcars) +\n  geom_line(aes(y = hp, x = mpg))\n\nggsave(\"/home/project/output/myplot.pdf\", myplot)\n\nI also created a new script that installs the dependencies of my script when building my Dockerfile. This way, when I run the container, nothing gets installed anymore. Here’s what this script looks like:\n\ngroundhog::set.groundhog.folder(\"/home/groundhog_folder\")\n\ngroundhog::groundhog.library(\"\n    library(purrr)\n    library(ggplot2)\",\n    \"2017-10-04\"\n    )\n\nIt’s exactly the beginning from the main script. Now here comes the Dockerfile, and this time it’s going to be a bit more complicated:\n\nFROM rocker/r-ver:3.4.4\n\nRUN echo \"options(repos = c(CRAN='https://packagemanager.rstudio.com/cran/latest'), download.file.method = 'libcurl')\" &gt;&gt; /usr/local/lib/R/etc/Rprofile.site\n\nRUN mkdir /home/project\n\nRUN mkdir /home/groundhog_folder\n\nRUN mkdir /home/project/output\n\nCOPY script.R /home/project/script.R\n\nCOPY install_deps.R /home/project/install_deps.R\n\nRUN R -e \"install.packages('groundhog');source('/home/project/install_deps.R')\"\n\nCMD R -e \"source('/home/project/script.R')\"\n\nAs you can see from the first line, this time we’re pulling an image that comes with R 3.4.4. This is because that version of R was the current version as of 2017-10-04, the date we assumed this script was written on. Because this is now quite old, we need to add some more stuff to the Dockerfile to make it work. First, I change the repositories to the current mirror from Posit. This is because the repositories from this image are set to MRAN at a fixed date. This was done at the time for reproducibility, but now MRAN is getting shutdown, so we need to change the repositories or else our container will not be able to download packages. Also, {groundhog} will take care of installing the right package versions. Then I create the necessary folders and run the install_deps.R script which is the one that will install the packages. This way, the packages get installed when building the Docker image, and not when running the container, which is preferable. Finally, the main script gets run, and an output gets produced. Here’s a video showing this process:\n\n\n\n\n\nNow all of this may seem complicated, and to be honest it is. Reproducibility is no easy task, but I hope that I’ve convinced you that by combining {renv} and Docker, or {groundhog} and Docker it is possible to rerun any analysis. But you do have to be familiar with these tools, and there’s also another issue by using Docker. Docker works on Windows, macOS and Linux, but the container that runs must be a Linux distribution, usually Ubuntu. But what if the original analysis was done on Windows and macOS? This can be a problem if the script relies on some Windows or macOS specific things, which even for a language available on all platforms like R can happen. For example, I’ve recently noticed that the tar() function in R, which is used to decompress tar.gz files, behaves differently on Windows than on Linux. So ideally, even if you’re running your analysis on Windows, you should then try to distribute a working Dockerfile alongside your paper (if you’re a researcher, or if you’re working in the private sector, you should do the same for each project). Of course, that is quite demanding, and you would need to learn about these tools, or hire someone to do that for you… But a natural question is then, well, “why use Docker at all? Since it’s easy to install older versions of R on Windows and macOS, wouldn’t an renv.lock file suffice? Or even just {groundhog} which is arguably even easier to use?”\n\n\nWell, more on this later. I still need to discuss scenario 2 first."
  },
  {
    "objectID": "posts/2023-01-12-repro_R.html#scenario-2-offering-an-environment-that-is-made-for-reproducibility",
    "href": "posts/2023-01-12-repro_R.html#scenario-2-offering-an-environment-that-is-made-for-reproducibility",
    "title": "MRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?",
    "section": "\nScenario 2: offering an environment that is made for reproducibility\n",
    "text": "Scenario 2: offering an environment that is made for reproducibility\n\n\nOk, so this one is easier. In this scenario, you have no idea what people are going to use, so you cannot generate an renv.lock file beforehand, and {groundhog} is of no help either, because, well, there’s no scripts yet to actually make reproducible. This is the situation I’ve had for this project that I’ve discussed at the end of last year, on code longevity of the R programming language. The solution is to write a Dockerfile that people can modify and run; this in turn produces some results that can then be shared. This Dockerfile pulls from another Dockerfile, and that other Dockerfile is made for reproducibility. How? Because that other Dockerfile is based on Ubuntu 22.04, compiles R 4.2.2 from source, and sets the repositories to https://packagemanager.rstudio.com/cran/linux/jammy/2022-11-21 . This way, the packages get downloaded exactly as they were on November 21st 2022. So every time this image defined from this Dockerfile gets built, we get exactly the same environment.\n\n\nIt should also be noted that this solution can be used in the case of scenario 1bis. Let’s say I have a script from August 2018; by using a Docker image that ships the current version of R at that time (which should be R version 3.5.x) and Ubuntu (which at the time was 18.04, codenamed Bionic Beaver) and then using the Posit package manager at a frozen date, for example https://packagemanager.rstudio.com/cran/linux/bionic/2018-08-16 I should be able to reproduce an environment that is close enough. However the problem is that Posit’s package manager earliest available date is Octobre 2017, so anything before that would not be possible to reproduce.\n\n\nOk, great, here are the solutions for reproducibility. But there are still problems."
  },
  {
    "objectID": "posts/2023-01-12-repro_R.html#a-single-point-of-failure-docker",
    "href": "posts/2023-01-12-repro_R.html#a-single-point-of-failure-docker",
    "title": "MRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?",
    "section": "\nA single point of failure: Docker\n",
    "text": "A single point of failure: Docker\n\n\nLet’s be blunt: having Docker as the common denominator in all these solutions is a problem. This is because Docker represents a single point of failure. But the problem is not Docker itself, but the infrastructure.\n\n\nLet me explain: Docker is based on many different open source parts, and that’s great. There’s also Podman, which is basically a drop-in replacement (when combined with other tools) made by Red Hat, which is completely open source as well. So the risk does not come from there, because even if for some reason Docker would disappear, or get abandoned or whatever, we could still work with Podman, and it would likely be possible to create a fork from Docker.\n\n\nBut the issue is the infrastructure. For now, using Docker and more importantly hosting images is free for personal use, education, open source communities and small businesses. So this means that a project like Rocker likely pays nothing for hosting all the images they produce (but who knows, I may be wrong on this). And Rocker makes a lot of images. See, at the top of the Dockerfiles I’ve used in this blog post, there’s always a statement like:\n\nFROM rocker/r-ver:4.2.1\n\nas explained before, this states that a pre-built image that ships R version 4.2.1 on Ubuntu gets downloaded. But from where? This image gets downloaded from Docker Hub, see here.\n\n\nThis means that you can download this pre-built image and don’t need to build it each time you need to work with it. You can simply use that as a base for your work, like the image built for reproducibility described in scenario 2. But what happens if at some point in the future Docker changes its licensing model? What if they still have a free tier, but massively limit the amount of images that get hosted for free? What if they get rid of the free tier entirely? This is a massive risk that needs to be managed in my opinion. There is the option of the Rocker project hosting the images themselves. It is possible to create your own, self-hosted Docker registry and not use Docker Hub, after all. But this is costly not only in terms of storage, but also of manpower to maintain all this. But maybe worse than that is: what if at some point in the future you cannot rebuild these images, at all? You would need to make sure that these pre-built images do not get lost. And this is already happening because of MRAN getting shutdown. In this blog post I’ve used the rocker/r-ver:3.4.4 image to run code from 2017. The problem is that if you look at its Dockerfile, you see that building this image requires MRAN. So in other words, once MRAN is offline, it won’t be possible to rebuild this image, and the Rocker project will need to make sure that the pre-built image that is currently available on Docker Hub stays available forever. Because if not, it would be quite hard to rerun code from, say, 2017. Same goes for Posit’s package manager. Posit’s package manager could be used as a drop-in replacement for MRAN, but for how long? Even though Posit is a very responsible company, I believe that it is dangerous that such a crucial service is managed by only one company.\n\n\nAnd rebuilding old images will be necessary. This is now the part where I answer the question from above:\n\n\n“why use Docker at all? Since it’s easy to install older versions of R on Windows and macOS, wouldn’t an renv.lock file suffice? Or even just {groundhog} which is arguably even easier to use?”\n\n\nThe problem is hardware. You see, Apple has changed hardware architecture recently, their new computers switched from Intel based hardware to their own proprietary architecture (Apple Silicon) based on the ARM specification. And what does that mean concretely? It means that all the binary packages that were built for Intel based Apple computers cannot work on their new computers. Which means that if you have a recent M1 Macbook and need to install old CRAN packages (for example, by using {groundhog}), these need to be compiled to work on M1. You cannot even install older versions of R, unless you also compile those from source! Now I have read about a compatibility layer called Rosetta which enables to run binaries compiled for the Intel architecture on the ARM architecture, and maybe this works well with R and CRAN binaries compiled for Intel architecture. Maybe, I don’t know. But my point is that you never know what might come in the future, and thus needing to be able to compile from source is important, because compiling from source is what requires the least amount of dependencies that are outside of your control. Relying on binaries is not future-proof.\n\n\nAnd for you Windows users, don’t think that the preceding paragraph does not concern you. I think that it is very likely that Microsoft will push in the future for OEM manufacturers to develop more ARM based computers. There is already an ARM version of Windows after all, and it has been around for quite some time, and I think that Microsoft will not kill that version any time in the future. This is because ARM is much more energy efficient than other architectures, and any manufacturer can build its own ARM cpus by purchasing a license, which can be quite interesting. For example in the case of Apple silicon cpus, Apple can now get exactly the cpus they want for their machines and make their software work seamlessly with it. I doubt that others will pass the chance to do the same.\n\n\nAlso, something else that might happen is that we might move towards more and more cloud based computing, but I think that this scenario is less likely than the one from before. But who knows. And in that case it is quite likely that the actual code will be running on Linux servers that will likely be ARM based because of energy costs. Here again, if you want to run your historical code, you’ll have to compile old packages and R versions from source.\n\n\nBasically, binary packages are in my opinion not a future-proof option, so that’s why something like Docker will stay, and become ever more relevant. But as I argued before, that’s a single point of failure.\n\n\nBut there might be a way we can solve this and not have to rely on Docker at all."
  },
  {
    "objectID": "posts/2023-01-12-repro_R.html#guix-toward-practical-transparent-verifiable-and-long-term-reproducible-research",
    "href": "posts/2023-01-12-repro_R.html#guix-toward-practical-transparent-verifiable-and-long-term-reproducible-research",
    "title": "MRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?",
    "section": "\nGuix: toward practical transparent verifiable and long-term reproducible research\n",
    "text": "Guix: toward practical transparent verifiable and long-term reproducible research\n\n\nThe title of this section is the same as the title from a research paper published in 2022 that you can read here.\n\n\nThis paper presents and shows how to use Guix, which is a tool to build, from scratch and in a reproducible manner, the computational environment that was used to run some code for research. Very importantly, Guix doesn’t rely on containers, virtual machines or anything like that. From my, albeit still limited, understanding of how it works, Guix requires some recipes telling it how it should build software. Guix integrates with CRAN, so it’s possible to tell Guix “hey, could you build ggplot2” and Guix does as instructed. It builds {ggplot2} and all the required dependencies. And what’s great about Guix is that it’s also possible to build older versions of software.\n\n\nThe authors illustrate this by reproducing results from a 2019 paper, by recreating the environment used at the time, 3 years later.\n\n\nThis could be a great solution, because it would always allow the recreation of computational environments from source. So architecture changes would not be a problem, making Guix quite future proof. The issue I’ve found though, is that Guix only works on Linux. So if you’re working on Windows or macOS, you would need Docker to recreate a computational environment with Guix. So you could think that we’re back to square one, but actually no. Because you could always have a Linux machine or server that you would use for reproducibility, on which Linux is installed, thus eliminating the need for Docker, thus removing that risk entirely.\n\n\nI’m currently exploring Guix and will report on it in greater detail in the future. In the meantime, I hope to have convinced you that, while reproducibility is no easy task, the tools that are currently available can help you set up reproducible project. However, for something to be and stay truly reproducible, some long term maintenance is also required."
  },
  {
    "objectID": "posts/2023-01-12-repro_R.html#conclusion",
    "href": "posts/2023-01-12-repro_R.html#conclusion",
    "title": "MRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?",
    "section": "\nConclusion\n",
    "text": "Conclusion\n\n\nSo here we are, these are, as far as I know, the options to make your code reproducible. But it is no easy task and it takes time. Unfortunately, many scientists are not really concerned with making their code reproducible, simply because there is no real incentive for them to do it. And it’s a task that is becoming more and more complex as there are other risks that need to be managed, like the transition from Intel based architectures for cpus towards ARM. I’m pretty sure there are many scientists who have absolutely no idea what Intel based cpus or ARM cpus or Rosetta or whatever are. So telling them they need to make their code reproducible is one thing, telling them they need to make it so future proof that architecture changes won’t matter is like asking for the Moon.\n\n\nSo research software engineers will become more and more crucial, and should be integrated to research teams to deal with this question (and this also holds true for the private sector; there should be someone whose job is make code reproducible across the organization).\n\n\nAnyways, if you read until here, I appreciate it. This was a long blog post. If you want to know more, you can read this other blog post of mine that explains how to use Docker, and also this other blog post that explains why Docker is not optional (but now that I’ve discovered Guix, maybe it’s Guix that is not optional).\n\n\nBy the way, if you want to grab the scripts and Dockerfiles from this blog post, you can get them here."
  },
  {
    "objectID": "posts/2022-06-02-arcane.html",
    "href": "posts/2022-06-02-arcane.html",
    "title": "R will always be arcane to those who do not make a serious effort to learn it…",
    "section": "",
    "text": "R will always be arcane to those who do not make a serious effort to learn it. It is not meant to be intuitive and easy for casual users to just plunge into. It is far too complex and powerful for that. But the rewards are great for serious data analysts who put in the effort.\n\n\n— Berton Gunter R-help August 2007\n\n\n\nI’ve posted this quote on twitter the other day and it sparked some discussion. Personally I agree with this quote, and I’ll explain why.\n\n\nJust like any tool aimed at professionals, R requires people to spend time to actually master it. There is no ifs or buts. Just like I don’t want a casual carpenter doing my carpentry, or a casual electrician doing the wiring in my house, I don’t think anyone should want to be a casual R user. Now of course, depending on your needs, you might not need to learn everything the language has to offer. I certainly don’t know everything R has to offer, far from it. But whatever task you need to fulfill, take the time to learn the required syntax and packages. As Berton Gunter said in 2007, the rewards are great if you put in the effort. You need to create top notch plots? Master {ggplot2}. Need to create top notch web apps? {shiny}, and so on and so forth… you get the idea. But as a shiny expert, you might not need to know, nor care, about R’s object oriented capabilities for example.\n\n\nThat’s fine.\n\n\n\nEvelyn Hall: I would like to know how (if) I can extract some of the information from the summary of my nlme.\n\n\nSimon Blomberg: This is R. There is no if. Only how.\n\n\n— Evely Hall and Simon ’Yoda’ Blomberg, R-help April 2005\n\n\n\nI remember being extremely frustrated when I started to learn R, not because the language was overly complex, (even if that was the case in the beginning, but honestly, that’s true for any language, even for supposedly piss-easy languages like Python) but because my professors kept saying “no need to learn the language in great detail, we’re economists after all, not programmers”. That didn’t seem right, and now that I’ve been working with R for years (and with economists for some time as well), it certainly is important, even for economists, to be quite fluent in at least one programming language like R. How fluent should you be? Well, enough that you can test new ideas, or explore new data without much googling nor friction. Your creativity and curiosity cannot be limited by your lack of knowledge of the tools you need to use.\n\n\nSome people posit that the {tidyverse} (and Rstudio, the GUI interface) made R more accessible. I’d say yes and no. On one hand, the tidyverse has following nice things going for it:\n\n\n\nConsistent api across packages. That definitely makes R easier to learn!\n\n\nMade the %&gt;% operator famous, which improves readability.\n\n\nTop notch documentation, and also many packages come with books that you can read online for free! That certainly makes R easier to learn.\n\n\n\n(and Rstudio was the first, really good, GUI for R).\n\n\nBut while this is all true, on the other hand, the {tidyverse} also makes it possible to write code like this (I’ll be using the package::function() to make the origin of the functions clear):\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(ggfortify) # Not part of the tidyverse, but needed to make ggplot2::autoplot work on lm\nlibrary(ggplot2)\nlibrary(broom) # Not part of the tidyverse, but adheres to the *tidy* principles\n\nresult &lt;- mtcars %&gt;%\n  dplyr::group_nest(am) %&gt;%\n  dplyr::mutate(models = purrr::map(data, ~lm(hp ~ mpg + cyl, data = .))) %&gt;%\n  dplyr::mutate(diag_plots = purrr::map(models, ggplot2::autoplot)) %&gt;%\n  dplyr::mutate(model_summary = purrr::map(models, broom::tidy))\n\nresult is now a data frame with several columns:\n\nresult\n## # A tibble: 2 × 5\n##      am                data models diag_plots model_summary   \n##   &lt;dbl&gt; &lt;list&lt;tibble[,10]&gt;&gt; &lt;list&gt; &lt;list&gt;     &lt;list&gt;          \n## 1     0           [19 × 10] &lt;lm&gt;   &lt;ggmltplt&gt; &lt;tibble [3 × 5]&gt;\n## 2     1           [13 × 10] &lt;lm&gt;   &lt;ggmltplt&gt; &lt;tibble [3 × 5]&gt;\n\nam defines the groups, and then data, models and model_summary are list-columns containing complex objects (data frames, models, and plots, respectively). And don’t get me wrong here, this is not code that I made look complicated on purpose. This type of workflow is canon in the tidyverse lore. This is how you can avoid for loops and keep every result together neatly in a single object.\n\n\nLet’s look at another esoteric example: imagine I want to publish a paper and am only interested in the coefficients of the model where the p-value is less than .05 (lol):\n\nmtcars %&gt;%\n  dplyr::group_nest(am) %&gt;%\n  dplyr::mutate(models = purrr::map(data, ~lm(hp ~ mpg + cyl, data = .))) %&gt;%\n  dplyr::mutate(model_summary = purrr::map(models, broom::tidy)) %&gt;%\n  dplyr::mutate(model_summary = purrr::map(model_summary, \\(x)(filter(x, p.value &lt; .05))))\n## # A tibble: 2 × 4\n##      am                data models model_summary   \n##   &lt;dbl&gt; &lt;list&lt;tibble[,10]&gt;&gt; &lt;list&gt; &lt;list&gt;          \n## 1     0           [19 × 10] &lt;lm&gt;   &lt;tibble [2 × 5]&gt;\n## 2     1           [13 × 10] &lt;lm&gt;   &lt;tibble [1 × 5]&gt;\n\nI’ve mapped an anomymous function to the model summary, to filter out p-values greater than .05. Do you think this looks comprehensible to the beginner? I don’t think so. But I also don’t think that the beginners must stay beginners, and this is what matters.\n\n\n\nActually, I see it as part of my job to inflict R on people who are perfectly happy to have never heard of it. Happiness doesn’t equal proficient and efficient. In some cases the proficiency of a person serves a greater good than their momentary happiness.\n\n\n— Patrick Burns, R-help April 2005\n\n\n\nI’d argue that R, as arcane as it is (or not), is very likely one of the easiest languages to learn, and this is because there are a lot, and I mean a lot, of resources online:\n\n\n\nFree books (just take a look at the big book of R to find everything you need)\n\n\nYoutube channels dedicated to R (I’m shamelessly plugging mine)\n\n\nPackages with great documentation (take a look at the easystats suite for an example, or modelsummary and marginaleffects, both by Vincent Arel Bundock, and I’m not citing many, many others here)\n\n\nSlack channels where you can get help\n\n\nThe community of R users on twitter (check out the #RStats hashtag)\n\n\nThe RStudio Community forums\n\n\nAnd of course, the good old R-help mailing list\n\n\n\nAnd that’s only the free stuff. If you can afford it, there’s plenty of courses available as well. But no amount of free or paid content will be enough if you don’t invest enough time to learn the language, and this is true of anything. There are no secret recipes.\n\n\nP.S.: I got all these quotes from the {fortunes} package."
  },
  {
    "objectID": "posts/2013-11-07-gmm-with-rmd.html",
    "href": "posts/2013-11-07-gmm-with-rmd.html",
    "title": "Nonlinear Gmm with R - Example with a logistic regression",
    "section": "",
    "text": "In this post, I will explain how you can use the R gmm package to estimate a non-linear model, and more specifically a logit model. For my research, I have to estimate Euler equations using the Generalized Method of Moments. I contacted Pierre Chaussé, the creator of the gmm library for help, since I was having some difficulties. I am very grateful for his help (without him, I'd still probably be trying to estimate my model!).\n\n\nTheoretical background, motivation and data set\n\n\nI will not dwell in the theory too much, because you can find everything you need here. I think it’s more interesting to try to understand why someone would use the Generalized Method of Moments instead of maximization of the log-likelihood. Well, in some cases, getting the log-likelihood can be quite complicated, as can be the case for arbitrary, non-linear models (for example if you want to estimate the parameters of a very non-linear utility function). Also, moment conditions can sometimes be readily available, so using GMM instead of MLE is trivial. And finally, GMM is… well, a very general method: every popular estimator can be obtained as a special case of the GMM estimator, which makes it quite useful.\n\n\nAnother question that I think is important to answer is: why this post? Well, because that’s exactly the kind of post I would have loved to have found 2 months ago, when I was beginning to work with the GMM. Most posts I found presented the gmm package with very simple and trivial examples, which weren’t very helpful. The example presented below is not very complicated per se, but much more closer to a real-world problem than most stuff that is out there. At least, I hope you will find it useful!\n\n\nFor illustration purposes, I'll use data from Marno Verbeek's A guide to modern Econometrics, used in the illustration on page 197. You can download the data from the book's companion page here under the section Data sets or from the Ecdat package in R, which I’ll be using.\n\n\nImplementation in R\n\n\nI don't estimate the exact same model, but only use a subset of the variables available in the data set. Keep in mind that this post is just for illustration purposes.\n\n\nFirst load the gmm package and load the data set:\n\n\nlibrary(gmm)\nlibrary(Ecdat)\ndata(\"Benefits\")\n\nBenefits &lt;- transform(\n  Benefits,\n  age2 = age**2,\n  rr2 = rr**2\n  )\n\n\nWe can then estimate a logit model with the glm() function:\n\n\nnative &lt;- glm(ui ~ age + age2 + dkids + dykids + head + male + married + rr + rr2,\n              data = Benefits,\n              family = binomial(link = \"logit\"),\n              na.action = na.pass)\n\nsummary(native)\n\n## \n## Call:\n## glm(formula = y ~ age + age2 + dkids + dykids + head + male + \n##     married + rr + rr2, family = binomial(link = \"logit\"), na.action = na.pass)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -1.889  -1.379   0.788   0.896   1.237  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)   \n## (Intercept) -1.00534    0.56330   -1.78   0.0743 . \n## age          0.04909    0.02300    2.13   0.0328 * \n## age2        -0.00308    0.00293   -1.05   0.2924   \n## dkids       -0.10922    0.08374   -1.30   0.1921   \n## dykids       0.20355    0.09490    2.14   0.0320 * \n## head        -0.21534    0.07941   -2.71   0.0067 **\n## male        -0.05988    0.08456   -0.71   0.4788   \n## married      0.23354    0.07656    3.05   0.0023 **\n## rr           3.48590    1.81789    1.92   0.0552 . \n## rr2         -5.00129    2.27591   -2.20   0.0280 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 6086.1  on 4876  degrees of freedom\n## Residual deviance: 5983.9  on 4867  degrees of freedom\n## AIC: 6004\n## \n## Number of Fisher Scoring iterations: 4\n\n\nNow comes the interesting part: how can you estimate such a non-linear model with the gmm() function from the gmm package?\n\n\nFor every estimation with the Generalized Method of Moments, you will need valid moment conditions. It turns out that in the case of the logit model, this moment condition is quite simple:\n\n\\[E[X' * (Y-\\Lambda(X'\\theta))] = 0\\]\n\nwhere ( () ) is the logistic function. Let's translate this condition into code. First, we need the logistic function:\n\n\nlogistic &lt;- function(theta, data) {\n    return(1/(1 + exp(-data %*% theta)))\n}\n\n\nand let's also define a new data frame, to make our life easier with the moment conditions (don’t forget to add a column of ones to the matrix, hence the 1 after y):\n\n\ndat &lt;- data.matrix(with(Benefits,\n                        cbind(ui, 1, age, age2, dkids,\n                              dykids, head, sex,\n                              married, rr, rr2)))\n\n\nand now the moment condition itself:\n\n\nmoments &lt;- function(theta, data) {\n  y &lt;- as.numeric(data[, 1])\n  x &lt;- data.matrix(data[, 2:11])\n  m &lt;- x * as.vector((y - logistic(theta, x)))\n  return(cbind(m))\n}\n\n\nThe moment condition(s) are given by a function which returns a matrix with as many columns as moment conditions (same number of columns as parameters for just-identified models).\n\n\nTo use the gmm() function to estimate our model, we need to specify some initial values to get the maximization routine going. One neat trick is simply to use the coefficients of a linear regression; I found it to work well in a lot of situations:\n\n\ninit &lt;- (lm(ui ~ age + age2 + dkids + dykids + head + sex + married + rr + rr2,\n            data = Benefits))$coefficients\n\n\nAnd finally, we have everything to use gmm():\n\n\nmy_gmm &lt;- gmm(moments, x = dat, t0 = init, type = \"iterative\", crit = 1e-25, wmatrix = \"optimal\", method = \"Nelder-Mead\", control = list(reltol = 1e-25, maxit = 20000))\n\nsummary(my_gmm)\n\n\nPlease, notice the options crit=1e-25,method=“Nelder-Mead”,control=list(reltol=1e-25,maxit=20000): these options mean that the Nelder-Mead algorithm is used, and to specify further options to the Nelder-Mead algorithm, the control option is used. This is very important, as Pierre Chaussé explained to me: non-linear optimization is an art, and most of the time the default options won't cut it and will give you false results. To add insult to injury, the Generalized Method of Moments itself is very capricious and you will also have to play around with different initial values to get good results. As you can see, the Convergence code equals 10, which is a code specific to the Nelder-Mead method which indicates «degeneracy of the Nelder–Mead simplex.» . I’m not sure if this is a bad thing though, but other methods can give you better results. I’d suggest you try always different maximization routines with different starting values to see if your estimations are robust. Here, the results are very similar to what we obtained with the built-in function glm() so we can stop here.\n\n\nShould you notice any error whatsoever, do not hesitate to tell me."
  },
  {
    "objectID": "posts/2023-09-15-nix_for_r_part5.html",
    "href": "posts/2023-09-15-nix_for_r_part5.html",
    "title": "Reproducible data science with Nix, part 5 – Reproducible literate programming with Nix and Quarto",
    "section": "",
    "text": "This blog post is a copy-paste from this vignette"
  },
  {
    "objectID": "posts/2023-09-15-nix_for_r_part5.html#introduction",
    "href": "posts/2023-09-15-nix_for_r_part5.html#introduction",
    "title": "Reproducible data science with Nix, part 5 – Reproducible literate programming with Nix and Quarto",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nThis vignette will walk you through setting up a development environment with {rix} that can be used to compile Quarto documents into PDFs. We are going to use the Quarto template for the JSS to illustrate the process. The first section will show a simple way of achieving this, which will also be ideal for interactive development (writing the doc). The second section will discuss a way to build the document in a completely reproducible manner once it’s done."
  },
  {
    "objectID": "posts/2023-09-15-nix_for_r_part5.html#starting-with-the-basics-simple-but-not-entirely-reproducible",
    "href": "posts/2023-09-15-nix_for_r_part5.html#starting-with-the-basics-simple-but-not-entirely-reproducible",
    "title": "Reproducible data science with Nix, part 5 – Reproducible literate programming with Nix and Quarto",
    "section": "\nStarting with the basics (simple but not entirely reproducible)\n",
    "text": "Starting with the basics (simple but not entirely reproducible)\n\n\nThis approach will not be the most optimal, but it will be the simplest. We will start by building a development environment with all our dependencies, and we can then use it to compile our document interactively. But this approach is not quite reproducible and requires manual actions. In the next section we will show you to build a 100% reproducible document in a single command.\n\n\nSince we need both the {quarto} R package as well as the quarto engine, we add both of them to the r_pkgs and system_pkgs of arguments of {rix}. Because we want to compile a PDF, we also need to have texlive installed, as well as some LaTeX packages. For this, we use the tex_pkgs argument:\n\nlibrary(rix)\n\npath_default_nix &lt;- tempdir()\n\nrix(r_ver = \"4.3.1\",\n    r_pkgs = c(\"quarto\"),\n    system_pkgs = \"quarto\",\n    tex_pkgs = c(\"amsmath\"),\n    ide = \"other\",\n    shell_hook = \"\",\n    project_path = path_default_nix,\n    overwrite = TRUE,\n    print = TRUE)\n## # This file was generated by the {rix} R package v0.4.1 on 2023-12-19\n## # with following call:\n## # &gt;rix(r_ver = \"976fa3369d722e76f37c77493d99829540d43845\",\n## #  &gt; r_pkgs = c(\"quarto\"),\n## #  &gt; system_pkgs = \"quarto\",\n## #  &gt; tex_pkgs = c(\"amsmath\"),\n## #  &gt; ide = \"other\",\n## #  &gt; project_path = path_default_nix,\n## #  &gt; overwrite = TRUE,\n## #  &gt; print = TRUE,\n## #  &gt; shell_hook = \"\")\n## # It uses nixpkgs' revision 976fa3369d722e76f37c77493d99829540d43845 for reproducibility purposes\n## # which will install R version 4.3.1\n## # Report any issues to https://github.com/b-rodrigues/rix\n## let\n##  pkgs = import (fetchTarball \"https://github.com/NixOS/nixpkgs/archive/976fa3369d722e76f37c77493d99829540d43845.tar.gz\") {};\n##  rpkgs = builtins.attrValues {\n##   inherit (pkgs.rPackages) quarto;\n## };\n##   tex = (pkgs.texlive.combine {\n##   inherit (pkgs.texlive) scheme-small amsmath;\n## });\n##  system_packages = builtins.attrValues {\n##   inherit (pkgs) R glibcLocalesUtf8 quarto;\n## };\n##   in\n##   pkgs.mkShell {\n##     LOCALE_ARCHIVE = if pkgs.system == \"x86_64-linux\" then  \"${pkgs.glibcLocalesUtf8}/lib/locale/locale-archive\" else \"\";\n##     LANG = \"en_US.UTF-8\";\n##     LC_ALL = \"en_US.UTF-8\";\n##     LC_TIME = \"en_US.UTF-8\";\n##     LC_MONETARY = \"en_US.UTF-8\";\n##     LC_PAPER = \"en_US.UTF-8\";\n##     LC_MEASUREMENT = \"en_US.UTF-8\";\n## \n##     buildInputs = [  rpkgs tex system_packages  ];\n##       \n##   }\n\n(Save these lines into a script called build_env.R for instance, and run the script into a new folder made for this project.)\n\n\nBy default, {rix} will install the “small” version of the texlive distribution available on Nix. To see which texlive packages get installed with this small version, you can click here. We start by adding the amsmath package then build the environment using:\n\nnix_build()\n\nThen, drop into the Nix shell with nix-shell, and run quarto add quarto-journals/jss. This will install the template linked above. Then, in the folder that contains build_env.R, the generated default.nix and result download the following files from here:\n\n\n\narticle-visualization.pdf\n\n\nbibliography.bib\n\n\ntemplate.qmd\n\n\n\nand try to compile template.qmd by running:\n\nquarto render template.qmd --to jss-pdf\n\nYou should get the following error message:\n\nQuitting from lines 99-101 [unnamed-chunk-1] (template.qmd)\nError in `find.package()`:\n! there is no package called 'MASS'\nBacktrace:\n 1. utils::data(\"quine\", package = \"MASS\")\n 2. base::find.package(package, lib.loc, verbose = verbose)\nExecution halted\n\n\nSo there’s an R chunk in template.qmd that uses the {MASS} package. Change build_env.R to generate a new default.nix file that will now add {MASS} to the environment when built:\n\nrix(r_ver = \"4.3.1\",\n    r_pkgs = c(\"quarto\", \"MASS\"),\n    system_pkgs = \"quarto\",\n    tex_pkgs = c(\"amsmath\"),\n    ide = \"other\",\n    shell_hook = \"\",\n    project_path = path_default_nix,\n    overwrite = TRUE,\n    print = TRUE)\n## # This file was generated by the {rix} R package v0.4.1 on 2023-12-19\n## # with following call:\n## # &gt;rix(r_ver = \"976fa3369d722e76f37c77493d99829540d43845\",\n## #  &gt; r_pkgs = c(\"quarto\",\n## #  &gt; \"MASS\"),\n## #  &gt; system_pkgs = \"quarto\",\n## #  &gt; tex_pkgs = c(\"amsmath\"),\n## #  &gt; ide = \"other\",\n## #  &gt; project_path = path_default_nix,\n## #  &gt; overwrite = TRUE,\n## #  &gt; print = TRUE,\n## #  &gt; shell_hook = \"\")\n## # It uses nixpkgs' revision 976fa3369d722e76f37c77493d99829540d43845 for reproducibility purposes\n## # which will install R version 4.3.1\n## # Report any issues to https://github.com/b-rodrigues/rix\n## let\n##  pkgs = import (fetchTarball \"https://github.com/NixOS/nixpkgs/archive/976fa3369d722e76f37c77493d99829540d43845.tar.gz\") {};\n##  rpkgs = builtins.attrValues {\n##   inherit (pkgs.rPackages) quarto MASS;\n## };\n##   tex = (pkgs.texlive.combine {\n##   inherit (pkgs.texlive) scheme-small amsmath;\n## });\n##  system_packages = builtins.attrValues {\n##   inherit (pkgs) R glibcLocalesUtf8 quarto;\n## };\n##   in\n##   pkgs.mkShell {\n##     LOCALE_ARCHIVE = if pkgs.system == \"x86_64-linux\" then  \"${pkgs.glibcLocalesUtf8}/lib/locale/locale-archive\" else \"\";\n##     LANG = \"en_US.UTF-8\";\n##     LC_ALL = \"en_US.UTF-8\";\n##     LC_TIME = \"en_US.UTF-8\";\n##     LC_MONETARY = \"en_US.UTF-8\";\n##     LC_PAPER = \"en_US.UTF-8\";\n##     LC_MEASUREMENT = \"en_US.UTF-8\";\n## \n##     buildInputs = [  rpkgs tex system_packages  ];\n##       \n##   }\n\nTrying to compile the document results now in another error message:\n\ncompilation failed- no matching packages\nLaTeX Error: File `orcidlink.sty' not found\n\nThis means that the LaTeX orcidlink package is missing, and we can solve the problem by adding “orcidlink” to the list of tex_pkgs. Rebuild the environment and try again to compile the template. Trying again yields a new error:\n\ncompilation failed- no matching packages\nLaTeX Error: File `tcolorbox.sty' not found.\n\nJust as before, add the tcolorbox package to the list of tex_pkgs. You will need to do this several times for some other packages. There is unfortunately no easier way to list the dependencies and requirements of a LaTeX document.\n\n\nThis is what the final script to build the environment looks like:\n\nrix(r_ver = \"4.3.1\",\n    r_pkgs = c(\"quarto\", \"MASS\"),\n    system_pkgs = \"quarto\",\n    tex_pkgs = c(\n      \"amsmath\",\n      \"environ\",\n      \"fontawesome5\",\n      \"orcidlink\",\n      \"pdfcol\",\n      \"tcolorbox\",\n      \"tikzfill\"\n    ),\n    ide = \"other\",\n    shell_hook = \"\",\n    project_path = path_default_nix,\n    overwrite = TRUE,\n    print = TRUE)\n## # This file was generated by the {rix} R package v0.4.1 on 2023-12-19\n## # with following call:\n## # &gt;rix(r_ver = \"976fa3369d722e76f37c77493d99829540d43845\",\n## #  &gt; r_pkgs = c(\"quarto\",\n## #  &gt; \"MASS\"),\n## #  &gt; system_pkgs = \"quarto\",\n## #  &gt; tex_pkgs = c(\"amsmath\",\n## #  &gt; \"environ\",\n## #  &gt; \"fontawesome5\",\n## #  &gt; \"orcidlink\",\n## #  &gt; \"pdfcol\",\n## #  &gt; \"tcolorbox\",\n## #  &gt; \"tikzfill\"),\n## #  &gt; ide = \"other\",\n## #  &gt; project_path = path_default_nix,\n## #  &gt; overwrite = TRUE,\n## #  &gt; print = TRUE,\n## #  &gt; shell_hook = \"\")\n## # It uses nixpkgs' revision 976fa3369d722e76f37c77493d99829540d43845 for reproducibility purposes\n## # which will install R version 4.3.1\n## # Report any issues to https://github.com/b-rodrigues/rix\n## let\n##  pkgs = import (fetchTarball \"https://github.com/NixOS/nixpkgs/archive/976fa3369d722e76f37c77493d99829540d43845.tar.gz\") {};\n##  rpkgs = builtins.attrValues {\n##   inherit (pkgs.rPackages) quarto MASS;\n## };\n##   tex = (pkgs.texlive.combine {\n##   inherit (pkgs.texlive) scheme-small amsmath environ fontawesome5 orcidlink pdfcol tcolorbox tikzfill;\n## });\n##  system_packages = builtins.attrValues {\n##   inherit (pkgs) R glibcLocalesUtf8 quarto;\n## };\n##   in\n##   pkgs.mkShell {\n##     LOCALE_ARCHIVE = if pkgs.system == \"x86_64-linux\" then  \"${pkgs.glibcLocalesUtf8}/lib/locale/locale-archive\" else \"\";\n##     LANG = \"en_US.UTF-8\";\n##     LC_ALL = \"en_US.UTF-8\";\n##     LC_TIME = \"en_US.UTF-8\";\n##     LC_MONETARY = \"en_US.UTF-8\";\n##     LC_PAPER = \"en_US.UTF-8\";\n##     LC_MEASUREMENT = \"en_US.UTF-8\";\n## \n##     buildInputs = [  rpkgs tex system_packages  ];\n##       \n##   }\n\nThe template will now compile with this environment. To look for a LaTeX package, you can use the search engine on CTAN.\n\n\nAs stated in the beginning of this section, this approach is not the most optimal, but it has its merits, especially if you’re still working on the document. Once the environment is set up, you can simply work on the doc and compile it as needed using quarto render. In the next section, we will explain how to build a 100% reproducible document."
  },
  {
    "objectID": "posts/2023-09-15-nix_for_r_part5.html#reproducible-literate-programming",
    "href": "posts/2023-09-15-nix_for_r_part5.html#reproducible-literate-programming",
    "title": "Reproducible data science with Nix, part 5 – Reproducible literate programming with Nix and Quarto",
    "section": "\n100% reproducible literate programming\n",
    "text": "100% reproducible literate programming\n\n\nLet’s not forget that Nix is not just a package manager, but also a programming language. The default.nix files that {rix} generates are written in this language, which was made entirely for the purpose of building software. If you are not a developer, you may not realise it but the process of compiling a Quarto or LaTeX document is very similar to the process of building any piece of software. So we can use Nix to compile a document in a completely reproducible environment.\n\n\nFirst, let’s fork the repo that contains the Quarto template we need. We will fork this repo. This repo contains the template.qmd file that we can change (which is why we fork it, in practice we would replace this template.qmd by our own, finished, source .qmd file). Now we need to change our default.nix:\n\nlet\n pkgs = import (fetchTarball \"https://github.com/NixOS/nixpkgs/archive/976fa3369d722e76f37c77493d99829540d43845.tar.gz\") {};\n rpkgs = builtins.attrValues {\n   inherit (pkgs.rPackages) quarto MASS;\n };\n tex = (pkgs.texlive.combine {\n   inherit (pkgs.texlive) scheme-small amsmath environ fontawesome5 orcidlink pdfcol tcolorbox tikzfill;\n });\n system_packages = builtins.attrValues {\n   inherit (pkgs) R quarto;\n };\n in\n pkgs.mkShell {\n   buildInputs = [  rpkgs tex system_packages  ];\n }\n\nto the following:\n\nlet\n pkgs = import (fetchTarball \"https://github.com/NixOS/nixpkgs/archive/976fa3369d722e76f37c77493d99829540d43845.tar.gz\") {};\n rpkgs = builtins.attrValues {\n  inherit (pkgs.rPackages) quarto MASS;\n };\n tex = (pkgs.texlive.combine {\n  inherit (pkgs.texlive) scheme-small amsmath environ fontawesome5 orcidlink pdfcol tcolorbox tikzfill;\n });\n system_packages = builtins.attrValues {\n  inherit (pkgs) R quarto;\n };\n in\n pkgs.stdenv.mkDerivation {\n   name = \"my-paper\";\n   src = pkgs.fetchgit {\n       url = \"https://github.com/b-rodrigues/my_paper/\";\n       branchName = \"main\";\n       rev = \"715e9f007d104c23763cebaf03782b8e80cb5445\";\n       sha256 = \"sha256-e8Xg7nJookKoIfiJVTGoJkvCuFNTT83YZ6SK3GT2T8g=\";\n     };\n   buildInputs = [  rpkgs tex system_packages  ];\n   buildPhase =\n     ''\n     # Deno needs to add stuff to $HOME/.cache\n     # so we give it a home to do this\n     mkdir home\n     export HOME=$PWD/home\n     quarto add --no-prompt $src\n     quarto render $PWD/template.qmd --to jss-pdf\n     '';\n   installPhase =\n     ''\n     mkdir -p $out\n     cp template.pdf $out/\n     '';\n }\n\nSo we changed the second part of the file, we’re not building a shell anymore using mkShell, but a derivation. Derivation is Nix jargon for package, or software. So what is our derivation? First, we clone the repo we forked just before (I forked the repository and called it my_paper):\n\npkgs.stdenv.mkDerivation {\n  name = \"my-paper\";\n  src = pkgs.fetchgit {\n      url = \"https://github.com/b-rodrigues/my_paper/\";\n      branchName = \"main\";\n      rev = \"715e9f007d104c23763cebaf03782b8e80cb5445\";\n      sha256 = \"sha256-e8Xg7nJookKoIfiJVTGoJkvCuFNTT83YZ6SK3GT2T8g=\";\n    };\n\nThis repo contains our quarto template, and because we’re using a specific commit, we will always use exactly this release of the template for our document. This is in contrast to before where we used quarto add quarto-journals/jss to install the template. Doing this interactively makes our project not reproducible because if we compile our Quarto doc today, we would be using the template as it is today, but if we compile the document in 6 months, then we would be using the template as it would be in 6 months (I should say that it is possible to install specific releases of Quarto templates using following notation: quarto add quarto-journals/jss@v0.9.2 so this problem can be mitigated).\n\n\nThe next part of the file contains following lines:\n\nbuildInputs = [  rpkgs tex system_packages  ];\nbuildPhase =\n  ''\n  # Deno needs to add stuff to $HOME/.cache\n  # so we give it a home to do this\n  mkdir home\n  export HOME=$PWD/home\n  quarto add --no-prompt $src\n  quarto render $PWD/template.qmd --to jss-pdf\n  '';\n\nThe buildInputs are the same as before. What’s new is the buildPhase. This is actually the part in which the document gets compiled. The first step is to create a home directory. This is because Quarto needs to save the template we want to use in /home/.cache/deno. If you’re using quarto interactively, that’s not an issue, since your home directory will be used. But with Nix, things are different, so we need to create an empty directory and specify this as the home. This is what these two lines do:\n\nmkdir home\nexport HOME=$PWD/home\n\n($PWD —Print Working Directory— is a shell variable referring to the current working directory.)\n\n\nNow, we need to install the template that we cloned from Github. For this we can use quarto add just as before, but instead of installing it directly from Github, we install it from the repository that we cloned. We also add the –no-prompt flag so that the template gets installed without asking us for confirmation. This is similar to how when building a Docker image, we don’t want any interactive prompt to show up, or else the process will get stuck. $src refers to the path of our downloaded Github repository. Finally we can compile the document:\n\nquarto render $PWD/template.qmd --to jss-pdf\n\nThis will compile the template.qmd (our finished paper). Finally, there’s the installPhase:\n\ninstallPhase =\n  ''\n  mkdir -p $out\n  cp template.pdf $out/\n  '';\n\n$out is a shell variable defined inside the build environment and refers to the path, so we can use it to create a directory that will contain our output (the compiled PDF file). So we use mkdir -p to recursively create all the directory structure, and then copy the compiled document to $out/. We can now build our document by running nix_build(). Now, you may be confused by the fact that you won’t see the PDF in your working directory. But remember that software built by Nix will always be stored in the Nix store, so our PDF is also in the store, since this is what we built. To find it, run:\n\nreadlink result\n\nwhich will show the path to the PDF. You could use this to open the PDF in your PDF viewer application (on Linux at least):\n\nxdg-open $(readlink result)/template.pdf"
  },
  {
    "objectID": "posts/2023-09-15-nix_for_r_part5.html#conclusion",
    "href": "posts/2023-09-15-nix_for_r_part5.html#conclusion",
    "title": "Reproducible data science with Nix, part 5 – Reproducible literate programming with Nix and Quarto",
    "section": "\nConclusion\n",
    "text": "Conclusion\n\n\nThis vignette showed two approaches, both have their merits: the first approach that is more interactive is useful while writing the document. You get access to a shell and can work on the document and compile it quickly. The second approach is more useful once the document is ready and you want to have a way of quickly rebuilding it for reproducibility purposes. This approach should also be quite useful in a CI/CD environment."
  },
  {
    "objectID": "posts/2022-03-12-purely.html",
    "href": "posts/2022-03-12-purely.html",
    "title": "Capture errors, warnings and messages",
    "section": "",
    "text": "In my last video I tried to add a feature to my {loud} package (more info here) and I succeeded. But in succeeding in realised that I would need to write a bit more code than what I expected. To make a long story short: it is possible to capture errors using purrr::safely():\na is now a list with elements $result and $error. If everything goes right, $result holds the result of the operation, and if everything goes wrong, $result is NULL but $error now contains the error message. This is especially useful in non-interactive contexts. There is another similar function in {purrr} called quietly(), which captures warnings and messages:\nas you can see, providing a negative number to log() does not cause an error, but simply a warning. A result of NaN is returned (you can try with log(-10) in your console). quietly() captures the warning message and returns a list of 4 elements, $result, $output, $warnings and $messages. The problem here, is that:\nreturns something useless: $result is NaN (because that’s what log() returns for negative numbers) but $error is NULL since no error was thrown, but only a warning! We have a similar problem with quiet_log():\nhere, the error message is thrown, but not captured, since quietly() does not capture error messages.\nSo, are we back to square one? Not necessarily, since you could compose both functions:\nAs you can see, in the case of a2, the warning was captured, and in the case of b2 the error was captured. The problem, is that the resulting object is quite complex. It’s a list where $result is itself a list in case of a warning, or $error is a list in case of an error.\nI tried to write a function that would decorate a function (as do safely() and quietly()), which in turn would then return a simple list and capture, errors, warnings and messages. I came up with this code, after re-reading Advanced R, in particular this chapter:\nMessages get captured:\nas do warnings:\nas do errors:\nThe structure of the result is always $result and $log. In case everything goes well $result holds the result:\nAnd another example, with a more complex call:\nBut in case something goes wrong, the error message will get captured.\nLet’s try here to select a column that does not exist:\nCompare to what happens with select():"
  },
  {
    "objectID": "posts/2022-03-12-purely.html#update-2022-03-13",
    "href": "posts/2022-03-12-purely.html#update-2022-03-13",
    "title": "Capture errors, warnings and messages",
    "section": "\nUpdate 2022-03-13\n",
    "text": "Update 2022-03-13\n\n\nAfter writing this post I realised that the error message of select does not get captured. This is the only example I’ve found where the error message does not get caught. This seems to be related to the fact that tidyverse function have their own class of error messages that inherit from error. For some reason, there are no issues with other functions, for example:\n\npurely(group_by)(mtcars, bm)\n## $result\n## [1] NA\n## \n## $log\n##                                             \n## \"Must group by variables found in `.data`.\"\n\nI will need to solve this…"
  },
  {
    "objectID": "posts/2022-03-12-purely.html#post-continued",
    "href": "posts/2022-03-12-purely.html#post-continued",
    "title": "Capture errors, warnings and messages",
    "section": "\nPost continued…\n",
    "text": "Post continued…\n\n\nThe code (and thus the pipeline) completely fails! I’ve added this function to my {loud} package, but the biggest benefit of all this is that the main function of the package, loudly() now uses purely() under the hood to provide more useful log messages in case of failure:\n\nsuppressPackageStartupMessages(library(loud))\n\nloud_sqrt &lt;- loudly(sqrt)\nloud_mean &lt;- loudly(mean)\nloud_exp &lt;- loudly(exp)\n\n\nresult_pipe &lt;- -1:-10 |&gt;\n  loud_mean() %&gt;=% # This results in a negative number...\n  loud_sqrt() %&gt;=% # which sqrt() does not know how to handle\n  loud_exp()\n\nIf we now inspect result_pipe, we find a complete log of what went wrong:\n\nresult_pipe\n## $result\n## NULL\n## \n## $log\n## [1] \"Log start...\"                                                                                                                                                            \n## [2] \"✔ mean(-1:-10) started at 2022-03-13 14:17:30 and ended at 2022-03-13 14:17:30\"                                                                                          \n## [3] \"✖ CAUTION - ERROR: sqrt(.l$result) started at 2022-03-13 14:17:30 and failed at 2022-03-13 14:17:30 with following message: NaNs produced\"                               \n## [4] \"✖ CAUTION - ERROR: exp(.l$result) started at 2022-03-13 14:17:30 and failed at 2022-03-13 14:17:30 with following message: non-numeric argument to mathematical function\"\n\nIf you want to know more about {loud}, I suggest you read my previous blog post and if you need a more realistic example, take a look at this.\n\n\nIf you try it, please let me know!"
  },
  {
    "objectID": "posts/2019-05-18-xml2.html",
    "href": "posts/2019-05-18-xml2.html",
    "title": "For posterity: install {xml2} on GNU/Linux distros",
    "section": "",
    "text": "Today I’ve removed my system’s R package and installed MRO instead. While re-installing all packages, I’ve encountered one of the most frustrating error message for someone installing packages from source:\n\nError : /tmp/Rtmpw60aCp/R.INSTALL7819efef27e/xml2/man/read_xml.Rd:47: unable to load shared object\n'/usr/lib64/R/library/xml2/libs/xml2.so': \nlibicui18n.so.58: cannot open shared object file: No such file or directory ERROR: \ninstalling Rd objects failed for package ‘xml2’ \n\nThis library, libicui18n.so.58 is a pain in the butt. However, you can easily install it if you install miniconda. After installing miniconda, you can look for it with:\n\n[19-05-18 18:26] cbrunos in ~/ ➤ locate libicui18n.so.58\n\n/home/cbrunos/miniconda3/lib/libicui18n.so.58\n/home/cbrunos/miniconda3/lib/libicui18n.so.58.2\n/home/cbrunos/miniconda3/pkgs/icu-58.2-h9c2bf20_1/lib/libicui18n.so.58\n/home/cbrunos/miniconda3/pkgs/icu-58.2-h9c2bf20_1/lib/libicui18n.so.58.2\n\n\nSo now you need to tell R where to look for this library. The following Stackoverflow answer saved the day. Add the following lines to R_HOME/etc/ldpaths (in my case, it was in /opt/microsoft/ropen/3.5.2/lib64/R/etc/):\n\nLD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/username/miniconda3/lib/\nexport LD_LIBRARY_PATH\n\nand try to install xml2 again, and it should work! If not, just abandon the idea of using R and switch to doing data science with VBA, it’ll be less frustrating.\n\n\nSomething else, if you install Microsoft R Open, you’ll be stuck with some older packages, because by default MRO uses a snapshot of CRAN from a given day as a mirror. To get the freshest packages, add the following line to your .Rprofile file (which should be located in your HOME):\n\noptions(repos = c(CRAN = \"http://cran.rstudio.com/\"))\n\nAnd to finish this short blog post, add the following line to your .Rprofile if you get the following error messages when trying to install a package from github:\n\nremotes::install_github('rstudio/DT') Downloading GitHub repo rstudio/DT@master tar: \nThis does not look like a tar archive gzip: stdin: unexpected end of file tar: Child returned \nstatus 1 tar: Error is not recoverable: exiting now tar: This does not look like a tar archive \ngzip: stdin: unexpected end of file tar: Child returned status 1 tar: Error is not recoverable: \nexiting now Error in getrootdir(untar(src, list = TRUE)) : length(file_list) &gt; 0 is not TRUE Calls: \n&lt;Anonymous&gt; ... source_pkg -&gt; decompress -&gt; getrootdir -&gt; stopifnot In addition: Warning messages: 1: \nIn utils::untar(tarfile, ...) : ‘tar -xf '/tmp/RtmpitCFRe/file2677442609b8.tar.gz' -C \n'/tmp/RtmpitCFRe/remotes267752f2629f'’ returned error code 2 2: \nIn system(cmd, intern = TRUE) : running command 'tar -tf '/tmp/RtmpitCFRe/file2677442609b8.tar.gz'' \nhad status 2 Execution halted\n\nThe solution, which can found here\n\noptions(\"download.file.method\" = \"libcurl\")"
  },
  {
    "objectID": "posts/2020-04-12-basic_ggplot2.html",
    "href": "posts/2020-04-12-basic_ggplot2.html",
    "title": "How to basic: bar plots",
    "section": "",
    "text": "This blog post shows how to make bar plots and area charts. It’s mostly a list of recipes, indented for myself. These are plots I have often to do in reports and would like to have the code handy somewhere. Maybe this will be helpful to some of you as well. Actually, this post is exactly how I started my blog post. I wanted to have a repository of recipes, and with time the blog grew to what it is now (tutorials and me exploring methods and datasets with R)."
  },
  {
    "objectID": "posts/2020-04-12-basic_ggplot2.html#bar-charts",
    "href": "posts/2020-04-12-basic_ggplot2.html#bar-charts",
    "title": "How to basic: bar plots",
    "section": "\nBar charts\n",
    "text": "Bar charts\n\n\nBar charts are quite simple plots, but there are enough variations of them that they deserve one single blog post. However, don’t expect many explanations.\n\n\nLet’s first start by loading some data, and the usually required packages:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\nlibrary(colorspace)\ndata(gss_cat)\n\nVery often, what one wants to show are counts:\n\ngss_cat %&gt;%\n  count(marital, race)\n## # A tibble: 18 x 3\n##    marital       race      n\n##  * &lt;fct&gt;         &lt;fct&gt; &lt;int&gt;\n##  1 No answer     Other     2\n##  2 No answer     Black     2\n##  3 No answer     White    13\n##  4 Never married Other   633\n##  5 Never married Black  1305\n##  6 Never married White  3478\n##  7 Separated     Other   110\n##  8 Separated     Black   196\n##  9 Separated     White   437\n## 10 Divorced      Other   212\n## 11 Divorced      Black   495\n## 12 Divorced      White  2676\n## 13 Widowed       Other    70\n## 14 Widowed       Black   262\n## 15 Widowed       White  1475\n## 16 Married       Other   932\n## 17 Married       Black   869\n## 18 Married       White  8316\n\nLet’s lump marital statuses that appear less than 10% of the time into an “Other” category:\n\n(\n  counts_marital_race &lt;- gss_cat %&gt;%\n    mutate(marital = fct_lump(marital, prop = 0.1)) %&gt;%\n    count(marital, race)\n)\n## # A tibble: 12 x 3\n##    marital       race      n\n##  * &lt;fct&gt;         &lt;fct&gt; &lt;int&gt;\n##  1 Never married Other   633\n##  2 Never married Black  1305\n##  3 Never married White  3478\n##  4 Divorced      Other   212\n##  5 Divorced      Black   495\n##  6 Divorced      White  2676\n##  7 Married       Other   932\n##  8 Married       Black   869\n##  9 Married       White  8316\n## 10 Other         Other   182\n## 11 Other         Black   460\n## 12 Other         White  1925\n\nThe simplest bar plot:\n\nggplot(counts_marital_race) +\n  geom_col(aes(x = marital, y = n, fill = race)) +\n  scale_fill_discrete_qualitative(palette = \"Dark 3\") +\n  brotools::theme_blog()\n\n\n\n\nNow with position = \"dodge\":\n\nggplot(counts_marital_race) +\n  geom_col(aes(x = marital, y = n, fill = race), position = \"dodge\") +\n  scale_fill_discrete_qualitative(palette = \"Dark 3\") +\n  brotools::theme_blog()\n\n\n\n\nMoving the legend around with theme(legend.position = …):\n\nggplot(counts_marital_race) +\n  geom_col(aes(x = marital, y = n, fill = race), position = \"dodge\") +\n  scale_fill_discrete_qualitative(palette = \"Dark 3\") +\n  brotools::theme_blog() +\n  theme(legend.position = \"left\")\n\n\n\n\nCounting by year as well:\n\n(\n  counts_marital_race_year &lt;- gss_cat %&gt;%\n    mutate(marital = fct_lump(marital, prop = 0.1)) %&gt;%\n    count(year, marital, race) %&gt;%\n    ungroup()\n)\n## # A tibble: 96 x 4\n##     year marital       race      n\n##  * &lt;int&gt; &lt;fct&gt;         &lt;fct&gt; &lt;int&gt;\n##  1  2000 Never married Other    60\n##  2  2000 Never married Black   157\n##  3  2000 Never married White   495\n##  4  2000 Divorced      Other    20\n##  5  2000 Divorced      Black    60\n##  6  2000 Divorced      White   361\n##  7  2000 Married       Other    78\n##  8  2000 Married       Black   121\n##  9  2000 Married       White  1079\n## 10  2000 Other         Other    17\n## # … with 86 more rows\n\nWhen you want to show how a variable evolves through time, area chart are handy:\n\ncounts_marital_race_year %&gt;%\n  group_by(year, marital) %&gt;%\n  summarise(n = sum(n)) %&gt;%\n  ggplot() +\n  geom_area(aes(x = year, y = n, fill = marital)) +\n  scale_fill_discrete_qualitative(palette = \"Dark 3\") +\n  brotools::theme_blog() \n\n\n\n\nNow with facets:\n\ncounts_marital_race_year %&gt;%\n  ggplot() +\n  geom_area(aes(x = year, y = n, fill = marital)) +\n  facet_wrap(facets = vars(race), ncol = 1) +\n  scale_fill_discrete_qualitative(palette = \"Dark 3\") +\n  brotools::theme_blog() \n\n\n\n\nBut what if I want each plot to have its own y axis?\n\ncounts_marital_race_year %&gt;%\n  ggplot() +\n  geom_area(aes(x = year, y = n, fill = marital)) +\n  facet_wrap(facets = vars(race), ncol = 1, scales = \"free_y\") +\n  scale_fill_discrete_qualitative(palette = \"Dark 3\") +\n  brotools::theme_blog() \n\n\n\n\nNow doing an area chart but with relative frequencies:\n\ncounts_marital_race_year %&gt;% \n  group_by(year, marital) %&gt;% \n  summarise(n = sum(n)) %&gt;%  \n  mutate(freq = n/sum(n)) %&gt;% \n  ggplot() +\n  geom_area(aes(x = year, y = freq, fill = marital)) +\n  scale_fill_discrete_qualitative(palette = \"Dark 3\") +\n  brotools::theme_blog() \n\n\n\n\nWith facet_wrap():\n\ncounts_marital_race_year %&gt;% \n  group_by(race, year, marital) %&gt;% \n  summarise(n = sum(n)) %&gt;%  \n  mutate(freq = n/sum(n)) %&gt;% \n  ggplot() +\n  geom_area(aes(x = year, y = freq, fill = marital)) +\n  facet_wrap(facets = vars(race), ncol = 1, scales = \"free_y\") +\n  scale_fill_discrete_qualitative(palette = \"Dark 3\") +\n  brotools::theme_blog() \n\n\n\n\nWant to replace 2000 with “2000-01-01”? First need to create vector of prettier dates and positions:\n\npretty_dates &lt;- counts_marital_race_year %&gt;%\n  mutate(pretty_dates = paste0(year, \"-01-01\")) %&gt;%\n  pull(pretty_dates) %&gt;%\n  unique()\n\nposition_dates &lt;- counts_marital_race_year %&gt;%\n  pull(year) %&gt;%\n  unique() %&gt;%\n  sort() \n\nscale_x_continuous() can now use this. Using guide = guide_axis(n.dodge = 2) to avoid overlapping labels:\n\ncounts_marital_race_year %&gt;% \n  group_by(race, year, marital) %&gt;% \n  summarise(n = sum(n)) %&gt;%  \n  mutate(freq = n/sum(n)) %&gt;%\n  ggplot() +\n  geom_area(aes(x = year, y = freq, fill = marital)) +\n  facet_wrap(facets = vars(race), ncol = 1, scales = \"free_y\") +\n  scale_x_continuous(\"Year of survey\", labels = pretty_dates,\n                     breaks = position_dates, guide = guide_axis(n.dodge = 2)) +\n  scale_fill_discrete_qualitative(palette = \"Dark 3\") +\n  brotools::theme_blog() \n\n\n\n\nAdding labels is not trivial. Here it is not working:\n\ncounts_marital_race_year %&gt;% \n  group_by(race, year, marital) %&gt;% \n  summarise(n = sum(n)) %&gt;%  \n  mutate(freq = n/sum(n)) %&gt;% \n  ggplot() +\n  geom_area(aes(x = year, y = freq, fill = marital)) +\n  facet_wrap(facets = vars(race), ncol = 1, scales = \"free_y\") +\n  scale_x_continuous(\"Year of survey\", labels = pretty_dates,\n                     breaks = position_dates, guide = guide_axis(n.dodge = 2)) +\n  geom_label(aes(x = year, y = freq, label = round(100 * freq))) +\n  scale_fill_discrete_qualitative(palette = \"Dark 3\") +\n  brotools::theme_blog() \n\n\n\n\nAnother failed attempt. I leave it here for posterity. My first idea was first to sort the grouped data set by descending frequency, and then to reorder the factor variable marital by descending position, which is the cumulative percentage. This would work fine, if the same factor levels would have had the same order for each of the race categories. However, this is not the case. For blacks, the most frequent category is “Never Married”. As you can see below, this trick worked well for 2 categories out of 3:\n\ncounts_marital_race_year %&gt;% \n  group_by(race, year, marital) %&gt;% \n  summarise(n = sum(n)) %&gt;%  \n  mutate(freq = n/sum(n)) %&gt;%\n  group_by(year, race) %&gt;%  \n  arrange(desc(freq)) %&gt;% \n  mutate(position = cumsum(freq)) %&gt;% \n  mutate(marital = fct_reorder(marital, desc(position))) %&gt;% \n  ggplot() +\n  geom_area(aes(x = year, y = freq, fill = marital)) +\n  facet_wrap(facets = vars(race), ncol = 1, scales = \"free\") +\n  scale_x_continuous(\"Year of survey\", labels = pretty_dates,\n                     breaks = position_dates, guide = guide_axis(n.dodge = 2)) +\n  geom_label(aes(x = year, y = position, label = round(100 * freq))) +\n  scale_fill_discrete_qualitative(palette = \"Dark 3\") +\n  brotools::theme_blog() \n\n\n\n\nSo to remedy this, is not reorder too early; first, we need to reorder the factor variable by frequency. Then, we arrange the data by the now reordered marital variable, and then we can compute the position using the cumulative frequency.\n\ncounts_marital_race_year %&gt;% \n  group_by(race, year, marital) %&gt;% \n  summarise(n = sum(n)) %&gt;%  \n  mutate(freq = n/sum(n)) %&gt;%\n  group_by(year, race) %&gt;%  \n  mutate(marital = fct_reorder(marital, freq)) %&gt;% \n  arrange(desc(marital)) %&gt;% \n  mutate(position = cumsum(freq)) %&gt;% \n  ggplot() +\n  geom_area(aes(x = year, y = freq, fill = marital)) +\n  facet_wrap(facets = vars(race), ncol = 1, scales = \"free\") +\n  scale_x_continuous(\"Year of survey\", labels = pretty_dates,\n                     breaks = position_dates, guide = guide_axis(n.dodge = 2)) +\n  geom_label(aes(x = year, y = position, label = round(100 * freq))) +\n  scale_fill_discrete_qualitative(palette = \"Dark 3\") +\n  brotools::theme_blog() \n\n\n\n\nWe can place the labels a bit better (in the middle of their respective areas), like so:\n\ncounts_marital_race_year %&gt;% \n  group_by(race, year, marital) %&gt;% \n  summarise(n = sum(n)) %&gt;%  \n  mutate(freq = n/sum(n)) %&gt;%\n  group_by(year, race) %&gt;%  \n  mutate(marital = fct_reorder(marital, freq)) %&gt;% \n  arrange(desc(marital)) %&gt;% \n  mutate(position = cumsum(freq)) %&gt;% mutate(prev_pos = lag(position, default = 0)) %&gt;%\n  mutate(position = (position + prev_pos)/2) %&gt;%  \n  ggplot() +\n  geom_area(aes(x = year, y = freq, fill = marital)) +\n  facet_wrap(facets = vars(race), ncol = 1, scales = \"free\") +\n  scale_x_continuous(\"Year of survey\", labels = pretty_dates,\n                     breaks = position_dates, guide = guide_axis(n.dodge = 2)) +\n  geom_label(aes(x = year, y = position, label = round(100 * freq))) +\n  scale_fill_discrete_qualitative(palette = \"Dark 3\") +\n  brotools::theme_blog() \n\n\n\n\nNow let’s focus on the variable tvhours. We want to show the total watched hours, but also the total across all the categories of race and marital in a faceted bar plot:\n\n(\n  total_tv &lt;- gss_cat %&gt;%\n    group_by(year, race, marital) %&gt;%\n    summarise(total_tv = sum(tvhours, na.rm = TRUE))\n)\n## # A tibble: 127 x 4\n## # Groups:   year, race [24]\n##     year race  marital       total_tv\n##    &lt;int&gt; &lt;fct&gt; &lt;fct&gt;            &lt;int&gt;\n##  1  2000 Other No answer            2\n##  2  2000 Other Never married      103\n##  3  2000 Other Separated           16\n##  4  2000 Other Divorced            17\n##  5  2000 Other Widowed             24\n##  6  2000 Other Married            122\n##  7  2000 Black Never married      452\n##  8  2000 Black Separated          135\n##  9  2000 Black Divorced           156\n## 10  2000 Black Widowed            183\n## # … with 117 more rows\n\nThis tibble has the total watched hours by year, race and marital status variables. How to add the total by year and race categories? For this, by are first going to use the group_split():\n\ntotal_tv_split &lt;- total_tv %&gt;%\n  select(race, year, marital, total_tv) %&gt;%\n  mutate(year = as.character(year)) %&gt;%  \n  group_split(year, race)\n## Warning: ... is ignored in group_split(&lt;grouped_df&gt;), please use\n## group_by(..., .add = TRUE) %&gt;% group_split()\n\nI have to re-order the columns with select(), because when using janitor::adorn_totals(), which I will be using below to add totals, the first column must be a character column (it serves as an identifier column).\n\n\nThis creates a list with 3 races times 6 years, so 24 elements. Each element of the list is a tibble with each unique combination of year and race:\n\nlength(total_tv_split)\n## [1] 24\ntotal_tv_split[1:2]\n## &lt;list_of&lt;\n##   tbl_df&lt;\n##     race    : factor&lt;f4a07&gt;\n##     year    : character\n##     marital : factor&lt;82ceb&gt;\n##     total_tv: integer\n##   &gt;\n## &gt;[2]&gt;\n## [[1]]\n## # A tibble: 6 x 4\n##   race  year  marital       total_tv\n##   &lt;fct&gt; &lt;chr&gt; &lt;fct&gt;            &lt;int&gt;\n## 1 Other 2000  No answer            2\n## 2 Other 2000  Never married      103\n## 3 Other 2000  Separated           16\n## 4 Other 2000  Divorced            17\n## 5 Other 2000  Widowed             24\n## 6 Other 2000  Married            122\n## \n## [[2]]\n## # A tibble: 5 x 4\n##   race  year  marital       total_tv\n##   &lt;fct&gt; &lt;chr&gt; &lt;fct&gt;            &lt;int&gt;\n## 1 Black 2000  Never married      452\n## 2 Black 2000  Separated          135\n## 3 Black 2000  Divorced           156\n## 4 Black 2000  Widowed            183\n## 5 Black 2000  Married            320\n\nWhy do this? To use janitor::adorn_totals(), which adds row-wise totals to a data frame, or to each data frame if a list of data frames gets passed to it. I need to still transform the data a little bit. After using adorn_totals(), I bind my list of data frames together, and then fill down the year column (when using adorn_totals(), character columns like year are filled with \"-\", but I chose to fill it with NA_character_). I then replace the NA value from the marital column with the string \"Total\" and then reorder the marital column by value of total_tv:\n\ntotal_tv_split &lt;- total_tv_split %&gt;%\n  adorn_totals(fill = NA_character_) %&gt;%\n  map(as.data.frame) %&gt;%  \n  bind_rows() %&gt;%\n  fill(year, .direction = \"down\") %&gt;%\n  mutate(marital = ifelse(is.na(marital), \"Total\", marital)) %&gt;%\n  mutate(marital = fct_reorder(marital, total_tv))\n\nI can finally create my plot. Because I have added “Total” as a level in the marital column, it now appears seamlessly in the plot:\n\nggplot(total_tv_split) +\n  geom_col(aes(x = marital, y = total_tv, fill = race)) +\n  facet_wrap(facets = vars(year), nrow = 2) +\n  scale_fill_discrete_qualitative(palette = \"Dark 3\") +\n  scale_x_discrete(guide = guide_axis(n.dodge = 3)) +\n  brotools::theme_blog() \n\n\n\n\nTo finish this list of recipes, let’s do a pyramid plot now (inspiration from here:\n\ndata_pyramid &lt;- gss_cat %&gt;%\n  filter(year == \"2000\", marital %in% c(\"Married\", \"Never married\")) %&gt;%\n  group_by(race, marital, rincome) %&gt;%  \n  summarise(total_tv = sum(tvhours, na.rm = TRUE))\n\nggplot(data_pyramid, aes(x = rincome, y = total_tv, fill = marital)) +\n  geom_col(data = filter(data_pyramid, marital == \"Married\")) +\n  geom_col(data = filter(data_pyramid, marital == \"Never married\"), aes(y = total_tv * (-1))) +\n  facet_wrap(facets = vars(race), nrow = 1, scales = \"free_x\") +\n  coord_flip() +\n  scale_fill_discrete_qualitative(palette = \"Dark 3\") +\n  brotools::theme_blog() \n\n\n\n\nHappy Easter!"
  },
  {
    "objectID": "posts/2025-02-17-rstats-on-nix.html#heres-why",
    "href": "posts/2025-02-17-rstats-on-nix.html#heres-why",
    "title": "Why we forked nixpkgs",
    "section": "Here’s why",
    "text": "Here’s why\nnixpkgs is a GitHub repository that contains tens of thousands of Nix expressions used by the Nix package manager to install software. By default, the nix package manager will pull expressions from NixOS/nixpkgs, but when using {rix} our fork rstats-on-nix/nixpkgs is used instead.\nBecause forks can sometimes be a bit controversial, we decided a blog post was in order.\nFirst of all, let’s make something clear: this doesn’t mean that we don’t contribute to upstream anymore, quite the contrary. But Nix is first and foremost the package manager of a Linux distribution, NixOS, and as such, the way it does certain things only make sense in that context. For our needs, having a fork gives us more flexibility. Let me explain.\nAs you’ll know, if you’ve been using {rix} and thus Nix, it is possible to use a commit of the nixpkgs GitHub repository as the source for your packages. For example, the 6a9bda32519e710a0c0ab8ecfabe9307ab90ef0c commit of nixpkgs will provide {dplyr} version 1.1.4 while this commit 407f8825b321617a38b86a4d9be11fd76d513da2 will provide version 1.0.7.\nWhile it is technically possible for Nix to provide many versions of the same package (for example, you can install the latest Emacs by installing the emacs package, or Emacs 28 by installing emacs28) this ultimately depends on whether the maintainer wishes to do so, or whether it is practical. As you can imagine, with more than 20’000 CRAN and Bioconductor packages, that is not possible for us (by “us”, I mean the maintainers of the R ecosystem for Nix). So for a given nixpkgs commit, you won’t be able to easily install a specific version of {dplyr} that is not included in that particular nixpkgs commit. Instead, you can install it from source, and this is possible with {rix} by writing something like:\n\nrix(..., r_pkgs = \"dplyr@1.0.7\", ...)\n\nbut because this attempts to install the package from source, it can fail if that package needs Nix-specific fixes to work.\nAlso, it isn’t practical to update the whole of the R packages set on Nix every day: so while CRAN and Bioconductor get updates daily, the R packages set on Nix gets updated only around new releases of R. Again, this is a consequence of Nix being first and foremost the package manager of a Linux distribution with its own governance and way of doing things.\nThis is where the rstats-on-nix fork of nixpkgs is interesting: because it is a fork, we can afford to do things in a way that could not be possible or practical for upstream.\nThe first thing this fork allows us to do is offer a daily snapshot of CRAN. Every day, thanks to Github Actions, the R packages set gets updated, and the result commited to a dated branch. This has been going on since the 14th of December 2024 (see here). So when you set a date as in rix(date = \"2024-12-14\", ...) this the fork that is going to get used. But this doesn’t mean that we recommend you use any date from the rstats-on-nix/nixpkgs fork: instead, each Monday, another action uses this fork and tries to build a set of popular packages on Linux and macOS, and only if this succeeds is the date added through a PR to the list of available dates on {rix}!\nThe reason this is done like this is to manage another risk of the upstream nixpkgs. As you know, nixpkgs is huge, and though the utmost care is taken by contributors and the PR review process is very strict, it can happen that updating packages breaks other packages. For example recently RStudio was in a broken state due to an issue in one its dependencies, boost. This is not the fault of anyone in particular: it’s just that packages get updated and packages that depend on them should get updated as well: but if that doesn’t happen quickly enough, the nixpkgs maintainer faces a conundrum. Either he or she doesn’t update the package because it breaks others, but not updating a package could be a security vulnerability, or he or she updates the package, but now others, perhaps less critical packages are broken and need to be fixed, either by their upstream developers, or by the nixpkgs maintainer of said packages. In the case of RStudio a fix was proposed and promptly merged, but if you wanted to install RStudio during the time it took to fix it, you would have faced an error message, which isn’t great if all you want is use Nix shells as development environments.\nSo for us, having a fork allows us to backport these fixes and so if you try to install RStudio using the latest available date, which is \"2025-02-10\", it’s going to work, whereas if you tried to build it on that date using upstream nixpkgs you’d be facing an error!\nWe spent quite some time backporting fixes: we went back all the way to 2019. The way this works, is that we start by checking out a nixpkgs commit on selected dates, then we “update” the R packages set by using the Posit CRAN and Bioconductor daily snapshots. Then, we backport as many fixes as possible, and ensure that a selection of popular packages work on both x86-linux (which includes Windows, through WSL) and aarch64-darwin (the M-series of Macs). Then we commit everything to a dated branch of the rstats-on-nix/nixpkgs fork. You can check out all the available dates by running: rix::available_dates(). We’re pretty confindent that you should not face any issues when using Nix to build reproducible environments for R. However, should you face a problem, don’t hesitate to open an issue!\nWe have now packages and R versions working on Linux and macOS from March 2019 to now. See this repository that contains the scripts that allowed us to do it. Backporting fixes was especially important for Apple Silicon computers, as it took some time for this platform to work correctly on Nix. By backporting fixes, we can now provide olders versions of these packages for Apple Silicon as well!\nUsing this approach, our fork now contains many more versions of working R packages than upstream. {rix} will thus likely keep pointing towards our fork in the future, and not upstream anymore. This should provide a much better user experience. An issue with our fork though, is that by backporting fixes, we essentially create new Nix packages that are not included in upstream, and thus, these are not built by Hydra, Nix’s CI platform which builds binary packages. In practice this means that anyone using our fork will have to compile many packages from source. Now this is pretty bad, as building packages from source takes quite some time. But fear not, because thanks to Cachix we now also have a dedicated binary cache of packages that complements the default, public Nix cache! We provide instructions on how to use Cachix, it’s very easy, it’s just running 2 additional commands after installing Nix. Using Cachix speeds up the installation process of packages tremendously. I want to give my heartfelt thanks to Domen Kožar for sponsoring the cache!\nAnother thing we do with our fork is run an action every day at midnight, that monitors the health of the R packages set. Of course, we don’t build every CRAN package, merely a handful, but these are among the most popular or the most at-risk of being in a broken state. See here."
  },
  {
    "objectID": "posts/2025-02-17-rstats-on-nix.html#also-theres-a-new-rix-release-on-cran",
    "href": "posts/2025-02-17-rstats-on-nix.html#also-theres-a-new-rix-release-on-cran",
    "title": "Why we forked nixpkgs",
    "section": "Also, there’s a new rix release on CRAN",
    "text": "Also, there’s a new rix release on CRAN\n{rix} now handles remote packages that have remote dependencies (themselves with remote dependencies) much better thanks to code by Michael Heming.\nWe also spent quite some time making {rix} work better with IDEs and have also documented that in a new vignette. The difference with previous releases of {rix}, is that now when a user supplies an IDE name to the ide argument of the rix() function, that IDE will get installed by Nix, which was previously not the case. This only really affects VS Code, as before, setting ide = \"code\" would only add the {languageserver} server package to the list of R packages to install. That was confusing, because if ide = \"rstudio\", then RStudio would be installed. So we decided that if ide = \"some editor\", then that editor should be installed by Nix. The vignette linked above explains in great detail how you can configure your editor to work with Nix shells.\nIf you decide to give {rix} a try, please let us know how it goes!"
  },
  {
    "objectID": "posts/2018-11-21-lux_castle.html",
    "href": "posts/2018-11-21-lux_castle.html",
    "title": "The best way to visit Luxembourguish castles is doing data science + combinatorial optimization",
    "section": "",
    "text": "Inspired by David Schoch’s blog post, Traveling Beerdrinker Problem. Check out his blog, he has some amazing posts!"
  },
  {
    "objectID": "posts/2018-11-21-lux_castle.html#introduction",
    "href": "posts/2018-11-21-lux_castle.html#introduction",
    "title": "The best way to visit Luxembourguish castles is doing data science + combinatorial optimization",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nLuxembourg, as any proper European country, is full of castles. According to Wikipedia,\n\n\n“By some optimistic estimates, there are as many as 130 castles in Luxembourg but more realistically there are probably just over a hundred, although many of these could be considered large residences or manor houses rather than castles”.\n\n\nI see the editors are probably German or French, calling our castles manor houses! They only say that because Luxembourg is small, so our castles must be small too, right?\n\n\nBanter aside, with that many castles, what is the best way to visit them all? And by best way I mean shortest way. This is a classical Travelling salesman problem. To solve this, I need the following elements:\n\n\n\nA list of castles to visit, with their coordinates\n\n\nThe distances between these castles to each other\n\n\nA program to solve the TSP\n\n\n\nLet’s start by loading some packages:\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(rvest)\nlibrary(curl)\nlibrary(brotools)\nlibrary(RJSONIO)\nlibrary(TSP)\nlibrary(ggimage)\n\nFirst step; scrape the data."
  },
  {
    "objectID": "posts/2018-11-21-lux_castle.html#scraping-the-data-thats-the-data-science-part",
    "href": "posts/2018-11-21-lux_castle.html#scraping-the-data-thats-the-data-science-part",
    "title": "The best way to visit Luxembourguish castles is doing data science + combinatorial optimization",
    "section": "\nScraping the data (that’s the data science part)\n",
    "text": "Scraping the data (that’s the data science part)\n\n\nLet’s start by having a list of castles. For this, I go to the French Wikipedia page of Luxembourguish castles.\n\n\nThe Luxembourguish page is more exhaustive, but the names are in Luxembourguish, and I doubt that OpenStreetMap, which I’ll use to get the coordinates, understands Luxembourguish.\n\n\nThis list has around 50 castles, a reasonable amount of castles. Scraping the table is quite easy:\n\npage &lt;- read_html(\"https://fr.wikipedia.org/wiki/Liste_de_ch%C3%A2teaux_luxembourgeois\")\n\ncastles &lt;- page %&gt;%\n    html_node(\".wikitable\") %&gt;%\n    html_table(fill = TRUE) %&gt;%\n    select(Nom, Localité) %&gt;%\n    mutate(query = paste0(Nom, \", \", Localité))\n\nI also add a query column which concatenates the name of the castle (“Nom”) to where it is found (“Localité”). The query should be a better choice that simply the castle name to get the coordinates.\n\n\nNow, I need to add the coordinates to this data frame. For this, I use a function I found online that gets the coordinates from OpenStreetMap:\n\n## geocoding function using OSM Nominatim API\n## details: http://wiki.openstreetmap.org/wiki/Nominatim\n## made by: D.Kisler\n\n#https://datascienceplus.com/osm-nominatim-with-r-getting-locations-geo-coordinates-by-its-address/\n\nnominatim_osm &lt;- function(address = NULL){\n    if(suppressWarnings(is.null(address)))\n        return(data.frame())\n    tryCatch(\n        d &lt;- jsonlite::fromJSON(\n            gsub('\\\\@addr\\\\@', gsub('\\\\s+', '\\\\%20', address),\n                 'http://nominatim.openstreetmap.org/search/@addr@?format=json&addressdetails=0&limit=1')\n        ), error = function(c) return(data.frame())\n    )\n    if(length(d) == 0) return(data.frame())\n    return(data.frame(lon = as.numeric(d$lon), lat = as.numeric(d$lat)))\n}\n\nI can now easily add the coordinates by mapping the nominatim_osm() function to the query column I built before:\n\ncastles_osm &lt;- castles %&gt;%\n    mutate(geolocation = map(query, nominatim_osm))\n\nLet’s take a look at castles_osm:\n\nhead(castles_osm)\n##                            Nom    Localité\n## 1         Château d'Ansembourg  Ansembourg\n## 2 Nouveau Château d'Ansembourg  Ansembourg\n## 3             Château d'Aspelt      Aspelt\n## 4          Château de Beaufort    Beaufort\n## 5            Château de Beggen Dommeldange\n## 6       Château de Colmar-Berg Colmar-Berg\n##                                      query         geolocation\n## 1         Château d'Ansembourg, Ansembourg 6.046748, 49.700693\n## 2 Nouveau Château d'Ansembourg, Ansembourg   6.04760, 49.70085\n## 3                 Château d'Aspelt, Aspelt 6.222653, 49.524822\n## 4            Château de Beaufort, Beaufort 2.757293, 43.297466\n## 5           Château de Beggen, Dommeldange 6.137765, 49.643383\n## 6      Château de Colmar-Berg, Colmar-Berg 6.087944, 49.814687\n\nI now clean the data. There were several mistakes or castles that were not found, which I added manually. I did not notice these mistakes immediately, but when I computed the distances matrix I notices several inconsistencies; 0’s in positions other than the diagonal, as well as NAs. So I went back to the raw data and corrected what was wrong, this time by looking at Google Maps. Thankfully there were not that many mistakes. Below the whole workflow:\n\n# Little helper function to clean the lon and lat columns\nextract_numbers &lt;- function(string){\n    str_extract_all(string, \"\\\\d+\", simplify = TRUE) %&gt;%\n        paste0(collapse = \".\")\n}\n\ncastles &lt;- castles_osm %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Château de Wintrange\", \"6.3517223, 49.5021975\", geolocation)) %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Septfontaines, Rollingergrund\", \"6.1028634, 49.6257147\", geolocation)) %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Château de Septfontaines\", \"5.9617443, 49.7006292\", geolocation)) %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Château de Senningen\", \"6.2342581, 49.6464632\", geolocation)) %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Château de Schauwenburg\", \"6.0478341, 49.6110245\", geolocation)) %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Château de Schuttbourg\", \"5.8980951, 49.7878706\", geolocation)) %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Château de Meysembourg\", \"6.1864882, 49.7704348\", geolocation)) %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Château de Mamer\", \"6.0232432, 49.6262397\", geolocation)) %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Château de Born\", \"6.5125214, 49.7611168\", geolocation)) %&gt;%\n    # Found chateau de Betzdorf in Germany, not Luxembourg:\n    mutate(geolocation = ifelse(Nom == \"Château Betzdorf\", \"6.330278, 49.694167\", geolocation)) %&gt;%\n    # Found château de Clemency in France, not Luxembourg:\n    mutate(geolocation = ifelse(Nom == \"Château de Clemency\", \"5.874167, 49.598056\", geolocation)) %&gt;%\n    separate(geolocation, into = c(\"lon\", \"lat\"), sep = \",\") %&gt;%\n    filter(!is.na(lat)) %&gt;%\n    mutate(lon = map(lon, extract_numbers)) %&gt;%\n    mutate(lat = map(lat, extract_numbers)) %&gt;%\n    # Château de Beaufort found is in southern France, not the one in lux\n    # Château de Dudelange is wrong (same as Bettembourg)\n    # Château de Pétange is wrong (same as Differdange)\n    # Château d'Urspelt is wrong (same as Clervaux)\n    # Château d'Hesperange is wrong (same as Palais Grand-Ducal)\n    mutate(lon = ifelse(Nom == \"Château de Beaufort\", \"6.2865176\", lon),\n           lat = ifelse(Nom == \"Château de Beaufort\", \"49.8335306\", lat)) %&gt;%\n    mutate(lon = ifelse(Nom == \"Château Dudelange\", \"6.0578438\", lon),\n           lat = ifelse(Nom == \"Château Dudelange\", \"49.4905049\", lat)) %&gt;%\n    mutate(lon = ifelse(Nom == \"Château de Pétange\", \"6.105703\", lon),\n           lat = ifelse(Nom == \"Château de Pétange\", \"49.7704746\", lat)) %&gt;%\n    mutate(lon = ifelse(Nom == \"Château d' Urspelt\", \"6.043375\", lon),\n           lat = ifelse(Nom == \"Château d' Urspelt\", \"50.075342\", lat)) %&gt;%\n    mutate(lon = ifelse(Nom == \"Château d'Hesperange\", \"6.1524302\", lon),\n           lat = ifelse(Nom == \"Château d'Hesperange\", \"49.573071\", lat)) %&gt;%\n    mutate(latlon = paste0(lat, \",\", lon)) %&gt;%\n    mutate(lon = as.numeric(lon), lat = as.numeric(lat))\n\nIn the end, I have 48 castles, 2 of them were not found neither by OpenStreetMap nor Google Maps.\n\n\nNow I can get the distances matrix. For this, I opened an account at Graphhopper and used their Matrix API. When you open a free account, you get a standard account for free for two weeks, which was perfect for this little exercise.\n\n\nTo use the Matrix API you can make a call with curl from your terminal, like this:\n\ncurl \"https://graphhopper.com/api/1/matrix?point=49.932707,11.588051&point=50.241935,10.747375&point=50.118817,11.983337&type=json&vehicle=car&debug=true&out_array=weights&out_array=times&out_array=distances&key=[YOUR_KEY]\"\n\nTo use this from R, I use the {curl} package and the curl_download() function to download and write the output to disk.\n\n\nI built the url like this. First, the “points” part:\n\npoints &lt;- paste(castles$latlon, collapse = \"&point=\")\n\n\n\nClick if you want to see the “points” string\n\n\npoints\n## [1] \"49.70069265,6.04674779400653&point=49.7008533,6.04759957386294&point=49.5248216,6.2226525964455&point=49.8335306,6.2865176&point=49.64338295,6.1377647435619&point=49.8146867,6.08794389490417&point=49.5749356,5.9841033&point=49.5173197,6.09641390513718&point=49.8760687,6.22027097982788&point=49.694167,6.330278&point=49.7611168,6.5125214&point=49.70256665,6.21740997690437&point=49.905581,6.07950107769784&point=49.9127745,6.13764166375989&point=49.598056,5.874167&point=50.0544533,6.03028463135369&point=49.75943095,5.82586812555896&point=49.52132545,5.88917535225117&point=49.6345518,6.1386377&point=49.4905049,6.0578438&point=49.8600716,6.11163732377525&point=49.9110418,5.93440053120085&point=49.7475976,6.18681116161273&point=49.61092115,6.13288873913352&point=49.573071,6.1524302&point=49.71207855,6.05156617599082&point=49.6694157,5.9496767&point=49.7704143,6.18888954785334&point=49.6262397,6.0232432&point=49.7478579,6.10315847283333&point=49.7704348,6.1864882&point=49.6328906,6.25941956000154&point=49.7704746,6.105703&point=49.54325715,5.9262570638974&point=49.470114,6.3658507&point=49.719675,6.09334070925783&point=49.7878706,5.8980951&point=49.6110245,6.0478341&point=49.6464632,6.2342581&point=49.7006292,5.9617443&point=49.6257147,6.1028634&point=49.556964,6.380786&point=50.075342,6.043375&point=49.7682266,5.9803414&point=49.9348908,6.20279648757301&point=49.6604088,6.1337864&point=49.9664662,5.93854270968922&point=49.5021975,6.3517223\"\n\n\nThen, I added my key, and pasted these elements together to form the correct url:\n\nmy_key &lt;- \"my_key_was_here\"\n\nurl &lt;- paste0(\"https://graphhopper.com/api/1/matrix?point=\", points, \"&type=json&vehicle=car&debug=true&out_array=weights&out_array=times&out_array=distances&key=\", my_key)\n\nThen, I get the matrix like this:\n\ncastles_dist &lt;- \"distances_graphhopper.json\"\ncurl_download(url, castles_dist)\n\nLet’s take a look at the object:\n\ndistances &lt;- castles_dist$distances\n\n\n\nClick if you want to see the distance object\n\n\ndistances\n## [[1]]\n##  [1]     0    48 46364 38416 16619 20387 19617 31990 31423 46587 60894\n## [12] 19171 36961 30701 25734 52929 22843 42618 18138 40015 24860 39395\n## [23] 17163 18938 28107  2570 10882 16888 12302  9350 16599 32025 14369\n## [34] 40780 56004  6069 17602 16112 31552  8180 14523 49431 53199 13354\n## [45] 43769 15868 46237 53617\n## \n## [[2]]\n##  [1]    48     0 46412 38464 16667 20435 19665 32038 31471 46635 60942\n## [12] 19219 37009 30749 25781 52977 22890 42665 18186 40063 24908 39443\n## [23] 17211 18986 28155  2618 10930 16936 12350  9398 16647 32073 14417\n## [34] 40828 56052  6116 17650 16160 31599  8228 14571 49478 53247 13402\n## [45] 43817 15916 46285 53665\n## \n## [[3]]\n##  [1] 46900 46947     0 48698 30281 45548 30424 17056 56584 31187 52215\n## [12] 28799 62122 55862 39130 78090 66375 33585 23961 21009 50021 64556\n## [23] 35740 25853 10852 49283 43218 43052 37317 36283 42763 17250 39530\n## [34] 31748 14513 33919 60798 31885 22872 44629 37023 14605 78360 44602\n## [45] 68930 26320 71398 12126\n## \n## [[4]]\n##  [1] 38214 38261 48754     0 34949 23582 55661 48848 11274 29579 26880\n## [12] 25631 32853 20633 67818 46577 47882 65319 33355 58499 26540 40460\n## [23] 24359 38061 43577 37674 61661 21704 55760 29822 25030 36698 27956\n## [34] 63482 72862 32945 42596 50328 34302 42495 38740 46782 46847 35141\n## [45] 20752 33520 47302 56789\n## \n## [[5]]\n##  [1] 16494 16541 25311 35192     0 25375 19477 16687 36411 20939 42124\n## [12] 13107 41949 35689 27116 57917 44116 30670  1432 26337 29848 44383\n## [23] 18673  5767 11224 18877 20958 23922 15058 16110 23633 13255 19357\n## [34] 28833 40700 15310 30392 12024 12782 20050  7178 30661 58187 24429\n## [45] 48757  2511 51225 33346\n## \n## [[6]]\n##  [1] 18468 18516 43459 23632 29633     0 33496 43553 15352 45965 60272\n## [12] 23583 20890 14630 39612 36858 27623 60024 28268 53203  8789 21614\n## [23] 17217 32765 38282 17929 29164 13853 27119  8892 15798 31403  7026\n## [34] 58187 67567 13199 22337 27919 30929 22750 26330 48809 37128 14882\n## [45] 27698 21128 28456 51494\n## \n## [[7]]\n##  [1] 19645 19693 30022 55860 21434 35353     0 17740 46389 45070 59377\n## [12] 35962 51927 45668 10941 67895 36650 11975 16038 16675 39826 49570\n## [23] 42902 13747 19018 22029 13093 32857  7837 24321 32568 30508 29335\n## [34]  8659 39662 20998 33544  9053 30034 17958 19337 40306 68165 29296\n## [45] 58736 20683 59099 37275\n## \n## [[8]]\n##  [1] 33194 33242 17113 48244 16576 45094 16196     0 56130 37454 51761\n## [12] 28345 61668 55408 26667 77635 52670 21122 15199  8546 49567 64102\n## [23] 35286 12148 11167 35578 29512 42597 23612 35829 42308 22891 39076\n## [34] 19285 26753 30159 47093 12671 22418 30923 23317 27397 77906 44148\n## [45] 68476 25866 70944 24366\n## \n## [[9]]\n##  [1] 34049 34097 59040 11039 45215 18025 49077 59134     0 35131 34289\n## [12] 25588 18148  8956 55193 34944 47717 75605 43849 68785 11835 33390\n## [23] 19644 48347 53863 33510 44745 17010 42701 26274 22029 46984 22791\n## [34] 73768 83148 28780 42477 43501 46511 38331 41912 64390 35215 29033\n## [45] 11504 36710 37265 67075\n## \n## [[10]]\n##  [1] 40768 40815 31200 29561 26887 47108 44877 38064 35204     0 24550\n## [12] 11501 46805 40546 57034 62773 59493 54535 25521 47714 51581 66116\n## [23] 18215 27276 32792 40228 50876 23464 44975 37844 23175 16279 41090\n## [34] 52698 33741 35479 54253 39544 10472 52287 30906 22876 63043 46162\n## [45] 44908 19378 72959 30101\n## \n## [[11]]\n##  [1] 54812 54860 52014 26837 40931 61152 58921 52108 34149 24114     0\n## [12] 30135 56417 42752 71078 69452 73537 68579 39565 61758 65626 80160\n## [23] 33264 41320 46836 54272 64920 34961 59020 51888 38287 30323 55134\n## [34] 66742 46672 49523 68297 53588 33981 66331 44951 37998 69722 60206\n## [45] 39488 41924 87003 44227\n## \n## [[12]]\n##  [1] 19189 19237 28715 23758 13122 25545 35622 28809 25568 11495 27547\n## [12]     0 42119 35859 47779 58087 37930 45280 12217 38459 30018 44553\n## [23]  8427 18021 23537 18649 41621 13676 35721 16280 13387 16659 19527\n## [34] 43443 52822 13901 32690 30289 10679 28544 17602 34064 58357 24599\n## [45] 34791 11692 51395 36749\n## \n## [[13]]\n##  [1] 36813 36860 61804 32740 47978 20788 51841 61898 18594 46628 54032\n## [12] 41927     0 11355 57957 25913 31393 78369 46612 71548 10254 18035\n## [23] 33039 51110 56626 36273 47508 29674 45464 29037 31619 49747 26873\n## [34] 76531 85911 31544 28499 46264 49274 41094 44675 67153 26183 28043\n## [45] 22973 39473 21910 69838\n## \n## [[14]]\n##  [1] 30553 30601 55544 20685 41718 14528 45581 55638  9008 40368 42945\n## [12] 35668 11355     0 51697 26249 44221 72109 40353 65288  8339 26597\n## [23] 26779 44850 50367 30014 41249 23415 39204 22778 25360 43488 20613\n## [34] 70272 79652 25284 38981 40004 43014 34835 38415 60894 26519 25537\n## [45] 13328 33213 30473 63579\n## \n## [[15]]\n##  [1] 25606 25654 38728 67712 27136 41314 10938 26445 52350 56922 71229\n## [12] 37346 57888 51628     0 73856 27918 10658 28655 25381 45787 55480\n## [23] 42912 31851 30870 27990 14521 38818 16023 30282 38529 42360 35296\n## [34]  9692 48367 26996 35203 18968 41886 19386 25040 49012 74126 25555\n## [45] 64696 26385 63610 45980\n## \n## [[16]]\n##  [1]  52597  52644  77588  46234  63762  36572  67625  77681  34889  62412\n## [11]  69484  57711  25729  26142  73741      0  51707  94153  62396  87332\n## [21]  30382  29845  48823  66894  72410  52057  63292  45458  61248  44821\n## [31]  47403  65531  42657  92315 101695  47328  48813  62048  65058  56878\n## [41]  60459  82937   3927  48357  26809  55257  16900  85622\n## \n## [[17]]\n##  [1] 22742 22790 66383 47823 44393 27506 36726 52009 45012 59978 74285\n## [12] 37595 31384 44290 27751 49817     0 38346 45912 60034 38449 23624\n## [23] 32592 49107 48126 21474 20735 31486 33280 24718 31198 45415 24781\n## [34] 37380 76023 27212  9559 36224 44942 16608 42297 71343 51639 13101\n## [45] 57358 43642 33153 73636\n## \n## [[18]]\n##  [1] 43466 43513 32742 64592 35685 61442 12026 20459 72478 53802 68109\n## [12] 44693 78016 71756 10711 93983 38600     0 31781 19395 65915 80450\n## [23] 51634 28730 27749 45849 20641 58945 19674 52177 58656 39239 55424\n## [34]  6452 42381 40430 45885 20422 38766 25506 33588 43026 94253 53116\n## [45] 84824 34934 87292 39994\n## \n## [[19]]\n##  [1] 17092 17140 23308 33262  1432 30429 15982 15299 41465 25814 40121\n## [12] 12039 47003 40743 27715 62970 44714 29283     0 24950 34902 49437\n## [23] 17605  4379  9837 19476 21557 22854 15656 21164 22565 11252 24411\n## [34] 27446 39313 18800 37574 12622 10778 20648  5791 28657 63241 29483\n## [45] 53811  3497 56279 31342\n## \n## [[20]]\n##  [1] 40369 40417 21028 58519 26851 55368 16853  5415 66404 47729 62035\n## [12] 38620 71942 65683 25560 87910 59845 20015 25473     0 59842 74376\n## [23] 45561 22423 21442 42753 36687 52872 30786 46104 52583 33166 49351\n## [34] 18178 30668 37333 54268 25355 32693 38098 30492 31312 88180 50019\n## [45] 78751 31837 81219 28281\n## \n## [[21]]\n##  [1] 25435 25483 50426 23657 36601  9410 40463 50520  9511 52933 67240\n## [12] 30550 10254  8470 46579 30697 31384 66991 35235 60171     0 22368\n## [23] 19572 39732 45249 24896 36131 16208 34087 17660 18153 38370 15495\n## [34] 65154 74534 20166 26099 34887 37897 29717 33298 55776 30967 18643\n## [45] 21538 28096 29210 58461\n## \n## [[22]]\n##  [1] 39601 39649 64592 40786 50767 21730 50009 64686 34013 67099 81406\n## [12] 44716 18356 26775 55533 27246 27163 81157 49401 74337 22735     0\n## [23] 35431 53898 59415 39062 38903 32067 42694 31826 34012 52536 26191\n## [34] 79320 88700 34332 24269 49053 52063 31939 47464 69942 29069 20615\n## [45] 35072 42262 10583 72627\n## \n## [[23]]\n##  [1] 17968 18016 35386 24320 18687 19877 42293 35480 19624 18209 33230\n## [12]  8427 33133 26873 42921 49101 32654 51951 17782 45130 21032 35141\n## [23]     0 21756 30208 17139 28664  5714 26043  8833  5426 23330 10461\n## [34] 50114 59493 12123 27413 26843 20737 23268 23168 40735 49371 19323\n## [45] 29874 17258 41983 43420\n## \n## [[24]]\n##  [1] 17512 17560 15422 36059  4428 33226 13943 13260 44262 27280 41587\n## [12] 14836 49800 43540 24703 65768 41703 27243  3050 22910 37699 52234\n## [23] 20402     0  7797 19895 18546 25651 12645 23961 25362 12718 27208\n## [34] 25406 37273 16329 31410  8506 12244 19956  6213 26147 66038 32280\n## [45] 56608  6317 59076 34886\n## \n## [[25]]\n##  [1] 27872 27920 10877 44424 10352 41273 18191 11042 52309 33634 47940\n## [12] 24525 57847 51588 30348 73815 47348 27849  8975 20692 45747 60281\n## [23] 31466  5924     0 30256 24190 38777 18290 32009 38488 19071 35255\n## [34] 26012 35056 24837 41771 12858 18598 25601 17995 21917 74085 40327\n## [45] 64656 19340 67124 21432\n## \n## [[26]]\n##  [1]  2570  2618 48748 37876 19003 19847 22001 34374 30884 46048 60355\n## [12] 18632 36421 30162 28117 52389 21574 45001 28350 42399 24321 32360\n## [23] 17165 21322 30491     0 13266 16059 14686  8521 15770 31485 13830\n## [34] 43164 58387  5529 16334 18496 31012 10564 16906 48891 52659 12086\n## [45] 43230 18252 45698 56000\n## \n## [[27]]\n##  [1] 10882 10930 43104 61689 21113 31083 13102 28730 42119 50899 65206\n## [12] 31324 47657 41397 14188 63624 20735 20742 22632 36755 35556 38436\n## [23] 27859 25828 24847 13266     0 27584 10000 20046 27295 36337 25065\n## [34] 19286 52743 16764 22410 12945 35863  6736 19017 48063 63894 18161\n## [45] 54465 20362 47965 50356\n## \n## [[28]]\n##  [1] 16863 16910 42857 21661 23936 13872 32894 42951 16990 23458 34927\n## [12] 13676 29769 23509 39010 45736 31548 59422 23031 52601 17668 31777\n## [23]  5714 27005 37680 16033 27558     0 26517  7728   640 30801  9331\n## [34] 57585 66965 12597 26308 27318 25985 22162 25728 48207 46006 18217\n## [45] 26509 22507 38619 50892\n## \n## [[29]]\n##  [1] 12254 12302 36928 55514 14937 28982  7761 22554 40018 44724 59031\n## [12] 25148 45556 39296 16297 61524 33297 19546 16457 30579 33455 42179\n## [23] 25758 19652 18671 14638 10140 26486     0 17950 26197 30161 22964\n## [34] 16230 46568 14086 26153  3631 29688 11548 12841 41888 61794 21904\n## [45] 52364 14186 51708 44180\n## \n## [[30]]\n##  [1]  9350  9398 36159 28322 22333 12073 24364 36253 23110 38665 52972\n## [12] 16282 28647 22388 30480 44615 24850 52724 20968 45903 16547 31081\n## [23]  8859 25465 30981  8521 20046  7753 17987     0  7465 24103  5802\n## [34] 50887 60266  4067 19610 18787 23629 17344 17198 41508 44885 11519\n## [45] 35456 13828 37924 44193\n## \n## [[31]]\n##  [1] 16574 16622 42568 24986 23647 18483 32605 42662 21985 23169 38253\n## [12] 13387 31714 25454 38721 47681 31259 59133 22742 52312 19613 33722\n## [23]  5426 26716 37391 15744 27269   640 26229  7439     0 30512  9042\n## [34] 57296 66676 12308 26019 27029 25696 21873 25440 47918 47951 17928\n## [45] 28454 22218 40564 50603\n## \n## [[32]]\n##  [1] 26503 26551 16931 35994 12622 32844 30612 23800 43880 15938 30245\n## [12] 16095 49418 43158 42770 65386 45229 40271 11257 33450 37317 51852\n## [23] 23036 13012 18528 25964 36612 30348 30711 23579 30059     0 26826\n## [34] 38433 28491 21215 39989 25280  5038 38023 16642 16486 65656 31898\n## [45] 56226 13616 58694 22401\n## \n## [[33]]\n##  [1] 14182 14230 39173 27991 25347  7318 29210 39267 22746 41679 55986\n## [12] 19297 27681 21422 35326 43649 24862 55738 23982 48917 15581 26003\n## [23] 10461 28479 33996 13643 24878  9331 22833  5790  9042 27117     0\n## [34] 53901 63281  8913 19576 23633 26643 18464 22044 44523 43919 12121\n## [45] 29215 16842 35531 47208\n## \n## [[34]]\n##  [1] 28166 28214 31292 63142 34235 59992  8710 19009 71028 52352 66659\n## [12] 43243 76566 70306  9382 92533 36885  6378 30331 17945 64465 64447\n## [23] 50184 27280 26299 30550 18965 57495 16357 50727 57206 37789 53974\n## [34]     0 40931 38980 44170 17105 37316 23829 32138 41576 92803 34921\n## [45] 83374 33484 85842 38544\n## \n## [[35]]\n##  [1]  56656  56703  14836  71705  40037  68555  40180  26812  79591  33969\n## [11]  46710  51806  85129  78869  48886 101097  76131  43341  38660  30765\n## [21]  73028  87563  58747  35609  34628  59039  52974  66059  47073  59291\n## [31]  65770  28534  62537  41504      0  53620  70554  41641  45879  54384\n## [41]  46778  10804 101367  67609  91937  49327  94405   4470\n## \n## [[36]]\n##  [1]  6069  6116 38793 33128 15430 15099 20926 29765 26135 41299 55606\n## [12] 13883 31673 25413 27213 47641 27484 40392 23602 37789 19572 34107\n## [23] 11875 28099 33615  5529 16764 12602 14123  4067 12314 26737  9081\n## [34] 38555 53778     0 22244 14923 26263 14062 13334 44142 47911 14153\n## [45] 38481 11296 40949 46827\n## \n## [[37]]\n##  [1] 17599 17647 60259 42618 30514 22302 33512 45885 39869 54835 69142\n## [12] 32452 28491 39147 35036 46923  9559 56513 37137 53910 33306 20731\n## [23] 27449 41635 42002 16331 22406 26343 26197 19575 26054 40272 19576\n## [34] 54675 69899 22068     0 30007 39799 15442 28418 57678 48746  7897\n## [45] 52215 29998 30260 67512\n## \n## [[38]]\n##  [1] 16015 16063 31108 49694 12129 29728  8933 12719 40764 38904 53211\n## [12] 29795 46302 40042 19294 62270 36294 27361 13648 24759 34201 48736\n## [23] 26504  8314 12851 18398 13137 27231  3723 18696 26943 24341 23710\n## [34] 17000 40748 14832 29913     0 23868 14545 10032 36068 62540 25665\n## [45] 53110 11378 55578 38360\n## \n## [[39]]\n##  [1] 25665 25713 22868 35156 11784 32006 29774 22961 43042 10466 34023\n## [12] 15257 48580 42320 41932 64548 44391 39433 10419 32612 36479 51014\n## [23] 22198 12174 17690 25126 35774 29510 29873 22741 29221  5081 25988\n## [34] 37595 46975 20377 39151 24442     0 37185 15804 21298 64818 31060\n## [45] 55388 12778 57856 30902\n## \n## [[40]]\n##  [1]  8180  8228 44691 42719 20175 24690 17967 30317 35726 52486 66793\n## [12] 28309 41264 35005 19052 57232 16608 25606 21695 38341 29164 31472\n## [23] 23306 27415 26433 10564  6736 22201 11607 17344 21912 37924 18672\n## [34] 24151 54330 14062 15446 14551 37450     0 18079 49650 57502 11198\n## [45] 48073 19424 41001 51943\n## \n## [[41]]\n##  [1] 14500 14548 37450 38853  8571 28213 17483 23076 39249 31405 45712\n## [12] 17630 44787 38527 25122 60755 42122 33703  5844 31101 32686 47221\n## [23] 23196  6938 19193 16883 18965 25716 13064 17181 25428 16842 22195\n## [34] 31866 47090 13317 28398 10030 16369 18056     0 34248 61025 24150\n## [45] 51595  7820 54063 44703\n## \n## [[42]]\n##  [1] 44607 44655 14655 46763 30726 50948 40494 27126 61984 22848 38359\n## [12] 34199 67522 61262 49200 83489 76445 43655 29360 31079 55421 69956\n## [23] 41140 24844 22014 44067 53288 48451 47387 41683 48162 16486 44930\n## [34] 41818 10865 39319 58092 41955 21254 54699 47093     0 83759 50002\n## [45] 74330 31720 76798  7225\n## \n## [[43]]\n##  [1]  53049  53096  78040  46686  64214  37024  68076  78133  35341  62864\n## [11]  69936  58163  26181  26593  74193   3927  51887  94604  62848  87784\n## [21]  30834  37251  49275  67346  72862  52509  63744  45910  61700  45273\n## [31]  47855  65983  43109  92767 102147  47780  48993  62500  65510  57330\n## [41]  60911  83389      0  48032  27261  55709  18722  86074\n## \n## [[44]]\n##  [1] 13481 13529 44241 35163 30415 14847 29395 44335 31781 46747 61054\n## [12] 24364 28035 31060 30271 46467 13182 52395 29050 49792 25219 20275\n## [23] 19361 33547 39063 12213 18288 18256 22079 11488 17967 32185 12121\n## [34] 50557 68348 13981  7897 25889 31711 11325 24300 49590 53557     0\n## [45] 44128 21910 29803 52275\n## \n## [[45]]\n##  [1] 40446 40494 65437 20517 51611 24421 55474 65531 11556 44883 39488\n## [12] 34762 24423 13328 61590 26917 54113 82002 50246 75181 18231 34751\n## [23] 29845 54743 60259 39906 51142 26480 49097 32670 28426 53381 29188\n## [34] 80164 89544 35177 48873 49897 52907 44727 48308 70786 27187 35430\n## [45]     0 43106 33848 73471\n## \n## [[46]]\n##  [1] 15720 15767 25996 33807  2532 23092 18703 24296 34128 19554 42809\n## [12] 11722 39666 33406 26342 55633 43342 34923  4080 32321 27565 42100\n## [23] 17288  7632 20818 18103 20185 22537 14284 13827 22248 13940 17074\n## [34] 33086 48309 11317 30237 11250 11347 19276  7865 31345 55903 22146\n## [45] 46474     0 48942 34030\n## \n## [[47]]\n##  [1] 48326 48373 73317 53314 59491 30454 61401 73410 39547 75823 76564\n## [12] 53440 23890 33221 63609 16972 35356 74204 58125 83061 30123 12418\n## [23] 44155 62623 68139 47786 50295 40791 54086 40550 42736 61260 37582\n## [34] 73238 97424 43057 32463 57777 60787 43331 56188 78666 18794 32007\n## [45] 33889 50986     0 81351\n## \n## [[48]]\n##  [1] 53987 54035 12168 56733 37369 53583 37512 24144 64619 31546 44287\n## [12] 36834 70157 63897 46218 86125 73463 40673 35992 28097 58056 72591\n## [23] 43775 32941 31960 56371 50305 51087 44405 44319 50798 22413 47565\n## [34] 38836  4459 50952 67886 38973 30907 51716 44110  7225 86395 52637\n## [45] 76965 34355 79433     0\n\n\ndistances is a list where the first element is the distances from the first castle to all the others. Let’s make it a matrix:\n\ndistances_matrix &lt;- distances %&gt;%\n    reduce(rbind)\n\n\n\nClick if you want to see the distance matrix\n\n\ndistances_matrix\n##      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11]\n## out     0    48 46364 38416 16619 20387 19617 31990 31423 46587 60894\n##        48     0 46412 38464 16667 20435 19665 32038 31471 46635 60942\n##     46900 46947     0 48698 30281 45548 30424 17056 56584 31187 52215\n##     38214 38261 48754     0 34949 23582 55661 48848 11274 29579 26880\n##     16494 16541 25311 35192     0 25375 19477 16687 36411 20939 42124\n##     18468 18516 43459 23632 29633     0 33496 43553 15352 45965 60272\n##     19645 19693 30022 55860 21434 35353     0 17740 46389 45070 59377\n##     33194 33242 17113 48244 16576 45094 16196     0 56130 37454 51761\n##     34049 34097 59040 11039 45215 18025 49077 59134     0 35131 34289\n##     40768 40815 31200 29561 26887 47108 44877 38064 35204     0 24550\n##     54812 54860 52014 26837 40931 61152 58921 52108 34149 24114     0\n##     19189 19237 28715 23758 13122 25545 35622 28809 25568 11495 27547\n##     36813 36860 61804 32740 47978 20788 51841 61898 18594 46628 54032\n##     30553 30601 55544 20685 41718 14528 45581 55638  9008 40368 42945\n##     25606 25654 38728 67712 27136 41314 10938 26445 52350 56922 71229\n##     52597 52644 77588 46234 63762 36572 67625 77681 34889 62412 69484\n##     22742 22790 66383 47823 44393 27506 36726 52009 45012 59978 74285\n##     43466 43513 32742 64592 35685 61442 12026 20459 72478 53802 68109\n##     17092 17140 23308 33262  1432 30429 15982 15299 41465 25814 40121\n##     40369 40417 21028 58519 26851 55368 16853  5415 66404 47729 62035\n##     25435 25483 50426 23657 36601  9410 40463 50520  9511 52933 67240\n##     39601 39649 64592 40786 50767 21730 50009 64686 34013 67099 81406\n##     17968 18016 35386 24320 18687 19877 42293 35480 19624 18209 33230\n##     17512 17560 15422 36059  4428 33226 13943 13260 44262 27280 41587\n##     27872 27920 10877 44424 10352 41273 18191 11042 52309 33634 47940\n##      2570  2618 48748 37876 19003 19847 22001 34374 30884 46048 60355\n##     10882 10930 43104 61689 21113 31083 13102 28730 42119 50899 65206\n##     16863 16910 42857 21661 23936 13872 32894 42951 16990 23458 34927\n##     12254 12302 36928 55514 14937 28982  7761 22554 40018 44724 59031\n##      9350  9398 36159 28322 22333 12073 24364 36253 23110 38665 52972\n##     16574 16622 42568 24986 23647 18483 32605 42662 21985 23169 38253\n##     26503 26551 16931 35994 12622 32844 30612 23800 43880 15938 30245\n##     14182 14230 39173 27991 25347  7318 29210 39267 22746 41679 55986\n##     28166 28214 31292 63142 34235 59992  8710 19009 71028 52352 66659\n##     56656 56703 14836 71705 40037 68555 40180 26812 79591 33969 46710\n##      6069  6116 38793 33128 15430 15099 20926 29765 26135 41299 55606\n##     17599 17647 60259 42618 30514 22302 33512 45885 39869 54835 69142\n##     16015 16063 31108 49694 12129 29728  8933 12719 40764 38904 53211\n##     25665 25713 22868 35156 11784 32006 29774 22961 43042 10466 34023\n##      8180  8228 44691 42719 20175 24690 17967 30317 35726 52486 66793\n##     14500 14548 37450 38853  8571 28213 17483 23076 39249 31405 45712\n##     44607 44655 14655 46763 30726 50948 40494 27126 61984 22848 38359\n##     53049 53096 78040 46686 64214 37024 68076 78133 35341 62864 69936\n##     13481 13529 44241 35163 30415 14847 29395 44335 31781 46747 61054\n##     40446 40494 65437 20517 51611 24421 55474 65531 11556 44883 39488\n##     15720 15767 25996 33807  2532 23092 18703 24296 34128 19554 42809\n##     48326 48373 73317 53314 59491 30454 61401 73410 39547 75823 76564\n##     53987 54035 12168 56733 37369 53583 37512 24144 64619 31546 44287\n##     [,12] [,13] [,14] [,15]  [,16] [,17] [,18] [,19] [,20] [,21] [,22]\n## out 19171 36961 30701 25734  52929 22843 42618 18138 40015 24860 39395\n##     19219 37009 30749 25781  52977 22890 42665 18186 40063 24908 39443\n##     28799 62122 55862 39130  78090 66375 33585 23961 21009 50021 64556\n##     25631 32853 20633 67818  46577 47882 65319 33355 58499 26540 40460\n##     13107 41949 35689 27116  57917 44116 30670  1432 26337 29848 44383\n##     23583 20890 14630 39612  36858 27623 60024 28268 53203  8789 21614\n##     35962 51927 45668 10941  67895 36650 11975 16038 16675 39826 49570\n##     28345 61668 55408 26667  77635 52670 21122 15199  8546 49567 64102\n##     25588 18148  8956 55193  34944 47717 75605 43849 68785 11835 33390\n##     11501 46805 40546 57034  62773 59493 54535 25521 47714 51581 66116\n##     30135 56417 42752 71078  69452 73537 68579 39565 61758 65626 80160\n##         0 42119 35859 47779  58087 37930 45280 12217 38459 30018 44553\n##     41927     0 11355 57957  25913 31393 78369 46612 71548 10254 18035\n##     35668 11355     0 51697  26249 44221 72109 40353 65288  8339 26597\n##     37346 57888 51628     0  73856 27918 10658 28655 25381 45787 55480\n##     57711 25729 26142 73741      0 51707 94153 62396 87332 30382 29845\n##     37595 31384 44290 27751  49817     0 38346 45912 60034 38449 23624\n##     44693 78016 71756 10711  93983 38600     0 31781 19395 65915 80450\n##     12039 47003 40743 27715  62970 44714 29283     0 24950 34902 49437\n##     38620 71942 65683 25560  87910 59845 20015 25473     0 59842 74376\n##     30550 10254  8470 46579  30697 31384 66991 35235 60171     0 22368\n##     44716 18356 26775 55533  27246 27163 81157 49401 74337 22735     0\n##      8427 33133 26873 42921  49101 32654 51951 17782 45130 21032 35141\n##     14836 49800 43540 24703  65768 41703 27243  3050 22910 37699 52234\n##     24525 57847 51588 30348  73815 47348 27849  8975 20692 45747 60281\n##     18632 36421 30162 28117  52389 21574 45001 28350 42399 24321 32360\n##     31324 47657 41397 14188  63624 20735 20742 22632 36755 35556 38436\n##     13676 29769 23509 39010  45736 31548 59422 23031 52601 17668 31777\n##     25148 45556 39296 16297  61524 33297 19546 16457 30579 33455 42179\n##     16282 28647 22388 30480  44615 24850 52724 20968 45903 16547 31081\n##     13387 31714 25454 38721  47681 31259 59133 22742 52312 19613 33722\n##     16095 49418 43158 42770  65386 45229 40271 11257 33450 37317 51852\n##     19297 27681 21422 35326  43649 24862 55738 23982 48917 15581 26003\n##     43243 76566 70306  9382  92533 36885  6378 30331 17945 64465 64447\n##     51806 85129 78869 48886 101097 76131 43341 38660 30765 73028 87563\n##     13883 31673 25413 27213  47641 27484 40392 23602 37789 19572 34107\n##     32452 28491 39147 35036  46923  9559 56513 37137 53910 33306 20731\n##     29795 46302 40042 19294  62270 36294 27361 13648 24759 34201 48736\n##     15257 48580 42320 41932  64548 44391 39433 10419 32612 36479 51014\n##     28309 41264 35005 19052  57232 16608 25606 21695 38341 29164 31472\n##     17630 44787 38527 25122  60755 42122 33703  5844 31101 32686 47221\n##     34199 67522 61262 49200  83489 76445 43655 29360 31079 55421 69956\n##     58163 26181 26593 74193   3927 51887 94604 62848 87784 30834 37251\n##     24364 28035 31060 30271  46467 13182 52395 29050 49792 25219 20275\n##     34762 24423 13328 61590  26917 54113 82002 50246 75181 18231 34751\n##     11722 39666 33406 26342  55633 43342 34923  4080 32321 27565 42100\n##     53440 23890 33221 63609  16972 35356 74204 58125 83061 30123 12418\n##     36834 70157 63897 46218  86125 73463 40673 35992 28097 58056 72591\n##     [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33]\n## out 17163 18938 28107  2570 10882 16888 12302  9350 16599 32025 14369\n##     17211 18986 28155  2618 10930 16936 12350  9398 16647 32073 14417\n##     35740 25853 10852 49283 43218 43052 37317 36283 42763 17250 39530\n##     24359 38061 43577 37674 61661 21704 55760 29822 25030 36698 27956\n##     18673  5767 11224 18877 20958 23922 15058 16110 23633 13255 19357\n##     17217 32765 38282 17929 29164 13853 27119  8892 15798 31403  7026\n##     42902 13747 19018 22029 13093 32857  7837 24321 32568 30508 29335\n##     35286 12148 11167 35578 29512 42597 23612 35829 42308 22891 39076\n##     19644 48347 53863 33510 44745 17010 42701 26274 22029 46984 22791\n##     18215 27276 32792 40228 50876 23464 44975 37844 23175 16279 41090\n##     33264 41320 46836 54272 64920 34961 59020 51888 38287 30323 55134\n##      8427 18021 23537 18649 41621 13676 35721 16280 13387 16659 19527\n##     33039 51110 56626 36273 47508 29674 45464 29037 31619 49747 26873\n##     26779 44850 50367 30014 41249 23415 39204 22778 25360 43488 20613\n##     42912 31851 30870 27990 14521 38818 16023 30282 38529 42360 35296\n##     48823 66894 72410 52057 63292 45458 61248 44821 47403 65531 42657\n##     32592 49107 48126 21474 20735 31486 33280 24718 31198 45415 24781\n##     51634 28730 27749 45849 20641 58945 19674 52177 58656 39239 55424\n##     17605  4379  9837 19476 21557 22854 15656 21164 22565 11252 24411\n##     45561 22423 21442 42753 36687 52872 30786 46104 52583 33166 49351\n##     19572 39732 45249 24896 36131 16208 34087 17660 18153 38370 15495\n##     35431 53898 59415 39062 38903 32067 42694 31826 34012 52536 26191\n##         0 21756 30208 17139 28664  5714 26043  8833  5426 23330 10461\n##     20402     0  7797 19895 18546 25651 12645 23961 25362 12718 27208\n##     31466  5924     0 30256 24190 38777 18290 32009 38488 19071 35255\n##     17165 21322 30491     0 13266 16059 14686  8521 15770 31485 13830\n##     27859 25828 24847 13266     0 27584 10000 20046 27295 36337 25065\n##      5714 27005 37680 16033 27558     0 26517  7728   640 30801  9331\n##     25758 19652 18671 14638 10140 26486     0 17950 26197 30161 22964\n##      8859 25465 30981  8521 20046  7753 17987     0  7465 24103  5802\n##      5426 26716 37391 15744 27269   640 26229  7439     0 30512  9042\n##     23036 13012 18528 25964 36612 30348 30711 23579 30059     0 26826\n##     10461 28479 33996 13643 24878  9331 22833  5790  9042 27117     0\n##     50184 27280 26299 30550 18965 57495 16357 50727 57206 37789 53974\n##     58747 35609 34628 59039 52974 66059 47073 59291 65770 28534 62537\n##     11875 28099 33615  5529 16764 12602 14123  4067 12314 26737  9081\n##     27449 41635 42002 16331 22406 26343 26197 19575 26054 40272 19576\n##     26504  8314 12851 18398 13137 27231  3723 18696 26943 24341 23710\n##     22198 12174 17690 25126 35774 29510 29873 22741 29221  5081 25988\n##     23306 27415 26433 10564  6736 22201 11607 17344 21912 37924 18672\n##     23196  6938 19193 16883 18965 25716 13064 17181 25428 16842 22195\n##     41140 24844 22014 44067 53288 48451 47387 41683 48162 16486 44930\n##     49275 67346 72862 52509 63744 45910 61700 45273 47855 65983 43109\n##     19361 33547 39063 12213 18288 18256 22079 11488 17967 32185 12121\n##     29845 54743 60259 39906 51142 26480 49097 32670 28426 53381 29188\n##     17288  7632 20818 18103 20185 22537 14284 13827 22248 13940 17074\n##     44155 62623 68139 47786 50295 40791 54086 40550 42736 61260 37582\n##     43775 32941 31960 56371 50305 51087 44405 44319 50798 22413 47565\n##     [,34]  [,35] [,36] [,37] [,38] [,39] [,40] [,41] [,42]  [,43] [,44]\n## out 40780  56004  6069 17602 16112 31552  8180 14523 49431  53199 13354\n##     40828  56052  6116 17650 16160 31599  8228 14571 49478  53247 13402\n##     31748  14513 33919 60798 31885 22872 44629 37023 14605  78360 44602\n##     63482  72862 32945 42596 50328 34302 42495 38740 46782  46847 35141\n##     28833  40700 15310 30392 12024 12782 20050  7178 30661  58187 24429\n##     58187  67567 13199 22337 27919 30929 22750 26330 48809  37128 14882\n##      8659  39662 20998 33544  9053 30034 17958 19337 40306  68165 29296\n##     19285  26753 30159 47093 12671 22418 30923 23317 27397  77906 44148\n##     73768  83148 28780 42477 43501 46511 38331 41912 64390  35215 29033\n##     52698  33741 35479 54253 39544 10472 52287 30906 22876  63043 46162\n##     66742  46672 49523 68297 53588 33981 66331 44951 37998  69722 60206\n##     43443  52822 13901 32690 30289 10679 28544 17602 34064  58357 24599\n##     76531  85911 31544 28499 46264 49274 41094 44675 67153  26183 28043\n##     70272  79652 25284 38981 40004 43014 34835 38415 60894  26519 25537\n##      9692  48367 26996 35203 18968 41886 19386 25040 49012  74126 25555\n##     92315 101695 47328 48813 62048 65058 56878 60459 82937   3927 48357\n##     37380  76023 27212  9559 36224 44942 16608 42297 71343  51639 13101\n##      6452  42381 40430 45885 20422 38766 25506 33588 43026  94253 53116\n##     27446  39313 18800 37574 12622 10778 20648  5791 28657  63241 29483\n##     18178  30668 37333 54268 25355 32693 38098 30492 31312  88180 50019\n##     65154  74534 20166 26099 34887 37897 29717 33298 55776  30967 18643\n##     79320  88700 34332 24269 49053 52063 31939 47464 69942  29069 20615\n##     50114  59493 12123 27413 26843 20737 23268 23168 40735  49371 19323\n##     25406  37273 16329 31410  8506 12244 19956  6213 26147  66038 32280\n##     26012  35056 24837 41771 12858 18598 25601 17995 21917  74085 40327\n##     43164  58387  5529 16334 18496 31012 10564 16906 48891  52659 12086\n##     19286  52743 16764 22410 12945 35863  6736 19017 48063  63894 18161\n##     57585  66965 12597 26308 27318 25985 22162 25728 48207  46006 18217\n##     16230  46568 14086 26153  3631 29688 11548 12841 41888  61794 21904\n##     50887  60266  4067 19610 18787 23629 17344 17198 41508  44885 11519\n##     57296  66676 12308 26019 27029 25696 21873 25440 47918  47951 17928\n##     38433  28491 21215 39989 25280  5038 38023 16642 16486  65656 31898\n##     53901  63281  8913 19576 23633 26643 18464 22044 44523  43919 12121\n##         0  40931 38980 44170 17105 37316 23829 32138 41576  92803 34921\n##     41504      0 53620 70554 41641 45879 54384 46778 10804 101367 67609\n##     38555  53778     0 22244 14923 26263 14062 13334 44142  47911 14153\n##     54675  69899 22068     0 30007 39799 15442 28418 57678  48746  7897\n##     17000  40748 14832 29913     0 23868 14545 10032 36068  62540 25665\n##     37595  46975 20377 39151 24442     0 37185 15804 21298  64818 31060\n##     24151  54330 14062 15446 14551 37450     0 18079 49650  57502 11198\n##     31866  47090 13317 28398 10030 16369 18056     0 34248  61025 24150\n##     41818  10865 39319 58092 41955 21254 54699 47093     0  83759 50002\n##     92767 102147 47780 48993 62500 65510 57330 60911 83389      0 48032\n##     50557  68348 13981  7897 25889 31711 11325 24300 49590  53557     0\n##     80164  89544 35177 48873 49897 52907 44727 48308 70786  27187 35430\n##     33086  48309 11317 30237 11250 11347 19276  7865 31345  55903 22146\n##     73238  97424 43057 32463 57777 60787 43331 56188 78666  18794 32007\n##     38836   4459 50952 67886 38973 30907 51716 44110  7225  86395 52637\n##     [,45] [,46] [,47] [,48]\n## out 43769 15868 46237 53617\n##     43817 15916 46285 53665\n##     68930 26320 71398 12126\n##     20752 33520 47302 56789\n##     48757  2511 51225 33346\n##     27698 21128 28456 51494\n##     58736 20683 59099 37275\n##     68476 25866 70944 24366\n##     11504 36710 37265 67075\n##     44908 19378 72959 30101\n##     39488 41924 87003 44227\n##     34791 11692 51395 36749\n##     22973 39473 21910 69838\n##     13328 33213 30473 63579\n##     64696 26385 63610 45980\n##     26809 55257 16900 85622\n##     57358 43642 33153 73636\n##     84824 34934 87292 39994\n##     53811  3497 56279 31342\n##     78751 31837 81219 28281\n##     21538 28096 29210 58461\n##     35072 42262 10583 72627\n##     29874 17258 41983 43420\n##     56608  6317 59076 34886\n##     64656 19340 67124 21432\n##     43230 18252 45698 56000\n##     54465 20362 47965 50356\n##     26509 22507 38619 50892\n##     52364 14186 51708 44180\n##     35456 13828 37924 44193\n##     28454 22218 40564 50603\n##     56226 13616 58694 22401\n##     29215 16842 35531 47208\n##     83374 33484 85842 38544\n##     91937 49327 94405  4470\n##     38481 11296 40949 46827\n##     52215 29998 30260 67512\n##     53110 11378 55578 38360\n##     55388 12778 57856 30902\n##     48073 19424 41001 51943\n##     51595  7820 54063 44703\n##     74330 31720 76798  7225\n##     27261 55709 18722 86074\n##     44128 21910 29803 52275\n##         0 43106 33848 73471\n##     46474     0 48942 34030\n##     33889 50986     0 81351\n##     76965 34355 79433     0\n\n\nLet’s baptize the rows and columns:\n\ncolnames(distances_matrix) &lt;- castles$Nom\n\nrownames(distances_matrix) &lt;- castles$Nom\n\nNow that we have the data, we can solve the TSP."
  },
  {
    "objectID": "posts/2018-11-21-lux_castle.html#solving-the-travelling-salesman-problem-thats-the-combinatorial-optimization-part",
    "href": "posts/2018-11-21-lux_castle.html#solving-the-travelling-salesman-problem-thats-the-combinatorial-optimization-part",
    "title": "The best way to visit Luxembourguish castles is doing data science + combinatorial optimization",
    "section": "\nSolving the Travelling salesman problem (that’s the combinatorial optimization part)\n",
    "text": "Solving the Travelling salesman problem (that’s the combinatorial optimization part)\n\n\nLet’s first coerce the distances_matrix to an ATSP object, which is needed for the solver. ATSP stands for asymmetrical TSP. Asymmetrical because the distances_matrix is not symmetric, meaning that going from Castle A to Castle B is longer than going from Castle B to Castle A (for example).\n\natsp_castles &lt;- ATSP(distances_matrix)\n\nI then define a list of all the available methods:\n\nmethods &lt;- c(\"identity\", \"random\", \"nearest_insertion\",\n             \"cheapest_insertion\", \"farthest_insertion\", \"arbitrary_insertion\",\n             \"nn\", \"repetitive_nn\", \"two_opt\")\n\nAnd solve the problem with all the methods:\n\nsolutions &lt;- map(methods, ~solve_TSP(x = atsp_castles, method = ., two_opt = TRUE, rep = 10,  two_opt_repetitions = 10)) %&gt;%\n    set_names(methods)\n## Warning: executing %dopar% sequentially: no parallel backend registered\n\nI do this because the results vary depending on the methods, and I want to be exhaustive (solving this problem is quite fast, so there’s no reason not to do it):\n\nsolutions_df &lt;- solutions %&gt;%\n    map_df(as.numeric)\n\nsolutions_df is a data frame with the order of the castles to visit in rows and the method used in columns.\n\n\n\n\nClick if you want to see the solutions\n\n\nsolutions_df\n## # A tibble: 48 x 9\n##    identity random nearest_inserti… cheapest_insert… farthest_insert…\n##       &lt;dbl&gt;  &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n##  1        1     10               37               44               15\n##  2        2     11               17               37               27\n##  3       36      4               22               17               29\n##  4       33      9               47               40               38\n##  5        6     45               16               27               41\n##  6       44     43               43               15               19\n##  7       37     16               13               18                5\n##  8       17     47               21               34               46\n##  9       22     22               14                7               12\n## 10       47     13               45               20               23\n## # … with 38 more rows, and 4 more variables: arbitrary_insertion &lt;dbl&gt;,\n## #   nn &lt;dbl&gt;, repetitive_nn &lt;dbl&gt;, two_opt &lt;dbl&gt;\n\n\nNow, let’s extract the tour lengths, see which one is the minimum, then plot it.\n\ntour_lengths &lt;- solutions %&gt;%\n    map_dbl(tour_length)\n\nwhich.min(tour_lengths)\n## arbitrary_insertion \n##                   6\n\nThe total length of the tour is 474 kilometers (that’s 295 miles). Before plotting the data, let’s re-order it according to the solution:\n\ncastles_to_visit &lt;- castles[pull(solutions_df, names(which.min(tour_lengths))), ]"
  },
  {
    "objectID": "posts/2018-11-21-lux_castle.html#plot-the-solution",
    "href": "posts/2018-11-21-lux_castle.html#plot-the-solution",
    "title": "The best way to visit Luxembourguish castles is doing data science + combinatorial optimization",
    "section": "\nPlot the solution\n",
    "text": "Plot the solution\n\n\nTo plot the solution, I first use a data frame I created with the longitude and latitude of Luxembourguish communes, from the geojson file available on the OpenData Portal. I converted it to a data frame because it is easier to manipulate this way. The code to do that is in the appendix of this blog post:\n\ncommunes_df &lt;- read_csv(\"communes_df.csv\")\n## Parsed with column specification:\n## cols(\n##   lon = col_double(),\n##   lat = col_double(),\n##   commune = col_character()\n## )\n\nNow I can use {ggplot2} to create the map with the tour. I use geom_polygon() to build the map, geom_point() to add the castles, geom_path() to connect the points according to the solution I found and geom_point() again to highlight the starting castle:\n\nggplot() +\n    geom_polygon(data = communes_df, aes(x = lon, y = lat, group = commune), colour = \"grey\", fill = NA) +\n    geom_point(data = castles, aes(x = lon, y = lat), colour = \"#82518c\", size = 3) +\n    geom_path(data = castles_to_visit, aes(x = lon, y = lat), colour = \"#647e0e\") +\n    geom_point(data = (slice(castles_to_visit, 1)), aes(x = lon, y = lat), colour = \"white\", size = 5) +\n    theme_void() +\n    ggtitle(\"The shortest tour to visit 48 Luxembourguish castles\") +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank(),\n          legend.text = element_text(colour = \"white\"),\n          plot.background = element_rect(\"#272b30\"),\n          plot.title = element_text(colour = \"white\")) \n\n\n\n\nThe white point is the starting point of the tour. As a bonus, let’s do the same plot without points, but castles emojis instead (using the {ggimage} package):\n\nggplot() +\n    geom_polygon(data = communes_df, aes(x = lon, y = lat, group = commune), colour = \"grey\", fill = NA) +\n    geom_emoji(data = castles, aes(x = lon, y = lat, image = \"1f3f0\")) + # &lt;- this is the hex code for the \"european castle\" emoji\n    geom_path(data = castles_to_visit, aes(x = lon, y = lat), colour = \"#647e0e\") +\n    theme_void() +\n    ggtitle(\"The shortest tour to visit 48 Luxembourguish castles\") +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank(),\n          legend.text = element_text(colour = \"white\"),\n          plot.background = element_rect(\"#272b30\"),\n          plot.title = element_text(colour = \"white\"))\n## Warning: Ignoring unknown parameters: image_colour\n\n\n\n\nIt’s horrible."
  },
  {
    "objectID": "posts/2018-11-21-lux_castle.html#appendix",
    "href": "posts/2018-11-21-lux_castle.html#appendix",
    "title": "The best way to visit Luxembourguish castles is doing data science + combinatorial optimization",
    "section": "\nAppendix\n",
    "text": "Appendix\n\n\nThe code below converts the geojson that can be downloaded from the OpenData Portal to csv. A csv file is easier to handle. I only focus on the communes.\n\nlimadmin &lt;- RJSONIO::fromJSON(\"limadmin.geojson\")\n\ncommunes &lt;- limadmin$communes\n\nextract_communes &lt;- function(features){\n\n    res &lt;- features$geometry$coordinates %&gt;%\n        map(lift(rbind)) %&gt;%\n        as.data.frame() %&gt;%\n        rename(lon = X1,\n               lat = X2)\n\n    res %&gt;%\n        mutate(commune = features$properties[1])\n}\n\ncommunes_df &lt;- map(limadmin$communes$features, extract_communes)\n\n## Steinfort and Waldbredimus special treatment:\n\nsteinfort &lt;- limadmin$communes$features[[5]]$geometry$coordinates[[1]] %&gt;%\n    map(lift(rbind)) %&gt;%\n    as.data.frame() %&gt;%\n    rename(lon = X1,\n           lat = X2) %&gt;%\n    mutate(commune = \"Steinfort\")\n\nwaldbredimus &lt;- limadmin$communes$features[[44]]$geometry$coordinates[[1]] %&gt;%\n    map(lift(rbind)) %&gt;%\n    as.data.frame() %&gt;%\n    rename(lon = X1,\n           lat = X2) %&gt;%\n    mutate(commune = \"Waldbredimus\")\n\ncommunes_df[[5]] &lt;- NULL\ncommunes_df[[43]] &lt;- NULL\n\n\ncommunes_df &lt;- bind_rows(communes_df, list(steinfort, waldbredimus))"
  },
  {
    "objectID": "posts/2018-01-19-mapping_functions_with_any_cols.html",
    "href": "posts/2018-01-19-mapping_functions_with_any_cols.html",
    "title": "Mapping a list of functions to a list of datasets with a list of columns as arguments",
    "section": "",
    "text": "This week I had the opportunity to teach R at my workplace, again. This course was the “advanced R” course, and unlike the one I taught at the end of last year, I had one more day (so 3 days in total) where I could show my colleagues the joys of the tidyverse and R.\n\n\nTo finish the section on programming with R, which was the very last section of the whole 3 day course I wanted to blow their minds; I had already shown them packages from the tidyverse in the previous days, such as dplyr, purrr and stringr, among others. I taught them how to use ggplot2, broom and modelr. They also liked janitor and rio very much. I noticed that it took them a bit more time and effort for them to digest purrr::map() and purrr::reduce(), but they all seemed to see how powerful these functions were. To finish on a very high note, I showed them the ultimate purrr::map() use case.\n\n\nConsider the following; imagine you have a situation where you are working on a list of datasets. These datasets might be the same, but for different years, or for different countries, or they might be completely different datasets entirely. If you used rio::import_list() to read them into R, you will have them in a nice list. Let’s consider the following list as an example:\n\nlibrary(tidyverse)\ndata(mtcars)\ndata(iris)\n\ndata_list = list(mtcars, iris)\n\nI made the choice to have completely different datasets. Now, I would like to map some functions to the columns of these datasets. If I only worked on one, for example on mtcars, I would do something like:\n\nmy_summarise_f = function(dataset, cols, funcs){\n  dataset %&gt;%\n    summarise_at(vars(!!!cols), funs(!!!funcs))\n}\n\nAnd then I would use my function like so:\n\nmtcars %&gt;%\n  my_summarise_f(quos(mpg, drat, hp), quos(mean, sd, max))\n##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max\n## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93\n##   hp_max\n## 1    335\n\nmy_summarise_f() takes a dataset, a list of columns and a list of functions as arguments and uses tidy evaluation to apply mean(), sd(), and max() to the columns mpg, drat and hp of mtcars. That’s pretty useful, but not useful enough! Now I want to apply this to the list of datasets I defined above. For this, let’s define the list of columns I want to work on:\n\ncols_mtcars = quos(mpg, drat, hp)\ncols_iris = quos(Sepal.Length, Sepal.Width)\n\ncols_list = list(cols_mtcars, cols_iris)\n\nNow, let’s use some purrr magic to apply the functions I want to the columns I have defined in list_cols:\n\nmap2(data_list,\n     cols_list,\n     my_summarise_f, funcs = quos(mean, sd, max))\n## [[1]]\n##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max\n## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93\n##   hp_max\n## 1    335\n## \n## [[2]]\n##   Sepal.Length_mean Sepal.Width_mean Sepal.Length_sd Sepal.Width_sd\n## 1          5.843333         3.057333       0.8280661      0.4358663\n##   Sepal.Length_max Sepal.Width_max\n## 1              7.9             4.4\n\nThat’s pretty useful, but not useful enough! I want to also use different functions to different datasets!\n\n\nWell, let’s define a list of functions then:\n\nfuncs_mtcars = quos(mean, sd, max)\nfuncs_iris = quos(median, min)\n\nfuncs_list = list(funcs_mtcars, funcs_iris)\n\nBecause there is no map3(), we need to use pmap():\n\npmap(\n  list(\n    dataset = data_list,\n    cols = cols_list,\n    funcs = funcs_list\n  ),\n  my_summarise_f)\n## [[1]]\n##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max\n## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93\n##   hp_max\n## 1    335\n## \n## [[2]]\n##   Sepal.Length_median Sepal.Width_median Sepal.Length_min Sepal.Width_min\n## 1                 5.8                  3              4.3               2\n\nNow I’m satisfied! Let me tell you, this blew their minds 😄!\n\n\nTo be able to use things like that, I told them to always solve a problem for a single example, and from there, try to generalize their solution using functional programming tools found in purrr.\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2019-01-13-newspapers_mets_alto.html",
    "href": "posts/2019-01-13-newspapers_mets_alto.html",
    "title": "Making sense of the METS and ALTO XML standards",
    "section": "",
    "text": "Last week I wrote a blog post where I analyzed one year of newspapers ads from 19th century newspapers. The data is made available by the national library of Luxembourg. In this blog post, which is part 1 of a 2 part series, I extract data from the 257gb archive, which contains 10 years of publications of the L’Union, another 19th century Luxembourguish newspaper written in French. As I explained in the previous post, to make life easier to data scientists, the national library also included ALTO and METS files (which are a XML files used to describe the layout and contents of physical text sources, such as pages of a book or newspaper) which can be easily parsed by R.\nThis is how a ALTO file looks like:\nEach page of the newspaper of a given day has one ALTO file. This is how a METS file looks like:\nFor each daily issue of the newspaper, there is a METS file. So 1 METS file for 4 ALTO files.\nIn my last blog post, I only extracted the words from the ALTO file (red rectangles of the first screenshot) and did not touch the METS file. The problem of doing this is that I get all the words for each page, without knowing which come from the same article. If I want to know which words come from the same article, I need to use the info from the METS file. From the METS file I have the ID of the article, and some other metadata, such as the title of the article and the type of the article (which can be article, advertisement, etc). The information highlighted with the green rectangles in the METS file can be linked to the green rectangles from the ALTO files. My goal is to get the following data frame from the METS file:\nand this data frame from the ALTO files:\nAs you can see, by combining both data frames I can know which words come from the same article, which will be helpful for further analysis. A lot of things happened in the 1860s. I am really curious to see if and how these events where reported in a Luxembourguish newspaper. I am particularly curious about how long it took to report certain news from far away, such as the assassination of Abraham Lincoln. But before that I need to extract the data!\nI will only focus on the METS file. The logic for the ALTO file is the same. All the source code will be in the appendix of this blog post.\nFirst, let’s take a look at a METS file:\nThis is how it looks like:\nAs usual when you import text files like this, it’s always a good idea to split the file. I will split at the “DMDID” character. Take a look back at the second screenshot. The very first tag, first row, first word after div is “DMDID”. By splitting at this level, I will get back a list, where each element is the content of this div DMDID block. This is exactly what I need, since this block contains the information from the green rectangles. So let’s split the mets variable at this level:\nLet’s take a look at mets_articles:\nDoesn’t seem to be very helpful, but actually it is. We can see that mets_articles is a now a list of 25 elements.\nThis means that for each element of mets_articles, I need to get the identifier, the label, the type (the red rectangles from the screenshot), but also the information from the “BEGIN” element (the green rectangle).\nTo do this, I’ll be using regular expressions. In general, I start by experimenting in the console, and then when things start looking good, I write a function. Here is this function:\nThis function may seem complicated, but it simply encapsulates some pretty standard steps to get the data I need. I had to consider two cases. The first case is when I need to extract all the elements with str_extract_all(), or only the first occurrence, with str_extract(). Let’s test it on the first article of the mets_articles list:\nLet’s see what happens with all = TRUE:\nThis seems to work as intended. Since I need to call this function several times, I’ll be writing another function that extracts all I need:\nThis function uses complex regular expressions to extract the strings I need, and then puts the result into a data frame, with the tibble() function. I then use unnest(), because label, type, begins and id are not the same length. label, type and id are of length 1, while begins is longer. This means that when I put them into a data frame it looks like this:\nWith unnest(), I get a nice data frame:\nNow, I simply need to map this function to all the files and that’s it! For this, I will write yet another helper function:\nThis function takes the path to a METS file as input, and processes it using the steps I explained above. The only difference is that I add a column containing the name of the file that was processed, and write the resulting data frame directly to disk as a data frame. Finally, I can map this function to all the METS files:\nI use {furrr} to extract the data from all the files in parallel, by putting 8 cores of my CPU to work. This took around 3 minutes and 20 seconds to finish.\nThat’s it for now, stay tuned for part 2 where I will analyze this fresh data!"
  },
  {
    "objectID": "posts/2019-01-13-newspapers_mets_alto.html#appendix",
    "href": "posts/2019-01-13-newspapers_mets_alto.html#appendix",
    "title": "Making sense of the METS and ALTO XML standards",
    "section": "\nAppendix\n",
    "text": "Appendix\n\nextract_alto &lt;- function(article){\n    begins &lt;- article[1] %&gt;%\n        extractor(\"(?&lt;=^ID)(.*?)(?=HPOS)\", all = TRUE)\n\n    content &lt;- article %&gt;%\n        extractor(\"(?&lt;=CONTENT)(.*?)(?=WC)\", all = TRUE)\n\n    tibble::tribble(~begins, ~content,\n                    begins, content) %&gt;%\n        unnest()\n}\n\nalto_csv &lt;- function(page_path){\n\n    page &lt;- read_file(page_path)\n\n    doc_name &lt;- str_extract(page_path, \"(?&lt;=/text/).*\")\n\n    alto_articles &lt;- page %&gt;%\n        str_split(\"TextBlock \") %&gt;%\n        flatten_chr()\n\n    alto_df &lt;- map_df(alto_articles, extract_alto)\n\n    alto_df &lt;- alto_df %&gt;%\n        mutate(document = doc_name)\n\n    write_csv(alto_df, paste0(page_path, \".csv\"))\n}\n\n\nalto &lt;- read_file(\"1533660_newspaper_lunion_1860-11-14/text/1860-11-14_01-00001.xml\")\n\n\n# Extract content from alto files\n\npages_alto &lt;- str_match(list.files(path = \"./\", all.files = TRUE, recursive = TRUE), \".*/text/.*.xml\") %&gt;%\n    discard(is.na)\n\n\nlibrary(furrr)\n\nplan(multiprocess, workers = 8)\n\ntic &lt;- Sys.time()\nfuture_map(pages_alto, alto_csv)\ntoc &lt;- Sys.time()\n\ntoc - tic\n\n#Time difference of 18.64776 mins"
  },
  {
    "objectID": "posts/2014-11-11-benchmarks-r-blas-atlas-rro.html",
    "href": "posts/2014-11-11-benchmarks-r-blas-atlas-rro.html",
    "title": "R, R with Atlas, R with OpenBLAS and Revolution R Open: which is fastest?",
    "section": "",
    "text": "In this short post, I benchmark different “versions” of R. I compare the execution speeds of R, R linked against OpenBLAS, R linked against ATLAS and Revolution R Open. Revolution R Open is a new open source version of R made by Revolution Analytics. It is linked against MKL and should offer huge speed improvements over vanilla R. Also, it uses every cores of your computer by default, without any change whatsoever to your code.\n\n\nTL;DR: Revolution R Open is the fastest of all the benchmarked versions (with R linked against OpenBLAS and ATLAS just behind), and easier to setup.\n\n\nSetup\n\n\nI benchmarked these different versions of R using R-benchmark-25.R that you can download here. This benchmark file was created by Simon Urbanek.\n\n\nI ran the benchmarks on my OpenSUSE 13.2 computer with a Pentium Dual-Core CPU E6500@2.93GHz with 4GB of Ram. It's outdated, but it's still quite fast for most of my numerical computation needs. I installed “vanilla” R from the official OpenSUSE repositories which is currently at version 3.1.2.\n\n\nThen, I downloaded OpenBLAS and ATLAS also from the official OpenSUSE repositories and made R use these libraries instead of its own implementation of BLAS. The way I did that is a bit hacky, but works: first, go to /usr/lib64/R/lib and backup libRblas.so (rename it to libRblas.soBackup for instance). Then link /usr/lib64/libopenblas.so.0 to /usr/lib64/R/lib/libRblas, and that's it, R will use OpenBLAS. For ATLAS, you can do it in the same fashion, but you'll find the library in /usr/lib64/atlas/. These paths should be the same for any GNU/Linux distribution. For other operating systems, I'm sure you can find where these libraries are with Google.\n\n\nThe last version I benchmarked was Revolution R Open. This is a new version of R released by Revolution Analytics. Revolution Analytics had their own version of R, called Revolution R, for quite some time now. They decided to release a completely free as in freedom and free as in free beer version of this product which they now renamed Revolution R Open. You can download Revolution R Open here. You can have both “vanilla” R and Revolution R Open installed on your system.\n\n\nResults\n\n\nI ran the R-benchmark-25.R 6 times for every version but will only discuss the 4 best runs.\n\n\n\n\n\nR version\n\n\nFastest run\n\n\nSlowest run\n\n\nMean Run\n\n\n\n\nVanilla R\n\n\n63.65\n\n\n66.21\n\n\n64.61\n\n\n\n\nOpenBLAS R\n\n\n15.63\n\n\n18.96\n\n\n16.94\n\n\n\n\nATLAS R\n\n\n16.92\n\n\n21.57\n\n\n18.24\n\n\n\n\nRRO\n\n\n14.96\n\n\n16.08\n\n\n15.49\n\n\n\n\nAs you can read from the table above, Revolution R Open was the fastest of the four versions, but not significantly faster than BLAS or ATLAS R. However, RRO uses all the available cores by default, so if your code relies on a lot matrix algebra, RRO might be actually a lot more faster than OpenBLAS and ATLAS R. Another advantage of RRO is that it is very easy to install, and also works with Rstudio and is compatible with every R package to existence. “Vanilla” R is much slower than the other three versions, more than 3 times as slow!\n\n\nConclusion\n\n\nWith other benchmarks, you could get other results, but I don’t think that “vanilla” R could beat any of the other three versions. Whatever your choice, I recommend not using plain, “vanilla” R. The other options are much faster than standard R, and don't require much work to set up. I'd personally recommend Revolution R Open, as it is free software and compatible with CRAN packages and Rstudio."
  },
  {
    "objectID": "posts/2020-03-26-bepo_lu.html",
    "href": "posts/2020-03-26-bepo_lu.html",
    "title": "What would a keyboard optimised for Luxembourguish look like?",
    "section": "",
    "text": "I’ve been using the BÉPO layout for my keyboard since 2010-ish, and it’s been one of the best computing decisions I’ve ever taken. The BÉPO layout is an optimized layout for French, but it works quite well for many European languages, English included (the only issue you might have with the BÉPO layout for English is that the w is a bit far away).\n\n\nTo come up with the BÉPO layout, ideas from a man named August Dvorak were applied for the French language. Today, the keyboard layout that is optimised for English is called after him, the DVORAK layout. Dvorak’s ideas were quite simple; unlike the QWERTY layout, his layout had to be based on character frequency of the English language. The main idea is that the most used characters of the language should be on the home row of the keyboard. The home row is the row where you lay your fingers on the keyboard when you are not typing (see picture below).\n\n\n\n\n\nThe problem with the “standard” layouts, such as QWERTY, is that they’re all absolute garbage, and not optimized at all for typing on a computer. For instance, look at the heatmap below, which shows the most used characters on a QWERTY keyboard when typing an a standard English text:\n\n\n\n\n\n(Heatmap generated on https://www.patrick-wied.at/projects/heatmap-keyboard/.)\n\n\nAs you can see, most of the characters used to type this text are actually outside of the home row, and the majority of them on the left hand side of the keyboard. The idea of Dvorak was to first, put the most used characters on the home row, and second to try to have an equal split of characters, 50% for each hand.\n\n\nThe same text on the DVORAK layout, shows how superior it is:\n\n\n\n\n\nAs you can see, this is much much better. The same idea was applied to develop the BÉPO layout for French. And because character frequency is quite similar across languages, learning a layout such as the BÉPO not only translates to more efficient typing for French, but also for other languages, such as English, as already explained above.\n\n\nThe reason I’m writing this blog post is due, in part, to the confinement situation that many people on Earth are currently facing due to the Corona virus. I have a job where I spend my whole day typing, and am lucky enough to be able to work from home. Which means that I’m lucky enough to use my mechanical keyboard to work, which is really great. (I avoid taking my mechanical keyboard with me at work, because I am never very long in the same spot, between meeting and client assignments…). But to have a mechanical keyboard that’s easy to take with me, I decided to buy a second mechanical keyboard, a 40% keyboard from Ergodox (see picture below):\n\n\n\n\n\nBecause I don’t even want to see the QWERTY keycaps, I bought blank keycaps to replace the ones that come with the keyboard. Anyway, this made me think about how crazy it is that in 2020 people still use absolute garbage keyboard layouts (and keyboards by the way) to type on, when their job is basically only typing all day long. It made me so angry that I even made a video, which you enjoy here.\n\n\nThe other thing I thought about was the specific case of Luxembourg, a country with 3 official languages (Luxembourguish, French and German), a very large Portuguese minority, and where English became so important in recent years that the government distributed leaflets in English to the population (along with leaflets in French, Luxembourguish, German and Portuguese of course) explaining what is and is not allowed during the period of containment. What would a keyboard optimized for such a unique country look like?\n\n\nOf course, the answer that comes to mind quickly is to use the BÉPO layout; even though people routinely write in at least 3 of the above-mentioned languages, French is still the one that people use most of the time for written communication (at least, that’s my perception). The reason is that while Luxembourguish is the national language, and the language of the native population, French has always been the administrative language, and laws are still written in French only, even though they’re debated in Luxembourguish in the parliament. However, people also routinely write emails in German or English, and more and more people also write in Luxembourguish. This means that a keyboard optimized for Luxembourguish, or rather, for the multilinguistic nature of the Luxembourguish country, should take into account all these different languages. Another thing to keep in mind is that Luxembourguish uses many French words, and as such, writing these words should be easy.\n\n\nSo let’s start with the BÉPO layout as a base. This is what it looks like:\n\n\n\n\n\nA heatmap of character frequencies of a French, or even English, text would show that the most used characters are on the home row. If you compare DVORAK to BÉPO, you will see that the home row is fairly similar. But what strikes my colleagues when they see a picture of the BÉPO layout, is the fact that the characters é, è, ê, à and ç can be accessed directly. They are so used to having these characters only accessible by using some kind of modifier key that their first reaction is to think that this is completely stupid. However, what is stupid, is not having these letters easily accessible, and instead having, say, z easily accessible (the French “standard” layout is called AZERTY, which is very similar and just as stupid as the QWERTY layout. The letter Z is so easy to type on, but is almost non-existing in French!).\n\n\nSo let’s analyze character frequencies of a Luxembourguish text and see if the BÉPO layout could be a good fit. I used several text snippets from the Bible in Luxembourguish for this, and a few lines of R code:\n\nlibrary(tidyverse)\nlibrary(rvest)\nroot_url &lt;- \"https://cathol.lu/article\"\n\ntexts &lt;- seq(4869,4900)\n\nurls &lt;- c(\"https://cathol.lu/article4887\",\n          \"https://cathol.lu/article1851\",\n          \"https://cathol.lu/article1845\",\n          \"https://cathol.lu/article1863\",\n          \"https://cathol.lu/article1857\",\n          \"https://cathol.lu/article4885\",\n          \"https://cathol.lu/article1648\",\n          \"https://cathol.lu/article1842\",\n          \"https://cathol.lu/article1654\",\n          \"https://cathol.lu/article1849\",\n          \"https://cathol.lu/article1874\",\n          \"https://cathol.lu/article4884\",\n          \"https://cathol.lu/article1878\",\n          \"https://cathol.lu/article2163\",\n          \"https://cathol.lu/article2127\",\n          \"https://cathol.lu/article2185\",\n          \"https://cathol.lu/article4875\")\n\nNow that I’ve get the urls, let’s get the text out of it:\n\npages &lt;- urls %&gt;%\n  map(read_html)\n\ntexts &lt;- pages %&gt;%\n  map(~html_node(., xpath = '//*[(@id = \"art_texte\")]')) %&gt;%\n  map(html_text)\n\ntexts is a list containing the raw text from the website. I used several functions from the {rvest} package to do this. I won’t comment on them, because this is not a tutorial about webscraping (I’ve written several of those already), but a rant about keyboard layout gosh darn it.\n\n\nAnyway, let’s now take a look at the character frequencies, and put that in a neat data frame:\n\ncharacters &lt;- texts %&gt;%\n  map(~strsplit(., split = \"\")) %&gt;%\n  unlist() %&gt;%\n  map(~strsplit(., split = \"\")) %&gt;%\n  unlist() %&gt;%\n  tolower() %&gt;%\n  str_extract_all(pattern = \"[:alpha:]\") %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%  \n  as.data.frame()\n\nComputing the frequencies is now easy:\n\ncharacters &lt;- characters %&gt;%\n  mutate(frequencies = round(Freq/sum(Freq)*100, digits = 2)) %&gt;%\n  arrange(desc(frequencies)) %&gt;%  \n  janitor::clean_names()\n\nLet’s start with the obvious differences: there is not a single instance of the characters è, ê or ç, which are used in French only. There are however instances of ü, ä, and ë. These characters should be easily accessible, however their frequencies are so low, that they could still only be accessible using a modifier key, and it would not be a huge issue. However, since ç does not appear at all, maybe it could be replaced by ä and ê could be replaced by ë. But we must keep in mind that since the average Luxembourger has to very often switch between so many languages, I would suggest that these French characters that would be replaced should still be accessible using a modifier such as Alt Gr. As for the rest, the layout as it stands is likely quite ok. Well, actually I know it’s ok, because when I write in Luxembourguish using the BÉPO layout, I find it quite easy to do. But let’s grab a French and a German text, and see how the ranking of the characters compare. Let’s get some French text:\n\n\n\n\nClick to read the French text\n\n\nfrench &lt;- \"Au commencement, Dieu créa les cieux et la terre.\nLa terre était informe et vide: il y avait des ténèbres à la surface de l'abîme, et l'esprit de Dieu se mouvait au-dessus des eaux.\nDieu dit: Que la lumière soit! Et la lumière fut.\nDieu vit que la lumière était bonne; et Dieu sépara la lumière d'avec les ténèbres.\nDieu appela la lumière jour, et il appela les ténèbres nuit. Ainsi, il y eut un soir, et il y eut un matin: ce fut le premier jour.\nDieu dit: Qu'il y ait une étendue entre les eaux, et qu'elle sépare les eaux d'avec les eaux.\nEt Dieu fit l'étendue, et il sépara les eaux qui sont au-dessous de l'étendue d'avec les eaux qui sont au-dessus de l'étendue. Et cela fut ainsi.\nDieu appela l'étendue ciel. Ainsi, il y eut un soir, et il y eut un matin: ce fut le second jour.\nDieu dit: Que les eaux qui sont au-dessous du ciel se rassemblent en un seul lieu, et que le sec paraisse. Et cela fut ainsi.\nDieu appela le sec terre, et il appela l'amas des eaux mers. Dieu vit que cela était bon.\nPuis Dieu dit: Que la terre produise de la verdure, de l'herbe portant de la semence, des arbres fruitiers donnant du fruit selon leur espèce et ayant en eux leur semence sur la terre. Et cela fut ainsi.\nLa terre produisit de la verdure, de l'herbe portant de la semence selon son espèce, et des arbres donnant du fruit et ayant en eux leur semence selon leur espèce. Dieu vit que cela était bon.\nAinsi, il y eut un soir, et il y eut un matin: ce fut le troisième jour.\nDieu dit: Qu'il y ait des luminaires dans l'étendue du ciel, pour séparer le jour d'avec la nuit; que ce soient des signes pour marquer les époques, les jours et les années;\net qu'ils servent de luminaires dans l'étendue du ciel, pour éclairer la terre. Et cela fut ainsi.\nDieu fit les deux grands luminaires, le plus grand luminaire pour présider au jour, et le plus petit luminaire pour présider à la nuit; il fit aussi les étoiles.\nDieu les plaça dans l'étendue du ciel, pour éclairer la terre,\npour présider au jour et à la nuit, et pour séparer la lumière d'avec les ténèbres. Dieu vit que cela était bon.\nAinsi, il y eut un soir, et il y eut un matin: ce fut le quatrième jour.\nDieu dit: Que les eaux produisent en abondance des animaux vivants, et que des oiseaux volent sur la terre vers l'étendue du ciel.\nDieu créa les grands poissons et tous les animaux vivants qui se meuvent, et que les eaux produisirent en abondance selon leur espèce; il créa aussi tout oiseau ailé selon son espèce. Dieu vit que cela était bon.\nDieu les bénit, en disant: Soyez féconds, multipliez, et remplissez les eaux des mers; et que les oiseaux multiplient sur la terre.\nAinsi, il y eut un soir, et il y eut un matin: ce fut le cinquième jour.\nDieu dit: Que la terre produise des animaux vivants selon leur espèce, du bétail, des reptiles et des animaux terrestres, selon leur espèce. Et cela fut ainsi.\nDieu fit les animaux de la terre selon leur espèce, le bétail selon son espèce, et tous les reptiles de la terre selon leur espèce. Dieu vit que cela était bon.\nPuis Dieu dit: Faisons l'homme à notre image, selon notre ressemblance, et qu'il domine sur les poissons de la mer, sur les oiseaux du ciel, sur le bétail, sur toute la terre, et sur tous les reptiles qui rampent sur la terre.\nDieu créa l'homme à son image, il le créa à l'image de Dieu, il créa l'homme et la femme.\nDieu les bénit, et Dieu leur dit: Soyez féconds, multipliez, remplissez la terre, et l'assujettissez; et dominez sur les poissons de la mer, sur les oiseaux du ciel, et sur tout animal qui se meut sur la terre.\nEt Dieu dit: Voici, je vous donne toute herbe portant de la semence et qui est à la surface de toute la terre, et tout arbre ayant en lui du fruit d'arbre et portant de la semence: ce sera votre nourriture.\nEt à tout animal de la terre, à tout oiseau du ciel, et à tout ce qui se meut sur la terre, ayant en soi un souffle de vie, je donne toute herbe verte pour nourriture. Et cela fut ainsi.\nDieu vit tout ce qu'il avait fait et voici, cela était très bon. Ainsi, il y eut un soir, et il y eut un matin: ce fut le sixième jour.\nJoe Paterno, né le 21 décembre 1926 à Brooklyn et mort le 22 janvier 2012 à State College, est un joueur et entraîneur américain de football américain universitaire. Figure historique et emblématique des Nittany Lions de Penn State entre 1966 et 2011, il est l'entraîneur le plus victorieux de l'histoire du football américain universitaire avec 409 succès en Division I. Son image est toutefois ternie en fin de carrière à cause de soupçons de négligence dans une affaire d'agressions sexuelles sur mineurs.\n\nLors de ses brillantes études de droit à l'université Brown, Joe Paterno joue au football américain et est entraîné par Rip Engle. Ce dernier, embauché par l'université de Penn State, le recrute comme entraîneur assistant en 1950. Pendant quinze saisons, l'assistant fait ses preuves avant de devenir entraîneur principal des Nittany Lions en 1965. Surnommé JoePa, il connaît rapidement le succès. Invaincu en 1968 et 1969, il est désiré par plusieurs franchises de la National Football League (NFL), mais refuse pour conserver son rôle d'éducateur. Entraîneur de l'équipe universitaire championne en 1982 et 1986, vainqueur des quatre principaux Bowls universitaires, il intègre le College Football Hall of Fame en 2007 alors qu'il est encore en activité, un accomplissement rare.\n\nReconnu pour ses succès sportifs, académiques et son exemplarité, JoePa est adulé comme une icône populaire dans la région de State College. Onze jours après avoir célébré sa 409e victoire avec les Lions, il est démis de ses fonctions à la suite du scandale des agressions sexuelles de l'Université d'État de Pennsylvanie. Accusé d'avoir couvert les abus sexuels de Jerry Sandusky, son image est ternie par cette affaire au retentissement international. Il meurt deux mois plus tard des suites d'un cancer du poumon.\nChacun peut publier immédiatement du contenu en ligne, à condition de respecter les règles essentielles établies par la Fondation Wikimedia et par la communauté ; par exemple, la vérifiabilité du contenu, l'admissibilité des articles et garder une attitude cordiale.\n\nDe nombreuses pages d’aide sont à votre disposition, notamment pour créer un article, modifier un article ou insérer une image. N’hésitez pas à poser une question pour être aidé dans vos premiers pas, notamment dans un des projets thématiques ou dans divers espaces de discussion.\n\nLes pages de discussion servent à centraliser les réflexions et les remarques permettant d’améliorer les articles.\nEn 1894, l’explorateur Gustav Adolf von Götzen suivait les traces d’un missionnaire en provenance de la cote orientale d’Afrique. Pendant qu’il se rendait au Rwanda, il découvre un petit village des pécheurs appelé Ngoma qui traduit signifie tam tam, par déformation il écrivit Goma. Ngoma devint un poste belge en face de celui de Rubavu (au Rwanda) habité par les Allemands. Au début, la cohabitation entre ces deux postes n’était pas facile. À un certain moment, les chefs coutumiers du Rwanda, en complicité avec les Allemands attaquent les Belges de Goma. Ces derniers se réfugient à Bukavu et laissent les envahisseurs occuper la ville. Après des négociations, les Allemands replient vers le Rwanda et les Belges reprennent leur position initiale comme poste colonial. L’afflux des colonisateurs dans ce village joue un rôle important dans son évolution pour devenir une grande agglomération. Les colonisateurs venaient d’installer le chef lieu du district Belge à Rutshuru ou vivait l’administrateur colonial. Le chef lieu passera de Rutshuru à Goma.\n\nEn ce moment, Goma reste un poste de transaction lacustre avec Bukavu qui était une ville minière. Plus tard, Rutshuru, Masisi, Kalehe, Gisenyi, etc. déverseront leurs populations dans Goma, à la rechercher de l’emploi au près des colonisateurs. C’est en cette période que vu le jour le quartier Birere (un bidonville de Goma) autour des entrepôts, bureaux et habitations des colons. Le nom Birere (littéralement feuilles de bananier) vient du fait qu’à l’époque, les gens y construisaient en feuilles des bananiers.\n\nLa ville est la base arrière de l'opération Turquoise organisée en 1994 à la fin du génocide rwandais.\n\nLa ville et ses environs abriteront dans des camps autour de 650 000 réfugiés hutus de 1994 jusqu'à la chute du Zaïre, dont certains supposés anciens génocidaires. Selon des ONG, l'AFDL procède à des massacres dans les camps entre 1996 et 19971.\n\nDe 1998 à 2002/2003, la ville, sous contrôle du Rassemblement congolais pour la démocratie (RCD) pro-rwandais échappe au contrôle du gouvernement congolais.\n\nDe nombreux viols, massacres et crimes de guerre y ont été perpétrés entre 1996 et 2006 par les troupes des généraux rebelles du RCD, essentiellement sous les généraux Nkundabatware et Mutebusi.\n\nEn 2002, le Nyiragongo entra en éruption, et une coulée de lave atteignit le centre de la ville. La lave n'a pas atteint le lac Kivu fort heureusement, en effet ce lac est un lac méromictique et un changement brutal de chaleur aurait des conséquences graves : Éruption limnique.\n\nDébordant de populations fuyant les violences, Goma compte en 2012 plus de 400 000 habitants. Ceux qui ne peuvent pas trouver d'abri remplissent les camps de réfugiés, où l'ONU et les ONG se débattent pour leur fournir nourriture, eau et combustible.\"\n\ncharacters_fr &lt;- french %&gt;%\n  map(~strsplit(., split = \"\")) %&gt;%\n  unlist() %&gt;%\n  map(~strsplit(., split = \"\")) %&gt;%\n  unlist() %&gt;%\n  tolower() %&gt;%\n  str_extract_all(pattern = \"[:alpha:]\") %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%  \n  as.data.frame() %&gt;%  \n  mutate(frequencies = round(Freq/sum(Freq)*100, digits = 2)) %&gt;%\n  arrange(desc(frequencies)) %&gt;%  \n  janitor::clean_names()\n\nLet’s now do the same for German:\n\n\n\n\nClick to read the German text\n\n\ngerman &lt;- \"Am Anfang schuf Gott Himmel und Erde.\nUnd die Erde war wüst und leer, und es war finster auf der Tiefe; und der Geist Gottes schwebte auf dem Wasser.\nUnd Gott sprach: Es werde Licht! und es ward Licht.\nUnd Gott sah, daß das Licht gut war. Da schied Gott das Licht von der Finsternis\nund nannte das Licht Tag und die Finsternis Nacht. Da ward aus Abend und Morgen der erste Tag.\nUnd Gott sprach: Es werde eine Feste zwischen den Wassern, und die sei ein Unterschied zwischen den Wassern.\nDa machte Gott die Feste und schied das Wasser unter der Feste von dem Wasser über der Feste. Und es geschah also.\nUnd Gott nannte die Feste Himmel. Da ward aus Abend und Morgen der andere Tag.\nUnd Gott sprach: Es sammle sich das Wasser unter dem Himmel an besondere Örter, daß man das Trockene sehe. Und es geschah also.\nUnd Gott nannte das Trockene Erde, und die Sammlung der Wasser nannte er Meer. Und Gott sah, daß es gut war.\nUnd Gott sprach: Es lasse die Erde aufgehen Gras und Kraut, das sich besame, und fruchtbare Bäume, da ein jeglicher nach seiner Art Frucht trage und habe seinen eigenen Samen bei sich selbst auf Erden. Und es geschah also.\nUnd die Erde ließ aufgehen Gras und Kraut, das sich besamte, ein jegliches nach seiner Art, und Bäume, die da Frucht trugen und ihren eigenen Samen bei sich selbst hatten, ein jeglicher nach seiner Art. Und Gott sah, daß es gut war.\nDa ward aus Abend und Morgen der dritte Tag.\nUnd Gott sprach: Es werden Lichter an der Feste des Himmels, die da scheiden Tag und Nacht und geben Zeichen, Zeiten, Tage und Jahre\nund seien Lichter an der Feste des Himmels, daß sie scheinen auf Erden. Und es geschah also.\nUnd Gott machte zwei große Lichter: ein großes Licht, das den Tag regiere, und ein kleines Licht, das die Nacht regiere, dazu auch Sterne.\nUnd Gott setzte sie an die Feste des Himmels, daß sie schienen auf die Erde\nund den Tag und die Nacht regierten und schieden Licht und Finsternis. Und Gott sah, daß es gut war.\nDa ward aus Abend und Morgen der vierte Tag.\nUnd Gott sprach: Es errege sich das Wasser mit webenden und lebendigen Tieren, und Gevögel fliege auf Erden unter der Feste des Himmels.\nUnd Gott schuf große Walfische und allerlei Getier, daß da lebt und webt, davon das Wasser sich erregte, ein jegliches nach seiner Art, und allerlei gefiedertes Gevögel, ein jegliches nach seiner Art. Und Gott sah, daß es gut war.\nUnd Gott segnete sie und sprach: Seid fruchtbar und mehrt euch und erfüllt das Wasser im Meer; und das Gefieder mehre sich auf Erden.\nDa ward aus Abend und Morgen der fünfte Tag.\nUnd Gott sprach: Die Erde bringe hervor lebendige Tiere, ein jegliches nach seiner Art: Vieh, Gewürm und Tiere auf Erden, ein jegliches nach seiner Art. Und es geschah also.\nUnd Gott machte die Tiere auf Erden, ein jegliches nach seiner Art, und das Vieh nach seiner Art, und allerlei Gewürm auf Erden nach seiner Art. Und Gott sah, daß es gut war.\nUnd Gott sprach: Laßt uns Menschen machen, ein Bild, das uns gleich sei, die da herrschen über die Fische im Meer und über die Vögel unter dem Himmel und über das Vieh und über die ganze Erde und über alles Gewürm, das auf Erden kriecht.\nUnd Gott schuf den Menschen ihm zum Bilde, zum Bilde Gottes schuf er ihn; und schuf sie einen Mann und ein Weib.\nUnd Gott segnete sie und sprach zu ihnen: Seid fruchtbar und mehrt euch und füllt die Erde und macht sie euch untertan und herrscht über die Fische im Meer und über die Vögel unter dem Himmel und über alles Getier, das auf Erden kriecht.\nUnd Gott sprach: Seht da, ich habe euch gegeben allerlei Kraut, das sich besamt, auf der ganzen Erde und allerlei fruchtbare Bäume, die sich besamen, zu eurer Speise,\nund allem Getier auf Erden und allen Vögeln unter dem Himmel und allem Gewürm, das da lebt auf Erden, daß sie allerlei grünes Kraut essen. Und es geschah also.\nUnd Gott sah alles an, was er gemacht hatte; und siehe da, es war sehr gut. Da ward aus Abend und Morgen der sechste Tag.\nWährend des Bürgerkrieges und Völkermords im nahe angrenzenden Ruanda 1994 war Goma eines der Hauptziele für Flüchtlinge. Unter diesen waren nebst Zivilisten auch Mittäter des Genozids. Nachdem über eine Million Flüchtlinge die Stadt erreicht hatten, brach in den Lagern eine Cholera-Epidemie aus, die mehrere Tausend Opfer forderte. In den Jahren 1997 und 1998, als der Bürgerkrieg im Kongo nach dem Sturz von Präsident Mobutu Sese Seko eskalierte, eroberten ruandische Regierungstruppen Goma. Im Zuge der Verfolgung von Hutu, die in der Stadt Zuflucht gesucht hatten, töteten sie auch Hunderte Unbeteiligte.\n\nIm Jahre 2002 wurde die Stadt von einem Lavastrom aus dem etwa 14 km entfernten Nyiragongo im Norden zu großen Teilen zerstört. Viele Gebäude gerade im Stadtzentrum sowie der Flughafen Goma waren betroffen. Von den 3.000 Metern der Start- und Landebahn sind bis heute noch fast 1.000 Meter unter einer Lavaschicht begraben, so dass der internationale Verkehr ihn meidet. Rund 250.000 Einwohner der Stadt mussten flüchten. Es gab 147 Todesopfer, viele Flüchtlinge blieben obdachlos oder haben sich am Rande der Lavafelder Notunterkünfte gebaut. Seit April 2009 wird unter Führung der Welthungerhilfe das Rollfeld des Flughafens von der Lava befreit. Die Bedrohung, dass sich bei einer erneuten Eruption Lavamassen aus dem innerhalb des Vulkankraters befindlichen Lavasee erneut ins Tal und auf die Stadt ergießen, besteht nach wie vor.[3]\n\nAm 15. April 2008 raste nach dem Start vom Flughafen Goma eine Douglas DC-9 mit 79 Passagieren und 6 Besatzungsmitgliedern über das südliche Startbahnende hinaus in das Wohn- und Marktgebiet Birere. Etwa 40 Personen aus dem angrenzenden Siedlungsgebiet kamen ums Leben, mindestens 53 Passagiere und die 6 Besatzungsmitglieder überlebten jedoch. Das Feuer aus dem brennenden Wrack konnte sich aufgrund des starken Regens nicht ausbreiten, Anwohner konnten das Feuer zusätzlich eindämmen.\n\nZehntausende Menschen flohen Ende Oktober 2008 aufgrund einer Offensive von Tutsi-Rebellen aus der Stadt.[4]\n\nAm 21. November 2012 wurden große Teile der Stadt von der gegen die Zentralregierung unter Präsident Joseph Kabila kämpfenden Rebellenbewegung M23 eingenommen. Dort stationierte UNO-Friedens-Truppen griffen im Gegensatz zu früheren Aktivitäten nicht mehr ein.[5] Am 1. Dezember begannen sie nach Überschreitung eines Ultimatums der Internationalen Konferenz der Großen Seen Afrikas und zwei Resolutionen des UN-Sicherheitsrats, sich aus der Stadt zurückzuziehen.\n\nIm Jahre 2019 wurden mehrere Einzelfälle von Ebola in der Stadt registriert, nachdem die Ebola Epidemie bereits zuvor im Ostkongo ausgebrochen war.[6]\n\nSeit 1959 ist Goma Sitz des römisch-katholischen Bistums Goma.\nDie Transporteure werden Frachtführer (in Österreich Frächter) genannt. Sie organisieren nicht den Transport, sondern führen diesen aus, meistens im Auftrag eines Spediteurs. Die Höhe der Fracht wird im Frachtvertrag vereinbart und in der Regel im Frachtbrief festgehalten. Seit mit der Transportrechtsreform 1998 in Deutschland die Erstellung eines Frachtbriefes für nationale Transporte nicht mehr zwingend erforderlich ist, sondern auch Lieferscheine, Ladelisten oder vergleichbare Papiere als Warenbegleitdokument verwendet werden können, wird zunehmend kein Frachtbrief mehr ausgestellt. Beim Frachtbrief gibt es drei Originalausfertigungen. Eine Ausfertigung verbleibt beim Absender, nachdem ihm darauf der Frachtführer die Übernahme des Frachtguts bestätigt hat. Die zweite verbleibt nach Ablieferung des Frachtguts als Ablieferbestätigung beim Frachtführer und die dritte erhält der Empfänger.\n\nFür die Verladung des Frachtguts ist der Absender zuständig. Er ist dabei gem. § 412 HGB für eine beförderungssichere Verladung des Frachtguts verantwortlich, wohingegen der Frachtführer für die verkehrssichere Verladung (z. B. Gewichtsverteilung, Einhaltung der zulässigen Achslasten), als auch für die Ladungssicherung zu sorgen hat.\n\nBei Kontrollen muss der Frachtbrief den Zoll- und Polizeibehörden, sowie dem Bundesamt für Güterverkehr (BAG) ausgehändigt werden.\n\nEs gibt anmeldepflichtige Frachtgüter, für deren Transport es einer ausdrücklichen behördlichen Genehmigung bedarf. Schwertransporte erfordern eine behördliche Ausnahmegenehmigung und bei Überschreiten bestimmter Abmessungen sind gemäß § 29 Absatz. 3 StVO (Übermäßige Straßennutzung) definitiv Begleitfahrzeuge und/oder eine Begleitung durch die Polizei vorgeschrieben, um Sicherungsmaßnahmen einzuleiten und für einen reibungslosen Ablauf zu sorgen. Fällt das zu befördernde Frachtgut unter die Gefahrgutverordnung, muss das Transportfahrzeug neben der Einhaltung gefahrgutrelevanter Vorschriften auch mit entsprechenden Warntafeln gekennzeichnet sein. Darüber hinaus benötigt dann der Fahrzeugführer und ein eventueller Beifahrer auch eine ADR-Bescheinigung.\n\nDie Aufteilung der Frachtkosten zwischen Absender und Empfänger wird über die im Kaufvertrag festgehaltenen Lieferbedingungen geregelt, im internationalen Warenverkehr durch die Incoterms.\"\n\ncharacters_gr &lt;- german %&gt;%\n  map(~strsplit(., split = \"\")) %&gt;%\n  unlist() %&gt;%\n  map(~strsplit(., split = \"\")) %&gt;%\n  unlist() %&gt;%\n  tolower() %&gt;%\n  str_extract_all(pattern = \"[:alpha:]\") %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%  \n  as.data.frame() %&gt;%  \n  mutate(frequencies = round(Freq/sum(Freq)*100, digits = 2)) %&gt;%\n  arrange(desc(frequencies)) %&gt;%\n  janitor::clean_names()\n\nLet’s now visualize how the rankings evolve between these three languages. For this, I’m using the newggslopegraph() function from the {CGPfunctions} package:\n\ncharacters$rank &lt;- seq(1, 30)\ncharacters_fr$rank &lt;- seq(1, 36)\ncharacters_gr$rank &lt;- seq(1, 28)\n\ncharacters_fr &lt;- characters_fr %&gt;%\n  select(letters = x, rank, frequencies) %&gt;%\n  mutate(language = \"french\")\n\ncharacters_gr &lt;- characters_gr %&gt;%\n  select(letters = x, rank, frequencies) %&gt;%\n  mutate(language = \"german\")\n\ncharacters &lt;- characters %&gt;%\n  select(letters = x, rank, frequencies) %&gt;%\n  mutate(language = \"luxembourguish\")\n\ncharacters_df &lt;- bind_rows(characters, characters_fr, characters_gr)\n\nCGPfunctions::newggslopegraph(characters_df, \n                              language,\n                              rank,\n                              letters,\n                              Title = \"Character frequency ranking for the Luxembourguish official languages\",\n                              SubTitle = NULL,\n                              Caption = NULL,\n                              YTextSize = 4) \n## Registered S3 methods overwritten by 'lme4':\n##   method                          from\n##   cooks.distance.influence.merMod car \n##   influence.merMod                car \n##   dfbeta.influence.merMod         car \n##   dfbetas.influence.merMod        car\n\n\n\n\n\n\nClick to look at the raw data, which contains the frequencies\n\n\ncharacters_df \n##    letters rank frequencies       language\n## 1        e    1       16.19 luxembourguish\n## 2        n    2        9.61 luxembourguish\n## 3        s    3        6.94 luxembourguish\n## 4        a    4        6.56 luxembourguish\n## 5        i    5        6.44 luxembourguish\n## 6        t    6        6.16 luxembourguish\n## 7        d    7        5.56 luxembourguish\n## 8        r    8        5.42 luxembourguish\n## 9        h    9        5.21 luxembourguish\n## 10       u   10        3.76 luxembourguish\n## 11       g   11        3.70 luxembourguish\n## 12       m   12        3.26 luxembourguish\n## 13       o   13        3.07 luxembourguish\n## 14       l   14        2.81 luxembourguish\n## 15       c   15        2.51 luxembourguish\n## 16       w   16        2.23 luxembourguish\n## 17       é   17        1.56 luxembourguish\n## 18       k   18        1.42 luxembourguish\n## 19       f   19        1.34 luxembourguish\n## 20       ä   20        1.18 luxembourguish\n## 21       z   21        1.03 luxembourguish\n## 22       p   22        1.02 luxembourguish\n## 23       j   23        0.78 luxembourguish\n## 24       ë   24        0.72 luxembourguish\n## 25       b   25        0.68 luxembourguish\n## 26       v   26        0.68 luxembourguish\n## 27       ü   27        0.13 luxembourguish\n## 28       q   28        0.01 luxembourguish\n## 29       x   29        0.01 luxembourguish\n## 30       y   30        0.01 luxembourguish\n## 31       e    1       15.40         french\n## 32       s    2        7.81         french\n## 33       i    3        7.63         french\n## 34       t    4        7.52         french\n## 35       a    5        7.47         french\n## 36       u    6        7.03         french\n## 37       r    7        6.74         french\n## 38       n    8        6.70         french\n## 39       l    9        6.31         french\n## 40       o   10        4.74         french\n## 41       d   11        4.14         french\n## 42       c   12        3.00         french\n## 43       p   13        2.43         french\n## 44       m   14        2.39         french\n## 45       é   15        1.84         french\n## 46       v   16        1.42         french\n## 47       b   17        1.08         french\n## 48       f   18        1.08         french\n## 49       g   19        0.96         french\n## 50       q   20        0.82         french\n## 51       x   21        0.57         french\n## 52       è   22        0.51         french\n## 53       h   23        0.51         french\n## 54       y   24        0.44         french\n## 55       à   25        0.40         french\n## 56       j   26        0.37         french\n## 57       z   27        0.18         french\n## 58       w   28        0.14         french\n## 59       î   29        0.11         french\n## 60       k   30        0.11         french\n## 61       ô   31        0.08         french\n## 62       ç   32        0.03         french\n## 63       ê   33        0.01         french\n## 64       ï   34        0.01         french\n## 65       ö   35        0.01         french\n## 66       ù   36        0.01         french\n## 67       e    1       16.58         german\n## 68       n    2        9.00         german\n## 69       r    3        8.05         german\n## 70       a    4        6.71         german\n## 71       d    5        6.55         german\n## 72       t    6        6.47         german\n## 73       s    7        6.38         german\n## 74       i    8        6.35         german\n## 75       u    9        4.63         german\n## 76       h   10        4.44         german\n## 77       g   11        3.78         german\n## 78       l   12        3.09         german\n## 79       c   13        2.80         german\n## 80       m   14        2.49         german\n## 81       o   15        2.29         german\n## 82       f   16        2.28         german\n## 83       b   17        2.12         german\n## 84       w   18        1.11         german\n## 85       v   19        0.85         german\n## 86       z   20        0.84         german\n## 87       ü   21        0.77         german\n## 88       p   22        0.65         german\n## 89       k   23        0.63         german\n## 90       ä   24        0.34         german\n## 91       ß   25        0.33         german\n## 92       ö   26        0.26         german\n## 93       j   27        0.19         german\n## 94       y   28        0.01         german\n\n\nCertain things pop out of this plot: the rankings of the German and Luxembourguish languages are more similar than the rankings of French and Luxembourguish, but overall, the three languages have practically the same top 10 characters. Using the same base as the BÉPO layout should be comfortable enough, but the characters h and g, which are not very common in French, are much more common in Luxembourguish, and should thus be better placed. I would advise against using the German ergonomic/optimized layout, however, because as I said in the beginning, French is still probably the most written language, certainly more often written than German. So even though the frequencies of characters are very similar between Luxembourguish and German, I would still prefer to use the French BÉPO layout.\n\n\nI don’t know if there ever will be an ergonomic/optimized layout for Luxembourguish, but I sure hope that more and more people will start using layouts such as the BÉPO, which are really great to use. It takes some time to get used to, but in general in about one week of usage, maybe two, you should be as fast as you were on the legacy layout."
  },
  {
    "objectID": "posts/2019-01-31-newspapers_shiny_app.html",
    "href": "posts/2019-01-31-newspapers_shiny_app.html",
    "title": "Using Data Science to read 10 years of Luxembourguish newspapers from the 19th century",
    "section": "",
    "text": "I have been playing around with historical newspaper data (see here and here). I have extracted the data from the largest archive available, as described in the previous blog post, and now created a shiny dashboard where it is possible to visualize the most common words per article, as well as read a summary of each article. The summary was made using a method called textrank, using the {textrank} package, which extracts relevant sentences using the Pagerank (developed by Google) algorithm. You can read the scientific paper here for more info.\n\n\nYou can play around with the app by clicking here. In the next blog post, I will explain how I created the app, step by step. It’s going to be a long blog post!\n\n\nUsing the app, I noticed that some war happened around November 1860 in China, which turned out to be the Second Opium War. The war actually ended in October 1860, but apparently the news took several months to travel to Europe.\n\n\nI also learned that already in the 1861, there was public transportation between some Luxembourguish villages, and French villages that were by the border (see the publication from the 17th of December 1861).\n\n\nLet me know if you find about historical events using my app!"
  },
  {
    "objectID": "posts/2019-10-05-parallel_maxlik.html#intro",
    "href": "posts/2019-10-05-parallel_maxlik.html#intro",
    "title": "Split-apply-combine for Maximum Likelihood Estimation of a linear model",
    "section": "\nIntro\n",
    "text": "Intro\n\n\nMaximum likelihood estimation is a very useful technique to fit a model to data used a lot in econometrics and other sciences, but seems, at least to my knowledge, to not be so well known by machine learning practitioners (but I may be wrong about that). Other useful techniques to confront models to data used in econometrics are the minimum distance family of techniques such as the general method of moments or Bayesian approaches, while machine learning practitioners seem to favor the minimization of a loss function (the mean squared error in the case of linear regression for instance).\n\n\nWhen I taught at the university, students had often some problems to understand the technique. It is true that it is not as easy to understand as ordinary least squares, but I’ll try to explain to the best of my abilities.\n\n\nGiven a sample of data, what is the unknown probability distribution that most likely generated it? For instance, if your sample only contains 0’s and 1’s, and the proportion of 1’s is 80%, what do you think is the most likely distribution that generated it? The probability distribution that most likely generated such a dataset is a binomial distribution with probability of success equal to 80%. It might have been a binomial distribution with probability of success equal to, say, 60%, but the most likely one is one with probability of success equal to 80%.\n\n\nTo perform maximum likelihood estimation, one thus needs to assume a certain probability distribution, and then look for the parameters that maximize the likelihood that this distribution generated the observed data. So, now the question is, how to maximize this likelihood? And mathematically speaking, what is a likelihood?"
  },
  {
    "objectID": "posts/2019-10-05-parallel_maxlik.html#some-theory",
    "href": "posts/2019-10-05-parallel_maxlik.html#some-theory",
    "title": "Split-apply-combine for Maximum Likelihood Estimation of a linear model",
    "section": "\nSome theory\n",
    "text": "Some theory\n\n\nFirst of all, let’s assume that each observation from your dataset not only was generated from the same distribution, but that each observation is also independent from each other. For instance, if in your sample you have data on people’s wages and socio-economic background, it is safe to assume, under certain circumstances, that the observations are independent.\n\n\nLet (X_i) be random variables, and (x_i) be their realizations (actual observed values). Let’s assume that the (X_i) are distributed according to a certain probability distribution (D) with density (f()) where () is a parameter of said distribution. Because our sample is composed of i.i.d. random variables, the probability that it was generated by our distribution (D()) is:\n\n\n[_{i=1}^N Pr(X_i = x_i)]\n\n\nIt is customary to take the log of this expression:\n\n\n[({i=1}^N Pr(X_i = x_i)) = {i=1}^N (Pr(X_i = x_i))]\n\n\nThe expression above is called the log-likelihood, (logL(; x_1, …, x_N)). Maximizing this function yields (^*), the value of the parameter that makes the sample the most probable. In the case of linear regression, the density function to use is the one from the Normal distribution."
  },
  {
    "objectID": "posts/2019-10-05-parallel_maxlik.html#maximum-likelihood-of-the-linear-model-as-an-example-of-the-split-apply-combine-strategy",
    "href": "posts/2019-10-05-parallel_maxlik.html#maximum-likelihood-of-the-linear-model-as-an-example-of-the-split-apply-combine-strategy",
    "title": "Split-apply-combine for Maximum Likelihood Estimation of a linear model",
    "section": "\nMaximum likelihood of the linear model as an example of the split-apply-combine strategy\n",
    "text": "Maximum likelihood of the linear model as an example of the split-apply-combine strategy\n\n\nHadley Wickham’s seminal paper, The Split-Apply-Combine Strategy for Data Analysis presents the split-apply-combine strategy, which should remind the reader of the map-reduce framework from Google. The idea is to recognize that in some cases big problems are simply an aggregation of smaller problems. This is the case for Maximum Likelihood Estimation of the linear model as well. The picture below illustrates how Maximum Likelihood works, in the standard case:\n\n\n\n\n\nLet’s use R to do exactly this. Let’s first start by simulating some data:\n\nlibrary(\"tidyverse\")\nsize &lt;- 500000\n\nx1 &lt;- rnorm(size)\nx2 &lt;- rnorm(size)\nx3 &lt;- rnorm(size)\n\ndep_y &lt;- 1.5 + 2*x1 + 3*x2 + 4*x3 + rnorm(size)\n\nx_data &lt;- cbind(dep_y, 1, x1, x2, x3)\n\nx_df &lt;- as.data.frame(x_data) %&gt;%\n  rename(iota = V2)\n\nhead(x_df)\n##       dep_y iota         x1          x2         x3\n## 1  1.637044    1  0.2287198  0.91609653 -0.4006215\n## 2 -1.684578    1  1.2780291 -0.02468559 -1.4020914\n## 3  1.289595    1  1.0524842  0.30206515 -0.3553641\n## 4 -3.769575    1 -2.5763576  0.13864796 -0.3181661\n## 5 13.110239    1 -0.9376462  0.77965301  3.0351646\n## 6  5.059152    1  0.7488792 -0.10049061  0.1307225\n\nNow that this is done, let’s write a function to perform Maximum Likelihood Estimation:\n\nloglik_linmod &lt;- function(parameters, x_data){\n  sum_log_likelihood &lt;- x_data %&gt;%\n    mutate(log_likelihood =\n             dnorm(dep_y,\n                   mean = iota*parameters[1] + x1*parameters[2] + x2*parameters[3] + x3*parameters[4],\n                   sd = parameters[5],\n                   log = TRUE)) %&gt;%\n    summarise(sum(log_likelihood))\n\n  -1 * sum_log_likelihood\n}\n\nThe function returns minus the log likelihood, because optim() which I will be using to optimize the log-likelihood function minimizes functions by default (minimizing the opposite of a function is the same as maximizing a function). Let’s optimize the function and see if we’re able to find the parameters of the data generating process, 1.5, 2, 3, 4 and 1 (the standard deviation of the error term):\n\noptim(c(1,1,1,1,1), loglik_linmod, x_data = x_df)\n\nWe successfully find the parameters of our data generating process!\n\n\nNow, what if I’d like to distribute the computation of the contribution to the likelihood of each observations across my 12 cores? The goal is not necessarily to speed up the computations but to be able to handle larger than RAM data. If I have data that is too large to fit in memory, I could split it into chunks, compute the contributions to the likelihood of each chunk, sum everything again, and voila! This is illustrated below:\n\n\n\n\n\nTo do this, I use the {disk.frame} package, and only need to change my loglik_linmod() function slightly:\n\nlibrary(\"disk.frame\")\nx_diskframe &lt;- as.disk.frame(x_df) #Convert the data frame to a disk.frame\n\nloglik_linmod_df &lt;- function(parameters, x_data){\n  sum_log_likelihood &lt;- x_data %&gt;%\n    mutate(log_likelihood =\n             dnorm(dep_y,\n                   mean = iota*parameters[1] + x1*parameters[2] + x2*parameters[3] + x3*parameters[4],\n                   sd = parameters[5],\n                   log = TRUE)) %&gt;% \n    chunk_summarise(sum(log_likelihood))\n\n  out &lt;- sum_log_likelihood %&gt;%\n    collect() %&gt;%\n    pull() %&gt;%\n    sum()\n\n  -out\n}\n\nThe function is applied to each chunk, and chunk_summarise() computes the sum of the contributions inside each chunk. Thus, I first need to use collect() to transfer the chunk-wise sums in memory and then use pull() to convert it to an atomic vector, and finally sum them all again.\n\n\nLet’s now optimize this function:\n\noptim(rep(1, 5), loglik_linmod_df, x_data = x_diskframe)\n## $par\n## [1] 1.5351722 1.9566144 3.0067978 4.0202956 0.9889412\n## \n## $value\n## [1] 709977.2\n## \n## $counts\n## function gradient \n##      502       NA \n## \n## $convergence\n## [1] 1\n## \n## $message\n## NULL\n\nThis is how you can use the split-apply-combine approach for maximum likelihood estimation of a linear model! This approach is quite powerful, and the familiar map() and reduce() functions included in {purrr} can also help with this task. However, this only works if you can split your problem into chunks, which is sometimes quite hard to achieve.\n\n\nHowever, as usual, there is rarely a need to write your own functions, as {disk.frame} includes the dfglm() function which can be used to estimate any generalized linear model using disk.frame objects!"
  },
  {
    "objectID": "posts/2018-01-05-lists_all_the_way2.html",
    "href": "posts/2018-01-05-lists_all_the_way2.html",
    "title": "It’s lists all the way down, part 2: We need to go deeper",
    "section": "",
    "text": "Shortly after my previous blog post, I saw this tweet on my timeline:\n\n\n\nThe purrr resolution for 2018 - learn at least one purrr function per week - is officially launched with encouragement and inspiration from @statwonk and @hadleywickham. We start with modify_depth: https://t.co/dCMnSHP7Pl. Please join to learn and share. #rstats\n\n— Isabella R. Ghement (@IsabellaGhement) January 3, 2018\n\n\n\nThis is a great initiative, and a big coincidence, as I just had blogged about nested lists and how to map over them. I also said this in my previous blog post:\n\n\n\nThere is also another function that you might want to study, modify_depth() which solves related issues but I will end the blog post here. I might talk about it in a future blog post.\n\n\n\nAnd so after I got this reply from @IsabellaGhement:\n\n\n\nBruno, I would love it if you would chime in with an explicit contrast between nested map calls (which I personally find a bit clunky) and alternatives. In other words, present solutions side-by-side and highlight pros and cons. That would be very useful! 🤗\n\n— Isabella R. Ghement (@IsabellaGhement) January 4, 2018\n\n\n\nWhat else was I supposed to do than blog about purrr::modify_depth()?\n\n\nBear in mind that I was not really familiar with this function before writing my last blog post; and even then, I decided to keep it for another blog post, which is this one. Which came much faster than what I had originally planned. So I might have missed some functionality; if that’s the case don’t hesitate to tweet me an example or send me an email! (bruno at brodrigues dot co)\n\n\nSo what is this blog post about? It’s about lists, nested lists, and some things that you can do with them. Let’s use the same example as in my last post:\n\nlibrary(tidyverse)\ndata(mtcars)\n\nnice_function = function(df, param1, param2){\n  df = df %&gt;%\n    filter(cyl == param1, am == param2) %&gt;%\n    mutate(result = mpg * param1 * (2 - param2))\n\n  return(df)\n}\n\nnice_function(mtcars, 4, 0)\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\nvalues_cyl = c(4, 6, 8)\n\nvalues_am = c(0, 1)\n\nNow that we’re here, we would like to apply nice_function() to each element of values_cyl and values_am. In essence, loop over these values. But because loops are not really easy to manipulate, (as explained, in part, here) I use the map* family of functions included in purrr (When I teach R, I only show loops in the advanced topics chapter of my notes). So let’s “loop” over values_cyl and values_am with map() (and not map_df(); there is a reason for this, bear with me):\n\n(result = map(values_am, ~map(values_cyl, nice_function, df = mtcars, param2 = .)))\n## [[1]]\n## [[1]][[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\n## \n## [[1]][[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## \n## [[1]][[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## \n## \n## [[2]]\n## [[2]][[1]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## \n## [[2]][[2]]\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2\n## \n## [[2]][[3]]\n##    mpg cyl disp  hp drat   wt qsec vs am gear carb result\n## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4\n## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0\n\nUntil now, nothing new compared to my previous post (so if you have a hard time to follow what I’m doing here, go read it here).\n\n\nAs far as I know, there is no way, in this example, to avoid this nested map call. However, suppose now that you want to apply a function to each single data frame contained in the list result. Of course, here, you could simply use bind_rows() to have a single data frame and then apply your function to it. But suppose that you want to keep this list structure; at the end, I will give an example of why you might want that, using another purrr function, walk() and Thomas’ J. Leeper brilliant rio package.\n\n\nSo suppose you want to use this function here:\n\ndouble_col = function(dataset, col){\n  col = enquo(col)\n  col_name = paste0(\"double_\", quo_name(col))\n  dataset %&gt;%\n    mutate(!!col_name := 2*(!!col))\n}\n\nto double the values of a column of a dataset. It uses tidyeval’s enquo(), quo_name() and !!() functions to make it work with tidyverse functions such as mutate(). You can use it like this:\n\ndouble_col(mtcars, hp)\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb double_hp\n## 1  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4       220\n## 2  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4       220\n## 3  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1       186\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1       220\n## 5  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2       350\n## 6  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1       210\n## 7  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4       490\n## 8  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2       124\n## 9  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2       190\n## 10 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4       246\n## 11 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4       246\n## 12 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3       360\n## 13 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3       360\n## 14 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3       360\n## 15 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4       410\n## 16 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4       430\n## 17 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4       460\n## 18 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1       132\n## 19 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2       104\n## 20 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1       130\n## 21 21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1       194\n## 22 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2       300\n## 23 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2       300\n## 24 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4       490\n## 25 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2       350\n## 26 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1       132\n## 27 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2       182\n## 28 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2       226\n## 29 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4       528\n## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6       350\n## 31 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8       670\n## 32 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2       218\n\nNice, but you want to use this function on all of the data frames contained in your result list. You can use a nested map() as before:\n\nmap(result, ~map(., .f = double_col, col = disp))\n## [[1]]\n## [[1]][[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2\n## \n## [[1]][[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8       516.0\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2       450.0\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4       335.2\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6       335.2\n## \n## [[1]][[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2       720.0\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8       720.0\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4       551.6\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8       551.6\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2       551.6\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4       944.0\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4       920.0\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2       880.0\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0       636.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2       608.0\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8       700.0\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2       800.0\n## \n## \n## [[2]]\n## [[2]][[1]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2       216.0\n## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6       157.4\n## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6       151.4\n## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6       142.2\n## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2       158.0\n## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0       240.6\n## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6       190.2\n## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6       242.0\n## \n## [[2]][[2]]\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0         320\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0         320\n## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2         290\n## \n## [[2]][[3]]\n##    mpg cyl disp  hp drat   wt qsec vs am gear carb result double_disp\n## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4         702\n## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0         602\n\nbut there’s an easier solution, which is using modify_depth():\n\n(result = modify_depth(result, .depth = 2, double_col, col = disp))\n## [[1]]\n## [[1]][[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2\n## \n## [[1]][[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8       516.0\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2       450.0\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4       335.2\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6       335.2\n## \n## [[1]][[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2       720.0\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8       720.0\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4       551.6\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8       551.6\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2       551.6\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4       944.0\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4       920.0\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2       880.0\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0       636.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2       608.0\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8       700.0\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2       800.0\n## \n## \n## [[2]]\n## [[2]][[1]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2       216.0\n## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6       157.4\n## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6       151.4\n## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6       142.2\n## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2       158.0\n## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0       240.6\n## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6       190.2\n## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6       242.0\n## \n## [[2]][[2]]\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0         320\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0         320\n## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2         290\n## \n## [[2]][[3]]\n##    mpg cyl disp  hp drat   wt qsec vs am gear carb result double_disp\n## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4         702\n## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0         602\n\nSo how does it work? modify_depth() needs a list and a .depth argument, which corresponds to where you you want to apply your function. The following lines of code might help you understand:\n\n# Depth of 1:\n\nresult[[1]]\n## [[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2\n## \n## [[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8       516.0\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2       450.0\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4       335.2\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6       335.2\n## \n## [[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2       720.0\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8       720.0\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4       551.6\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8       551.6\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2       551.6\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4       944.0\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4       920.0\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2       880.0\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0       636.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2       608.0\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8       700.0\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2       800.0\n\nIn this example, a depth of 1 corresponds to a list of three data frame. Can you use your function double_col() on a list of three data frames? No, because the domain of double_col() is the set of data frames, not the set of lists of data frames. So you need to go deeper:\n\n# Depth of 2:\n\nresult[[1]][[1]] # or try result[[1]][[2]] or result[[1]][[3]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2\n\nAt the depth of 2, you’re dealing with data frames! So you can use your function double_col(). With a depth of 2, one might not see the added value of modify_depth() over nested map calls, but if you have to go even deeper, nested map calls are very confusing and verbose.\n\n\nNow for the last part; why doing all this, and not simply bind all the rows, apply double_col() and call it a day? Well, suppose that there is a reason you have these data frames inside lists; for example, the first element, i.e., result[[1]] might be data for, say, Portugal, for 3 different years. result[[2]] however, is data for France, for the same years. Suppose also that you have to give this data, after having worked on it, to a colleague (or to another institution) in the Excel format; one Excel workbook per country, one sheet per year. This example might seem contrived, but I have been confronted to this exact situation very often. Well, if you bind all the rows together, how are you going to save the data in the workbooks like you are required to?\n\n\nWell, thanks to rio, one line of code is enough:\n\nlibrary(rio)\n\nwalk2(result, list(\"portugal.xlsx\", \"france.xlsx\"), export)\n\nI know what you’re thinking; Bruno, that’s two lines of code!. Yes, but I had to load rio. Also, walk() (and walk2()) are basically the same as map(), but you use walk() over map() when you are only interested in the side effect of the function you are applying over your list; here, export() which is rio’s function to write data to disk. The side effect of this function is… writing data to disk! You could have used map2() just the same, but I wanted to show you walk2() (however, you cannot replace map() by walk() in most cases; try it and see what happens).\n\n\nHere’s what it looks like:\n\n\n\n\n\nI have two Excel workbooks, (one per list), where each sheet is a data frame!"
  },
  {
    "objectID": "posts/2022-11-16-open_source_repro.html",
    "href": "posts/2022-11-16-open_source_repro.html",
    "title": "Open source is a hard requirement for reproducibility",
    "section": "",
    "text": "Open source is a hard requirement for reproducibility.\n\n\nNo ifs nor buts. And I’m not only talking about the code you typed for your research paper/report/analysis. I’m talking about the whole ecosystem that you used to type your code.\n\n\n(I won’t be talking about making the data available, because I think this is another blog post on its own.)\n\n\nIs your code open? That’s good. But is it code for a proprietary program, like STATA, SAS or MATLAB? Then your project is not reproducible. It doesn’t matter if this code is well documented and written and available on Github. This project is not reproducible.\n\n\nWhy?\n\n\nBecause there is on way to re-execute your code with the exact same version of this proprietary program down the line. As I’m writing these lines, MATLAB, for example, is at version R2022b. And it is very unlikely that you can buy version, say, R2008a. Maybe you can. Maybe MATLAB offers this option. But maybe they don’t. And maybe if they do today, they won’t in the future. There’s no guarantee. And if you’re running old code written for version R2008a, there’s no guarantee that it will produce the exact same results on version 2022b. And let’s not even mention the toolboxes (if you’re not familiar with MATLAB’s toolboxes, they’re the equivalent of packages or libraries in other programming languages). These evolve as well, and there’s no guarantee that you can purchase older versions of said toolboxes. And also, is a project truly reproducible (even if old programs can be purchased) if it’s behind a paywall?\n\n\nAnd let me be clear, what I’m describing here with MATLAB could also be said for any other proprietary programs still commonly (unfortunately) used in research and in statistics (like STATA or SAS).\n\n\nThen there’s another problem: let’s suppose you’ve written a nice, thoroughly tested and documented script, and made it available on Github (and let’s even assume that the data is available for people to freely download, and that the paper is open access). Let’s assume further that you’ve used R or Python, or any other open source programming language. Could this study/analysis be said to be reproducible? Well, if the analysis ran on a proprietary operating system, then the conclusion is: your project is not reproducible.\n\n\nThis is because the operating system the code runs on can also influence the reproducibility of the project. There are some specificities in operating systems that may make certain things work differently. Admittedly, this is in practice rarely a problem, but it does happen, especially if you’re working with very high precision floating point arithmetic.\n\n\nSo where does that leave us? Basically, for something to be truly reproducible, it has to respect the following bullet points:\n\n\n\nSource code must obviously to be available and thoroughly tested and document;\n\n\nTo be written with an open source programming language (nocode tools are by default non-reproducible and belong in the trash);\n\n\nThe project needs to be run on an open source operating system.\n\n\n(Data and paper need obviously to be accessible as well)\n\n\n\nAnd the whole thing would ideally be packaged using Docker or Podman. This means that someone could run an analysis in a single command, like:\n\ndocker run --rm --name my_analysis_container researchers_name/reproducible_project\n\nWhere reproducible_project is a Docker image, which would not only be based (very often) on the Ubuntu operating system (the most popular Linux distribution) but also contain, already installed and ready-to-use, the required programming language and the required libraries to run the project. Also, usually, the researcher would have added the required scripts and commands such that the command above, automatically, and without any further input, reruns the whole analysis. The entry cost to Docker (or similar tools) might seem high, but it is worth it, and the only way to have a truly 100% reproducible pipeline. If you’re using the R programming language for your analyses, you can use the pre-built Docker images from the amazing Rocker project. If you’re interested, I show how you can build a reproducible pipeline using these images in this chapter of my course I teach at university (as of writing this blog post, this chapter is not complete yet, but it will be by Sunday evening at the latest, as I have to teach this on Monday morning at the University).\n\n\nOpen source programming languages and libraries can be dockerized and the Docker images can be distributed. Maybe one day we will always have a Docker image alongside a research paper.\n\n\nOne can dream."
  },
  {
    "objectID": "posts/2019-05-04-diffindiff_part2.html",
    "href": "posts/2019-05-04-diffindiff_part2.html",
    "title": "Fast food, causality and R packages, part 2",
    "section": "",
    "text": "I am currently working on a package for the R programming language; its initial goal was to simply distribute the data used in the Card and Krueger 1994 paper that you can read here (PDF warning). However, I decided that I would add code to perform diff-in-diff.\n\n\nIn my previous blog post I showed how to set up the structure of your new package. In this blog post, I will only focus on getting Card and Krueger’s data and prepare it for distribution. The next blog posts will focus on writing a function to perform difference-in-differences.\n\n\nIf you want to distribute data through a package, you first need to use the usethis::use_data_raw() function (as shown in part 1).\n\n\nThis creates a data-raw folder, and inside you will find the DATASET.R script. You can edit this script to prepare the data.\n\n\nFirst, let’s download the data from Card’s website, unzip it and load the data into R. All these operations will be performed from R:\n\nlibrary(tidyverse)\n\ntempfile_path &lt;- tempfile()\n\ndownload.file(\"http://davidcard.berkeley.edu/data_sets/njmin.zip\", destfile = tempfile_path)\n\ntempdir_path &lt;- tempdir()\n\nunzip(tempfile_path, exdir = tempdir_path)\n\nTo download and unzip a file from R, first, you need to define where you want to save the file. Because I am not interested in keeping the downloaded file, I use the tempfile() function to get a temporary file in my /tmp/ folder (which is the folder that contains temporary files and folders in a GNU+Linux system). Then, using download.file() I download the file, and save it in my temporary file. I then create a temporary directory using tempdir() (the idea is the same as with tempfile()), and use this folder to save the files that I will unzip, using the unzip() function. This folder now contains several files:\n\ncheck.sas\ncodebook\npublic.csv\nread.me\nsurvey1.nj\nsurvey2.nj\n\ncheck.sas is the SAS script Card and Krueger used. It’s interesting, because it is quite simple, quite short (170 lines long) and yet the impact of Card and Krueger’s research was and has been very important for the field of econometrics. This script will help me define my own functions. codebook, you guessed it, contains the variables’ descriptions. I will use this to name the columns of the data and to write the dataset’s documentation.\n\n\npublic.csv is the data. It does not contain any column names:\n\n 46 1 0 0 0 0 0 1 0 0  0 30.00 15.00  3.00   .    19.0   .   1    .  2  6.50 16.50  1.03  1.03  0.52  3  3 1 1 111792  1  3.50 35.00  3.00  4.30  26.0  0.08 1 2  6.50 16.50  1.03   .    0.94  4  4    \n 49 2 0 0 0 0 0 1 0 0  0  6.50  6.50  4.00   .    26.0   .   0    .  2 10.00 13.00  1.01  0.90  2.35  4  3 1 1 111292  .  0.00 15.00  4.00  4.45  13.0  0.05 0 2 10.00 13.00  1.01  0.89  2.35  4  4    \n506 2 1 0 0 0 0 1 0 0  0  3.00  7.00  2.00   .    13.0  0.37 0  30.0 2 11.00 10.00  0.95  0.74  2.33  3  3 1 1 111292  .  3.00  7.00  4.00  5.00  19.0  0.25 . 1 11.00 11.00  0.95  0.74  2.33  4  3    \n 56 4 1 0 0 0 0 1 0 0  0 20.00 20.00  4.00  5.00  26.0  0.10 1   0.0 2 10.00 12.00  0.87  0.82  1.79  2  2 1 1 111492  .  0.00 36.00  2.00  5.25  26.0  0.15 0 2 10.00 12.00  0.92  0.79  0.87  2  2    \n 61 4 1 0 0 0 0 1 0 0  0  6.00 26.00  5.00  5.50  52.0  0.15 1   0.0 3 10.00 12.00  0.87  0.77  1.65  2  2 1 1 111492  . 28.00  3.00  6.00  4.75  13.0  0.15 0 2 10.00 12.00  1.01  0.84  0.95  2  2    \n 62 4 1 0 0 0 0 1 0 0  2  0.00 31.00  5.00  5.00  26.0  0.07 0  45.0 2 10.00 12.00  0.87  0.77  0.95  2  2 1 1 111492  .   .     .     .     .    26.0   .   0 2 10.00 12.00   .    0.84  1.79  3  3    \n\nMissing data is defined by . and the delimiter is the space character. read.me is a README file. Finally, survey1.nj and survey2.nj are the surveys that were administered to the fast food restaurants’ managers; one in February (before the raise) and the second one in November (after the minimum wage raise).\n\n\nThe next lines import the codebook:\n\ncodebook &lt;- read_lines(file = paste0(tempdir_path, \"/codebook\"))\n\nvariable_names &lt;- codebook %&gt;%\n    `[`(8:59) %&gt;%\n    `[`(-c(5, 6, 13, 14, 32, 33)) %&gt;%\n    str_sub(1, 13) %&gt;%\n    str_squish() %&gt;%\n    str_to_lower()\n\nOnce I import the codebook, I select lines 8 to 59 using the [() function. If you’re not familiar with this notation, try the following in a console:\n\nseq(1, 100)[1:10]\n##  [1]  1  2  3  4  5  6  7  8  9 10\n\nand compare:\n\nseq(1, 100) %&gt;% \n  `[`(., 1:10)\n##  [1]  1  2  3  4  5  6  7  8  9 10\n\nboth are equivalent, as you can see. You can also try the following:\n\n1 + 10\n## [1] 11\n1 %&gt;% \n  `+`(., 10)\n## [1] 11\n\nUsing the same trick, I remove lines that I do not need, and then using stringr::str_sub(1, 13) I only keep the first 13 characters (which are the variable names, plus some white space characters) and then, to remove all the unneeded white space characters I use stringr::squish(), and then change the column names to lowercase.\n\n\nI then load the data, and add the column names that I extracted before:\n\ndataset &lt;- read_table2(paste0(tempdir_path, \"/public.dat\"),\n                      col_names = FALSE)\n\ndataset &lt;- dataset %&gt;%\n    select(-X47) %&gt;%\n    `colnames&lt;-`(., variable_names) %&gt;%\n    mutate_all(as.numeric) %&gt;%\n    mutate(sheet = as.character(sheet))\n\nI use the same trick as before. I rename the 47th column, which is empty, I name the columns with colnames&lt;-().\n\n\nAfter this, I perform some data cleaning. It’s mostly renaming categories of categorical variables, and creating a “true” panel format. Several variables were measured at several points in time. Variables that were measured a second time have a “2” at the end of their name. I remove these variables, and add an observation data variable. So my data as twice as many rows as the original data, but that format makes it way easier to work with. Below you can read the full code:\n\n\n\n\nClick if you want to see the code\n\n\ndataset &lt;- dataset %&gt;%\n    mutate(chain = case_when(chain == 1 ~ \"bk\",\n                             chain == 2 ~ \"kfc\",\n                             chain == 3 ~ \"roys\",\n                             chain == 4 ~ \"wendys\")) %&gt;%\n    mutate(state = case_when(state == 1 ~ \"New Jersey\",\n                             state == 0 ~ \"Pennsylvania\")) %&gt;%\n    mutate(region = case_when(southj == 1 ~ \"southj\",\n              centralj == 1 ~ \"centralj\",\n              northj == 1 ~ \"northj\",\n              shore == 1 ~ \"shorej\",\n              pa1 == 1 ~ \"pa1\",\n              pa2 == 1 ~ \"pa2\")) %&gt;%\n    mutate(meals = case_when(meals == 0 ~ \"None\",\n                             meals == 1 ~ \"Free meals\",\n                             meals == 2 ~ \"Reduced price meals\",\n                             meals == 3 ~ \"Both free and reduced price meals\")) %&gt;%\n    mutate(meals2 = case_when(meals2 == 0 ~ \"None\",\n                             meals2 == 1 ~ \"Free meals\",\n                             meals2 == 2 ~ \"Reduced price meals\",\n                             meals2 == 3 ~ \"Both free and reduced price meals\")) %&gt;%\n    mutate(status2 = case_when(status2 == 0 ~ \"Refused 2nd interview\",\n                               status2 == 1 ~ \"Answered 2nd interview\",\n                               status2 == 2 ~ \"Closed for renovations\",\n                               status2 == 3 ~ \"Closed permanently\",\n                               status2 == 4 ~ \"Closed for highway construction\",\n                               status2 == 5 ~ \"Closed due to Mall fire\")) %&gt;%\n    mutate(co_owned = if_else(co_owned == 1, \"Yes\", \"No\")) %&gt;%\n    mutate(bonus = if_else(bonus == 1, \"Yes\", \"No\")) %&gt;%\n    mutate(special2 = if_else(special2 == 1, \"Yes\", \"No\")) %&gt;%\n    mutate(type2 = if_else(type2 == 1, \"Phone\", \"Personal\")) %&gt;%\n    select(sheet, chain, co_owned, state, region, everything()) %&gt;%\n    select(-southj, -centralj, -northj, -shore, -pa1, -pa2) %&gt;%\n    mutate(date2 = lubridate::mdy(date2)) %&gt;%\n    rename(open2 = open2r) %&gt;%\n    rename(firstinc2 = firstin2)\n\ndataset1 &lt;- dataset %&gt;%\n    select(-ends_with(\"2\"), -sheet, -chain, -co_owned, -state, -region, -bonus) %&gt;%\n    mutate(type = NA_character_,\n           status = NA_character_,\n           date = NA)\n\ndataset2 &lt;- dataset %&gt;%\n    select(ends_with(\"2\")) %&gt;%\n    #mutate(bonus = NA_character_) %&gt;%\n    rename_all(~str_remove(., \"2\"))\n\nother_cols &lt;- dataset %&gt;%\n    select(sheet, chain, co_owned, state, region, bonus)\n\nother_cols_1 &lt;- other_cols %&gt;%\n    mutate(observation = \"February 1992\")\n\nother_cols_2 &lt;- other_cols %&gt;%\n    mutate(observation = \"November 1992\")\n\ndataset1 &lt;- bind_cols(other_cols_1, dataset1)\ndataset2 &lt;- bind_cols(other_cols_2, dataset2)\n\nnjmin &lt;- bind_rows(dataset1, dataset2) %&gt;%\n    select(sheet, chain, state, region, observation, everything())\n\n\nThe line I would like to comment is the following:\n\ndataset %&gt;%\n    select(-ends_with(\"2\"), -sheet, -chain, -co_owned, -state, -region, -bonus)\n\nThis select removes every column that ends with the character “2” (among others). I split the data in two, to then bind the rows together and thus create my long dataset. I then save the data into the data/ folder:\n\nusethis::use_data(njmin, overwrite = TRUE)\n\nThis saves the data as an .rda file. To enable users to read the data by typing data(“njmin”), you need to create a data.R script in the R/ folder. You can read my data.R script below:\n\n\n\n\nClick if you want to see the code\n\n\n#' Data from the Card and Krueger 1994 paper *Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania*\n#'\n#' This dataset was downloaded and distributed with the permission of David Card. The original\n#' data contains 410 observations and 46 variables. The data distributed in this package is\n#' exactly the same, but was changed from a wide to a long dataset, which is better suited for\n#' manipulation with *tidyverse* functions.\n#'\n#' @format A data frame with 820 rows and 28 variables:\n#' \\describe{\n#'   \\item{\\code{sheet}}{Sheet number (unique store id).}\n#'   \\item{\\code{chain}}{The fastfood chain: bk is Burger King, kfc is Kentucky Fried Chicken, wendys is Wendy's, roys is Roy Rogers.}\n#'   \\item{\\code{state}}{State where the restaurant is located.}\n#'   \\item{\\code{region}}{pa1 is northeast suburbs of Phila, pa2 is Easton etc, centralj is central NJ, northj is northern NJ, southj is south NJ.}\n#'   \\item{\\code{observation}}{Date of first (February 1992) and second (November 1992) observation.}\n#'   \\item{\\code{co_owned}}{\"Yes\" if company owned.}\n#'   \\item{\\code{ncalls}}{Number of call-backs. Is 0 if contacted on first call.}\n#'   \\item{\\code{empft}}{Number full-time employees.}\n#'   \\item{\\code{emppt}}{Number part-time employees.}\n#'   \\item{\\code{nmgrs}}{Number of managers/assistant managers.}\n#'   \\item{\\code{wage_st}}{Starting wage ($/hr).}\n#'   \\item{\\code{inctime}}{Months to usual first raise.}\n#'   \\item{\\code{firstinc}}{Usual amount of first raise (\\$/hr).}\n#'   \\item{\\code{bonus}}{\"Yes\" if cash bounty for new workers.}\n#'   \\item{\\code{pctaff}}{\\% of employees affected by new minimum.}\n#'   \\item{\\code{meals}}{Free/reduced priced code.}\n#'   \\item{\\code{open}}{Hour of opening.}\n#'   \\item{\\code{hrsopen}}{Number of hours open per day.}\n#'   \\item{\\code{psode}}{Price of medium soda, including tax.}\n#'   \\item{\\code{pfry}}{Price of small fries, including tax.}\n#'   \\item{\\code{pentree}}{Price of entree, including tax.}\n#'   \\item{\\code{nregs}}{Number of cash registers in store.}\n#'   \\item{\\code{nregs11}}{Number of registers open at 11:00 pm.}\n#'   \\item{\\code{type}}{Type of 2nd interview.}\n#'   \\item{\\code{status}}{Status of 2nd interview.}\n#'   \\item{\\code{date}}{Date of 2nd interview.}\n#'   \\item{\\code{nregs11}}{\"Yes\" if special program for new workers.}\n#' }\n#' @source \\url{http://davidcard.berkeley.edu/data_sets.html}\n\"njmin\"\n\n\nI have documented the data, and using roxygen2::royxgenise() to create the dataset’s documentation.\n\n\nThe data can now be used to create some nifty plots:\n\nggplot(njmin, aes(wage_st)) + geom_density(aes(fill = state), alpha = 0.3) +\n    facet_wrap(vars(observation)) + theme_blog() +\n    theme(legend.title = element_blank(), plot.caption = element_text(colour = \"white\")) +\n    labs(title = \"Distribution of starting wage rates in fast food restaurants\",\n         caption = \"On April 1st, 1992, New Jersey's minimum wage rose from $4.25 to $5.05. Source: Card and Krueger (1994)\")\n## Warning: Removed 41 rows containing non-finite values (stat_density).\n\n\n\n\nIn the next blog post, I am going to write a first function to perform diff and diff, and we will learn how to make it available to users, document and test it!"
  },
  {
    "objectID": "posts/2023-10-20-nix_for_r_part7.html",
    "href": "posts/2023-10-20-nix_for_r_part7.html",
    "title": "Reproducible data science with Nix, part 7 – Building a Quarto book using Nix on Github Actions",
    "section": "",
    "text": "Back in June I self-published a book on Amazon’s Kindle Direct Publishing service and wrote a blog post detailling how you could achieve that using Quarto, which you can read here. The book is about building reproducible analytical pipelines with R. For the purposes of this post I made a template on Github that you could fork and use as a starting point to write your own book. The book also gets built using Github Actions each time you push new changes: a website gets built, an E-book for e-ink devices and a Amazon KDP-ready PDF for print get also built. That template used dedicated actions to install the required version of R, Quarto, and R packages (using {renv}).\n\n\nLet’s take a look at the workflow file:\n\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Checkout repo\n        uses: actions/checkout@v3\n\n      - name: Setup pandoc\n        uses: r-lib/actions/setup-pandoc@v2\n\n      - name: Setup R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: '4.3.1'\n\n      - name: Setup renv\n        uses: r-lib/actions/setup-renv@v2\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          # To install LaTeX to build PDF book \n          tinytex: true \n          # uncomment below and fill to pin a version\n          #version: 1.3.353\n\n      - name: Publish to GitHub Pages (and render)\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # this secret is always available for github actions\n\nAs you can see, there are a lot of different moving pieces to get this to work. Since then I discovered Nix (if you’ve not been following my adventures, there’s 6 other parts to this series as of today), and now I wrote another template that uses Nix to handle the book’s dependencies instead of dedicated actions and {renv}. You can find the repository here.\n\n\nHere is what the workflow file looks like:\n\nname: Build book using Nix\n\non:\n  push:\n    branches:\n      - main\n      - master\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout Code\n      uses: actions/checkout@v3\n\n    - name: Install Nix\n      uses: DeterminateSystems/nix-installer-action@main\n      with:\n        logger: pretty\n        log-directives: nix_installer=trace\n        backtrace: full\n\n    - name: Nix cache\n      uses: DeterminateSystems/magic-nix-cache-action@main\n\n    - name: Build development environment\n      run: |\n        nix-build\n\n    - name: Publish to GitHub Pages (and render)\n      uses: b-rodrigues/quarto-nix-actions/publish@main\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} \n\nThe first thing you should notice is that this file is much shorter.\n\n\nThe first step, Checkout Code makes the code available to the rest of the steps. I then install Nix on this runner using the Determinate Systems nix-installer-action and then I use another action from Determinate Systems, the magic-nix-cache-action. This action caches all the packages so that they don’t need to get re-built each time a change gets pushed, speeding up the process by a lot. The development environment gets then built using nix-build.\n\n\nFinally, an action I defined runs, quarto-nix-actions/publish. This is a fork of the quarto-actions/publish action which you can find here. My fork simply makes sure that the quarto render and quarto publish commands run in the Nix environment defined for the project.\n\n\nYou can see the book website here; read it, it’s explains everything in much more details than this blog post! But if you’re busy, read continue reading this blog post instead.\n\n\nThe obvious next question is why bother with this second, Nix-centric, approach?\n\n\nThere are at least three reasons. The first is that it is possible to define so-called default.nix files that the Nix package manager then uses to build a fully reproducible development environment. This environment will contain all the packages that you require, and will not interfere with any other packages installed on your system. This essentially means that you can have project-specific default.nix files, each specifying the requirements for specific projects. This file can then be used as-is on any other platform to re-create your environment. The second reason is that when installing a package that requires system-level dependencies, {rJava} for example, all the lower-level dependencies get automatically installed as well. Forget about reading error messages of install.packages() to find which system development library you need to install first. The third reason is that you can pin a specific revision of nixpkgs to ensure reproducibility.\n\n\nThe nixpkgs mono-repository is “just” a Github repository which you can find here: https://github.com/NixOS/nixpkgs. This repository contains Nix expressions to build and install more than 80’000 packages and you can search for installable Nix packages here.\n\n\nBecause nixpkgs is a “just” Github repository, it is possible to use a specific commit hash to install the packages as they were at a specific point in time. For example, if you use this commit, 7c9cc5a6e, you’ll get the very latest packages as of the 19th of October 2023, but if you used this one instead: 976fa3369, you’ll get packages from the 19th of August 2023.\n\n\nThis ability to deal with both underlying system-level dependencies and pin package versions at a specific commit is extremely useful on Git(Dev)Ops platforms like Github Actions. Debugging installation failures of packages can be quite frustrating, especially on Github Actions, and especially if you’re not already familiar with how Linux distributions work. Having a tool that handles all of that for you is amazing. The difficult part is writing these default.nix files that the Nix package manager requires to actually build these development environments. But don’t worry, with my co-author Philipp Baumann, we developed an R package called {rix} which generates these default.nix files for you.\n\n\n{rix} is an R package that makes it very easy to generate very complex default.nix files. These files can in turn be used by the Nix package manager to build project-specific environments. The book’s Github repository contains a file called define_env.R with the following content:\n\nlibrary(rix)\n\nrix(r_ver = \"4.3.1\",\n    r_pkgs = c(\"quarto\"),\n    system_pkgs = \"quarto\",\n    tex_pkgs = c(\n      \"amsmath\",\n      \"framed\",\n      \"fvextra\",\n      \"environ\",\n      \"fontawesome5\",\n      \"orcidlink\",\n      \"pdfcol\",\n      \"tcolorbox\",\n      \"tikzfill\"\n    ),\n    ide = \"other\",\n    shell_hook = \"\",\n    project_path = \".\",\n    overwrite = TRUE,\n    print = TRUE)\n\n{rix} ships the rix() function which takes several arguments. These arguments allow you to specify an R version, a list of R packages, a list of system packages, TeXLive packages and other options that allow you to specify your requirements. Running this code generates this default.nix file:\n\n# This file was generated by the {rix} R package v0.4.1 on 2023-10-19\n# with following call:\n# &gt;rix(r_ver = \"976fa3369d722e76f37c77493d99829540d43845\",\n#  &gt; r_pkgs = c(\"quarto\"),\n#  &gt; system_pkgs = \"quarto\",\n#  &gt; tex_pkgs = c(\"amsmath\",\n#  &gt; \"framed\",\n#  &gt; \"fvextra\",\n#  &gt; \"environ\",\n#  &gt; \"fontawesome5\",\n#  &gt; \"orcidlink\",\n#  &gt; \"pdfcol\",\n#  &gt; \"tcolorbox\",\n#  &gt; \"tikzfill\"),\n#  &gt; ide = \"other\",\n#  &gt; project_path = \".\",\n#  &gt; overwrite = TRUE,\n#  &gt; print = TRUE,\n#  &gt; shell_hook = \"\")\n# It uses nixpkgs' revision 976fa3369d722e76f37c77493d99829540d43845 for reproducibility purposes\n# which will install R version 4.3.1\n# Report any issues to https://github.com/b-rodrigues/rix\nlet\n pkgs = import (fetchTarball \"https://github.com/NixOS/nixpkgs/archive/976fa3369d722e76f37c77493d99829540d43845.tar.gz\") {};\n rpkgs = builtins.attrValues {\n  inherit (pkgs.rPackages) quarto;\n};\n  tex = (pkgs.texlive.combine {\n  inherit (pkgs.texlive) scheme-small amsmath framed fvextra environ fontawesome5 orcidlink pdfcol tcolorbox tikzfill;\n});\n system_packages = builtins.attrValues {\n  inherit (pkgs) R glibcLocalesUtf8 quarto;\n};\n  in\n  pkgs.mkShell {\n    LOCALE_ARCHIVE = if pkgs.system == \"x86_64-linux\" then  \"${pkgs.glibcLocalesUtf8}/lib/locale/locale-archive\" else \"\";\n    LANG = \"en_US.UTF-8\";\n    LC_ALL = \"en_US.UTF-8\";\n    LC_TIME = \"en_US.UTF-8\";\n    LC_MONETARY = \"en_US.UTF-8\";\n    LC_PAPER = \"en_US.UTF-8\";\n    LC_MEASUREMENT = \"en_US.UTF-8\";\n\n    buildInputs = [  rpkgs tex system_packages  ];\n  }\n\nThis file defines the environment that is needed to build your book: be it locally on your machine, or on a GitOps platform like Github Actions. All that matters is that you have the Nix package manager installed (thankfully, it’s available for Windows –through WSL2–, Linux and macOS).\n\n\nBeing able to work locally on a specific environment, defined through code, and use that environment on the cloud as well, is great. It doesn’t matter that the code runs on Ubuntu on the Github Actions runner, and if that operating system is not the one you use as well. Thanks to Nix, your code will run on exactly the same environment. Because of that, you can use ubuntu-latest as your runner, because exactly the same packages will always get installed. This is not the case with my first template that uses dedicated actions and {renv}: there, the runner uses ubuntu-22.04, a fixed version of the Ubuntu operating system. The risk here, is that once these runners get decommissioned (Ubuntu 22.04 is a long-term support release of Ubuntu, so it’ll stop getting updated sometime in 2027), my code won’t be able to run anymore. This is because there’s no guarantee that the required version of R, Quarto, and all the other packages I need will be installable on that new release of Ubuntu. So for example, suppose I have the package {foo} at version 1.0 that requires the system-level development library bar-dev at version 0.4 to be installed on Ubuntu. This is not an issue now, as Ubuntu 22.04 ships version 0.4 of bar-dev. But it is very unlikely that the future version of Ubuntu from 2027 will ship that version, and there’s no guarantee my package will successfully build and work as expected with a more recent version of bar-dev. With Nix, this is not an issue; because I pin a specific commit of nixpkgs, not only will {foo} at version 1.0 get installed, its dependency bar-dev at version 0.4 will get installed by Nix as well, and get used to build {foo}. It doesn’t matter that my underlying operating system ships a more recent version of bar-dev. I really insist on this point, because this is not something that you can easily deal with, even with Docker. This is because when you use Docker, you need to be able to rebuild the image as many times as you need (the alternative is to store, forever, the built image), and just like for Github Actions runners, the underlying Ubuntu image will be decommissioned and stop working one day.\n\n\nIn other words, if you need long-term reproducibility, you should really consider using Nix, and even if you don’t need long-term reproducibility, you should really consider using Nix. This is because Nix makes things much easier. But there is one point where Nix is at a huge disadvantage when compared to the alternatives: the entry cost is quite high, as I’ve discussed in my previous blog post. But I’m hoping that through my blog posts, this entry cost is getting lowered for R users!"
  },
  {
    "objectID": "posts/2019-03-20-pivot.html",
    "href": "posts/2019-03-20-pivot.html",
    "title": "Pivoting data frames just got easier thanks to pivot_wide() and pivot_long()",
    "section": "",
    "text": "Update: pivot_wide() and pivot_long() are now called pivot_wider() and pivot_longer(), so the code below needs to be updated accondingly.\n\n\nThere’s a lot going on in the development version of {tidyr}. New functions for pivoting data frames, pivot_wide() and pivot_long() are coming, and will replace the current functions, spread() and gather(). spread() and gather() will remain in the package though:\n\n{{% tweet “1108107722128613377” %}}\n\nIf you want to try out these new functions, you need to install the development version of {tidyr}:\n\ndevtools::install_github(\"tidyverse/tidyr\")\n\nand you can read the vignette here. Because these functions are still being developed, some more changes might be introduced, but I guess that the main functionality will not change much.\n\n\nLet’s play around with these functions and the mtcars data set. First let’s load the packages and the data:\n\nlibrary(tidyverse)\ndata(mtcars)\n\nFirst, let’s create a wide dataset, by spreading the levels of the “am” column to two new columns:\n\nmtcars_wide1 &lt;- mtcars %&gt;% \n    pivot_wide(names_from = \"am\", values_from = \"mpg\") \n\nmtcars_wide1 %&gt;% \n    select(`0`, `1`, everything())\n## # A tibble: 32 x 11\n##      `0`   `1`   cyl  disp    hp  drat    wt  qsec    vs  gear  carb\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1  NA    21       6  160    110  3.9   2.62  16.5     0     4     4\n##  2  NA    21       6  160    110  3.9   2.88  17.0     0     4     4\n##  3  NA    22.8     4  108     93  3.85  2.32  18.6     1     4     1\n##  4  21.4  NA       6  258    110  3.08  3.22  19.4     1     3     1\n##  5  18.7  NA       8  360    175  3.15  3.44  17.0     0     3     2\n##  6  18.1  NA       6  225    105  2.76  3.46  20.2     1     3     1\n##  7  14.3  NA       8  360    245  3.21  3.57  15.8     0     3     4\n##  8  24.4  NA       4  147.    62  3.69  3.19  20       1     4     2\n##  9  22.8  NA       4  141.    95  3.92  3.15  22.9     1     4     2\n## 10  19.2  NA       6  168.   123  3.92  3.44  18.3     1     4     4\n## # … with 22 more rows\n\npivot_wide()’s arguments are quite explicit: names_from = is where you specify the column that will be spread across the data frame, meaning, the levels of this column will become new columns. values_from = is where you specify the column that will fill in the values of the new columns.\n\n\n“0” and “1” are the new columns (“am” had two levels, 0 and 1), which contain the miles per gallon for manual and automatic cars respectively. Let’s also take a look at the data frame itself:\n\nmtcars_wide1 %&gt;% \n    select(`0`, `1`, everything())\n## # A tibble: 32 x 11\n##      `0`   `1`   cyl  disp    hp  drat    wt  qsec    vs  gear  carb\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1  NA    21       6  160    110  3.9   2.62  16.5     0     4     4\n##  2  NA    21       6  160    110  3.9   2.88  17.0     0     4     4\n##  3  NA    22.8     4  108     93  3.85  2.32  18.6     1     4     1\n##  4  21.4  NA       6  258    110  3.08  3.22  19.4     1     3     1\n##  5  18.7  NA       8  360    175  3.15  3.44  17.0     0     3     2\n##  6  18.1  NA       6  225    105  2.76  3.46  20.2     1     3     1\n##  7  14.3  NA       8  360    245  3.21  3.57  15.8     0     3     4\n##  8  24.4  NA       4  147.    62  3.69  3.19  20       1     4     2\n##  9  22.8  NA       4  141.    95  3.92  3.15  22.9     1     4     2\n## 10  19.2  NA       6  168.   123  3.92  3.44  18.3     1     4     4\n## # … with 22 more rows\n\nNow suppose that we want to spread the values of “am” times “cyl”, and filling the data with the values of “mpg”:\n\nmtcars_wide2 &lt;- mtcars %&gt;% \n    pivot_wide(names_from = c(\"am\", \"cyl\"), values_from = \"mpg\") \n\nmtcars_wide2 %&gt;% \n    select(matches(\"^0|1\"), everything())\n## # A tibble: 32 x 14\n##    `1_6` `1_4` `0_6` `0_8` `0_4` `1_8`  disp    hp  drat    wt  qsec    vs\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1    21  NA    NA    NA    NA      NA  160    110  3.9   2.62  16.5     0\n##  2    21  NA    NA    NA    NA      NA  160    110  3.9   2.88  17.0     0\n##  3    NA  22.8  NA    NA    NA      NA  108     93  3.85  2.32  18.6     1\n##  4    NA  NA    21.4  NA    NA      NA  258    110  3.08  3.22  19.4     1\n##  5    NA  NA    NA    18.7  NA      NA  360    175  3.15  3.44  17.0     0\n##  6    NA  NA    18.1  NA    NA      NA  225    105  2.76  3.46  20.2     1\n##  7    NA  NA    NA    14.3  NA      NA  360    245  3.21  3.57  15.8     0\n##  8    NA  NA    NA    NA    24.4    NA  147.    62  3.69  3.19  20       1\n##  9    NA  NA    NA    NA    22.8    NA  141.    95  3.92  3.15  22.9     1\n## 10    NA  NA    19.2  NA    NA      NA  168.   123  3.92  3.44  18.3     1\n## # … with 22 more rows, and 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;\n\nAs you can see, this is easily achieved by simply providing more columns to names_from =.\n\n\nFinally, it is also possible to use an optional data set which contains the specifications of the new columns:\n\nmtcars_spec &lt;- mtcars %&gt;% \n    expand(am, cyl, .value = \"mpg\") %&gt;%\n    unite(\".name\", am, cyl, remove = FALSE)\n\nmtcars_spec\n## # A tibble: 6 x 4\n##   .name    am   cyl .value\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n## 1 0_4       0     4 mpg   \n## 2 0_6       0     6 mpg   \n## 3 0_8       0     8 mpg   \n## 4 1_4       1     4 mpg   \n## 5 1_6       1     6 mpg   \n## 6 1_8       1     8 mpg\n\nThis optional data set defines how the columns “0_4”, “0_6” etc are constructed, and also the value that shall be used to fill in the values. “am” and “cyl” will be used to create the “.name” and the “mpg” column will be used for the “.value”:\n\nmtcars %&gt;% \n    pivot_wide(spec = mtcars_spec) %&gt;% \n    select(matches(\"^0|1\"), everything())\n## # A tibble: 32 x 14\n##    `0_4` `0_6` `0_8` `1_4` `1_6` `1_8`  disp    hp  drat    wt  qsec    vs\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1  NA    NA    NA    NA      21    NA  160    110  3.9   2.62  16.5     0\n##  2  NA    NA    NA    NA      21    NA  160    110  3.9   2.88  17.0     0\n##  3  NA    NA    NA    22.8    NA    NA  108     93  3.85  2.32  18.6     1\n##  4  NA    21.4  NA    NA      NA    NA  258    110  3.08  3.22  19.4     1\n##  5  NA    NA    18.7  NA      NA    NA  360    175  3.15  3.44  17.0     0\n##  6  NA    18.1  NA    NA      NA    NA  225    105  2.76  3.46  20.2     1\n##  7  NA    NA    14.3  NA      NA    NA  360    245  3.21  3.57  15.8     0\n##  8  24.4  NA    NA    NA      NA    NA  147.    62  3.69  3.19  20       1\n##  9  22.8  NA    NA    NA      NA    NA  141.    95  3.92  3.15  22.9     1\n## 10  NA    19.2  NA    NA      NA    NA  168.   123  3.92  3.44  18.3     1\n## # … with 22 more rows, and 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;\n\nUsing a spec is especially useful if you need to make new levels that are not in the data. For instance, suppose that there are actually 10-cylinder cars too, but they do not appear in our sample. We would like to make the fact that they’re missing explicit:\n\nmtcars_spec2 &lt;- mtcars %&gt;% \n    expand(am, \"cyl\" = c(cyl, 10), .value = \"mpg\") %&gt;%\n    unite(\".name\", am, cyl, remove = FALSE)\n\nmtcars_spec2\n## # A tibble: 8 x 4\n##   .name    am   cyl .value\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n## 1 0_4       0     4 mpg   \n## 2 0_6       0     6 mpg   \n## 3 0_8       0     8 mpg   \n## 4 0_10      0    10 mpg   \n## 5 1_4       1     4 mpg   \n## 6 1_6       1     6 mpg   \n## 7 1_8       1     8 mpg   \n## 8 1_10      1    10 mpg\nmtcars %&gt;% \n    pivot_wide(spec = mtcars_spec2) %&gt;% \n    select(matches(\"^0|1\"), everything())\n## # A tibble: 32 x 16\n##    `0_4` `0_6` `0_8` `0_10` `1_4` `1_6` `1_8` `1_10`  disp    hp  drat\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1  NA    NA    NA       NA  NA      21    NA     NA  160    110  3.9 \n##  2  NA    NA    NA       NA  NA      21    NA     NA  160    110  3.9 \n##  3  NA    NA    NA       NA  22.8    NA    NA     NA  108     93  3.85\n##  4  NA    21.4  NA       NA  NA      NA    NA     NA  258    110  3.08\n##  5  NA    NA    18.7     NA  NA      NA    NA     NA  360    175  3.15\n##  6  NA    18.1  NA       NA  NA      NA    NA     NA  225    105  2.76\n##  7  NA    NA    14.3     NA  NA      NA    NA     NA  360    245  3.21\n##  8  24.4  NA    NA       NA  NA      NA    NA     NA  147.    62  3.69\n##  9  22.8  NA    NA       NA  NA      NA    NA     NA  141.    95  3.92\n## 10  NA    19.2  NA       NA  NA      NA    NA     NA  168.   123  3.92\n## # … with 22 more rows, and 5 more variables: wt &lt;dbl&gt;, qsec &lt;dbl&gt;,\n## #   vs &lt;dbl&gt;, gear &lt;dbl&gt;, carb &lt;dbl&gt;\n\nAs you can see, we now have two more columns have been added, and they are full of NA’s.\n\n\nNow, let’s try to go from wide to long data sets, using pivot_long():\n\nmtcars_wide1 %&gt;% \n  pivot_long(cols = c(`1`, `0`), names_to = \"am\", values_to = \"mpg\") %&gt;% \n  select(am, mpg, everything())\n## # A tibble: 64 x 11\n##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb\n##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 1      21       6   160   110  3.9   2.62  16.5     0     4     4\n##  2 0      NA       6   160   110  3.9   2.62  16.5     0     4     4\n##  3 1      21       6   160   110  3.9   2.88  17.0     0     4     4\n##  4 0      NA       6   160   110  3.9   2.88  17.0     0     4     4\n##  5 1      22.8     4   108    93  3.85  2.32  18.6     1     4     1\n##  6 0      NA       4   108    93  3.85  2.32  18.6     1     4     1\n##  7 1      NA       6   258   110  3.08  3.22  19.4     1     3     1\n##  8 0      21.4     6   258   110  3.08  3.22  19.4     1     3     1\n##  9 1      NA       8   360   175  3.15  3.44  17.0     0     3     2\n## 10 0      18.7     8   360   175  3.15  3.44  17.0     0     3     2\n## # … with 54 more rows\n\nThe arguments of pivot_long() are quite explicit too, and similar to the ones in pivot_wide(). cols = is where the user specifies the columns that need to be pivoted. names_to = is where the user can specify the name of the new columns, whose levels will be exactly the ones specified to cols =. values_to = is where the user specifies the column name of the new column that will contain the values.\n\n\nIt is also possible to specify the columns that should not be transformed, by using -:\n\nmtcars_wide1 %&gt;% \n  pivot_long(cols = -matches(\"^[[:alpha:]]\"), names_to = \"am\", values_to = \"mpg\") %&gt;% \n  select(am, mpg, everything())\n## # A tibble: 64 x 11\n##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb\n##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 1      21       6   160   110  3.9   2.62  16.5     0     4     4\n##  2 0      NA       6   160   110  3.9   2.62  16.5     0     4     4\n##  3 1      21       6   160   110  3.9   2.88  17.0     0     4     4\n##  4 0      NA       6   160   110  3.9   2.88  17.0     0     4     4\n##  5 1      22.8     4   108    93  3.85  2.32  18.6     1     4     1\n##  6 0      NA       4   108    93  3.85  2.32  18.6     1     4     1\n##  7 1      NA       6   258   110  3.08  3.22  19.4     1     3     1\n##  8 0      21.4     6   258   110  3.08  3.22  19.4     1     3     1\n##  9 1      NA       8   360   175  3.15  3.44  17.0     0     3     2\n## 10 0      18.7     8   360   175  3.15  3.44  17.0     0     3     2\n## # … with 54 more rows\n\nHere the columns that should not be modified are all those that start with a letter, hence the “1” regular expression. It is also possible to remove all the NA’s from the data frame, with na.rm =.\n\nmtcars_wide1 %&gt;% \n  pivot_long(cols = c(`1`, `0`), names_to = \"am\", values_to = \"mpg\", na.rm = TRUE) %&gt;% \n  select(am, mpg, everything())\n## # A tibble: 32 x 11\n##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb\n##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 1      21       6  160    110  3.9   2.62  16.5     0     4     4\n##  2 1      21       6  160    110  3.9   2.88  17.0     0     4     4\n##  3 1      22.8     4  108     93  3.85  2.32  18.6     1     4     1\n##  4 0      21.4     6  258    110  3.08  3.22  19.4     1     3     1\n##  5 0      18.7     8  360    175  3.15  3.44  17.0     0     3     2\n##  6 0      18.1     6  225    105  2.76  3.46  20.2     1     3     1\n##  7 0      14.3     8  360    245  3.21  3.57  15.8     0     3     4\n##  8 0      24.4     4  147.    62  3.69  3.19  20       1     4     2\n##  9 0      22.8     4  141.    95  3.92  3.15  22.9     1     4     2\n## 10 0      19.2     6  168.   123  3.92  3.44  18.3     1     4     4\n## # … with 22 more rows\n\nWe can also pivot data frames where the names of the columns are made of two or more variables, for example in our mtcars_wide2 data frame:\n\nmtcars_wide2 %&gt;% \n    select(matches(\"^0|1\"), everything())\n## # A tibble: 32 x 14\n##    `1_6` `1_4` `0_6` `0_8` `0_4` `1_8`  disp    hp  drat    wt  qsec    vs\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1    21  NA    NA    NA    NA      NA  160    110  3.9   2.62  16.5     0\n##  2    21  NA    NA    NA    NA      NA  160    110  3.9   2.88  17.0     0\n##  3    NA  22.8  NA    NA    NA      NA  108     93  3.85  2.32  18.6     1\n##  4    NA  NA    21.4  NA    NA      NA  258    110  3.08  3.22  19.4     1\n##  5    NA  NA    NA    18.7  NA      NA  360    175  3.15  3.44  17.0     0\n##  6    NA  NA    18.1  NA    NA      NA  225    105  2.76  3.46  20.2     1\n##  7    NA  NA    NA    14.3  NA      NA  360    245  3.21  3.57  15.8     0\n##  8    NA  NA    NA    NA    24.4    NA  147.    62  3.69  3.19  20       1\n##  9    NA  NA    NA    NA    22.8    NA  141.    95  3.92  3.15  22.9     1\n## 10    NA  NA    19.2  NA    NA      NA  168.   123  3.92  3.44  18.3     1\n## # … with 22 more rows, and 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;\n\nAll the columns that start with either “0” or “1” must be pivoted:\n\nmtcars_wide2 %&gt;% \n  pivot_long(cols = matches(\"0|1\"), names_to = \"am_cyl\", values_to = \"mpg\", na.rm = TRUE) %&gt;% \n  select(am_cyl, everything())\n## # A tibble: 32 x 10\n##    am_cyl  disp    hp  drat    wt  qsec    vs  gear  carb   mpg\n##    &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 1_6     160    110  3.9   2.62  16.5     0     4     4  21  \n##  2 1_6     160    110  3.9   2.88  17.0     0     4     4  21  \n##  3 1_4     108     93  3.85  2.32  18.6     1     4     1  22.8\n##  4 0_6     258    110  3.08  3.22  19.4     1     3     1  21.4\n##  5 0_8     360    175  3.15  3.44  17.0     0     3     2  18.7\n##  6 0_6     225    105  2.76  3.46  20.2     1     3     1  18.1\n##  7 0_8     360    245  3.21  3.57  15.8     0     3     4  14.3\n##  8 0_4     147.    62  3.69  3.19  20       1     4     2  24.4\n##  9 0_4     141.    95  3.92  3.15  22.9     1     4     2  22.8\n## 10 0_6     168.   123  3.92  3.44  18.3     1     4     4  19.2\n## # … with 22 more rows\n\nNow, there is one new column, “am_cyl” which must still be transformed by separating “am_cyl” into two new columns:\n\nmtcars_wide2 %&gt;% \n  pivot_long(cols = matches(\"0|1\"), names_to = \"am_cyl\", values_to = \"mpg\", na.rm = TRUE) %&gt;% \n  separate(am_cyl, into = c(\"am\", \"cyl\"), sep = \"_\") %&gt;% \n  select(am, cyl, everything())\n## # A tibble: 32 x 11\n##    am    cyl    disp    hp  drat    wt  qsec    vs  gear  carb   mpg\n##    &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 1     6      160    110  3.9   2.62  16.5     0     4     4  21  \n##  2 1     6      160    110  3.9   2.88  17.0     0     4     4  21  \n##  3 1     4      108     93  3.85  2.32  18.6     1     4     1  22.8\n##  4 0     6      258    110  3.08  3.22  19.4     1     3     1  21.4\n##  5 0     8      360    175  3.15  3.44  17.0     0     3     2  18.7\n##  6 0     6      225    105  2.76  3.46  20.2     1     3     1  18.1\n##  7 0     8      360    245  3.21  3.57  15.8     0     3     4  14.3\n##  8 0     4      147.    62  3.69  3.19  20       1     4     2  24.4\n##  9 0     4      141.    95  3.92  3.15  22.9     1     4     2  22.8\n## 10 0     6      168.   123  3.92  3.44  18.3     1     4     4  19.2\n## # … with 22 more rows\n\nIt is also possible to achieve this using a data frame with the specification of what you need:\n\nmtcars_spec_long &lt;- mtcars_wide2 %&gt;% \n  pivot_long_spec(matches(\"0|1\"), values_to = \"mpg\") %&gt;% \n  separate(name, c(\"am\", \"cyl\"), sep = \"_\")\n\nmtcars_spec_long\n## # A tibble: 6 x 4\n##   .name .value am    cyl  \n##   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;\n## 1 1_6   mpg    1     6    \n## 2 1_4   mpg    1     4    \n## 3 0_6   mpg    0     6    \n## 4 0_8   mpg    0     8    \n## 5 0_4   mpg    0     4    \n## 6 1_8   mpg    1     8\n\nProviding this spec to pivot_long() solves the issue:\n\nmtcars_wide2 %&gt;% \n  pivot_long(spec = mtcars_spec_long, na.rm = TRUE) %&gt;% \n  select(am, cyl, everything())\n## # A tibble: 32 x 11\n##    am    cyl    disp    hp  drat    wt  qsec    vs  gear  carb   mpg\n##    &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 1     6      160    110  3.9   2.62  16.5     0     4     4  21  \n##  2 1     6      160    110  3.9   2.88  17.0     0     4     4  21  \n##  3 1     4      108     93  3.85  2.32  18.6     1     4     1  22.8\n##  4 0     6      258    110  3.08  3.22  19.4     1     3     1  21.4\n##  5 0     8      360    175  3.15  3.44  17.0     0     3     2  18.7\n##  6 0     6      225    105  2.76  3.46  20.2     1     3     1  18.1\n##  7 0     8      360    245  3.21  3.57  15.8     0     3     4  14.3\n##  8 0     4      147.    62  3.69  3.19  20       1     4     2  24.4\n##  9 0     4      141.    95  3.92  3.15  22.9     1     4     2  22.8\n## 10 0     6      168.   123  3.92  3.44  18.3     1     4     4  19.2\n## # … with 22 more rows\n\nStay tuned to Hadley Wickham’s twitter as there will definitely be announcements soon!"
  },
  {
    "objectID": "posts/2021-12-17-expand_knitr.html",
    "href": "posts/2021-12-17-expand_knitr.html",
    "title": "How to write code that returns (Rmarkdown) code",
    "section": "",
    "text": "One of the most useful aspects of using a programming language instead of… well, not using a programming language, is that you can write code in a way that minimizes, and ideally, eliminates the need to repeat yourself.\n\n\nFor instance, you can write a function to show you a frequency table, like so:\n\nsuppressMessages(library(dplyr))\n\ncreate_table &lt;- function(dataset, var){\n\n  var &lt;- enquo(var)\n\n  dataset %&gt;%\n    count(!!var) %&gt;%\n    knitr::kable()\n\n}\n\nAnd can now get some fancy looking tables by simply writing:\n\ncreate_table(mtcars, cyl)\n\n\n\n\ncyl\n\n\nn\n\n\n\n\n\n\n4\n\n\n11\n\n\n\n\n6\n\n\n7\n\n\n\n\n8\n\n\n14\n\n\n\n\n\nIf I want such tables for hundreds of columns, I can use this function and loop over the columns and not have to write the code inside the body of the function over and over again. You’ll notice that the function create_table() makes use of some advanced programming techniques I have discussed here. There’s also an alternative way of programming with {dplyr}, using the {{}} construct I discussed here, but I couldn’t get what I’m going to show you here to work with {{}}.\n\n\nRecently, I had to create a Rmarkdown document with many sections, where each section title was a question from a survey and the content was a frequency table. I wanted to write a fuction that would create a section with the right question title, and then show the table, and I wanted to then call this function over all the questions from the survey and have my document automatically generated.\n\n\nThe result should look like this, but it would be a PDF instead of HTML.\n\n\nLet’s first load the data and see how it looks like:\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(readr)\n\nsuppressMessages(\n  survey_data &lt;- read_csv(\n    \"https://gist.githubusercontent.com/b-rodrigues/0c2249dec5a9c9477e0d1ad9964a1340/raw/873bcc7532b8bad613235f029884df1d0b947c90/survey_example.csv\"\n  )\n)\n\nglimpse(survey_data)\n## Rows: 100\n## Columns: 4\n## $ `Random question?`                         &lt;chr&gt; \"no\", \"yes\", \"yes\", \"yes\", …\n## $ `Copy of Random question?`                 &lt;chr&gt; \"yes\", \"yes\", \"no\", \"yes\", …\n## $ `Copy of Copy of Random question?`         &lt;chr&gt; \"yes\", \"no\", \"no\", \"yes\", \"…\n## $ `Copy of Copy of Copy of Random question?` &lt;chr&gt; \"yes\", \"yes\", \"no\", \"yes\", …\n\nEach column name is the question, and each row is one answer to the survey question. To create the document I showed above, you’d probably write something like this:\n\n\n## Random question?\n\n` ``{r}\n\ncreate_table(survey_data, `Random question?`)\n\n` ``\n\n## Copy of Random question?\n\n` ``{r}\n\ncreate_table(survey_data, `Copy of Random question?`)\n\n` ``\n\n## Copy of Copy of Random question?\n\n` ``{r}\n\ncreate_table(survey_data, `Copy of Copy of Random question?`)\n\n` ``\n\n## Copy of Copy of Copy of Random question?\n\n` ``{r}\n\ncreate_table(survey_data, `Copy of Copy of Copy of Random question?`)\n\n` ``\n\n\nAs you can see, this gets tedious very quickly, especially if you have 100’s of variables. So how to not repeat yourself? The solution has two steps; first you should try to automate what you have as much as possible. Ideally, you don’t want to have to write the complete question every time. So first, let’s replace the questions by simpler variable names:\n\nquestions &lt;- colnames(survey_data)\n\ncodes &lt;- paste0(\"var_\", seq(1, length(questions)))\n\nlookup &lt;- bind_cols(\"codes\" = codes, \"questions\" = questions)\n\ncolnames(survey_data) &lt;- codes\n\nlookup is a data frame with the questions and their respective codes:\n\nlookup\n## tibble [4, 2] \n## codes     chr var_1 var_2 var_3 var_4\n## questions chr Random question? Copy of Random question? Cop~\n\nand our data now has simpler variable names:\n\nglimpse(survey_data)\n## Rows: 100\n## Columns: 4\n## $ var_1 &lt;chr&gt; \"no\", \"yes\", \"yes\", \"yes\", \"no\", NA, \"no\", NA, \"no\", \"no\", \"no\",…\n## $ var_2 &lt;chr&gt; \"yes\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"yes\", NA, \"yes\", NA, \"n…\n## $ var_3 &lt;chr&gt; \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"yes\",…\n## $ var_4 &lt;chr&gt; \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"no\",…\n\nDoing this allows us to replace the source code of our Rmarkdown like so:\n\n## `r lookup$questions[grepl(\"var_1\", lookup$codes)]`\n\n` ``{r}\ncreate_table(survey_data, var_1)\n` ``\n\nThis already makes things easier, as now you only have to change var_1 to var_2 to var_3… the inline code gets executed and the right title (the question text) appears. But how to go further? I don’t want to have to copy and paste this and change var_1 to var_2 etc… So the second step of the two-step solution is to use a function called knitr_expand() described here. The idea of knitr::knitr_expand() is that it uses some Rmd source as a template, and also allows the user to define some variables that will be replaced at compile time. Simple examples are available here. I want to build upon that, because I need to pass my variable (in this case var_1 for instance) to my function create_table().\n\n\nThe solution is to write another function that uses knitr::knitr_expand(). This is how it could look like:\n\ncreate_table &lt;- function(dataset, var){\n\n  dataset %&gt;%\n    count(!!var) %&gt;%\n    knitr::kable()\n\n}\n\n\nreturn_section &lt;- function(var){\n\n  a &lt;- knitr::knit_expand(text = c(\"## {{question}}\",   create_table(survey_data, var)),\n                          question =  lookup$questions[grepl(quo_name(var), lookup$codes)])\n\n  cat(a, sep = \"\\n\")\n}\n\nI needed to edit create_table() a little bit, and remove the line var &lt;- enquo(var). This is because now, I won’t be passing a variable down to the function, but a quosure, and there is a very good reason for it, you’ll see. return_section() makes use of knitr_expand(), and the text = argument is the template that will get expanded. {{question}} will get replaced by the variable I defined which is the code I wrote above to automatically get the question text. Finally, var will get replaced by the variable I pass to the function.\n\n\nFirst, let’s get it running on one single variable:\n\nreturn_section(quo(var_1))\n## ## Random question?\n## |var_1 |  n|\n## |:-----|--:|\n## |no    | 40|\n## |yes   | 44|\n## |NA    | 16|\n\nAs you see, I had to use quo(var_1) and not only var_1. But apart from this, the function seems to work well. Putting this in an Rmarkdown document would create a section with the question as the text of the section and a frequency table as the body. I could now copy and paste this and only have to change var_1. But I don’t want to have to copy and paste! So the idea would be to loop the function over a list of variables.\n\n\nI have such a list already:\n\ncodes\n## [1] \"var_1\" \"var_2\" \"var_3\" \"var_4\"\n\nBut it’s not a list of quosures, but a list of strings, and this is not going to work (it will return an error):\n\nwalk(codes, return_section)\n\n(I’m using walk() instead of map() because return_section() doesn’t return an object, but only shows something on screen. This is called a side effect, and walk() allows you to loop properly over functions that only return side effects).\n\n\nThe problem I have now is to convert strings to quosures. This is possible using rlang::sym():\n\nsym_codes &lt;- map(codes, sym)\n\nAnd now I’m done:\n\nwalk(sym_codes, return_section)\n## ## Random question?\n## |var_1 |  n|\n## |:-----|--:|\n## |no    | 40|\n## |yes   | 44|\n## |NA    | 16|\n## ## Copy of Random question?\n## |var_2 |  n|\n## |:-----|--:|\n## |no    | 52|\n## |yes   | 32|\n## |NA    | 16|\n## ## Copy of Copy of Random question?\n## |var_3 |  n|\n## |:-----|--:|\n## |no    | 46|\n## |yes   | 47|\n## |NA    |  7|\n## ## Copy of Copy of Copy of Random question?\n## |var_4 |  n|\n## |:-----|--:|\n## |no    | 48|\n## |yes   | 42|\n## |NA    | 10|\n\nPutting this in an Rmarkdown source create a PDF (or Word, or HTML) document with one section per question, and without have to do copy-pasting which is quite error-prone. Here is the final Rmarkdown file. You’ll notice that the last chunk has the option results = ‘asis’, which is needed for this trick to work."
  },
  {
    "objectID": "posts/2018-12-30-reticulate.html",
    "href": "posts/2018-12-30-reticulate.html",
    "title": "R or Python? Why not both? Using Anaconda Python within R with {reticulate}",
    "section": "",
    "text": "This short blog post illustrates how easy it is to use R and Python in the same R Notebook thanks to the {reticulate} package. For this to work, you might need to upgrade RStudio to the current preview version. Let’s start by importing {reticulate}:\n\nlibrary(reticulate)\n\n{reticulate} is an RStudio package that provides “a comprehensive set of tools for interoperability between Python and R”. With it, it is possible to call Python and use Python libraries within an R session, or define Python chunks in R markdown. I think that using R Notebooks is the best way to work with Python and R; when you want to use Python, you simply use a Python chunk:\n\n```{python}\nyour python code here\n```\n\nThere’s even autocompletion for Python object methods:\n\n\n\n\n\nFantastic!\n\n\nHowever, if you wish to use Python interactively within your R session, you must start the Python REPL with the repl_python() function, which starts a Python REPL. You can then do whatever you want, even access objects from your R session, and then when you exit the REPL, any object you created in Python remains accessible in R. I think that using Python this way is a bit more involved and would advise using R Notebooks if you need to use both languages.\n\n\nI installed the Anaconda Python distribution to have Python on my system. To use it with {reticulate} I must first use the use_python() function that allows me to set which version of Python I want to use:\n\n# This is an R chunk\nuse_python(\"~/miniconda3/bin/python\")\n\nI can now load a dataset, still using R:\n\n# This is an R chunk\ndata(mtcars)\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nand now, to access the mtcars data frame, I simply use the r object:\n\n# This is a Python chunk\nprint(r.mtcars.describe())\n##              mpg        cyl        disp   ...            am       gear     carb\n## count  32.000000  32.000000   32.000000   ...     32.000000  32.000000  32.0000\n## mean   20.090625   6.187500  230.721875   ...      0.406250   3.687500   2.8125\n## std     6.026948   1.785922  123.938694   ...      0.498991   0.737804   1.6152\n## min    10.400000   4.000000   71.100000   ...      0.000000   3.000000   1.0000\n## 25%    15.425000   4.000000  120.825000   ...      0.000000   3.000000   2.0000\n## 50%    19.200000   6.000000  196.300000   ...      0.000000   4.000000   2.0000\n## 75%    22.800000   8.000000  326.000000   ...      1.000000   4.000000   4.0000\n## max    33.900000   8.000000  472.000000   ...      1.000000   5.000000   8.0000\n## \n## [8 rows x 11 columns]\n\n.describe() is a Python Pandas DataFrame method to get summary statistics of our data. This means that mtcars was automatically converted from a tibble object to a Pandas DataFrame! Let’s check its type:\n\n# This is a Python chunk\nprint(type(r.mtcars))\n## &lt;class 'pandas.core.frame.DataFrame'&gt;\n\nLet’s save the summary statistics in a variable:\n\n# This is a Python chunk\nsummary_mtcars = r.mtcars.describe()\n\nLet’s access this from R, by using the py object:\n\n# This is an R chunk\nclass(py$summary_mtcars)\n## [1] \"data.frame\"\n\nLet’s try something more complex. Let’s first fit a linear model in Python, and see how R sees it:\n\n# This is a Python chunk\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nmodel = smf.ols('mpg ~ hp', data = r.mtcars).fit()\nprint(model.summary())\n##                             OLS Regression Results                            \n## ==============================================================================\n## Dep. Variable:                    mpg   R-squared:                       0.602\n## Model:                            OLS   Adj. R-squared:                  0.589\n## Method:                 Least Squares   F-statistic:                     45.46\n## Date:                Sun, 10 Feb 2019   Prob (F-statistic):           1.79e-07\n## Time:                        00:25:51   Log-Likelihood:                -87.619\n## No. Observations:                  32   AIC:                             179.2\n## Df Residuals:                      30   BIC:                             182.2\n## Df Model:                           1                                         \n## Covariance Type:            nonrobust                                         \n## ==============================================================================\n##                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n## ------------------------------------------------------------------------------\n## Intercept     30.0989      1.634     18.421      0.000      26.762      33.436\n## hp            -0.0682      0.010     -6.742      0.000      -0.089      -0.048\n## ==============================================================================\n## Omnibus:                        3.692   Durbin-Watson:                   1.134\n## Prob(Omnibus):                  0.158   Jarque-Bera (JB):                2.984\n## Skew:                           0.747   Prob(JB):                        0.225\n## Kurtosis:                       2.935   Cond. No.                         386.\n## ==============================================================================\n## \n## Warnings:\n## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nJust for fun, I ran the linear regression with the Scikit-learn library too:\n\n# This is a Python chunk\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression  \nregressor = LinearRegression()  \nx = r.mtcars[[\"hp\"]]\ny = r.mtcars[[\"mpg\"]]\nmodel_scikit = regressor.fit(x, y)\nprint(model_scikit.intercept_)\n## [30.09886054]\nprint(model_scikit.coef_)\n## [[-0.06822828]]\n\nLet’s access the model variable in R and see what type of object it is in R:\n\n# This is an R chunk\nmodel_r &lt;- py$model\nclass(model_r)\n## [1] \"statsmodels.regression.linear_model.RegressionResultsWrapper\"\n## [2] \"statsmodels.base.wrapper.ResultsWrapper\"                     \n## [3] \"python.builtin.object\"\n\nSo because this is a custom Python object, it does not get converted into the equivalent R object. This is described here. However, you can still use Python methods from within an R chunk!\n\n# This is an R chunk\nmodel_r$aic\n## [1] 179.2386\nmodel_r$params\n##   Intercept          hp \n## 30.09886054 -0.06822828\n\nI must say that I am very impressed with the {reticulate} package. I think that even if you are primarily a Python user, this is still very interesting to know in case you need a specific function from an R package. Just write all your script inside a Python Markdown chunk and then use the R function you need from an R chunk! Of course there is also a way to use R from Python, a Python library called rpy2 but I am not very familiar with it. From what I read, it seems to be also quite simple to use."
  },
  {
    "objectID": "posts/2018-12-24-modern_objects.html",
    "href": "posts/2018-12-24-modern_objects.html",
    "title": "Objects types and some useful R functions for beginners",
    "section": "",
    "text": "This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free here. This is taken from Chapter 2, which explains the different R objects you can manipulate as well as some functions to get you started."
  },
  {
    "objectID": "posts/2018-12-24-modern_objects.html#objects-types-and-useful-r-functions-to-get-started",
    "href": "posts/2018-12-24-modern_objects.html#objects-types-and-useful-r-functions-to-get-started",
    "title": "Objects types and some useful R functions for beginners",
    "section": "\nObjects, types and useful R functions to get started\n",
    "text": "Objects, types and useful R functions to get started\n\n\nAll objects in R have a given type. You already know most of them, as these types are also used in mathematics. Integers, floating point numbers, or floats, matrices, etc, are all objects you are already familiar with. But R has other, maybe lesser known data types (that you can find in a lot of other programming languages) that you need to become familiar with. But first, we need to learn how to assign a value to a variable. This can be done in two ways:\n\na &lt;- 3\n\nor\n\na = 3\n\nin very practical terms, there is no difference between the two. I prefer using &lt;- for assigning values to variables and reserve = for passing arguments to functions, for example:\n\nspam &lt;- mean(x = c(1,2,3))\n\nI think this is less confusing than:\n\nspam = mean(x = c(1,2,3))\n\nbut as I explained above you can use whatever you feel most comfortable with.\n\n\n\nThe numeric class\n\n\nTo define single numbers, you can do the following:\n\na &lt;- 3\n\nThe class() function allows you to check the class of an object:\n\nclass(a)\n## [1] \"numeric\"\n\nDecimals are defined with the character .:\n\na &lt;- 3.14\n\nR also supports integers. If you find yourself in a situation where you explicitly need an integer and not a floating point number, you can use the following:\n\na  &lt;- as.integer(3)\nclass(a)\n## [1] \"integer\"\n\nThe as.integer() function is very useful, because it converts its argument into an integer. There is a whole family of as.*() functions. To convert a into a floating point number again:\n\nclass(as.numeric(a))\n## [1] \"numeric\"\n\nThere is also is.numeric() which tests whether a number is of the numeric class:\n\nis.numeric(a)\n## [1] TRUE\n\nThese functions are very useful, there is one for any of the supported types in R. Later, we are going to learn about the {purrr} package, which is a very powerful package for functional programming. This package includes further such functions.\n\n\n\n\nThe character class\n\n\nUse ” “ to define characters (called strings in other programming languages):\n\na &lt;- \"this is a string\"\nclass(a)\n## [1] \"character\"\n\nTo convert something to a character you can use the as.character() function:\n\na &lt;- 4.392\n\nclass(a)\n## [1] \"numeric\"\nclass(as.character(a))\n## [1] \"character\"\n\nIt is also possible to convert a character to a numeric:\n\na &lt;- \"4.392\"\n\nclass(a)\n## [1] \"character\"\nclass(as.numeric(a))\n## [1] \"numeric\"\n\nBut this only works if it makes sense:\n\na &lt;- \"this won't work, chief\"\n\nclass(a)\n## [1] \"character\"\nas.numeric(a)\n## Warning: NAs introduced by coercion\n## [1] NA\n\nA very nice package to work with characters is {stringr}, which is also part of the {tidyverse}.\n\n\n\n\nThe factor class\n\n\nFactors look like characters, but are very different. They are the representation of categorical variables. A {tidyverse} package to work with factors is {forcats}. You would rarely use factor variables outside of datasets, so for now, it is enough to know that this class exists. We are going to learn more about factor variables in Chapter 4, by using the {forcats} package.\n\n\n\n\nThe Date class\n\n\nDates also look like characters, but are very different too:\n\nas.Date(\"2019/03/19\")\n## [1] \"2019-03-19\"\nclass(as.Date(\"2019/03/19\"))\n## [1] \"Date\"\n\nManipulating dates and time can be tricky, but thankfully there’s a {tidyverse} package for that, called {lubridate}. We are going to go over this package in Chapter 4.\n\n\n\n\nThe logical class\n\n\nThis class is the result of logical comparisons, for example, if you type:\n\n4 &gt; 3\n## [1] TRUE\n\nR returns TRUE, which is an object of class logical:\n\nk &lt;- 4 &gt; 3\nclass(k)\n## [1] \"logical\"\n\nIn other programming languages, logicals are often called bools. A logical variable can only have two values, either TRUE or FALSE. You can test the truthiness of a variable with isTRUE():\n\nk &lt;- 4 &gt; 3\nisTRUE(k)\n## [1] TRUE\n\nHow can you test if a variable is false? There is not a isFALSE() function (at least not without having to load a package containing this function), but there is way to do it:\n\nk &lt;- 4 &gt; 3\n!isTRUE(k)\n## [1] FALSE\n\nThe ! operator indicates negation, so the above expression could be translated as is k not TRUE?. There are other such operators, namely &, &&, |, ||. & means and and | stands for or. You might be wondering what the difference between & and && is? Or between | and ||? & and | work on vectors, doing pairwise comparisons:\n\none &lt;- c(TRUE, FALSE, TRUE, FALSE)\ntwo &lt;- c(FALSE, TRUE, TRUE, TRUE)\none & two\n## [1] FALSE FALSE  TRUE FALSE\n\nCompare this to the && operator:\n\none &lt;- c(TRUE, FALSE, TRUE, FALSE)\ntwo &lt;- c(FALSE, TRUE, TRUE, TRUE)\none && two\n## [1] FALSE\n\nThe && and || operators only compare the first element of the vectors and stop as soon as a the return value can be safely determined. This is called short-circuiting. Consider the following:\n\none &lt;- c(TRUE, FALSE, TRUE, FALSE)\ntwo &lt;- c(FALSE, TRUE, TRUE, TRUE)\nthree &lt;- c(TRUE, TRUE, FALSE, FALSE)\none && two && three\n## [1] FALSE\none || two || three\n## [1] TRUE\n\nThe || operator stops as soon it evaluates to TRUE whereas the && stops as soon as it evaluates to FALSE. Personally, I rarely use || or && because I get confused. I find using | or & in combination with the all() or any() functions much more useful:\n\none &lt;- c(TRUE, FALSE, TRUE, FALSE)\ntwo &lt;- c(FALSE, TRUE, TRUE, TRUE)\nany(one & two)\n## [1] TRUE\nall(one & two)\n## [1] FALSE\n\nany() checks whether any of the vector’s elements are TRUE and all() checks if all elements of the vector are TRUE.\n\n\nAs a final note, you should know that is possible to use T for TRUE and F for FALSE but I would advise against doing this, because it is not very explicit.\n\n\n\n\nVectors and matrices\n\n\nYou can create a vector in different ways. But first of all, it is important to understand that a vector in most programming languages is nothing more than a list of things. These things can be numbers (either integers or floats), strings, or even other vectors. A vector in R can only contain elements of one single type. This is not the case for a list, which is much more flexible. We will talk about lists shortly, but let’s first focus on vectors and matrices.\n\n\n\nThe c() function\n\n\nA very important function that allows you to build a vector is c():\n\na &lt;- c(1,2,3,4,5)\n\nThis creates a vector with elements 1, 2, 3, 4, 5. If you check its class:\n\nclass(a)\n## [1] \"numeric\"\n\nThis can be confusing: you where probably expecting a to be of class vector or something similar. This is not the case if you use c() to create the vector, because c() doesn’t build a vector in the mathematical sense, but a so-called atomic vector. Checking its dimension:\n\ndim(a)\n## NULL\n\nreturns NULL because an atomic vector doesn’t have a dimension. If you want to create a true vector, you need to use cbind() or rbind().\n\n\nBut before continuing, be aware that atomic vectors can only contain elements of the same type:\n\nc(1, 2, \"3\")\n## [1] \"1\" \"2\" \"3\"\n\nbecause “3” is a character, all the other values get implicitly converted to characters. You have to be very careful about this, and if you use atomic vectors in your programming, you have to make absolutely sure that no characters or logicals or whatever else are going to convert your atomic vector to something you were not expecting.\n\n\n\n\ncbind() and rbind()\n\n\nYou can create a true vector with cbind():\n\na &lt;- cbind(1, 2, 3, 4, 5)\n\nCheck its class now:\n\nclass(a)\n## [1] \"matrix\"\n\nThis is exactly what we expected. Let’s check its dimension:\n\ndim(a)\n## [1] 1 5\n\nThis returns the dimension of a using the LICO notation (number of LInes first, the number of COlumns).\n\n\nIt is also possible to bind vectors together to create a matrix.\n\nb &lt;- cbind(6,7,8,9,10)\n\nNow let’s put vector a and b into a matrix called matrix_c using rbind(). rbind() functions the same way as cbind() but glues the vectors together by rows and not by columns.\n\nmatrix_c &lt;- rbind(a,b)\nprint(matrix_c)\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    2    3    4    5\n## [2,]    6    7    8    9   10\n\n\n\nThe matrix class\n\n\nR also has support for matrices. For example, you can create a matrix of dimension (5,5) filled with 0’s with the matrix() function:\n\nmatrix_a &lt;- matrix(0, nrow = 5, ncol = 5)\n\nIf you want to create the following matrix:\n\n\n[ B = (\n\\[\\begin{array}{ccc}\n2 &amp; 4 &amp; 3 \\\\\n1 &amp; 5 &amp; 7\n\\end{array}\\]\n) ]\n\n\nyou would do it like this:\n\nB &lt;- matrix(c(2, 4, 3, 1, 5, 7), nrow = 2, byrow = TRUE)\n\nThe option byrow = TRUE means that the rows of the matrix will be filled first.\n\n\nYou can access individual elements of matrix_a like so:\n\nmatrix_a[2, 3]\n## [1] 0\n\nand R returns its value, 0. We can assign a new value to this element if we want. Try:\n\nmatrix_a[2, 3] &lt;- 7\n\nand now take a look at matrix_a again.\n\nprint(matrix_a)\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    0    0    0    0    0\n## [2,]    0    0    7    0    0\n## [3,]    0    0    0    0    0\n## [4,]    0    0    0    0    0\n## [5,]    0    0    0    0    0\n\nRecall our vector b:\n\nb &lt;- cbind(6,7,8,9,10)\n\nTo access its third element, you can simply write:\n\nb[3]\n## [1] 8\n\nI have heard many people praising R for being a matrix based language. Matrices are indeed useful, and statisticians are used to working with them. However, I very rarely use matrices in my day to day work, and prefer an approach based on data frames (which will be discussed below). This is because working with data frames makes it easier to use R’s advanced functional programming language capabilities, and this is where R really shines in my opinion. Working with matrices almost automatically implies using loops and all the iterative programming techniques, à la Fortran, which I personally believe are ill-suited for interactive statistical programming (as discussed in the introduction).\n\n\n\n\n\nThe list class\n\n\nThe list class is a very flexible class, and thus, very useful. You can put anything inside a list, such as numbers:\n\nlist1 &lt;- list(3, 2)\n\nor other lists constructed with c():\n\nlist2 &lt;- list(c(1, 2), c(3, 4))\n\nyou can also put objects of different classes in the same list:\n\nlist3 &lt;- list(3, c(1, 2), \"lists are amazing!\")\n\nand of course create list of lists:\n\nmy_lists &lt;- list(list1, list2, list3)\n\nTo check the contents of a list, you can use the structure function str():\n\nstr(my_lists)\n## List of 3\n##  $ :List of 2\n##   ..$ : num 3\n##   ..$ : num 2\n##  $ :List of 2\n##   ..$ : num [1:2] 1 2\n##   ..$ : num [1:2] 3 4\n##  $ :List of 3\n##   ..$ : num 3\n##   ..$ : num [1:2] 1 2\n##   ..$ : chr \"lists are amazing!\"\n\nor you can use RStudio’s Environment pane:\n\n\n\n\n\nYou can also create named lists:\n\nlist4 &lt;- list(\"a\" = 2, \"b\" = 8, \"c\" = \"this is a named list\")\n\nand you can access the elements in two ways:\n\nlist4[[1]]\n## [1] 2\n\nor, for named lists:\n\nlist4$c\n## [1] \"this is a named list\"\n\nLists are used extensively because they are so flexible. You can build lists of datasets and apply functions to all the datasets at once, build lists of models, lists of plots, etc… In the later chapters we are going to learn all about them. Lists are central objects in a functional programming workflow for interactive statistical analysis.\n\n\n\n\nThe data.frame and tibble classes\n\n\nIn the next chapter we are going to learn how to import datasets into R. Once you import data, the resulting object is either a data.frame or a tibble depending on which package you used to import the data. tibbles extend data.frames so if you know about data.frame objects already, working with tibbles will be very easy. tibbles have a better print() method, and some other niceties.\n\n\nHowever, I want to stress that these objects are central to R and are thus very important; they are actually special cases of lists, discussed above. There are different ways to print a data.frame or a tibble if you wish to inspect it. You can use View(my_data) to show the my_data data.frame in the View pane of RStudio:\n\n\n\n\n\nYou can also use the str() function:\n\nstr(my_data)\n\nAnd if you need to access an individual column, you can use the $ sign, same as for a list:\n\nmy_data$col1\n\n\n\nFormulas\n\n\nWe will learn more about formulas later, but because it is an important object, it is useful if you already know about them early on. A formula is defined in the following way:\n\nmy_formula &lt;- ~x\n\nclass(my_formula)\n## [1] \"formula\"\n\nFormula objects are defined using the ~ symbol. Formulas are useful to define statistical models, for example for a linear regression:\n\nlm(y ~ x)\n\nor also to define anonymous functions, but more on this later."
  },
  {
    "objectID": "posts/2018-12-24-modern_objects.html#models",
    "href": "posts/2018-12-24-modern_objects.html#models",
    "title": "Objects types and some useful R functions for beginners",
    "section": "\nModels\n",
    "text": "Models\n\n\nA statistical model is an object like any other in R:\n\ndata(mtcars)\n\nmy_model &lt;- lm(mpg ~ hp, mtcars)\n\nclass(my_model)\n## [1] \"lm\"\n\nmy_model is an object of class lm. You can apply different functions to a model object:\n\nsummary(my_model)\n## \n## Call:\n## lm(formula = mpg ~ hp, data = mtcars)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.7121 -2.1122 -0.8854  1.5819  8.2360 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***\n## hp          -0.06823    0.01012  -6.742 1.79e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.863 on 30 degrees of freedom\n## Multiple R-squared:  0.6024, Adjusted R-squared:  0.5892 \n## F-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\nThis class will be explored in later chapters.\n\n\n\nNULL, NA and NaN\n\n\nThe NULL, NA and NaN classes are pretty special. NULL is returned when the result of function is undetermined. For example, consider list4:\n\nlist4\n## $a\n## [1] 2\n## \n## $b\n## [1] 8\n## \n## $c\n## [1] \"this is a named list\"\n\nif you try to access an element that does not exist, such as d, you will get NULL back:\n\nlist4$d\n## NULL\n\nNaN means “Not a Number” and is returned when a function return something that is not a number:\n\nsqrt(-1)\n## Warning in sqrt(-1): NaNs produced\n## [1] NaN\n\nor:\n\n0/0\n## [1] NaN\n\nBasically, numbers that cannot be represented as floating point numbers are NaN.\n\n\nFinally, there’s NA which is closely related to NaN but is used for missing values. NA stands for Not Available. There are several types of NAs:\n\n\n\nNA_integer_\n\n\nNA_real_\n\n\nNA_complex_\n\n\nNA_character_\n\n\n\nbut these are in principle only used when you need to program your own functions and need to explicitly test for the missingness of, say, a character value.\n\n\nTo test whether a value is NA, use the is.na() function.\n\n\n\n\nUseful functions to get you started\n\n\nThis section will list several basic R functions that are very useful and should be part of your toolbox.\n\n\n\nSequences\n\n\nThere are several functions that create sequences, seq(), seq_along() and rep(). rep() is easy enough:\n\nrep(1, 10)\n##  [1] 1 1 1 1 1 1 1 1 1 1\n\nThis simply repeats 1 10 times. You can repeat other objects too:\n\nrep(\"HAHA\", 10)\n##  [1] \"HAHA\" \"HAHA\" \"HAHA\" \"HAHA\" \"HAHA\" \"HAHA\" \"HAHA\" \"HAHA\" \"HAHA\" \"HAHA\"\n\nTo create a sequence, things are not as straightforward. There is seq():\n\nseq(1, 10)\n##  [1]  1  2  3  4  5  6  7  8  9 10\nseq(70, 80)\n##  [1] 70 71 72 73 74 75 76 77 78 79 80\n\nIt is also possible to provide a by argument:\n\nseq(1, 10, by = 2)\n## [1] 1 3 5 7 9\n\nseq_along() behaves similarly, but returns the length of the object passed to it. So if you pass list4 to seq_along(), it will return a sequence from 1 to 3:\n\nseq_along(list4)\n## [1] 1 2 3\n\nwhich is also true for seq() actually:\n\nseq(list4)\n## [1] 1 2 3\n\nbut these two functions behave differently for arguments of length equal to 1:\n\nseq(10)\n##  [1]  1  2  3  4  5  6  7  8  9 10\nseq_along(10)\n## [1] 1\n\nSo be quite careful about that. I would advise you do not use seq(), but only seq_along() and seq_len(). seq_len() only takes arguments of length 1:\n\nseq_len(10)\n##  [1]  1  2  3  4  5  6  7  8  9 10\nseq_along(10)\n## [1] 1\n\nThe problem with seq() is that it is unpredictable; depending on its input, the output will either be an integer or a sequence. When programming, it is better to have function that are stricter and fail when confronted to special cases, instead of returning some result. This is a bit of a recurrent issue with R, and the functions from the {tidyverse} mitigate this issue by being stricter than their base R counterparts. For example, consider the ifelse() function from base R:\n\nifelse(3 &gt; 5, 1, \"this is false\")\n## [1] \"this is false\"\n\nand compare it to {dplyr}’s implementation, if_else():\n\nif_else(3 &gt; 5, 1, \"this is false\")\nError: `false` must be type double, not character\nCall `rlang::last_error()` to see a backtrace\n\nif_else() fails because the return value when FALSE is not a double (a real number) but a character. This might seem unnecessarily strict, but at least it is predictable. This makes debugging easier when used inside functions. In Chapter 8 we are going to learn how to write our own functions, and being strict makes programming easier.\n\n\n\n\nBasic string manipulation\n\n\nFor now, we have not closely studied character objects, we only learned how to define them. Later, in Chapter 5 we will learn about the {stringr} package which provides useful function to work with strings. However, there are several base R functions that are very useful that you might want to know nonetheless, such as paste() and paste0():\n\npaste(\"Hello\", \"amigo\")\n## [1] \"Hello amigo\"\n\nbut you can also change the separator if needed:\n\npaste(\"Hello\", \"amigo\", sep = \"--\")\n## [1] \"Hello--amigo\"\n\npaste0() is the same as paste() but does not have any sep argument:\n\npaste0(\"Hello\", \"amigo\")\n## [1] \"Helloamigo\"\n\nIf you provide a vector of characters, you can also use the collapse argument, which places whatever you provide for collapse between the characters of the vector:\n\npaste0(c(\"Joseph\", \"Mary\", \"Jesus\"), collapse = \", and \")\n## [1] \"Joseph, and Mary, and Jesus\"\n\nTo change the case of characters, you can use toupper() and tolower():\n\ntolower(\"HAHAHAHAH\")\n## [1] \"hahahahah\"\ntoupper(\"hueuehuehuheuhe\")\n## [1] \"HUEUEHUEHUHEUHE\"\n\n\n\nMathematical functions\n\n\nFinally, there are the classical mathematical functions that you know and love:\n\n\n\nsqrt()\n\n\nexp()\n\n\nlog()\n\n\nabs()\n\n\nsin(), cos(), tan(), and others\n\n\nsum(), cumsum(), prod(), cumprod()\n\n\nmax(), min()\n\n\n\nand many others…"
  },
  {
    "objectID": "posts/2023-04-25-10_years.html",
    "href": "posts/2023-04-25-10_years.html",
    "title": "I’ve been blogging for 10 years",
    "section": "",
    "text": "I’ve been blogging for 10 years, actually even a bit more than that, my very first blog is not online anymore and if I remember correctly was in French. I think it was around 2010 when I was a Master’s student.\n\n\nAnyways, here are my thoughts on why I think blogging is a very nice activity if you’re interested in technology or science.\n\n\nThe primary reason I started my blog was to have a repository of code snippets that I could re-use. Anytime I had to do something for my thesis or for work, I would write instructions around the code that I’ve used to explain how and why things worked out. But I needed a spot to save these scripts, and it turns out that a blog was the best solution for this: it doesn’t require any subscription to a (very often proprietary) service to store my notes for me, and I need 0 discipline to maintain a blog. Simply write a post, push to Github, website gets updated. If I would store the notes myself on my computer instead, this would mean a lot of work, and I would need to think about how to make them available across devices.\n\n\nThe other reason is that I thought that this would be a good way for me to contribute to the wider free software and open source ecosystem. I’m not a programmer, so contributing code would be quite difficult for me. I’ve recently published a package, so in the end I ended up contributing code, but that was more due to “luck” finding an actual problem that hadn’t been solved (well, that’s not really the case, logging in R had been solved, but not using a monad and for some reason I had become obsessed with monads in 2022) and also thanks to the help of much better programmers than myself. So writing and posting these blog posts would be my way to contribute to the community. I think that this was the right decision, as I’ve had many people throughout the years thank me for some of my blog posts that helped them with some of their tasks.\n\n\nNow, not all blog posts were about problems that I encountered throughout my career. There were also some blog posts about topics that piqued my interest, but purely out of curiosity. For example the ones about NLP, like this one, among others. I’m lucky enough that I find enjoyment in programming and exploring data for fun, so I do write a lot of code. But how did I manage to consistently write blog posts for 10 years+?\n\n\nI think that part of the reason is that I have literally 0 commitment to this blog. I don’t force myself to write on a schedule and sometimes don’t write for months like in 2020 where I didn’t write for 5 months because I was busy renovating my home and taking care of my baby daughter. I didn’t even miss writing. I think that one of the reasons this works is also because I have absolutely 0 trackers on this website. For all I know, this post will get read by 3 people, me, you and my mom. The only clue I have that one particular post was successful is if people reshare the announcement I make on social media, or if they contact me with questions or praise, or if it gets picked up for the R weekly highlights. By not having any trackers and not having really a clue of what people like, I avoid falling into the “recommendation engine” or “SEO” trap. If I did use trackers and knew exactly what my audience wanted, I’d be very incentivized to just continue writing what would generate the most traffic. And the issue with that, is that it would feel like a job, and I very likely would have abandoned this endeavour a long time ago. What generates a lot of traffic are posts mostly aimed at beginners, tutorials that explain how to do a t-test or make a bar graph with two y-axes, but sorry, I’m not interested. I don’t hate beginners, but I don’t only want to write tutorials to serve ads to my readers and tell them to subscribe to newsletters, bla bla bla. I have no qualms with people doing this, but that’s simply not my thing. Not interested (and don’t get me wrong, I have nothing against tutorial blog post, quite the contrary, especially when they present some lesser-known features of a package).\n\n\nAlso, the first thing I install on my web-browser is an ad-blocker, and I would be a huge hypocrite if I tracked my blog’s visits. To be fully transparent, I do use Goatcounter for my latest book here, but I don’t adjust the book’s content to the audience. It’s just to know if people read it, because writing a book takes some effort and I was curious. I might remove it in the future though. Again, nothing against people trying to live off the internet, I myself accept donations, but if I started to look into what drives people to click and donate, I’d turn this into a job, and that would be the fastest way for me to hate blogging.\n\n\nSo the reason this has been working is simply because I avoided considering this as an obligation, a duty or a job, and in order to do this, I avoid collecting data.\n\n\nSo blogging is a nice way to store code snippets and quickly find them. These code snippets can in turn help other people facing similar issues. But what are some other benefits of blogging? First, by turning your code into a full-fledged blog post, it also forces you to think more carefully about the solution you found. Very often, while writing the blog post to a problem I’ve solved, I often find a simpler, more elegant solution, and then use that solution instead to solve my problem. It’s a similar idea to rubber duck debugging, or writing a Minimal Reproducible Example when opening an issue because (you think) you found a nasty bug somewhere in that package you use daily and never bothered reading the documentation for carefully. Blogging is also a way to get some feedback by other people, and sometimes people show me other ways of doing things, like here. Blogging also helped me meet people in the real world, and discuss with them about certain of the topics I’ve blogged about. I think that’s really neat.\n\nBlogging also helps (at least it helped me) realize what I really enjoy about statistics, programming, and data science, and once you have a nice collection of blog posts, you could turn them easily into a book. That’s another way of contributing to the community. I’ve written two books, the latest one seems to really interest people:\n\n\nMy book is done. I need to write a conclusion to the last chapter and will likely rewrite some paragraphs, but I won’t make major changes anymore. So if you’re interested in building reproducible analytical pipelines with #RStats, take a look it’s free: https://t.co/Vx8LGpddwR pic.twitter.com/PKFb2P4pJK\n\n— Bruno Rodrigues (@brodriguesco@fosstodon.org) (@brodriguesco) April 23, 2023\n\n\n\nThat’s my most “viral” tweet! Before it was this classic:\n\n\n\n“doing science” pic.twitter.com/T0TJQ6vY6q\n\n— Bruno Rodrigues (@brodriguesco@fosstodon.org) (@brodriguesco) September 30, 2021\n\n\n\nI’m sure that many people, maybe even you!, work in an industry that tackles many interesting problems, and could share that with the world either through blog posts or books, or videos (yes, I also do that sometimes), and many people would find that interesting. I’ve had people tell me that they have nothing interesting to write about, so they don’t even want to start. Who cares, just write.\n\n\nSo it’d be cool if you blogged. I like it, so maybe you will as well?\n\n\nAnyways, thanks for reading, especially if you’ve been reading my blog for years.\n\n\nHere’s to 10 more years!"
  },
  {
    "objectID": "posts/2019-11-06-explainability_econometrics.html#intro",
    "href": "posts/2019-11-06-explainability_econometrics.html#intro",
    "title": "Intrumental variable regression and machine learning",
    "section": "\nIntro\n",
    "text": "Intro\n\n\nJust like the question “what’s the difference between machine learning and statistics” has shed a lot of ink (since at least Breiman (2001)), the same question but where statistics is replaced by econometrics has led to a lot of discussion, as well. I like this presentation by Hal Varian from almost 6 years ago. There’s a slide called “What econometrics can learn from machine learning”, which summarises in a few bullet points Varian (2014) and the rest of the presentation discusses what machine learning can learn from econometrics. Varian argues that the difference between machine learning and econometrics is that machine learning focuses on prediction, while econometrics on inference and causality (and to a lesser extent prediction as well). Varian cites some methods that have been in the econometricians’ toolbox for decades (at least for some of them), such as regression discontinuity, difference in differences and instrumental variables regression. Another interesting paper is Mullainathan and Spiess, especially the section called What Do We (Not) Learn from Machine Learning Output?. The authors discuss the tempting idea of using LASSO to perform variable (feature) selection. Econometricians might be tempted to use LASSO to perform variable selection, and draw conclusions such as The variable (feature) “Number of rooms” has not been selected by LASSO, thus it plays no role in the prediction of house prices. However, when variables (features) are highly correlated, LASSO selects variables essentially randomly, without any meaningful impact on model performance (for prediction). I found this paragraph quite interesting (emphasis mine):\n\n\nThis problem is ubiquitous in machine learning. The very appeal of these algorithms is that they can fit many different functions. But this creates an Achilles’ heel: more functions mean a greater chance that two functions with very different coefficients can produce similar prediction quality. As a result, how an algorithm chooses between two very different functions can effectively come down to the flip of a coin. In econometric terms, while the lack of standard errors illustrates the limitations to making inference after model selection, the challenge here is (uniform) model selection consistency itself.\n\n\nAssuming that we successfully dealt with model selection, we still have to content with significance of coefficients. There is recent research into this topic, such as Horel and Giesecke, but I wonder to what extent explainability could help with this. I have been looking around for papers that discuss explainability in the context of the social sciences but have not found any. If any of the readers of this blog are aware of such papers, please let me know.\n\n\nJust to wrap up Mullainathan and Spiess; the authors then suggest to use machine learning mainly for prediction tasks, such as using images taken using satellites to predict future harvest size (the authors cite Lobell (2013)), or for tasks that have an implicit prediction component. For instance in the case of instrumental variables regression, two stages least squares is often used, and the first stage is a prediction task. Propensity score matching is another prediction task, where machine learning could be used. Other examples are presented as well. In this blog post, I’ll explore two stages least squares and see what happens when a random forest is used for the first step."
  },
  {
    "objectID": "posts/2019-11-06-explainability_econometrics.html#instrumental-variables-regression-using-two-stage-least-squares",
    "href": "posts/2019-11-06-explainability_econometrics.html#instrumental-variables-regression-using-two-stage-least-squares",
    "title": "Intrumental variable regression and machine learning",
    "section": "\nInstrumental variables regression using two-stage least squares\n",
    "text": "Instrumental variables regression using two-stage least squares\n\n\nLet’s work out a textbook (literally) example of instrumental variable regression. The below example is taken from Wooldrige’s Econometric analysis of cross section and panel data, and is an exercise made using data from Mroz (1987) The sensitivity of an empirical model of married women’s hours of work to economic and statistical assumptions.\n\n\nLet’s first load the needed packages and the data \"mroz\" included in the {wooldridge} package:\n\nlibrary(tidyverse)\nlibrary(randomForest)\nlibrary(wooldridge)\nlibrary(AER)\nlibrary(Metrics)\n\ndata(\"mroz\")\n\nLet’s only select the women that are in the labour force (inlf == 1), and let’s run a simple linear regression. The dependent variable, or target, is lwage, the logarithm of the wage, and the explanatory variables, or features are exper, expersq and educ. For a full description of the data, click below:\n\n\n\n\nDescription of data\n\n\n\nmroz {wooldridge}   R Documentation\nmroz\n\nDescription\n\nWooldridge Source: T.A. Mroz (1987), “The Sensitivity of an Empirical Model of Married Women’s Hours of Work to Economic and Statistical Assumptions,” Econometrica 55, 765-799. Professor Ernst R. Berndt, of MIT, kindly provided the data, which he obtained from Professor Mroz. Data loads lazily.\n\nUsage\n\ndata('mroz')\nFormat\n\nA data.frame with 753 observations on 22 variables:\n\ninlf: =1 if in lab frce, 1975\n\nhours: hours worked, 1975\n\nkidslt6: # kids &lt; 6 years\n\nkidsge6: # kids 6-18\n\nage: woman's age in yrs\n\neduc: years of schooling\n\nwage: est. wage from earn, hrs\n\nrepwage: rep. wage at interview in 1976\n\nhushrs: hours worked by husband, 1975\n\nhusage: husband's age\n\nhuseduc: husband's years of schooling\n\nhuswage: husband's hourly wage, 1975\n\nfaminc: family income, 1975\n\nmtr: fed. marg. tax rte facing woman\n\nmotheduc: mother's years of schooling\n\nfatheduc: father's years of schooling\n\nunem: unem. rate in county of resid.\n\ncity: =1 if live in SMSA\n\nexper: actual labor mkt exper\n\nnwifeinc: (faminc - wage*hours)/1000\n\nlwage: log(wage)\n\nexpersq: exper^2\n\nUsed in Text\n\npages 249-251, 260, 294, 519-520, 530, 535, 535-536, 565-566, 578-579, 593- 595, 601-603, 619-620, 625\n\nSource\n\nhttps://www.cengage.com/cgi-wadsworth/course_products_wp.pl?fid=M20b&product_isbn_issn=9781111531041\n\n\n\n\nworking_w &lt;- mroz %&gt;% \n    filter(inlf == 1)\n\nwage_lm &lt;- lm(lwage ~ exper + expersq + educ, \n              data = working_w)\n\nsummary(wage_lm)\n## \n## Call:\n## lm(formula = lwage ~ exper + expersq + educ, data = working_w)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -3.08404 -0.30627  0.04952  0.37498  2.37115 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -0.5220406  0.1986321  -2.628  0.00890 ** \n## exper        0.0415665  0.0131752   3.155  0.00172 ** \n## expersq     -0.0008112  0.0003932  -2.063  0.03974 *  \n## educ         0.1074896  0.0141465   7.598 1.94e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6664 on 424 degrees of freedom\n## Multiple R-squared:  0.1568, Adjusted R-squared:  0.1509 \n## F-statistic: 26.29 on 3 and 424 DF,  p-value: 1.302e-15\n\nNow, we see that education is statistically significant and the effect is quite high. The return to education is about 11%. Now, let’s add some more explanatory variables:\n\nwage_lm2 &lt;- lm(lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage + city + educ, \n              data = working_w)\n\nsummary(wage_lm2)\n## \n## Call:\n## lm(formula = lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + \n##     huswage + city + educ, data = working_w)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -3.07431 -0.30500  0.05477  0.37871  2.31157 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -0.3853695  0.3163043  -1.218  0.22378    \n## exper        0.0398817  0.0133651   2.984  0.00301 ** \n## expersq     -0.0007400  0.0003985  -1.857  0.06402 .  \n## kidslt6     -0.0564071  0.0890759  -0.633  0.52692    \n## kidsge6     -0.0143165  0.0276579  -0.518  0.60499    \n## husage      -0.0028828  0.0049338  -0.584  0.55934    \n## huswage      0.0177470  0.0102733   1.727  0.08482 .  \n## city         0.0119960  0.0725595   0.165  0.86877    \n## educ         0.0986810  0.0151589   6.510 2.16e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6669 on 419 degrees of freedom\n## Multiple R-squared:  0.1654, Adjusted R-squared:  0.1495 \n## F-statistic: 10.38 on 8 and 419 DF,  p-value: 2.691e-13\n\nThe return to education lowers a bit, but is still significant. Now, the issue is that education is not exogenous (randomly assigned), and is thus correlated with the error term of the regression, due to an omitted variable for instance contained in the error term, that is correlated with education (for example work ethic).\n\n\nTo deal with this, econometricians use instrumental variables (IV) regression. I won’t go into detail here; just know that this method can deal with these types of issues. The Wikipedia page gives a good intro on what this is all about. This short paper is also quite interesting in introducing instrumental variables.\n\n\nIn practice, IV is done in two steps. First, regress the endogenous variable, in our case education, on all the explanatory variables from before, plus so called instruments. Instruments are variables that are correlated with the endogenous variable, here education, but uncorrelated to the error term. They only affect the target variable through their correlation with the endogenous variable. We will be using the education level of the parents of the women, as well as the education levels of their husbands as intruments. The assumption is that the parents’, as well as the husband’s education are exogenous in the log wage of the woman. This assumption can of course be challenged, but let’s say that it holds.\n\n\nTo conclude stage 1, we obtain the predictions of education:\n\nfirst_stage &lt;- lm(educ ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage \n                  + city + motheduc +  fatheduc + huseduc, data = working_w)\n\nworking_w$predictions_first_stage &lt;- predict(first_stage)\n\nWe are now ready for the second stage. In the regression from before:\n\nwage_lm2 &lt;- lm(lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage + city + educ, \n              data = working_w)\n\nwe now replace educ with the predictions of stage 1:\n\nsecond_stage &lt;- lm(lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage \n                   + city + predictions_first_stage,\n                  data = working_w)\n\nsummary(second_stage)\n## \n## Call:\n## lm(formula = lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + \n##     huswage + city + predictions_first_stage, data = working_w)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -3.13493 -0.30004  0.03046  0.37142  2.27199 \n## \n## Coefficients:\n##                           Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)              0.1763588  0.4206911   0.419   0.6753   \n## exper                    0.0419047  0.0139885   2.996   0.0029 **\n## expersq                 -0.0007881  0.0004167  -1.891   0.0593 . \n## kidslt6                 -0.0255934  0.0941128  -0.272   0.7858   \n## kidsge6                 -0.0234422  0.0291914  -0.803   0.4224   \n## husage                  -0.0042628  0.0051919  -0.821   0.4121   \n## huswage                  0.0263802  0.0114511   2.304   0.0217 * \n## city                     0.0215685  0.0759034   0.284   0.7764   \n## predictions_first_stage  0.0531993  0.0263735   2.017   0.0443 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6965 on 419 degrees of freedom\n## Multiple R-squared:  0.08988,    Adjusted R-squared:  0.0725 \n## F-statistic: 5.172 on 8 and 419 DF,  p-value: 3.581e-06\n\nWe see that education, now instrumented by the parents’ and the husband’s education is still significant, but the effect is much lower. The return to education is now about 5%. However, should our assumption hold, this effect is now causal. However there are some caveats. The IV estimate is a local average treatment effect, meaning that we only get the effect on those individuals that were affected by the treatment. In this case, it would mean that the effect we recovered is only for women who were not planning on, say, studying, but only did so under the influence of their parents (or vice-versa).\n\n\nIV regression can also be achieved using the ivreg() function from the {AER} package:\n\ninst_reg &lt;- ivreg(lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage + city + educ \n                  | .-educ + motheduc + fatheduc + huseduc,\n                  data = working_w)\n\nsummary(inst_reg)\n## \n## Call:\n## ivreg(formula = lwage ~ exper + expersq + kidslt6 + kidsge6 + \n##     husage + huswage + city + educ | . - educ + motheduc + fatheduc + \n##     huseduc, data = working_w)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -3.10175 -0.30407  0.03379  0.35255  2.25107 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)  0.1763588  0.4071522   0.433   0.6651   \n## exper        0.0419047  0.0135384   3.095   0.0021 **\n## expersq     -0.0007881  0.0004033  -1.954   0.0514 . \n## kidslt6     -0.0255934  0.0910840  -0.281   0.7789   \n## kidsge6     -0.0234422  0.0282519  -0.830   0.4071   \n## husage      -0.0042628  0.0050249  -0.848   0.3967   \n## huswage      0.0263802  0.0110826   2.380   0.0177 * \n## city         0.0215685  0.0734606   0.294   0.7692   \n## educ         0.0531993  0.0255247   2.084   0.0377 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6741 on 419 degrees of freedom\n## Multiple R-Squared: 0.1475,  Adjusted R-squared: 0.1312 \n## Wald test: 5.522 on 8 and 419 DF,  p-value: 1.191e-06\n\nOk, great, now let’s see how a machine learning practitioner who took an econometrics MOOC might tackle the issue. The first step will be to split the data into training and testing sets:\n\nset.seed(42)\nsample &lt;- sample.int(n = nrow(working_w), size = floor(.90*nrow(working_w)), replace = F)\ntrain &lt;- working_w[sample, ]\ntest  &lt;- working_w[-sample, ]\n\nLet’s now run the same analysis as above, but let’s compute the RMSE of the first stage regression on the testing data as well:\n\nfirst_stage &lt;- lm(educ ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage \n                  + city + motheduc +  fatheduc + huseduc, data = train)\n\ntest$predictions_first_stage &lt;- predict(first_stage, newdata = test)\n\nlm_rmse &lt;- rmse(predicted = test$predictions_first_stage, actual = test$educ)\n\ntrain$predictions_first_stage &lt;- predict(first_stage)\n\nThe first stage is done, let’s go with the second stage:\n\nsecond_stage &lt;- lm(lwage ~ exper + expersq + kidslt6 + kidsge6 + \n                       husage + huswage + city + predictions_first_stage,\n                  data = train)\n\nsummary(second_stage)\n## \n## Call:\n## lm(formula = lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + \n##     huswage + city + predictions_first_stage, data = train)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -3.09828 -0.28606  0.05248  0.37258  2.29947 \n## \n## Coefficients:\n##                           Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)             -0.0037711  0.4489252  -0.008  0.99330   \n## exper                    0.0449370  0.0145632   3.086  0.00218 **\n## expersq                 -0.0008394  0.0004344  -1.933  0.05404 . \n## kidslt6                 -0.0630522  0.0963953  -0.654  0.51345   \n## kidsge6                 -0.0197164  0.0306834  -0.643  0.52089   \n## husage                  -0.0034744  0.0054358  -0.639  0.52310   \n## huswage                  0.0219622  0.0118602   1.852  0.06484 . \n## city                     0.0679668  0.0804317   0.845  0.39863   \n## predictions_first_stage  0.0618777  0.0283253   2.185  0.02954 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6952 on 376 degrees of freedom\n## Multiple R-squared:  0.1035, Adjusted R-squared:  0.08438 \n## F-statistic: 5.424 on 8 and 376 DF,  p-value: 1.764e-06\n\nThe coefficients here are a bit different due to the splitting, but that’s not an issue. Ok, great, but our machine learning engineer is in love with random forests, so he wants to use a random forest for the prediction task of the first stage:\n\nlibrary(randomForest)\n\nfirst_stage_rf &lt;- randomForest(educ ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage \n                  + city + motheduc +  fatheduc + huseduc, \n                               data = train)\n\ntest$predictions_first_stage_rf &lt;- predict(first_stage_rf, newdata = test)\n\nrf_rmse &lt;- rmse(predicted = test$predictions_first_stage_rf, actual = test$educ)\n\ntrain$predictions_first_stage_rf &lt;- predict(first_stage_rf)\n\nLet’s compare the RMSE’s of the two first stages. The RMSE of the first stage using linear regression was 2.0558723 and for the random forest 2.0000417. Our machine learning engineer is happy, because the random forest has better performance. Let’s now use the predictions for the second stage:\n\nsecond_stage_rf_lm &lt;- lm(lwage ~ exper + expersq + kidslt6 + kidsge6 + \n                             husage + huswage + city + predictions_first_stage_rf,\n                  data = train)\n\nsummary(second_stage_rf_lm)\n## \n## Call:\n## lm(formula = lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + \n##     huswage + city + predictions_first_stage_rf, data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.0655 -0.3198  0.0376  0.3710  2.3277 \n## \n## Coefficients:\n##                              Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)                -0.0416945  0.4824998  -0.086  0.93118   \n## exper                       0.0460311  0.0145543   3.163  0.00169 **\n## expersq                    -0.0008594  0.0004344  -1.978  0.04863 * \n## kidslt6                    -0.0420827  0.0952030  -0.442  0.65872   \n## kidsge6                    -0.0211208  0.0306490  -0.689  0.49117   \n## husage                     -0.0033102  0.0054660  -0.606  0.54514   \n## huswage                     0.0229111  0.0118142   1.939  0.05322 . \n## city                        0.0688384  0.0805209   0.855  0.39314   \n## predictions_first_stage_rf  0.0629275  0.0306877   2.051  0.04100 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6957 on 376 degrees of freedom\n## Multiple R-squared:  0.1021, Adjusted R-squared:  0.08302 \n## F-statistic: 5.346 on 8 and 376 DF,  p-value: 2.251e-06\n\nThe results are pretty similar. Now, why not go a bit further and use a random forest for the second stage as well?"
  },
  {
    "objectID": "posts/2019-11-06-explainability_econometrics.html#two-stage-random-forests",
    "href": "posts/2019-11-06-explainability_econometrics.html#two-stage-random-forests",
    "title": "Intrumental variable regression and machine learning",
    "section": "\nTwo-stage random forests\n",
    "text": "Two-stage random forests\n\n\nI have tried to find literature on this, but did not find anything that really fits what I’ll be doing here. Honestly, I don’t know if this is sound theoretically, but it does have intuitive appeal. Using random forests instead of the linear regressions of each stages poses at least the following question: how can we interpret the results of the second stage? As you have seen above, interpretation of the coefficients and standard errors is important, and random forests do not provide this. My idea is to use explainability techniques of black box models, such as partial dependence plots. In this setting, the whole first stage could be interpreted as a feature engineering step. Let’s do it and see what happens.\n\n\nWe already have the first step from before, so let’s go straight to the first step:\n\nsecond_stage_rf_rf &lt;- randomForest(lwage ~ exper + expersq + kidslt6 + kidsge6 + \n                                       husage + huswage + city + predictions_first_stage_rf,\n                  data = train)\n\nLet’s now use the {iml} package for explainability. Let’s start first by loading the package, defining a predictor object, and then get model-agnostic feature importance:\n\nlibrary(\"iml\")\n\npredictor &lt;- Predictor$new(\n  model = second_stage_rf_rf, \n  data = select(test, exper, expersq,\n                kidslt6, kidsge6,\n                husage, huswage, city,\n                predictions_first_stage_rf), \n  y = test$lwage, \n  predict.fun = predict,\n  class = \"regression\"\n  )\n\nThe plot below shows the ratio of the original model error and model error after permutation. A higher value indicates that this feature is important:\n\nimp_rf &lt;- FeatureImp$new(predictor, loss = \"rmse\")\n\nplot(imp_rf)\n\n\n\n\nAccording to this measure of feature importance, there does not seem to be any feature that is important in predicting log wage. This is similar to the result we had with linear regression: most coefficients were not statistically significant, but some were. Does that mean that we should not trust the results of linear regression? After all, how likely is it that log wages can be modeled as a linear combination of features?\n\n\nLet’s see if the random forest was able to undercover strong interaction effects:\n\ninteractions &lt;- Interaction$new(predictor, feature = \"predictions_first_stage_rf\")\n\nplot(interactions)\n\n\n\n\nThis can seem to be a surprising result: education interacts strongly with the number of kids greater than 6 in household. But is it? Depending on a woman’s age, in order to have 2 or 3 kids with ages between 6 and 18, she would have needed to start having them young, and thus could not have pursued a master’s degree, or a PhD. The interaction strength is measured as the share of variance that is explained by the interaction.\n\n\nLet’s now take a look at the partial dependence plots and individual conditional expectation curves. Let me quote the advantages of pdps from Christoph Molnar’s book on interpretable machine learning:\n\n\nThe calculation for the partial dependence plots has a causal interpretation. We intervene on a feature and measure the changes in the predictions. In doing so, we analyze the causal relationship between the feature and the prediction. The relationship is causal for the model – because we explicitly model the outcome as a function of the features – but not necessarily for the real world!\n\n\nThat sounds good. If we can defend the assumption that our instruments are valid, then the relationship should between the feature and the prediction should be causal, and not only for the model. However, pdps have a shortcoming. Again, quoting Christoph Molnar:\n\n\nThe assumption of independence is the biggest issue with PD plots. It is assumed that the feature(s) for which the partial dependence is computed are not correlated with other features.\n\n\nLet’s take a look at the correlation of features\n\ncorr_vars &lt;- cor(select(test, exper, expersq,\n                        kidslt6, kidsge6,\n                        husage, huswage, city,\n                        predictions_first_stage_rf)) %&gt;% \n    as.data.frame() %&gt;% \n    rownames_to_column() %&gt;% \n    pivot_longer(-rowname, names_to = \"vars2\") %&gt;% \n    rename(vars1 = rowname)\n\nhead(corr_vars)\n## # A tibble: 6 x 3\n##   vars1 vars2    value\n##   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;\n## 1 exper exper    1    \n## 2 exper expersq  0.959\n## 3 exper kidslt6 -0.272\n## 4 exper kidsge6 -0.360\n## 5 exper husage   0.487\n## 6 exper huswage -0.181\ncorr_vars %&gt;% \n    mutate(value = abs(value)) %&gt;% \n    filter(value != 1, value &gt; 0.2) %&gt;%\n    filter(vars1 == \"predictions_first_stage_rf\")\n## # A tibble: 5 x 3\n##   vars1                      vars2   value\n##   &lt;chr&gt;                      &lt;chr&gt;   &lt;dbl&gt;\n## 1 predictions_first_stage_rf expersq 0.243\n## 2 predictions_first_stage_rf kidslt6 0.292\n## 3 predictions_first_stage_rf husage  0.217\n## 4 predictions_first_stage_rf huswage 0.494\n## 5 predictions_first_stage_rf city    0.369\n\nOnly 5 variables have a correlation greater than 0.2 with education, and the one with highest correlation is the husband’s wage. It would seem that this situation is ideal to use pdps and ice curves. Before computing them however, let’s read about ice curves:\n\n\nIndividual conditional expectation curves are even more intuitive to understand than partial dependence plots. One line represents the predictions for one instance if we vary the feature of interest.\n\n\nhowever:\n\n\nICE curves can only display one feature meaningfully, because two features would require the drawing of several overlaying surfaces and you would not see anything in the plot. ICE curves suffer from the same problem as PDPs: If the feature of interest is correlated with the other features, then some points in the lines might be invalid data points according to the joint feature distribution. If many ICE curves are drawn, the plot can become overcrowded and you will not see anything. The solution: Either add some transparency to the lines or draw only a sample of the lines. In ICE plots it might not be easy to see the average. This has a simple solution: Combine individual conditional expectation curves with the partial dependence plot. Unlike partial dependence plots, ICE curves can uncover heterogeneous relationships.\n\n\nSo great, let’s go:\n\ninst_effect &lt;- FeatureEffect$new(predictor, \"predictions_first_stage_rf\", method = \"pdp+ice\")\n\nplot(inst_effect) \n\n\n\n\nInteresting, we see that the curves are fairly similar, but there seem to be two groups: one group were adding education years increases wages, and another where the effect seems to remain constant.\n\n\nLet’s try to dig a bit deeper, and get explanations for individual predictions. For this, I create two new observations that have exactly the same features, but one without children older than 6 and another with two children older than 6:\n\n(new_obs &lt;- data.frame(\n    exper = rep(10, 2),\n    expersq = rep(100, 2),\n    kidslt6 = rep(1, 2),\n    kidsge6 = c(0, 2),\n    husage = rep(35, 2),\n    huswage = rep(6, 2),\n    city = rep(1, 2),\n    predictions_first_stage_rf = rep(10, 2)\n))\n##   exper expersq kidslt6 kidsge6 husage huswage city\n## 1    10     100       1       0     35       6    1\n## 2    10     100       1       2     35       6    1\n##   predictions_first_stage_rf\n## 1                         10\n## 2                         10\n\nLet’s see what the model predicts:\n\npredict(second_stage_rf_rf, newdata = new_obs)\n##        1        2 \n## 1.139720 1.216423\n\nLet’s try to understand the difference between these two predictions. For this, we will be using Shapley values as described here. Shapley values use game theory to compute the contribution of each feature towards the prediction of one particular observation. Interpretation of the Shapley values is as follows (quoting Christoph Molnar’s book): Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value.\n\n\nLet’s compute the Shapley values of all the features:\n\nshapley_1 &lt;-  Shapley$new(predictor, x.interest = new_obs[1, ], sample.size = 100)\nshapley_2 &lt;-  Shapley$new(predictor, x.interest = new_obs[2, ], sample.size = 100)\nplot(shapley_1)\n\n\n\nplot(shapley_2)\n\n\n\n\nThe average prediction is 1.21 and the prediction for the first new observation is 1.14, which is 0.07 below the average prediction. This difference of 0.07 is the sum of the Shapley values. For the second observation, the prediction is 1.22, so 0.01 above the average prediction. The order and magnitude of contributions is not the same as well; and surprisingly, the contribution of the instrumented education to the prediction is negative.\n\n\nOk, let’s end this here. I’m quite certain that explainability methods will help econometricians adopt more machine learning methods in the future, and I am also excited to see the research of causality in machine learning and AI continue."
  },
  {
    "objectID": "posts/2018-11-01-nethack.html",
    "href": "posts/2018-11-01-nethack.html",
    "title": "From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack",
    "section": "",
    "text": "If someone told me a decade ago (back before I'd ever heard the term \"roguelike\") what I'd be doing today, I would have trouble believing this…Yet here we are. pic.twitter.com/N6Hh6A4tWl\n\n— Josh Ge (@GridSageGames) June 21, 2018"
  },
  {
    "objectID": "posts/2018-11-01-nethack.html#update-07-11-2018",
    "href": "posts/2018-11-01-nethack.html#update-07-11-2018",
    "title": "From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack",
    "section": "\nUpdate 07-11-2018\n",
    "text": "Update 07-11-2018\n\n\nThe {nethack} package currently on Github contains a sample of 6000 NetHack games played on the alt.org/nethack public server between April and November 2018. This data was kindly provided by @paxed. The tutorial in this blog post is still useful if you want to learn more about scraping with R and building a data package."
  },
  {
    "objectID": "posts/2018-11-01-nethack.html#abstract",
    "href": "posts/2018-11-01-nethack.html#abstract",
    "title": "From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack",
    "section": "\nAbstract\n",
    "text": "Abstract\n\n\nIn this post, I am going to show you how you can scrape tables from a website, and then create a package with the tidied data to share with the world. The data I am going to scrape comes from a NetHack public server (link). The data I discuss in this blog post is available in the {nethack} package I created and I will walk you through the process of releasing your package on CRAN. However, {nethack} is too large to be on CRAN (75 mb, while the maximum allowed is 5mb), so you can install it to play around with the data from github:\n\ndevtools::install_github(\"b-rodrigues/nethack\")\n\nAnd to use it:\n\nlibrary(nethack)\ndata(\"nethack\")\n\nThe data contains information on games played from 2001 to 2018; 322485 rows and 14 columns. I will analyze the data in a future blog post. This post focuses on getting and then sharing the data. By the way, all the content from the public server I scrape is under the CC BY 4.0 license.\n\n\nI built the package by using the very useful {devtools} package."
  },
  {
    "objectID": "posts/2018-11-01-nethack.html#introduction",
    "href": "posts/2018-11-01-nethack.html#introduction",
    "title": "From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nNetHack is a game released in 1987 that is still being played and developed today. NetHack is a roguelike game, meaning that is has procedurally generated dungeons and permadeath. If you die, you have to start over, and because the dungeons are procedurally generated, this means that you cannot learn the layout of the dungeons you explore or know when ennemies are going to attack or even what ennemies are going to attack. Ennemies are not the only thing that you have to be careful about; you can die from a lot of different events, as you will see in this post. Objects that you find, such as a silver ring, might be helpful in a run, but be cursed in the next run.\n\n\nThe latest version of the game, 3.6.1, was released on April 27th 2018, and this is how it looks like:\n\n\n\n\n\nThe graphics are… bare-bones to say the least. The game runs inside a terminal emulator and is available for any platform. The goal of NetHack is to explore a dungeon and go down every level until you find the Amulet of Yendor. Once you find this Amulet, you have to go all the way back upstairs, enter and fight your way through the Elemental Planes, enter the final Astral Plane, and then finally offer the Amulet of Yendor to your god to finish the game. Needless to say, NetHack is very difficult and players can go years without ever finishing the game.\n\n\nWhen you start an new game, you have to create a character, which can have several attributes. You have to choose a race (human, elf, orc, etc), a role (tourist, samurai, mage, etc) and an alignment (neutral, law, chaos) and these choices impact your base stats.\n\n\nIf you can’t get past the ASCII graphics, you can play NetHack with tileset:\n\n\n\n\n\nYou can install NetHack on your computer or you can play online on a public server, such as this one. There are several advantages when playing on a pubic server; the player does not have to install anyhing, and we data enthusiasts have access to a mine of information! For example, you can view the following table which contains data on all the games played on October 25th 2018. These tables start in the year 2001, and I am going to scrape the info from these tables, which will allow me to answer several questions. For instance, what is the floor most players die on? What kills most players? What role do players choose more often? I will explore this questions in a future blog post, but for now I will focus on scraping the data and realeasing it as a package to CRAN."
  },
  {
    "objectID": "posts/2018-11-01-nethack.html#scraping-the-data",
    "href": "posts/2018-11-01-nethack.html#scraping-the-data",
    "title": "From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack",
    "section": "\nScraping the data\n",
    "text": "Scraping the data\n\n\nTo scrape the data I wrote a big function that does several things:\n\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n\nscrape_one_day &lt;- function(link){\n\n    convert_to_seconds &lt;- function(time_string){\n        time_numeric &lt;- time_string %&gt;%\n            str_split(\":\", simplify = TRUE) %&gt;%\n            as.numeric\n     \n     time_in_seconds &lt;- time_numeric * c(3600, 60, 1)\n     \n     if(is.na(time_in_seconds)){\n         time_in_seconds &lt;- 61\n     } else {\n         time_in_seconds &lt;- sum(time_in_seconds)\n     }\n     return(time_in_seconds)\n    }\n\n    Sys.sleep(1)\n\n    date &lt;- str_extract(link, \"\\\\d{8}\")\n\n    read_lines_slow &lt;- function(...){\n        Sys.sleep(1)\n        read_lines(...)\n    }\n    \n    page &lt;- read_html(link)\n\n        # Get links\n    dumplogs &lt;- page %&gt;% \n        html_nodes(xpath = '//*[(@id = \"perday\")]//td') %&gt;%\n        html_children() %&gt;%\n        html_attr(\"href\") %&gt;%\n        keep(str_detect(., \"dumplog\"))\n\n    # Get table\n    table &lt;- page %&gt;%\n        html_node(xpath = '//*[(@id = \"perday\")]') %&gt;%\n        html_table(fill = TRUE)\n\n    if(is_empty(dumplogs)){\n        print(\"dumplogs empty\")\n        dumplogs &lt;- rep(NA, nrow(table))\n    } else {\n        dumplogs &lt;- dumplogs\n    }\n    \n    final &lt;- table %&gt;%\n        janitor::clean_names() %&gt;%\n        mutate(dumplog_links = dumplogs)\n\n    print(paste0(\"cleaning data of date \", date))\n    \n    clean_final &lt;- final %&gt;%\n        select(-x) %&gt;%\n        rename(role = x_2,\n               race = x_3,\n               gender = x_4,\n               alignment = x_5) %&gt;%\n        mutate(time_in_seconds = map(time, convert_to_seconds)) %&gt;%\n        filter(!(death %in% c(\"quit\", \"escaped\")), time_in_seconds &gt; 60) %&gt;%\n        mutate(dumplog = map(dumplog_links, ~possibly(read_lines_slow, otherwise = NA)(.))) %&gt;%\n        mutate(time_in_seconds = ifelse(time_in_seconds == 61, NA, time_in_seconds))\n\n    saveRDS(clean_final, paste0(\"datasets/data_\", date, \".rds\"))\n\n}\n\nLet’s go through each part. The first part is a function that converts strings like “02:21:76” to seconds:\n\nconvert_to_seconds &lt;- function(time_string){\n    time_numeric &lt;- time_string %&gt;%\n        str_split(\":\", simplify = TRUE) %&gt;%\n        as.numeric\n \ntime_in_seconds &lt;- time_numeric * c(3600, 60, 1)\n \nif(is.na(time_in_seconds)){\n  time_in_seconds &lt;- 61\n  } else {\n    time_in_seconds &lt;- sum(time_in_seconds)\n    }\nreturn(time_in_seconds)\n}\n\nI will use this function on the column that gives the length of the run. However, before March 2008 this column is always empty, this is why I have the if()…else() statement at the end; if the time in seconds is NA, then I make it 61 seconds. I do this because I want to keep runs longer than 60 seconds, something I use filter() for later. But when filtering, if the condition returns NA (which happens when you do NA &gt; 60) then you get an error, and the function fails.\n\n\nThe website links I am going to scrape all have the date of the day the runs took place. I am going to keep this date because I will need to name the datasets I am going to write to disk:\n\ndate &lt;- str_extract(link, \"\\\\d{8}\")\n\nNext, I define this function:\n\nread_lines_slow &lt;- function(...){\n    Sys.sleep(1)\n    read_lines(...)\n}\n\nIt is a wrapper around the readr::read_lines() with a call to Sys.sleep(1). I will be scraping a lot of pages, so letting one second pass between each page will not overload the servers so much.\n\n\nI then read the link with read_html() and start by getting the links of the dumplogs:\n\npage &lt;- read_html(link)\n\n# Get links\ndumplogs &lt;- page %&gt;% \n    html_nodes(xpath = '//*[(@id = \"perday\")]//td') %&gt;%\n    html_children() %&gt;%\n    html_attr(\"href\") %&gt;%\n    keep(str_detect(., \"dumplog\"))\n\nYou might be wondering what are dumplogs. Take a look at this screenshot:\n\n\n\n\n\nWhen you click on those d’s, you land on a page like this one (I archived it to be sure that this link will not die). These logs contain a lot of info that I want to keep. To find the right xpath to scrape the links, //*[(@id = \"perday\")]//td, I used the SelectorGadget* extension for Chrome. First I chose the table:\n\n\n\n\n\nand then the links I am interested in:\n\n\n\n\n\nPutting them together, I get the right “xpath”. But just as with the time of the run, dumplogs are only available after a certain date. So in case the dumplogs column is empty, I relpace it with NA.\n\nif(is_empty(dumplogs)){\n    print(\"dumplogs empty\")\n    dumplogs &lt;- rep(NA, nrow(table))\n} else {\n    dumplogs &lt;- dumplogs\n}\n\nThe rest is quite simple:\n\n# Get table\ntable &lt;- page %&gt;%\n    html_node(xpath = '//*[(@id = \"perday\")]') %&gt;%\n    html_table(fill = TRUE)\n               \nfinal &lt;- table %&gt;%\n    janitor::clean_names() %&gt;%\n    mutate(dumplog_links = dumplogs)\n           \nprint(paste0(\"cleaning data of date \", date))\n\nI scrape the table, and then join the dumplog links to the table inside a new column called “dumplog_links”.\n\n\nBecause what follows is a long process, I print a message to let me know the progress of the scraping.\n\n\nNow the last part:\n\nclean_final &lt;- final %&gt;%\n    select(-x) %&gt;%\n    rename(role = x_2,\n           race = x_3,\n           gender = x_4,\n           alignment = x_5) %&gt;%\n    mutate(time_in_seconds = map(time, convert_to_seconds)) %&gt;%\n    filter(!(death %in% c(\"quit\", \"escaped\")), time_in_seconds &gt; 60) %&gt;%\n    mutate(dumplog = map(dumplog_links, ~possibly(read_lines_slow, otherwise = NA)(.))) %&gt;%\n    mutate(time_in_seconds = ifelse(time_in_seconds == 61, NA, time_in_seconds))\n\nI first remove and remane columns. Then I convert the “time” column into seconds and also remove runs that lasted less than 60 seconds or that ended either in “quit” (the player left the game) or “escaped” (the player left the dungeon and the game ended immediately). There are a lot of runs like that and they’re not interesting. Finally, and this is what takes long, I create a new list-column where each element is the contents of the dumplog for that run. I wrap read_lines_slow() around purrr::possibly() because dumplogs are missing for certains runs and when I try to read them I get an 404 error back. Getting such an error stops the whole process, so with purrr::possibly() I can specify that in that case I want NA back. Basically, a function wrapped inside purrr::possibly() never fails! Finally, if a game lasts for 61 seconds, I convert it back to NA (remember this was used to avoid having problems with the filter() function).\n\n\nFinally, I export what I scraped to disk:\n\nsaveRDS(clean_final, paste0(\"datasets/data_\", date, \".rds\"))\n\nThis is where I use the date; to name the data. This is really important because scraping takes a very long time, so if I don’t write the progress to disk as it goes, I might lose hours of work if my internet goes down, or if computer freezes or whatever.\n\n\nIn the lines below I build the links that I am going to scrape. They’re all of the form: https://alt.org/nethack/gamesday.php?date=YYYYMMDD so it’s quite easy to create a list of dates to scrape, for example, for the year 2017:\n\nlink &lt;- \"https://alt.org/nethack/gamesday.php?date=\"\n\ndates &lt;- seq(as.Date(\"2017/01/01\"), as.Date(\"2017/12/31\"), by = \"day\") %&gt;%\n    str_remove_all(\"-\")\n\nlinks &lt;- paste0(link, dates)\n\nNow I can easily scrape the data. To make extra sure that I will not have problems during the scraping process, for example if on a given day no games were played (and thus there is no table to scrape, which would result in an error) , I use the same trick as above by using purrr::possibly():\n\nmap(links, ~possibly(scrape_one_day, otherwise = NULL)(.))\n\nThe scraping process took a very long time. I scraped all the data by letting my computer run for three days!\n\n\nAfter this long process, I import all the .rds files into R:\n\npath_to_data &lt;- Sys.glob(\"datasets/*.rds\")\nnethack_data &lt;- map(path_to_data, readRDS)\n\nand take a look at one of them:\n\nnethack_data[[5812]] %&gt;% \n  View()\n\nLet’s convert the “score” column to integer. For this, I will need to convert strings that look like “12,232” to integers. I’ll write a short function to do this:\n\nto_numeric &lt;- function(string){\n  str_remove_all(string, \",\") %&gt;%\n    as.numeric\n}\nnethack_data &lt;- nethack_data %&gt;%\n  map(~mutate(., score = to_numeric(score)))\n\nLet’s merge the data into a single data frame:\n\nnethack_data &lt;- bind_rows(nethack_data)\n\nNow that I have a nice data frame, I will remove some columns and start the process of making a packages. I remove the columns that I created and that are now useless (such as the dumplog_links column).\n\nnethack_data &lt;- nethack_data %&gt;%\n  select(rank, score, name, time, turns, lev_max, hp_max, role, race, gender, alignment, death,\n         date, dumplog)\n\nExport this to .rds format, as it will be needed later:\n\nsaveRDS(nethack_data, \"nethack_data.rds\")"
  },
  {
    "objectID": "posts/2018-11-01-nethack.html#making-a-package-to-share-your-data-with-the-world",
    "href": "posts/2018-11-01-nethack.html#making-a-package-to-share-your-data-with-the-world",
    "title": "From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack",
    "section": "\nMaking a package to share your data with the world\n",
    "text": "Making a package to share your data with the world\n\n\nAs stated in the beginning of this post, I will walk you through the process of creating and releasing your package on CRAN. However, the data I scraped was too large to be made available as a CRAN package. But you can still get the data from Github (link is in the abstract at the beginning of the post).\n\n\nMaking a data package is a great way to learn how to make packages, because it is relatively easy to do (for example, you do not need to write unit tests). First, let’s start a new project in RStudio:\n\n\n\n\n\nThen select “R package”:\n\n\n\n\n\nThen name your package, create a git repository and then click on “Create Project”:\n\n\n\n\n\nRStudio wil open the hello.R script which you can now modify. You got to learn from the best, so I suggest that you modify hello.R by taking inspiration from the babynames package made by Hadley Wickham which you can find here. You do not need the first two lines, and can focus on lines 4 to 13. Then, rename the script to data.R. This is how {nethack}’s looks like:\n\n#' NetHack runs data.\n#'\n#' Data on NetHack runs scraped from https://alt.org/nethack/gamesday.php\n#'\n#' @format A data frame with 14 variables: \\code{rank}, \\code{score},\n#'   \\code{name}, \\code{time}, \\code{turns}, \\code{lev_max}, \\code{hp_max}, \\code{role}, \\code{race},\n#'   \\code{gender}, \\code{alignment}, \\code{death}, \\code{date} and \\code{dumplog}\n#' \\describe{\n#' \\item{rank}{The rank of the player on that day}\n#' \\item{score}{The score the player achieved on that run}\n#' \\item{name}{The name of the player}\n#' \\item{time}{The time the player took to finish the game}\n#' \\item{turns}{The number of turns the player played before finishing the game}\n#' \\item{lev_max}{First digit: the level the player died on; second digit: the deepest explored level}\n#' \\item{hp_max}{The maximum character health points the player achieved}\n#' \\item{role}{The role the player chose to play as}\n#' \\item{race}{The race the player chose to play as}\n#' \\item{gender}{The gender the playr chose to play as}\n#' \\item{alignement}{The alignement the playr chose to play as}\n#' \\item{death}{The reason of death of the character}\n#' \\item{date}{The date the game took place}\n#' \\item{dumplog}{The log of the end game; this is a list column}\n#' }\n\"nethack\"\n\nThe comments are special, the “#” is followed by a ’; these are special comments that will be parsed by roxygen2::roxygenise() and converted to documentation files.\n\n\nNext is the DESCRIPTION file. Here is how {nethack}’s looks like:\n\nPackage: nethack\nType: Package\nTitle: Data from the Video Game NetHack\nVersion: 0.1.0\nAuthors@R: person(\"Bruno André\", \"Rodrigues Coelho\", email = \"bruno@brodrigues.co\",\n                  role = c(\"aut\", \"cre\"))\nDescription: Data from NetHack runs played between 2001 to 2018 on \n    &lt;https://alt.org/nethack/&gt;, a NetHack public server.\nDepends: R (&gt;= 2.10)\nLicense: CC BY 4.0\nEncoding: UTF-8\nLazyData: true\nRoxygenNote: 6.1.0\n\nAdapt yours accordingly. I chose the license CC BY 4.0 because this was the licence under which the original data was published. It is also a good idea to add a Vignette:\n\ndevtools::use_vignette(\"the_nethack_package\")\n\nVignettes are very useful documentation with more details and examples.\n\n\nIt is also good practice to add the script that was used to scrape the data. Such scripts go into data-raw/. Create this folder with:\n\ndevtools::use_data_raw()\n\nThis creates the data-raw/ folder where I save the script that scrapes the data. Now is time to put the data in the package. Start by importing the data:\n\nnethack &lt;- readRDS(\"nethack_data.rds\")\n\nTo add the data to your package, you can use the following command:\n\ndevtools::use_data(nethack, compress = \"xz\")\n\nThis will create the data/ folder and put the data in there in the .rda format. I use the “compress” option to make the data smaller. You can now create the documentation by running:\n\nroxygen2::roxygenise()\n\nPay attention to the log messages: you might need to remove files (for example the documentation hello.R, under the folder man/).\n\n\nNow you can finaly run R CMD Check by clicking the Check button on the “Build” pane:\n\n\n\n\n\nThis will extensively check the package for ERRORS, WARNINGS and NOTES. You need to make sure that the check passes without any ERRORS or WARNINGS and try as much as possible to remove all NOTES too. If you cannot remove a NOTE, for example in my case the following:\n\nchecking installed package size ... NOTE\n  installed size is 169.7Mb\n  sub-directories of 1Mb or more:\n    data  169.6Mb\nR CMD check results\n0 errors | 0 warnings  | 1 note \n\nYou should document it in a new file called cran-comments.md:\n\n## Test environments\n* local openSUSE Tumbleweed install, R 3.5.1\n* win-builder (devel and release)\n\n## R CMD check results\nThere were no ERRORs or WARNINGs.\n\nThere was 1 NOTE:\n\n    *   installed size is 169.7Mb\nsub-directories of 1Mb or more:\n    data  169.6Mb\n\nThe dataset contains 17 years of NetHack games played, hence the size. This package will not be updated often (max once a year).\n\nOnce you have eliminated all errors and warnings, you are almost ready to go.\n\n\nYou need now to test the package on different platforms. This depends a bit on the system you run, for me, because I run openSUSE (a GNU+Linux distribution) I have to test on Windows. This can be done with:\n\n devtools::build_win(version = \"R-release\")\n\nand:\n\n devtools::build_win(version = \"R-devel\")\n\nExplain that you have tested your package on several platforms in the cran-comments.md file.\n\n\nFinally you can add a README.md and a NEWS.md file and start the process of publishing the package on CRAN:\n\ndevtools:release()\n\nIf you want many more details than what you can find in this blog post, I urge you to read “R Packages” by Hadley Wickham, which you can read for free here."
  },
  {
    "objectID": "posts/2019-06-04-cosine_sim.html",
    "href": "posts/2019-06-04-cosine_sim.html",
    "title": "Using cosine similarity to find matching documents: a tutorial using Seneca’s letters to his friend Lucilius",
    "section": "",
    "text": "Lately I’ve been interested in trying to cluster documents, and to find similar documents based on their contents. In this blog post, I will use Seneca’s Moral letters to Lucilius and compute the pairwise cosine similarity of his 124 letters. Computing the cosine similarity between two vectors returns how similar these vectors are. A cosine similarity of 1 means that the angle between the two vectors is 0, and thus both vectors have the same direction. Seneca’s Moral letters to Lucilius deal mostly with philosophical topics, as Seneca was, among many other things, a philosopher of the stoic school. The stoic school of philosophy is quite interesting, but it has been unfortunately misunderstood, especially in modern times. There is now a renewed interest for this school, see Modern Stoicism.\n\n\nThe first step is to scrape the letters. The code below scrapes the letters, and saves them into a list. I first start by writing a function that gets the raw text. Note the xpath argument of the html_nodes() function. I obtained this complex expression by using the SelectorGadget extension for Google Chrome, and then selecting the right element of the web page. See this screenshot if my description was not very clear.\n\n\nThen, the extract_text() function extracts the text from the letter. The only line that might be a bit complex is discard(~==(., \"\")) which removes every empty line.\n\n\nFinally, there’s the get_letter() function that actually gets the letter by calling the first two functions. In the last line, I get all the letters into a list by mapping the list of urls to the get_letter() function.\n\nlibrary(tidyverse)\nlibrary(rvest)\n\nbase_url &lt;- \"https://en.wikisource.org/wiki/Moral_letters_to_Lucilius/Letter_\"\n\nletter_numbers &lt;- seq(1, 124)\n\nletter_urls &lt;- paste0(base_url, letter_numbers)\n\nget_raw_text &lt;- function(base_url, letter_number){\n  paste0(base_url, letter_number) %&gt;%\n    read_html() %&gt;%\n    html_nodes(xpath ='//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"mw-parser-output\", \" \" ))]') %&gt;%  \n    html_text()\n}\n\n\nextract_text &lt;- function(raw_text, letter_number){\n  raw_text &lt;- raw_text %&gt;%\n    str_split(\"\\n\") %&gt;%  \n    flatten_chr() %&gt;%  \n    discard(~`==`(., \"\"))\n\n  start &lt;- 5\n\n  end &lt;- str_which(raw_text, \"Footnotes*\")\n\n  raw_text[start:(end-1)] %&gt;%\n    str_remove_all(\"\\\\[\\\\d{1,}\\\\]\") %&gt;%\n    str_remove_all(\"\\\\[edit\\\\]\")\n}\n\nget_letter &lt;- function(base_url, letter_number){\n\n  raw_text &lt;- get_raw_text(base_url, letter_number)\n\n  extract_text(raw_text, letter_number)\n}\n\nletters_to_lucilius &lt;- map2(base_url, letter_numbers, get_letter)\n\nNow that we have the letters saved in a list, we need to process the text a little bit. In order to compute the cosine similarity between the letters, I need to somehow represent them as vectors. There are several ways of doing this, and I am going to compute the tf-idf of each letter. The tf-idf will give me a vector for each letter, with zero and non-zero values. Zero values represent words that are common to all letters, and thus do not have any predictive power. Non-zero values are words that are not present in all letters, but maybe only a few. I expect that letters that discuss death for example, will have the word death in them, and letters that do not discuss death will not have this word. The word death thus has what I call predictive power, in that it helps us distinguish the letters discussing death from the other letters that do not discuss it. The same reasoning can be applied for any topic.\n\n\nSo, to get the tf-idf of each letter, I first need to put them in a tidy dataset. I will use the {tidytext} package for this. First, I load the required packages, convert each letter to a dataframe of one column that contains the text, and save the letter’s titles into another list:\n\nlibrary(tidytext)\nlibrary(SnowballC)\nlibrary(stopwords)\nlibrary(text2vec)\n\nletters_to_lucilius_df &lt;- map(letters_to_lucilius, ~tibble(\"text\" = .))\n\nletter_titles &lt;- letters_to_lucilius_df %&gt;%\n  map(~slice(., 1)) %&gt;%\n  map(pull)\n\nNow, I add this title to each dataframe as a new column, called title:\n\nletters_to_lucilius_df &lt;-  map2(.x = letters_to_lucilius_df, .y = letter_titles,\n                                ~mutate(.x, title = .y)) %&gt;%\n  map(~slice(., -1))\n\nI can now use unnest_tokens() to transform the datasets. Before, I had the whole text of the letter in one column. After using unnest_tokens() I now have a dataset with one row per word. This will make it easy to compute frequencies by letters, or what I am interested in, the tf-idf of each letter:\n\ntokenized_letters &lt;- letters_to_lucilius_df %&gt;%\n  bind_rows() %&gt;%\n  group_by(title) %&gt;%\n  unnest_tokens(word, text)\n\nI can now remove stopwords, using the data containing in the {stopwords} package:\n\nstopwords_en &lt;- tibble(\"word\" = stopwords(\"en\", source  = \"smart\"))\n\ntokenized_letters &lt;- tokenized_letters %&gt;%\n  anti_join(stopwords_en) %&gt;%\n  filter(!str_detect(word, \"\\\\d{1,}\"))\n## Joining, by = \"word\"\n\nNext step, wordstemming, meaning, going from “dogs” to “dog”, or from “was” to “be”. If you do not do wordstemming, “dogs” and “dog” will be considered different words, even though they are not. wordStem() is a function from {SnowballC}.\n\ntokenized_letters &lt;- tokenized_letters %&gt;%\n  mutate(word = wordStem(word, language = \"en\"))\n\nFinally, I can compute the tf-idf of each letter and cast the data as a sparse matrix:\n\ntfidf_letters &lt;- tokenized_letters %&gt;%\n  count(title, word, sort  = TRUE) %&gt;%\n  bind_tf_idf(word, title, n)\n\nsparse_matrix &lt;- tfidf_letters %&gt;%\n  cast_sparse(title, word, tf)\n\nLet’s take a look at the sparse matrix:\n\nsparse_matrix[1:10, 1:4]\n## 10 x 4 sparse Matrix of class \"dgCMatrix\"\n##                                                                   thing\n## CXIII. On the Vitality of the Soul and Its Attributes       0.084835631\n## LXVI. On Various Aspects of Virtue                          0.017079890\n## LXXXVII. Some Arguments in Favour of the Simple Life        0.014534884\n## CXVII. On Real Ethics as Superior to Syllogistic Subtleties 0.025919732\n## LXXVI. On Learning Wisdom in Old Age                        0.021588946\n## CII. On the Intimations of Our Immortality                  0.014662757\n## CXXIV. On the True Good as Attained by Reason               0.010139417\n## XCIV. On the Value of Advice                                0.009266409\n## LXXXI. On Benefits                                          0.007705479\n## LXXXV. On Some Vain Syllogisms                              0.013254786\n##                                                                     live\n## CXIII. On the Vitality of the Soul and Its Attributes       0.0837751856\n## LXVI. On Various Aspects of Virtue                          .           \n## LXXXVII. Some Arguments in Favour of the Simple Life        0.0007267442\n## CXVII. On Real Ethics as Superior to Syllogistic Subtleties 0.0050167224\n## LXXVI. On Learning Wisdom in Old Age                        0.0025906736\n## CII. On the Intimations of Our Immortality                  0.0019550342\n## CXXIV. On the True Good as Attained by Reason               .           \n## XCIV. On the Value of Advice                                0.0023166023\n## LXXXI. On Benefits                                          0.0008561644\n## LXXXV. On Some Vain Syllogisms                              0.0022091311\n##                                                                   good\n## CXIII. On the Vitality of the Soul and Its Attributes       0.01166490\n## LXVI. On Various Aspects of Virtue                          0.04132231\n## LXXXVII. Some Arguments in Favour of the Simple Life        0.04578488\n## CXVII. On Real Ethics as Superior to Syllogistic Subtleties 0.04849498\n## LXXVI. On Learning Wisdom in Old Age                        0.04663212\n## CII. On the Intimations of Our Immortality                  0.05180841\n## CXXIV. On the True Good as Attained by Reason               0.06717364\n## XCIV. On the Value of Advice                                0.01081081\n## LXXXI. On Benefits                                          0.01626712\n## LXXXV. On Some Vain Syllogisms                              0.01472754\n##                                                                 precept\n## CXIII. On the Vitality of the Soul and Its Attributes       .          \n## LXVI. On Various Aspects of Virtue                          .          \n## LXXXVII. Some Arguments in Favour of the Simple Life        .          \n## CXVII. On Real Ethics as Superior to Syllogistic Subtleties .          \n## LXXVI. On Learning Wisdom in Old Age                        .          \n## CII. On the Intimations of Our Immortality                  .          \n## CXXIV. On the True Good as Attained by Reason               0.001267427\n## XCIV. On the Value of Advice                                0.020463320\n## LXXXI. On Benefits                                          .          \n## LXXXV. On Some Vain Syllogisms                              .\n\nWe can consider each row of this matrix as the vector representing a letter, and thus compute the cosine similarity between letters. For this, I am using the sim2() function from the {text2vec} package. I then create the get_similar_letters() function that returns similar letters for a given reference letter:\n\nsimilarities &lt;- sim2(sparse_matrix, method = \"cosine\", norm = \"l2\") \n\nget_similar_letters &lt;- function(similarities, reference_letter, n_recommendations = 3){\n  sort(similarities[reference_letter, ], decreasing = TRUE)[1:(2 + n_recommendations)]\n}\nget_similar_letters(similarities, 19)\n##          XXX. On Conquering the Conqueror \n##                                 1.0000000 \n##                  XXIV. On Despising Death \n##                                 0.6781600 \n##      LXXXII. On the Natural Fear of Death \n##                                 0.6639736 \n## LXX. On the Proper Time to Slip the Cable \n##                                 0.5981706 \n## LXXVIII. On the Healing Power of the Mind \n##                                 0.4709679\nget_similar_letters(similarities, 99)\n##                              LXI. On Meeting Death Cheerfully \n##                                                     1.0000000 \n##                     LXX. On the Proper Time to Slip the Cable \n##                                                     0.5005015 \n## XCIII. On the Quality, as Contrasted with the Length, of Life \n##                                                     0.4631796 \n##                         CI. On the Futility of Planning Ahead \n##                                                     0.4503093 \n##                              LXXVII. On Taking One's Own Life \n##                                                     0.4147019\nget_similar_letters(similarities, 32)\n##                                    LIX. On Pleasure and Joy \n##                                                   1.0000000 \n##          XXIII. On the True Joy which Comes from Philosophy \n##                                                   0.4743672 \n##                          CIX. On the Fellowship of Wise Men \n##                                                   0.4526835 \n## XC. On the Part Played by Philosophy in the Progress of Man \n##                                                   0.4498278 \n##         CXXIII. On the Conflict between Pleasure and Virtue \n##                                                   0.4469312\nget_similar_letters(similarities, 101)\n##                    X. On Living to Oneself \n##                                  1.0000000 \n##          LXXIII. On Philosophers and Kings \n##                                  0.3842292 \n##                  XLI. On the God within Us \n##                                  0.3465457 \n##                       XXXI. On Siren Songs \n##                                  0.3451388 \n## XCV. On the Usefulness of Basic Principles \n##                                  0.3302794\n\nAs we can see from these examples, this seems to be working quite well: the first title is the title of the reference letter, will the next 3 are the suggested letters. The problem is that my matrix is not in the right order, and thus reference letter 19 does not correspond to letter 19 of Seneca… I have to correct that, but not today."
  },
  {
    "objectID": "posts/2022-07-23-grepl_vs_stringi.html",
    "href": "posts/2022-07-23-grepl_vs_stringi.html",
    "title": "What’s the fastest way to search and replace strings in a data frame?",
    "section": "",
    "text": "I’ve tweeted this:\n\n\n\nJust changed like 100 grepl calls to stringi::stri_detect and my pipeline now runs 4 times faster #RStats\n\n— Bruno Rodrigues (@brodriguesco) July 20, 2022\n\n\n\nmuch discussed ensued. Some people were surprised, because in their experience, grepl() was faster than alternatives, especially if you set the perl parameter in grepl() to TRUE. My use case was quite simple; I have a relatively large data set (half a million lines) with one column with several misspelling of city names. So I painstakingly wrote some code to correct the spelling of the major cities (those that came up often enough to matter. Minor cities were set to “Other”. Sorry, Wiltz!)\n\n\nSo in this short blog post, I benchmark some code to see if what I did the other day was a fluke. Maybe something weird with my R installation on my work laptop running Windows 10 somehow made stri_detect() run faster than grepl()? I don’t even know if something like that is possible. I’m writing these lines on my Linux machine, unlike the code I run at work. So maybe if I find some differences, they could be due to the different OS running. I don’t want to have to deal with Windows on my days off (for my blood pressure’s sake), so I’m not running this benchmark on my work laptop. So that part we’ll never know.\n\n\nAnyways, let’s start by getting some data. I’m not commenting the code below, because that’s not the point of this post.\n\nlibrary(dplyr)\nlibrary(stringi)\nlibrary(stringr)\nlibrary(re2)\n\nadult &lt;- vroom::vroom(\n  \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n)\n\nadult_colnames &lt;- readLines(\n  \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\"\n)\n\nadult_colnames &lt;- adult_colnames[97:110] %&gt;%\n  str_extract(\".*(?=:)\") %&gt;%\n  str_replace_all(\"-\", \"_\")\n\nadult_colnames &lt;- c(adult_colnames, \"wage\")\n\ncolnames(adult) &lt;- adult_colnames\n\nadult\n## # A tibble: 32,560 × 15\n##      age workclass    fnlwgt educa…¹ educa…² marit…³ occup…⁴ relat…⁵ race  sex  \n##    &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n##  1    50 Self-emp-no…  83311 Bachel…      13 Marrie… Exec-m… Husband White Male \n##  2    38 Private      215646 HS-grad       9 Divorc… Handle… Not-in… White Male \n##  3    53 Private      234721 11th          7 Marrie… Handle… Husband Black Male \n##  4    28 Private      338409 Bachel…      13 Marrie… Prof-s… Wife    Black Fema…\n##  5    37 Private      284582 Masters      14 Marrie… Exec-m… Wife    White Fema…\n##  6    49 Private      160187 9th           5 Marrie… Other-… Not-in… Black Fema…\n##  7    52 Self-emp-no… 209642 HS-grad       9 Marrie… Exec-m… Husband White Male \n##  8    31 Private       45781 Masters      14 Never-… Prof-s… Not-in… White Fema…\n##  9    42 Private      159449 Bachel…      13 Marrie… Exec-m… Husband White Male \n## 10    37 Private      280464 Some-c…      10 Marrie… Exec-m… Husband Black Male \n## # … with 32,550 more rows, 5 more variables: capital_gain &lt;dbl&gt;,\n## #   capital_loss &lt;dbl&gt;, hours_per_week &lt;dbl&gt;, native_country &lt;chr&gt;, wage &lt;chr&gt;,\n## #   and abbreviated variable names ¹​education, ²​education_num, ³​marital_status,\n## #   ⁴​occupation, ⁵​relationship\n## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nLet’s now write the functions used for benchmarking. There will be 5 of them:\n\n\n\nOne using grepl() without any fancy options;\n\n\nOne using grepl() where perl is set to TRUE;\n\n\nOne that uses stringi::stri_detect();\n\n\nOne that uses stringr::str_detect();\n\n\nOne that uses re2::re2_detect().\n\n\n\nBelow you can read the functions. They’re all pretty much the same, only the function looking for the string changes. These functions look for a string in the marital_status variable and create a new variable with a corresponding integer.\n\nwith_grepl &lt;- function(dataset){\n  dataset |&gt;\n    mutate(married = case_when(\n             grepl(\"Married\", marital_status) ~ 1,\n             grepl(\"married\", marital_status) ~ 2,\n             TRUE ~ 3)\n           )\n}\n\nwith_grepl_perl &lt;- function(dataset){\n  dataset |&gt;\n    mutate(married = case_when(\n             grepl(\"Married\", marital_status, perl = TRUE) ~ 1,\n             grepl(\"married\", marital_status, perl = TRUE) ~ 2,\n             TRUE ~ 3)\n           )\n}\n\nwith_stringi &lt;- function(dataset){\n  dataset |&gt;\n    mutate(married = case_when(\n             stri_detect(marital_status, regex = \"Married\") ~ 1,\n             stri_detect(marital_status, regex = \"married\") ~ 2,\n             TRUE ~ 3)\n           )\n}\n\nwith_stringr &lt;- function(dataset){\n  dataset |&gt;\n    mutate(married = case_when(\n             str_detect(marital_status, \"Married\") ~ 1,\n             str_detect(marital_status, \"married\") ~ 2,\n             TRUE ~ 3)\n           )\n}\n\nwith_re2 &lt;- function(dataset){\n  dataset |&gt;\n    mutate(married = case_when(\n             re2_detect(marital_status, \"Married\") ~ 1,\n             re2_detect(marital_status, \"married\") ~ 2,\n             TRUE ~ 3)\n           )\n}\n\nNow I make extra sure these functions actually return the exact same thing. So for this I’m running them once on the data and use testthat::expect_equal(). It’s a bit unwieldy, so if you have a better way of doing this, please let me know.\n\nrun_grepl &lt;- function(){\n  with_grepl(adult) %&gt;%\n    count(married, marital_status)\n}\n\none &lt;- run_grepl()\n\nrun_grepl_perl &lt;- function(){\n  with_grepl_perl(adult) %&gt;%\n    count(married, marital_status)\n}\n\ntwo &lt;- run_grepl_perl()\n\nrun_stringi &lt;- function(){\n  with_stringi(adult) %&gt;%\n    count(married, marital_status)\n}\n\nthree &lt;- run_stringi()\n\nrun_stringr &lt;- function(){\n  with_stringr(adult) %&gt;%\n    count(married, marital_status)\n}\n\nfour &lt;- run_stringr()\n\nrun_re2 &lt;- function(){\n  with_re2(adult) %&gt;%\n    count(married, marital_status)\n}\n\nfive &lt;- run_re2()\n\none_eq_two &lt;- testthat::expect_equal(one, two)\none_eq_three &lt;- testthat::expect_equal(one, three)\nthree_eq_four &lt;- testthat::expect_equal(three, four)\n\ntestthat::expect_equal(\n            one_eq_two,\n            one_eq_three\n          )\n\ntestthat::expect_equal(\n            one_eq_three,\n            three_eq_four\n          )\n\ntestthat::expect_equal(\n            one,\n            five)\n\ntestthat::expect_equal() does not complain, so I’m pretty sure my functions, while different, return the exact same thing. Now, we’re ready for the benchmark itself. Let’s run these function 500 times using {microbenchmark}:\n\nmicrobenchmark::microbenchmark(\n     run_grepl(),\n     run_grepl_perl(),\n     run_stringi(),\n     run_stringr(),\n     run_re2(),\n     times = 500\n)\n## Unit: milliseconds\n##              expr      min       lq     mean   median       uq      max neval\n##       run_grepl() 24.37832 24.89573 26.64820 25.50033 27.05967 115.0769   500\n##  run_grepl_perl() 19.03446 19.41323 20.91045 19.89093 21.16683 104.3917   500\n##     run_stringi() 23.01141 23.40151 25.00304 23.82441 24.83598 104.8065   500\n##     run_stringr() 22.98317 23.44332 25.32851 23.92721 25.18168 145.5861   500\n##         run_re2() 22.22656 22.60817 24.07254 23.05895 24.22048 108.6825   500\n\nThere you have it folks! The winner is grepl() with perl = TRUE, and then it’s pretty much tied between stringi(), stringr() and re2() (maybe there’s a slight edge for re2()) and grepl() without perl = TRUE is last. But don’t forget that this is running on my machine with Linux installed on it; maybe you’ll get different results on different hardware and OSs! So if you rely a lot on grepl() and other such string manipulation function, maybe run a benchmark on your hardware first. How come switching from grepl() (without perl = TRUE though) to stri_detect() made my pipeline at work run 4 times faster I don’t know. Maybe it has also to do with the size of the data, and the complexity of the regular expression used to detect the problematic strings?"
  },
  {
    "objectID": "posts/2018-11-03-nethack_analysis.html#abstract",
    "href": "posts/2018-11-03-nethack_analysis.html#abstract",
    "title": "Analyzing NetHack data, part 1: What kills the players",
    "section": "\nAbstract\n",
    "text": "Abstract\n\n\nIn this post, I will analyse the data I scraped and put into an R package, which I called {nethack}. NetHack is a roguelike game; for more context, read my previous blog post. You can install the {nethack} package and play around with the data yourself by installing it from github:\n\ndevtools::install_github(\"b-rodrigues/nethack\")\n\nAnd to use it:\n\nlibrary(nethack)\ndata(\"nethack\")\n\nThe data contains information on games played from 2001 to 2018; 322485 rows and 14 columns. I will analyze the data in a future blog post. This post focuses on getting and then sharing the data. By the way, all the content from the public server I scrape is under the CC BY 4.0 license.\n\n\nI built the package by using the very useful {devtools} package."
  },
  {
    "objectID": "posts/2018-11-03-nethack_analysis.html#introduction",
    "href": "posts/2018-11-03-nethack_analysis.html#introduction",
    "title": "Analyzing NetHack data, part 1: What kills the players",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nWhat I want from this first analysis are several, simple things: how many players manage to ascend (meaning, winning), what monster kills most players, and finally extract data from the dumplog column. The dumplog column is a bit special; each element of the dumplog column is a log file that contains a lot of information from the last turns of a player. I will leave this for a future blog post, though.\n\n\nLet’s load some packages first:\n\nlibrary(nethack)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(magrittr)\nlibrary(brotools)\n\n{brotools} is my own package that contains some functions that I use daily. If you want to install it, run the following line:\n\ndevtools::install_github(\"b-rodrigues/brotools\")\n\nThe documentation is not up-to-date, I think I’ll do that and release it on CRAN. Some day.\n\n\nNow, let’s load the “nethack” data, included in the {nethack} package:\n\n##   rank score     name time turns lev_max hp_max role race gender alignment\n## 1    1   360      jkm &lt;NA&gt;    NA     2/2  -2/25  Sam  Hum    Mal       Law\n## 2    2   172 yosemite &lt;NA&gt;    NA     1/1  -1/10  Tou  Hum    Fem       Neu\n## 3    3  2092    dtype &lt;NA&gt;    NA     6/7  -2/47  Val  Hum    Fem       Neu\n## 4    4    32   joorko &lt;NA&gt;    NA     1/1   0/15  Sam  Hum    Mal       Law\n## 5    5   118    jorko &lt;NA&gt;    NA     1/1   0/11  Rog  Orc    Fem       Cha\n## 6    6  1757   aaronl &lt;NA&gt;    NA     5/5   0/60  Bar  Hum    Mal       Neu\n##                                                      death       date\n## 1                                   killed by a brown mold 2001-10-24\n## 2                                       killed by a jackal 2001-10-24\n## 3                                     killed by a fire ant 2001-10-24\n## 4                                       killed by a jackal 2001-10-24\n## 5                                       killed by a jackal 2001-10-24\n## 6 killed by a hallucinogen-distorted ghoul, while helpless 2001-10-24\n##   dumplog\n## 1      NA\n## 2      NA\n## 3      NA\n## 4      NA\n## 5      NA\n## 6      NA\ndata(\"nethack\")\n\nhead(nethack)\n\nLet’s create some variables that might be helpful (or perhaps not, we’ll see):\n\nnethack %&lt;&gt;% \n  mutate(date = ymd(date),\n         year = year(date),\n         month = month(date),\n         day = day(date))\n\nThis makes it easy to look at the data from, say, June 2017:\n\nnethack %&gt;%\n  filter(year == 2017, month == 6) %&gt;%\n  brotools::describe()\n## # A tibble: 17 x 17\n##    variable type   nobs    mean      sd mode    min     max   q05   q25\n##    &lt;chr&gt;    &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 day      Nume…  1451    17.4  9.00e0 1         1      30     2   10 \n##  2 month    Nume…  1451     6    0.     6         6       6     6    6 \n##  3 rank     Nume…  1451    47.1  2.95e1 1         1     100     4   20 \n##  4 score    Nume…  1451 38156.   3.39e5 488       0 5966425    94  402.\n##  5 turns    Nume…  1451  4179.   1.23e4 812       1  291829   204  860.\n##  6 year     Nume…  1451  2017    0.     2017   2017    2017  2017 2017 \n##  7 alignme… Char…  1451    NA   NA      Law      NA      NA    NA   NA \n##  8 death    Char…  1451    NA   NA      kill…    NA      NA    NA   NA \n##  9 gender   Char…  1451    NA   NA      Mal      NA      NA    NA   NA \n## 10 hp_max   Char…  1451    NA   NA      -1/16    NA      NA    NA   NA \n## 11 lev_max  Char…  1451    NA   NA      4/4      NA      NA    NA   NA \n## 12 name     Char…  1451    NA   NA      ohno…    NA      NA    NA   NA \n## 13 race     Char…  1451    NA   NA      Hum      NA      NA    NA   NA \n## 14 role     Char…  1451    NA   NA      Kni      NA      NA    NA   NA \n## 15 time     Char…  1451    NA   NA      01:1…    NA      NA    NA   NA \n## 16 dumplog  List   1451    NA   NA      &lt;NA&gt;     NA      NA    NA   NA \n## 17 date     Date   1451    NA   NA      &lt;NA&gt;     NA      NA    NA   NA \n## # … with 7 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, q95 &lt;dbl&gt;,\n## #   n_missing &lt;int&gt;, n_unique &lt;int&gt;, starting_date &lt;date&gt;,\n## #   ending_date &lt;date&gt;\n\nLet’s also take a look at a dumplog:\n\n\n\n\nClick to expand; the dumplog is quite long\n\n\nnethack %&gt;%\n    filter(year == 2018, month == 10) %&gt;%\n    slice(1) %&gt;%\n    pull(dumplog)\n## [[1]]\n##   [1] \"Unix NetHack Version 3.6.1 - last build Fri Apr 27 19:25:48 2018. (d4ebae12f1a709d1833cf466dd0c553fb97518d2)\"\n##   [2] \"\"                                                                                                            \n##   [3] \"Game began 2018-09-30 22:27:18, ended 2018-10-01 00:01:12.\"                                                  \n##   [4] \"\"                                                                                                            \n##   [5] \"brothertrebius, neutral female gnomish Ranger\"                                                               \n##   [6] \"\"                                                                                                            \n##   [7] \"                  -----\"                                                                                     \n##   [8] \"   --------       |....#     -----      --------\"                                                            \n##   [9] \"   |/..%.=|      #...^|######|...|    ##.......|\"                                                            \n##  [10] \"   |/[%..%|      #|...|     #|...|    # |......|\"                                                            \n##  [11] \"   |......|      #-----     #-...-######....&lt;..|\"                                                            \n##  [12] \"   -----|--      ###         -|-.-  #   |......|\"                                                            \n##  [13] \"        ##         ##           #   #   ----f---\"                                                            \n##  [14] \"         ####       #           #  ##       f@Y\"                                                             \n##  [15] \"            #       #           #  #\"                                                                        \n##  [16] \"       -----.-------#           #  #\"                                                                        \n##  [17] \"       |........%..|#           #  #\"                                                                        \n##  [18] \"       |............#           #  #\"                                                                        \n##  [19] \"       |...........|          0##  #\"                                                                        \n##  [20] \"       |...........|         -.--- #\"                                                                        \n##  [21] \"       -------------         |^..|##\"                                                                        \n##  [22] \"                             |...|#\"                                                                         \n##  [23] \"                             |0&gt;..#\"                                                                         \n##  [24] \"                             -----\"                                                                          \n##  [25] \"\"                                                                                                            \n##  [26] \"Brothertre the Trailblazer   St:15 Dx:12 Co:16 In:13 Wi:15 Ch:6  Neutral\"                                    \n##  [27] \"Dlvl:6  $:59 HP:0(54) Pw:40(40) AC:0  Exp:8 T:7398  Satiated Burdened\"                                       \n##  [28] \"\"                                                                                                            \n##  [29] \"Latest messages:\"                                                                                            \n##  [30] \" In what direction? l\"                                                                                       \n##  [31] \" You shoot 2 arrows.\"                                                                                        \n##  [32] \" The 1st arrow hits the ape.\"                                                                                \n##  [33] \" The 2nd arrow hits the ape!\"                                                                                \n##  [34] \" The ape hits!\"                                                                                              \n##  [35] \" The ape hits!\"                                                                                              \n##  [36] \" The ape bites!\"                                                                                             \n##  [37] \" You ready: q - 9 uncursed arrows.\"                                                                          \n##  [38] \" In what direction? l\"                                                                                       \n##  [39] \" The arrow hits the ape.\"                                                                                    \n##  [40] \" The ape hits!\"                                                                                              \n##  [41] \" The ape hits!\"                                                                                              \n##  [42] \" The ape bites!\"                                                                                             \n##  [43] \" The ape hits!\"                                                                                              \n##  [44] \" The ape hits!\"                                                                                              \n##  [45] \" The ape misses!\"                                                                                            \n##  [46] \" In what direction? l\"                                                                                       \n##  [47] \" You shoot 2 arrows.\"                                                                                        \n##  [48] \" The 1st arrow hits the ape!\"                                                                                \n##  [49] \" The 2nd arrow hits the ape.\"                                                                                \n##  [50] \" The ape misses!\"                                                                                            \n##  [51] \" The ape hits!\"                                                                                              \n##  [52] \" The ape misses!\"                                                                                            \n##  [53] \" In what direction? l\"                                                                                       \n##  [54] \" You shoot 2 arrows.\"                                                                                        \n##  [55] \" The 1st arrow misses the ape.\"                                                                              \n##  [56] \" The 2nd arrow hits the ape.\"                                                                                \n##  [57] \" The ape misses!\"                                                                                            \n##  [58] \" The ape hits!\"                                                                                              \n##  [59] \" The ape bites!\"                                                                                             \n##  [60] \" In what direction? l\"                                                                                       \n##  [61] \" The arrow hits the ape!\"                                                                                    \n##  [62] \" The ape hits!\"                                                                                              \n##  [63] \" The ape misses!\"                                                                                            \n##  [64] \" The ape bites!\"                                                                                             \n##  [65] \" You hear someone cursing shoplifters.\"                                                                      \n##  [66] \" The ape misses!\"                                                                                            \n##  [67] \" The ape hits!\"                                                                                              \n##  [68] \" The ape bites!\"                                                                                             \n##  [69] \" What do you want to write with? [- amnqsvBJM-OWZ or ?*] -\"                                                  \n##  [70] \" You write in the dust with your fingertip.\"                                                                 \n##  [71] \" What do you want to write in the dust here? Elbereth\"                                                       \n##  [72] \" The ape hits!\"                                                                                              \n##  [73] \" The ape hits!\"                                                                                              \n##  [74] \" You die...\"                                                                                                 \n##  [75] \" Do you want your possessions identified? [ynq] (y) y\"                                                       \n##  [76] \" Do you want to see your attributes? [ynq] (y) n\"                                                            \n##  [77] \" Do you want an account of creatures vanquished? [ynaq] (y) n\"                                               \n##  [78] \" Do you want to see your conduct? [ynq] (y) n\"                                                               \n##  [79] \" Do you want to see the dungeon overview? [ynq] (y) q\"                                                       \n##  [80] \"\"                                                                                                            \n##  [81] \"Inventory:\"                                                                                                  \n##  [82] \" Coins\"                                                                                                      \n##  [83] \"  $ - 59 gold pieces\"                                                                                        \n##  [84] \" Weapons\"                                                                                                    \n##  [85] \"  m - 17 blessed +1 arrows\"                                                                                  \n##  [86] \"  n - a blessed +0 arrow\"                                                                                    \n##  [87] \"  q - 3 +0 arrows (in quiver)\"                                                                               \n##  [88] \"  s - a +0 bow (weapon in hand)\"                                                                             \n##  [89] \"  B - 11 +1 darts\"                                                                                           \n##  [90] \"  N - 11 +0 darts\"                                                                                           \n##  [91] \"  a - a +1 dagger (alternate weapon; not wielded)\"                                                           \n##  [92] \" Armor\"                                                                                                      \n##  [93] \"  T - an uncursed +0 dwarvish iron helm (being worn)\"                                                        \n##  [94] \"  z - an uncursed +0 pair of leather gloves (being worn)\"                                                    \n##  [95] \"  U - a cursed -4 pair of iron shoes (being worn)\"                                                           \n##  [96] \"  e - an uncursed +2 cloak of displacement (being worn)\"                                                     \n##  [97] \"  h - a blessed +0 dwarvish mithril-coat (being worn)\"                                                       \n##  [98] \" Comestibles\"                                                                                                \n##  [99] \"  f - 3 uncursed cram rations\"                                                                               \n## [100] \"  j - 2 uncursed food rations\"                                                                               \n## [101] \"  L - an uncursed food ration\"                                                                               \n## [102] \"  P - an uncursed lembas wafer\"                                                                              \n## [103] \"  I - an uncursed lizard corpse\"                                                                             \n## [104] \"  o - an uncursed tin of spinach\"                                                                            \n## [105] \" Scrolls\"                                                                                                    \n## [106] \"  G - 2 uncursed scrolls of blank paper\"                                                                     \n## [107] \"  t - an uncursed scroll of confuse monster\"                                                                 \n## [108] \"  V - an uncursed scroll of identify\"                                                                        \n## [109] \" Potions\"                                                                                                    \n## [110] \"  x - an uncursed potion of gain ability\"                                                                    \n## [111] \"  H - a blessed potion of sleeping\"                                                                          \n## [112] \"  g - 3 uncursed potions of water\"                                                                           \n## [113] \" Rings\"                                                                                                      \n## [114] \"  O - an uncursed ring of slow digestion (on left hand)\"                                                     \n## [115] \"  v - an uncursed ring of stealth (on right hand)\"                                                           \n## [116] \" Tools\"                                                                                                      \n## [117] \"  p - an uncursed magic lamp\"                                                                                \n## [118] \"  k - an uncursed magic whistle\"                                                                             \n## [119] \"  Q - an uncursed mirror\"                                                                                    \n## [120] \"  C - an uncursed saddle\"                                                                                    \n## [121] \"  D - an uncursed stethoscope\"                                                                               \n## [122] \"  y - a +0 unicorn horn\"                                                                                     \n## [123] \"  i - 7 uncursed wax candles\"                                                                                \n## [124] \" Gems/Stones\"                                                                                                \n## [125] \"  W - an uncursed flint stone\"                                                                               \n## [126] \"  M - an uncursed worthless piece of red glass\"                                                              \n## [127] \"  Z - an uncursed worthless piece of violet glass\"                                                           \n## [128] \"  J - an uncursed worthless piece of white glass\"                                                            \n## [129] \"\"                                                                                                            \n## [130] \"Brothertrebius the Ranger's attributes:\"                                                                     \n## [131] \"\"                                                                                                            \n## [132] \"Background:\"                                                                                                 \n## [133] \" You were a Trailblazer, a level 8 female gnomish Ranger.\"                                                   \n## [134] \" You were neutral, on a mission for Venus\"                                                                   \n## [135] \" who was opposed by Mercury (lawful) and Mars (chaotic).\"                                                    \n## [136] \"\"                                                                                                            \n## [137] \"Final Characteristics:\"                                                                                      \n## [138] \" You had 0 hit points (max:54).\"                                                                             \n## [139] \" You had 40 magic power (max:40).\"                                                                           \n## [140] \" Your armor class was 0.\"                                                                                    \n## [141] \" You had 1552 experience points.\"                                                                            \n## [142] \" You entered the dungeon 7398 turns ago.\"                                                                    \n## [143] \" Your strength was 15 (limit:18/50).\"                                                                        \n## [144] \" Your dexterity was 12 (limit:18).\"                                                                          \n## [145] \" Your constitution was 16 (limit:18).\"                                                                       \n## [146] \" Your intelligence was 13 (limit:19).\"                                                                       \n## [147] \" Your wisdom was 15 (limit:18).\"                                                                             \n## [148] \" Your charisma was 6 (limit:18).\"                                                                            \n## [149] \"\"                                                                                                            \n## [150] \"Final Status:\"                                                                                               \n## [151] \" You were satiated.\"                                                                                         \n## [152] \" You were burdened; movement was slightly slowed.\"                                                           \n## [153] \" You were wielding a bow.\"                                                                                   \n## [154] \"\"                                                                                                            \n## [155] \"Final Attributes:\"                                                                                           \n## [156] \" You were piously aligned.\"                                                                                  \n## [157] \" You were telepathic.\"                                                                                       \n## [158] \" You had automatic searching.\"                                                                               \n## [159] \" You had infravision.\"                                                                                       \n## [160] \" You were displaced.\"                                                                                        \n## [161] \" You were stealthy.\"                                                                                         \n## [162] \" You had slower digestion.\"                                                                                  \n## [163] \" You were guarded.\"                                                                                          \n## [164] \" You are dead.\"                                                                                              \n## [165] \"\"                                                                                                            \n## [166] \"Vanquished creatures:\"                                                                                       \n## [167] \"  a warhorse\"                                                                                                \n## [168] \"  a tengu\"                                                                                                   \n## [169] \"  a quivering blob\"                                                                                          \n## [170] \" an iron piercer\"                                                                                            \n## [171] \"  2 black lights\"                                                                                            \n## [172] \"  a gold golem\"                                                                                              \n## [173] \"  a werewolf\"                                                                                                \n## [174] \"  3 lizards\"                                                                                                 \n## [175] \"  2 dingoes\"                                                                                                 \n## [176] \"  a housecat\"                                                                                                \n## [177] \"  a white unicorn\"                                                                                           \n## [178] \"  2 dust vortices\"                                                                                           \n## [179] \"  a plains centaur\"                                                                                          \n## [180] \" an ape\"                                                                                                     \n## [181] \"  a Woodland-elf\"                                                                                            \n## [182] \"  2 soldier ants\"                                                                                            \n## [183] \"  a bugbear\"                                                                                                 \n## [184] \" an imp\"                                                                                                     \n## [185] \"  a wood nymph\"                                                                                              \n## [186] \"  a water nymph\"                                                                                             \n## [187] \"  a rock piercer\"                                                                                            \n## [188] \"  a pony\"                                                                                                    \n## [189] \"  3 fog clouds\"                                                                                              \n## [190] \"  a yellow light\"                                                                                            \n## [191] \"  a violet fungus\"                                                                                           \n## [192] \"  2 gnome lords\"                                                                                             \n## [193] \"  2 gnomish wizards\"                                                                                         \n## [194] \"  2 gray oozes\"                                                                                              \n## [195] \"  2 elf zombies\"                                                                                             \n## [196] \"  a straw golem\"                                                                                             \n## [197] \"  a paper golem\"                                                                                             \n## [198] \"  2 giant ants\"                                                                                              \n## [199] \"  2 little dogs\"                                                                                             \n## [200] \"  3 floating eyes\"                                                                                           \n## [201] \"  8 dwarves\"                                                                                                 \n## [202] \"  a homunculus\"                                                                                              \n## [203] \"  3 kobold lords\"                                                                                            \n## [204] \"  3 kobold shamans\"                                                                                          \n## [205] \" 13 hill orcs\"                                                                                               \n## [206] \"  4 rothes\"                                                                                                  \n## [207] \"  2 centipedes\"                                                                                              \n## [208] \"  3 giant bats\"                                                                                              \n## [209] \"  6 dwarf zombies\"                                                                                           \n## [210] \"  a werejackal\"                                                                                              \n## [211] \"  3 iguanas\"                                                                                                 \n## [212] \" 23 killer bees\"                                                                                             \n## [213] \" an acid blob\"                                                                                               \n## [214] \"  a coyote\"                                                                                                  \n## [215] \"  3 gas spores\"                                                                                              \n## [216] \"  5 hobbits\"                                                                                                 \n## [217] \"  7 manes\"                                                                                                   \n## [218] \"  2 large kobolds\"                                                                                           \n## [219] \"  a hobgoblin\"                                                                                               \n## [220] \"  2 giant rats\"                                                                                              \n## [221] \"  2 cave spiders\"                                                                                            \n## [222] \"  a yellow mold\"                                                                                             \n## [223] \"  6 gnomes\"                                                                                                  \n## [224] \"  8 garter snakes\"                                                                                           \n## [225] \"  2 gnome zombies\"                                                                                           \n## [226] \"  8 geckos\"                                                                                                  \n## [227] \" 11 jackals\"                                                                                                 \n## [228] \"  5 foxes\"                                                                                                   \n## [229] \"  2 kobolds\"                                                                                                 \n## [230] \"  2 goblins\"                                                                                                 \n## [231] \"  a sewer rat\"                                                                                               \n## [232] \"  6 grid bugs\"                                                                                               \n## [233] \"  3 lichens\"                                                                                                 \n## [234] \"  2 kobold zombies\"                                                                                          \n## [235] \"  5 newts\"                                                                                                   \n## [236] \"206 creatures vanquished.\"                                                                                   \n## [237] \"\"                                                                                                            \n## [238] \"No species were genocided or became extinct.\"                                                                \n## [239] \"\"                                                                                                            \n## [240] \"Voluntary challenges:\"                                                                                       \n## [241] \" You never genocided any monsters.\"                                                                          \n## [242] \" You never polymorphed an object.\"                                                                           \n## [243] \" You never changed form.\"                                                                                    \n## [244] \" You used no wishes.\"                                                                                        \n## [245] \"\"                                                                                                            \n## [246] \"The Dungeons of Doom: levels 1 to 6\"                                                                         \n## [247] \"   Level 1:\"                                                                                                 \n## [248] \"      A fountain.\"                                                                                           \n## [249] \"   Level 2:\"                                                                                                 \n## [250] \"      A sink.\"                                                                                               \n## [251] \"   Level 3:\"                                                                                                 \n## [252] \"      A general store, a fountain.\"                                                                          \n## [253] \"   Level 4:\"                                                                                                 \n## [254] \"      A general store, a fountain.\"                                                                          \n## [255] \"      Stairs down to The Gnomish Mines.\"                                                                     \n## [256] \"   Level 5:\"                                                                                                 \n## [257] \"      A fountain.\"                                                                                           \n## [258] \"   Level 6: &lt;- You were here.\"                                                                               \n## [259] \"      A general store.\"                                                                                      \n## [260] \"      Final resting place for\"                                                                               \n## [261] \"         you, killed by an ape.\"                                                                             \n## [262] \"The Gnomish Mines: levels 5 to 8\"                                                                            \n## [263] \"   Level 5:\"                                                                                                 \n## [264] \"   Level 6:\"                                                                                                 \n## [265] \"   Level 7:\"                                                                                                 \n## [266] \"      Many shops, a temple, some fountains.\"                                                                 \n## [267] \"   Level 8:\"                                                                                                 \n## [268] \"\"                                                                                                            \n## [269] \"Game over:\"                                                                                                  \n## [270] \"                       ----------\"                                                                           \n## [271] \"                      /          \\\\\"                                                                         \n## [272] \"                     /    REST    \\\\\"                                                                        \n## [273] \"                    /      IN      \\\\\"                                                                       \n## [274] \"                   /     PEACE      \\\\\"                                                                      \n## [275] \"                  /                  \\\\\"                                                                     \n## [276] \"                  |  brothertrebius  |\"                                                                      \n## [277] \"                  |      59 Au       |\"                                                                      \n## [278] \"                  | killed by an ape |\"                                                                      \n## [279] \"                  |                  |\"                                                                      \n## [280] \"                  |                  |\"                                                                      \n## [281] \"                  |                  |\"                                                                      \n## [282] \"                  |       2018       |\"                                                                      \n## [283] \"                 *|     *  *  *      | *\"                                                                    \n## [284] \"        _________)/\\\\\\\\_//(\\\\/(/\\\\)/\\\\//\\\\/|_)_______\"                                                       \n## [285] \"\"                                                                                                            \n## [286] \"Goodbye brothertrebius the Ranger...\"                                                                        \n## [287] \"\"                                                                                                            \n## [288] \"You died in The Dungeons of Doom on dungeon level 6 with 6652 points,\"                                       \n## [289] \"and 59 pieces of gold, after 7398 moves.\"                                                                    \n## [290] \"You were level 8 with a maximum of 54 hit points when you died.\"                                             \n## [291] \"\"\n\n\nNow, I am curious to see how many games are played per day:\n\nruns_per_day &lt;- nethack %&gt;%\n  group_by(date) %&gt;%\n  count() %&gt;%\n  ungroup() \n\n\nggplot(runs_per_day, aes(y = n, x = date)) + \n  geom_point(colour = \"#0f4150\") + \n  geom_smooth(colour = \"#82518c\") + \n  theme_blog()\n## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\nThe number of games seems to be stable since 2015, around 50. But what is also interesting is not only the number of games played, but also how many of these games resulted in a win.\n\n\nFor this, let’s also add a new column that tells us whether the played ascended (won the game) or not:\n\nnethack %&lt;&gt;%\n  mutate(Ascended = ifelse(death == \"ascended\", \"Ascended\", \"Died an horrible death\"))\n\nI’m curious to see how many players managed to ascend… NetHack being as hard as diamonds, probably not a lot:\n\nascensions_per_day &lt;- nethack %&gt;%\n  group_by(date, Ascended) %&gt;%\n  count() %&gt;%\n  rename(Total = n)\n\nggplot(ascensions_per_day) + \n  geom_area(aes(y = Total, x = as.Date(date), fill = Ascended)) +\n  theme_blog() +\n  labs(y = \"Number of runs\", x = \"Date\") +\n  scale_fill_blog() +\n  theme(legend.title = element_blank())\n\n\n\n\nYeah, just as expected. Because there is so much data, it’s difficult to see clearly, though. Depending on the size of the screen you’re reading this, it might seem that in some days there are a lot of ascensions. This is only an impression due to the resolution of the picture. Let’s see the share of ascensions per year (and how many times the quests fail miserably), and this will become more apparent:\n\nascensions_per_day %&gt;%\n  mutate(Year = year(as.Date(date))) %&gt;%\n  group_by(Year, Ascended) %&gt;%\n  summarise(Total = sum(Total, na.rm = TRUE)) %&gt;%\n  group_by(Year) %&gt;%\n  mutate(denom = sum(Total, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Share = Total/denom) %&gt;%\n  ggplot() + \n  geom_col(aes(y = Share, x = Year, fill = Ascended)) + \n  theme_blog() + \n  scale_fill_blog() + \n  theme(legend.title = element_blank())\n\n\n\n\nI will now convert the “time” column to seconds. I am not yet sure that this column is really useful, because NetHack is a turn based game. This means that when the player does not move, neither do the monsters. So the seconds spent playing might not be a good proxy for actual time spent playing. But it makes for a good exercise:\n\nconvert_to_seconds &lt;- function(time_string){\n    time_numeric &lt;- time_string %&gt;%\n        str_split(\":\", simplify = TRUE) %&gt;%\n        as.numeric\n\n    time_in_seconds &lt;- sum(time_numeric * c(3600, 60, 1))\n\n    time_in_seconds \n}\n\nThe strings I want to convert are of the form “01:34:43”, so I split at the “:” and then convert the result to numeric. I end up with an atomic vector (c(1, 34, 43)). Then I multiple each element by the right number of seconds, and sum that to get the total. Let’s apply it to my data:\n\nnethack %&lt;&gt;%\n  mutate(time_in_seconds = map_dbl(time, convert_to_seconds))\n\nWhat is the distribution of “time_in_seconds”?\n\nnethack %&gt;%\n  describe(time_in_seconds)\n## # A tibble: 1 x 15\n##   variable type    nobs   mean     sd mode    min    max   q05   q25 median\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n## 1 time_in… Nume… 322485 23529. 2.73e5 &lt;NA&gt;     61 2.72e7   141   622   1689\n## # … with 4 more variables: q75 &lt;dbl&gt;, q95 &lt;dbl&gt;, n_missing &lt;int&gt;,\n## #   n_unique &lt;lgl&gt;\n\nWe see that the minimum of time_in_seconds is 61 whereas the maximum is of the order of 27200000… This must be a mistake, because that is almost one year!\n\nnethack %&gt;%\n  filter(time_in_seconds == max(time_in_seconds, na.rm = TRUE))\n##   rank      score   name       time   turns lev_max  hp_max role race\n## 1   28 3173960108 fisted 7553:41:49 6860357    4/47 362/362  Wiz  Elf\n##   gender alignment                      death       date dumplog year\n## 1    Mal       Neu drowned in a pool of water 2017-02-02      NA 2017\n##   month day               Ascended time_in_seconds\n## 1     2   2 Died an horrible death        27193309\n\nWell… maybe “fisted” wanted to break the record of the longest NetHack game ever. Congratulations!\n\n\nLet’s take a look at the density but cut it at 90th percentile:\n\nnethack %&gt;%\n  filter(!is.na(time_in_seconds),\n         time_in_seconds &lt; quantile(time_in_seconds, 0.9, na.rm = TRUE)) %&gt;%\n  ggplot() + \n  geom_density(aes(x = time_in_seconds), colour = \"#82518c\") + \n  theme_blog()\n\n\n\n\nAs expected, the distribution is right skewed. However, as explained above NetHack is a turn based game, meaning that if the player does not move, the monsters won’t move either. Perhaps it makes more sense to look at the turns column:\n\nnethack %&gt;%\n  describe(turns)\n## # A tibble: 1 x 15\n##   variable type    nobs  mean     sd mode    min    max   q05   q25 median\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n## 1 turns    Nume… 322485 4495. 19853. &lt;NA&gt;      1 6.86e6   202   871   1818\n## # … with 4 more variables: q75 &lt;dbl&gt;, q95 &lt;dbl&gt;, n_missing &lt;int&gt;,\n## #   n_unique &lt;lgl&gt;\n\nThe maximum is quite large too. Just like before, let’s focus by cutting the variable at the 90th percentile:\n\nnethack %&gt;%\n  filter(!is.na(turns),\n         turns &lt; quantile(turns, 0.9, na.rm = TRUE)) %&gt;% \n  ggplot() + \n  geom_density(aes(x = turns), colour = \"#82518c\") + \n  theme_blog()\n\n\n\n\nI think that using turns makes more sense. In the a future blog post, I will estimate a survival model and see how long players survive, and will use turns instead of time_in_seconds."
  },
  {
    "objectID": "posts/2018-11-03-nethack_analysis.html#analysis",
    "href": "posts/2018-11-03-nethack_analysis.html#analysis",
    "title": "Analyzing NetHack data, part 1: What kills the players",
    "section": "\nAnalysis\n",
    "text": "Analysis\n\n\n\nWhat kills the players\n\n\nTo know what kills players so much, some cleaning of the death column is in order. Death can occur from poisoning, starvation, accidents, drowning… of course monsters can kill the player too. Here are some values of the death variable:\n\nburned by a tower of flame\nchoked on a lichen corpse\ndied of starvation\nfell into a pit of iron spikes\nkilled by a gnome\nkilled by a gnome called Blabla\nkilled by a gnome called Blabla while sleeping\nslipped while mounting a saddled pony\nslipped while mounting a saddled pony called Jolly Jumper\nzapped her/himself with a spell\n\nTo know what is the most frequent cause of death, I have to do some cleaning, because if not, “killed by a gnome” and “killed by a gnome called Blabla” would be two different causes of death. In the end, what interests me is to know how many times the player got killed by a gnome.\n\n\nThe following lines do a cleanup of the death variable:\n\nnethack %&lt;&gt;% \n  mutate(death2 = case_when(str_detect(death, \"poisoned\") ~ \"poisoned\",\n                            str_detect(death, \"slipped\") ~ \"accident\",\n                            str_detect(death, \"petrified\") ~ \"petrified\",\n                            str_detect(death, \"choked\") ~ \"accident\",\n                            str_detect(death, \"caught.*self\") ~ \"accident\",\n                            str_detect(death, \"starvation\") ~ \"starvation\",\n                            str_detect(death, \"drowned\") ~ \"drowned\",\n                            str_detect(death, \"fell\") ~ \"fell\",\n                            str_detect(death, \"zapped\") ~ \"zapped\",\n                            str_detect(death, \"killed\") ~ \"killed\",\n                            TRUE ~ death)) %&gt;%\n  mutate(death3 = str_extract(death, \"(?&lt;=by|while).*\")) %&gt;%\n  mutate(death3 = case_when(str_detect(death3, \",|\\\\bcalled\\\\b\") ~ str_extract(death3, \"(.*?),|(.*?)\\\\bcalled\\\\b\"), \n                            TRUE ~ death3)) %&gt;%\n  mutate(death3 = str_remove(death3, \",|called|\\\\ban?\"),\n         death3 = str_trim(death3))\n\ndeath2 is a new variable, in which I broadly categorize causes of death. Using regular expressions I detect causes of death and aggregate some categories, for instance “slipped” and “chocked” into “accident”. Then, I want to extract everything that comes after the strings “by” or while, and put the result into a new variable called death3. Then I detect the string “,” or “called”; if one of these strings is present, I extract everything that comes before “,” or that comes before “called”. Finally, I remove “,”, “called” or “a” or “an” from the string and trim the whitespaces.\n\n\nLet’s take a look at these new variables:\n\nset.seed(123)\nnethack %&gt;%\n    select(name, death, death2, death3) %&gt;%\n    sample_n(10)\n##              name                     death   death2        death3\n## 92740   DianaFury     killed by a death ray   killed     death ray\n## 254216    Oddabit         killed by a tiger   killed         tiger\n## 131889    shachaf      killed by a fire ant   killed      fire ant\n## 284758        a43  poisoned by a killer bee poisoned    killer bee\n## 303283      goast         killed by a gecko   killed         gecko\n## 14692     liberty    killed by a gnome king   killed    gnome king\n## 170303     arch18                  ascended ascended          &lt;NA&gt;\n## 287786 foolishwtf           killed by a bat   killed           bat\n## 177826    Renleve     killed by a giant bat   killed     giant bat\n## 147248      TheOV killed by a black unicorn   killed black unicorn\n\nNow, it is quite easy to know what monsters are the meanest buttholes; let’s focus on the top 15. Most likely, these are going to be early game monsters. Let’ see:\n\nnethack %&gt;%\n    filter(!is.na(death3)) %&gt;%\n    count(death3) %&gt;%\n    top_n(15) %&gt;%\n    mutate(death3 = fct_reorder(death3, n, .desc = FALSE)) %&gt;%\n    ggplot() + \n    geom_col(aes(y = n, x = death3)) + \n    coord_flip() + \n    theme_blog() + \n    scale_fill_blog() + \n    ylab(\"Number of deaths caused\") +\n    xlab(\"Monster\")\n## Selecting by n\n\n\n\n\nSeems like soldier ants are the baddest, followed by jackals and dwarfs. As expected, these are mostly early game monsters. Thus, it would be interesting to look at this distribution, but at different stages in the game. Let’s create a categorical variable that discretizes turns, and then create one plot per category:\n\n\n\n\nClick to expand\n\n\nnethack %&gt;%\n    filter(!is.na(death3)) %&gt;%\n    filter(!is.na(turns)) %&gt;%\n    mutate(turn_flag = case_when(between(turns, 1, 5000) ~ \"Less than 5000\",\n                                 between(turns, 5001, 10000) ~ \"Between 5001 and 10000\",\n                                 between(turns, 10001, 20000) ~ \"Between 10001 and 20000\",\n                                 between(turns, 20001, 40000) ~ \"Between 20001 and 40000\",\n                                 between(turns, 40001, 60000) ~ \"Between 40001 and 60000\",\n                                 turns &gt; 60000 ~ \"More than 60000\")) %&gt;%\n    mutate(turn_flag = factor(turn_flag, levels = c(\"Less than 5000\", \n                                                    \"Between 5001 and 10000\",\n                                                    \"Between 10001 and 20000\",\n                                                    \"Between 20001 and 40000\",\n                                                    \"Between 40001 and 60000\",\n                                                    \"More than 60000\"), ordered = TRUE)) %&gt;%\n    group_by(turn_flag) %&gt;%\n    count(death3) %&gt;%\n    top_n(15) %&gt;%\n    nest() %&gt;%\n    mutate(data = map(data, ~mutate(., death3 = fct_reorder(death3, n, .desc = TRUE))))  %&gt;%\n    mutate(plots = map2(.x = turn_flag,\n                         .y = data,\n                         ~ggplot(data = .y) + \n                             geom_col(aes(y = n, x = death3)) + \n                             coord_flip() + \n                             theme_blog() + \n                             scale_fill_blog() + \n                             ylab(\"Number of deaths caused\") +\n                             xlab(\"Monster\") + \n                             ggtitle(.x))) %&gt;%\n    pull(plots)\n## Selecting by n\n## [[1]]\n\n\n\n## \n## [[2]]\n\n\n\n## \n## [[3]]\n\n\n\n## \n## [[4]]\n\n\n\n## \n## [[5]]\n\n\n\n## \n## [[6]]\n\n\n\nFinally, for this section, I want to know if there are levels, or floors, where players die more often than others. For this, we can take a look at the lev_max column. Observations in this column are of the form “8/10”. This means that the player died on level 8, but the lowest level that was explored was the 10th. Let’s do this for the year 2017 first. Before anything, I have to explain the layout of the levels of the game. You can see a diagram here. The player starts on floor 1, and goes down to level 53. Then, the player can ascend, by going on levels -1 to -5. But there are more levels than these ones. -6 and -9 are the sky, and the player can teleport there (but will fall to his death). If the player teleports to level -10, he’ll enter heaven (and die too). Because these levels are special, I do not consider them here. I do not consider level 0 either, which is “Nowhere”. Let’s get the number of players who died on each floor, but also compute the cumulative death count:\n\ndied_on_level &lt;- nethack %&gt;%\n    filter(Ascended == \"Died an horrible death\") %&gt;%\n    mutate(died_on = str_extract(lev_max, \"-?\\\\d{1,}\")) %&gt;%\n    mutate(died_on = as.numeric(died_on)) %&gt;%\n    group_by(year) %&gt;%\n    count(died_on) %&gt;% \n    filter(died_on &gt;= -5, died_on != 0) %&gt;%\n    mutate(died_on = case_when(died_on == -1 ~ 54,\n                               died_on == -2 ~ 55,\n                               died_on == -3 ~ 56,\n                               died_on == -4 ~ 57,\n                               died_on == -5 ~ 58,\n                               TRUE ~ died_on)) %&gt;%\n    arrange(desc(died_on)) %&gt;%\n    mutate(cumul_deaths = cumsum(n))\n\nLet’s take a look:\n\nhead(died_on_level)\n## # A tibble: 6 x 4\n## # Groups:   year [6]\n##    year died_on     n cumul_deaths\n##   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;        &lt;int&gt;\n## 1  2002      58     5            5\n## 2  2003      58    11           11\n## 3  2004      58    19           19\n## 4  2005      58    28           28\n## 5  2006      58    25           25\n## 6  2007      58    22           22\n\nNow, let’s compute the number of players who ascended and add this to the cumulative count:\n\nascended_yearly &lt;- nethack %&gt;%\n    filter(Ascended == \"Ascended\") %&gt;%\n    group_by(year) %&gt;%\n    count(Ascended)\n\nLet’s take a look:\n\nhead(ascended_yearly)\n## # A tibble: 6 x 3\n## # Groups:   year [6]\n##    year Ascended     n\n##   &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;\n## 1  2001 Ascended     4\n## 2  2002 Ascended    38\n## 3  2003 Ascended   132\n## 4  2004 Ascended   343\n## 5  2005 Ascended   329\n## 6  2006 Ascended   459\n\nI will modify the dataset a little bit and merge it with the previous one:\n\nascended_yearly %&lt;&gt;%\n  rename(ascended_players = `n`) %&gt;%\n  select(-Ascended)\n\nLet’s add this to the data frame from before by merging both, and then we can compute the surviving players:\n\ndied_on_level %&lt;&gt;%\n  full_join(ascended_yearly, by = \"year\") %&gt;%\n  mutate(surviving_players = cumul_deaths + ascended_players)\n\nNow we can compute the share of players who died on each level:\n\ndied_on_level %&gt;%\n    mutate(death_rate = n/surviving_players) %&gt;% \n    ggplot(aes(y = death_rate, x = as.factor(died_on))) + \n    geom_line(aes(group = year, alpha = year), colour = \"#82518c\") +\n    theme_blog() + \n    ylab(\"Death rate\") +\n    xlab(\"Level\") + \n    theme(axis.text.x = element_text(angle = 90),\n          legend.position = \"none\") + \n    scale_y_continuous(labels = scales::percent)\n\n\n\n\nLooks like level 7 is consistently the most dangerous! The death rate there is more than 35%!\n\n\nThat’s it for this blog post, in the next one, I will focus on what players kill!"
  },
  {
    "objectID": "posts/2016-12-24-functional-programming-and-unit-testing-for-data-munging-with-r-available-on-leanpub.html",
    "href": "posts/2016-12-24-functional-programming-and-unit-testing-for-data-munging-with-r-available-on-leanpub.html",
    "title": "Functional programming and unit testing for data munging with R available on Leanpub",
    "section": "",
    "text": "The book I’ve been working on these pasts months (you can read about it here, and read it for free here) is now available on Leanpub! You can grab a copy and read it on your ebook reader or on your computer, and what’s even better is that it is available for free (but you can also decide to buy it if you really like it). Here is the link on Leanpub.\nIn the book, I show you the basics of functional programming, unit testing and package development for the R programming language. The end goal is to make your data tidy in a reproducible way!\nJust a heads up: as the book is right now, the formatting is not perfect and images are missing. This is because I use bookdown to write the book and convert it to Leanpub’s markdown flavour is not trivial. I will find a solution to automate the conversion from bookdown’s version to Leanpub’s markdown and try to keep both versions in sync. Of course, once the book will be finished, the version on Leanpub and on my website are going to be completely identical. If you want to read it on your computer offline, you can also download a pdf from the book’s website, by clicking on the pdf icon in the top left corner. Do not hesitate to send me an email if you want to give me feedback (just click on the red envelope in the top right corner) or tweet me @brodriguesco."
  },
  {
    "objectID": "posts/2019-09-03-disk_frame.html",
    "href": "posts/2019-09-03-disk_frame.html",
    "title": "{disk.frame} is epic",
    "section": "",
    "text": "Note: When I started writing this blog post, I encountered a bug and filed a bug report that I encourage you to read. The responsiveness of the developer was exemplary. Not only did Zhuo solve the issue in record time, he provided ample code snippets to illustrate the solutions. Hats off to him!\n\n\nThis blog post is a short presentation of {disk.frame} a package that makes it easy to work with data that is too large to fit on RAM, but not large enough that it could be called big data. Think data that is around 30GB for instance, or more, but nothing at the level of TBs.\n\n\nI have already written a blog post about this topic, using Spark and the R library {sparklyr}, where I showed how to set up {sparklyr} to import 30GB of data. I will import the same file here, and run a very simple descriptive analysis. If you need context about the file I’ll be using, just read the previous blog post.\n\n\nThe first step, as usual, is to load the needed packages:\n\nlibrary(tidyverse)\nlibrary(disk.frame)\n\nThe next step is to specify how many cores you want to dedicate to {disk.frame}; of course, the more cores you use, the faster the operations:\n\nsetup_disk.frame(workers = 6)\noptions(future.globals.maxSize = Inf)\n\nsetup_disk.frame(workers = 6) means that 6 cpu threads will be dedicated to importing and working on the data. The second line, future.globals.maxSize = Inf means that an unlimited amount of data will be passed from worker to worker, as described in the documentation.\n\n\nNow comes the interesting bit. If you followed the previous blog post, you should have a 30GB csv file. This file was obtained by merging a lot of smaller sized csv files. In practice, you should keep the files separated, and NOT merge them. This makes things much easier. However, as I said before, I want to be in the situation, which already happened to me in the past, where I get a big-sized csv file and I am to provide an analysis on that data. So, let’s try to read in that big file, which I called combined.csv:\n\npath_to_data &lt;- \"path/to/data/\"\n\nflights.df &lt;- csv_to_disk.frame(\n  paste0(path_to_data, \"combined.csv\"), \n  outdir = paste0(path_to_data, \"combined.df\"),\n  in_chunk_size = 2e6,\n  backend = \"LaF\")\n\nLet’s go through these lines, one at a time. In the first line, I simply define the path to the folder that contains the data. The next chunk is where I read in the data using the csv_to_disk_frame() function. The first option is simply the path to the csv file. The second option outdir = is where you need to define the directory that will hold the intermediary files, which are in the fst format. This folder, that contains these fst files, is the disk.frame. fst files are created by the {fst} package, which provides a fast, easy and flexible way to serialize data frames. This means that files that are in that format can be read and written much much faster than by other means (see a benchmark of {fst} here). The next time you want to import the data, you can use the disk.frame() function and point it to the combined.df folder. in_chunk_size = specifies how many lines are to be read in one swoop, and backend = is the underlying engine that reads in the data, in this case the {LaF} package. The default backend is {data.table} and there is also a {readr} backend. As written in the note at the beginning of the blog post, I encourage you to read the github issue to learn why I am using the LaF backend (the {data.table} and {readr} backend work as well).\n\n\nNow, let’s try to replicate what I did in my previous blog post, namely, computing the average delay in departures per day. With {disk.frame}, one has to be very careful about something important however; all the group_by() operations are performed per chunk. This means that a second group_by() call might be needed. For more details, I encourage you to read the documentation.\n\n\nThe code below is what I want to perform; group by day, and compute the average daily flight delay:\n\nmean_dep_delay &lt;- flights.df %&gt;%\n  group_by(YEAR, MONTH, DAY_OF_MONTH) %&gt;%\n  summarise(mean_delay = mean(DEP_DELAY, na.rm = TRUE))\n\nHowever, because with {disk.frame}, group_by() calls are performed per chunk, the code must now be changed. The first step is to compute the sum of delays within each chunk, and count the number of days within each chunk. This is the time consuming part:\n\ntic &lt;- Sys.time()\nmean_dep_delay &lt;- flights.df %&gt;%\n  group_by(YEAR, MONTH, DAY_OF_MONTH) %&gt;%\n  summarise(sum_delay = sum(DEP_DELAY, na.rm = TRUE), n = n()) %&gt;%\n  collect()\n(toc = Sys.time() - tic)\nTime difference of 1.805515 mins\n\nThis is pretty impressive! It is much faster than with {sparklyr}. But we’re not done yet, we still need to compute the average:\n\nmean_dep_delay &lt;- mean_dep_delay %&gt;%\n  group_by(YEAR, MONTH, DAY_OF_MONTH) %&gt;%\n  summarise(mean_delay = sum(sum_delay)/sum(n))\n\nIt is important to keep in mind that group_by() works by chunks when dealing with disk.frame objects.\n\n\nTo conclude, we can plot the data:\n\nlibrary(lubridate)\ndep_delay &lt;- mean_dep_delay %&gt;%\n  arrange(YEAR, MONTH, DAY_OF_MONTH) %&gt;%\n  mutate(date = ymd(paste(YEAR, MONTH, DAY_OF_MONTH, sep = \"-\")))\n\nggplot(dep_delay, aes(date, mean_delay)) +\n  geom_smooth(colour = \"#82518c\") + \n  brotools::theme_blog()\n## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n{disk.frame} is really promising, and I will monitor this package very closely. I might write another blog post about it, focusing this time on using machine learning with disk.frame objects."
  },
  {
    "objectID": "posts/2023-06-29-book_quarto.html",
    "href": "posts/2023-06-29-book_quarto.html",
    "title": "How to self-publish a technical book on Leanpub and Amazon using Quarto",
    "section": "",
    "text": "UPDATE: I’ve update this blog post on the 30 of June 2023. I corrected a statement where I said that the _quarto.yml file is where you can choose the version of R to compile the book. That’s wrong, choosing the version of R to compile the book is done on the Github Actions workflow. I’ve also added some answers to questions I got on social media.\nSo I’ve recently self-published a book on both Leanpub as an Ebook and Amazon as a physical book and thought that it would be worth it to write down how to do it. I’ve wasted some time getting everything to work flawlessly so if you’re looking for a guide on how to create both an ebook and a print-ready PDF for Amazon’s Kindle Direct Publishing service using Quarto, you’ve come to the right place.\nIf you don’t want to waste time reading, just fork this repo and use that as a starting point. Each time you push a change to the repo, a website, Epub and PDF get generated using Github Actions. If you want to understand the details, read on."
  },
  {
    "objectID": "posts/2023-06-29-book_quarto.html#your-books-dependencies",
    "href": "posts/2023-06-29-book_quarto.html#your-books-dependencies",
    "title": "How to self-publish a technical book on Leanpub and Amazon using Quarto",
    "section": "\nYour book’s dependencies\n",
    "text": "Your book’s dependencies\n\n\nYou should start by creating an renv.lock file. This file will list all the dependencies of your book. For example, if you’re using {ggplot2} to make graphs or {flextable} for tables, the renv.lock file will list them and then this file will be used to download the required packages by Github Actions (more on Github Actions later) to create the book. The template comes with one such renv.lock file, but you should generate one specific to your project. Simply install {renv} and run:\n\nrenv::init()\n\nAnswer “Y” to the question and wait a little. The renv.lock file should appear alongside the source of your book now. If you need to install more packages to keep writing your book, install them as usual (using install.packages(“package”)) but then don’t forget to create a new renv.lock file using renv::snapshot()."
  },
  {
    "objectID": "posts/2023-06-29-book_quarto.html#quarto.yml",
    "href": "posts/2023-06-29-book_quarto.html#quarto.yml",
    "title": "How to self-publish a technical book on Leanpub and Amazon using Quarto",
    "section": "\n_quarto.yml\n",
    "text": "_quarto.yml\n\n\nWhatever output format, everything gets defined in the _quarto.yml file in the root directory of your book. This file is where you can choose which themes to use for your website for example, which output formats you want to compile your book into, etc. I’ll discuss each option for each format below."
  },
  {
    "objectID": "posts/2023-06-29-book_quarto.html#generating-the-website",
    "href": "posts/2023-06-29-book_quarto.html#generating-the-website",
    "title": "How to self-publish a technical book on Leanpub and Amazon using Quarto",
    "section": "\nGenerating the website\n",
    "text": "Generating the website\n\n\nI’m using Github Actions to generate each format of the book. Github Actions is essentially a computer hosted by Github that you can use to run arbitrary commands. These commands must be written in a specific file which must be put in a specific place in your repository. Here is that file for the repo I linked above. I won’t go into details because I’ve explained how Github Actions works here already, but you’ll notice that you can choose a version of R to compile your document and also a different version of Quarto. This can be useful for reproducibility.\n\n\nCreate a new branch called gh-pages and then go to settings, then on the side-bar on the left choose “Actions”, and scroll down. Then, in “Workflow persmissions”, check “Read and Write permissions” and “Allow Github Actions to create and approve pull requests”. Then go to “Pages” which you can find on the side-bar on the left, and choose “Deploy from a branch” under “Build and deployment” and choose “gh-pages” and “root”:\n\n\n\n\n\n\n\nEach time you push, the website to your book will get updated. Here are the options I chose for my website, which you can find in the _quarto.yml file:\n\nhtml:\n   theme:\n     light: flatly\n     dark: solar\n   css:\n     epub.css\n\nSo my website is available in two themes, a dark and light one. I highly recommend you also provide two themes. You can also provide a .css file to customize the appearance of you website, and of your ebook (that’s because an Epub is actually a bunch of html files). The .css I’m using is quite simple, the only thing it’s doing is making sure that images won’t be wider than the web-page. You can view the website of this template book here."
  },
  {
    "objectID": "posts/2023-06-29-book_quarto.html#generating-an-ebook",
    "href": "posts/2023-06-29-book_quarto.html#generating-an-ebook",
    "title": "How to self-publish a technical book on Leanpub and Amazon using Quarto",
    "section": "\nGenerating an Ebook\n",
    "text": "Generating an Ebook\n\n\nLet’s continue with the .epub format. This is an Ebook format that can be read on E-readers such as the Kindle from Amazon. If you want to sell an Ebook on Leanpub (or send it to your Kindle), it needs to pass a tool called epubcheck.\n\n\nI’ve already written about generating ebooks in the past (here). However, the advice in that blog post was only valid because there were bugs in the version of Quarto that was current at the time and so I showed some workarounds. With the current version, no workarounds are needed anymore, Epubs generated by Quarto now pass epubcheck. Check the source, specifically index.qmd to see how I include graphics."
  },
  {
    "objectID": "posts/2023-06-29-book_quarto.html#generating-a-print-ready-pdf",
    "href": "posts/2023-06-29-book_quarto.html#generating-a-print-ready-pdf",
    "title": "How to self-publish a technical book on Leanpub and Amazon using Quarto",
    "section": "\nGenerating a print-ready PDF\n",
    "text": "Generating a print-ready PDF\n\n\nThis was the hardest part: I’m using Amazon’s KDP service to sell physical copies of the book and the PDF needs to be in a specific format. I’m using the 6 x 9 format without bleed, which seems to be the most common. If you look again at the _quarto.yml file, you should see this:\n\n  pdf:\n    keep-tex: true\n    documentclass: scrbook\n    classoption: [paper=6in:9in,pagesize=pdftex,headinclude=on,footinclude=on,12pt]\n    include-in-header:\n      text: |\n        \\usepackage{fvextra}\n        \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n        \\areaset[0.50in]{4.5in}{8in}\n    include-before-body:\n      text: |\n        \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n           showspaces = false,\n           showtabs = false,\n           breaksymbolleft={},\n           breaklines\n           % Note: setting commandchars=\\\\\\{\\} here will cause an error \n        }\n    fig-pos: 'H'\n\nWhat’s important is ‘classoption’:\n\nclassoption: [paper=6in:9in,pagesize=pdftex,headinclude=on,footinclude=on,12pt]\n\nThis is where I can choose the dimensions of the book (6 x 9) and the size of the font (12pt). Then:\n\ninclude-in-header:\n  text: |\n    \\usepackage{fvextra}\n    \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n    \\areaset[0.50in]{4.5in}{8in}\n\nWith fvextra and the call to  I make sure that long lines of code get wrapped in the PDF. Without this, long lines of code would simply continue outside the margins of the PDF, and Amazon’s KDP doesn’t accept a PDF like this.\n\n\n: this is the area that well be used for writing. These are the correct dimensions for a 6 by 9 book without bleed. Then, I keep customizing the verbatim environment:\n\ninclude-before-body:\n  text: |\n    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n       showspaces = false,\n       showtabs = false,\n       breaksymbolleft={},\n       breaklines\n       % Note: setting commandchars=\\\\\\{\\} here will cause an error \n    }\n\nFinally, the last option:\n\nfig-pos: 'H'\n\nThis ensures that the figures are placed EXACTLY where you say they should be in the final PDF. For those of you that use LaTeX, you know that LaTeX sometimes takes some liberties with figure placement. I’ve been told the lie that LaTeX knows where to place the figures perfectly well many times but I don’t buy it. So use fig-pos: ‘H’ to avoid lots of frustration.\n\n\nThat’s it! You should now be able to generate a book that is print-ready, and an Epub that passes epubcheck as well as website. You can now just focus on writing. Also check the source of index.qmd for to see how to include text in the PDF only (or not show text in the PDF)."
  },
  {
    "objectID": "posts/2023-06-29-book_quarto.html#my-personal-experience-and-some-faq",
    "href": "posts/2023-06-29-book_quarto.html#my-personal-experience-and-some-faq",
    "title": "How to self-publish a technical book on Leanpub and Amazon using Quarto",
    "section": "\nMy personal experience and some FAQ\n",
    "text": "My personal experience and some FAQ\n\n\nOverall, I enjoyed using both Leanpub and Amazon to publish my book. Leanpub is really nice, because they really offer a very nice deal: you get 80% of the sales price as royalties, which is the highest share I’ve seen. Also the people working there are really nice, I’ve had the chance to discuss with Len Epp, Leanpub’s cofounder, in his Frontmatter podcast and would definitely continue using their platform in the future. Regarding Amazon I must say that the experience was quite good as well; the only friction I had was getting the PDF in the right format for printing, but that’s hardly something that Amazon can be blamed for. After all if the PDF’s formatting is bad, the books will look like crap as well. One thing you should know though is that you need to get a VAT number to sell books on Amazon. I live in Luxembourg and getting one is trivial, but in other countries this may be more complex. You should know as well that Leanpub can sell the physical copies of your book, through Amazon, for you. They essentially act as a publisher then. This way, you don’t need to get a VAT number, if that’s difficult for you. But you’ll need to share the Amazon royalties with Leanpub.\n\n\nWhen publishing a book through Amazon’s KDP service, you also get access to a basic book cover editor that you can use to create a cover for your book. You can provide an image and then use the editor to create the cover, but you can also provide a ready-made cover if you have the talent to make one using an image editing software. Once you’re done, and click “Publish” on Amazon, the book will get reviewed by a human. This process can take up to three days, but in my case it took only 4 to 6 hours. However, my book was rejected, twice. One time was because one of the images I used in the book had colour bars in the bottom right corner, that I needed to remove, and the other time was because the “g” in my name, “Rodrigues” was cut by the cover editor and it was hard to tell if it was a “g” or a “q”. Once I corrected both issues, the book was available for order on Amazon.com within the day. The other marketplaces, like France and Germany took one day more, and the UK marketplace took 4 days.\n\n\n\nReferences\n\n\nI’m sorry, I have no idea where I found all of this stuff. I looked for it for some time, and if memory serves most of this came from stackoverflow."
  },
  {
    "objectID": "posts/2020-04-27-nace_explorer.html",
    "href": "posts/2020-04-27-nace_explorer.html",
    "title": "Exploring NACE codes",
    "section": "",
    "text": "A quick one today. If you work with economic data, you’ll be confronted to NACE code sooner or later. NACE stands for Nomenclature statistique des Activités économiques dans la Communauté Européenne. It’s a standard classification of economic activities. It has 4 levels, and you can learn more about it here.\n\n\nEach level adds more details; consider this example:\n\nC - Manufacturing\nC10 - Manufacture of food products\nC10.1 - Processing and preserving of meat and production of meat products\nC10.1.1 - Processing and preserving of meat\nC10.1.2 - Processing and preserving of poultry meat\nC10.1.3 - Production of meat and poultry meat products\n\nSo a company producing meat and poultry meat products would have NACE code level 4 C10.1.3 with it. Today for work I had to create a nice visualisation of the hierarchy of the NACE classification. It took me a bit of time to find a nice solution, so that’s why I’m posting it here. Who knows, it might be useful for other people. First let’s get the data. Because finding it is not necessarily very easy if you’re not used to navigating Eurostat’s website, I’ve put the CSV into a gist:\n\nlibrary(tidyverse)\nlibrary(data.tree)\nlibrary(igraph)\nlibrary(GGally)\nnace_code &lt;- read_csv(\"https://gist.githubusercontent.com/b-rodrigues/4218d6daa8275acce80ebef6377953fe/raw/99bb5bc547670f38569c2990d2acada65bb744b3/nace_rev2.csv\")\n## Parsed with column specification:\n## cols(\n##   Order = col_double(),\n##   Level = col_double(),\n##   Code = col_character(),\n##   Parent = col_character(),\n##   Description = col_character(),\n##   `This item includes` = col_character(),\n##   `This item also includes` = col_character(),\n##   Rulings = col_character(),\n##   `This item excludes` = col_character(),\n##   `Reference to ISIC Rev. 4` = col_character()\n## )\nhead(nace_code)\n## # A tibble: 6 x 10\n##    Order Level Code  Parent Description `This item incl… `This item also…\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;            &lt;chr&gt;           \n## 1 398481     1 A     &lt;NA&gt;   AGRICULTUR… \"This section i… &lt;NA&gt;            \n## 2 398482     2 01    A      Crop and a… \"This division … This division a…\n## 3 398483     3 01.1  01     Growing of… \"This group inc… &lt;NA&gt;            \n## 4 398484     4 01.11 01.1   Growing of… \"This class inc… &lt;NA&gt;            \n## 5 398485     4 01.12 01.1   Growing of… \"This class inc… &lt;NA&gt;            \n## 6 398486     4 01.13 01.1   Growing of… \"This class inc… &lt;NA&gt;            \n## # … with 3 more variables: Rulings &lt;chr&gt;, `This item excludes` &lt;chr&gt;,\n## #   `Reference to ISIC Rev. 4` &lt;chr&gt;\n\nSo there’s a bunch of columns we don’t need, so we’re going to ignore them. What I’ll be doing is transforming this data frame into a data tree, using the {data.tree} package. For this, I need columns that provide the hierarchy. I’m doing this with the next chunk of code. I won’t explain each step, but the idea is quite simple. I’m using the Level column to create new columns called Level1, Level2, etc. I’m then doing some cleaning:\n\nnace_code &lt;- nace_code %&gt;%\n  select(Level, Code)\n\nnace_code &lt;- nace_code %&gt;%\n  mutate(Level1 = ifelse(Level == 1, Code, NA)) %&gt;%\n  fill(Level1, .direction = \"down\") %&gt;%  \n  mutate(Level2 = ifelse(Level == 2, Code, NA)) %&gt;%\n  fill(Level2, .direction = \"down\") %&gt;%  \n  mutate(Level3 = ifelse(Level == 3, Code, NA)) %&gt;%\n  fill(Level3, .direction = \"down\") %&gt;%  \n  mutate(Level4 = ifelse(Level == 4, Code, NA)) %&gt;%  \n  filter(!is.na(Level4))\n\nLet’s take a look at how the data looks now:\n\nhead(nace_code)\n## # A tibble: 6 x 6\n##   Level Code  Level1 Level2 Level3 Level4\n##   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; \n## 1     4 01.11 A      01     01.1   01.11 \n## 2     4 01.12 A      01     01.1   01.12 \n## 3     4 01.13 A      01     01.1   01.13 \n## 4     4 01.14 A      01     01.1   01.14 \n## 5     4 01.15 A      01     01.1   01.15 \n## 6     4 01.16 A      01     01.1   01.16\n\nI can now create the hierarchy using by creating a column called pathString and passing that data frame to data.tree::as.Node(). Because some sections, like C (manufacturing) are very large, I do this separately for each section by using the group_by()-nest() trick. This way, I can create a data.tree object for each section. Finally, to create the plots, I use igraph::as.igraph() and pass this to GGally::ggnet2(), which takes care of creating the plots. This took me quite some time to figure out, but the result is a nice looking PDF that the colleagues can now use:\n\nnace_code2 &lt;- nace_code %&gt;%\n  group_by(Level1, Level2) %&gt;%\n  nest() %&gt;%\n  mutate(nace = map(data, ~mutate(., pathString = paste(\"NACE2\",\n                                       Level1,\n                                       Level2,\n                                       Level3,\n                                       Level4,\n                                       sep = \"/\")))) %&gt;%\n  mutate(plots = map(nace, ~as.igraph(as.Node(.)))) %&gt;%\n  mutate(plots = map(plots, ggnet2, label = TRUE))\n\n\npdf(\"nace_maps.pdf\")\npull(nace_code2, plots)\ndev.off()\n\nHere’s how the pdf looks like:\n\n\n\n\n\nIf you want to read more about {data.tree}, you can do so here and you can also read more about the ggnet2() here."
  },
  {
    "objectID": "posts/2017-03-29-make-ggplot2-purrr.html",
    "href": "posts/2017-03-29-make-ggplot2-purrr.html",
    "title": "Make ggplot2 purrr",
    "section": "",
    "text": "Update: I’ve included another way of saving a separate plot by group in this article, as pointed out by @monitus. Actually, this is the preferred solution; using dplyr::do() is deprecated, according to Hadley Wickham himself.\n\n\nI’ll be honest: the title is a bit misleading. I will not use purrr that much in this blog post. Actually, I will use one single purrr function, at the very end. I use dplyr much more. However Make ggplot2 purrr sounds better than Make ggplot dplyr or whatever the verb for dplyr would be.\n\n\nAlso, this blog post was inspired by a stackoverflow question and in particular one of the answers. So I don’t bring anything new to the table, but I found this stackoverflow answer so useful and so underrated (only 16 upvotes as I’m writing this!) that I wanted to write something about it.\n\n\nBasically the idea of this blog post is to show how to create graphs using ggplot2, but by grouping by a factor variable beforehand. To illustrate this idea, let’s use the data from the Penn World Tables 9.0. The easiest way to get this data is to install the package called pwt9 with:\n\ninstall.packages(\"pwt9\")\n\nand then load the data with:\n\ndata(\"pwt9.0\")\n\nNow, let’s load the needed packages. I am also using ggthemes which makes themeing your ggplots very easy. I’ll be making Tufte-style plots.\n\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(pwt9)\n\nFirst let’s select a list of countries:\n\ncountry_list &lt;- c(\"France\", \"Germany\", \"United States of America\", \"Luxembourg\", \"Switzerland\", \"Greece\")\n\nsmall_pwt &lt;- pwt9.0 %&gt;%\n  filter(country %in% country_list)\n\nLet’s us also order the countries in the data frame as I have written them in country_list:\n\nsmall_pwt &lt;- small_pwt %&gt;%\n  mutate(country = factor(country, levels = country_list, ordered = TRUE))\n\nYou might be wondering why this is important. At the end of the article, we are going to save the plots to disk. If we do not re-order the countries inside the data frame as in country_list, the name of the files will not correspond to the correct plots!\n\n\nUpdate: While this can still be interesting to know, especially if you want to order the bars of a barplot made with ggplot2, I included a suggestion by @expersso that does not require your data to be ordered!\n\n\nNow when you want to plot the same variable by countries, say avh (Average annual hours worked by persons engaged), the usual way to do this is with one of facet_wrap() or facet_grid():\n\nggplot(data = small_pwt) + theme_tufte() +\n  geom_line(aes(y = avh, x = year)) +\n  facet_wrap(~country)\n\n\n\nggplot(data = small_pwt) + theme_tufte() +\n  geom_line(aes(y = avh, x = year)) +\n  facet_grid(country~.)\n\n\n\n\nAs you can see, for this particular example, facet_grid() is not very useful, but do notice its argument, country~., which is different from facet_wrap()’s argument. This way, I get the graphs stacked horizontally. If I had used facet_grid(~country) the graphs would be side by side and completely unreadable.\n\n\nNow, let’s go to the meat of this post: what if you would like to have one single graph for each country? You’d probably think of using dplyr::group_by() to form the groups and then the graphs. This is the way to go, but you also have to use dplyr::do(). This is because as far as I understand, ggplot2 is not dplyr-aware, and using an arbitrary function with groups is only possible with dplyr::do().\n\n\nUpdate: As explained in the intro above, I also added the solution that uses tidyr::nest():\n\n# Ancient, deprecated way of doing this\nplots &lt;- small_pwt %&gt;%\n  group_by(country) %&gt;%\n  do(plot = ggplot(data = .) + theme_tufte() +\n       geom_line(aes(y = avh, x = year)) +\n       ggtitle(unique(.$country)) +\n       ylab(\"Year\") +\n       xlab(\"Average annual hours worked by persons engaged\"))\n\nAnd this is the approach that uses tidyr::nest():\n\n# Preferred approach\nplots &lt;- small_pwt %&gt;%\n  group_by(country) %&gt;%\n  nest() %&gt;%\n  mutate(plot = map2(data, country, ~ggplot(data = .x) + theme_tufte() +\n       geom_line(aes(y = avh, x = year)) +\n       ggtitle(.y) +\n       ylab(\"Year\") +\n       xlab(\"Average annual hours worked by persons engaged\")))\n\nIf you know dplyr at least a little bit, the above lines should be easy for you to understand. But notice how we get the title of the graphs, with ggtitle(unique(.$country)), which was actually the point of the stackoverflow question.\n\n\nUpdate: The modern version uses tidyr::nest(). Its documentation tells us:\n\n\nThere are many possible ways one could choose to nest columns inside a data frame. nest() creates a list of data frames containing all the nested variables: this seems to be the most useful form in practice. Let’s take a closer look at what it does exactly:\n\nsmall_pwt %&gt;%\n  group_by(country) %&gt;%\n  nest() %&gt;%\n  head()\n## # A tibble: 6 x 2\n##   country                  data              \n##   &lt;ord&gt;                    &lt;list&gt;            \n## 1 Switzerland              &lt;tibble [65 × 46]&gt;\n## 2 Germany                  &lt;tibble [65 × 46]&gt;\n## 3 France                   &lt;tibble [65 × 46]&gt;\n## 4 Greece                   &lt;tibble [65 × 46]&gt;\n## 5 Luxembourg               &lt;tibble [65 × 46]&gt;\n## 6 United States of America &lt;tibble [65 × 46]&gt;\n\nThis is why I love lists in R; we get a tibble where each element of the column data is itself a tibble. We can now apply any function that we know works on lists.\n\n\nWhat might be surprising though, is the object that is created by this code. Let’s take a look at plots:\n\nprint(plots)\n## # A tibble: 6 x 3\n##   country                  data               plot    \n##   &lt;ord&gt;                    &lt;list&gt;             &lt;list&gt;  \n## 1 Switzerland              &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 2 Germany                  &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 3 France                   &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 4 Greece                   &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 5 Luxembourg               &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 6 United States of America &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n\nAs dplyr::do()’s documentation tells us, the return values get stored inside a list. And this is exactly what we get back; a list of plots! Lists are a very flexible and useful class, and you cannot spell list without purrr (at least not when you’re a neRd).\n\n\nHere are the final lines that use purrr::map2() to save all these plots at once inside your working directory:\n\n\nUpdate: I have changed the code below which does not require your data frame to be ordered according to the variable country_list.\n\n# file_names &lt;- paste0(country_list, \".pdf\")\n\nmap2(paste0(plots$country, \".pdf\"), plots$plot, ggsave)\n\nAs I said before, if you do not re-order the countries inside the data frame, the names of the files and the plots will not match. Try running all the code without re-ordering, you’ll see!\n\n\nI hope you found this post useful. You can follow me on twitter for blog updates.\n\n\nUpdate: Many thanks to the readers of this article and for their useful suggestions. I love the R community; everyday I learn something new and useful!"
  },
  {
    "objectID": "posts/2021-09-04-quest_fast.html",
    "href": "posts/2021-09-04-quest_fast.html",
    "title": "The quest for fast(er?) row-oriented workflows",
    "section": "",
    "text": "Part 2 of this blog post is available here\n\n\nThe past few weeks I have been exploring the speed of R. It started with this video in which I explained that R is not necessarily slower than any other interpreted language, as long as you’re using the built-in, optimized functions. However should you write your own implementation of an algorithm, especially if that algorithm requires the use of one (or more…) loops, it’ll run slowly. As I’ve also mentioned in two other videos, here and here there are many ways to avoid loops, and you should do so if possible.\n\n\nTo continue exploring this is in more detail, I’ve written two very basic implementations of a genetic algorithm. The first version uses only {tidyverse} functions and the other only base R functions. My intuition was that base would be faster, but the code would likely be less “readable” (I discuss this code in greater detail in a series of videos, you can watch part 1 and part 2 if you’re interested in the nitty-gritty details). Code readability is quite subjective, but I think that there are some general “truths” regarding it, namely that it seems to often be that case that fast code is code that is “less” readable, and vice-versa. This blog post explores this trade-off in the context of row-oriented workflows.\n\n\nOnce I was done writing the two versions of the genetic algorithm for the video (a {tidyverse} one and a base one), I profiled the code and realised that, yes base was much much faster, but also that the reason the {tidyverse} version was running so slowly was because of one single row-based operation. Trying to replace this row-based operation, but remaining inside the {tidyverse} made for an interesting challenge. I will explain what I did in this blog post, so first let’s set up the example:\n\nlibrary(tidyverse)\nlibrary(brotools)\n\nLet’s first generate some data. For this, I’m going to use a function I wrote for my genetic algorithm. I won’t explain how it works, so if you’re curious, you can watch the videos I mention in the introduction where this is all explained in detail:\n\ninit_pop &lt;- function(objective_function, pop_size = 100, upper_bound = 1, lower_bound = 0){\n\n  parameters &lt;- formals(objective_function)[[1]] %&gt;%\n    eval\n\n  purrr::rerun(length(parameters), runif(n = pop_size,\n                                         min = lower_bound,\n                                         max = upper_bound)) %&gt;%\n    dplyr::bind_cols() %&gt;%\n    janitor::clean_names()\n\n}\n\nThis function takes another function, the objective function to be optimized, as an argument, and checks how many parameters this objective functions has, and generates a population of random solutions (if you don’t understand what this all means don’t worry. What matters is that this generates a random dataset whith as many columns as the objective function has arguments).\n\n\nThe next function is my objective function:\n\nmy_function &lt;- function(x = c(0, 0, 0, 0, 0, 0)){\n  x1 &lt;- x[1]\n  x2 &lt;- x[2]\n  x3 &lt;- x[3]\n  x4 &lt;- x[4]\n  x5 &lt;- x[5]\n  x6 &lt;- x[6]\n\n  -(x1**2 + x2 - 11)**2 - (x1 + x2**2 - 7)**2 - (x3**3 + x4 - 11)**2 - (x5 + x6**2 - 7)**2\n}\n\n(This is not the same as in the videos, which only has two arguments.)\n\n\nLet’s generate some data:\n\ndataset &lt;- init_pop(my_function) %&gt;%\n  as.data.frame()\nhead(dataset)\n##          x1        x2          x3        x4         x5        x6\n## 1 0.3045714 0.1436057 0.003754794 0.9144551 0.53070392 0.6127125\n## 2 0.3155244 0.8890011 0.556325257 0.5688512 0.02928638 0.5626903\n## 3 0.8363487 0.6361570 0.667718047 0.4704217 0.10547741 0.5278469\n## 4 0.8207208 0.1286540 0.189744816 0.3309174 0.76311349 0.7019268\n## 5 0.7244419 0.1284358 0.235967085 0.8444759 0.38697023 0.9818212\n## 6 0.2882561 0.9702481 0.983408531 0.1510577 0.84844059 0.7678110\n\nNow, on the actual problem: I need to add another column, with the value of my_function(), evaluated on a per row basis. As an example, for the first row, this would be the result of:\n\nmy_function(dataset[1, ])\n##          x1\n## 1 -299.2624\n\nMany people would probably solve this using a for loop, so let’s write a function to do just that (benchmarking will make it easier if the code is inside a function):\n\nrun_loop &lt;- function(dataset, my_function = my_function){\n\n  dataset$score &lt;- 0\n\n  for(i in seq(1, nrow(dataset))){\n\n    dataset$score[i] &lt;- my_function(dataset[i, ])\n  }\n\n  dataset\n\n}\n\n\nrun_loop(dataset, my_function = my_function) %&gt;%\n  head\n##          x1        x2          x3        x4         x5        x6     score\n## 1 0.3045714 0.1436057 0.003754794 0.9144551 0.53070392 0.6127125 -299.2624\n## 2 0.3155244 0.8890011 0.556325257 0.5688512 0.02928638 0.5626903 -284.4934\n## 3 0.8363487 0.6361570 0.667718047 0.4704217 0.10547741 0.5278469  -275.027\n## 4 0.8207208 0.1286540 0.189744816 0.3309174 0.76311349 0.7019268 -288.6529\n## 5 0.7244419 0.1284358 0.235967085 0.8444759 0.38697023 0.9818212 -281.0109\n## 6 0.2882561 0.9702481 0.983408531 0.1510577 0.84844059 0.7678110 -261.1376\n\nThe advantage of loops is that you don’t need to really know a lot about R to get it done; if you’ve learned some programming language some time during your studies, you learned about for loops. But they’re generally slower than other methods and error-prone (typos for example, or if you’re looping over several indeces, it can get quite complex…). And they’re, in my humble opinion, not always very easy to understand. This is not the case here, because it is quite a simple example, but often, it can get quite confusing to understand what is going on.\n\n\nSo what would be a more “R-specific” way of doing it (specific in the sense that it is not a universal solution like a for-loop), and which avoids using a loop? apply() would here be the best candidate:\n\napply(dataset, MARGIN = 1, FUN = my_function)\n##   [1] -299.2624 -284.4934 -275.0270 -288.6529 -281.0109 -261.1376 -293.7069\n##   [8] -264.7833 -270.6977 -258.5214 -299.6117 -275.8491 -306.8555 -284.7410\n##  [15] -298.6167 -299.2872 -294.9865 -264.5808 -272.8924 -289.5542 -306.3602\n##  [22] -293.4290 -305.9189 -276.9193 -286.1938 -291.7530 -289.3610 -290.8470\n##  [29] -303.5995 -261.4664 -280.6596 -287.2716 -282.6859 -293.5323 -304.2287\n##  [36] -286.9913 -258.3523 -275.9231 -304.3919 -250.9952 -286.7151 -255.0904\n##  [43] -312.2109 -254.5034 -255.9284 -287.8201 -285.9853 -290.8199 -309.0086\n##  [50] -311.4288 -271.1889 -299.3821 -290.1711 -281.0423 -294.1406 -275.8203\n##  [57] -274.1912 -257.7994 -308.3508 -271.5294 -293.3045 -296.9122 -277.8800\n##  [64] -296.9870 -314.1470 -270.0065 -288.3262 -252.3774 -263.9164 -286.9263\n##  [71] -302.5980 -281.0731 -269.0754 -301.6335 -294.3153 -268.4932 -263.6926\n##  [78] -306.9723 -271.8796 -292.6175 -294.0995 -303.4289 -280.5853 -277.6487\n##  [85] -262.2476 -310.0217 -281.7774 -292.7697 -295.8509 -269.0880 -253.2403\n##  [92] -279.8632 -293.0479 -258.1470 -303.6226 -306.4314 -293.4026 -275.8508\n##  [99] -269.6470 -285.0784\n\nAppending this to a dataframe can be done within a mutate() call (here again I’m encapsulating this inside a function, for benchmarking purposes):\n\nrun_apply &lt;- function(dataset, my_function = my_function){\n\n  dataset %&gt;%\n    mutate(score = apply(., MARGIN = 1, my_function))\n\n}\n\nrun_apply(dataset, my_function = my_function) %&gt;%\n  head()\n##          x1        x2          x3        x4         x5        x6     score\n## 1 0.3045714 0.1436057 0.003754794 0.9144551 0.53070392 0.6127125 -299.2624\n## 2 0.3155244 0.8890011 0.556325257 0.5688512 0.02928638 0.5626903 -284.4934\n## 3 0.8363487 0.6361570 0.667718047 0.4704217 0.10547741 0.5278469 -275.0270\n## 4 0.8207208 0.1286540 0.189744816 0.3309174 0.76311349 0.7019268 -288.6529\n## 5 0.7244419 0.1284358 0.235967085 0.8444759 0.38697023 0.9818212 -281.0109\n## 6 0.2882561 0.9702481 0.983408531 0.1510577 0.84844059 0.7678110 -261.1376\n\nMARGIN = 1 means that the function is applied on the rows, whereas MARGIN = 2 would apply the function over columns.\n\n\nIn terms of readability, I think that this is maybe a bit less readable than the for-loop, just because for-loops as very very ubiquitous. But it’s super simple once you understand how apply() works.\n\n\nNow, what would be a {tidyverse}-only approach? And why would you want to do a {tidyverse}-only approach anyways? Generally, I would argue that scripts written using {tidyverse} functions and style are easier to read. For example, I tweeted this code snippet some time ago:\n\nblogdown::shortcode(\"tweet\",\n                    \"1431718740341764099\"\n                    )\n\n{{% tweet “1431718740341764099” %}}\n\n\nand in my opinion the example in my tweet shows clearly that the {tidyverse} code is more easily understood and readable. Of course, some people disagree… However, in this case here, I’m not sure that a {tidyverse} approach would be more readable. The solution using apply() seems to me to be quite good. Let’s see how the {tidyverse} approach, which leverages rowwise(), looks like:\n\nrun_rowwise &lt;- function(dataset, my_function = my_function){\n  dataset %&gt;%\n    rowwise() %&gt;%\n    mutate(score = my_function(c_across(everything()))) %&gt;%\n    ungroup()\n}\n\nrun_rowwise(dataset, my_function = my_function) %&gt;%\n  head()\n## # A tibble: 6 × 7\n##      x1    x2      x3    x4     x5    x6 score\n##   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 0.305 0.144 0.00375 0.914 0.531  0.613 -299.\n## 2 0.316 0.889 0.556   0.569 0.0293 0.563 -284.\n## 3 0.836 0.636 0.668   0.470 0.105  0.528 -275.\n## 4 0.821 0.129 0.190   0.331 0.763  0.702 -289.\n## 5 0.724 0.128 0.236   0.844 0.387  0.982 -281.\n## 6 0.288 0.970 0.983   0.151 0.848  0.768 -261.\n\nThis runs, but much, much, more slower than with apply() (but faster than a for-loop, as we shall see) . Plus, it does look much, much more complicated than the simple apply() version! So why do it like this? You even need several functions - rowwise(), c_across() and everything() - to make it work! So why? Well, there is one use case in which this approach enables you to do something that I don’t think is possible (or at least easily possible) with apply() which is applying the function, but only over certain columns. For example, if you want to apply the function only over the columns which names all start with the letter “c”, you could write something like this:\n\nmtcars %&gt;%\n  rowwise() %&gt;%\n  mutate(score = mean(c_across(starts_with(\"c\")))) %&gt;%\n  ungroup()\n## # A tibble: 32 × 12\n##      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb score\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4   5  \n##  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4   5  \n##  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1   2.5\n##  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1   3.5\n##  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2   5  \n##  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1   3.5\n##  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4   6  \n##  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2   3  \n##  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2   3  \n## 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4   5  \n## # … with 22 more rows\n\nNow this is not needed here, so apply() clearly wins in terms readability (and speed as well). But in cases like the above, where you need to compute only over several columns, I think that the {tidyverse} version not only is very readible, but actually offers a solution to the problem. I’m not quite sure you could solve this easily with base, but please prove me wrong.\n\n\nIn any case, there’s another way to approach our original problem using {tidyverse} functions, but we still need the help of a base function.\n\n\nThe next approach uses the fact that map() needs both a list and a function as an input. As a refresher, here’s how map works:\n\n# We have a list\n\nmy_list &lt;- list(\"a\" = 2,\n                \"b\" = 4)\n\n# and we have a function, say sqrt, which we want to apply to each element of this list\n\nmap(my_list, sqrt)\n## $a\n## [1] 1.414214\n## \n## $b\n## [1] 2\n\nSo what we need is a way to mimick the basic approach which works on one “element” (in this case, a row of the dataframe), and extend that idea to a “list of rows”. Now, the issue is that a dataframe is actually a list of columns, not rows. So if you’re using map() over a dataframe, you will be looping over the columns, not the rows, as in the example below:\n\n# This applies the function class() to each colum of mtcars\nmtcars %&gt;%\n  map(class)\n## $mpg\n## [1] \"numeric\"\n## \n## $cyl\n## [1] \"numeric\"\n## \n## $disp\n## [1] \"numeric\"\n## \n## $hp\n## [1] \"numeric\"\n## \n## $drat\n## [1] \"numeric\"\n## \n## $wt\n## [1] \"numeric\"\n## \n## $qsec\n## [1] \"numeric\"\n## \n## $vs\n## [1] \"numeric\"\n## \n## $am\n## [1] \"numeric\"\n## \n## $gear\n## [1] \"numeric\"\n## \n## $carb\n## [1] \"numeric\"\n\nNow the question becomes; is there a way to turn a dataframe, which is a list of columns, into a list of rows? Yes, there is, using asplit():\n\nasplit(mtcars, MARGIN = 1) %&gt;%\n  head()\n## $`Mazda RX4`\n##    mpg    cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb \n##  21.00   6.00 160.00 110.00   3.90   2.62  16.46   0.00   1.00   4.00   4.00 \n## \n## $`Mazda RX4 Wag`\n##     mpg     cyl    disp      hp    drat      wt    qsec      vs      am    gear \n##  21.000   6.000 160.000 110.000   3.900   2.875  17.020   0.000   1.000   4.000 \n##    carb \n##   4.000 \n## \n## $`Datsun 710`\n##    mpg    cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb \n##  22.80   4.00 108.00  93.00   3.85   2.32  18.61   1.00   1.00   4.00   1.00 \n## \n## $`Hornet 4 Drive`\n##     mpg     cyl    disp      hp    drat      wt    qsec      vs      am    gear \n##  21.400   6.000 258.000 110.000   3.080   3.215  19.440   1.000   0.000   3.000 \n##    carb \n##   1.000 \n## \n## $`Hornet Sportabout`\n##    mpg    cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb \n##  18.70   8.00 360.00 175.00   3.15   3.44  17.02   0.00   0.00   3.00   2.00 \n## \n## $Valiant\n##    mpg    cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb \n##  18.10   6.00 225.00 105.00   2.76   3.46  20.22   1.00   0.00   3.00   1.00\n\nasplit() splits a dataframe along rows (with the MARGIN argument set to 1) or along columns (with MARGIN = 2). As you can see with the code above, the mtcars dataset is now a list of rows. Each element of this list is a single vector of values. Now that my dataframe is now a list of rows, well, I can simply use map() to apply any function over its rows:\n\nrun_map &lt;- function(dataset, my_function = my_function){\n  dataset %&gt;%\n    mutate(score = map_dbl(asplit(., 1), .f = my_function))\n}\n\nrun_map(dataset, my_function = my_function) %&gt;%\n  head()\n##          x1        x2          x3        x4         x5        x6     score\n## 1 0.3045714 0.1436057 0.003754794 0.9144551 0.53070392 0.6127125 -299.2624\n## 2 0.3155244 0.8890011 0.556325257 0.5688512 0.02928638 0.5626903 -284.4934\n## 3 0.8363487 0.6361570 0.667718047 0.4704217 0.10547741 0.5278469 -275.0270\n## 4 0.8207208 0.1286540 0.189744816 0.3309174 0.76311349 0.7019268 -288.6529\n## 5 0.7244419 0.1284358 0.235967085 0.8444759 0.38697023 0.9818212 -281.0109\n## 6 0.2882561 0.9702481 0.983408531 0.1510577 0.84844059 0.7678110 -261.1376\n\nSo we now have 4 approaches to solve the issue:\n\n\n\nrun_loop(): uses a for-loop\n\n\nrun_apply(): uses apply(), a base R function\n\n\nrun_rowwise(): a “pure” {tidyverse} approach\n\n\nrun_map(): a cross between a {tidyverse} and a base approach\n\n\n\nLet’s set up a function to run some benchmarks and see which runs faster. I’ll create a list of increasingly large data frames over which I’ll run all the above functions:\n\nlist_datasets &lt;- map(seq(2, 5), ~init_pop(objective_function = my_function,\n                                          pop_size = `^`(10, .x)))\n\nThe function below will run the benchmarks over all the data frames:\n\nrun_benchmarks &lt;- function(dataset, times = 5){\n  microbenchmark::microbenchmark(\n                    run_loop(dataset, my_function = my_function),\n                    run_apply(dataset, my_function = my_function),\n                    run_rowwise(dataset, my_function = my_function),\n                    run_map(dataset, my_function = my_function),\n                    times = times,\n                    unit = \"s\"\n                  )\n}\n\nI’ll run this in parallel using {furrr}:\n\nlibrary(furrr)\n\nplan(multisession, workers = 2)\n\nbenchmark_results &lt;- future_map(list_datasets, run_benchmarks)\n\nLet’s take a look at the results:\n\nbenchmark_data &lt;- map2(.x = benchmark_results, .y = 10^seq(2, 5), .f = ~mutate(tibble(.x), pop_size = .y)) %&gt;%\n  bind_rows() %&gt;%\n  mutate(expr = str_remove_all(expr, \"\\\\(.*\\\\)\")) %&gt;%  \n  group_by(expr, pop_size) %&gt;%\n  mutate(time_seconds = time/10^9) %&gt;%\n  summarise(fastest_run = min(time_seconds),\n            average_run = mean(time_seconds),\n            slowest_run = max(time_seconds))\n## `summarise()` has grouped output by 'expr'. You can override using the `.groups` argument.\nbenchmark_data %&gt;%\n  ggplot(aes(y = average_run, x = pop_size)) +\n  geom_ribbon(aes(ymin = fastest_run, ymax = slowest_run, fill = expr), alpha = .6) +\n  geom_line(aes(group = expr, col = expr)) +\n  ylab(\"Seconds\") +\n  xlab(\"Rows in the dataset\") +\n  ggtitle(\"Speed of rowwise operations using different methods\") +\n  theme_blog()\n\n\n\n\nUsing a for-loop for row-wise operations is clearly the slowest solution. Let’s take a closer look at the remaining 3 options:\n\nbenchmark_data %&gt;%\n  filter(!grepl(\"loop\", expr)) %&gt;% \n  ggplot(aes(y = average_run, x = pop_size)) +\n  geom_ribbon(aes(ymin = fastest_run, ymax = slowest_run, fill = expr), alpha = .6) +\n  ylab(\"Seconds\") +\n  xlab(\"Rows in the dataset\") +\n  ggtitle(\"Speed of rowwise operations using different methods\") +\n  theme_blog()\n\n\n\n\nrowwise() loses here, but unless you have to literally run such code hundreds of times, it is still tolerable. Gives you enough time to browse some memes. But if you have to run such operations millions of times, you might want to look at either using apply() or the other approach that uses asplit() and map(). Let’s take a closer look at these two:\n\nbenchmark_data %&gt;%\n  filter(!grepl(\"loop|rowwise\", expr)) %&gt;%\n  ggplot(aes(y = average_run, x = pop_size)) +\n  geom_ribbon(aes(ymin = fastest_run, ymax = slowest_run, fill = expr), alpha = .6) +\n  geom_line(aes(group = expr, col = expr)) +\n  ylab(\"Seconds\") +\n  xlab(\"Rows in the dataset\") +\n  ggtitle(\"Speed of rowwise operations using different methods\") +\n  theme_blog()\n\n\n\n\nInterestingly, the fastest run using map() was faster than the fastest run using apply(), but on average, both methods seem to be equivalent. In conclusion, if you need speed and you need to compute over every column apply() is a clear winner. But if you need row-wise operations, but only on a subset of columns, rowwise(), even though it is slow, seems to be the only solution.\n\n\nI wonder if there is a way to use c_across() with the map() approach, and potentially have the benefits of map() (as fast as apply()) and rowwise() (computing only over certain columns…). Another subject to explore later.\n\n\nPart 2 of this blog post is available here"
  },
  {
    "objectID": "posts/2021-03-19-no_loops_tidyeval.html",
    "href": "posts/2021-03-19-no_loops_tidyeval.html",
    "title": "How to treat as many files as fit on your hard disk without loops (sorta) nor running out of memory all the while being as lazy as possible",
    "section": "",
    "text": "tl;dr\n\n\nThis blog post is going to be long, and deal with many topics. But I think you’re going to enjoy it. So get a hot beverage and relax. Take the time to read. We don’t take enough time to read anymore. It’s a shame. But if you’re really busy, the tl;dr is that I found out a way of combining tidy evaluation and functional programming to analyze potentially millions of files (as many as fit on your hard disk) without running out of memory in R. As an example, I’m going to use the 15000ish Excel files from the Enron Corpus. It’s a pretty neat blog post, if I may say so myself, so you definitely should read it. If at the end you think I wasted your time, you can file a complaint here.\n\n\n\n\nIntroduction\n\n\nIf you’ve been a faithful reader of this blog, or if you watch my youtube channel you’ve very likely seen me write code that looks like this:\n\nlibrary(tidyverse)\nlibrary(rlang)\nlibrary(tidyxl)\nlibrary(brotools)\nmtcars_plot &lt;- mtcars %&gt;%\n  group_nest(am) %&gt;% #shortcut for group_by(am) %&gt;% nest() \n  mutate(plots = map2(.y = am, .x = data, ~{ggplot(data = .x) +\n                              geom_smooth(aes(y = mpg, x = hp), colour = \"#82518c\") +\n                                ggtitle(paste0(\"Miles per gallon as a function of horse power for am = \", .y)) +\n                                theme_blog()}))\n\nThis creates a new data frame that looks like this:\n\nmtcars_plot\n## # A tibble: 2 x 3\n##      am           data plots \n##   &lt;dbl&gt; &lt;list&lt;tibble&gt;&gt; &lt;list&gt;\n## 1     0      [19 × 10] &lt;gg&gt;  \n## 2     1      [13 × 10] &lt;gg&gt;\n\nIn three lines of code, I grouped the mtcars dataframe by the variable am and then created two plots, which are contained in a new column called plots. If you’re unfamiliar with R, it is quite likely that you’ve never seen anything like this. If you have experience with functional programming languages though, you might recognize what’s going on. Essentially, map2() loops over two variables, am and data (this variable is not in the original data frame, but gets created as a result of the group_nest(am) call) and applies a function, in this case a call to ggplot(), to generate two plots… If you’ve never seen this before, I invite you to read the section dedicated to this type of workflows on my ebook.\n\n\nLet’s take a look at the plots:\n\nmtcars_plot %&gt;%\n  pull(plots)\n## [[1]]\n## `geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n## \n## [[2]]\n## `geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\nThe advantage of this workflow is that you don’t have to think much about anything -once you understand how it works-. The alternative would be two create two separate data frames, and create two separate plots. That’s a totally valid solution, unless you need to create hundreds of plots. With the workflow above, it doesn’t matter if the am variable has 2 or 2000 levels. The code would look exactly the same.\n\n\nThis workflow is very flexible. You can even use this approach to read in, and analyze, many, many files. As many as, for instance, 15931 Excel files from an American oil company that went bust in the early 2000’s, Enron.\n\n\n\n\nThe Enron Corpus\n\n\nI won’t go into much detail about the Enron Corpus, but to make a long story short: Big evil American oil company went bust, company emails got released for research purposes after being purchased for 10000USD by a computer scientist, and many of these emails had Excel spreadsheets attached to them. Other computer scientist released spreadsheets for research purposes. You can read the whole story on Felienne Hermans’ blog (read it, it’s quite interesting).\n\n\nAnyways, you can now get this treasure trove of nightmarish Excel spreadsheets by clicking here (this is the link provided in the blog post by Felienne Hermans). I already discussed this in a previous blog post.\n\n\nOn Felienne Hermans’ blog post, you can spot the following table:\n\n\n\n\n\nI’m going to show how this table could be replicated using R and the mutate()-map() workflow above.\n\n\nFirst, let’s load one single spreadsheet with {tidyxl} and get some of the code ready that we will need. Let’s get all the paths to all the files in a vector:\n\nlist_paths &lt;- list.files(path = \"~/six_to/spreadsheets\",\n                         pattern = \".xlsx\",\n                         full.names = TRUE)\n\nLet’s work with the first one. Let’s read it in with {tidyxl}:\n\n(example_xlsx &lt;- xlsx_cells(list_paths[1]))\n## # A tibble: 19,859 x 21\n##    sheet       address   row   col is_blank data_type error logical numeric\n##    &lt;chr&gt;       &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;lgl&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;     &lt;dbl&gt;\n##  1 Preschedule A1          1     1 FALSE    date      &lt;NA&gt;  NA           NA\n##  2 Preschedule B1          1     2 TRUE     blank     &lt;NA&gt;  NA           NA\n##  3 Preschedule C1          1     3 TRUE     blank     &lt;NA&gt;  NA           NA\n##  4 Preschedule D1          1     4 TRUE     blank     &lt;NA&gt;  NA           NA\n##  5 Preschedule E1          1     5 TRUE     blank     &lt;NA&gt;  NA           NA\n##  6 Preschedule F1          1     6 TRUE     blank     &lt;NA&gt;  NA           NA\n##  7 Preschedule G1          1     7 TRUE     blank     &lt;NA&gt;  NA           NA\n##  8 Preschedule H1          1     8 TRUE     blank     &lt;NA&gt;  NA           NA\n##  9 Preschedule I1          1     9 TRUE     blank     &lt;NA&gt;  NA           NA\n## 10 Preschedule J1          1    10 TRUE     blank     &lt;NA&gt;  NA           NA\n## # … with 19,849 more rows, and 12 more variables: date &lt;dttm&gt;, character &lt;chr&gt;,\n## #   character_formatted &lt;list&gt;, formula &lt;chr&gt;, is_array &lt;lgl&gt;,\n## #   formula_ref &lt;chr&gt;, formula_group &lt;int&gt;, comment &lt;chr&gt;, height &lt;dbl&gt;,\n## #   width &lt;dbl&gt;, style_format &lt;chr&gt;, local_format_id &lt;int&gt;\n\nThe beauty of {tidyxl} is that it can read in a very complex and ugly Excel file without any issues. Each cell of the spreadsheet is going to be one row of the data set, the contents of all cells is now easily accessible. Let’s see how many sheets are in there:\n\nexample_xlsx %&gt;%\n  summarise(n_sheets = n_distinct(sheet))\n## # A tibble: 1 x 1\n##   n_sheets\n##      &lt;int&gt;\n## 1       11\n\n11… that’s already quite a lot. How many formulas are there per sheet?\n\nexample_xlsx %&gt;%\n  mutate(is_formula = !is.na(formula)) %&gt;%  \n  group_by(sheet) %&gt;%\n  summarise(n_formula = sum(is_formula)) %&gt;%\n  arrange(desc(n_formula))\n## # A tibble: 11 x 2\n##    sheet                  n_formula\n##    &lt;chr&gt;                      &lt;int&gt;\n##  1 Preschedule                 2651\n##  2 Deals                        324\n##  3 Economics                    192\n##  4 Balancing                     97\n##  5 Fuel                          70\n##  6 Comp                           0\n##  7 EPEData                        0\n##  8 HeatRate                       0\n##  9 spin reserve log sheet         0\n## 10 Top                            0\n## 11 Unit Summary                   0\n\nThere’s a sheet in there with 2651 formulas. This is insane. Anyways, as you can see, {tidyxl} makes analyzing what’s inside such Excel files quite simple. Let’s now create functions that will compute what we need. I won’t recreate everything from the table, but you’ll very quickly get the idea. Let’s start with a function to count spreadsheets that contain at least one formula:\n\nat_least_one_formula &lt;- function(x){\n\n  (any(!is.na(x$formula)))\n\n}\n\nLet’s get the number of worksheets:\n\nn_sheets &lt;- function(x){\n\n  x %&gt;%\n    summarise(n_sheets =  n_distinct(sheet)) %&gt;%\n    pull(n_sheets)\n\n}\n\nAnd how many formulas are contained in a spreadsheet:\n\nn_formulas &lt;- function(x){\n\n  x %&gt;%\n    mutate(is_formula = !is.na(formula)) %&gt;%\n    summarise(n_formula = sum(is_formula)) %&gt;%\n    pull(n_formula)\n\n}\n\nLet’s stop here. We could of course continue adding functions, but that’s enough to illustrate what’s coming. Let’s just define one last function. This function will call all three functions defined above, and return the result in a dataframe. You’ll see why soon enough:\n\nget_stats &lt;- function(x){\n\n  tribble(~has_formula, ~n_sheets, ~n_formulas,\n          at_least_one_formula(x), n_sheets(x), n_formulas(x))\n\n}\n\nLet’s try it out on our single spreadsheet:\n\nget_stats(example_xlsx)\n## # A tibble: 1 x 3\n##   has_formula n_sheets n_formulas\n##   &lt;lgl&gt;          &lt;int&gt;      &lt;int&gt;\n## 1 TRUE              11       3334\n\nNeat.\n\n\nNow, let’s see how we can apply these function to 15k+ Excel spreadsheets.\n\n\n\n\nNo loops ever allowed\n\n\n10 years ago, I was confronted to a similar problem. I had a pretty huge amount of files on a computer that I needed to analyze for a chapter of my Phd thesis. The way I solved this issue was by writing a loop that looked horrible and did what I needed on each file. It did the job, but it did not look good, and was a nightmare whenever I needed to modify it, which I needed to do often. I had to think about a structure to hold the results; it was a nested list with I think 4 or 5 levels, and I had to keep track of the dimensions in my head to make sure I was writing the right result in the right spot. It wasn’t pleasant. Until this week, I thought that such a loop was the only real solution to such a problem.\n\n\nBut a comment on one of my youtube video changed this:\n\n\n\n\n\n\n\nThe comment was made on this video in which I create a data set like in the introduction to this blog post, but instead of having 2 groups (and thus 2 datasets), I had 100. Now, in the video this wasn’t an issue, but what if instead of having 100 datasets, I had 15k+? And what if these datasets were quite huge? For example, the largest spreadsheet in the Enron Corpus is 40MiB. Loading it with {tidyxl} returns a tibble with 17 million rows, and needs 2GiB of RAM in a clean R session. If you want to read in all the 15k+, you’re simply going to run out of memory even before you could analyze anything. As I’ve written above, the solution would be to loop over each file, do whatever I need done, and save the results in some kind of structure (very likely some complex nested list). Or is it the only solution? Turns out that I tried some things out and found a solution that does not require changing my beloved mutate()-map() workflow.\n\n\nLet’s first start by putting the paths in a data frame:\n\n(enron &lt;- enframe(list_paths, name = NULL, value = \"paths\"))\n## # A tibble: 15,871 x 1\n##    paths                                                                        \n##    &lt;chr&gt;                                                                        \n##  1 /home/cbrunos/six_to/spreadsheets/albert_meyers__1__1-25act.xlsx             \n##  2 /home/cbrunos/six_to/spreadsheets/albert_meyers__2__1-29act.xlsx             \n##  3 /home/cbrunos/six_to/spreadsheets/andrea_ring__10__ENRONGAS(1200).xlsx       \n##  4 /home/cbrunos/six_to/spreadsheets/andrea_ring__11__ENRONGAS(0101).xlsx       \n##  5 /home/cbrunos/six_to/spreadsheets/andrea_ring__12__ENRONGAS(1200).xlsx       \n##  6 /home/cbrunos/six_to/spreadsheets/andrea_ring__13__Trader & Products 5-15-01…\n##  7 /home/cbrunos/six_to/spreadsheets/andrea_ring__14__Trader & Products 5-16-01…\n##  8 /home/cbrunos/six_to/spreadsheets/andrea_ring__15__IFERCnov.xlsx             \n##  9 /home/cbrunos/six_to/spreadsheets/andrea_ring__16__ifercdec.xlsx             \n## 10 /home/cbrunos/six_to/spreadsheets/andrea_ring__17__IFERCJan.xlsx             \n## # … with 15,861 more rows\n\nFor the purposes of this blog post, let’s limit ourselves to 30 spreadsheets. This won’t impact how the code is going to work, nor memory usage. It’s just that I won’t my post to compile quickly while I’m writing:\n\n(enron &lt;- head(enron, 30)) \n## # A tibble: 30 x 1\n##    paths                                                                        \n##    &lt;chr&gt;                                                                        \n##  1 /home/cbrunos/six_to/spreadsheets/albert_meyers__1__1-25act.xlsx             \n##  2 /home/cbrunos/six_to/spreadsheets/albert_meyers__2__1-29act.xlsx             \n##  3 /home/cbrunos/six_to/spreadsheets/andrea_ring__10__ENRONGAS(1200).xlsx       \n##  4 /home/cbrunos/six_to/spreadsheets/andrea_ring__11__ENRONGAS(0101).xlsx       \n##  5 /home/cbrunos/six_to/spreadsheets/andrea_ring__12__ENRONGAS(1200).xlsx       \n##  6 /home/cbrunos/six_to/spreadsheets/andrea_ring__13__Trader & Products 5-15-01…\n##  7 /home/cbrunos/six_to/spreadsheets/andrea_ring__14__Trader & Products 5-16-01…\n##  8 /home/cbrunos/six_to/spreadsheets/andrea_ring__15__IFERCnov.xlsx             \n##  9 /home/cbrunos/six_to/spreadsheets/andrea_ring__16__ifercdec.xlsx             \n## 10 /home/cbrunos/six_to/spreadsheets/andrea_ring__17__IFERCJan.xlsx             \n## # … with 20 more rows\n\nOk, so now, in order to read in all these files, I would write the following code:\n\nenron %&gt;%\n  mutate(datasets = map(paths, xlsx_cells))\n\nThis would create a new column called datasets where each element would be a complete data set. If I run this in my 30 examples, it might be ok. But if I run it on the full thing, there’s no way I’m not going to run out of RAM. So how to solve this issue? How to run my neat get_stats() function on all datasets if I cannot read in the data? The solution is to only read in the data when I need it, and only one dataset at a time. The solution is to build a lazy tibble. And this is possible using quo(). To quickly grasp what quo() does, let’s try calling the following expression once with, and once without quo():\n\nrunif(10)\n##  [1] 0.98342755 0.13500737 0.06196822 0.61304269 0.30600919 0.48015570\n##  [7] 0.05747049 0.04535318 0.37880304 0.70647563\n\nThis runs runif(10) returning 10 randomly generated numbers, as expected.\n\nquo(unif(10))\n## &lt;quosure&gt;\n## expr: ^unif(10)\n## env:  global\n\nThis instead returns a quosure, which to be honest, is a complex beast. I’m not sure I get it myself. The definition, is that quosures are quoted expressions that keep track of an environment. For our practical purposes, we can use that to delay when the data gets read in, and that’s all that matters:\n\n(enron &lt;- enron %&gt;%\n   mutate(datasets = map(paths, ~quo(xlsx_cells(.)))))\n## # A tibble: 30 x 2\n##    paths                                                                datasets\n##    &lt;chr&gt;                                                                &lt;list&gt;  \n##  1 /home/cbrunos/six_to/spreadsheets/albert_meyers__1__1-25act.xlsx     &lt;quosur…\n##  2 /home/cbrunos/six_to/spreadsheets/albert_meyers__2__1-29act.xlsx     &lt;quosur…\n##  3 /home/cbrunos/six_to/spreadsheets/andrea_ring__10__ENRONGAS(1200).x… &lt;quosur…\n##  4 /home/cbrunos/six_to/spreadsheets/andrea_ring__11__ENRONGAS(0101).x… &lt;quosur…\n##  5 /home/cbrunos/six_to/spreadsheets/andrea_ring__12__ENRONGAS(1200).x… &lt;quosur…\n##  6 /home/cbrunos/six_to/spreadsheets/andrea_ring__13__Trader & Product… &lt;quosur…\n##  7 /home/cbrunos/six_to/spreadsheets/andrea_ring__14__Trader & Product… &lt;quosur…\n##  8 /home/cbrunos/six_to/spreadsheets/andrea_ring__15__IFERCnov.xlsx     &lt;quosur…\n##  9 /home/cbrunos/six_to/spreadsheets/andrea_ring__16__ifercdec.xlsx     &lt;quosur…\n## 10 /home/cbrunos/six_to/spreadsheets/andrea_ring__17__IFERCJan.xlsx     &lt;quosur…\n## # … with 20 more rows\n\nThis takes less than a second to run, and not just because I only have 30 paths. Even if I was working on the complete 15k+ datasets, this would run in an instant. That’s because we’re actually not reading in anything yet. We’re only setting the scene.\n\n\nThe magic happens now: we’re going to now map our function that computes the stats we need. We only need to change one thing. Let’s see:\n\nget_stats &lt;- function(x){\n\n  x &lt;- eval_tidy(x)\n\n  tribble(~has_formula, ~n_sheets, ~n_formulas,\n          at_least_one_formula(x), n_sheets(x), n_formulas(x))\n\n}\n\nI’ve added this line:\n\nx &lt;- eval_tidy(x)\n\nThis evaluates the quosure, thus instantiating the dataset, and then proceeds to make all the computations. Let’s see what happens when we run this on our lazy tibble:\n\n(enron &lt;- enron %&gt;%\n   mutate(stats = map(datasets, get_stats)))\n## # A tibble: 30 x 3\n##    paths                                                  datasets  stats       \n##    &lt;chr&gt;                                                  &lt;list&gt;    &lt;list&gt;      \n##  1 /home/cbrunos/six_to/spreadsheets/albert_meyers__1__1… &lt;quosure&gt; &lt;tibble [1 …\n##  2 /home/cbrunos/six_to/spreadsheets/albert_meyers__2__1… &lt;quosure&gt; &lt;tibble [1 …\n##  3 /home/cbrunos/six_to/spreadsheets/andrea_ring__10__EN… &lt;quosure&gt; &lt;tibble [1 …\n##  4 /home/cbrunos/six_to/spreadsheets/andrea_ring__11__EN… &lt;quosure&gt; &lt;tibble [1 …\n##  5 /home/cbrunos/six_to/spreadsheets/andrea_ring__12__EN… &lt;quosure&gt; &lt;tibble [1 …\n##  6 /home/cbrunos/six_to/spreadsheets/andrea_ring__13__Tr… &lt;quosure&gt; &lt;tibble [1 …\n##  7 /home/cbrunos/six_to/spreadsheets/andrea_ring__14__Tr… &lt;quosure&gt; &lt;tibble [1 …\n##  8 /home/cbrunos/six_to/spreadsheets/andrea_ring__15__IF… &lt;quosure&gt; &lt;tibble [1 …\n##  9 /home/cbrunos/six_to/spreadsheets/andrea_ring__16__if… &lt;quosure&gt; &lt;tibble [1 …\n## 10 /home/cbrunos/six_to/spreadsheets/andrea_ring__17__IF… &lt;quosure&gt; &lt;tibble [1 …\n## # … with 20 more rows\n\nWhat happened here is nothing short of black magic: one by one, each quosure was instantiated, and the required stats were computed, then the dataset was thrown into the garbage before moving on to the next quosure. This means that RAM usage was kept to a minimum, and I could have run this over my 15k+ spreadsheets without any issue. You can watch me run similar code in my video here; I show how my RAM usage does not move even though I’m mapping over all the Excel sheets. The column stats now holds one dataframe with one row and three columns for each Excel file. Because stats is a list-column of dataframes, we can use unnest() to get to the data. Let’s take a closer look on one dataframe:\n\nenron %&gt;%\n  head(1) %&gt;%\n  select(paths, stats) %&gt;%\n  unnest(cols = stats)\n## # A tibble: 1 x 4\n##   paths                                          has_formula n_sheets n_formulas\n##   &lt;chr&gt;                                          &lt;lgl&gt;          &lt;int&gt;      &lt;int&gt;\n## 1 /home/cbrunos/six_to/spreadsheets/albert_meye… TRUE              11       3334\n\nWe see that by using unnest(), the two columns inside the nested dataframe get expanded and become columns of the “main” dataframe.\n\n\nWe’re done, but let’s clean up the dataset a little bit and take a look at the results:\n\n(\n  enron &lt;- enron %&gt;%\n    mutate(excel_file = str_remove(paths, \"/home/cbrunos/six_to/spreadsheets/\")) %&gt;%\n    select(-paths, -datasets) %&gt;%\n    unnest(cols = stats)\n)\n## # A tibble: 30 x 4\n##    has_formula n_sheets n_formulas excel_file                                   \n##    &lt;lgl&gt;          &lt;int&gt;      &lt;int&gt; &lt;chr&gt;                                        \n##  1 TRUE              11       3334 albert_meyers__1__1-25act.xlsx               \n##  2 TRUE              11       3361 albert_meyers__2__1-29act.xlsx               \n##  3 TRUE               4        550 andrea_ring__10__ENRONGAS(1200).xlsx         \n##  4 TRUE               4        549 andrea_ring__11__ENRONGAS(0101).xlsx         \n##  5 TRUE               4        550 andrea_ring__12__ENRONGAS(1200).xlsx         \n##  6 FALSE              0          0 andrea_ring__13__Trader & Products 5-15-01 E…\n##  7 FALSE              0          0 andrea_ring__14__Trader & Products 5-16-01 E…\n##  8 TRUE               1        169 andrea_ring__15__IFERCnov.xlsx               \n##  9 TRUE               1        177 andrea_ring__16__ifercdec.xlsx               \n## 10 TRUE               1        162 andrea_ring__17__IFERCJan.xlsx               \n## # … with 20 more rows\n\nGetting some statistics is now easy:\n\nenron %&gt;%\n  summarise(average_n_formulas = mean(n_formulas),\n            max_sheets = max(n_sheets))\n## # A tibble: 1 x 2\n##   average_n_formulas max_sheets\n##                &lt;dbl&gt;      &lt;int&gt;\n## 1               490.         11\n\nBy the way, now that we see that the code works, we can run it on all the spreadsheets simply by not running the following line:\n\n(enron &lt;- head(enron, 30)) \n\nAlso, we can quite easily run all of this in parallel using {furrr}:\n\nlibrary(furrr)\n## Loading required package: future\nplan(multiprocess, workers = 12)\n\nenron &lt;- enframe(list_paths, name = NULL, value = \"paths\")\n\nenron &lt;- head(enron, 1200) #just to compile the document faster, I only consider 1200 Excel spreadsheets\n\nenron &lt;- enron %&gt;%\n   mutate(datasets = map(paths, ~quo(xlsx_cells(.))))\n\nstart &lt;- Sys.time()\nenron &lt;- enron %&gt;%\n  mutate(stats = future_map(datasets, get_stats))\nSys.time() - start\n## Time difference of 36.86839 secs\n\nSame code, no parallelization (it takes longer, obviously):\n\nenron &lt;- enframe(list_paths, name = NULL, value = \"paths\")\n\nenron &lt;- head(enron, 1200)\n\nenron &lt;- enron %&gt;%\n   mutate(datasets = map(paths, ~quo(xlsx_cells(.))))\n\nstart &lt;- Sys.time()\nenron &lt;- enron %&gt;%\n  mutate(stats = map(datasets, get_stats))\nSys.time() - start\n## Time difference of 1.217199 mins\n\nI think this is pretty neat."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html",
    "href": "posts/2017-03-08-lesser_known_tricks.html",
    "title": "Lesser known dplyr tricks",
    "section": "",
    "text": "In this blog post I share some lesser-known (at least I believe they are) tricks that use mainly functions from dplyr."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#removing-unneeded-columns",
    "href": "posts/2017-03-08-lesser_known_tricks.html#removing-unneeded-columns",
    "title": "Lesser known dplyr tricks",
    "section": "\nRemoving unneeded columns\n",
    "text": "Removing unneeded columns\n\n\nDid you know that you can use - in front of a column name to remove it from a data frame?\n\nmtcars %&gt;% \n    select(-disp) %&gt;% \n    head()\n##                    mpg cyl  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#re-ordering-columns",
    "href": "posts/2017-03-08-lesser_known_tricks.html#re-ordering-columns",
    "title": "Lesser known dplyr tricks",
    "section": "\nRe-ordering columns\n",
    "text": "Re-ordering columns\n\n\nStill using select(), it is easy te re-order columns in your data frame:\n\nmtcars %&gt;% \n    select(cyl, disp, hp, everything()) %&gt;% \n    head()\n##                   cyl disp  hp  mpg drat    wt  qsec vs am gear carb\n## Mazda RX4           6  160 110 21.0 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag       6  160 110 21.0 3.90 2.875 17.02  0  1    4    4\n## Datsun 710          4  108  93 22.8 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive      6  258 110 21.4 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout   8  360 175 18.7 3.15 3.440 17.02  0  0    3    2\n## Valiant             6  225 105 18.1 2.76 3.460 20.22  1  0    3    1\n\nAs its name implies everything() simply means all the other columns."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#renaming-columns-with-rename",
    "href": "posts/2017-03-08-lesser_known_tricks.html#renaming-columns-with-rename",
    "title": "Lesser known dplyr tricks",
    "section": "\nRenaming columns with rename()\n",
    "text": "Renaming columns with rename()\n\nmtcars &lt;- rename(mtcars, spam_mpg = mpg)\nmtcars &lt;- rename(mtcars, spam_disp = disp)\nmtcars &lt;- rename(mtcars, spam_hp = hp)\n\nhead(mtcars)\n##                   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am\n## Mazda RX4             21.0   6       160     110 3.90 2.620 16.46  0  1\n## Mazda RX4 Wag         21.0   6       160     110 3.90 2.875 17.02  0  1\n## Datsun 710            22.8   4       108      93 3.85 2.320 18.61  1  1\n## Hornet 4 Drive        21.4   6       258     110 3.08 3.215 19.44  1  0\n## Hornet Sportabout     18.7   8       360     175 3.15 3.440 17.02  0  0\n## Valiant               18.1   6       225     105 2.76 3.460 20.22  1  0\n##                   gear carb\n## Mazda RX4            4    4\n## Mazda RX4 Wag        4    4\n## Datsun 710           4    1\n## Hornet 4 Drive       3    1\n## Hornet Sportabout    3    2\n## Valiant              3    1"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#selecting-columns-with-a-regexp",
    "href": "posts/2017-03-08-lesser_known_tricks.html#selecting-columns-with-a-regexp",
    "title": "Lesser known dplyr tricks",
    "section": "\nSelecting columns with a regexp\n",
    "text": "Selecting columns with a regexp\n\n\nIt is easy to select the columns that start with “spam” with some helper functions:\n\nmtcars %&gt;% \n    select(contains(\"spam\")) %&gt;% \n    head()\n##                   spam_mpg spam_disp spam_hp\n## Mazda RX4             21.0       160     110\n## Mazda RX4 Wag         21.0       160     110\n## Datsun 710            22.8       108      93\n## Hornet 4 Drive        21.4       258     110\n## Hornet Sportabout     18.7       360     175\n## Valiant               18.1       225     105\n\ntake also a look at starts_with(), ends_with(), contains(), matches(), num_range(), one_of() and everything()."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#create-new-columns-with-mutate-and-if_else",
    "href": "posts/2017-03-08-lesser_known_tricks.html#create-new-columns-with-mutate-and-if_else",
    "title": "Lesser known dplyr tricks",
    "section": "\nCreate new columns with mutate() and if_else()\n",
    "text": "Create new columns with mutate() and if_else()\n\nmtcars %&gt;% \n    mutate(vs_new = if_else(\n        vs == 1, \n        \"one\", \n        \"zero\", \n        NA_character_)) %&gt;% \n    head()\n##   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am gear carb vs_new\n## 1     21.0   6       160     110 3.90 2.620 16.46  0  1    4    4   zero\n## 2     21.0   6       160     110 3.90 2.875 17.02  0  1    4    4   zero\n## 3     22.8   4       108      93 3.85 2.320 18.61  1  1    4    1    one\n## 4     21.4   6       258     110 3.08 3.215 19.44  1  0    3    1    one\n## 5     18.7   8       360     175 3.15 3.440 17.02  0  0    3    2   zero\n## 6     18.1   6       225     105 2.76 3.460 20.22  1  0    3    1    one\n\nYou might want to create a new variable conditionally on several values of another column:\n\nmtcars %&gt;% \n    mutate(carb_new = case_when(.$carb == 1 ~ \"one\",\n                                .$carb == 2 ~ \"two\",\n                                .$carb == 4 ~ \"four\",\n                                 TRUE ~ \"other\")) %&gt;% \n    head(15)\n##    spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am gear carb\n## 1      21.0   6     160.0     110 3.90 2.620 16.46  0  1    4    4\n## 2      21.0   6     160.0     110 3.90 2.875 17.02  0  1    4    4\n## 3      22.8   4     108.0      93 3.85 2.320 18.61  1  1    4    1\n## 4      21.4   6     258.0     110 3.08 3.215 19.44  1  0    3    1\n## 5      18.7   8     360.0     175 3.15 3.440 17.02  0  0    3    2\n## 6      18.1   6     225.0     105 2.76 3.460 20.22  1  0    3    1\n## 7      14.3   8     360.0     245 3.21 3.570 15.84  0  0    3    4\n## 8      24.4   4     146.7      62 3.69 3.190 20.00  1  0    4    2\n## 9      22.8   4     140.8      95 3.92 3.150 22.90  1  0    4    2\n## 10     19.2   6     167.6     123 3.92 3.440 18.30  1  0    4    4\n## 11     17.8   6     167.6     123 3.92 3.440 18.90  1  0    4    4\n## 12     16.4   8     275.8     180 3.07 4.070 17.40  0  0    3    3\n## 13     17.3   8     275.8     180 3.07 3.730 17.60  0  0    3    3\n## 14     15.2   8     275.8     180 3.07 3.780 18.00  0  0    3    3\n## 15     10.4   8     472.0     205 2.93 5.250 17.98  0  0    3    4\n##    carb_new\n## 1      four\n## 2      four\n## 3       one\n## 4       one\n## 5       two\n## 6       one\n## 7      four\n## 8       two\n## 9       two\n## 10     four\n## 11     four\n## 12    other\n## 13    other\n## 14    other\n## 15     four\n\nMind the .$ before the variable carb. There is a github issue about this, and it is already fixed in the development version of dplyr, which means that in the next version of dplyr, case_when() will work as any other specialized dplyr function inside mutate()."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#apply-a-function-to-certain-columns-only-by-rows",
    "href": "posts/2017-03-08-lesser_known_tricks.html#apply-a-function-to-certain-columns-only-by-rows",
    "title": "Lesser known dplyr tricks",
    "section": "\nApply a function to certain columns only, by rows\n",
    "text": "Apply a function to certain columns only, by rows\n\nmtcars %&gt;%\n    select(am, gear, carb) %&gt;%\n    purrr::by_row(sum, .collate = \"cols\", .to = \"sum_am_gear_carb\") -&gt; mtcars2\nhead(mtcars2)\n\nFor this, I had to use purrr’s by_row() function. You can then add this column to your original data frame:\n\nmtcars &lt;- cbind(mtcars, \"sum_am_gear_carb\" = mtcars2$sum_am_gear_carb)\nhead(mtcars)\n##                   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am\n## Mazda RX4             21.0   6       160     110 3.90 2.620 16.46  0  1\n## Mazda RX4 Wag         21.0   6       160     110 3.90 2.875 17.02  0  1\n## Datsun 710            22.8   4       108      93 3.85 2.320 18.61  1  1\n## Hornet 4 Drive        21.4   6       258     110 3.08 3.215 19.44  1  0\n## Hornet Sportabout     18.7   8       360     175 3.15 3.440 17.02  0  0\n## Valiant               18.1   6       225     105 2.76 3.460 20.22  1  0\n##                   gear carb sum_am_gear_carb\n## Mazda RX4            4    4                9\n## Mazda RX4 Wag        4    4                9\n## Datsun 710           4    1                6\n## Hornet 4 Drive       3    1                4\n## Hornet Sportabout    3    2                5\n## Valiant              3    1                4"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#use-do-to-do-any-arbitrary-operation",
    "href": "posts/2017-03-08-lesser_known_tricks.html#use-do-to-do-any-arbitrary-operation",
    "title": "Lesser known dplyr tricks",
    "section": "\nUse do() to do any arbitrary operation\n",
    "text": "Use do() to do any arbitrary operation\n\nmtcars %&gt;% \n    group_by(cyl) %&gt;% \n    do(models = lm(spam_mpg ~ drat + wt, data = .)) %&gt;% \n    broom::tidy(models)\n## # A tibble: 9 x 6\n## # Groups:   cyl [3]\n##     cyl term        estimate std.error statistic p.value\n##   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1     4 (Intercept)   33.2      17.1       1.94  0.0877 \n## 2     4 drat           1.32      3.45      0.384 0.711  \n## 3     4 wt            -5.24      2.22     -2.37  0.0456 \n## 4     6 (Intercept)   30.7       7.51      4.08  0.0151 \n## 5     6 drat          -0.444     1.17     -0.378 0.725  \n## 6     6 wt            -2.99      1.57     -1.91  0.129  \n## 7     8 (Intercept)   29.7       7.09      4.18  0.00153\n## 8     8 drat          -1.47      1.63     -0.903 0.386  \n## 9     8 wt            -2.45      0.799    -3.07  0.0107\n\ndo() is useful when you want to use any R function (user defined functions work too!) with dplyr functions. First I grouped the observations by cyl and then ran a linear model for each group. Then I converted the output to a tidy data frame using broom::tidy()."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#using-dplyr-functions-inside-your-own-functions",
    "href": "posts/2017-03-08-lesser_known_tricks.html#using-dplyr-functions-inside-your-own-functions",
    "title": "Lesser known dplyr tricks",
    "section": "\nUsing dplyr functions inside your own functions\n",
    "text": "Using dplyr functions inside your own functions\n\nextract_vars &lt;- function(data, some_string){\n    \n  data %&gt;%\n    select_(lazyeval::interp(~contains(some_string))) -&gt; data\n    \n  return(data)\n}\n\nextract_vars(mtcars, \"spam\")\n##                     spam_mpg spam_disp spam_hp\n## Mazda RX4               21.0     160.0     110\n## Mazda RX4 Wag           21.0     160.0     110\n## Datsun 710              22.8     108.0      93\n## Hornet 4 Drive          21.4     258.0     110\n## Hornet Sportabout       18.7     360.0     175\n## Valiant                 18.1     225.0     105\n## Duster 360              14.3     360.0     245\n## Merc 240D               24.4     146.7      62\n## Merc 230                22.8     140.8      95\n## Merc 280                19.2     167.6     123\n## Merc 280C               17.8     167.6     123\n## Merc 450SE              16.4     275.8     180\n## Merc 450SL              17.3     275.8     180\n## Merc 450SLC             15.2     275.8     180\n## Cadillac Fleetwood      10.4     472.0     205\n## Lincoln Continental     10.4     460.0     215\n## Chrysler Imperial       14.7     440.0     230\n## Fiat 128                32.4      78.7      66\n## Honda Civic             30.4      75.7      52\n## Toyota Corolla          33.9      71.1      65\n## Toyota Corona           21.5     120.1      97\n## Dodge Challenger        15.5     318.0     150\n## AMC Javelin             15.2     304.0     150\n## Camaro Z28              13.3     350.0     245\n## Pontiac Firebird        19.2     400.0     175\n## Fiat X1-9               27.3      79.0      66\n## Porsche 914-2           26.0     120.3      91\n## Lotus Europa            30.4      95.1     113\n## Ford Pantera L          15.8     351.0     264\n## Ferrari Dino            19.7     145.0     175\n## Maserati Bora           15.0     301.0     335\n## Volvo 142E              21.4     121.0     109\n\nAbout this last point, you can read more about it here.\n\n\nHope you liked this small list of tricks!"
  },
  {
    "objectID": "posts/2017-11-14-peace_r.html",
    "href": "posts/2017-11-14-peace_r.html",
    "title": "Peace of mind with purrr",
    "section": "",
    "text": "I think what I enjoy the most about functional programming is the peace of mind that comes with it. With functional programming, there’s a lot of stuff you don’t need to think about. You can write functions that are general enough so that they solve a variety of problems. For example, imagine for a second that R does not have the sum() function anymore. If you want to compute the sum of, say, the first 100 integers, you could write a loop that would do that for you:\n\nnumbers = 0\n\nfor (i in 1:100){\n  numbers = numbers + i\n}\n\nprint(numbers)\n\n[1] 5050\n\n\nThe problem with this approach, is that you cannot reuse any of the code there, even if you put it inside a function. For instance, what if you want to merge 4 datasets together? You would need something like this:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata(mtcars)\n\nmtcars1 = mtcars %&gt;%\n  mutate(id = \"1\")\n\nmtcars2 = mtcars %&gt;%\n  mutate(id = \"2\")\n\nmtcars3 = mtcars %&gt;%\n  mutate(id = \"3\")\n\nmtcars4 = mtcars %&gt;%\n  mutate(id = \"4\")\n\ndatasets = list(mtcars1, mtcars2, mtcars3, mtcars4)\n\ntemp = datasets[[1]]\n\nfor(i in 1:3){\n  temp = full_join(temp, datasets[[i+1]])\n}\n\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\n\n\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\n\nglimpse(temp)\n\nRows: 128\nColumns: 12\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n$ id   &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n\n\nOf course, the logic is very similar as before, but you need to think carefully about the structure holding your elements (which can be numbers, datasets, characters, etc…) as well as be careful about indexing correctly… and depending on the type of objects you are working on, you might need to tweak the code further.\nHow would a functional programming approach make this easier? Of course, you could use purrr::reduce() to solve these problems. However, since I assumed that sum() does not exist, I will also assume that purrr::reduce() does not exist either and write my own, clumsy implementation. Here’s the code:\n\nmy_reduce = function(a_list, a_func, init = NULL, ...){\n\n  if(is.null(init)){\n    init = `[[`(a_list, 1)\n    a_list = tail(a_list, -1)\n  }\n\n  car = `[[`(a_list, 1)\n  cdr = tail(a_list, -1)\n  init = a_func(init, car, ...)\n\n  if(length(cdr) != 0){\n    my_reduce(cdr, a_func, init, ...)\n  }\n  else {\n    init\n  }\n}\n\nThis can look much more complicated than before, but the idea is quite simple; if you know about recursive functions (recursive functions are functions that call themselves). I won’t explain how the function works, because it is not the main point of the article (but if you’re curious, I encourage you to play around with it). The point is that now, I can do the following:\n\nmy_reduce(list(1,2,3,4,5), `+`)\n\n[1] 15\n\nmy_reduce(datasets, full_join) %&gt;% glimpse\n\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\n\n\nRows: 128\nColumns: 12\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n$ id   &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n\n\nBut since my_reduce() is very general, I can even do this:\n\nmy_reduce(list(1, 2, 3, 4, \"5\"), paste)\n\n[1] \"1 2 3 4 5\"\n\n\nOf course, paste() is vectorized, so you could just as well do paste(1, 2, 3, 4, 5), but again, I want to insist on the fact that writing functions, even if they look a bit complicated, can save you a huge amount of time in the long run.\nBecause I know that my function is quite general, I can be confident that it will work in a lot of different situations; as long as the a_func argument is a binary operator that combines the elements inside a_list, it’s going to work. And I don’t need to think about indexing, about having temporary variables or thinking about the structure that will hold my results."
  },
  {
    "objectID": "posts/2020-02-08-harold_part2.html",
    "href": "posts/2020-02-08-harold_part2.html",
    "title": "Dynamic discrete choice models, reinforcement learning and Harold, part 2",
    "section": "",
    "text": "In this blog post, I present a paper that has really interested me for a long time. This is part2, where I will briefly present the model of the paper, and try to play around with the data. If you haven’t, I suggest you read part 1 where I provide more context."
  },
  {
    "objectID": "posts/2020-02-08-harold_part2.html#rusts-model",
    "href": "posts/2020-02-08-harold_part2.html#rusts-model",
    "title": "Dynamic discrete choice models, reinforcement learning and Harold, part 2",
    "section": "\nRust’s model\n",
    "text": "Rust’s model\n\n\nWelcome to part 2 of this series, which might or might not have a part 3. I have been quite busy with this paper and especially with reinforcement learning these past couple of weeks, but in the meantime, other things have taken some of my time, so who knows if I’ll keep discussing this paper.\n\n\nBefore going into the data, let me describe the model very broadly. The problem is as follows: each month, Harold Zurcher must decide whether to simply perform some basic maintenance on the buses he’s responsible for, or he can decide to completely replace the engine. Let his utility function be as follows:\n\n\n[ u(x_t, i_t, _1) = {\n\\[\\begin{array}{lcl}\n-c(x_t, \\theta_1) &amp; \\text{if} &amp; i_t = 0, \\\\\n-[\\overline{P} - \\underline{P} + c(0, \\theta_1)] &amp; \\text{if} &amp; i_t = 1,\\\\\n\\end{array}\\]\n. ]\n\n\nwhere (x_t) is the state variable, the reading of the odometer at month (t), (i_t) is Harold Zurcher’s decision at time (t). (i_t = 0) is the decision to keep the engine, (i_t = 1) is the decision to replace. Each time the engine is replaced, the state variable (x_t) regenerates to 0. That is why John Rust, the paper’s author, calls the problem under study a regenerative optimal stopping model. If (i_t = 0) (keep the engine) is chosen, then the cost of normal maintenance is (c(x_t, _1)), if (i_t = 1) (change the engine) then the cost is (), which is the price of the new engine. However, it is still possible to sell the old engine for scrap value, (). The replacement cost is equal to (c(0, _1)). (_1) is a vector of parameters of the cost function to estimate. Because Harold Zurcher is forward looking, and does not want to simply maximize the current month’s utility, he seeks to maximize his intertemporal utility function. The optimal policy would be the solution to the following equation:\n\n\n[ V_{} = E{ _{j = t}{j-t}u(x_j, f_j, _1) | x_t} ]\n\n\nThis is a so-called value function, which is the total reward at the solution of the problem.\n\n\nThe state variable evolves according to a stochastic process given by the following transition probability:\n\n\n[ p(x_{t+1} | x_t, i_t, _2) = {\n\\[\\begin{array}{lllll}\n\\theta_2 \\exp\\{\\theta_2(x_{t+1} - x_t)\\} &amp; \\text{if} &amp; i_t = 0 &amp; \\text{and} &amp; x_{t+1} \\geq x_t \\\\\n\\theta_2 \\exp\\{\\theta_2(x_{t+1})\\} &amp; \\text{if} &amp; i_t = 0 &amp; \\text{and} &amp; x_{t+1} \\geq 0 \\\\\n0 &amp; \\text{otherwise}\\\\\n\\end{array}\\]\n. ]\n\n\n(_2) is the parameter of the exponential distribution, another parameter to estimate. I’ll stop with one more equation, the Bellman equation:\n\n\n[ V_(x_t) = _{i_t C(x_t)} [u(x_t, i_t, 1) + EV(x_t, i_t)] ]\n\n\nwhere (C(x_t) = {0, 1}) is the action set. The value function is the unique solution to this Bellman equation.\n\n\nAs you can see, this is quite complex (and I have not detailed everything!) but the advantage of models is that one can estimate its structural parameters and put a dollar value on the expected replacement cost, ( - ) in addition to validating the very first hypothesis of the paper; does Harold Zurcher behave optimally?\n\n\nIn what follows, I’ll use the {ReinforcementLearning} package to try to find the optimal policy rule. The optimal policy rule tells us what is the best action at each period. Reinforcement learning is an approach that is widely used in machine learning to solve problems very similar to the one that I described above. However, as we shall see, it will fail here, and there’s a very good reason for that. First, let’s load the data that was prepared last time:\n\nall_bus_data &lt;- read_csv(\"https://raw.githubusercontent.com/b-rodrigues/rust/ee15fb87fc4ba5db28d055c97a898b328725f53c/datasets/processed_data/all_buses.csv\")\n## Parsed with column specification:\n## cols(\n##   bus_id = col_double(),\n##   date = col_date(format = \"\"),\n##   odometer_reading = col_double(),\n##   replacement = col_double(),\n##   bus_family = col_character()\n## )\nhead(all_bus_data)\n## # A tibble: 6 x 5\n##   bus_id date       odometer_reading replacement bus_family\n##    &lt;dbl&gt; &lt;date&gt;                &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;     \n## 1   4239 1974-12-01           140953           0 a452372   \n## 2   4239 1975-01-01           142960           0 a452372   \n## 3   4239 1975-02-01           145380           0 a452372   \n## 4   4239 1975-03-01           148140           0 a452372   \n## 5   4239 1975-04-01           150921           0 a452372   \n## 6   4239 1975-05-01           153839           0 a452372\n\nIn the paper, the author groups the 4 following bus families, so I’ll be doing the same:\n\nfamily_group &lt;- c(\"g870\", \"rt50\", \"t8h203\", \"a530875\")\n\ngroup1_4 &lt;- all_bus_data %&gt;%\n  filter(bus_family %in% family_group)\n\nggplot(group1_4) + \n  geom_line(aes(y = odometer_reading, x = date, group = bus_id, col = bus_family)) + \n  geom_point(aes(y = ifelse(odometer_reading*replacement == 0, NA, odometer_reading*replacement), \n                 x = date), col = \"red\") +\n  labs(title = paste0(\"Odometer readings for bus families \", paste0(family_group, collapse = \", \")),\n       caption = \"The red dots are replacement events.\") + \n  theme(plot.caption = element_text(colour = \"white\")) +\n  brotools::theme_blog()\n## Warning: Removed 8200 rows containing missing values (geom_point).\n\n\n\n\nThere are 104 buses in this subset of data. Let’s discretize the odometer reading using the ntile() function. Discretizing the state variable will make computation faster:\n\ngroup1_4 &lt;- group1_4 %&gt;%  \n  mutate(state_at_replacement = ifelse(replacement == 1, odometer_reading, NA)) %&gt;%\n  group_by(bus_id) %&gt;%\n  fill(state_at_replacement, .direction = \"down\") %&gt;%\n  ungroup() %&gt;%  \n  mutate(state_at_replacement = odometer_reading - state_at_replacement) %&gt;%\n  mutate(state_at_replacement = ifelse(is.na(state_at_replacement), odometer_reading, state_at_replacement)) %&gt;%  \n  mutate(state = ntile(state_at_replacement, 50))\n\nLet me also save the bus ids in a vector, I’ll need it later:\n\nbuses &lt;- unique(group1_4$bus_id)\n\nTo use the dataset with the {ReinforcementLearning} package, it must first be prepared:\n\ngroup1_4 &lt;- group1_4 %&gt;%\n  group_by(bus_id) %&gt;%  \n  mutate(next_state = lead(state, 1)) %&gt;%\n  mutate(replacement = lead(replacement, 1)) %&gt;%  \n  mutate(action = replacement) %&gt;% \n  select(state, action, reward = replacement, next_state) %&gt;%\n  mutate(reward = (-1)*reward) %&gt;%\n  mutate(action = ifelse(is.na(action), 0, action),\n         reward = ifelse(is.na(reward), 0, reward)) %&gt;%  \n  mutate(next_state = ifelse(is.na(next_state), state + 1, next_state)) %&gt;% \n  mutate(state = as.character(state),\n         next_state = as.character(next_state),\n         action = as.character(action)) \n## Adding missing grouping variables: `bus_id`\n\nLet’s see how the data looks:\n\nhead(group1_4)\n## # A tibble: 6 x 5\n## # Groups:   bus_id [1]\n##   bus_id state action reward next_state\n##    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;     \n## 1   5297 2     0           0 3         \n## 2   5297 3     0           0 4         \n## 3   5297 4     0           0 5         \n## 4   5297 5     0           0 6         \n## 5   5297 6     0           0 8         \n## 6   5297 8     0           0 9\n\nSo when action 0 (do nothing) is chosen, the value of the state is increased by one. If action 1 (replace) is chosen:\n\ngroup1_4 %&gt;%\n  filter(action == \"1\") %&gt;%\n  head\n## # A tibble: 6 x 5\n## # Groups:   bus_id [6]\n##   bus_id state action reward next_state\n##    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;     \n## 1   5297 34    1          -1 1         \n## 2   5299 42    1          -1 1         \n## 3   5300 43    1          -1 1         \n## 4   5301 36    1          -1 1         \n## 5   5302 30    1          -1 1         \n## 6   5303 49    1          -1 1\n\nThe state goes back to 1, and the reward is -1.\n\n\nNow, let’s split the dataset into two: a training dataset and a testing dataset:\n\nset.seed(1234)\ntrain_buses &lt;- sample(buses, size = round(length(buses)*.8))\n\ntest_buses &lt;- setdiff(buses, train_buses)\n\nThere will be 83 in the training data and 21 in the testing data:\n\ntrain_data &lt;- group1_4 %&gt;%\n  filter(bus_id %in% train_buses)\n\ntest_data &lt;- group1_4 %&gt;%\n  filter(bus_id %in% test_buses)\n\nWe’re finally ready to use the {ReinforcementLearning} package.\n\nlibrary(ReinforcementLearning)\nmodel &lt;- ReinforcementLearning(train_data,\n                                         s = \"state\",\n                                         a = \"action\",\n                                         r = \"reward\",\n                                         s_new = \"next_state\")\n\nNow what’s the result?\n\nmodel\n## State-Action function Q\n##     0        1\n## X30 0 -0.19000\n## X31 0  0.00000\n## X1  0  0.00000\n## X32 0  0.00000\n## X2  0  0.00000\n## X33 0 -0.10000\n## X3  0  0.00000\n## X34 0 -0.19000\n## X4  0  0.00000\n## X35 0  0.00000\n## X5  0  0.00000\n## X36 0 -0.19000\n## X6  0  0.00000\n## X37 0 -0.10000\n## X7  0  0.00000\n## X38 0  0.00000\n## X8  0  0.00000\n## X39 0 -0.34390\n## X9  0  0.00000\n## X10 0  0.00000\n## X40 0 -0.10000\n## X11 0  0.00000\n## X41 0 -0.10000\n## X12 0  0.00000\n## X42 0 -0.34390\n## X13 0  0.00000\n## X43 0 -0.40951\n## X14 0  0.00000\n## X44 0 -0.19000\n## X45 0 -0.34390\n## X15 0  0.00000\n## X46 0 -0.27100\n## X16 0  0.00000\n## X47 0 -0.19000\n## X17 0  0.00000\n## X48 0 -0.40951\n## X18 0  0.00000\n## X49 0 -0.34390\n## X19 0  0.00000\n## X50 0 -0.34390\n## X20 0  0.00000\n## X21 0  0.00000\n## X22 0  0.00000\n## X23 0  0.00000\n## X24 0  0.00000\n## X25 0  0.00000\n## X26 0  0.00000\n## X27 0  0.00000\n## X28 0  0.00000\n## X29 0 -0.10000\n## \n## Policy\n## X30 X31  X1 X32  X2 X33  X3 X34  X4 X35  X5 X36  X6 X37  X7 X38  X8 X39  X9 X10 \n## \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \n## X40 X11 X41 X12 X42 X13 X43 X14 X44 X45 X15 X46 X16 X47 X17 X48 X18 X49 X19 X50 \n## \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \n## X20 X21 X22 X23 X24 X25 X26 X27 X28 X29 \n## \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \n## \n## Reward (last iteration)\n## [1] -48\n\nWe see that the optimal policy is always to do nothing! This is actually “normal” here, as we are using historical data; and in this data, there is no information on the likelihood of severe engine failure if one does not replace it completely at some point! So the agent does not see the point in replacing the engine; it costs money and does not seem to bring in any benefit!\n\n\nAnother way of using the {ReinforcementLearning} package is to write a function that simulates the environment. One could write such a function, and add in it a probability of severe failure with a very big cost. This probability would increase as the state (number of miles driven) increases as well. With such a function, there would be simulations where the cost of doing nothing would be very high, and as such, hopefully, the agent would learn that replacing the engine once might be a better course of action than doing nothing.\n\n\nThis might be the subject of part 3 of this series!"
  },
  {
    "objectID": "posts/2022-11-30-pipelines-as.html",
    "href": "posts/2022-11-30-pipelines-as.html",
    "title": "Functional programming explains why containerization is needed for reproducibility",
    "section": "",
    "text": "I’ve had some discussions online and in the real world about this blog post and I’d like to restate why containerization is needed for reproducibility, and do so from the lens of functional programming.\n\n\nWhen setting up a pipeline, wether you’re a functional programming enthusiast or not, you’re aiming at setting it up in a way that this pipeline is the composition of (potentially) many referentially transparent and pure functions.\n\n\nAs a reminder:\n\n\n\n\nreferentially transparent functions are functions that always return the same output for the same given input. So for example f(x, y):=x+y is referentially transparent, but h(x):=x+y is not. Because y is not an input of h, h will look for y in the global environment. Depending on the value of y, h(1) might equal 10 one day, but 100 the next. Let’s say that f(1, 10) is always equal to 11. Because this is true, you could replace f(1, 10) everywhere it appears with 11. But consider the following example of a function that is not referentially transparent, rnorm(). Try rnorm(1) several times… It will always give a different result! This is because rnorm() looks for the seed in the global environment and uses that to generate a random number.\n\n\n\n\npure functions are functions without side effects. So a function just does its thing, and does not interact with anything else; doesn’t change anything in the global environment, doesn’t print anything on screen, doesn’t write anything to disk. Basically, pure functions are functions that do nothing else but computing stuff. Now this may seem limiting, and to some extent it is, so we will need to relax this a bit: we’ll be ok with functions that output stuff, but only the very last function in the pipeline will be allowed to do it.\n\n\n\n\nTo be pure, a function needs to be referentially transparent.\n\n\nOk so now that we know what referentially transparent and pure functions are, let’s explain why we want a pipeline to be a composition of such functions. Function composition is an operation that takes two functions g and f and returns a new function h such that h(x) = g(f(x)). Formally:\n\nh = g ∘ f such that h(x) = g(f(x))\n\n∘ is the composition operator. You can read g ∘ f as g after f. In R, you can compose functions very easily, simply by using |&gt; or %&gt;%:\n\nh &lt;- f |&gt; g\n\nf |&gt; g can be read as f then g, which is equivalent to g after f (ok, using |&gt; is chaining rather than composing functions, but the net effect is the same).\n\n\nSo h would be our complete pipeline, which would be the composition, or chaining, of as many functions as needed:\n\nh &lt;- a |&gt; b |&gt; c |&gt; d ... |&gt; z\n\nIf all the functions are pure (and referentially transparent) then we’re assured that h will always produce the same outputs for the same inputs. As stated above, z will be allowed to not be pure an actually output something (like a rendered Quarto document) to disk. Ok so that’s great, and all, but why does the title of this blog post say that containerization is needed?\n\n\nThe problem is that all the functions we use have “hidden” inputs, and are never truly referentially transparent. These inputs are the following:\n\n\n\nVersion of R (or whatever programming language you’re using)\n\n\nVersions of the packages you’re using\n\n\nOperating system and its version (and all the different operating system dependencies that get used at run- or compile time)\n\n\n\nFor example, let’s take a look at this function:\n\nf &lt;- function(x){\n  if (c(TRUE, FALSE)) x \n}\n\nwhich will return the following on R 4.1 (which was released on May 2021):\n\nf(1)\n[1] 1\nWarning message:\nIn if (c(TRUE, FALSE)) 1 :\n  the condition has length &gt; 1 and only the first element will be used\n\nSo a result 1 and a warning. On R 4.2.2 (the current version as of writing), the exact same call returns:\n\nError in if (c(TRUE, FALSE)) 1 : the condition has length &gt; 1\n\nThese types of breaking changes are rare in R, at least to my knowledge (I’m actually looking into this in greater detail, 2023 will likely be the year I show my findings), but in this case it illustrates my point: code that was behaving in a certain way started behaving in another way, even though nothing changed. What changed was the version of R, even though the function itself was pure. This wouldn’t be so surprising if instead of f(x), the function was something like f(x, r_version). In this case, the calls above would be something like:\n\nf(1, r_version = \"4.1\")\n\nand this would always return:\n\n[1] 1\nWarning message:\nIn if (c(TRUE, FALSE)) 1 :\n  the condition has length &gt; 1 and only the first element will be used\n\nbut changing the call to this:\n\nf(1, r_version = \"4.2.2\")\n\nwould return the error:\n\nError in if (c(TRUE, FALSE)) 1 : the condition has length &gt; 1\n\nregardless of the version of R we’re running, so our function would be referentially transparent.\n\n\nAlas, this is not possible, at least not like this.\n\n\nHence why tools like Docker, Podman (a Docker alternative) or Guix (which I learned about recently but never used, yet, and as far as I know, not a containerization solution, but a solution actually based on functional programming) are crucial to ensure that your pipeline is truly reproducible. Basically, using Docker you turn the hidden inputs defined before (versions of tools and OS) explicit. Take a look at this Dockerfile:\n\nFROM rocker/r-ver:4.1.0\n\nRUN R -e \"f &lt;- function(x){if (c(TRUE, FALSE)) x};f(1)\"\n\nCMD [\"R\"]\n\nhere’s what happens when you build it:\n\n➤ docker build -t my_pipeline .\nSending build context to Docker daemon  2.048kB\nStep 1/3 : FROM rocker/r-ver:4.1.0\n4.1.0: Pulling from rocker/r-ver\n\neaead16dc43b: Already exists \n35eac095fa03: Pulling fs layer\nc0088a79f8ab: Pulling fs layer\n28e8d0ade0c0: Pulling fs layer\nDigest: sha256:860c56970de1d37e9c376ca390617d50a127b58c56fbb807152c2e976ce02002\nStatus: Downloaded newer image for rocker/r-ver:4.1.0\n ---&gt; d83268fb6cda\nStep 2/3 : RUN R -e \"f &lt;- function(x){if (c(TRUE, FALSE)) x};f(1)\"\n ---&gt; Running in a158e4ab474f\n\nR version 4.1.0 (2021-05-18) -- \"Camp Pontanezen\"\nCopyright (C) 2021 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt; f &lt;- function(x){if (c(TRUE, FALSE)) x};f(1)\n[1] 1\nWarning message:\nIn if (c(TRUE, FALSE)) x :&gt; \n&gt; \n\n  the condition has length &gt; 1 and only the first element will be used\nRemoving intermediate container a158e4ab474f\n ---&gt; 49e2eb20a535\nStep 3/3 : CMD [\"R\"]\n ---&gt; Running in ccda657c4d95\nRemoving intermediate container ccda657c4d95\n ---&gt; 5a432adbe6ff\nSuccessfully built 5a432adbe6ff\nSuccessfully tagged my_package:latest\n\nas you can read from above, this starts the container with R version 4.1.0 and runs the code in it. We get back our result with the warning (it should be noted that in practice, you would structure your Dockerfile differently for running an actual pipeline).\n\n\nThis Dockerfile starts by using rocker/r-ver:4.1 as a basis. You can find this image in the versioned repository from the Rocker Project. This base image starts off from Ubuntu Focal Fossa so (Ubuntu version 20.04), uses R version 4.1.0 and even uses frozen CRAN repository as of 2021-08-09. It then runs our pipeline (or in this case, our simple function) in this, fixed environment. Our function essentially became f(x, os_version, r_version, packages_version) instead of just f(x). By changing the first statement of the Dockerfile:\n\nFROM rocker/r-ver:4.1.0\n\nto this:\n\nFROM rocker/r-ver:3.5.0\n\nwe can even do some archaeology and run the pipeline on R version 3.5.0! This has great potential and hopefully one day Docker or similar solution will become just another tool in scientists/analysts toolbox.\n\n\nIf you want to start using Docker for your projects, I’ve written this tutorial and even a whole ebook."
  },
  {
    "objectID": "posts/2018-11-14-luxairport.html",
    "href": "posts/2018-11-14-luxairport.html",
    "title": "Easy time-series prediction with R: a tutorial with air traffic data from Lux Airport",
    "section": "",
    "text": "In this blog post, I will show you how you can quickly and easily forecast a univariate time series. I am going to use data from the EU Open Data Portal on air passenger transport. You can find the data here. I downloaded the data in the TSV format for Luxembourg Airport, but you could repeat the analysis for any airport.\n\n\nOnce you have the data, load some of the package we are going to need:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(forecast)\nlibrary(tsibble)\nlibrary(brotools)\n\nand define the following function:\n\nihs &lt;- function(x){\n    log(x + sqrt(x**2 + 1))\n}\n\nThis function, the inverse hyperbolic sine, is useful to transform data in a manner that is very close to logging it, but that allows for 0’s. The data from Eurostat is not complete for some reason, so there are some 0 sometimes. To avoid having to log 0, which in R yields -Inf, I use this transformation.\n\n\nNow, let’s load the data:\n\navia &lt;- read_tsv(\"avia_par_lu.tsv\")\n## Parsed with column specification:\n## cols(\n##   .default = col_character()\n## )\n## See spec(...) for full column specifications.\n\nLet’s take a look at the data:\n\nhead(avia)\n## # A tibble: 6 x 238\n##   `unit,tra_meas,… `2018Q1` `2018M03` `2018M02` `2018M01` `2017Q4` `2017Q3`\n##   &lt;chr&gt;            &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;   \n## 1 FLIGHT,CAF_PAS,… 511      172       161       178       502      475     \n## 2 FLIGHT,CAF_PAS,… :        :         :         :         :        :       \n## 3 FLIGHT,CAF_PAS,… :        :         :         :         399      306     \n## 4 FLIGHT,CAF_PAS,… 485      167       151       167       493      497     \n## 5 FLIGHT,CAF_PAS,… 834      293       267       274       790      728     \n## 6 FLIGHT,CAF_PAS,… :        :         :         :         :        :       \n## # … with 231 more variables: `2017Q2` &lt;chr&gt;, `2017Q1` &lt;chr&gt;,\n## #   `2017M12` &lt;chr&gt;, `2017M11` &lt;chr&gt;, `2017M10` &lt;chr&gt;, `2017M09` &lt;chr&gt;,\n## #   `2017M08` &lt;chr&gt;, `2017M07` &lt;chr&gt;, `2017M06` &lt;chr&gt;, `2017M05` &lt;chr&gt;,\n## #   `2017M04` &lt;chr&gt;, `2017M03` &lt;chr&gt;, `2017M02` &lt;chr&gt;, `2017M01` &lt;chr&gt;,\n## #   `2017` &lt;chr&gt;, `2016Q4` &lt;chr&gt;, `2016Q3` &lt;chr&gt;, `2016Q2` &lt;chr&gt;,\n## #   `2016Q1` &lt;chr&gt;, `2016M12` &lt;chr&gt;, `2016M11` &lt;chr&gt;, `2016M10` &lt;chr&gt;,\n## #   `2016M09` &lt;chr&gt;, `2016M08` &lt;chr&gt;, `2016M07` &lt;chr&gt;, `2016M06` &lt;chr&gt;,\n## #   `2016M05` &lt;chr&gt;, `2016M04` &lt;chr&gt;, `2016M03` &lt;chr&gt;, `2016M02` &lt;chr&gt;,\n## #   `2016M01` &lt;chr&gt;, `2016` &lt;chr&gt;, `2015Q4` &lt;chr&gt;, `2015Q3` &lt;chr&gt;,\n## #   `2015Q2` &lt;chr&gt;, `2015Q1` &lt;chr&gt;, `2015M12` &lt;chr&gt;, `2015M11` &lt;chr&gt;,\n## #   `2015M10` &lt;chr&gt;, `2015M09` &lt;chr&gt;, `2015M08` &lt;chr&gt;, `2015M07` &lt;chr&gt;,\n## #   `2015M06` &lt;chr&gt;, `2015M05` &lt;chr&gt;, `2015M04` &lt;chr&gt;, `2015M03` &lt;chr&gt;,\n## #   `2015M02` &lt;chr&gt;, `2015M01` &lt;chr&gt;, `2015` &lt;chr&gt;, `2014Q4` &lt;chr&gt;,\n## #   `2014Q3` &lt;chr&gt;, `2014Q2` &lt;chr&gt;, `2014Q1` &lt;chr&gt;, `2014M12` &lt;chr&gt;,\n## #   `2014M11` &lt;chr&gt;, `2014M10` &lt;chr&gt;, `2014M09` &lt;chr&gt;, `2014M08` &lt;chr&gt;,\n## #   `2014M07` &lt;chr&gt;, `2014M06` &lt;chr&gt;, `2014M05` &lt;chr&gt;, `2014M04` &lt;chr&gt;,\n## #   `2014M03` &lt;chr&gt;, `2014M02` &lt;chr&gt;, `2014M01` &lt;chr&gt;, `2014` &lt;chr&gt;,\n## #   `2013Q4` &lt;chr&gt;, `2013Q3` &lt;chr&gt;, `2013Q2` &lt;chr&gt;, `2013Q1` &lt;chr&gt;,\n## #   `2013M12` &lt;chr&gt;, `2013M11` &lt;chr&gt;, `2013M10` &lt;chr&gt;, `2013M09` &lt;chr&gt;,\n## #   `2013M08` &lt;chr&gt;, `2013M07` &lt;chr&gt;, `2013M06` &lt;chr&gt;, `2013M05` &lt;chr&gt;,\n## #   `2013M04` &lt;chr&gt;, `2013M03` &lt;chr&gt;, `2013M02` &lt;chr&gt;, `2013M01` &lt;chr&gt;,\n## #   `2013` &lt;chr&gt;, `2012Q4` &lt;chr&gt;, `2012Q3` &lt;chr&gt;, `2012Q2` &lt;chr&gt;,\n## #   `2012Q1` &lt;chr&gt;, `2012M12` &lt;chr&gt;, `2012M11` &lt;chr&gt;, `2012M10` &lt;chr&gt;,\n## #   `2012M09` &lt;chr&gt;, `2012M08` &lt;chr&gt;, `2012M07` &lt;chr&gt;, `2012M06` &lt;chr&gt;,\n## #   `2012M05` &lt;chr&gt;, `2012M04` &lt;chr&gt;, `2012M03` &lt;chr&gt;, `2012M02` &lt;chr&gt;,\n## #   `2012M01` &lt;chr&gt;, `2012` &lt;chr&gt;, …\n\nSo yeah, useless in that state. The first column actually is composed of 3 columns, merged together, and instead of having one column with the date, and another with the value, we have one column per date. Some cleaning is necessary before using this data.\n\n\nLet’s start with going from a wide to a long data set:\n\navia %&gt;%\n    select(\"unit,tra_meas,airp_pr\\\\time\", contains(\"20\")) %&gt;%\n    gather(date, passengers, -`unit,tra_meas,airp_pr\\\\time`)\n\nThe first line makes it possible to only select the columns that contain the string “20”, so selecting columns from 2000 onward. Then, using gather, I go from long to wide. The data looks like this now:\n\n## # A tibble: 117,070 x 3\n##    `unit,tra_meas,airp_pr\\\\time`  date   passengers\n##    &lt;chr&gt;                          &lt;chr&gt;  &lt;chr&gt;     \n##  1 FLIGHT,CAF_PAS,LU_ELLX_AT_LOWW 2018Q1 511       \n##  2 FLIGHT,CAF_PAS,LU_ELLX_BE_EBBR 2018Q1 :         \n##  3 FLIGHT,CAF_PAS,LU_ELLX_CH_LSGG 2018Q1 :         \n##  4 FLIGHT,CAF_PAS,LU_ELLX_CH_LSZH 2018Q1 485       \n##  5 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDF 2018Q1 834       \n##  6 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDI 2018Q1 :         \n##  7 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDM 2018Q1 1095      \n##  8 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDR 2018Q1 :         \n##  9 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDT 2018Q1 :         \n## 10 FLIGHT,CAF_PAS,LU_ELLX_DK_EKCH 2018Q1 :         \n## # … with 117,060 more rows\n\nNow, let’s separate the first column into 3 columns:\n\navia %&gt;%\n    select(\"unit,tra_meas,airp_pr\\\\time\", contains(\"20\")) %&gt;%\n    gather(date, passengers, -`unit,tra_meas,airp_pr\\\\time`) %&gt;%\n     separate(col = `unit,tra_meas,airp_pr\\\\time`, into = c(\"unit\", \"tra_meas\", \"air_pr\\\\time\"), sep = \",\")\n\nThis separates the first column into 3 new columns, “unit”, “tra_meas” and “air_pr”. This step is not necessary for the rest of the analysis, but might as well do it. The data looks like this now:\n\n## # A tibble: 117,070 x 5\n##    unit   tra_meas `air_pr\\\\time`  date   passengers\n##    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt;     \n##  1 FLIGHT CAF_PAS  LU_ELLX_AT_LOWW 2018Q1 511       \n##  2 FLIGHT CAF_PAS  LU_ELLX_BE_EBBR 2018Q1 :         \n##  3 FLIGHT CAF_PAS  LU_ELLX_CH_LSGG 2018Q1 :         \n##  4 FLIGHT CAF_PAS  LU_ELLX_CH_LSZH 2018Q1 485       \n##  5 FLIGHT CAF_PAS  LU_ELLX_DE_EDDF 2018Q1 834       \n##  6 FLIGHT CAF_PAS  LU_ELLX_DE_EDDI 2018Q1 :         \n##  7 FLIGHT CAF_PAS  LU_ELLX_DE_EDDM 2018Q1 1095      \n##  8 FLIGHT CAF_PAS  LU_ELLX_DE_EDDR 2018Q1 :         \n##  9 FLIGHT CAF_PAS  LU_ELLX_DE_EDDT 2018Q1 :         \n## 10 FLIGHT CAF_PAS  LU_ELLX_DK_EKCH 2018Q1 :         \n## # … with 117,060 more rows\n\nThe next steps are simple renamings. I have copy-pasted the information from the Eurostat page where you can view the data online. If you click here:\n\n\n\n\n\nyou will be able to select the variables you want displayed in the table, as well as the dictionary of the variables. I simply copy pasted it and recoded the variables. You can take a look at the whole cleaning workflow by clicking “Click to expand” below:\n\n\n\n\nClick here to take a look at the whole cleaning workflow\n\n\navia_clean &lt;- avia %&gt;%\n    select(\"unit,tra_meas,airp_pr\\\\time\", contains(\"20\")) %&gt;%\n    gather(date, passengers, -`unit,tra_meas,airp_pr\\\\time`) %&gt;%\n    separate(col = `unit,tra_meas,airp_pr\\\\time`, into = c(\"unit\", \"tra_meas\", \"air_pr\\\\time\"), sep = \",\") %&gt;%\n    mutate(tra_meas = fct_recode(tra_meas,\n         `Passengers on board` = \"PAS_BRD\",\n         `Passengers on board (arrivals)` = \"PAS_BRD_ARR\",\n         `Passengers on board (departures)` = \"PAS_BRD_DEP\",\n         `Passengers carried` = \"PAS_CRD\",\n         `Passengers carried (arrival)` = \"PAS_CRD_ARR\",\n         `Passengers carried (departures)` = \"PAS_CRD_DEP\",\n         `Passengers seats available` = \"ST_PAS\",\n         `Passengers seats available (arrivals)` = \"ST_PAS_ARR\",\n         `Passengers seats available (departures)` = \"ST_PAS_DEP\",\n         `Commercial passenger air flights` = \"CAF_PAS\",\n         `Commercial passenger air flights (arrivals)` = \"CAF_PAS_ARR\",\n         `Commercial passenger air flights (departures)` = \"CAF_PAS_DEP\")) %&gt;%\n    mutate(unit = fct_recode(unit,\n                             Passenger = \"PAS\",\n                             Flight = \"FLIGHT\",\n                             `Seats and berths` = \"SEAT\")) %&gt;%\n    mutate(destination = fct_recode(`air_pr\\\\time`,\n                                     `WIEN-SCHWECHAT` = \"LU_ELLX_AT_LOWW\",\n                                     `BRUSSELS` = \"LU_ELLX_BE_EBBR\",\n                                     `GENEVA` = \"LU_ELLX_CH_LSGG\",\n                                     `ZURICH` = \"LU_ELLX_CH_LSZH\",\n                                     `FRANKFURT/MAIN` = \"LU_ELLX_DE_EDDF\",\n                                     `HAMBURG` = \"LU_ELLX_DE_EDDH\",\n                                     `BERLIN-TEMPELHOF` = \"LU_ELLX_DE_EDDI\",\n                                     `MUENCHEN` = \"LU_ELLX_DE_EDDM\",\n                                     `SAARBRUECKEN` = \"LU_ELLX_DE_EDDR\",\n                                     `BERLIN-TEGEL` = \"LU_ELLX_DE_EDDT\",\n                                     `KOBENHAVN/KASTRUP` = \"LU_ELLX_DK_EKCH\",\n                                     `HURGHADA / INTL` = \"LU_ELLX_EG_HEGN\",\n                                     `IRAKLION/NIKOS KAZANTZAKIS` = \"LU_ELLX_EL_LGIR\",\n                                     `FUERTEVENTURA` = \"LU_ELLX_ES_GCFV\",\n                                     `GRAN CANARIA` = \"LU_ELLX_ES_GCLP\",\n                                     `LANZAROTE` = \"LU_ELLX_ES_GCRR\",\n                                     `TENERIFE SUR/REINA SOFIA` = \"LU_ELLX_ES_GCTS\",\n                                     `BARCELONA/EL PRAT` = \"LU_ELLX_ES_LEBL\",\n                                     `ADOLFO SUAREZ MADRID-BARAJAS` = \"LU_ELLX_ES_LEMD\",\n                                     `MALAGA/COSTA DEL SOL` = \"LU_ELLX_ES_LEMG\",\n                                     `PALMA DE MALLORCA` = \"LU_ELLX_ES_LEPA\",\n                                     `SYSTEM - PARIS` = \"LU_ELLX_FR_LF90\",\n                                     `NICE-COTE D'AZUR` = \"LU_ELLX_FR_LFMN\",\n                                     `PARIS-CHARLES DE GAULLE` = \"LU_ELLX_FR_LFPG\",\n                                     `STRASBOURG-ENTZHEIM` = \"LU_ELLX_FR_LFST\",\n                                     `KEFLAVIK` = \"LU_ELLX_IS_BIKF\",\n                                     `MILANO/MALPENSA` = \"LU_ELLX_IT_LIMC\",\n                                     `BERGAMO/ORIO AL SERIO` = \"LU_ELLX_IT_LIME\",\n                                     `ROMA/FIUMICINO` = \"LU_ELLX_IT_LIRF\",\n                                     `AGADIR/AL MASSIRA` = \"LU_ELLX_MA_GMAD\",\n                                     `AMSTERDAM/SCHIPHOL` = \"LU_ELLX_NL_EHAM\",\n                                     `WARSZAWA/CHOPINA` = \"LU_ELLX_PL_EPWA\",\n                                     `PORTO` = \"LU_ELLX_PT_LPPR\",\n                                     `LISBOA` = \"LU_ELLX_PT_LPPT\",\n                                     `STOCKHOLM/ARLANDA` = \"LU_ELLX_SE_ESSA\",\n                                     `MONASTIR/HABIB BOURGUIBA` = \"LU_ELLX_TN_DTMB\",\n                                     `ENFIDHA-HAMMAMET INTERNATIONAL` = \"LU_ELLX_TN_DTNH\",\n                                     `ENFIDHA ZINE EL ABIDINE BEN ALI` = \"LU_ELLX_TN_DTNZ\",\n                                     `DJERBA/ZARZIS` = \"LU_ELLX_TN_DTTJ\",\n                                     `ANTALYA (MIL-CIV)` = \"LU_ELLX_TR_LTAI\",\n                                     `ISTANBUL/ATATURK` = \"LU_ELLX_TR_LTBA\",\n                                     `SYSTEM - LONDON` = \"LU_ELLX_UK_EG90\",\n                                     `MANCHESTER` = \"LU_ELLX_UK_EGCC\",\n                                     `LONDON GATWICK` = \"LU_ELLX_UK_EGKK\",\n                                     `LONDON/CITY` = \"LU_ELLX_UK_EGLC\",\n                                     `LONDON HEATHROW` = \"LU_ELLX_UK_EGLL\",\n                                     `LONDON STANSTED` = \"LU_ELLX_UK_EGSS\",\n                                     `NEWARK LIBERTY INTERNATIONAL, NJ.` = \"LU_ELLX_US_KEWR\",\n                                     `O.R TAMBO INTERNATIONAL` = \"LU_ELLX_ZA_FAJS\")) %&gt;%\n    mutate(passengers = as.numeric(passengers)) %&gt;%\n    select(unit, tra_meas, destination, date, passengers)\n## Warning: NAs introduced by coercion\n\n\nThere is quarterly data and monthly data. Let’s separate the two:\n\navia_clean_quarterly &lt;- avia_clean %&gt;%\n    filter(tra_meas == \"Passengers on board (arrivals)\",\n           !is.na(passengers)) %&gt;%\n    filter(str_detect(date, \"Q\")) %&gt;%\n    mutate(date = yq(date))\n\nIn the “date” column, I detect the observations with “Q” in their name, indicating that it is quarterly data. I do the same for monthly data, but I have to add the string “01” to the dates. This transforms a date that looks like this “2018M1” to this “2018M101”. “2018M101” can then be converted into a date by using the ymd() function from lubridate. yq() was used for the quarterly data.\n\navia_clean_monthly &lt;- avia_clean %&gt;%\n    filter(tra_meas == \"Passengers on board (arrivals)\",\n           !is.na(passengers)) %&gt;%\n    filter(str_detect(date, \"M\")) %&gt;%\n    mutate(date = paste0(date, \"01\")) %&gt;%\n    mutate(date = ymd(date)) %&gt;%\n    select(destination, date, passengers)\n\nTime for some plots. Let’s start with the raw data:\n\navia_clean_monthly %&gt;%\n    group_by(date) %&gt;%\n    summarise(total = sum(passengers)) %&gt;%\n    ggplot() +\n    ggtitle(\"Raw data\") +\n    geom_line(aes(y = total, x = date), colour = \"#82518c\") +\n    scale_x_date(date_breaks = \"1 year\", date_labels = \"%m-%Y\") + \n    theme_blog()\n\n\n\n\nAnd now with the logged data (or rather, the data transformed using the inverted hyperbolic sine transformation):\n\navia_clean_monthly %&gt;%\n    group_by(date) %&gt;%\n    summarise(total = sum(passengers)) %&gt;%\n    mutate(total_ihs = ihs(total)) %&gt;%\n    ggplot() +\n    ggtitle(\"Logged data\") +\n    geom_line(aes(y = total_ihs, x = date), colour = \"#82518c\") +\n    scale_x_date(date_breaks = \"1 year\", date_labels = \"%m-%Y\") + \n    theme_blog()\n\n\n\n\nWe clearly see a seasonal pattern in the data. There is also an upward trend. We will have to deal with these two problems if we want to do some forecasting. For this, let’s limit ourselves to data from before 2015, and convert the “passengers” column from the data to a time series object, using the ts() function:\n\navia_clean_train &lt;- avia_clean_monthly %&gt;%\n    select(date, passengers) %&gt;%\n    filter(year(date) &lt; 2015) %&gt;%\n    group_by(date) %&gt;%\n    summarise(total_passengers = sum(passengers)) %&gt;%\n    pull(total_passengers) %&gt;%\n    ts(., frequency = 12, start = c(2005, 1))\n\nWe will try to pseudo-forecast the data from 2015 to the last point available, March 2018. First, let’s tranform the data:\n\nlogged_data &lt;- ihs(avia_clean_train)\n\nTaking the log, or ihs of the data deals with stabilizing the variance of the time series.\n\n\nThere might also be a need to difference the data. Computing the differences between consecutive observations makes the time-series stationary. This will be taken care of by the auto.arima() function, if needed. The auto.arima() function returns the best ARIMA model according to different statistical criterions, such as the AIC, AICc or BIC.\n\n(model_fit &lt;- auto.arima(logged_data))\n## Series: logged_data \n## ARIMA(2,1,1)(2,1,0)[12] \n## \n## Coefficients:\n##           ar1      ar2      ma1     sar1     sar2\n##       -0.4061  -0.2431  -0.3562  -0.5590  -0.3282\n## s.e.   0.2003   0.1432   0.1994   0.0911   0.0871\n## \n## sigma^2 estimated as 0.004503:  log likelihood=137.11\n## AIC=-262.21   AICc=-261.37   BIC=-246.17\n\nauto.arima() found that the best model would be an (ARIMA(2, 1, 1)(2, 1, 0)_{12}). This is an seasonal autoregressive model, with p = 2, d = 1, q = 1, P = 2 and D = 1.\n\nmodel_forecast &lt;- forecast(model_fit, h = 39)\n\nI can now forecast the model for the next 39 months (which correspond to the data available).\n\n\nTo plot the forecast, one could do a simple call to the plot function. But the resulting plot is not very aesthetic. To plot my own, I have to grab the data that was forecast, and do some munging again:\n\npoint_estimate &lt;- model_forecast$mean %&gt;%\n    as_tsibble() %&gt;%\n    rename(point_estimate = value,\n           date = index)\n\nupper &lt;- model_forecast$upper %&gt;%\n    as_tsibble() %&gt;%\n    spread(key, value) %&gt;%\n    rename(date = index,\n           upper80 = `80%`,\n           upper95 = `95%`)\n\nlower &lt;- model_forecast$lower %&gt;%\n    as_tsibble() %&gt;%\n    spread(key, value) %&gt;%\n    rename(date = index,\n           lower80 = `80%`,\n           lower95 = `95%`)\n\nestimated_data &lt;- reduce(list(point_estimate, upper, lower), full_join, by = \"date\")\n\nas_tsibble() is a function from the {tsibble} package that converts objects that are time-series aware to time-aware tibbles. If you are not familiar with ts_tibble(), I urge you to run the above lines one by one, and especially to compare as_tsibble() with the standard as_tibble() from the {tibble} package.\n\n\nThis is how estimated_data looks:\n\nhead(estimated_data)\n## # A tsibble: 6 x 6 [1M]\n##       date point_estimate upper80 upper95 lower80 lower95\n##      &lt;mth&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n## 1 2015 Jan           11.9    12.0    12.1    11.8    11.8\n## 2 2015 Feb           11.9    12.0    12.0    11.8    11.7\n## 3 2015 Mar           12.1    12.2    12.3    12.0    12.0\n## 4 2015 Apr           12.2    12.3    12.4    12.1    12.1\n## 5 2015 May           12.3    12.4    12.4    12.2    12.1\n## 6 2015 Jun           12.3    12.4    12.5    12.2    12.1\n\nWe can now plot the data, with the forecast, and with the 95% confidence interval:\n\navia_clean_monthly %&gt;%\n    group_by(date) %&gt;%\n    summarise(total = sum(passengers)) %&gt;%\n    mutate(total_ihs = ihs(total)) %&gt;%\n    ggplot() +\n    ggtitle(\"Logged data\") +\n    geom_line(aes(y = total_ihs, x = date), colour = \"#82518c\") +\n    scale_x_date(date_breaks = \"1 year\", date_labels = \"%m-%Y\") +\n    geom_ribbon(data = estimated_data, aes(x = date, ymin = lower95, ymax = upper95), fill = \"#666018\", alpha = 0.2) +\n    geom_line(data = estimated_data, aes(x = date, y = point_estimate), linetype = 2, colour = \"#8e9d98\") +\n    theme_blog()\n\n\n\n\nThe pseudo-forecast (the dashed line) is not very far from the truth, only overestimating the seasonal peaks, but the true line is within the 95% confidence interval, which is good!"
  },
  {
    "objectID": "posts/2018-11-10-nethack_analysis_part2.html",
    "href": "posts/2018-11-10-nethack_analysis_part2.html",
    "title": "Analyzing NetHack data, part 2: What players kill the most",
    "section": "",
    "text": "Link to webscraping the data\nLink to Analysis, part 1"
  },
  {
    "objectID": "posts/2018-11-10-nethack_analysis_part2.html#introduction",
    "href": "posts/2018-11-10-nethack_analysis_part2.html#introduction",
    "title": "Analyzing NetHack data, part 2: What players kill the most",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nThis is the third blog post that deals with data from the game NetHack, and oh boy, did a lot of things happen since the last blog post! Here’s a short timeline of the events:\n\n\n\nI scraped data from alt.org/nethack and made a package with the data available on Github (that package was too big for CRAN)\n\n\nThen, I analyzed the data, focusing on what monsters kill the players the most, and also where players die the most\n\n\n@GridSageGames, developer of the roguelike Cogmind and moderator of the roguelike subreddit, posted the blog post on reddit\n\n\nI noticed that actually, by scraping the data like I did, I only got a sample of 100 daily games\n\n\nThis point was also discussed on Reddit, and bhhak, an UnNetHack developer (UnNetHack is a fork of NetHack) suggested I used the xlogfiles instead\n\n\nxlogfiles are log files generated by NetHack, and are also available on alt.org/nethack\n\n\nI started scraping them, and getting a lot more data\n\n\nI got contacted on twitter by @paxed, an admin of alt.org/nethack:\n\n\n{{% tweet “1059333642592366593” %}}\n\n\nHe gave me access to ALL THE DATA on alt.org/nethack!\n\n\nThe admins of alt.org/nethack will release all the data to the public!\n\n\n\nSo, I will now continue with the blog post I wanted to do in the first place; focusing now on what roles players choose to play the most, and also which monsters they kill the most. BUT! Since all the data will be released to the public, my {nethack} package that contains data that I scraped is not that useful anymore. So I changed the nature of the package. Now the package contains some functions: a function to parse and prepare the xlogfiles from NetHack that you can download from alt.org/nethack (or from any other public server), a function to download dumplogs such as this one. These dumplogs contain a lot of info that I will extract in this blog post, using another function included in the {nethack} package. The package also contains a sample of 6000 runs from NetHack version 3.6.1.\n\n\nYou can install the package with the following command line:\n\ndevtools::install_github(\"b-rodrigues/nethack\")"
  },
  {
    "objectID": "posts/2018-11-10-nethack_analysis_part2.html#the-nethack-package",
    "href": "posts/2018-11-10-nethack_analysis_part2.html#the-nethack-package",
    "title": "Analyzing NetHack data, part 2: What players kill the most",
    "section": "\nThe {nethack} package\n",
    "text": "The {nethack} package\n\n\nIn part 1 I showed what killed players the most. Here, I will focus on what monsters players kill the most. Let’s start by loading some packages:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(magrittr)\nlibrary(ggridges)\nlibrary(brotools)\nlibrary(rvest)\nlibrary(nethack)\n\nLet’s first describe the data:\n\nbrotools::describe(nethack) %&gt;% \n  print(n = Inf)\n## # A tibble: 23 x 17\n##    variable type    nobs     mean       sd mode       min     max      q05\n##    &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n##  1 deathdn… Nume… 6.00e3  8.45e-1  1.30e+0 2       0.      7.00e0   0.    \n##  2 deathlev Nume… 6.00e3  4.32e+0  3.69e+0 10     -5.00e0  4.50e1   1.00e0\n##  3 deaths   Nume… 6.00e3  8.88e-1  3.54e-1 1       0.      5.00e0   0.    \n##  4 endtime  Nume… 6.00e3  1.53e+9  4.72e+6 1534…   1.52e9  1.54e9   1.53e9\n##  5 hp       Nume… 6.00e3  6.64e+0  4.96e+1 -1     -9.40e1  1.79e3  -8.00e0\n##  6 maxhp    Nume… 6.00e3  3.82e+1  5.29e+1 57      2.00e0  1.80e3   1.10e1\n##  7 maxlvl   Nume… 6.00e3  5.52e+0  6.36e+0 10      1.00e0  5.30e1   1.00e0\n##  8 points   Nume… 6.00e3  4.69e+4  4.18e+5 10523   0.      9.92e6   1.40e1\n##  9 realtime Nume… 6.00e3  4.42e+3  1.60e+4 4575    0.      3.23e5   6.90e1\n## 10 startti… Nume… 6.00e3  1.53e+9  4.72e+6 1534…   1.52e9  1.54e9   1.53e9\n## 11 turns    Nume… 6.00e3  3.60e+3  9.12e+3 6797    3.10e1  1.97e5   9.49e1\n## 12 align    Char… 6.00e3 NA       NA       Cha    NA      NA       NA     \n## 13 align0   Char… 6.00e3 NA       NA       Cha    NA      NA       NA     \n## 14 death    Char… 6.00e3 NA       NA       kill…  NA      NA       NA     \n## 15 gender   Char… 6.00e3 NA       NA       Fem    NA      NA       NA     \n## 16 gender0  Char… 6.00e3 NA       NA       Fem    NA      NA       NA     \n## 17 killed_… Char… 6.00e3 NA       NA       fain…  NA      NA       NA     \n## 18 name     Char… 6.00e3 NA       NA       drud…  NA      NA       NA     \n## 19 race     Char… 6.00e3 NA       NA       Elf    NA      NA       NA     \n## 20 role     Char… 6.00e3 NA       NA       Wiz    NA      NA       NA     \n## 21 dumplog  List  1.33e6 NA       NA       &lt;NA&gt;   NA      NA       NA     \n## 22 birthda… Date  6.00e3 NA       NA       &lt;NA&gt;   NA      NA       NA     \n## 23 deathda… Date  6.00e3 NA       NA       &lt;NA&gt;   NA      NA       NA     \n## # ... with 8 more variables: q25 &lt;dbl&gt;, median &lt;dbl&gt;, q75 &lt;dbl&gt;,\n## #   q95 &lt;dbl&gt;, n_missing &lt;int&gt;, n_unique &lt;int&gt;, starting_date &lt;date&gt;,\n## #   ending_date &lt;date&gt;\n\nAll these columns are included in xlogfiles. The data was prepared using two functions, included in {nethack}:\n\nxlog &lt;- read_delim(\"~/path/to/nethack361_xlog.csv\", \"\\t\", escape_double = FALSE, \n                   col_names = FALSE, trim_ws = TRUE)\n\nxlog_df &lt;- clean_xlog(xlog)\n\nnethack361_xlog.csv is the raw xlogfiles that you can get from NetHack public servers. clean_xlog() is a function that parses an xlogfile and returns a clean data frame. xlog_df will be a data frame that will look just as the one included in {nethack}. It is then possible to get the dumplog from each run included in xlog_df using get_dumplog():\n\nxlog_df &lt;- get_dumplog(xlog_df)\n\nThis function adds a column called dumplog with the dumplog of that run. I will now analyze the dumplog file, by focusing on monsters vanquished, genocided or extinct. In a future blogpost I will focus on other achievements."
  },
  {
    "objectID": "posts/2018-11-10-nethack_analysis_part2.html#roles-played-and-other-starting-stats",
    "href": "posts/2018-11-10-nethack_analysis_part2.html#roles-played-and-other-starting-stats",
    "title": "Analyzing NetHack data, part 2: What players kill the most",
    "section": "\nRoles played (and other starting stats)\n",
    "text": "Roles played (and other starting stats)\n\n\nI will take a look at the races, roles, gender and alignment players start with the most. I will do pie charts to visualize these variable, so first, let’s start by writing a general function that allows me to do just that:\n\ncreate_pie &lt;- function(dataset, variable, repel = FALSE){\n\n  if(repel){\n    geom_label &lt;- function(...){\n      ggrepel::geom_label_repel(...)\n    }\n  }\n\n  variable &lt;- enquo(variable)\n\n  dataset %&gt;%\n    count((!!variable)) %&gt;%\n    mutate(total = sum(n),\n           freq = n/total,\n           labels = scales::percent(freq)) %&gt;% \n    arrange(desc(freq)) %&gt;%\n    ggplot(aes(x = \"\", y = freq, fill = (!!variable))) + \n    geom_col() + \n    geom_label(aes(label = labels), position = position_stack(vjust = 0.25), show.legend = FALSE) + \n    coord_polar(\"y\") + \n    theme_blog() + \n    scale_fill_blog() + \n    theme(legend.title = element_blank(),\n          panel.grid = element_blank(),\n          axis.text = element_blank(),\n          axis.title = element_blank())\n}\n\nNow I can easily plot the share of races chosen:\n\ncreate_pie(nethack, race)\n\n\n\n\nor the share of alignment:\n\ncreate_pie(nethack, align0)\n\n\n\n\nSame for the share of gender:\n\ncreate_pie(nethack, gender0)\n\n\n\n\nand finally for the share of roles:\n\ncreate_pie(nethack, role, repel = TRUE) \n\n\n\n\ncreate_pie() is possible thanks to tidy evaluation in {ggplot2}, which makes it possible to write a function that passes data frame columns down to aes(). Before version 3.0 of {ggplot2} this was not possible, and writing such a function would have been a bit more complicated. Now, it’s as easy as pie, if I dare say.\n\n\nSomething else I want to look at, is the distribution of turns by role:\n\nnethack %&gt;%\n  filter(turns &lt; quantile(turns, 0.98)) %&gt;%\n  ggplot(aes(x = turns, y = role, group = role, fill = role)) +\n    geom_density_ridges(scale = 6, size = 0.25, rel_min_height = 0.01) + \n    theme_blog() + \n    scale_fill_blog() + \n    theme(axis.text.y = element_blank(),\n          axis.title.y = element_blank())\n## Picking joint bandwidth of 486\n\n\n\n\nI use the very cool {ggridges} package for that. The distribution seems to mostly be the same (of course, one should do a statistical test to be sure), but the one for the role “Valkyrie” seems to be quite different from the others. It is known that it is easier to win the game playing as a Valkyrie, but a question remains: is it really easier as a Valkyrie, or do good players tend to play as Valkyries more often?"
  },
  {
    "objectID": "posts/2018-11-10-nethack_analysis_part2.html#creatures-vanquished-genocided-or-extinct",
    "href": "posts/2018-11-10-nethack_analysis_part2.html#creatures-vanquished-genocided-or-extinct",
    "title": "Analyzing NetHack data, part 2: What players kill the most",
    "section": "\nCreatures vanquished, genocided or extinct\n",
    "text": "Creatures vanquished, genocided or extinct\n\n\nThe dumplog lists which, and how many of which, creatures were vanquished during the run, as well as creatures that were genocided and extinct. The player can genocide an entire species by reading a scroll of genocide (or by sitting on a throne). A species gets extinct if the player manages to kill every monster from that species (there’s other ways too, but for the sake of simplicity, let’s just say that when the players kills every monster from a species, the species is extinct). The following lines are an extract of a dumplog:\n\n\"Vanquished creatures:\"\n\"    Baalzebub\"\n\"    Orcus\"\n\"    Juiblex\"\n\"the Wizard of Yendor (4 times)\"\n\"    Pestilence (thrice)\"\n\"    Famine\"\n\"    Vlad the Impaler\"\n\"  4 arch-liches\"\n\"  an arch-lich\"\n\"  a high priest\"\n\"...\"\n\"...\"\n\"...\"\n\"2873 creatures vanquished.\" \n\nIf I want to analyze this, I have to first solve some problems:\n\n\n\nReplace “a” and “an” by “1”\n\n\nPut the digit in the string “(4 times)” in front of the name of the monster (going from “the Wizard of Yendor (4 times)” to “4 the Wizard of Yendor”)\n\n\nDo something similar for “twice” and “thrice”\n\n\nPut everything into singular (for example, arch-liches into arch-lich)\n\n\nTrim whitespace\n\n\nExtract the genocided or extinct status from the dumplog too\n\n\nFinally, return a data frame with all the needed info\n\n\n\nI wrote a function called extracted_defeated_monsters() and included it in the {nethack} package. I discuss this function in appendix, but what it does is extracting information from dumplog files about vanquished, genocided or extinct monsters and returns a tidy dataframe with that info. This function has a lot of things going on inside it, so if you’re interested in learning more about regular expression and other {tidyverse} tricks, I really encourage you to read its source code.\n\n\nI can now easily add this info to my data:\n\nnethack %&lt;&gt;%\n  mutate(monsters_destroyed = map(dumplog, ~possibly(extract_defeated_monsters, otherwise = NA)(.)))\n\nLet’s take a look at one of them:\n\nnethack$monsters_destroyed[[117]]\n## # A tibble: 285 x 3\n##    value monster              status\n##    &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; \n##  1     1 baalzebub            &lt;NA&gt;  \n##  2     1 orcu                 &lt;NA&gt;  \n##  3     1 juiblex              &lt;NA&gt;  \n##  4     4 the wizard of yendor &lt;NA&gt;  \n##  5     3 pestilence           &lt;NA&gt;  \n##  6     1 famine               &lt;NA&gt;  \n##  7     1 vlad the impaler     &lt;NA&gt;  \n##  8     4 arch-lich            &lt;NA&gt;  \n##  9     1 high priest          &lt;NA&gt;  \n## 10     1 medusa               &lt;NA&gt;  \n## # ... with 275 more rows\nnethack$monsters_destroyed[[117]] %&gt;% \n  count(status)\n## # A tibble: 3 x 2\n##   status        n\n##   &lt;chr&gt;     &lt;int&gt;\n## 1 extinct       2\n## 2 genocided     7\n## 3 &lt;NA&gt;        276\n\nThe status variable tells us if that monster was genocided or extinct during that run. status equal to “NA” means vanquished.\n\n\nIt is now possible to look at, say, the top 15 vanquished monsters (normalized):\n\nnethack %&gt;%\n  filter(!is.na(monsters_destroyed)) %&gt;%\n  pull(monsters_destroyed) %&gt;%\n  bind_rows %&gt;%\n  group_by(monster) %&gt;%\n  summarise(total = sum(value)) %&gt;%\n  top_n(15) %&gt;%\n  ungroup() %&gt;%\n  mutate(norm_total = (total - min(total))/(max(total) - min(total))) %&gt;%\n  mutate(monster = fct_reorder(monster, norm_total, .desc = FALSE)) %&gt;%\n  ggplot() + \n  geom_col(aes(y = norm_total, x = monster)) + \n  coord_flip() + \n  theme_blog() + \n  scale_fill_blog() + \n  ylab(\"Ranking\") +\n  xlab(\"Monster\")\n## Selecting by total\n\n\n\n\nIn this type of graph, the most vanquished monster, “gnome” has a value of 1, and the least vanquished one, 0. This normalization step is also used in the pre-processing step of machine learning algorithms. This helps convergence of the gradient descent algorithm for instance.\n\n\nMonsters can also get genocided or extinct. Let’s make a pie chart of the proportion of genocided and extinct monsters (I lump monsters that are genocided or extinct less than 5% of the times into a category called other). Because I want two pie charts, I nest the data after having grouped it by the status variable. This is a trick I discussed in this blog post and that I use very often:\n\nnethack %&gt;%\n  filter(!is.na(monsters_destroyed)) %&gt;%\n  pull(monsters_destroyed) %&gt;%\n  bind_rows %&gt;%\n  filter(!is.na(status)) %&gt;%\n  group_by(status) %&gt;% \n  count(monster) %&gt;% \n  mutate(monster = fct_lump(monster, prop = 0.05, w = n)) %&gt;% \n  group_by(status, monster) %&gt;% \n  summarise(total_count = sum(n)) %&gt;%\n  mutate(freq = total_count/sum(total_count),\n         labels = scales::percent(freq)) %&gt;%\n  arrange(desc(freq)) %&gt;%\n  group_by(status) %&gt;%\n  nest() %&gt;%\n  mutate(pie_chart = map2(.x = status,\n                          .y = data,\n                          ~ggplot(data = .y,\n                                  aes(x = \"\", y = freq, fill = (monster))) + \n    geom_col() + \n    ggrepel::geom_label_repel(aes(label = labels), position = position_stack(vjust = 0.25), show.legend = FALSE) + \n    coord_polar(\"y\") + \n    theme_blog() + \n    scale_fill_blog() + \n      ggtitle(.x) +\n    theme(legend.title = element_blank(),\n          panel.grid = element_blank(),\n          axis.text = element_blank(),\n          axis.title = element_blank())\n  )) %&gt;%\n  pull(pie_chart)\n## Warning in mutate_impl(.data, dots): Unequal factor levels: coercing to\n## character\n## Warning in mutate_impl(.data, dots): binding character and factor vector,\n## coercing into character vector\n\n## Warning in mutate_impl(.data, dots): binding character and factor vector,\n## coercing into character vector\n## [[1]]\n\n\n\n## \n## [[2]]\n\n\n\n\nThat was it for this one, the graphs are not that super sexy, but the amount of work that went into making them was quite consequent. The main reason was that parsing xlogfiles was a bit tricky, but the main challenge was extracting information from dumplog files. This proved to be a bit more complicated than expected (just take a look at the source code of extract_defeated_monsters() to get an idea…)."
  },
  {
    "objectID": "posts/2018-11-10-nethack_analysis_part2.html#bonus-plot",
    "href": "posts/2018-11-10-nethack_analysis_part2.html#bonus-plot",
    "title": "Analyzing NetHack data, part 2: What players kill the most",
    "section": "\nBonus plot\n",
    "text": "Bonus plot\n\n\n\nCorrect number of daily games\n\n\nThe daily number of games are available here. Let’s extract this info and remake the plot that shows the number of runs per day:\n\ngames &lt;- read_html(\"https://alt.org/nethack/dailygames_ct.html\") %&gt;%\n        html_nodes(xpath = '//table') %&gt;%\n        html_table(fill = TRUE) \n\nThis extracts all the tables and puts them into a list. Let’s take a look at one:\n\nhead(games[[1]])\n##   2018  2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018\n## 1         NA    1    2    3    4    5    6    7    8    9   10   11   12\n## 2  Jan 11639  275  370  394  363  392  276  288  324  297  411  413  430\n## 3  Feb 10819  375  384  359  376  440  345  498  457  416  376  421  416\n## 4  Mar 12148  411  403  421  392  447  391  451  298  350  309  309  369\n## 5  Apr 13957  456  513  482  516  475  490  397  431  436  438  541  493\n## 6  May 13361  595  509  576  620  420  443  407  539  440  446  404  282\n##   2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018\n## 1   13   14   15   16   17   18   19   20   21   22   23   24   25   26\n## 2  331  341  318  483  408  424  464  412  371  430  348  315  359  375\n## 3  385  367  443  324  283  341  385  398  361  379  399  276  455  460\n## 4  390  358  362  345  388  360  411  382  371  400  410  417  328  431\n## 5  593  537  396  578  403  435  526  448  339  377  476  492  528  393\n## 6  265  358  419  564  483  429  423  299  424  404  450  408  355  409\n##   2018 2018 2018 2018 2018\n## 1   27   28   29   30   31\n## 2  432  371  385  440  399\n## 3  353  347   NA   NA   NA\n## 4  386  484  493  486  395\n## 5  407  421  463  477   NA\n## 6  417  433  360  391  389\n\nLet’s clean this up.\n\nclean_table &lt;- function(df){\n  # Promotes first row to header\n  colnames(df) &lt;- df[1, ]\n  df &lt;- df[-1, ]\n  \n  # Remove column with total from the month\n  df &lt;- df[, -2]\n  \n  # Name the first column \"month\"\n  \n  colnames(df)[1] &lt;- \"month\"\n  \n  # Now put it in a tidy format\n  df %&gt;%\n    gather(day, games_played, -month)\n}\n\nNow I can clean up all the tables. I apply this function to each element of the list games. I also add a year column:\n\ngames &lt;- map(games, clean_table) %&gt;%\n  map2_dfr(.x = ., \n       .y = seq(2018, 2001),\n       ~mutate(.x, year = .y))\n\nNow I can easily create the plot I wanted\n\ngames %&lt;&gt;%\n  mutate(date = lubridate::ymd(paste(year, month, day, sep = \"-\")))\n## Warning: 122 failed to parse.\nggplot(games, aes(y = games_played, x = date)) + \n  geom_point(colour = \"#0f4150\") + \n  geom_smooth(colour = \"#82518c\") + \n  theme_blog() + \n  ylab(\"Total games played\")\n## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n## Warning: Removed 452 rows containing non-finite values (stat_smooth).\n## Warning: Removed 452 rows containing missing values (geom_point).\n\n\n\n\nThere’s actually a lot more games than 50 per day being played!"
  },
  {
    "objectID": "posts/2018-11-10-nethack_analysis_part2.html#appendix",
    "href": "posts/2018-11-10-nethack_analysis_part2.html#appendix",
    "title": "Analyzing NetHack data, part 2: What players kill the most",
    "section": "\nAppendix\n",
    "text": "Appendix\n\n\n\nFuzzy matching\n\n\nIf you take a look at the extract_defeated_monsters() source code, you’ll see that at some point I “singularize” monster names. I decided to deal with this singular/plural issue, “by hand”, but also explored other possibilities, such as matching the plural nouns with the singular nouns fuzzily. In the end it didn’t work out so well, but here’s the code for future reference.\n\nmonster_list &lt;- read_html(\"https://nethackwiki.com/wiki/Monsters_(by_difficulty)\") %&gt;%\n    html_nodes(\".prettytable\") %&gt;% \n    .[[1]] %&gt;%\n    html_table(fill = TRUE)\n\nmonster_list %&lt;&gt;%\n    select(monster = Name)\n\nhead(monster_list)\n##      monster\n## 1 Demogorgon\n## 2   Asmodeus\n## 3  Baalzebub\n## 4   Dispater\n## 5     Geryon\n## 6      Orcus\nlibrary(fuzzyjoin)\n\ntest_vanquished &lt;- extract_defeated_monsters(nethack$dumplog[[117]])\n\nhead(test_vanquished)\n## # A tibble: 6 x 3\n##   value monster              status\n##   &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; \n## 1     1 baalzebub            &lt;NA&gt;  \n## 2     1 orcu                 &lt;NA&gt;  \n## 3     1 juiblex              &lt;NA&gt;  \n## 4     4 the wizard of yendor &lt;NA&gt;  \n## 5     3 pestilence           &lt;NA&gt;  \n## 6     1 famine               &lt;NA&gt;\n\nYou can take a look at the result by expanding:\n\n\n\n\nClick to expand\n\n\nstringdist_left_join(test_vanquished, monster_list) %&gt;% \n  count(monster.y) %&gt;%\n  print(n = Inf)\n## Joining by: \"monster\"\n## # A tibble: 297 x 2\n##     monster.y                   n\n##     &lt;chr&gt;                   &lt;int&gt;\n##   1 acid blob                   1\n##   2 air elemental               2\n##   3 Aleax                       1\n##   4 aligned priest              1\n##   5 Angel                       1\n##   6 ape                         2\n##   7 arch-lich                   1\n##   8 Baalzebub                   1\n##   9 baby black dragon           1\n##  10 baby crocodile              1\n##  11 baby gray dragon            1\n##  12 baby green dragon           1\n##  13 baby long worm              1\n##  14 baby orange dragon          1\n##  15 baby white dragon           1\n##  16 baby yellow dragon          1\n##  17 balrog                      1\n##  18 baluchitherium              1\n##  19 barbed devil                1\n##  20 barrow wight                1\n##  21 bat                         1\n##  22 black dragon                1\n##  23 black light                 1\n##  24 black naga                  1\n##  25 black pudding               1\n##  26 black unicorn               1\n##  27 blue dragon                 1\n##  28 blue jelly                  1\n##  29 bone devil                  1\n##  30 brown mold                  1\n##  31 brown pudding               1\n##  32 bugbear                     1\n##  33 captain                     1\n##  34 carnivorous ape             1\n##  35 cave spider                 1\n##  36 centipede                   1\n##  37 chameleon                   1\n##  38 chickatrice                 2\n##  39 clay golem                  1\n##  40 cobra                       1\n##  41 cockatrice                  2\n##  42 couatl                      1\n##  43 coyote                      1\n##  44 crocodile                   1\n##  45 demilich                    1\n##  46 dingo                       1\n##  47 disenchanter                1\n##  48 dog                         1\n##  49 doppelganger                1\n##  50 dust vortex                 1\n##  51 dwarf                       2\n##  52 dwarf king                  1\n##  53 dwarf lord                  1\n##  54 dwarf mummy                 1\n##  55 dwarf zombie                1\n##  56 earth elemental             1\n##  57 electric eel                1\n##  58 elf                         1\n##  59 elf mummy                   1\n##  60 elf zombie                  1\n##  61 elf-lord                    1\n##  62 Elvenking                   1\n##  63 energy vortex               1\n##  64 erinys                      1\n##  65 ettin                       1\n##  66 ettin mummy                 1\n##  67 ettin zombie                1\n##  68 Famine                      1\n##  69 fire ant                    2\n##  70 fire elemental              2\n##  71 fire giant                  2\n##  72 fire vortex                 2\n##  73 flaming sphere              1\n##  74 flesh golem                 1\n##  75 floating eye                1\n##  76 fog cloud                   1\n##  77 forest centaur              1\n##  78 fox                         1\n##  79 freezing sphere             1\n##  80 frost giant                 1\n##  81 gargoyle                    1\n##  82 garter snake                1\n##  83 gas spore                   1\n##  84 gecko                       1\n##  85 gelatinous cube             1\n##  86 ghost                       2\n##  87 ghoul                       2\n##  88 giant ant                   3\n##  89 giant bat                   3\n##  90 giant beetle                1\n##  91 giant eel                   1\n##  92 giant mimic                 1\n##  93 giant mummy                 1\n##  94 giant rat                   3\n##  95 giant spider                1\n##  96 giant zombie                1\n##  97 glass piercer               1\n##  98 gnome                       1\n##  99 gnome king                  1\n## 100 gnome lord                  1\n## 101 gnome mummy                 1\n## 102 gnome zombie                1\n## 103 gnomish wizard              1\n## 104 goblin                      1\n## 105 gold golem                  2\n## 106 golden naga                 1\n## 107 golden naga hatchling       1\n## 108 gray ooze                   1\n## 109 gray unicorn                1\n## 110 Green-elf                   1\n## 111 gremlin                     1\n## 112 Grey-elf                    1\n## 113 grid bug                    1\n## 114 guardian naga               1\n## 115 guardian naga hatchling     1\n## 116 hell hound                  1\n## 117 hell hound pup              1\n## 118 hezrou                      1\n## 119 high priest                 1\n## 120 hill giant                  1\n## 121 hill orc                    1\n## 122 hobbit                      1\n## 123 hobgoblin                   1\n## 124 homunculus                  1\n## 125 horned devil                1\n## 126 horse                       2\n## 127 housecat                    1\n## 128 human                       1\n## 129 human mummy                 1\n## 130 human zombie                1\n## 131 ice devil                   1\n## 132 ice troll                   1\n## 133 ice vortex                  2\n## 134 iguana                      1\n## 135 imp                         1\n## 136 incubus                     1\n## 137 iron golem                  1\n## 138 iron piercer                1\n## 139 jabberwock                  1\n## 140 jackal                      1\n## 141 jaguar                      1\n## 142 jellyfish                   1\n## 143 Juiblex                     1\n## 144 Keystone Kop                1\n## 145 ki-rin                      1\n## 146 killer bee                  1\n## 147 kitten                      1\n## 148 kobold                      1\n## 149 kobold lord                 1\n## 150 kobold mummy                1\n## 151 kobold shaman               1\n## 152 kobold zombie               1\n## 153 Kop Lieutenant              1\n## 154 Kop Sergeant                1\n## 155 kraken                      2\n## 156 large cat                   1\n## 157 large dog                   1\n## 158 large kobold                1\n## 159 large mimic                 1\n## 160 leather golem               1\n## 161 leocrotta                   1\n## 162 leprechaun                  1\n## 163 lich                        2\n## 164 lichen                      2\n## 165 lieutenant                  1\n## 166 little dog                  1\n## 167 lizard                      1\n## 168 long worm                   1\n## 169 Lord Surtur                 1\n## 170 lurker above                1\n## 171 lynx                        1\n## 172 manes                       1\n## 173 marilith                    1\n## 174 master lich                 1\n## 175 master mind flayer          1\n## 176 Medusa                      1\n## 177 mind flayer                 1\n## 178 minotaur                    1\n## 179 monk                        2\n## 180 monkey                      1\n## 181 Mordor orc                  1\n## 182 mountain centaur            1\n## 183 mountain nymph              1\n## 184 mumak                       1\n## 185 nalfeshnee                  1\n## 186 Nazgul                      1\n## 187 newt                        1\n## 188 Norn                        1\n## 189 nurse                       2\n## 190 ochre jelly                 1\n## 191 ogre                        1\n## 192 ogre king                   1\n## 193 ogre lord                   1\n## 194 Olog-hai                    1\n## 195 orange dragon               1\n## 196 orc                         3\n## 197 orc mummy                   1\n## 198 orc shaman                  1\n## 199 orc zombie                  1\n## 200 orc-captain                 1\n## 201 Orcus                       1\n## 202 owlbear                     1\n## 203 page                        2\n## 204 panther                     1\n## 205 paper golem                 1\n## 206 Pestilence                  1\n## 207 piranha                     1\n## 208 pit fiend                   1\n## 209 pit viper                   1\n## 210 plains centaur              1\n## 211 pony                        1\n## 212 purple worm                 1\n## 213 pyrolisk                    1\n## 214 python                      1\n## 215 quantum mechanic            1\n## 216 quasit                      1\n## 217 queen bee                   1\n## 218 quivering blob              1\n## 219 rabid rat                   1\n## 220 ranger                      1\n## 221 raven                       2\n## 222 red dragon                  1\n## 223 red mold                    1\n## 224 red naga                    1\n## 225 rock mole                   1\n## 226 rock piercer                1\n## 227 rock troll                  1\n## 228 rogue                       2\n## 229 rope golem                  1\n## 230 roshi                       1\n## 231 rothe                       1\n## 232 rust monster                1\n## 233 salamander                  1\n## 234 sandestin                   1\n## 235 sasquatch                   1\n## 236 scorpion                    1\n## 237 sergeant                    1\n## 238 sewer rat                   1\n## 239 shade                       3\n## 240 shark                       2\n## 241 shocking sphere             1\n## 242 shrieker                    1\n## 243 silver dragon               1\n## 244 skeleton                    1\n## 245 small mimic                 1\n## 246 snake                       2\n## 247 soldier                     1\n## 248 soldier ant                 1\n## 249 spotted jelly               1\n## 250 stalker                     1\n## 251 steam vortex                1\n## 252 stone giant                 2\n## 253 stone golem                 1\n## 254 storm giant                 2\n## 255 straw golem                 1\n## 256 succubus                    1\n## 257 tengu                       1\n## 258 tiger                       1\n## 259 titanothere                 1\n## 260 trapper                     1\n## 261 troll                       1\n## 262 umber hulk                  1\n## 263 Uruk-hai                    1\n## 264 vampire                     1\n## 265 vampire bat                 1\n## 266 vampire lord                1\n## 267 violet fungus               1\n## 268 Vlad the Impaler            1\n## 269 vrock                       1\n## 270 warg                        2\n## 271 warhorse                    1\n## 272 water elemental             1\n## 273 water moccasin              1\n## 274 water nymph                 1\n## 275 werejackal                  2\n## 276 wererat                     2\n## 277 werewolf                    2\n## 278 white dragon                1\n## 279 white unicorn               1\n## 280 winged gargoyle             1\n## 281 winter wolf                 1\n## 282 winter wolf cub             1\n## 283 wizard                      1\n## 284 wolf                        1\n## 285 wood golem                  2\n## 286 wood nymph                  1\n## 287 Woodland-elf                1\n## 288 wraith                      1\n## 289 wumpus                      1\n## 290 xan                         3\n## 291 xorn                        2\n## 292 yellow dragon               1\n## 293 yellow light                1\n## 294 yellow mold                 1\n## 295 yeti                        1\n## 296 zruty                       1\n## 297 &lt;NA&gt;                        1\n\n\nAs you can see, some matches fail, especially for words that end in “y” in the singular, so “ies” in plural, or “fire vortices” that does not get matched to “fire vortex”. I tried all the methods but it’s either worse, or marginally better.\n\n\n\n\nExtracting info from dumplogfiles\n\n\n\n\nClick here to take a look at the source code from extract_defeated_monsters\n\n\n#' Extract information about defeated monsters from an xlogfile\n#' @param xlog A raw xlogfile\n#' @return A data frame with information on vanquished, genocided and extincted monsters\n#' @importFrom dplyr mutate select filter bind_rows full_join\n#' @importFrom tidyr separate\n#' @importFrom tibble as_tibble tibble\n#' @importFrom magrittr \"%&gt;%\"\n#' @importFrom purrr map2 possibly is_empty modify_if simplify discard\n#' @importFrom readr read_lines\n#' @importFrom stringr str_which str_replace_all str_replace str_trim str_detect str_to_lower str_extract_all str_extract\n#' @export\n#' @examples\n#' \\dontrun{\n#' get_dumplog(xlog)\n#' }\nextract_defeated_monsters &lt;- function(dumplog){\n\n    if(any(str_detect(dumplog, \"No creatures were vanquished.\"))){\n        return(NA)\n    } else {\n\n        start &lt;- dumplog %&gt;% # &lt;- dectect the start of the list\n            str_which(\"Vanquished creatures\")\n\n        end &lt;- dumplog %&gt;% # &lt;- detect the end of the list\n            str_which(\"\\\\d+ creatures vanquished.\")\n\n        if(is_empty(end)){ # This deals with the situation of only one vanquished creature\n            end &lt;- start + 2\n        }\n\n        list_creatures &lt;- dumplog[(start + 1):(end - 1)] %&gt;% # &lt;- extract the list\n            str_replace_all(\"\\\\s+an? \", \"1 \") %&gt;% # &lt;- replace a or an by 1\n            str_trim() # &lt;- trim white space\n\n        # The following function first extracts the digit in the string (123 times)\n        # and replaces the 1 with this digit\n        # This means that: \"1 the Wizard of Yendor (4 times)\" becomes \"4 the Wizard of Yendor (4 times)\"\n        str_extract_replace &lt;- function(string){\n            times &lt;- str_extract(string, \"\\\\d+(?=\\\\stimes)\")\n            str_replace(string, \"1\", times)\n        }\n\n        result &lt;- list_creatures %&gt;%\n            # If a string starts with a letter, add a 1\n            # This means that: \"Baalzebub\" becomes \"1 Baalzebub\"\n            modify_if(str_detect(., \"^[:alpha:]\"), ~paste(\"1\", .)) %&gt;%\n            # If the string \"(twice)\" is detected, replace \"1\" (that was added the line before) with \"2\"\n            modify_if(str_detect(., \"(twice)\"), ~str_replace(., \"1\", \"2\")) %&gt;%\n            # Same for \"(thrice)\"\n            modify_if(str_detect(., \"(thrice)\"), ~str_replace(., \"1\", \"3\")) %&gt;%\n            # Exctract the digit in \"digit times\" and replace the \"1\" with digit\n            modify_if(str_detect(., \"(\\\\d+ times)\"), str_extract_replace) %&gt;%\n            # Replace \"(times)\" or \"(twice)\" etc with \"\"\n            str_replace_all(\"\\\\(.*\\\\)\", \"\") %&gt;%\n            str_trim() %&gt;%\n            simplify() %&gt;%\n            # Convert the resulting list to a tibble. This tibble has one column:\n            # value\n            # 1 Baalzebub\n            # 2 dogs\n            #...\n            as_tibble() %&gt;%\n            # Use tidyr::separate to separate the \"value\" column into two columns. The extra pieces get merged\n            # So for example \"1 Vlad the Impaler\" becomes \"1\" \"Vlad the Impaler\" instead of \"1\" \"Vlad\" which\n            # would be the case without \"extra = \"merge\"\"\n            separate(value, into = c(\"value\", \"monster\"), extra = \"merge\") %&gt;%\n            mutate(value = as.numeric(value)) %&gt;%\n            mutate(monster = str_to_lower(monster))\n\n        # This function singularizes names:\n        singularize_monsters &lt;- function(nethack_data){\n            nethack_data %&gt;%\n                mutate(monster = str_replace_all(monster, \"mummies\", \"mummy\"),\n                       monster = str_replace_all(monster, \"jellies\", \"jelly\"),\n                       monster = str_replace_all(monster, \"vortices\", \"vortex\"),\n                       monster = str_replace_all(monster, \"elves\", \"elf\"),\n                       monster = str_replace_all(monster, \"wolves\", \"wolf\"),\n                       monster = str_replace_all(monster, \"dwarves\", \"dwarf\"),\n                       monster = str_replace_all(monster, \"liches\", \"lich\"),\n                       monster = str_replace_all(monster, \"baluchiteria\", \"baluchiterium\"),\n                       monster = str_replace_all(monster, \"homunculi\", \"homonculus\"),\n                       monster = str_replace_all(monster, \"mumakil\", \"mumak\"),\n                       monster = str_replace_all(monster, \"sasquatches\", \"sasquatch\"),\n                       monster = str_replace_all(monster, \"watchmen\", \"watchman\"),\n                       monster = str_replace_all(monster, \"zruties\", \"zruty\"),\n                       monster = str_replace_all(monster, \"xes$\", \"x\"),\n                       monster = str_replace_all(monster, \"s$\", \"\"))\n        }\n\n        result &lt;- singularize_monsters(result)\n    }\n    # If a player did not genocide or extinct any species, return the result:\n    if(any(str_detect(dumplog, \"No species were genocided or became extinct.\"))){\n        result &lt;- result %&gt;%\n            mutate(status = NA_character_)\n        return(result)\n    } else {\n\n        # If the player genocided or extincted species, add this info:\n        start &lt;- dumplog %&gt;% # &lt;- dectect the start of the list\n            str_which(\"Genocided or extinct species:\") # &lt;- sometimes this does not appear in the xlogfile\n\n        end &lt;- dumplog %&gt;% # &lt;- detect the end of the list\n            str_which(\"Voluntary challenges\")\n\n       if(is_empty(start)){# This deals with the situation start does not exist\n           start &lt;- end - 2\n       }\n\n        list_creatures &lt;- dumplog[(start + 1):(end - 1)] %&gt;% # &lt;- extract the list\n            str_trim() # &lt;- trim white space\n\n        extinct_species &lt;- list_creatures %&gt;%\n            str_extract_all(\"[:alpha:]+\\\\s(?=\\\\(extinct\\\\))\", simplify = T) %&gt;%\n            str_trim %&gt;%\n            discard(`==`(., \"\"))\n\n        extinct_species_df &lt;- tibble(monster = extinct_species, status = \"extinct\")\n\n        genocided_species_index &lt;- list_creatures %&gt;%\n            str_detect(pattern = \"extinct|species\") %&gt;%\n            `!`\n\n        genocided_species &lt;- list_creatures[genocided_species_index]\n\n        genocided_species_df &lt;- tibble(monster = genocided_species, status = \"genocided\")\n\n        genocided_or_extinct_df &lt;- singularize_monsters(bind_rows(extinct_species_df, genocided_species_df))\n\n        result &lt;- full_join(result, genocided_or_extinct_df, by = \"monster\") %&gt;%\n            filter(monster != \"\") # &lt;- this is to remove lines that were added by mistake, for example if start was empty\n\n        return(result)\n    }\n}"
  },
  {
    "objectID": "posts/2023-10-05-repro_overview.html",
    "href": "posts/2023-10-05-repro_overview.html",
    "title": "An overview of what’s out there for reproducibility with R",
    "section": "",
    "text": "In this short blog post I’ll be summarizing what I learnt these past years about reproducibility with R. I’ll give some high-level explanations about different tools and then link to different blog posts of mine.\nI see currently two main approaches with some commonalities, so let’s start with the commonalities."
  },
  {
    "objectID": "posts/2023-10-05-repro_overview.html#commonalities",
    "href": "posts/2023-10-05-repro_overview.html#commonalities",
    "title": "An overview of what’s out there for reproducibility with R",
    "section": "\nCommonalities\n",
    "text": "Commonalities\n\n\nThese are aspects that I think will help you build reproducible projects, but that are not strictly necessary. These are:\n\n\n\nGit for code versioning;\n\n\nunit tests (be it on your code or data);\n\n\nliterate programming;\n\n\npackaging code;\n\n\nbuild automation.\n\n\n\nI think that these aspects are really very important nice-to-haves, but depending on the project you might not have to use all these tools or techniques (but I would really recommend that you think very hard about these requirements and make sure that you actually, really, don’t need them).\n\n\nWhat’s also important is how you organize the work if you’re in a team. Making sure that everyone is on the same page and uses the same tools and approaches is very important.\n\n\nNow that we have the commonalities out of the way, let’s discuss the “two approaches”. Let’s start by the most popular one."
  },
  {
    "objectID": "posts/2023-10-05-repro_overview.html#docker-and-something-else",
    "href": "posts/2023-10-05-repro_overview.html#docker-and-something-else",
    "title": "An overview of what’s out there for reproducibility with R",
    "section": "\nDocker and “something else”\n",
    "text": "Docker and “something else”\n\n\nDocker is a very popular containerisation solution. The idea is to build an image that contains everything needed to run and rebuild your project in a single command. You can add a specific version of R with the required packages in it, your project files and so on. You could even add the data directly into the image or provide the required data at run-time, it’s up to you.\n\n\nThe “something else” can be several things, but they all deal with the problem of providing the right packages for your analysis. You see, if you run an analysis today, you’ll be using certain versions of packages. The same versions of packages need to be made available inside that Docker image. To do so, a popular choice for R users is to use {renv}, but there’s also {groundhog} and {rang}. You could also use CRAN snapshots from the Posit Public Package Manager. Whatever you choose, Docker by itself is not enough: Docker provides a base where you can then add these other things on top.\n\n\nTo know more, read this:\n\n\n\nhttps://www.brodrigues.co/blog/2022-11-19-raps/\n\n\nhttps://www.brodrigues.co/blog/2022-11-30-pipelines-as/\n\n\nhttps://www.brodrigues.co/blog/2023-05-08-dock_dev_env/\n\n\nhttps://www.brodrigues.co/blog/2023-01-12-repro_r/\n\n\n\nBy combining Docker plus any of the other packages listed above (or by using the PPPM) you can quite easily build reproducible projects, because what you end up doing, is essentially building something like a capsule that contains everything needed to run the project (this capsule is what is called an image). Then, you don’t run R and the scripts to build the project, you run the image, and within that image, R is executed on the provided scripts. This running instance of an image is called a container. This approach is by far the most popular and can even be used on Github Actions if your project is hosted on Github. On a scale from 1 to 10, I would say that the entry cost is about 3 if you already have some familiarity with Linux, but can go up to 7 if you’ve never touched Linux. What does Linux have to do with all this? Well, the Docker images that you are going to build will be based on Linux (most of the time the Ubuntu distribution) so familiarity with Linux or Ubuntu is a huge plus. You could use {renv}, {rang} or {groundhog} without Docker, directly on your PC, but the issue here is that your operating system and the version of R changes through time. And both of these can have an impact on the reproducibility of your project. Hence, why we use Docker to, in a sense, “freeze” both the underlying operating system and version of R inside that image, and then, every container executed from that image will have the required versions of software.\n\n\nOne issue with Docker is that if you build an image today, the underlying Linux distribution will get out of date at some point, and you won’t be able to rebuild the image. So you either need to build the image and store it forever, or you need to maintain your image and port your code to newer base Ubuntu images."
  },
  {
    "objectID": "posts/2023-10-05-repro_overview.html#nix",
    "href": "posts/2023-10-05-repro_overview.html#nix",
    "title": "An overview of what’s out there for reproducibility with R",
    "section": "\nNix\n",
    "text": "Nix\n\n\nNix is a package manager for Linux (and Windows through WSL) and macOS, but also a programming language that focuses on reproducibility of software builds, meaning that using Nix it’s possible to build software in a completely reproducible way. Nix is incredibly flexible, so it’s also possible to use it to build reproducible development environments, or run reproducible analytical pipelines. What Nix doesn’t easily allow, unlike {renv} for example, is to install a specific version of one specific package. But I also wrote a package called {rix} (co-authored by Philipp Baumann) that makes it easier for R users to get started with Nix and also allows to install arbitrary versions of packages easily using the Nix package manager. So you can define an environment with any version of R, plus corresponding packages, and install specific versions of specific packages if needed as well. Packages that are hosted on Github can also get easily installed if needed. Let me make this clear: using Nix, you install both R and R packages so there’s no need to use install.packages() anymore. Everything is managed by Nix.\n\n\nUsing Nix, we can define our environment and build instructions as code, and have the build process always produce exactly the same result. This definition of the environment and build instructions are written using the Nix programming language inside a simple text file, which then gets used to actually realize the build. This means that regardless of “when” or “where” you rebuild your project, exactly the same packages (all the way down to the system libraries and compilers and all that stuff we typically never think about) will get installed to rebuild the project.\n\n\nEssentially, using the Nix package manager, you can replace Docker + any of the other tools listed above to build reproducible projects. The issue with Nix however is that the entry cost is quite high: even if you’re already familiar with Linux and package managers, Nix is really an incredible deep tool. So I would say that the entry cost is around 9 out of 10…, but to bring this entry cost down, I have written 6 blog posts to get you started:\n\n\n\nhttps://www.brodrigues.co/blog/2023-07-13-nix_for_r_part1/\n\n\nhttps://www.brodrigues.co/blog/2023-07-19-nix_for_r_part2/\n\n\nhttps://www.brodrigues.co/blog/2023-07-30-nix_for_r_part3/\n\n\nhttps://www.brodrigues.co/blog/2023-08-12-nix_for_r_part4/\n\n\nhttps://www.brodrigues.co/blog/2023-09-15-nix_for_r_part5/\n\n\nhttps://www.brodrigues.co/blog/2023-09-20-nix_for_r_part6/\n\n\n\nAlso, by the way, it is entirely possible to build a Docker image based on Ubuntu, install the Nix package manager on it, and then use Nix inside Docker to install the right software to build a reproducible project. This approach is extremely flexible, as it uses the best of both worlds in my opinion: we can take advantage of the popularity of Docker so that we can run containers anywhere, but use Nix to truly have reproducible builds. This also solves the issue I discussed before: if you’re using Nix inside Docker, it doesn’t matter if the base image gets outdated: simply use a newer base image, and Nix will take care of always installing the right versions of the needed pieces of software for your project."
  },
  {
    "objectID": "posts/2023-10-05-repro_overview.html#conclusion",
    "href": "posts/2023-10-05-repro_overview.html#conclusion",
    "title": "An overview of what’s out there for reproducibility with R",
    "section": "\nConclusion\n",
    "text": "Conclusion\n\n\nSo which should you learn, Docker or Nix? While Docker is certainly more popular these days, I think that Nix is very interesting and not that hard to use once you learnt the basics (which does take some time). But the entry costs for any of these tools is in the end quite high and, very annoyingly, building reproducible projects does not get enough recognition, even in science where reproducibility is supposedly one of its corner stones. However, I think that you should definitely invest time in learning the tools and best practices required for building reproducible projects, because by making sure that a project is reproducible you end up increasing its quality as well. Furthermore, you avoid stressful situations where you get asked “hey, where did that graph/result/etc come from?” and you have no idea why the script that supposedly built that output does not reproduce the same output again.\n\n\nIf you read all the blog posts above but still want to learn and know more about reproducibility you can get my ebook at a discount or get a physical copy on Amazon or you can read it for free. That book does not discuss Nix, but I will very certainly be writing another book focusing this time on Nix during 2024."
  },
  {
    "objectID": "posts/2024-08-28-nix_for_r_part_12.html",
    "href": "posts/2024-08-28-nix_for_r_part_12.html",
    "title": "Reproducible data science with Nix, part 12 – Nix as a polyglot build automation tool for data science",
    "section": "",
    "text": "Nix is not only a package manager, but also a build automation tool, and you can use it to build polyglot data science pipelines in a completely reproducible way.\n\n\nFor example, suppose that you need to mix Python, R and maybe some others tools for a project (by the way, some believe this will become the norm in the coming years, use your favourite search engine to look for “polyglot data science” and you’ll see), and suppose that you want to define your project as a nice reproducible pipeline, and not simply a series of scripts. What are the options available to you?\n\n\nOne option would be to use the {targets} package for R, which allows you to do lay out your project as pipeline. But as amazing as {targets} is, it only works with R. If you also need Python, you would then need to also use the {reticulate} package to interface with it. But what do you do if you need some other command line tools? Well, you could wrap them in an R function using system() or system2(). But what if you need yet another language, like Julia? There might be a way to call Julia from R, but as you see, the more diverse tools you need, the more complex it gets. And it doesn’t really matter if you switch from {targets} to another such package that exists for, say, Python, you would always need to write wrappers or use packages that allow you to call the other programming languages that you need.\n\n\nAnother possibility is to use good old make. make is a tool from the GNU project that allows you to define targets, which would be the outputs of a script or call to some cli tool by writing so-called Makefiles. For an example of a Makefile in research, take a look at this one from a paper by Grant McDermott. You can use make as a to orchestrate several programming languages or cli tools, but you will need to write code to pass data from one script to the other. {targets} deals with that transparently by serialising all the targets’ outputs using saveRDS() but this only works because only R is supported. But if you’re trying to make R, Python, and whatever else work together, you will need to deal with this manually and find a common interface to pass data around.\n\n\nDespite this, using make, or some other tool on top of the required programming languages (and not tied to either one), is likely the best solution and it turns out that Nix can be used just like that! But why use Nix and not make then? Well, using Nix guarantees that whatever you produce will be completely reproducible. With make, you would need to either run it inside a Docker image or… inside a development environment built with Nix! I did something similar in this blog post where I ran a {targets} pipeline inside a Nix environment to make the analysis reproducible.\n\n\nBut if I’m already defining a reproducible development environment using Nix, why not go all the way and build a complete project using Nix? After all, Nix allows you to package software and what is software but 0’s and 1’s? And what is a trained model, a paper or report in the PDF format, predictions exported into a CSV file, etc, if not 0’s and 1’s?\n\n\nJust like with any other build automation tool, Nix will only rebuild the project if something changes, and will only rebuild the parts that need to be rebuilt. So if you change a file somewhere, only whatever depends on this file will get rebuilt, just like with {targets}, or make.\n\n\nIn the following repository you can find an example of this.\n\n\nThis is a very simple project: two functions are defined in the python_functions.py script. These functions are nothing special, and could be used interactively. One function reads a .csv file from the Internet and returns it, the other does some basic cleaning. Here are these two functions included in the python_functions.py file:\n\nfrom pandas import read_csv\n\ndef download_iris(iris_csv_url):\n    # Read the CSV file\n    df = read_csv(iris_csv_url)\n\n    return df\n\ndef process_iris(iris_csv_path):\n    # Read the CSV file\n    df = read_csv(iris_csv_path)\n\n    # Replace the species numbers with their corresponding names\n    species_mapping = {0: \"setosa\", 1: \"virginica\", 2: \"versicolor\"}\n    df['species'] = df['species'].replace(species_mapping)\n\n    return df\n\nThen, I want to use {ggplot2} to plot this data. You will notice the lack of R script in the repo. I did this on purpose, because I wanted to show how you could directly write R code inside of a Nix expression. But in practice, it is better to have Python code in a Python script, R code in an R script, and then use Nix to orchestrate the whole thing. But I just wanted to show you that you could, if you wanted to, have a completely self-contained Nix expression that encapsulates the business logic as well.\n\n\nThere’s also a .Qmd file: this is the file that will get compiled into a PDF document, and is the output of the whole project. It could be anything else! As I stated above, this is just 0’s and 1’s so it could very well be some other output, it doesn’t really matter.\n\n\nLet’s now take a look at the default.nix that builds the whole thing. Let’s start by the top-level definitions:\n\nlet\n  pkgs =\n    import\n      (fetchTarball \"https://github.com/NixOS/nixpkgs/archive/27285241da3bb285155d549a11192e9fdc3a0d04.tar.gz\")\n      { };\n\n  tex = (\n    pkgs.texlive.combine {\n      inherit (pkgs.texlive) scheme-small;\n    }\n  );\n\n  # Because building happens in sandbox that cannot connect to the internet\n  # we need to download assets beforehand\n  iris_path = pkgs.fetchurl {\n    url = \"https://raw.githubusercontent.com/b-rodrigues/nixbat/7c319bcdbe15e7f7182e7685b8de176a40d0bde9/iris.csv\";\n    hash = \"sha256-2H6THCXKxIt4yxnDDY+AZRmbxqs7FndCp4MqaAR1Cpw=\";\n  };\n\n  # Common python dependencies to use in my intermediary inputs\n  pythonEnv = pkgs.python312.withPackages (ps: with ps; [ pandas ]);\n\n  # Common python sources\n  python_src = pkgs.lib.fileset.toSource {\n    root = ./.;\n    fileset = ./python_functions.py;\n  };\n\nSome variables are defined there:\n\n\n\npkgs: this is the set of Nix packages to be used. All the dependencies of the project will get built using the Nix expressions available in the nixpkgs Github repository at a specific commit. This ensures that the output of this expression will always be exactly the same.\n\n\ntex: defines the set of LaTeX packages I need to compile the PDF.\n\n\niris_path: the Python function I use to load the data takes a path, or url, to read the iris dataset. Because building a derivation happens in a sandbox, I need to download assets beforehand. This is what the fetchurl function does. I can then refer to the file path using ${iris_path} later on.\n\n\npythonEnv: This lists the dependencies I will need to run my Python functions.\n\n\npythonSrc: Defines the path to the python_functions.py file.\n\n\n\nThen, I want to call each of my functions separately, and I want them to produce a single output. So for this, I now build a derivation, one per output. I start with the first one:\n\ndownloadCsv = pkgs.stdenv.mkDerivation {\n  name = \"download-csv\";\n  buildInputs =  [ pythonEnv ];\n  src = pythonSrc;\n  buildPhase = ''\n      python -c \"\nimport pandas as pd\nfrom python_functions import download_iris\n\niris_raw = download_iris('${iris_path}')\n\niris_raw.to_csv('iris_raw.csv', index=False)\n      \"\n    '';\n  installPhase = ''\n    mkdir -p $out\n    cp iris_raw.csv $out/\n  '';\n  };\n\nAt first sight, there might seem that a lot is going on, but let’s take a closer look:\n\n\n\nfirst I give it a name: name = “download-csv”\n\n\nsecond, I list its dependencies in buildInputs. This is what’s required to build the target!\n\n\nthen, I provide the source, in this case the python_functions.py file\n\n\n\nThen, I need to run the code, and this is what happens in the buildPhase. This is exactly the code you would write if you were using a script to glue your functions together. See how I use \\({iris_path}&lt;/code&gt; to refer to the path to the\nfile defined above. Finally, in the &lt;code&gt;installPhase&lt;/code&gt; I copy the &lt;code&gt;.csv&lt;/code&gt; file to\n&lt;code&gt;\\)out/, which essentially copies the file into the Nix store, making it available for the next derivations.\n\n\nIn the next derivation, I now use the second Python function to clean the data:\n\ncleanCsv = pkgs.stdenv.mkDerivation {\n    name = \"clean-csv\";\n    buildInputs =  [ pythonEnv ];\n    src = pythonSrc;\n    buildPhase = ''\n      python -c \"\nimport pandas as pd\nfrom python_functions import process_iris\n\niris = process_iris('${downloadCsv}/iris_raw.csv')\n\niris.to_csv('iris.csv', index=False)\n      \"\n    '';\n    installPhase = ''\n      mkdir -p $out\n      cp iris.csv $out/\n    '';\n  };\n\nThis is not very different than what I did before. Just notice how I refer to the output of the first derivation: ${downloadCsv}/iris_raw.csv.\n\n\nNow comes the last intermediary derivation, the one that uses R to create a plot:\n\ngeneratePlot = pkgs.stdenv.mkDerivation {\n    name = \"generate-plot\";\n    buildInputs = with pkgs; [\n      R\n      rPackages.ggplot2\n      rPackages.janitor\n    ];\n    dontUnpack = true;\n    buildPhase = ''\n            Rscript -e \"\n\n      library(ggplot2)\n      library(janitor)\n\n      iris &lt;- read.csv('${cleanCsv}/iris.csv') |&gt;\n        clean_names() |&gt;\n        transform(species = as.character(species))\n\n      p &lt;- ggplot(iris,\n                  aes(x = sepal_length, y = sepal_width, color = species)) +\n          geom_point(size = 3) +\n          labs(title = 'Sepal Length vs Sepal Width',\n               x = 'Sepal Length',\n               y = 'Sepal Width') +\n          theme_minimal() +\n          theme(plot.title = element_text(hjust = 0.5))\n\n\n      ggsave('plot.png', plot = p, width = 6, height = 4, dpi = 300)\n\n      \"\n    '';\n    installPhase = ''\n      mkdir -p $out\n      cp plot.png $out/\n    '';\n  };\n\nAs I said above, to make this better, it would need to be a function defined in its own R script, as this way there’s a nice separation of concerns. On one hand, there’s the business logic in Python and R scripts, and on the other there’s the orchestration in Nix. Putting R code in the Nix expression makes this less flexible, but I wanted to show you that this is also a possibility!\n\n\nNow comes the last part of the Nix expression, the actual thing I want to build, a PDF that uses the generated plot as an input:\n\nin\n# Derivation to generate the PDF report from Markdown\npkgs.stdenv.mkDerivation {\n  name = \"generate-report\";\n  buildInputs = [\n    pkgs.quarto\n    tex\n  ];\n  src = pkgs.lib.fileset.toSource {\n        root = ./.;\n        # Only include report.Qmd in the source\n        fileset = ./report.Qmd;\n  };\n  buildPhase = ''\n\n    cp ${generatePlot}/plot.png .\n\n    # Deno needs to add stuff to $HOME/.cache\n    # so we give it a home to do this\n    mkdir home\n    export HOME=$PWD/home\n    quarto render report.Qmd --to pdf\n\n  '';\n\n  installPhase = ''\n    mkdir -p $out\n    cp report.pdf $out/\n  '';\n}\n\nNotice the dependencies of this derivation: quarto and tex (tex is the variable I defined right at the beginning that lists LaTeX packages). I then need to specify report.Qmd as the source of this derivation, and copy the plot generated before in R into the working/build directory. There’s also a idiosyncrasy where a dependency of Quarto, Deno, needs to have a directory to save some stuff in it. Nix being Nix, we need to manually define such a home directory for reproducibility purposes. If it would be using my home/ directory on my machine, this wouldn’t be reproducible! We finish the buildPhase by rendering the document, and then install it into $out/. To build this project, you need to have Nix installed and then type nix-build, or alternatively, nix-build -Q which hides all the output of the build phases (so you don’t see any warnings or messages thrown by either Python or R).\n\n\nThis will build the PDF, which you can then find in the Nix store. You’ll notice a file called result appear next to all your other files from the project. In a terminal, call readlink result and this will show you the path to the generated PDF, which you can now read!\n\n\nIn conclusion, I think that this is a really useful way to orchestrate code written in different programming languages, but I would not use this for monolingual projects. For R, I’ll keep using {targets} together with a Nix shell to ensure reproducibility. Also, to really benefit from this, your code needs, ideally, to be written as a series of functions, each outputting a single object. Instead, if you write a script to orchestrate the whole thing in R or Python, and then put a Nix expression on top of it, I’m not sure it’s really worth it. Might as well just use a Nix shell then and execute your scripts in it.\n\n\nAlso, let me state that this is my first attempt at using Nix for such a purpose, and there might be a better/more elegant way of doing it, so if you have any input, don’t hesitate!\n\n\nThanks to the amazing Nix community for helping out!"
  },
  {
    "objectID": "posts/2020-09-20-shiny_raspberry.html",
    "href": "posts/2020-09-20-shiny_raspberry.html",
    "title": "The Raspberry Pi 4B as a shiny server",
    "section": "",
    "text": "This blog post will not have any code, but will document how I went from hosting apps on shinyapps.io to hosting shiny apps on my own server, which is a Raspberry Pi 4B with 8 gigs of ram. First of all, why hosting apps on a Raspberry Pi? And why not continue on shinyapps.io? Or why not get one of hose nifty droplets on DigitalOcean? Well for two reasons; one is that I wanted to have full control of the server, and learn some basic web dev/web engineering skills that I lacked. These services simplify the process of deploying and hosting a lot, which of course is a good thing if your only goal is to deploy apps. But I wanted to learn how to do it myself from scratch for some time. True, with a DigitalOcean droplet, I could have learned quite a lot about the whole process as well, but there’s a second problem; the minimum amount of processing power that the droplet needed to run shiny came at 10€ a month. Not a fortune, but already quite expensive for me, since I just wanted to learn some stuff on my free time. Which is why I got a Raspberry Pi 4B with 8 gigs of ram. It’s less than 100€, and now that I have it, I can do whatever I want whenever I want to. If I don’t touch it for several months, no harm done. And if I get tired of it, I’ll make a retro console out of it and play some old schools games. It’s a win-win situation if you ask me.\nSo first, you should get a Raspberry Pi. Those are quite easy to find online, and there’s many tutorials available on how to install Ubuntu (or any other Linux distro) on it, so I won’t bother with that. I also won’t explain to you how to ssh into your Raspberry Pi, again, there’s many tutorials online. More importantly, is how to get Shiny on it? There’s two solutions; you either install it from source, or you use Docker. I chose to use Docker, but maybe not in the way you’d expect; there’s a lot of talk online about dockerizing apps, complete with all their dependencies and environment. The advantage is that you’re guaranteed that deployment with be very smooth. But the big disadvantage is that these dockerized apps are huge, around 1GB, or sometimes more. It is true that disk space is quite cheap nowadays, but still… so I prefer to run a Shiny server from Docker, and then run the apps out of this server. My apps are thus very small, and it’s only the Shiny server that is huge. I found a Github repository from user havlev that explains how to do it here. I have followed this guide, and created my own docker container, which is based on havlev’s one. I added some dependencies (to the base Debian distro included, as well as some more R packages).\nIf you’re in a hurry, and want to use my Docker image, you can simply type the following on your Raspberry pi:\nThe first 5 commands will create some folders that we’ll need later on, while the last one will pull my Docker container, which is based on havlev’s one, launch the server and it’ll start listening to port 3838.\nI made an app (another blog post, focusing on this app, will follow soon), hosted on my Raspberry Pi that you can find here. I’ll also give you some pointers on how you can achieve that.\nBut let’s start from the beginning."
  },
  {
    "objectID": "posts/2020-09-20-shiny_raspberry.html#adding-dependencies-to-a-docker-container",
    "href": "posts/2020-09-20-shiny_raspberry.html#adding-dependencies-to-a-docker-container",
    "title": "The Raspberry Pi 4B as a shiny server",
    "section": "\nAdding dependencies to a Docker container\n",
    "text": "Adding dependencies to a Docker container\n\n\nSo let’s suppose that you’re me a few weeks ago, and that you find and follow havlev’s guide here. Getting the docker running is quite easy, you just need to set up Docker, and then find the line in the tutorial that starts with docker run…. You’ll get Shiny running with its hello world app. Now, how can you add more packages, either to the base Debian image, or R packages? For this part, I followed this guide. The idea is to “log in” to the console of the base Debian distro that is running from the container. First, find the ID of the container by typing the following command in the terminal:\n\ndocker ps\n\nYou should see something like this:\n\nubuntu@ubuntu:~$ docker ps\nCONTAINER ID        IMAGE                                COMMAND                  CREATED              STATUS              PORTS                    NAMES\n69420blazeit        brodriguesco/shiny_1_5:firstcommit   \"/etc/shiny-server/i…\"   About a minute ago   Up About a minute   0.0.0.0:3838-&gt;3838/tcp   rpi-shiny-server\n\nnow with the ID in hand, you can start any command line program from your Docker container, for instance bash:\n\ndocker exec -it 69420blazeit bash\n\nYou’ll be “logged in” as root:\n\nroot@69420blazeit:/# \n\nand from there, you can install Debian packages. The following two packages are necessary to install many R packages from source, so I recommend you install them:\n\nroot@69420blazeit:/# apt-get install libssl-dev libxml2-dev\n\nOnce these Debian packages are installed, you can start R by simply typing R in the same console, and install whatever packages your Shiny apps will need. In my case, I installed {golem} and several others, but this will be the subject of another blog post. We’re almost done with that; we now need to save the changes because if you restart the container, you’ll lose all these changes. To save these changes, let’s run the following command, but in a new terminal on your Raspberry Pi (on the “local” Ubuntu, not the Debian running in the container):\n\nubuntu@ubuntu:~$ docker commit -m \"added some dependencies\" 69420blazeit shiny_with_deps\n\nSo now you could run this container with the command from above, by replacing the adequate parts:\n\ndocker run -d -p 3838:3838 -v shiny-apps:/srv/shiny-server/ -v shiny-logs:/var/log/ -v shiny-conf:/etc/shiny-server/ --name rpi-shiny-server shiny_with_depsshiny_with_deps"
  },
  {
    "objectID": "posts/2020-09-20-shiny_raspberry.html#using-your-shiny-server",
    "href": "posts/2020-09-20-shiny_raspberry.html#using-your-shiny-server",
    "title": "The Raspberry Pi 4B as a shiny server",
    "section": "\nUsing your Shiny server\n",
    "text": "Using your Shiny server\n\n\nOk so now that the server is running, you can you deploy apps on it? Remember the folders that we created at the beginning of the blog post (or that you created if you followed havlev’s guide)? This is where you’ll drop your apps, the usual way. You create a folder there, and simply put the ui.R and server.R files in here, and that it. These folders can be found in your $HOME directory, and they are accessible to your docker container as well. Once you dropped one or two apps, you’ll be able to access them on a link similar as this one:\n\nhttp://192.168.178.55:3838/hello/\n\nwhere 192.168.178.55 is the local IP address of the Raspberry Pi, 3838 is the port the server is listening to, and /hello/ is the name of the subfolder contained in the ~/shiny-server/apps folder that you created before. What is left doing is making your Raspberry Pi a proper server that can be accessed from the internet. For this, you’ll need to ask your ISP for a dynamic IP address. Generally, you’ll have to pay some money for it; in my case, I’m paying 2€ a month. This address can then be used to access your Raspberry Pi from the internet. The problem, is that being dynamic, the address changes every time you restart your server. To solve this issue, you can use a free dynamic DNS. I use duckdns. This will allow you to have domain that you can share with the world. What’s nice is that if you follow their guide the redirection to the dynamic IP address will happen seamlessly every time it changes, so no need to think about it and do it manually.\n\n\nFinally, you’ll also have to open up port 3838 on your router. The procedure changes from router to router, but you should be able to find the instructions for your router quite easily. If not, you should also be able to get help from your ISP.\n\n\nThe end result is that you’ll have your own Shiny server running off a Raspberry Pi, and accessible over the internet! You’ll be able to deploy as many apps as you want, but of course, don’t forget that you’re running all this on a Raspberry Pi. While these machines have become quite powerful over the years, they won’t be powerful enough if you’re running some heavy duty apps with hundreds of concurrent users.\n\n\nIn my next blog post, I’ll walk you through the development of a Shiny app using the {golem} package, which you can find here."
  },
  {
    "objectID": "posts/2018-11-25-tidy_cv.html#introduction",
    "href": "posts/2018-11-25-tidy_cv.html#introduction",
    "title": "A tutorial on tidy cross-validation with R",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nThis blog posts will use several packages from the {tidymodels} collection of packages, namely {recipes}, {rsample} and {parsnip} to train a random forest the tidy way. I will also use {mlrMBO} to tune the hyper-parameters of the random forest."
  },
  {
    "objectID": "posts/2018-11-25-tidy_cv.html#set-up",
    "href": "posts/2018-11-25-tidy_cv.html#set-up",
    "title": "A tutorial on tidy cross-validation with R",
    "section": "\nSet up\n",
    "text": "Set up\n\n\nLet’s load the needed packages:\n\nlibrary(\"tidyverse\")\nlibrary(\"tidymodels\")\nlibrary(\"parsnip\")\nlibrary(\"brotools\")\nlibrary(\"mlbench\")\n\nLoad the data, included in the {mlrbench} package:\n\ndata(\"BostonHousing2\")\n\nI will train a random forest to predict the housing price, which is the cmedv column:\n\nhead(BostonHousing2)\n##         town tract      lon     lat medv cmedv    crim zn indus chas   nox\n## 1     Nahant  2011 -70.9550 42.2550 24.0  24.0 0.00632 18  2.31    0 0.538\n## 2 Swampscott  2021 -70.9500 42.2875 21.6  21.6 0.02731  0  7.07    0 0.469\n## 3 Swampscott  2022 -70.9360 42.2830 34.7  34.7 0.02729  0  7.07    0 0.469\n## 4 Marblehead  2031 -70.9280 42.2930 33.4  33.4 0.03237  0  2.18    0 0.458\n## 5 Marblehead  2032 -70.9220 42.2980 36.2  36.2 0.06905  0  2.18    0 0.458\n## 6 Marblehead  2033 -70.9165 42.3040 28.7  28.7 0.02985  0  2.18    0 0.458\n##      rm  age    dis rad tax ptratio      b lstat\n## 1 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n## 2 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n## 3 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n## 4 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n## 5 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n## 6 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n\nOnly keep relevant columns:\n\nboston &lt;- BostonHousing2 %&gt;% \n    select(-medv, -town, -lon, -lat) %&gt;% \n    rename(price = cmedv)\n\nI remove town, lat and lon because the information contained in the column tract is enough.\n\n\nTo train and evaluate the model’s performance, I split the data in two. One data set, which I call the training set, will be further split into two down below. I won’t touch the second data set, the test set, until the very end.\n\ntrain_test_split &lt;- initial_split(boston, prop = 0.9)\n\nhousing_train &lt;- training(train_test_split)\n\nhousing_test &lt;- testing(train_test_split)\n\nI want to train a random forest to predict price of houses, but random forests have so-called hyperparameters, which are parameters that cannot be estimated, or learned, from the data. Instead, these parameters have to be chosen by the analyst. In order to choose them, you can use values from the literature that seemed to have worked well (like is done in Macro-econometrics) or you can further split the train set into two, create a grid of hyperparameter, train the model on one part of the data for all values of the grid, and compare the predictions of the models on the second part of the data. You then stick with the model that performed the best, for example, the model with lowest RMSE. The thing is, you can’t estimate the true value of the RMSE with only one value. It’s like if you wanted to estimate the height of the population by drawing one single observation from the population. You need a bit more observations. To approach the true value of the RMSE for a give set of hyperparameters, instead of doing one split, I’ll do 30. I then compute the average RMSE, which implies training 30 models for each combination of the values of the hyperparameters I am interested in.\n\n\nFirst, let’s split the training data again, using the mc_cv() function from {rsample} package. This function implements Monte Carlo cross-validation:\n\nvalidation_data &lt;- mc_cv(housing_train, prop = 0.9, times = 30)\n\nWhat does validation_data look like?\n\nvalidation_data\n## # # Monte Carlo cross-validation (0.9/0.1) with 30 resamples  \n## # A tibble: 30 x 2\n##    splits           id        \n##    &lt;list&gt;           &lt;chr&gt;     \n##  1 &lt;split [411/45]&gt; Resample01\n##  2 &lt;split [411/45]&gt; Resample02\n##  3 &lt;split [411/45]&gt; Resample03\n##  4 &lt;split [411/45]&gt; Resample04\n##  5 &lt;split [411/45]&gt; Resample05\n##  6 &lt;split [411/45]&gt; Resample06\n##  7 &lt;split [411/45]&gt; Resample07\n##  8 &lt;split [411/45]&gt; Resample08\n##  9 &lt;split [411/45]&gt; Resample09\n## 10 &lt;split [411/45]&gt; Resample10\n## # … with 20 more rows\n\nLet’s look further down:\n\nvalidation_data$splits[[1]]\n## &lt;411/45/456&gt;\n\nThe first value is the number of rows of the first set, the second value of the second, and the third was the original amount of values in the training data, before splitting again.\n\n\nHow should we call these two new data sets? The author of {rsample}, Max Kuhn, talks about the analysis and the assessment sets:\n\n\n\nrsample convention for now but I intend on using it everywhere. Reusing training and testing is insane.\n\n— Max Kuhn (@topepos) November 24, 2018\n\n\n\nNow, in order to continue I need pre-process the data. I will do this in three steps. The first and the second step are used to center and scale the numeric variables and the third step converts character and factor variables to dummy variables. This is needed because I will train a random forest, which cannot handle factor variables directly. Let’s define a recipe to do that, and start by pre-processing the testing set. I write a wrapper function around the recipe, because I will need to apply this recipe to various data sets:\n\nsimple_recipe &lt;- function(dataset){\n    recipe(price ~ ., data = dataset) %&gt;%\n        step_center(all_numeric()) %&gt;%\n        step_scale(all_numeric()) %&gt;%\n        step_dummy(all_nominal())\n}\n\nOnce the recipe is defined, I can use the prep() function, which estimates the parameters from the data which are needed to process the data. For example, for centering, prep() estimates the mean which will then be subtracted from the variables. With bake() the estimates are then applied on the data:\n\ntesting_rec &lt;- prep(simple_recipe(housing_test), testing = housing_test)\n\ntest_data &lt;- bake(testing_rec, newdata = housing_test)\n## Warning: Please use `new_data` instead of `newdata` with `bake`. \n## In recipes versions &gt;= 0.1.4, this will cause an error.\n\nIt is important to split the data before using prep() and bake(), because if not, you will use observations from the test set in the prep() step, and thus introduce knowledge from the test set into the training data. This is called data leakage, and must be avoided. This is why it is necessary to first split the training data into an analysis and an assessment set, and then also pre-process these sets separately. However, the validation_data object cannot now be used with recipe(), because it is not a dataframe. No worries, I simply need to write a function that extracts the analysis and assessment sets from the validation_data object, applies the pre-processing, trains the model, and returns the RMSE. This will be a big function, at the center of the analysis.\n\n\nBut before that, let’s run a simple linear regression, as a benchmark. For the linear regression, I will not use any CV, so let’s pre-process the training set:\n\ntrainlm_rec &lt;- prep(simple_recipe(housing_train), testing = housing_train)\n\ntrainlm_data &lt;- bake(trainlm_rec, newdata = housing_train)\n## Warning: Please use `new_data` instead of `newdata` with `bake`. \n## In recipes versions &gt;= 0.1.4, this will cause an error.\nlinreg_model &lt;- lm(price ~ ., data = trainlm_data)\n\nbroom::augment(linreg_model, newdata = test_data) %&gt;% \n    rmse(price, .fitted)\n## # A tibble: 1 x 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rmse    standard       0.438\n\nbroom::augment() adds the predictions to the test_data in a new column, .fitted. I won’t use this trick with the random forest, because there is no augment() method for random forests from the {ranger} which I’ll use. I’ll add the predictions to the data myself.\n\n\nOk, now let’s go back to the random forest and write the big function:\n\nmy_rf &lt;- function(mtry, trees, split, id){\n    \n    analysis_set &lt;- analysis(split)\n    \n    analysis_prep &lt;- prep(simple_recipe(analysis_set), training = analysis_set)\n    \n    analysis_processed &lt;- bake(analysis_prep, newdata = analysis_set)\n    \n    model &lt;- rand_forest(mtry = mtry, trees = trees) %&gt;%\n        set_engine(\"ranger\", importance = 'impurity') %&gt;%\n        fit(price ~ ., data = analysis_processed)\n\n    assessment_set &lt;- assessment(split)\n    \n    assessment_prep &lt;- prep(simple_recipe(assessment_set), testing = assessment_set)\n    \n    assessment_processed &lt;- bake(assessment_prep, newdata = assessment_set)\n\n    tibble::tibble(\"id\" = id,\n        \"truth\" = assessment_processed$price,\n        \"prediction\" = unlist(predict(model, new_data = assessment_processed)))\n}\n\nThe rand_forest() function is available from the {parsnip} package. This package provides an unified interface to a lot of other machine learning packages. This means that instead of having to learn the syntax of range() and randomForest() and, and… you can simply use the rand_forest() function and change the engine argument to the one you want (ranger, randomForest, etc).\n\n\nLet’s try this function:\n\nresults_example &lt;- map2_df(.x = validation_data$splits,\n                           .y = validation_data$id,\n                           ~my_rf(mtry = 3, trees = 200, split = .x, id = .y))\nhead(results_example)\n## # A tibble: 6 x 3\n##   id           truth prediction\n##   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Resample01  0.0235    -0.104 \n## 2 Resample01 -0.135     -0.0906\n## 3 Resample01 -0.378     -0.158 \n## 4 Resample01 -0.232      0.0623\n## 5 Resample01 -0.0859     0.0173\n## 6 Resample01  0.169      0.303\n\nI can now compute the RMSE when mtry = 3 and trees = 200:\n\nresults_example %&gt;%\n    group_by(id) %&gt;%\n    rmse(truth, prediction) %&gt;%\n    summarise(mean_rmse = mean(.estimate)) %&gt;%\n    pull\n## [1] 0.4319164\n\nThe random forest has already lower RMSE than the linear regression. The goal now is to lower this RMSE by tuning the mtry and trees hyperparameters. For this, I will use Bayesian Optimization methods implemented in the {mlrMBO} package."
  },
  {
    "objectID": "posts/2018-11-25-tidy_cv.html#bayesian-hyperparameter-optimization",
    "href": "posts/2018-11-25-tidy_cv.html#bayesian-hyperparameter-optimization",
    "title": "A tutorial on tidy cross-validation with R",
    "section": "\nBayesian hyperparameter optimization\n",
    "text": "Bayesian hyperparameter optimization\n\n\nI will re-use the code from above, and define a function that does everything from pre-processing to returning the metric I want to minimize by tuning the hyperparameters, the RMSE:\n\ntuning &lt;- function(param, validation_data){\n\n    mtry &lt;- param[1]\n    trees &lt;- param[2]\n\n    results &lt;- purrr::map2_df(.x = validation_data$splits,\n                       .y = validation_data$id,\n                       ~my_rf(mtry = mtry, trees = trees, split = .x, id = .y))\n\n    results %&gt;%\n        group_by(id) %&gt;%\n        rmse(truth, prediction) %&gt;%\n        summarise(mean_rmse = mean(.estimate)) %&gt;%\n        pull\n}\n\nThis is exactly the code from before, but it now returns the RMSE. Let’s try the function with the values from before:\n\ntuning(c(3, 200), validation_data)\n## [1] 0.4330951\n\nLet’s also plot the value of RMSE for mtry = 3 and trees from 200 to 300. This takes some time, because I need to evaluate this costly function 100 times. If evaluating the function was cheap, I could have made a 3D plot by varying values of mtry too, but then again if evaluating the function was cheap, I would run an exhaustive grid search to find the hyperparameters instead of using Bayesian optimization.\n\nplot_points &lt;- crossing(\"mtry\" = 3, \"trees\" = seq(200, 300))\n\nplot_data &lt;- plot_points %&gt;% \n    mutate(value = map_dbl(seq(200, 300), ~tuning(c(3, .), validation_data)))\nplot_data %&gt;% \n    ggplot(aes(y = value, x = trees)) + \n    geom_line(colour = \"#82518c\") + \n    theme_blog() +\n    ggtitle(\"RMSE for mtry = 3\")\n\n\n\n\nFor mtry = 3 the minimum seems to lie around 255. The function to minimize is not smooth at all.\n\n\nI now follow the code that can be found in the arxiv paper to run the optimization. I think I got the gist of the paper, but I did not understand everything yet. For now, I am still experimenting with the library at the moment, but from what I understand, a simpler model, called the surrogate model, is used to look for promising points and to evaluate the value of the function at these points. This seems somewhat similar (in spirit) to the Indirect Inference method as described in Gourieroux, Monfort, Renault.\n\n\nLet’s first load the package and create the function to optimize:\n\nlibrary(\"mlrMBO\")\nfn &lt;- makeSingleObjectiveFunction(name = \"tuning\",\n                                 fn = tuning,\n                                 par.set = makeParamSet(makeIntegerParam(\"x1\", lower = 3, upper = 8),\n                                                        makeIntegerParam(\"x2\", lower = 50, upper = 500)))\n\nThis function is based on the function I defined before. The parameters to optimize are also defined as are their bounds. I will look for mtry between the values of 3 and 8, and trees between 50 and 500.\n\n\nNow comes the part I didn’t quite get.\n\n# Create initial random Latin Hypercube Design of 10 points\nlibrary(lhs)# for randomLHS\ndes &lt;- generateDesign(n = 5L * 2L, getParamSet(fn), fun = randomLHS)\n\nI think this means that these 10 points are the points used to start the whole process. I did not understand why they have to be sampled from a hypercube, but ok. Then I choose the surrogate model, a random forest too, and predict the standard error. Here also, I did not quite get why the standard error can be an option.\n\n# Specify kriging model with standard error estimation\nsurrogate &lt;- makeLearner(\"regr.ranger\", predict.type = \"se\", keep.inbag = TRUE)\n\nHere I define some options:\n\n# Set general controls\nctrl &lt;- makeMBOControl()\nctrl &lt;- setMBOControlTermination(ctrl, iters = 10L)\nctrl &lt;- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())\n\nAnd this is the optimization part:\n\n# Start optimization\nresult &lt;- mbo(fn, des, surrogate, ctrl, more.args = list(\"validation_data\" = validation_data))\nresult\n## Recommended parameters:\n## x1=6; x2=381\n## Objective: y = 0.393\n## \n## Optimization path\n## 10 + 10 entries in total, displaying last 10 (or less):\n##    x1  x2         y dob eol error.message exec.time            ei\n## 11  6 370 0.3943479   1  NA          &lt;NA&gt;     8.913 -3.134568e-05\n## 12  6 362 0.3950402   2  NA          &lt;NA&gt;     8.844 -2.987934e-05\n## 13  6 373 0.3939587   3  NA          &lt;NA&gt;     8.939 -2.259674e-05\n## 14  6 394 0.3962875   4  NA          &lt;NA&gt;     9.342 -7.427682e-06\n## 15  6 368 0.3944954   5  NA          &lt;NA&gt;     8.760 -4.121337e-06\n## 16  6 378 0.3938796   6  NA          &lt;NA&gt;     8.949 -4.503591e-07\n## 17  6 381 0.3934176   7  NA          &lt;NA&gt;     9.109 -1.141853e-06\n## 18  6 380 0.3948077   8  NA          &lt;NA&gt;     9.026 -4.718394e-08\n## 19  6 381 0.3932636   9  NA          &lt;NA&gt;     9.022 -9.801395e-08\n## 20  6 383 0.3953004  10  NA          &lt;NA&gt;     9.184 -1.579619e-09\n##    error.model train.time prop.type propose.time           se      mean\n## 11        &lt;NA&gt;      0.014 infill_ei        0.449 0.0010924600 0.3955131\n## 12        &lt;NA&gt;      0.012 infill_ei        0.458 0.0007415920 0.3948705\n## 13        &lt;NA&gt;      0.012 infill_ei        0.460 0.0006116756 0.3947185\n## 14        &lt;NA&gt;      0.012 infill_ei        0.729 0.0003104694 0.3943572\n## 15        &lt;NA&gt;      0.023 infill_ei        0.444 0.0003446061 0.3945085\n## 16        &lt;NA&gt;      0.013 infill_ei        0.458 0.0002381887 0.3944642\n## 17        &lt;NA&gt;      0.013 infill_ei        0.492 0.0002106454 0.3943200\n## 18        &lt;NA&gt;      0.013 infill_ei        0.516 0.0002093524 0.3940764\n## 19        &lt;NA&gt;      0.014 infill_ei        0.756 0.0002481260 0.3941597\n## 20        &lt;NA&gt;      0.013 infill_ei        0.483 0.0001687982 0.3939285\n\nSo the recommended parameters are 6 for mtry and 381 for trees. The value of the RMSE is lower than before, and equals 0.393. Let’s now train the random forest on the training data with this values. First, I pre-process the training data:\n\ntraining_rec &lt;- prep(simple_recipe(housing_train), testing = housing_train)\n\ntrain_data &lt;- bake(training_rec, newdata = housing_train)\n## Warning: Please use `new_data` instead of `newdata` with `bake`. \n## In recipes versions &gt;= 0.1.4, this will cause an error.\n\nLet’s now train our final model and predict the prices:\n\nfinal_model &lt;- rand_forest(mtry = 6, trees = 381) %&gt;%\n        set_engine(\"ranger\", importance = 'impurity') %&gt;%\n        fit(price ~ ., data = train_data)\n\nprice_predict &lt;- predict(final_model, new_data = select(test_data, -price))\n\nLet’s transform the data back and compare the predicted prices to the true ones visually:\n\ncbind(price_predict * sd(housing_train$price) + mean(housing_train$price), \n      housing_test$price)\n##        .pred housing_test$price\n## 1  34.811111               34.7\n## 2  20.591304               22.9\n## 3  19.463920               18.9\n## 4  20.321990               21.7\n## 5  19.063132               17.5\n## 6  15.969125               14.5\n## 7  18.203023               15.6\n## 8  17.139943               13.9\n## 9  21.393329               24.2\n## 10 27.508482               25.0\n## 11 24.030162               24.1\n## 12 21.222857               21.2\n## 13 23.052677               22.2\n## 14 20.303233               19.3\n## 15 21.134554               21.7\n## 16 22.913097               18.5\n## 17 20.029506               18.8\n## 18 18.045923               16.2\n## 19 17.321006               13.3\n## 20 18.201785               13.4\n## 21 29.928316               32.5\n## 22 24.339983               26.4\n## 23 45.518316               42.3\n## 24 29.551251               26.7\n## 25 26.513473               30.1\n## 26 42.984738               46.7\n## 27 43.513001               48.3\n## 28 25.436146               23.3\n## 29 21.766247               24.3\n## 30 36.328740               36.0\n## 31 32.830061               31.0\n## 32 38.736098               35.2\n## 33 31.573311               32.0\n## 34 19.847848               19.4\n## 35 23.401032               23.1\n## 36 22.000914               19.4\n## 37 20.155696               18.7\n## 38 21.342003               22.6\n## 39 20.846330               19.9\n## 40 13.752108               13.8\n## 41 12.499064               13.1\n## 42 15.019987               16.3\n## 43  8.489851                7.2\n## 44  7.803981               10.4\n## 45 18.629488               20.8\n## 46 14.645669               14.3\n## 47 15.094423               15.2\n## 48 20.470057               17.7\n## 49 15.147170               13.3\n## 50 15.880035               13.6\n\nLet’s now compute the RMSE:\n\ntibble::tibble(\"truth\" = test_data$price,\n        \"prediction\" = unlist(price_predict)) %&gt;% \n    rmse(truth, prediction)\n## # A tibble: 1 x 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rmse    standard       0.327\n\nVery nice."
  },
  {
    "objectID": "posts/2017-03-27-introducing_brotools.html",
    "href": "posts/2017-03-27-introducing_brotools.html",
    "title": "Introducing brotools",
    "section": "",
    "text": "I’m happy to announce my first R package, called brotools. This is a package that contains functions that are specific to my needs but that you might find also useful. I blogged about some of these functions, so if you follow my blog you might already be familiar with some of them. It is not on CRAN and might very well never be. The code is hosted on bitbucket and you can install the package with\n\ndevtools::install_bitbucket(\"b-rodrigues/brotools\")\n\nHope you’ll find the brotools useful!"
  },
  {
    "objectID": "posts/2024-04-04-nix_for_r_part_11.html#intro",
    "href": "posts/2024-04-04-nix_for_r_part_11.html#intro",
    "title": "Reproducible data science with Nix, part 11 – build and cache binaries with Github Actions and Cachix",
    "section": "\nIntro\n",
    "text": "Intro\n\n\nI have this package on CRAN called {chronicler} and last month I got an email from CRAN telling me that building the package was failing, and I had two weeks to fix it.\n\n\nI immediately thought that some dependency that my package depends on got updated, and somehow broke something. But when I checked the results of the build, I was surprised, to say the least:\n\n\n\n\n\n\n\nHow come my package was only failing on Fedora? Now that was really weird. There was no way this was right. Also, I couldn’t reproduce this bug on my local machine… but I could reproduce it on Github Actions, on Ubuntu (but it was ok on CRAN’s Debian which is really close to Ubuntu!), but couldn’t reproduce it either on Windows! What was going on? So I started digging, and my first idea was to look at the list of packages that got released on CRAN on that day (March 12th 2024) or just before, and saw something that caught my eye: a new version of {tidyselect} had just been released and even though my package doesn’t directly depend on it, I knew that this package was likely a dependency of some direct dependency of {chronicler}. So I looked into the release notes, and there it was:\n\n* `eval_select()` out-of-bounds errors now use the verb \"select\" rather than\n  \"subset\" in the error message for consistency with `dplyr::select()` (#271).\n\nI knew this was what I was looking for, because the unit test that was failing to pass was a test that should error because dplyr::select() was being used on a column that didn’t exist. So the success of that test was defined as finding the following error message in the log, which contained the word subset but now it should be select.\n\n\nBut why was this failing only on Fedora on CRAN and on Ubuntu on Github Actions (but ok on Debian on CRAN)? And why couldn’t I reproduce the bug on my OpenSuse Linux computer, even though I was building a bleeding edge development environment using Nix?\n\n\nAnd then it hit me like my older brother used to.\n\n\nWhen building packages, CRAN doesn’t seem to use pre-compiled binaries on Fedora, so packages get built from source. This means that it takes longer to test on Fedora, as packages have to be built from source, but it also means that only the very latest releases of packages get used. On other platforms, pre-compiled binaries get used if available, and because {tidyselect} had just come out that very day, older binaries of {tidyselect} were being used on these platforms, but not on Fedora. And because these older binaries didn’t include this change, the unit test was still passing successfully on there.\n\n\nOn Github Actions, code coverage was computed using covr::codecov() which installs the package in a temporary directory and seems to pull its dependencies directly from CRAN. Because CRAN doesn’t offer Linux binaries packages got compiled from source, hence why the test was failing there, as the very latest version of {tidyselect} was being used (btw, use Dirk Eddelbuettel’s r2u if you binaries for Ubuntu).\n\n\nAnd on my local machine, even though I was using the latest commit of nixpkgs to have the most bleeding edge packages for my environment, I had forgotten that the R packages on nixpkgs always lag behind the CRAN releases.\n\n\nThis is because R packages on nixpkgs tend to get updated alongside a new release of R, and the reason is to ensure a certain level of quality. You see, the vast majority of CRAN (and Bioconductor) packages are made available through nixpkgs in a fully automated way. But some packages do require some manual intervention to work on Nix. And we only know this if we try to build these packages, but building packages requires quite a lot of resources. I go into more detail here, but in summary we can’t build CRAN packages every single day to see if everything works well, so we only rebuild the whole tree whenever there’s a new release of R. Packages get built on a CI infrastructure called Hydra, and then get cached on cache.nixos.org so whenever someone wants to install a package, a pre-built binary gets pulled from the cache instead of getting installed from source. For packages that don’t need compiling this is not that big of a time save, but for packages that do need to get compiled it is huge. Depending on which packages you want to install, if you had to build everything from source, it could potentially take hours, but if you can install pre-built binaries it’s just a matter of how quick your internet connection is.\n\n\nAnyways, I went back to my fork of nixpkgs and updated the expression defining the CRAN packages myself and installed the latest versions of packages from my fork.\n\n\nBefore the update, this was the error message I was testing against:\n\n\n\n\n\n\n\nand this was on version 1.2.0 of {tidyselect}:\n\n\n\n\n\n\n\nbut after the update, this was the error message:\n\n\n\n\n\n\n\non version 1.2.1 of {tidyselect}:\n\n\n\n\n\n\n\nso I found the issue, and updated my unit testing accordingly, and pushed the update to CRAN. All is well that ends well, but… this made me think. I needed to have an easy way to have bleeding edge packages on hand from Nix at all moments, and so I started working on it."
  },
  {
    "objectID": "posts/2024-04-04-nix_for_r_part_11.html#github-actions-to-the-rescue",
    "href": "posts/2024-04-04-nix_for_r_part_11.html#github-actions-to-the-rescue",
    "title": "Reproducible data science with Nix, part 11 – build and cache binaries with Github Actions and Cachix",
    "section": "\nGithub Actions to the rescue\n",
    "text": "Github Actions to the rescue\n\n\nAs described in my previous blog post updating the Nix expressions defining the R packages on nixpkgs involves running an R script that generates a Nix expression which then builds the R packages when needed. So what I did was create a Github actions that would run this R script every 6 hours, and push the changes to a branch of my nixpkgs fork. This way, I would always have the possibility to use this branch if I needed bleeding edge packages. Because this can be of interest to others, Philipp Baumann started a Github organisation hosting this fork of nixpkgs that gets updated daily which you can find here. Because this action needs to run several times a day, it should be on a schedule, but actions on a schedule can only run from master/main. But that’s not what we wanted, so instead, we are using another action, on another repository, that pushes a random file to the target repository to get the action going. You can find this repository here with complete instructions. So to summarise:\n\n\n\nAn action on schedule runs from b-rodrigues/trigger-r-updates and pushes a file to rstats-on-nix/nixpkgs on the r-daily-source branch\n\n\nThis triggers an action that updates all of nixpkgs, including R packages, and pushes all the updates to the r-daily branch (you can find it here)\n\n\nWe can now use the r-daily branch to get bleeding edge R packages on Nix!\n\n\n\nThis happens without any form of testing though, so packages could be in a broken state (hey, that’s the definition of bleeding edge, after all!), and also, if anyone would like to use this fork to build a development environment, they’d have to rebuild a lot of packages from source. Again, this is because these packages are defined in a fork of nixpkgs and they don’t get built on Hydra to populate the public cache that Nix uses by default. So while this fork is interesting because it provides bleeding edges packages, using it on a day-to-day basis can be quite tedious.\n\n\nAnd this is where Cachix comes into play."
  },
  {
    "objectID": "posts/2024-04-04-nix_for_r_part_11.html#setting-up-your-own-binary-cache-on-cachix",
    "href": "posts/2024-04-04-nix_for_r_part_11.html#setting-up-your-own-binary-cache-on-cachix",
    "title": "Reproducible data science with Nix, part 11 – build and cache binaries with Github Actions and Cachix",
    "section": "\nSetting up your own binary cache on Cachix\n",
    "text": "Setting up your own binary cache on Cachix\n\n\nCachix is an amazing tool that makes it incredibly easy to set up your own cache. Simply build the packages once, and push the binaries to the cache. As long as these packages don’t get updated, they’ll get pulled from the cache instead of getting rebuilt.\n\n\nSo now, here is what I do with my packages: I define a default.nix file that defines a development environment that uses my fork of nixpkgs as the source for packages. For example, here is this file that defines the environment for my {rix} package. I can use this environment to work on my package, and make sure that anyone else that wants to contribute, contributes using the same environment. As you can see on line 2, the rstats-on-nix bleeding edge fork gets used:\n\n pkgs = import (fetchTarball \"https://github.com/rstats-on-nix/nixpkgs/archive/refs/heads/r-daily.tar.gz\") {};\n\nThen, still on {rix}’s repository, I define a new action that builds this environment periodically, but using the binary cache I set up with Cachix. You can find this action here. So the r-daily branch of our nixpkgs fork gets updated every 6 hour and this environment gets updated every 12 hours, 30 minutes past the hour.\n\n\nNow, every time I want to work on my package, I simply use nix-build on my computer to update the development environment. This is what I see:\n\ncopying path '/nix/store/0l0iw4hz7xvykvhsjg8nqkvyl31js96l-r-stringr-1.5.1' from 'https://b-rodrigues.cachix.org'...\ncopying path '/nix/store/cw3lc7b0zydsricl5155jbmldm1vcyvr-r-tibble-3.2.1' from 'https://b-rodrigues.cachix.org'...\ncopying path '/nix/store/y32kpp09l34cdgksnr89cyvz6p5s94z8-r-tidyselect-1.2.1' from 'https://b-rodrigues.cachix.org'...\ncopying path '/nix/store/sw24yx1jwy9xzq8ai5m2gzaamvyi5r0h-r-rematch2-2.1.2' from 'https://b-rodrigues.cachix.org'...\ncopying path '/nix/store/z6b4vii7hvl9mc53ykxrwks1lkfzgmr4-r-dplyr-1.1.4' from 'https://b-rodrigues.cachix.org'...\n\nas you can see, packages get pulled from my cache. Packages that are already available from the usual, public, cache.nixos.org don’t get rebuilt nor cached in mine; they simply continue getting pulled directly from there. This makes using the development environment very easy, and guarantees I’m always mirroring the state of packages released on CRAN. The other interesting thing is that I can use that cache with other actions. For example, here is the action that runs the unit tests included in the package in an environment that has Nix installed on it (some unit tests need Nix to be available to run). On line 25 you can see that we install Nix and set our fork as the repository to use:\n\nnix_path: nixpkgs=https://github.com/rstats-on-nix/nixpkgs/archive/refs/heads/r-daily.tar.gz\n\nand just below, we set up the cache:\n\n- uses: cachix/cachix-action@v14\n  with:\n    name: b-rodrigues # this is the name of my cache\n\nBy using my cache, I make sure that the test runs with the freshest possible packages, and don’t run the risk of having a test succeed on an outdated environment. And you might have noticed that I am not authenticating to Cachix: to simply pull binaries, to authentication is needed!\n\n\nCachix has a free plan of up to 5Gb which is more than enough to set up several development environments like this, and is really, really, easy to set up, and it works on your computer and on Github Actions, as shown. If you want to use this development environment to contribute to {rix}, check out the instructions on Contributing.md file.\n\n\nYou can use the same approach to always have development environments ready for your different projects, and I will likely add the possibility to use this fork of nixpkgs with my {rix} package.\n\n\nThanks to Philipp Baumann for nudging me into the direction of using Cachix and showing the way!"
  },
  {
    "objectID": "posts/2023-09-29-voyager.html",
    "href": "posts/2023-09-29-voyager.html",
    "title": "ZSA Voyager review",
    "section": "",
    "text": "Now for something completely different than our usual programming: today I’m sharing my thoughts on the latest ZSA mechanical keyboard, the Voyager. First things first: this is in no way shape or form sponsored by ZSA. But Erez, if you’d like to send me money you’re more than welcome.\n\n\nHere’s what the keyboard looks like:\n\n\n\n\nYour browser does not support the video tag. \n\n\nYes, it comes with RGB LEDs. Why do mechanical keyboards come with RGB LEDs? No idea, I usually don’t care for them, but unlike other keyboards from ZSA, you cannot order the Voyager without them. So now my keyboard looks like a Christmas tree. And by the way, yes, you can get the good old regular QWERTY layout instead of the dots. I chose to get blank keys because I don’t look at my keyboard when typing.\n\n\nIt’s quite small and there aren’t many keys on it. But it’s very comfortable to use. I’ll try to explain why.\n\n\nIf you don’t know anything about mechanical keyboards, I think you might find this blog post useful. I’ll explain the basics and also why you might want to consider one if you’re a programmer.\n\n\nFirst of all, let me just get this out of the way: typing on a mechanical keyboard will not make you type any faster. I think that people that buy mechanical keyboards also tend to be people that spend some time learning how to touch-type, so yeah, they’ll type faster than most people that never bother to learn to touch-type, but two touch-typists, one that use a mechanical keyboard and another that uses a normal keyboard, will roughly type at the same speed.\n\n\nSo if not for speed, why bother with mechanical keyboards?\n\n\nIn my opinion, the main advantage of mechanical keyboards is customization. You can customize absolutely everything: not just how the keyboard looks, but also how it works. Many mechanical keyboards come with a firmware called QMK which enables you to program each key. So for instance I have a key that types “&lt;-” and another that types “%&gt;%”, very useful for an R programmer like myself. You can configure such things at the level of you favourite text editor, but it’s nice to also have the option at the level of the hardware, because it means that you can now easily type these programming symbols anywhere: on social media, an email, a forum… Configuring this firmware on keyboards made by ZSA, like the Voyager, is incredibly easy: there’s a web-application called Oryx that you can use for all they keyboards. Simply select the keys, change what you must and flash the new firmware to your keyboard! For example here, I’m configuring a key to output “,” when pressed, but to output “_” when double-tapped:\n\n\n\n\n\n\n\nAnd for the flashing process you don’t even have to install anything on your computer: if you’re using a Chromium based browser like Google Chrome, you can flash it from the Web Browser. You can even browse other people’s configurations, for example here’s mine (and you can even customize the RGB).\n\n\nI use the French ergonomic BÉPO layout, the English equivalent would be Dvorak. You can add different layers, for example by holding one key, all the other keys now output something different when pressed (like holding down the SHIFT key produces capital letters), but you can make any key switch layers and then any other key output anything. For example I have a layer in which I configured keys to move my mouse and click. I don’t use that very often, but in case I forget my mouse if I’m traveling, I could also use my keyboard as a mouse now.\n\n\nHardware can also be customized: the color of the keyboard, but also the keycaps (I have the blank ones, as you’ve seen above) and also the switches. If you’re not into mechanical keyboard I guess this doesn’t mean anything. Keycaps are these:\n\n\n\n\n\n\n\nand switches are these:\n\n\n\n\n\n\n\nAnd you can change either the caps, the switches or both. The keyboard is hot-swapable meaning that you can actually replace the switches. Here is a switch with a keycap on it that I removed from my keyboard:\n\n\n\n\nYour browser does not support the video tag. \n\n\nAgain, if you’re not into mechanical keyboard it’s difficult to see why this is really a nice thing: but being able to change caps and switches allows you to truly make the keyboard feel and sound just right for you.\n\n\nLet me explain: there’s switches that make no sound and that are very easy to press: they’re called linear switches. Then there’s switches that make a nice clicky sound and that require more force to press, and there’s switches that make even more noise and that require a lot of force to press. The harder ones are so-called “clicky” switches and the intermediate ones “tactile”. There’s a lot more subtlety than that, but even I don’t know everything about switches. What matters is that you can swap these, and find the ones that are just right for you. My first mechanical keyboard, also one from ZSA, the Ergodox EZ (pictured below) came with red switches. At the time, I had no idea what switches I should get, so I bought the reds because they were silent, and I figured that I would prefer silent ones. Turns out that I absolutely hated them. It didn’t fill right because they were extremely light, and simply by resting my hands on the keyboard I would press keys by mistake. Then I bought clicky switches, and since then haven’t looked back. Clicky switches make a nice “click” sound when you press them, because there’s actually a little mechanism that produces this noise when you press them. It’s like pushing an actual button. Much more satisfying, and much better, in my opinion, for typing. So for this board I got the white ones, which are the clickiest. It’s also the one’s I had for my other mechanical keyboard, the Planck EZ, also by ZSA:\n\n\n\n\n\n\n\nI also experimented with heavier ones on my other board (an Idobao ID75, a somewhat overgrown Planck, not by ZSA but also very customizable through VIAL):\n\n\n\n\n\n\n\nThe switches there are heavier, and I enjoy them a lot as well.\n\n\nNow, this keyboard isn’t cheap, but it does come with a lot of nice stuff in the box. You get 3 usb cables, 4 more switches, several keycaps more, and a carrying bag.\n\n\nAnd as you can see, it’s a so-called low profile keyboard:\n\n\n\n\n\n\n\nYou can even remove these little feet from the keyboard (they’re magnetic):\n\n\n\n\n\n\n\nto get it even lower:\n\n\n\n\n\n\n\nI’ve never had such a keyboard in the past and I must say that it’s really comfortable to use. I don’t need to use any wrist rests anymore, which is kinda nice. Because it’s low-profile the switches and keycaps are different from the usual ones you get for other mechanical keyboards:\n\n\n\n\n\n\n\nAnyways, I really enjoy this form factor, not just that it’s low profile, but also that it doesn’t have a lot of keys. I like this, because my hands don’t need to move at all. If I need numbers for example, I switch layers, and now the keys that would usually be directly under my fingers will output numbers when pressed. So instead of my fingers going to the keys, they keys go to my fingers. It gets some time to get used to this, but once you know how to do that, it’s just great.\n\n\nSo, should you buy a Voyager? I might not advise it to you for a first mechanical keyboard. There’s much cheaper ones that you can get and see if mechanical keyboards are for you. If you can, try some out in a store, I think it’s especially important to find the right switches for your style. As I’ve written above, I started with linear reds which I hated, thankfully I tried clicky whites before abandoning my mechanical keyboard adventure. If you’re already a hardened mechanical keyboard user, and are looking for a light keyboard that you can take with you on your travels, I think that it’s hard to overlook the Voyager. There are other nice, very transportable keyboards out there, but the build quality of ZSA and the firmware customization tool they provide, Oryx, is hard to beat."
  },
  {
    "objectID": "posts/2019-03-03-historical_vowpal.html",
    "href": "posts/2019-03-03-historical_vowpal.html",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 1",
    "section": "",
    "text": "Can I get enough of historical newspapers data? Seems like I don’t. I already wrote four (1, 2, 3 and 4) blog posts, but there’s still a lot to explore. This blog post uses a new batch of data announced on twitter:\nand this data could not have arrived at a better moment, since something else got announced via Twitter recently:\nI wanted to try using Vowpal Wabbit for a couple of weeks now because it seems to be the perfect tool for when you’re dealing with what I call big-ish data: data that is not big data, and might fit in your RAM, but is still a PITA to deal with. It can be data that is large enough to take 30 seconds to be imported into R, and then every operation on it lasts for minutes, and estimating/training a model on it might eat up all your RAM. Vowpal Wabbit avoids all this because it’s an online-learning system. Vowpal Wabbit is capable of training a model with data that it sees on the fly, which means VW can be used for real-time machine learning, but also for when the training data is very large. Each row of the data gets streamed into VW which updates the estimated parameters of the model (or weights) in real time. So no need to first import all the data into R!\nThe goal of this blog post is to get started with VW, and build a very simple logistic model to classify documents using the historical newspapers data from the National Library of Luxembourg, which you can download here (scroll down and download the Text Analysis Pack). The goal is not to build the best model, but a model. Several steps are needed for this: prepare the data, install VW and train a model using {RVowpalWabbit}."
  },
  {
    "objectID": "posts/2019-03-03-historical_vowpal.html#step-1-preparing-the-data",
    "href": "posts/2019-03-03-historical_vowpal.html#step-1-preparing-the-data",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 1",
    "section": "\nStep 1: Preparing the data\n",
    "text": "Step 1: Preparing the data\n\n\nThe data is in a neat .xml format, and extracting what I need will be easy. However, the input format for VW is a bit unusual; it resembles .psv files (Pipe Separated Values) but allows for more flexibility. I will not dwell much into it, but for our purposes, the file must look like this:\n\n1 | this is the first observation, which in our case will be free text\n2 | this is another observation, its label, or class, equals 2\n4 | this is another observation, of class 4\n\nThe first column, before the “|” is the target class we want to predict, and the second column contains free text.\n\n\nThe raw data looks like this:\n\n\n\n\nClick if you want to see the raw data\n\n\n&lt;OAI-PMH xmlns=\"http://www.openarchives.org/OAI/2.0/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd\"&gt;\n&lt;responseDate&gt;2019-02-28T11:13:01&lt;/responseDate&gt;\n&lt;request&gt;http://www.eluxemburgensia.lu/OAI&lt;/request&gt;\n&lt;ListRecords&gt;\n&lt;record&gt;\n&lt;header&gt;\n&lt;identifier&gt;digitool-publish:3026998-DTL45&lt;/identifier&gt;\n&lt;datestamp&gt;2019-02-28T11:13:01Z&lt;/datestamp&gt;\n&lt;/header&gt;\n&lt;metadata&gt;\n&lt;oai_dc:dc xmlns:oai_dc=\"http://www.openarchives.org/OAI/2.0/oai_dc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:dcterms=\"http://purl.org/dc/terms/\" xsi:schemaLocation=\"http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd\"&gt;\n&lt;dc:identifier&gt;\nhttps://persist.lu/ark:/70795/6gq1q1/articles/DTL45\n&lt;/dc:identifier&gt;\n&lt;dc:source&gt;newspaper/indeplux/1871-12-29_01&lt;/dc:source&gt;\n&lt;dcterms:isPartOf&gt;L'indépendance luxembourgeoise&lt;/dcterms:isPartOf&gt;\n&lt;dcterms:isReferencedBy&gt;\nissue:newspaper/indeplux/1871-12-29_01/article:DTL45\n&lt;/dcterms:isReferencedBy&gt;\n&lt;dc:date&gt;1871-12-29&lt;/dc:date&gt;\n&lt;dc:publisher&gt;Jean Joris&lt;/dc:publisher&gt;\n&lt;dc:relation&gt;3026998&lt;/dc:relation&gt;\n&lt;dcterms:hasVersion&gt;\nhttp://www.eluxemburgensia.lu/webclient/DeliveryManager?pid=3026998#panel:pp|issue:3026998|article:DTL45\n&lt;/dcterms:hasVersion&gt;\n&lt;dc:description&gt;\nCONSEIL COMMUNAL de la ville de Luxembourg. Séance du 23 décembre 1871. (Suite.) Art. 6. Glacière communale. M. le Bourgmcstr ¦ . Le collège échevinal propose un autro mode de se procurer de la glace. Nous avons dépensé 250 fr. cha- que année pour distribuer 30 kilos do glace; c’est une trop forte somme pour un résultat si minime. Nous aurions voulu nous aboucher avec des fabricants de bière ou autres industriels qui nous auraient fourni de la glace en cas de besoin. L’architecte qui été chargé de passer un contrat, a été trouver des négociants, mais ses démarches n’ont pas abouti. \n&lt;/dc:description&gt;\n&lt;dc:title&gt;\nCONSEIL COMMUNAL de la ville de Luxembourg. Séance du 23 décembre 1871. (Suite.)\n&lt;/dc:title&gt;\n&lt;dc:type&gt;ARTICLE&lt;/dc:type&gt;\n&lt;dc:language&gt;fr&lt;/dc:language&gt;\n&lt;dcterms:extent&gt;863&lt;/dcterms:extent&gt;\n&lt;/oai_dc:dc&gt;\n&lt;/metadata&gt;\n&lt;/record&gt;\n&lt;/ListRecords&gt;\n&lt;/OAI-PMH&gt;\n\n\nI need several things from this file:\n\n\n\nThe title of the newspaper: &lt;dcterms:isPartOf&gt;L’indépendance luxembourgeoise&lt;/dcterms:isPartOf&gt;\n\n\nThe type of the article: &lt;dc:type&gt;ARTICLE&lt;/dc:type&gt;. Can be Article, Advertisement, Issue, Section or Other.\n\n\nThe contents: &lt;dc:description&gt;CONSEIL COMMUNAL de la ville de Luxembourg. Séance du ….&lt;/dc:description&gt;\n\n\n\nI will only focus on newspapers in French, even though newspapers in German also had articles in French. This is because the tag &lt;dc:language&gt;fr&lt;/dc:language&gt; is not always available. If it were, I could simply look for it and extract all the content in French easily, but unfortunately this is not the case.\n\n\nFirst of all, let’s get the data into R:\n\nlibrary(\"tidyverse\")\nlibrary(\"xml2\")\nlibrary(\"furrr\")\n\nfiles &lt;- list.files(path = \"export01-newspapers1841-1878/\", all.files = TRUE, recursive = TRUE)\n\nThis results in a character vector with the path to all the files:\n\nhead(files)\n[1] \"000/1400000/1400000-ADVERTISEMENT-DTL78.xml\"   \"000/1400000/1400000-ADVERTISEMENT-DTL79.xml\"  \n[3] \"000/1400000/1400000-ADVERTISEMENT-DTL80.xml\"   \"000/1400000/1400000-ADVERTISEMENT-DTL81.xml\"  \n[5] \"000/1400000/1400000-MODSMD_ARTICLE1-DTL34.xml\" \"000/1400000/1400000-MODSMD_ARTICLE2-DTL35.xml\"\n\nNow I write a function that does the needed data preparation steps. I describe what the function does in the comments inside:\n\nto_vw &lt;- function(xml_file){\n\n    # read in the xml file\n    file &lt;- read_xml(paste0(\"export01-newspapers1841-1878/\", xml_file))\n\n    # Get the newspaper\n    newspaper &lt;- xml_find_all(file, \".//dcterms:isPartOf\") %&gt;% xml_text()\n\n    # Only keep the newspapers written in French\n    if(!(newspaper %in% c(\"L'UNION.\",\n                          \"L'indépendance luxembourgeoise\",\n                          \"COURRIER DU GRAND-DUCHÉ DE LUXEMBOURG.\",\n                          \"JOURNAL DE LUXEMBOURG.\",\n                          \"L'AVENIR\",\n                          \"L’Arlequin\",\n                          \"La Gazette du Grand-Duché de Luxembourg\",\n                          \"L'AVENIR DE LUXEMBOURG\",\n                          \"L'AVENIR DU GRAND-DUCHE DE LUXEMBOURG.\",\n                          \"L'AVENIR DU GRAND-DUCHÉ DE LUXEMBOURG.\",\n                          \"Le gratis luxembourgeois\",\n                          \"Luxemburger Zeitung – Journal de Luxembourg\",\n                          \"Recueil des mémoires et des travaux publiés par la Société de Botanique du Grand-Duché de Luxembourg\"))){\n        return(NULL)\n    } else {\n        # Get the type of the content. Can be article, advert, issue, section or other\n        type &lt;- xml_find_all(file, \".//dc:type\") %&gt;% xml_text()\n\n        type &lt;- case_when(type == \"ARTICLE\" ~ \"1\",\n                          type == \"ADVERTISEMENT\" ~ \"2\",\n                          type == \"ISSUE\" ~ \"3\",\n                          type == \"SECTION\" ~ \"4\",\n                          TRUE ~ \"5\"\n        )\n\n        # Get the content itself. Only keep alphanumeric characters, and remove any line returns or \n        # carriage returns\n        description &lt;- xml_find_all(file, \".//dc:description\") %&gt;%\n            xml_text() %&gt;%\n            str_replace_all(pattern = \"[^[:alnum:][:space:]]\", \"\") %&gt;%\n            str_to_lower() %&gt;%\n            str_replace_all(\"\\r?\\n|\\r|\\n\", \" \")\n\n        # Return the final object: one line that looks like this\n        # 1 | bla bla\n        paste(type, \"|\", description)\n    }\n\n}\n\nI can now run this code to parse all the files, and I do so in parallel, thanks to the {furrr} package:\n\nplan(multiprocess, workers = 12)\n\ntext_fr &lt;- files %&gt;%\n    future_map(to_vw)\n\ntext_fr &lt;- text_fr %&gt;%\n    discard(is.null)\n\nwrite_lines(text_fr, \"text_fr.txt\")"
  },
  {
    "objectID": "posts/2019-03-03-historical_vowpal.html#step-2-install-vowpal-wabbit",
    "href": "posts/2019-03-03-historical_vowpal.html#step-2-install-vowpal-wabbit",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 1",
    "section": "\nStep 2: Install Vowpal Wabbit\n",
    "text": "Step 2: Install Vowpal Wabbit\n\n\nTo easiest way to install VW must be using Anaconda, and more specifically the conda package manager. Anaconda is a Python (and R) distribution for scientific computing and it comes with a package manager called conda which makes installing Python (or R) packages very easy. While VW is a standalone piece of software, it can also be installed by conda or pip. Instead of installing the full Anaconda distribution, you can install Miniconda, which only comes with the bare minimum: a Python executable and the conda package manager. You can find Miniconda here and once it’s installed, you can install VW with:\n\nconda install -c gwerbin vowpal-wabbit \n\nIt is also possible to install VW with pip, as detailed here, but in my experience, managing Python packages with pip is not super. It is better to manage your Python distribution through conda, because it creates environments in your home folder which are independent of the system’s Python installation, which is often out-of-date."
  },
  {
    "objectID": "posts/2019-03-03-historical_vowpal.html#step-3-building-a-model",
    "href": "posts/2019-03-03-historical_vowpal.html#step-3-building-a-model",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 1",
    "section": "\nStep 3: Building a model\n",
    "text": "Step 3: Building a model\n\n\nVowpal Wabbit can be used from the command line, but there are interfaces for Python and since a few weeks, for R. The R interface is quite crude for now, as it’s still in very early stages. I’m sure it will evolve, and perhaps a Vowpal Wabbit engine will be added to {parsnip}, which would make modeling with VW really easy.\n\n\nFor now, let’s only use 10000 lines for prototyping purposes before running the model on the whole file. Because the data is quite large, I do not want to import it into R. So I use command line tools to manipulate this data directly from my hard drive:\n\n# Prepare data\nsystem2(\"shuf\", args = \"-n 10000 text_fr.txt &gt; small.txt\")\n\nshuf is a Unix command, and as such the above code should work on GNU/Linux systems, and most likely macOS too. shuf generates random permutations of a given file to standard output. I use &gt; to direct this output to another file, which I called small.txt. The -n 10000 options simply means that I want 10000 lines.\n\n\nI then split this small file into a training and a testing set:\n\n# Adapted from http://bitsearch.blogspot.com/2009/03/bash-script-to-split-train-and-test.html\n\n# The command below counts the lines in small.txt. This is not really needed, since I know that the \n# file only has 10000 lines, but I kept it here for future reference\n# notice the stdout = TRUE option. This is needed because the output simply gets shown in R's\n# command line and does get saved into a variable.\nnb_lines &lt;- system2(\"cat\", args = \"small.txt | wc -l\", stdout = TRUE)\n\nsystem2(\"split\", args = paste0(\"-l\", as.numeric(nb_lines)*0.99, \" small.txt data_split/\"))\n\nsplit is the Unix command that does the splitting. I keep 99% of the lines in the training set and 1% in the test set. This creates two files, aa and ab. I rename them using the mv Unix command:\n\nsystem2(\"mv\", args = \"data_split/aa data_split/small_train.txt\")\nsystem2(\"mv\", args = \"data_split/ab data_split/small_test.txt\")\n\nOk, now let’s run a model using the VW command line utility from R, using system2():\n\noaa_fit &lt;- system2(\"~/miniconda3/bin/vw\", args = \"--oaa 5 -d data_split/small_train.txt -f small_oaa.model\", stderr = TRUE)\n\nI need to point system2() to the vw executable, and then add some options. –oaa stands for one against all and is a way of doing multiclass classification; first, one class gets classified by a logistic classifier against all the others, then the other class against all the others, then the other…. The 5 in the option means that there are 5 classes.\n\n\n-d data_split/train.txt specifies the path to the training data. -f means “final regressor” and specifies where you want to save the trained model.\n\n\nThis is the output that get’s captured and saved into oaa_fit:\n\n [1] \"final_regressor = oaa.model\"                                             \n [2] \"Num weight bits = 18\"                                                    \n [3] \"learning rate = 0.5\"                                                     \n [4] \"initial_t = 0\"                                                           \n [5] \"power_t = 0.5\"                                                           \n [6] \"using no cache\"                                                          \n [7] \"Reading datafile = data_split/train.txt\"                                 \n [8] \"num sources = 1\"                                                         \n [9] \"average  since         example        example  current  current  current\"\n[10] \"loss     last          counter         weight    label  predict features\"\n[11] \"1.000000 1.000000            1            1.0        3        1       87\"\n[12] \"1.000000 1.000000            2            2.0        1        3     2951\"\n[13] \"1.000000 1.000000            4            4.0        1        3      506\"\n[14] \"0.625000 0.250000            8            8.0        1        1      262\"\n[15] \"0.625000 0.625000           16           16.0        1        2      926\"\n[16] \"0.500000 0.375000           32           32.0        4        1        3\"\n[17] \"0.375000 0.250000           64           64.0        1        1      436\"\n[18] \"0.296875 0.218750          128          128.0        2        2      277\"\n[19] \"0.238281 0.179688          256          256.0        2        2      118\"\n[20] \"0.158203 0.078125          512          512.0        2        2       61\"\n[21] \"0.125000 0.091797         1024         1024.0        2        2      258\"\n[22] \"0.096191 0.067383         2048         2048.0        1        1       45\"\n[23] \"0.085205 0.074219         4096         4096.0        1        1      318\"\n[24] \"0.076172 0.067139         8192         8192.0        2        1      523\"\n[25] \"\"                                                                        \n[26] \"finished run\"                                                            \n[27] \"number of examples = 9900\"                                               \n[28] \"weighted example sum = 9900.000000\"                                      \n[29] \"weighted label sum = 0.000000\"                                           \n[30] \"average loss = 0.073434\"                                                 \n[31] \"total feature number = 4456798\"  \n\nNow, when I try to run the same model using RVowpalWabbit::vw() I get the following error:\n\noaa_class &lt;- c(\"--oaa\", \"5\",\n               \"-d\", \"data_split/small_train.txt\",\n               \"-f\", \"vw_models/small_oaa.model\")\n\nresult &lt;- vw(oaa_class)\nError in Rvw(args) : unrecognised option '--oaa'\n\nI think the problem might be because I installed Vowpal Wabbit using conda, and the package cannot find the executable. I’ll open an issue with reproducible code and we’ll see.\n\n\nIn any case, that’s it for now! In the next blog post, we’ll see how to get the accuracy of this very simple model, and see how to improve it!"
  },
  {
    "objectID": "posts/2015-02-22-export-r-output-to-file.html",
    "href": "posts/2015-02-22-export-r-output-to-file.html",
    "title": "Export R output to a file",
    "section": "",
    "text": "Sometimes it is useful to export the output of a long-running R command. For example, you might want to run a time consuming regression just before leaving work on Friday night, but would like to get the output saved inside your Dropbox folder to take a look at the results before going back to work on Monday.\n\n\nThis can be achieved very easily using capture.output() and cat() like so:\n\nout &lt;- capture.output(summary(my_very_time_consuming_regression))\n\ncat(\"My title\", out, file=\"summary_of_my_very_time_consuming_regression.txt\", sep=\"\\n\", append=TRUE)\n\nmy_very_time_consuming_regression is an object of class lm for example. I save the output of summary(my_very_time_consuming_regression) as text using capture.output and save it in a variable called out. Finally, I save out to a file called summary_of_my_very_time_consuming_regression.txt with the first sentence being My title (you can put anything there). The file summary_of_my_very_time_consuming_regression.txt doesn’t have to already exist in your working directory. The option sep=\"\" is important or else the whole output will be written in a single line. Finally, append=TRUE makes sure your file won’t be overwritten; additional output will be appended to the file, which can be nice if you want to compare different versions of your model."
  },
  {
    "objectID": "posts/2013-01-29-method-of-simulated-moments-with-r.html",
    "href": "posts/2013-01-29-method-of-simulated-moments-with-r.html",
    "title": "Method of Simulated Moments with R",
    "section": "",
    "text": "This document details section 12.5.6. Unobserved Heterogeneity Example. The original source code giving the results from table 12.3 are available from the authors' site here and written for Stata. This is an attempt to translate the code to R.\n\n\nConsult the original source code if you want to read the authors' comments. If you want the R source code without all the commentaries, grab it here. This is not guaranteed to work, nor to be correct. It could set your pet on fire and/or eat your first born. Use at your own risk. I may, or may not, expand this example. Corrections, constructive criticism are welcome.\n\n\nThe model is the same as the one described here, so I won't go into details. The moment condition used is \\(E[(y_i-\\theta-u_i)]=0\\), so we can replace the expectation operator by the empirical mean:\n\n\n\\[\\dfrac{1}{N} \\sum_{i=1}^N(y_i - \\theta - E[u_i])=0\\]\n\n\nSupposing that \\(E[\\overline{u}]\\) is unknown, we can instead use the method of simulated moments for \\(\\theta\\) defined by:\n\n\n\\[\\dfrac{1}{N} \\sum_{i=1}^N(y_i - \\theta - \\dfrac{1}{S} \\sum_{s=1}^S u_i^s)=0\\]\n\n\nImport the data\n\n\nYou can consult the original source code to see how the authors simulated the data. To get the same results, and verify that I didn't make mistakes I prefer importing their data directly from their website.\n\n\ndata &lt;- read.table(\"http://cameron.econ.ucdavis.edu/mmabook/mma12p2mslmsm.asc\")\nu &lt;- data[, 1]\ne &lt;- data[, 2]\ny &lt;- data[, 3]\nnumobs &lt;- length(u)\nsimreps &lt;- 10000\n\n\nSimulation\n\n\nIn the code below, we simulate the equation defined above:\n\n\nusim &lt;- -log(-log(runif(simreps)))\nesim &lt;- rnorm(simreps, 0, 1)\n\nisim &lt;- 0\nwhile (isim &lt; simreps) {\n\n    usim = usim - log(-log(runif(simreps)))\n    esim = esim + rnorm(simreps, 0, 1)\n\n    isim = isim + 1\n\n}\n\nusimbar = usim/simreps\nesimbar = esim/simreps\n\ntheta = y - usimbar - esimbar\n\ntheta_msm &lt;- mean(theta)\napprox_sterror &lt;- sd(theta)/sqrt(simreps)\n\n\nThese steps yield the following results:\n\n\ntheta_msm\n\n[1] 1.187978\n\n\nand\n\napprox_sterror\n\n[1] 0.01676286"
  },
  {
    "objectID": "posts/2020-11-05-retire_data_science.html",
    "href": "posts/2020-11-05-retire_data_science.html",
    "title": "It’s time to retire the “data scientist” label",
    "section": "",
    "text": "The “Data Scientist” label served its purpose; it allowed us to signal a transition happening in our profession from using only applied mathematical statistical methods to something else, which now also involves the use of a subset of software engineering practices. This transition was mentioned back in 2010 by Deborah Nolan (https://www.stat.berkeley.edu/~statcur/Preprints/ComputingCurric3.pdf), and this transition might now be complete. Version control systems, document generation from annotated source code (or even full reports generation à la rmarkdown), containers and build automation tools have now entered the toolbox of the run-of-the-mill statistician. Maybe not all of these tools, of course, it largely depends on what it is exactly you do, but certainly some of these. Same goes for software engineering practices. I have had the opportunity to work with some old-school statisticians (and still do), and the difference is clear; just like old school users of WYSIWYG editors like Word don’t use its “newest” features such as “Track changes” (and thus keep writing their text in different colors to signal which paragraphs are new), or the concept of versions of a document synced on Sharepoint (and thus keep multiple versions of the same document with different names) old school statisticians have not included the tools I mentioned before in their toolbox.\n\n\n\n\n\nNow don’t get me wrong here; that is absolutely ok. We need and respect old school statisticians because they’ve been in the business of getting insights from data for longer than I’ve been alive. This blog post is not a jab at them because they don’t know how to use git (if you interpret it like that, that’s on you). Old school statisticians now have very senior positions and for many of them, their job does not involve getting their hands dirty on data anymore; most of them are now more like managers or mentors, and share their deep knowledge with their more junior team members. (Obviously there’s exceptions, when I say all old school statisticians do this or that, I don’t mean all of them, but most of them. Of course, I don’t have any evidence to back that up).\n\n\nWhat this blog post is about is the label “Data Scientist” that gets used by these more junior team members and by companies that want to hire talented and motivated young people. This label, and the purported difference between a “Data Scientist” and “statistician” does not make any sense in 2020 anymore. (I know I’m beating a dead horse here, but this is my blog. I’ll blog about dead horses as much as I want thank you very much.)\n\n\nFirstly, this label has always been confusing. “Data Scientist”… what does it even mean? The fact it took so long to find a definition, and that almost everyone working in the profession has a different one speaks volumes. Also, don’t all scientists use data? Data from experiments, from observational studies, from surveys, from the literature?\n\n\nSecondly, I don’t believe that you can get a degree in statistics today without any exposition whatsoever to at least some of the tools I mentioned before. I really doubt that there’s people out there getting Master’s degrees in statistics without having ever touched these tools, or the unix command line. The degrees they’re going for might not focus a lot on these tools, true, but they certainly touch upon them. And of course, once they join a team at their first job, they’ll get more exposed to these tools and incorporate them in their day to day work. So, they’re not statisticians anymore? Their degree magically transformed into a data science degree?\n\n\nBut what about data science degrees? Are the students graduating with these degrees statisticians? I’d argue that yes, they are indeed statisticians; it’s just that they took a statistics degree that might have focused more than usual on these “new” practices/tools, and changed its name to “Data Science degree” for marketing purposes.\n\n\nAnyways, the label “Data Scientist” is now completely defunct; as I mentioned in the very beginning, it served us well to signal that a transition was happening in the profession. I believe that this transition is now complete, or should be nearing its final stages. Also, this transition was not only about the tools used, but also about the deliverables. Statisticians now don’t only deliver tables, graphs and studies but more and more of them deliver products. This product can be a package implementing a bleeding edge statistical method for the profession as a whole, or it can be part of a piece of software that needs it to run (like your smartphone keyboard using a statistical model for word predictions). See this paper for an interesting exposition about how curricula and deliverables have evolved in the past two decades.\n\n\nCurrently, this label gets used by people that try to get insights from data. But we already have a word for them; statisticians. It’s just that the tools of the statistician have evolved over the past decade or so. Actually, I would perhaps even make another distinction; we should reserve the label of “statistician” to people that do statistics without ever touching any data. The other statisticians, the ones that get dirty wrestling in the mud with the data (they’re the pigs that like it from that famous quote) should be called “data janitors”. I’m not even joking; not only does that term already exist and gets used, I think it suits what we do perfectly. What do janitors do? They clean stuff and put things in order. We clean data and put it in order; meaning creating summary tables, visualizations, interactive applications, and models. Oh, and we do so (preferably) in a reproducible way.\n\n\nHope you enjoyed! If you found this blog post useful, you might want to follow me on twitter for blog post updates and buy me an espresso or paypal.me, or buy my ebook on Leanpub.\n\n\n\nBuy me an Espresso"
  },
  {
    "objectID": "posts/2020-01-26-harold.html#introduction",
    "href": "posts/2020-01-26-harold.html#introduction",
    "title": "Dynamic discrete choice models, reinforcement learning and Harold, part 1",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nI want to write about an Econometrica paper written in 1987 (jstor link) by John Rust, currently Professor of Economics at Georgetown University, paper which has been on my mind for the past 10 years or so. Why? Because it is a seminal paper in the econometric literature, but it is quite a bizarre one in some aspects. In this paper, John Rust estimates a structural dynamic discrete choice model on real data, and Professor Rust even had to develop his own novel algorithm, which he called NFXP, which stands for Nested Fixed Point algorithm, to estimate the model. Such models hare now part of the toolbox of structural econometricians, because said models are suited to model decision making in a changing environment. How much should you save today for retirement? Should you go to university? If yes, which major should you choose? Should you get a PhD? Should you have kids? How many? With whom? As you see, kind reader, these models are at the center point of what makes life so interesting, and sometimes so scary as well; what will be the impact of our decisions today on future rewards? Some would say that only the Almighty would know, but structural econometricians now know as well, thanks to John Rust.\n\n\nIt is thus completely natural that Professor Rust chose a very important topic and gathered some very important data to illustrate the inner workings of such a complicated, and yet fundamentally important model.\n\n\nJohn Rust chose to tell the story of one named Harold Zurcher, superintendent of the Madison, Wisconsin, Metropolitan Bus Company and his monthly decision making process on whether to replace the engine of the buses of the company’s fleet, or not."
  },
  {
    "objectID": "posts/2020-01-26-harold.html#and-thine-ears-shall-hear-a-word-behind-thee-saying-this-is-the-way-walk-ye-in-it-when-ye-turn-to-the-right-hand-and-when-ye-turn-to-the-left.-isaiah-3021",
    "href": "posts/2020-01-26-harold.html#and-thine-ears-shall-hear-a-word-behind-thee-saying-this-is-the-way-walk-ye-in-it-when-ye-turn-to-the-right-hand-and-when-ye-turn-to-the-left.-isaiah-3021",
    "title": "Dynamic discrete choice models, reinforcement learning and Harold, part 1",
    "section": "\nAnd thine ears shall hear a word behind thee, saying, This is the way, walk ye in it, when ye turn to the right hand, and when ye turn to the left., Isaiah 30:21\n",
    "text": "And thine ears shall hear a word behind thee, saying, This is the way, walk ye in it, when ye turn to the right hand, and when ye turn to the left., Isaiah 30:21\n\n\nJohn Rust’s goal is to write down a model of Harold Zurcher’s behaviour, which he assumes follows an optimal stopping rule: a strategy which specifies whether or not to replace the current bus engine each period as a function of observed and unobserved state variables. But, dear reader, you might wonder, Why model the decisions of Harold Zurcher? Why not any other, more pressing, issue?\n\n\nQuoting the author gives an answer: Admittedly, few people are likely to take particular interest in Harold Zurcher and bus engine replacement, per se. I focus on a particular individual and a specific capital good because it provides a simple, concrete framework to illustrate two ideas: (i) a “bottom-up” approach for modelling replacement investment and (ii) a “nested fixed point” algorithm for estimating dynamic programming models of discrete choice. And this is what made me absolutely love this paper; I am 100% certain that today, anyone, especially when starting an academic career, could not, and would not, write a paper where one would model something so… non-consequential. And yet, John Rust not only wrote such a paper, his paper is seminal in the literature of structural econometrics. For me, this is one of the best papers I ever read. I read this paper around 2010-ish, and have thought about it on and off since then. I now want to explore the data from his paper, and make you discover it as well.\n\n\nIn this blog post, I will focus on the data of the paper, which you can download in its raw, original format or tidy format in the github repo I set up here. In the next blog post, I’ll discuss the model in greater detail, with a focus on Harold Zurcher’s decisions. I’ll then discuss the similarities between reinforcement learning (the title of this blog post was not 100% clickbait) and dynamic discrete stochastic models and use the {ReinforcementLearning} package to try to estimate the optimal policy. I haven’t tried the package’s function on this paper’s data yet, so I have no idea if it’s going to work out. We’ll see."
  },
  {
    "objectID": "posts/2020-01-26-harold.html#the-papers-data",
    "href": "posts/2020-01-26-harold.html#the-papers-data",
    "title": "Dynamic discrete choice models, reinforcement learning and Harold, part 1",
    "section": "\nThe paper’s data\n",
    "text": "The paper’s data\n\n\nHarold Zurcher provided monthly data on odometer readings from 162 buses of the Madison Metro fleet to John Rust.\n\n\n(\n\n\nI sometimes wonder how this discussion went.\n\n\n- Hello Mr Zurcher, I’m an economist, my name is John Rust, and I am interested in dynamic discrete choice models and their estimation. I would like to write an empirical paper for a prestigious journal, and would like to know if you would be so kind as to provide me with data for my paper.\n\n\n- You what?\n\n\n)\n\n\nThe time period goes from December, 1974 to May, 1985. There are 9 groups of buses, but for a reason that is not explained in the paper only 8 groups of buses are studied. In addition to the monthly odometer readings, there is also the date of a first, or second engine replacement. This is the decision that Harold Zurcher faces each month: should he replace, or not, the engine? This is a simplification from the author; in actuality, Harold Zurcher could also perform a routine maintenance or replace individual components as well. The idea to focus on the third option (complete replacement of the engine) is justified by John Rust as being part of a general “preventive maintenance” strategy. Indeed, if a component of the engine fails at low mileage, it is rather safe to simply replace that component. However, should one component of the engine fail at a much higher mileage, then it is very likely that other components would fail as well in the near future. As such, it is much safer to completely replace the engine, either with a brand new one, or with one freshly rebuilt from the company’s machine shop. John Rust points out that Harold Zurcher assured him that rebuilt engines are every bit as good, if not better, than engines purchased brand new.\n\n\nNow, to the data itself. The data comes in a format unlike anything I had ever seen before. Let’s take a look at the head of one single file, for instance a452372.asc (.asc stands for ascii, as far as I know):\n\n   4239 \n      2 \n     72 \n      1 \n     76 \n 166100 \n      0 \n      0 \n      0 \n     12 \n     74 \n 140953 \n 142960 \n 145380 \n 148140 \n\nThen, on line 138, the data for the second bus of this groups starts:\n\n   4240 \n      2 \n     72 \n      1 \n     75 \n 177900 \n      0 \n      0 \n      0 \n     12 \n     74 \n 174402 \n 175116 \n\nand so on for each bus of this group. The other files are structured in the same way.\n\n\nThis is quite cryptic, but thankfully, the data is well documented in the manual of the NFXP software that John Rust wrote for this paper (remember the algorithm he wrote to estimate the model? He shared his code with a nice manual, a very good practice that unfortunately is not widespread enough in econometric circles, even to this day). From this manual, we can read that the 11 first lines of the file are some kind of metadata:\n\n\n\n\n\nRow  \n\n\nMeaning\n\n\nObservation\n\n\n\n\n\n\n1  \n\n\nbus number\n\n\n4239\n\n\n\n\n2  \n\n\nmonth purchased\n\n\n2\n\n\n\n\n3  \n\n\nyear purchased\n\n\n72\n\n\n\n\n4  \n\n\nmonth of 1st engine replacement\n\n\n1\n\n\n\n\n5  \n\n\nyear of 1st engine replacement\n\n\n76\n\n\n\n\n6  \n\n\nodometer at replacement\n\n\n166100\n\n\n\n\n7  \n\n\nmonth of 2nd replacement\n\n\n0\n\n\n\n\n8  \n\n\nyear of 2nd replacement\n\n\n0\n\n\n\n\n9  \n\n\nodometer at replacement\n\n\n0\n\n\n\n\n10  \n\n\nmonth odometer data begins\n\n\n12\n\n\n\n\n11  \n\n\nyear odometer data begins\n\n\n74\n\n\n\n\n12  \n\n\nodometer reading\n\n\n140953\n\n\n\n\n\nWith this knowledge, the first step is thus to build a tidy data frame. To achieve this, I first load the relevant packages, and read in all the data at once:\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\ndata_file_path &lt;- Sys.glob(\"datasets/*.asc\")\n\ndata_files &lt;- map(data_file_path, read_lines)\n\ndata_files is a list of 9 elements, where each element is one of the raw data files (a42372.asc, a452374.asc, ….)\n\n&gt; str(data_files)\nList of 9\n $ : chr [1:2466] \"   4239 \" \"      2 \" \"     72 \" \"      1 \" ...\n $ : chr [1:1370] \"   4287 \" \"     10 \" \"     74 \" \"     11 \" ...\n $ : chr [1:2466] \"   5257 \" \"      5 \" \"     72 \" \"      6 \" ...\n $ : chr [1:1644] \"   5275 \" \"     10 \" \"     74 \" \"      9 \" ...\n $ : chr [1:4736] \"   5297 \" \"      8 \" \"     75 \" \"      4 \" ...\n $ : chr [1:440] \"   1334 \" \"      3 \" \"     77 \" \"      0 \" ...\n $ : chr [1:540] \"   4403 \" \"      5 \" \"     83 \" \"      0 \" ...\n $ : chr [1:240] \"   2386 \" \"      5 \" \"     81 \" \"      0 \" ...\n $ : chr [1:3888] \"   4338 \" \"      3 \" \"     79 \" \"      3 \" ...\n\nto process all this data, I wrote this monster function:\n\nprocess_bus_data &lt;- function(data_file){\n  data_file &lt;- as.numeric(data_file)\n  first_bus &lt;- data_file[1]\n  second_bus &lt;- first_bus + 1\n  second_bus_index &lt;- which(data_file == second_bus)\n\n  nb_data_points &lt;- second_bus_index - 1\n\n  nb_buses &lt;- length(data_file) / nb_data_points\n\n  indices &lt;- nb_data_points * seq(1, nb_buses)\n\n  indices &lt;- c(0, indices)\n\n  sep_data_sets &lt;- map(indices, ~`[`(data_file, (. + 1):(. + nb_data_points) ))\n\n  headers_list &lt;- map(sep_data_sets, ~`[`(., 1:11))\n\n  header_elements &lt;- c(\"bus number\", \"month purchased\", \"year purchased\",\n                       \"month of 1st engine replacement\", \"year of 1st engine replacement\",\n                       \"odometer at replacement\", \"month of 2nd replacement\",\n                       \"year of 2nd replacement\", \"odometer at replacement\",\n                       \"month odometer data begins\", \"year odometer data begins\")\n\n  create_start_date &lt;- function(one_dataset){\n      one_dataset &lt;- pull(one_dataset)\n      month &lt;- one_dataset[10]\n      year &lt;- paste0(\"19\", one_dataset[11])\n\n      month &lt;- ifelse(nchar(month) == 1, paste0(\"0\", month), month)\n\n      ymd(paste0(year, \"-\", month, \"-01\"))\n  }\n\n  create_first_replacement &lt;- function(one_dataset){\n      one_dataset &lt;- pull(one_dataset, odometer_reading)\n      month &lt;- one_dataset[4]\n      year &lt;- paste0(\"19\", one_dataset[5])\n\n      month &lt;- ifelse(nchar(month) == 1, paste0(\"0\", month), month)\n\n      ymd(paste0(year, \"-\", month, \"-01\"))\n  }\n\n  create_second_replacement &lt;- function(one_dataset){\n      one_dataset &lt;- pull(one_dataset, odometer_reading)\n      month &lt;- one_dataset[7]\n      year &lt;- paste0(\"19\", one_dataset[8])\n\n      month &lt;- ifelse(nchar(month) == 1, paste0(\"0\", month), month)\n\n      ymd(paste0(year, \"-\", month, \"-01\"))\n  }\n\n  get_bus_id &lt;- function(one_dataset){\n      one_dataset &lt;- pull(one_dataset, odometer_reading)\n      one_dataset[1]\n  }\n\n  named_headers &lt;- map(headers_list, ~set_names(., header_elements))\n\n\n  raw_data &lt;- map(sep_data_sets, ~tibble(\"odometer_reading\" = .))\n  raw_data &lt;- map(raw_data, ~mutate(., \"date\" = create_start_date(.)))\n  raw_data &lt;- map(raw_data, ~mutate(., \"first_replacement_date\" = create_first_replacement(.)))\n  raw_data &lt;- map(raw_data, ~mutate(., \"second_replacement_date\" = create_second_replacement(.)))\n  raw_data &lt;- map(raw_data, ~mutate(., \"bus_id\" = get_bus_id(.)))\n  raw_data &lt;- map(raw_data, ~slice(., -c(1:11)))\n\n  fill_dates &lt;- function(vector){\n      for(i in 2:length(vector)){\n          vector[i] &lt;- add_with_rollback(vector[i-1], months(1))\n          # the line below can be uncommented to skip the 2 months of strike in 1980\n          #vector[i] &lt;- if_else(vector[i] == ymd(\"1980-07-01\"), add_with_rollback(vector[i], months(2)),\n          #                    vector[i])\n      }\n      vector\n  }\n\n  raw_data &lt;- raw_data %&gt;%\n      map(~mutate(., date = fill_dates(date)))\n\n  raw_data &lt;- map(raw_data, ~mutate(., \"replacement_1\" = if_else(date == first_replacement_date, 1, 0, 0)))\n  raw_data &lt;- map(raw_data, ~mutate(., \"replacement_2\" = if_else(date == second_replacement_date, 1, 0, 0)))\n  raw_data &lt;- map(raw_data, ~mutate(., replacement = replacement_1 + replacement_2))\n  raw_data &lt;- map(raw_data, ~select(., bus_id, date, odometer_reading, replacement,\n                                    -replacement_1, -replacement_2, -first_replacement_date, -second_replacement_date))\n\n  return(raw_data)\n}\n\nNow, as usual, I didn’t write this in one go. First, I experimented bits and pieces of code on one single dataset, and then only started putting these pieces together into this big function.\n\n\nI won’t go through this function line by line, because it would take me ages. I think there are two majors things to understand in this function:\n\n\n\nfirst identify the start of a particular bus’s data;\n\n\nsecond this function uses some intermediary {purrr} magic.\n\n\n\nSo first step, identify the start of the monthly odometer reading for one bus. For the first bus this is quite simple, as it is simply the start of the file. But when does the data for the second bus start? Thankfully, buses’ ids are numbers, and they’re in incrementing order in the data. I use this to get the index of the second bus, and compute the number of rows between the id of the first and second bus, which gives me the number of months of odometer readings for the first bus.\n\n  data_file &lt;- as.numeric(data_file)\n  first_bus &lt;- data_file[1]\n  second_bus &lt;- first_bus + 1\n  second_bus_index &lt;- which(data_file == second_bus)\n\n  nb_data_points &lt;- second_bus_index - 1\n\nThen, I get the number of buses in the data, and create a vector with all the indices of the buses’ ids:\n\n  nb_buses &lt;- length(data_file) / nb_data_points\n\n  indices &lt;- nb_data_points * seq(1, nb_buses)\n\n  indices &lt;- c(0, indices)\n\n  sep_data_sets &lt;- map(indices, ~`[`(data_file, (. + 1):(. + nb_data_points) ))\n\nI end up with a list of lists, sep_data_sets. The first element of my list is now a list, with the data from the a452372.asc file, where each element is the data for a single bus.\n\n\nFor instance, here is the first element of sep_data_sets:\n\nstr(sep_data_sets[[1]])\nList of 19\n $ : num [1:137] 4239 2 72 1 76 ...\n $ : num [1:137] 4240 2 72 1 75 ...\n $ : num [1:137] 4241 2 72 5 75 ...\n $ : num [1:137] 4242 2 72 2 76 ...\n $ : num [1:137] 4243 2 72 4 76 ...\n $ : num [1:137] 4244 2 72 3 78 ...\n $ : num [1:137] 4245 2 72 1 75 ...\n $ : num [1:137] 4246 2 72 3 75 ...\n $ : num [1:137] 4247 2 72 9 80 ...\n $ : num [1:137] 4248 2 72 2 75 ...\n $ : num [1:137] 4249 2 72 7 75 ...\n $ : num [1:137] 4250 2 72 4 80 ...\n $ : num [1:137] 4251 2 72 1 79 ...\n $ : num [1:137] 4252 2 72 5 76 ...\n $ : num [1:137] 4253 2 72 1 77 ...\n $ : num [1:137] 4254 2 72 3 76 ...\n $ : num [1:137] 4255 2 72 1 76 ...\n $ : num [1:137] 4256 2 72 9 77 ...\n $ : num [1:137] NA NA NA NA NA NA NA NA NA NA ...\n\nSo there are 18 buses in the first group of data (the last line full of NA’s is due to the fact that I messed up my indices vector, I’ll simply remove these at the end).\n\n\nThat’s the first step. The second step, is to make use of this list structure to apply some cleaning functions to each dataset using {purrr}. I explain the approach in my ebook, which you can read for free here. The idea is to use a function that would work on a single element of your list, and then mapping this over all the elements of the list. For instance, remember that the 11 first elements of the data are some kind of header? To extract those for one single vector of observations, one would use:\n\nmy_vector[1:11]\n\nor, equivalently:\n\n`[`(my_vector, 1:11)\n\nWell, when faced with a list of vectors, one maps this function over the whole list using map():\n\nmap(my_list_of_vectors, `[`(1:11))\n\nThis is the logic of this big process_bus_data() function. If something’s not clear after you study it, drop me an email or tweet.\n\n\nAnyways, now that I cleaned the data, here’s how it looks:\n\nall_buses &lt;- read_csv(\"https://raw.githubusercontent.com/b-rodrigues/rust/ee15fb87fc4ba5db28d055c97a898b328725f53c/datasets/processed_data/all_buses.csv\")\n## Parsed with column specification:\n## cols(\n##   bus_id = col_double(),\n##   date = col_date(format = \"\"),\n##   odometer_reading = col_double(),\n##   replacement = col_double(),\n##   bus_family = col_character()\n## )\nhead(all_buses)\n## # A tibble: 6 x 5\n##   bus_id date       odometer_reading replacement bus_family\n##    &lt;dbl&gt; &lt;date&gt;                &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;     \n## 1   4239 1974-12-01           140953           0 a452372   \n## 2   4239 1975-01-01           142960           0 a452372   \n## 3   4239 1975-02-01           145380           0 a452372   \n## 4   4239 1975-03-01           148140           0 a452372   \n## 5   4239 1975-04-01           150921           0 a452372   \n## 6   4239 1975-05-01           153839           0 a452372\n\nThis tidy data frame now has the bus id, the odometer readings with the right date, and whether a replacement occurred at that date. I said the right date, but in the original documentation of the data, John Rust mentions a two month strike in July and August 1980, and he removed these points from the data since the odometer readings where the same. I did not skip July and August when I created the dates, even though I have added the code to do it in the function above, because it does not matter.\n\n\nI have 166 in my sample, while John Rust writes in the paper that his sample contains 162. I do not know why I have 4 more buses.\n\n\nLet’s try to reproduce Table 2a of the paper (mileage at replacement):\n\nall_buses %&gt;% \n    group_by(bus_id) %&gt;% \n    filter(replacement == 1) %&gt;% \n    group_by(bus_family) %&gt;% \n    summarise_at(.vars = vars(odometer_reading), \n                 .funs = list(~max(.), ~min(.), ~mean(.), ~sd(.)))\n## # A tibble: 6 x 5\n##   bus_family    max    min    mean     sd\n##   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n## 1 a452372    334393 130810 193175. 53533.\n## 2 a452374    237287  82370 151495  61246.\n## 3 a530872    413132 170508 278292. 78529.\n## 4 a530874    325336 117986 247119  60818.\n## 5 a530875    388254 120709 263405. 64556.\n## 6 t8h203     273369 125643 200685. 37120.\n\nI find different slightly results, for instance, for bus family t8h203 I find an average of 200’685 miles, while the original author found 199’733. This difference comes very likely from the fact that the author probably uses the value from the header, “odometer at replacement”, at position 6, while I use the value of the odometer at that month, which is always slightly different.\n\n\nLet’s try to reproduce Table 2b, as well, mileage for buses who did not have a replacement:\n\nall_buses %&gt;% \n    group_by(bus_id) %&gt;% \n    filter(all(replacement == 0)) %&gt;% \n    group_by(bus_family) %&gt;% \n    summarise_at(.vars = vars(odometer_reading), \n                 .funs = list(~max(.), ~min(.), ~mean(.), ~sd(.)))\n## # A tibble: 7 x 5\n##   bus_family    max   min    mean      sd\n##   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n## 1 a452374    299040  4249 156135.  81992.\n## 2 a530874    326843 13065 197547.  86692.\n## 3 a530875    352450   129 188193. 104453.\n## 4 d309        65045   294  30643.  17063.\n## 5 g870       120151   483  49582.  32353.\n## 6 rt50       161748  1743  77506.  44674.\n## 7 t8h203     280802  2950 127964.  72300.\n\nHere I find exactly the same values as the author. To finish this quite long blog post, let’s now plot the data:\n\nggplot(all_buses) + \n    geom_line(aes(y = odometer_reading, x = date, group = bus_id, col = bus_family)) + \n    labs(title = \"Odometer readings\") +\n    brotools::theme_blog()\n\n\n\n\nLet’s add some dots to mark the points in time where replacements happened:\n\nggplot(all_buses) + \n    geom_line(aes(y = odometer_reading, x = date, group = bus_id, col = bus_family)) + \n    geom_point(aes(y = ifelse(odometer_reading*replacement == 0, NA, odometer_reading*replacement), \n                              x = date), col = \"red\") +\n    labs(title = \"Odometer readings and points in time where engine replacement occurred\") +\n    brotools::theme_blog()\n## Warning: Removed 15840 rows containing missing values (geom_point).\n\n\n\n\nLet’s create a graph for each bus family:\n\nggplot(all_buses) + \n    geom_line(aes(y = odometer_reading, x = date, group = bus_id), col = \"#82518c\") +\n    geom_point(aes(y = ifelse(odometer_reading*replacement == 0, NA, odometer_reading*replacement), \n                              x = date), col = \"red\") +\n    facet_wrap(~bus_family) + \n    labs(title = \"Odometer readings and points in time where engine replacement occurred\") +\n    brotools::theme_blog()\n## Warning: Removed 15840 rows containing missing values (geom_point).\n\n\n\n\nIn the next blog post, I’ll explore how recent reinforcement learning methods might help us get the optimal policy from the data!"
  },
  {
    "objectID": "posts/2019-02-10-stringr_package.html",
    "href": "posts/2019-02-10-stringr_package.html",
    "title": "Manipulating strings with the {stringr} package",
    "section": "",
    "text": "This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free here. This is taken from Chapter 4, in which I introduce the {stringr} package."
  },
  {
    "objectID": "posts/2019-02-10-stringr_package.html#manipulate-strings-with-stringr",
    "href": "posts/2019-02-10-stringr_package.html#manipulate-strings-with-stringr",
    "title": "Manipulating strings with the {stringr} package",
    "section": "\nManipulate strings with {stringr}\n",
    "text": "Manipulate strings with {stringr}\n\n\n{stringr} contains functions to manipulate strings. In Chapter 10, I will teach you about regular expressions, but the functions contained in {stringr} allow you to already do a lot of work on strings, without needing to be a regular expression expert.\n\n\nI will discuss the most common string operations: detecting, locating, matching, searching and replacing, and exctracting/removing strings.\n\n\nTo introduce these operations, let us use an ALTO file of an issue of The Winchester News from October 31, 1910, which you can find on this link (to see how the newspaper looked like, click here). I re-hosted the file on a public gist for archiving purposes. While working on the book, the original site went down several times…\n\n\nALTO is an XML schema for the description of text OCR and layout information of pages for digitzed material, such as newspapers (source: ALTO Wikipedia page). For more details, you can read my blogpost on the matter, but for our current purposes, it is enough to know that the file contains the text of newspaper articles. The file looks like this:\n\n&lt;TextLine HEIGHT=\"138.0\" WIDTH=\"2434.0\" HPOS=\"4056.0\" VPOS=\"5814.0\"&gt;\n&lt;String STYLEREFS=\"ID7\" HEIGHT=\"108.0\" WIDTH=\"393.0\" HPOS=\"4056.0\" VPOS=\"5838.0\" CONTENT=\"timore\" WC=\"0.82539684\"&gt;\n&lt;ALTERNATIVE&gt;timole&lt;/ALTERNATIVE&gt;\n&lt;ALTERNATIVE&gt;tlnldre&lt;/ALTERNATIVE&gt;\n&lt;ALTERNATIVE&gt;timor&lt;/ALTERNATIVE&gt;\n&lt;ALTERNATIVE&gt;insole&lt;/ALTERNATIVE&gt;\n&lt;ALTERNATIVE&gt;landed&lt;/ALTERNATIVE&gt;\n&lt;/String&gt;\n&lt;SP WIDTH=\"74.0\" HPOS=\"4449.0\" VPOS=\"5838.0\"/&gt;\n&lt;String STYLEREFS=\"ID7\" HEIGHT=\"105.0\" WIDTH=\"432.0\" HPOS=\"4524.0\" VPOS=\"5847.0\" CONTENT=\"market\" WC=\"0.95238096\"/&gt;\n&lt;SP WIDTH=\"116.0\" HPOS=\"4956.0\" VPOS=\"5847.0\"/&gt;\n&lt;String STYLEREFS=\"ID7\" HEIGHT=\"69.0\" WIDTH=\"138.0\" HPOS=\"5073.0\" VPOS=\"5883.0\" CONTENT=\"as\" WC=\"0.96825397\"/&gt;\n&lt;SP WIDTH=\"74.0\" HPOS=\"5211.0\" VPOS=\"5883.0\"/&gt;\n&lt;String STYLEREFS=\"ID7\" HEIGHT=\"69.0\" WIDTH=\"285.0\" HPOS=\"5286.0\" VPOS=\"5877.0\" CONTENT=\"were\" WC=\"1.0\"&gt;\n&lt;ALTERNATIVE&gt;verc&lt;/ALTERNATIVE&gt;\n&lt;ALTERNATIVE&gt;veer&lt;/ALTERNATIVE&gt;\n&lt;/String&gt;\n&lt;SP WIDTH=\"68.0\" HPOS=\"5571.0\" VPOS=\"5877.0\"/&gt;\n&lt;String STYLEREFS=\"ID7\" HEIGHT=\"111.0\" WIDTH=\"147.0\" HPOS=\"5640.0\" VPOS=\"5838.0\" CONTENT=\"all\" WC=\"1.0\"/&gt;\n&lt;SP WIDTH=\"83.0\" HPOS=\"5787.0\" VPOS=\"5838.0\"/&gt;\n&lt;String STYLEREFS=\"ID7\" HEIGHT=\"111.0\" WIDTH=\"183.0\" HPOS=\"5871.0\" VPOS=\"5835.0\" CONTENT=\"the\" WC=\"0.95238096\"&gt;\n&lt;ALTERNATIVE&gt;tll&lt;/ALTERNATIVE&gt;\n&lt;ALTERNATIVE&gt;Cu&lt;/ALTERNATIVE&gt;\n&lt;ALTERNATIVE&gt;tall&lt;/ALTERNATIVE&gt;\n&lt;/String&gt;\n&lt;SP WIDTH=\"75.0\" HPOS=\"6054.0\" VPOS=\"5835.0\"/&gt;\n&lt;String STYLEREFS=\"ID3\" HEIGHT=\"132.0\" WIDTH=\"351.0\" HPOS=\"6129.0\" VPOS=\"5814.0\" CONTENT=\"cattle\" WC=\"0.95238096\"/&gt;\n&lt;/TextLine&gt;\n\nWe are interested in the strings after CONTENT=. We are going to use functions from the {stringr} package to get the strings after CONTENT=. In Chapter 10, we are going to explore this file again, but using complex regular expressions to get all the content in one go.\n\n\n\nGetting text data into Rstudio\n\n\nFirst of all, let us read in the file:\n\nwinchester &lt;- read_lines(\"https://gist.githubusercontent.com/b-rodrigues/5139560e7d0f2ecebe5da1df3629e015/raw/e3031d894ffb97217ddbad1ade1b307c9937d2c8/gistfile1.txt\")\n\nEven though the file is an XML file, I still read it in using read_lines() and not read_xml() from the {xml2} package. This is for the purposes of the current exercise, and also because I always have trouble with XML files, and prefer to treat them as simple text files, and use regular expressions to get what I need.\n\n\nNow that the ALTO file is read in and saved in the winchester variable, you might want to print the whole thing in the console. Before that, take a look at the structure:\n\nstr(winchester)\n##  chr [1:43] \"\" ...\n\nSo the winchester variable is a character atomic vector with 43 elements. So first, we need to understand what these elements are. Let’s start with the first one:\n\nwinchester[1]\n## [1] \"\"\n\nOk, so it seems like the first element is part of the header of the file. What about the second one?\n\nwinchester[2]\n## [1] \"&lt;meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=UTF-8\\\"&gt;&lt;base href=\\\"https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml\\\"&gt;&lt;style&gt;body{margin-left:0;margin-right:0;margin-top:0}#bN015htcoyT__google-cache-hdr{background:#f5f5f5;font:13px arial,sans-serif;text-align:left;color:#202020;border:0;margin:0;border-bottom:1px solid #cecece;line-height:16px;padding:16px 28px 24px 28px}#bN015htcoyT__google-cache-hdr *{display:inline;font:inherit;text-align:inherit;color:inherit;line-height:inherit;background:none;border:0;margin:0;padding:0;letter-spacing:0}#bN015htcoyT__google-cache-hdr a{text-decoration:none;color:#1a0dab}#bN015htcoyT__google-cache-hdr a:hover{text-decoration:underline}#bN015htcoyT__google-cache-hdr a:visited{color:#609}#bN015htcoyT__google-cache-hdr div{display:block;margin-top:4px}#bN015htcoyT__google-cache-hdr b{font-weight:bold;display:inline-block;direction:ltr}&lt;/style&gt;&lt;div id=\\\"bN015htcoyT__google-cache-hdr\\\"&gt;&lt;div&gt;&lt;span&gt;This is Google's cache of &lt;a href=\\\"https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml\\\"&gt;https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml&lt;/a&gt;.&lt;/span&gt;&nbsp;&lt;span&gt;It is a snapshot of the page as it appeared on 21 Jan 2019 05:18:18 GMT.&lt;/span&gt;&nbsp;&lt;span&gt;The &lt;a href=\\\"https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml\\\"&gt;current page&lt;/a&gt; could have changed in the meantime.&lt;/span&gt;&nbsp;&lt;a href=\\\"http://support.google.com/websearch/bin/answer.py?hl=en&amp;p=cached&amp;answer=1687222\\\"&gt;&lt;span&gt;Learn more&lt;/span&gt;.&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=\\\"display:inline-block;margin-top:8px;margin-right:104px;white-space:nowrap\\\"&gt;&lt;span style=\\\"margin-right:28px\\\"&gt;&lt;span style=\\\"font-weight:bold\\\"&gt;Full version&lt;/span&gt;&lt;/span&gt;&lt;span style=\\\"margin-right:28px\\\"&gt;&lt;a href=\\\"http://webcache.googleusercontent.com/search?q=cache:2BVPV8QGj3oJ:https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml&amp;hl=en&amp;gl=lu&amp;strip=1&amp;vwsrc=0\\\"&gt;&lt;span&gt;Text-only version&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;span style=\\\"margin-right:28px\\\"&gt;&lt;a href=\\\"http://webcache.googleusercontent.com/search?q=cache:2BVPV8QGj3oJ:https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml&amp;hl=en&amp;gl=lu&amp;strip=0&amp;vwsrc=1\\\"&gt;&lt;span&gt;View source&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;span style=\\\"display:inline-block;margin-top:8px;color:#717171\\\"&gt;&lt;span&gt;Tip: To quickly find your search term on this page, press &lt;b&gt;Ctrl+F&lt;/b&gt; or &lt;b&gt;⌘-F&lt;/b&gt; (Mac) and use the find bar.&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div style=\\\"position:relative;\\\"&gt;&lt;?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?&gt;\"\n\nSame. So where is the content? The file is very large, so if you print it in the console, it will take quite some time to print, and you will not really be able to make out anything. The best way would be to try to detect the string CONTENT and work from there.\n\n\n\n\nDetecting, getting the position and locating strings\n\n\nWhen confronted to an atomic vector of strings, you might want to know inside which elements you can find certain strings. For example, to know which elements of winchester contain the string CONTENT, use str_detect():\n\nwinchester %&gt;%\n  str_detect(\"CONTENT\")\n##  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [23] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [34] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n\nThis returns a boolean atomic vector of the same length as winchester. If the string CONTENT is nowhere to be found, the result will equal FALSE, if not it will equal TRUE. Here it is easy to see that the last element contains the string CONTENT. But what if instead of having 43 elements, the vector had 24192 elements? And hundreds would contain the string CONTENT? It would be easier to instead have the indices of the vector where one can find the word CONTENT. This is possible with str_which():\n\nwinchester %&gt;%\n  str_which(\"CONTENT\")\n## [1] 43\n\nHere, the result is 43, meaning that the 43rd element of winchester contains the string CONTENT somewhere. If we need more precision, we can use str_locate() and str_locate_all(). To explain how both these functions work, let’s create a very small example:\n\nancient_philosophers &lt;- c(\"aristotle\", \"plato\", \"epictetus\", \"seneca the younger\", \"epicurus\", \"marcus aurelius\")\n\nNow suppose I am interested in philosophers whose name ends in us. Let us use str_locate() first:\n\nancient_philosophers %&gt;%\n  str_locate(\"us\")\n##      start end\n## [1,]    NA  NA\n## [2,]    NA  NA\n## [3,]     8   9\n## [4,]    NA  NA\n## [5,]     7   8\n## [6,]     5   6\n\nYou can interpret the result as follows: in the rows, the index of the vector where the string us is found. So the 3rd, 5th and 6th philosopher have us somewhere in their name. The result also has two columns: start and end. These give the position of the string. So the string us can be found starting at position 8 of the 3rd element of the vector, and ends at position 9. Same goes for the other philisophers. However, consider Marcus Aurelius. He has two names, both ending with us. However, str_locate() only shows the position of the us in Marcus.\n\n\nTo get both us strings, you need to use str_locate_all():\n\nancient_philosophers %&gt;%\n  str_locate_all(\"us\")\n## [[1]]\n##      start end\n## \n## [[2]]\n##      start end\n## \n## [[3]]\n##      start end\n## [1,]     8   9\n## \n## [[4]]\n##      start end\n## \n## [[5]]\n##      start end\n## [1,]     7   8\n## \n## [[6]]\n##      start end\n## [1,]     5   6\n## [2,]    14  15\n\nNow we get the position of the two us in Marcus Aurelius. Doing this on the winchester vector will give use the position of the CONTENT string, but this is not really important right now. What matters is that you know how str_locate() and str_locate_all() work.\n\n\nSo now that we know what interests us in the 43nd element of winchester, let’s take a closer look at it:\n\nwinchester[43]\n\nAs you can see, it’s a mess:\n\n&lt;TextLine HEIGHT=\\\"126.0\\\" WIDTH=\\\"1731.0\\\" HPOS=\\\"17160.0\\\" VPOS=\\\"21252.0\\\"&gt;&lt;String HEIGHT=\\\"114.0\\\" WIDTH=\\\"354.0\\\" HPOS=\\\"17160.0\\\" VPOS=\\\"21264.0\\\" CONTENT=\\\"0tV\\\" WC=\\\"0.8095238\\\"/&gt;&lt;SP WIDTH=\\\"131.0\\\" HPOS=\\\"17514.0\\\" VPOS=\\\"21264.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"111.0\\\" WIDTH=\\\"474.0\\\" HPOS=\\\"17646.0\\\" VPOS=\\\"21258.0\\\" CONTENT=\\\"BATES\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"140.0\\\" HPOS=\\\"18120.0\\\" VPOS=\\\"21258.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"114.0\\\" WIDTH=\\\"630.0\\\" HPOS=\\\"18261.0\\\" VPOS=\\\"21252.0\\\" CONTENT=\\\"President\\\" WC=\\\"1.0\\\"&gt;&lt;ALTERNATIVE&gt;Prcideht&lt;/ALTERNATIVE&gt;&lt;ALTERNATIVE&gt;Pride&lt;/ALTERNATIVE&gt;&lt;/String&gt;&lt;/TextLine&gt;&lt;TextLine HEIGHT=\\\"153.0\\\" WIDTH=\\\"1689.0\\\" HPOS=\\\"17145.0\\\" VPOS=\\\"21417.0\\\"&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"105.0\\\" WIDTH=\\\"258.0\\\" HPOS=\\\"17145.0\\\" VPOS=\\\"21439.0\\\" CONTENT=\\\"WM\\\" WC=\\\"0.82539684\\\"&gt;&lt;TextLine HEIGHT=\\\"120.0\\\" WIDTH=\\\"2211.0\\\" HPOS=\\\"16788.0\\\" VPOS=\\\"21870.0\\\"&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"96.0\\\" WIDTH=\\\"102.0\\\" HPOS=\\\"16788.0\\\" VPOS=\\\"21894.0\\\" CONTENT=\\\"It\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"72.0\\\" HPOS=\\\"16890.0\\\" VPOS=\\\"21894.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"96.0\\\" WIDTH=\\\"93.0\\\" HPOS=\\\"16962.0\\\" VPOS=\\\"21885.0\\\" CONTENT=\\\"is\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"80.0\\\" HPOS=\\\"17055.0\\\" VPOS=\\\"21885.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"102.0\\\" WIDTH=\\\"417.0\\\" HPOS=\\\"17136.0\\\" VPOS=\\\"21879.0\\\" CONTENT=\\\"seldom\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"80.0\\\" HPOS=\\\"17553.0\\\" VPOS=\\\"21879.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"96.0\\\" WIDTH=\\\"267.0\\\" HPOS=\\\"17634.0\\\" VPOS=\\\"21873.0\\\" CONTENT=\\\"hard\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"81.0\\\" HPOS=\\\"17901.0\\\" VPOS=\\\"21873.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"87.0\\\" WIDTH=\\\"111.0\\\" HPOS=\\\"17982.0\\\" VPOS=\\\"21879.0\\\" CONTENT=\\\"to\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"81.0\\\" HPOS=\\\"18093.0\\\" VPOS=\\\"21879.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"96.0\\\" WIDTH=\\\"219.0\\\" HPOS=\\\"18174.0\\\" VPOS=\\\"21870.0\\\" CONTENT=\\\"find\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"77.0\\\" HPOS=\\\"18393.0\\\" VPOS=\\\"21870.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"69.0\\\" WIDTH=\\\"66.0\\\" HPOS=\\\"18471.0\\\" VPOS=\\\"21894.0\\\" CONTENT=\\\"a\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"77.0\\\" HPOS=\\\"18537.0\\\" VPOS=\\\"21894.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"78.0\\\" WIDTH=\\\"384.0\\\" HPOS=\\\"18615.0\\\" VPOS=\\\"21888.0\\\" CONTENT=\\\"succes\\\" WC=\\\"0.82539684\\\"&gt;&lt;ALTERNATIVE&gt;success&lt;/ALTERNATIVE&gt;&lt;/String&gt;&lt;/TextLine&gt;&lt;TextLine HEIGHT=\\\"126.0\\\" WIDTH=\\\"2316.0\\\" HPOS=\\\"16662.0\\\" VPOS=\\\"22008.0\\\"&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"75.0\\\" WIDTH=\\\"183.0\\\" HPOS=\\\"16662.0\\\" VPOS=\\\"22059.0\\\" CONTENT=\\\"sor\\\" WC=\\\"1.0\\\"&gt;&lt;ALTERNATIVE&gt;soar&lt;/ALTERNATIVE&gt;&lt;/String&gt;&lt;SP WIDTH=\\\"72.0\\\" HPOS=\\\"16845.0\\\" VPOS=\\\"22059.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"90.0\\\" WIDTH=\\\"168.0\\\" HPOS=\\\"16917.0\\\" VPOS=\\\"22035.0\\\" CONTENT=\\\"for\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"72.0\\\" HPOS=\\\"17085.0\\\" VPOS=\\\"22035.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"69.0\\\" WIDTH=\\\"267.0\\\" HPOS=\\\"17157.0\\\" VPOS=\\\"22050.0\\\" CONTENT=\\\"even\\\" WC=\\\"1.0\\\"&gt;&lt;ALTERNATIVE&gt;cen&lt;/ALTERNATIVE&gt;&lt;ALTERNATIVE&gt;cent&lt;/ALTERNATIVE&gt;&lt;/String&gt;&lt;SP WIDTH=\\\"77.0\\\" HPOS=\\\"17434.0\\\" VPOS=\\\"22050.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"66.0\\\" WIDTH=\\\"63.0\\\" HPOS=\\\"17502.0\\\" VPOS=\\\"22044.0\\\"\n\nThe file was imported without any newlines. So we need to insert them ourselves, by splitting the string in a clever way.\n\n\n\n\nSplitting strings\n\n\nThere are two functions included in {stringr} to split strings, str_split() and str_split_fixed(). Let’s go back to our ancient philosophers. Two of them, Seneca the Younger and Marcus Aurelius have something else in common than both being Roman Stoic philosophers. Their names are composed of several words. If we want to split their names at the space character, we can use str_split() like this:\n\nancient_philosophers %&gt;%\n  str_split(\" \")\n## [[1]]\n## [1] \"aristotle\"\n## \n## [[2]]\n## [1] \"plato\"\n## \n## [[3]]\n## [1] \"epictetus\"\n## \n## [[4]]\n## [1] \"seneca\"  \"the\"     \"younger\"\n## \n## [[5]]\n## [1] \"epicurus\"\n## \n## [[6]]\n## [1] \"marcus\"   \"aurelius\"\n\nstr_split() also has a simplify = TRUE option:\n\nancient_philosophers %&gt;%\n  str_split(\" \", simplify = TRUE)\n##      [,1]        [,2]       [,3]     \n## [1,] \"aristotle\" \"\"         \"\"       \n## [2,] \"plato\"     \"\"         \"\"       \n## [3,] \"epictetus\" \"\"         \"\"       \n## [4,] \"seneca\"    \"the\"      \"younger\"\n## [5,] \"epicurus\"  \"\"         \"\"       \n## [6,] \"marcus\"    \"aurelius\" \"\"\n\nThis time, the returned object is a matrix.\n\n\nWhat about str_split_fixed()? The difference is that here you can specify the number of pieces to return. For example, you could consider the name “Aurelius” to be the middle name of Marcus Aurelius, and the “the younger” to be the middle name of Seneca the younger. This means that you would want to split the name only at the first space character, and not at all of them. This is easily achieved with str_split_fixed():\n\nancient_philosophers %&gt;%\n  str_split_fixed(\" \", 2)\n##      [,1]        [,2]         \n## [1,] \"aristotle\" \"\"           \n## [2,] \"plato\"     \"\"           \n## [3,] \"epictetus\" \"\"           \n## [4,] \"seneca\"    \"the younger\"\n## [5,] \"epicurus\"  \"\"           \n## [6,] \"marcus\"    \"aurelius\"\n\nThis gives the expected result.\n\n\nSo how does this help in our case? Well, if you look at how the ALTO file looks like, at the beginning of this section, you will notice that every line ends with the “&gt;” character. So let’s split at that character!\n\nwinchester_text &lt;- winchester[43] %&gt;%\n  str_split(\"&gt;\")\n\nLet’s take a closer look at winchester_text:\n\nstr(winchester_text)\n## List of 1\n##  $ : chr [1:19706] \"&lt;/processingStepSettings\" \"&lt;processingSoftware\" \"&lt;softwareCreator\" \"iArchives&lt;/softwareCreator\" ...\n\nSo this is a list of length one, and the first, and only, element of that list is an atomic vector with 19706 elements. Since this is a list of only one element, we can simplify it by saving the atomic vector in a variable:\n\nwinchester_text &lt;- winchester_text[[1]]\n\nLet’s now look at some lines:\n\nwinchester_text[1232:1245]\n##  [1] \"&lt;SP WIDTH=\\\"66.0\\\" HPOS=\\\"5763.0\\\" VPOS=\\\"9696.0\\\"/\"                                                                         \n##  [2] \"&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"108.0\\\" WIDTH=\\\"612.0\\\" HPOS=\\\"5829.0\\\" VPOS=\\\"9693.0\\\" CONTENT=\\\"Louisville\\\" WC=\\\"1.0\\\"\"\n##  [3] \"&lt;ALTERNATIVE\"                                                                                                                \n##  [4] \"Loniile&lt;/ALTERNATIVE\"                                                                                                        \n##  [5] \"&lt;ALTERNATIVE\"                                                                                                                \n##  [6] \"Lenities&lt;/ALTERNATIVE\"                                                                                                       \n##  [7] \"&lt;/String\"                                                                                                                    \n##  [8] \"&lt;/TextLine\"                                                                                                                  \n##  [9] \"&lt;TextLine HEIGHT=\\\"150.0\\\" WIDTH=\\\"2520.0\\\" HPOS=\\\"4032.0\\\" VPOS=\\\"9849.0\\\"\"                                                 \n## [10] \"&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"108.0\\\" WIDTH=\\\"510.0\\\" HPOS=\\\"4032.0\\\" VPOS=\\\"9861.0\\\" CONTENT=\\\"Tobacco\\\" WC=\\\"1.0\\\"/\"  \n## [11] \"&lt;SP WIDTH=\\\"113.0\\\" HPOS=\\\"4542.0\\\" VPOS=\\\"9861.0\\\"/\"                                                                        \n## [12] \"&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"105.0\\\" WIDTH=\\\"696.0\\\" HPOS=\\\"4656.0\\\" VPOS=\\\"9861.0\\\" CONTENT=\\\"Warehouse\\\" WC=\\\"1.0\\\"\" \n## [13] \"&lt;ALTERNATIVE\"                                                                                                                \n## [14] \"WHrchons&lt;/ALTERNATIVE\"\n\nThis now looks easier to handle. We can narrow it down to the lines that only contain the string we are interested in, “CONTENT”. First, let’s get the indices:\n\ncontent_winchester_index &lt;- winchester_text %&gt;%\n  str_which(\"CONTENT\")\n\nHow many lines contain the string “CONTENT”?\n\nlength(content_winchester_index)\n## [1] 4462\n\nAs you can see, this reduces the amount of data we have to work with. Let us save this is a new variable:\n\ncontent_winchester &lt;- winchester_text[content_winchester_index]\n\n\n\nMatching strings\n\n\nMatching strings is useful, but only in combination with regular expressions. As stated at the beginning of this section, we are going to learn about regular expressions in Chapter 10, but in order to make this section useful, we are going to learn the easiest, but perhaps the most useful regular expression: .*.\n\n\nLet’s go back to our ancient philosophers, and use str_match() and see what happens. Let’s match the “us” string:\n\nancient_philosophers %&gt;%\n  str_match(\"us\")\n##      [,1]\n## [1,] NA  \n## [2,] NA  \n## [3,] \"us\"\n## [4,] NA  \n## [5,] \"us\"\n## [6,] \"us\"\n\nNot very useful, but what about the regular expression .*? How could it help?\n\nancient_philosophers %&gt;%\n  str_match(\".*us\")\n##      [,1]             \n## [1,] NA               \n## [2,] NA               \n## [3,] \"epictetus\"      \n## [4,] NA               \n## [5,] \"epicurus\"       \n## [6,] \"marcus aurelius\"\n\nThat’s already very interesting! So how does .* work? To understand, let’s first start by using . alone:\n\nancient_philosophers %&gt;%\n  str_match(\".us\")\n##      [,1] \n## [1,] NA   \n## [2,] NA   \n## [3,] \"tus\"\n## [4,] NA   \n## [5,] \"rus\"\n## [6,] \"cus\"\n\nThis also matched whatever symbol comes just before the “u” from “us”. What if we use two . instead?\n\nancient_philosophers %&gt;%\n  str_match(\"..us\")\n##      [,1]  \n## [1,] NA    \n## [2,] NA    \n## [3,] \"etus\"\n## [4,] NA    \n## [5,] \"urus\"\n## [6,] \"rcus\"\n\nThis time, we get the two symbols that immediately precede “us”. Instead of continuing like this we now use the , which matches zero or more of .. So by combining  and ., we can match any symbol repeatedly, until there is nothing more to match. Note that there is also +, which works similarly to *, but it matches one or more symbols.\n\n\nThere is also a str_match_all():\n\nancient_philosophers %&gt;%\n  str_match_all(\".*us\")\n## [[1]]\n##      [,1]\n## \n## [[2]]\n##      [,1]\n## \n## [[3]]\n##      [,1]       \n## [1,] \"epictetus\"\n## \n## [[4]]\n##      [,1]\n## \n## [[5]]\n##      [,1]      \n## [1,] \"epicurus\"\n## \n## [[6]]\n##      [,1]             \n## [1,] \"marcus aurelius\"\n\nIn this particular case it does not change the end result, but keep it in mind for cases like this one:\n\nc(\"haha\", \"huhu\") %&gt;%\n  str_match(\"ha\")\n##      [,1]\n## [1,] \"ha\"\n## [2,] NA\n\nand:\n\nc(\"haha\", \"huhu\") %&gt;%\n  str_match_all(\"ha\")\n## [[1]]\n##      [,1]\n## [1,] \"ha\"\n## [2,] \"ha\"\n## \n## [[2]]\n##      [,1]\n\nWhat if we want to match names containing the letter “t”? Easy:\n\nancient_philosophers %&gt;%\n  str_match(\".*t.*\")\n##      [,1]                \n## [1,] \"aristotle\"         \n## [2,] \"plato\"             \n## [3,] \"epictetus\"         \n## [4,] \"seneca the younger\"\n## [5,] NA                  \n## [6,] NA\n\nSo how does this help us with our historical newspaper? Let’s try to get the strings that come after “CONTENT”:\n\nwinchester_content &lt;- winchester_text %&gt;%\n  str_match(\"CONTENT.*\")\n\nLet’s use our faithful str() function to take a look:\n\nwinchester_content %&gt;%\n  str\n##  chr [1:19706, 1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ...\n\nHum, there’s a lot of NA values! This is because a lot of the lines from the file did not have the string “CONTENT”, so there is no match possible. Let’s us remove all these NAs. Because the result is a matrix, we cannot use the filter() function from {dplyr}. So we need to convert it to a tibble first:\n\nwinchester_content &lt;- winchester_content %&gt;%\n  as.tibble() %&gt;%\n  filter(!is.na(V1))\n## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics).\n## This warning is displayed once per session.\n\nBecause matrix columns do not have names, when a matrix gets converted into a tibble, the firt column gets automatically called V1. This is why I filter on this column. Let’s take a look at the data:\n\nhead(winchester_content)\n## # A tibble: 6 x 1\n##   V1                                  \n##   &lt;chr&gt;                               \n## 1 \"CONTENT=\\\"J\\\" WC=\\\"0.8095238\\\"/\"   \n## 2 \"CONTENT=\\\"a\\\" WC=\\\"0.8095238\\\"/\"   \n## 3 \"CONTENT=\\\"Ira\\\" WC=\\\"0.95238096\\\"/\"\n## 4 \"CONTENT=\\\"mj\\\" WC=\\\"0.8095238\\\"/\"  \n## 5 \"CONTENT=\\\"iI\\\" WC=\\\"0.8095238\\\"/\"  \n## 6 \"CONTENT=\\\"tE1r\\\" WC=\\\"0.8095238\\\"/\"\n\n\n\nSearching and replacing strings\n\n\nWe are getting close to the final result. We still need to do some cleaning however. Since our data is inside a nice tibble, we might as well stick with it. So let’s first rename the column and change all the strings to lowercase:\n\nwinchester_content &lt;- winchester_content %&gt;% \n  mutate(content = tolower(V1)) %&gt;% \n  select(-V1)\n\nLet’s take a look at the result:\n\nhead(winchester_content)\n## # A tibble: 6 x 1\n##   content                             \n##   &lt;chr&gt;                               \n## 1 \"content=\\\"j\\\" wc=\\\"0.8095238\\\"/\"   \n## 2 \"content=\\\"a\\\" wc=\\\"0.8095238\\\"/\"   \n## 3 \"content=\\\"ira\\\" wc=\\\"0.95238096\\\"/\"\n## 4 \"content=\\\"mj\\\" wc=\\\"0.8095238\\\"/\"  \n## 5 \"content=\\\"ii\\\" wc=\\\"0.8095238\\\"/\"  \n## 6 \"content=\\\"te1r\\\" wc=\\\"0.8095238\\\"/\"\n\nThe second part of the string, “wc=….” is not really interesting. Let’s search and replace this with an empty string, using str_replace():\n\nwinchester_content &lt;- winchester_content %&gt;% \n  mutate(content = str_replace(content, \"wc.*\", \"\"))\n\nhead(winchester_content)\n## # A tibble: 6 x 1\n##   content            \n##   &lt;chr&gt;              \n## 1 \"content=\\\"j\\\" \"   \n## 2 \"content=\\\"a\\\" \"   \n## 3 \"content=\\\"ira\\\" \" \n## 4 \"content=\\\"mj\\\" \"  \n## 5 \"content=\\\"ii\\\" \"  \n## 6 \"content=\\\"te1r\\\" \"\n\nWe need to use the regular expression from before to replace “wc” and every character that follows. The same can be use to remove “content=”:\n\nwinchester_content &lt;- winchester_content %&gt;% \n  mutate(content = str_replace(content, \"content=\", \"\"))\n\nhead(winchester_content)\n## # A tibble: 6 x 1\n##   content    \n##   &lt;chr&gt;      \n## 1 \"\\\"j\\\" \"   \n## 2 \"\\\"a\\\" \"   \n## 3 \"\\\"ira\\\" \" \n## 4 \"\\\"mj\\\" \"  \n## 5 \"\\\"ii\\\" \"  \n## 6 \"\\\"te1r\\\" \"\n\nWe are almost done, but some cleaning is still necessary:\n\n\n\n\nExctracting or removing strings\n\n\nNow, because I now the ALTO spec, I know how to find words that are split between two sentences:\n\nwinchester_content %&gt;% \n  filter(str_detect(content, \"hyppart\"))\n## # A tibble: 64 x 1\n##    content                                                               \n##    &lt;chr&gt;                                                                 \n##  1 \"\\\"aver\\\" subs_type=\\\"hyppart1\\\" subs_content=\\\"average\\\" \"           \n##  2 \"\\\"age\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"average\\\" \"            \n##  3 \"\\\"considera\\\" subs_type=\\\"hyppart1\\\" subs_content=\\\"consideration\\\" \"\n##  4 \"\\\"tion\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"consideration\\\" \"     \n##  5 \"\\\"re\\\" subs_type=\\\"hyppart1\\\" subs_content=\\\"resigned\\\" \"            \n##  6 \"\\\"signed\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"resigned\\\" \"        \n##  7 \"\\\"install\\\" subs_type=\\\"hyppart1\\\" subs_content=\\\"installed\\\" \"      \n##  8 \"\\\"ed\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"installed\\\" \"           \n##  9 \"\\\"be\\\" subs_type=\\\"hyppart1\\\" subs_content=\\\"before\\\" \"              \n## 10 \"\\\"fore\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"before\\\" \"            \n## # … with 54 more rows\n\nFor instance, the word “average” was split over two lines, the first part of the word, “aver” on the first line, and the second part of the word, “age”, on the second line. We want to keep what comes after “subs_content”. Let’s extract the word “average” using str_extract(). However, because only some words were split between two lines, we first need to detect where the string “hyppart1” is located, and only then can we extract what comes after “subs_content”. Thus, we need to combine str_detect() to first detect the string, and then str_extract() to extract what comes after “subs_content”:\n\nwinchester_content &lt;- winchester_content %&gt;% \n  mutate(content = if_else(str_detect(content, \"hyppart1\"), \n                           str_extract_all(content, \"content=.*\", simplify = TRUE), \n                           content))\n\nLet’s take a look at the result:\n\nwinchester_content %&gt;% \n  filter(str_detect(content, \"content\"))\n## # A tibble: 64 x 1\n##    content                                                          \n##    &lt;chr&gt;                                                            \n##  1 \"content=\\\"average\\\" \"                                           \n##  2 \"\\\"age\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"average\\\" \"       \n##  3 \"content=\\\"consideration\\\" \"                                     \n##  4 \"\\\"tion\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"consideration\\\" \"\n##  5 \"content=\\\"resigned\\\" \"                                          \n##  6 \"\\\"signed\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"resigned\\\" \"   \n##  7 \"content=\\\"installed\\\" \"                                         \n##  8 \"\\\"ed\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"installed\\\" \"      \n##  9 \"content=\\\"before\\\" \"                                            \n## 10 \"\\\"fore\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"before\\\" \"       \n## # … with 54 more rows\n\nWe still need to get rid of the string “content=” and then of all the strings that contain “hyppart2”, which are not needed now:\n\nwinchester_content &lt;- winchester_content %&gt;% \n  mutate(content = str_replace(content, \"content=\", \"\")) %&gt;% \n  mutate(content = if_else(str_detect(content, \"hyppart2\"), NA_character_, content))\n\nhead(winchester_content)\n## # A tibble: 6 x 1\n##   content    \n##   &lt;chr&gt;      \n## 1 \"\\\"j\\\" \"   \n## 2 \"\\\"a\\\" \"   \n## 3 \"\\\"ira\\\" \" \n## 4 \"\\\"mj\\\" \"  \n## 5 \"\\\"ii\\\" \"  \n## 6 \"\\\"te1r\\\" \"\n\nAlmost done! We only need to remove the “ characters:\n\nwinchester_content &lt;- winchester_content %&gt;% \n  mutate(content = str_replace_all(content, \"\\\"\", \"\")) \n\nhead(winchester_content)\n## # A tibble: 6 x 1\n##   content\n##   &lt;chr&gt;  \n## 1 \"j \"   \n## 2 \"a \"   \n## 3 \"ira \" \n## 4 \"mj \"  \n## 5 \"ii \"  \n## 6 \"te1r \"\n\nLet’s remove space characters with str_trim():\n\nwinchester_content &lt;- winchester_content %&gt;% \n  mutate(content = str_trim(content)) \n\nhead(winchester_content)\n## # A tibble: 6 x 1\n##   content\n##   &lt;chr&gt;  \n## 1 j      \n## 2 a      \n## 3 ira    \n## 4 mj     \n## 5 ii     \n## 6 te1r\n\nTo finish off this section, let’s remove stop words (words that do not add any meaning to a sentence, such as “as”, “and”…) and words that are composed of less than 3 characters. You can find a dataset with stopwords inside the {stopwords} package:\n\nlibrary(stopwords)\n\ndata(data_stopwords_stopwordsiso)\n\neng_stopwords &lt;- tibble(\"content\" = data_stopwords_stopwordsiso$en)\n\nwinchester_content &lt;- winchester_content %&gt;% \n  anti_join(eng_stopwords) %&gt;% \n  filter(nchar(content) &gt; 3)\n## Joining, by = \"content\"\nhead(winchester_content)\n## # A tibble: 6 x 1\n##   content   \n##   &lt;chr&gt;     \n## 1 te1r      \n## 2 jilas     \n## 3 edition   \n## 4 winchester\n## 5 news      \n## 6 injuries\n\nThat’s it for this section! You now know how to work with strings, but in Chapter 10 we are going one step further by learning about regular expressions, which offer much more power."
  },
  {
    "objectID": "posts/2021-04-17-post_strat.html",
    "href": "posts/2021-04-17-post_strat.html",
    "title": "Dealing with non-representative samples with post-stratification",
    "section": "",
    "text": "Let’s go back to stats 101: what do you do if you want to know how many people like to play bingo in a certain population? The answer, of course, is to ask a sample of people if they enjoy playing bingo, compute the proportion and then… we’re done! Right? Well not exactly. This works if your sample is representative, which in practice, is not often the case. I am not an expert of survey methods, very far from it, but I was recently confronted to a similar issue at work. So in this blog post I want to talk about estimating a proportion using a sample that is not representative of the population using a method called “post-stratification”.\n\n\nBy the way, before continuing, I also made a video about this topic if you’re interested, watch it here.\n\n\nThe data I use in this blog post is simulated; so I know the “truth”, since I made the data, and can thus compare the results from post-stratification to the truth. At the end of the blog post, I will post the complete source code, but for now, let’s suppose that this is my sample:\n\nlibrary(tidyverse)\nlibrary(survey)\nlibrary(janitor)\nlibrary(brotools)\nmy_sample_1\n## # A tibble: 904 x 2\n##    age_group likes_bingo_1\n##    &lt;chr&gt;             &lt;dbl&gt;\n##  1 20-49                 0\n##  2 20-49                 0\n##  3 20-49                 0\n##  4 20-49                 0\n##  5 20-49                 0\n##  6 20-49                 0\n##  7 20-49                 0\n##  8 20-49                 0\n##  9 20-49                 0\n## 10 20-49                 1\n## # … with 894 more rows\n\nLet’s suppose that we have asked people two questions: their age, and whether or not they like bingo. Using this sample, I obtain the following result:\n\nresult &lt;- mean(my_sample_1$likes_bingo_1)\n\nSo according to this sample, 38.38% of people in my population like bingo. But is that right? Let’s use the other piece of information we have: the interviewee’s ages. This is the distribution of the age group in my sample:\n\nmy_sample_1 %&gt;%\n  tabyl(age_group)\n##  age_group   n    percent\n##        19-  40 0.04424779\n##      20-49 174 0.19247788\n##      50-79 540 0.59734513\n##        80+ 150 0.16592920\n\nWe want to compare this to the distribution of the same age groups in the population. Thankfully, this is something that is readily available in most (all?) countries. National statistical institutes publish such data on a yearly basis. This is the distribution in the population:\n\nage_distribution_population\n##  age_group     n    percent\n##        19- 12825 0.21865516\n##      20-49 25833 0.44043032\n##      50-79 17779 0.30311658\n##        80+  2217 0.03779793\n\nAs we can see, our sample is completely off! Elderly people are over-represented while younger people are under-represented. Perhaps this happened because elderly people love bingo more than younger people and, when given the opportunity to confess their love for bingo, are more willing to answer to a survey. Whatever the reason, it would be unreasonable to assume that the proportion given by our sample is a good, unbiased, estimate of the true proportion in the population.\n\n\nWhat we would like to do here, is to compute weights for each individual in the sample, such that individuals from over-represented groups contribute less to the computation of the proportion than individuals from under-represented groups. This is where post-stratification and raking come into play. As already said, I’m not an expert of these methods. So don’t believe that this blog post is a tutorial. However, what I’m going to show you might come in handy.\n\n\nWe’re going to use the {survey} package to compute the weights using raking, by post-stratifying the sample on age group. This can be done with two commands:\n\nunweighted_data &lt;- svydesign(ids = ~1, data = my_sample_1)\n## Warning in svydesign.default(ids = ~1, data = my_sample_1): No weights or\n## probabilities supplied, assuming equal probability\nweighted_data &lt;- rake(design = unweighted_data,\n                      sample.margins = list(~age_group),\n                      population.margins = list(pop_marginal_age))\n\nThe first function, svydesign() allows you to create a new object based on your data, which specifies the design of your study. In this case, I have used ids = ~1 to say “I don’t have any weights, nor anything specific to tell you”. Next, using the rake() function, I can compute the weights. For this, I need the object I created before, the variable I want to post-stratify on, and then give a table that contains the distribution of said variable in the population. This table looks a bit different from the one I already showed you: it doesn’t contain the categories’ frequencies, and the variable containing the counts is called Freq (rake() looks for this variable so it must be named like this):\n\npop_marginal_age \n##   age_group  Freq\n## 1       19- 12825\n## 2     20-49 25833\n## 3     50-79 17779\n## 4       80+  2217\n\nWe can now take a look at the weights:\n\nsummary(weights(weighted_data))\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   14.78   32.92   32.92   64.88   32.92  320.62\n\nIn cases where you have very high or very low weights, the literature recommends trimming them. However, I have not seen anything very definitive on this, and it seems that practitioners rely on rules of thumb and gut feeling to know when to trim weights. In my example here, I don’t think it is needed, but as I said, I have no intuition for this. Anyways, we are now ready to compute the new proportion:\n\nsvymean(~likes_bingo_1, weighted_data)\n##                  mean     SE\n## likes_bingo_1 0.19343 0.0121\n\nThe result is quite different from before (it was 38.38% in the “raw” sample)! Because I have simulated the data, I can now compare to the “true” value:\n\neusilcP %&gt;%\n  summarise(mean(likes_bingo_1))\n##   mean(likes_bingo_1)\n## 1           0.1830225\n\nAnd we’re quite close!\n\n\nNow let’s continue a little bit, with a more complicated example. Imagine that I collected five samples, one per week. Each sample contains totally different people (no person gets asked twice). Also, imagine that while I’m collecting my samples and analyzing them, bingo fever is running amok in my country, always infecting more and more people. As time passes, the proportion of people who love bingo keeps increasing. So my population’s parameter keeps changing, and each week, when I get a new sample, the proportion in my sample will also grow on a weekly basis. Because of this, I have to compute weights each week. Thankfully, the distribution of age groups in my population can be assumed to stay constant, so I don’t need to think about that.\n\n\nLet’s take a look at my sample which contains 5 weeks of data:\n\nsamples\n## # A tibble: 31,590 x 3\n##    age_group week            yes\n##    &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;\n##  1 20-49     likes_bingo_1     0\n##  2 20-49     likes_bingo_2     0\n##  3 20-49     likes_bingo_3     0\n##  4 20-49     likes_bingo_4     0\n##  5 20-49     likes_bingo_5     0\n##  6 20-49     likes_bingo_1     0\n##  7 20-49     likes_bingo_2     0\n##  8 20-49     likes_bingo_3     0\n##  9 20-49     likes_bingo_4     0\n## 10 20-49     likes_bingo_5     1\n## # … with 31,580 more rows\n\nEach row is one person, and this person gets sample exactly once. The yes variable collects the answer to the question “do you like bingo?”. Let’s see how my proportion evolves through time:\n\n(samples_likes_bingo_through_time &lt;- samples %&gt;%\n  group_by(week) %&gt;%\n  summarise(freq = mean(yes)))\n## # A tibble: 5 x 2\n##   week           freq\n##   &lt;chr&gt;         &lt;dbl&gt;\n## 1 likes_bingo_1 0.256\n## 2 likes_bingo_2 0.446\n## 3 likes_bingo_3 0.550\n## 4 likes_bingo_4 0.618\n## 5 likes_bingo_5 0.662\n\nWe see that it keeps increasing: this is a good sign, since we know that this is also the case in the population. We just don’t know by how much. Let’s compute weights for each week, and then recompute estimated proportions using these weights. In order to do this, I will write a function that will make it easy to do just that:\n\ncompute_weekly_weights &lt;- function(sample_df){\n\n  unweighted_data &lt;- svydesign(ids = ~1, data = sample_df)\n\n  rake(design = unweighted_data,\n       sample.margins = list(~age_group),\n       population.margins = list(pop_marginal_age))\n\n}\n\nThis function does the exact same thing as before. But it will now make it easy to apply to each week using the group_by-nest-map approach:\n\nweighted_samples &lt;- samples %&gt;%\n  group_nest(week) %&gt;%\n  mutate(weights = map(data, compute_weekly_weights)) %&gt;%\n  mutate(svymeans = map(weights, ~svymean(~yes, .)))\n## Warning in svydesign.default(ids = ~1, data = sample_df): No weights or\n## probabilities supplied, assuming equal probability\n\n## Warning in svydesign.default(ids = ~1, data = sample_df): No weights or\n## probabilities supplied, assuming equal probability\n\n## Warning in svydesign.default(ids = ~1, data = sample_df): No weights or\n## probabilities supplied, assuming equal probability\n\n## Warning in svydesign.default(ids = ~1, data = sample_df): No weights or\n## probabilities supplied, assuming equal probability\n\n## Warning in svydesign.default(ids = ~1, data = sample_df): No weights or\n## probabilities supplied, assuming equal probability\n\nLet’s take a look at this object:\n\nweighted_samples\n## # A tibble: 5 x 4\n##   week                    data weights    svymeans     \n##   &lt;chr&gt;         &lt;list&lt;tibble&gt;&gt; &lt;list&gt;     &lt;list&gt;       \n## 1 likes_bingo_1    [6,318 × 2] &lt;srvy.ds2&gt; &lt;svystat [1]&gt;\n## 2 likes_bingo_2    [6,318 × 2] &lt;srvy.ds2&gt; &lt;svystat [1]&gt;\n## 3 likes_bingo_3    [6,318 × 2] &lt;srvy.ds2&gt; &lt;svystat [1]&gt;\n## 4 likes_bingo_4    [6,318 × 2] &lt;srvy.ds2&gt; &lt;svystat [1]&gt;\n## 5 likes_bingo_5    [6,318 × 2] &lt;srvy.ds2&gt; &lt;svystat [1]&gt;\n\nSo for each week, I have now a svydesign object and also a new, hopefully unbiased, proportion of people who like bingo. The following lines simply but this into a nice tibble:\n\nweighted_samples &lt;- weighted_samples %&gt;%\n  mutate(svymeans = map(svymeans, as_tibble)) %&gt;%\n  select(week, svymeans) %&gt;%\n  unnest(cols = svymeans) %&gt;%\n  rename(freq = mean,\n         SE = yes) %&gt;%\n  mutate(is = \"corrected_sample\")\n\nTo conclude, let’s create a plot that compares the proportions computed without using weights to the proportions computed with weights to the true values that I simulated myself. I put everything in a data frame and the create the plot:\n\nall_data &lt;- bind_rows(weighted_samples, # my corrected data\n                      mutate(samples_likes_bingo_through_time, is = \"raw_sample\"), # the raw samples\n                      mutate(likes_bingo_through_time, is = \"true_value\")) %&gt;% # the true, simulated, values\n  mutate(SE = ifelse(is.na(SE), 0, SE))\n\nggplot(all_data) +\n  geom_ribbon(aes(y = freq, x = week,\n                  ymin = freq - 2*SE,\n                  ymax = freq + 2*SE,\n                  group = is),\n              fill = \"pink\",\n              alpha = .3) +\n  geom_line(aes(y = freq, x = week, colour = is, group = is)) +\n  theme_blog()\n\n\n\n\nWe can see that the proportions computed without weights were clearly over-estimating the true share of bingo enthusiasts in the population. The weighted proportions are very close to the true values and are acceptable estimates of the true proportions!\n\n\nIf you want to take a look at the source code, go here."
  },
  {
    "objectID": "posts/2018-10-05-ggplot2_purrr_officer.html",
    "href": "posts/2018-10-05-ggplot2_purrr_officer.html",
    "title": "Exporting editable plots from R to Powerpoint: making ggplot2 purrr with officer",
    "section": "",
    "text": "A kind reader let me know that the function create_pptx() is now outdated, and proposed an update which you can find here: here. Thank you @Jeremy!\n\n\nI was recently confronted to the following problem: creating hundreds of plots that could still be edited by our client. What this meant was that I needed to export the graphs in Excel or Powerpoint or some other such tool that was familiar to the client, and not export the plots directly to pdf or png as I would normally do. I still wanted to use R to do it though, because I could do what I always do to when I need to perform repetitive tasks such as producing hundreds of plots; map over a list of, say, countries, and make one plot per country. This is something I discussed in a previous blog post, Make ggplot2 purrr.\n\n\nSo, after some online seaching, I found the {officer} package. This package allows you to put objects into Microsoft documents. For example, editable plots in a Powerpoint document. This is what I will show in this blog post.\n\n\nLet’s start by loading the required packages:\n\nlibrary(\"tidyverse\")\nlibrary(\"officer\")\nlibrary(\"rvg\")\n\nThen, I will use the data from the time use survey, which I discussed in a previous blog post Going from a human readable Excel file to a machine-readable csv with {tidyxl}.\n\n\nYou can download the data here.\n\n\nLet’s import and prepare it:\n\ntime_use &lt;- rio::import(\"clean_data.csv\")\n\n\ntime_use &lt;- time_use %&gt;%\n    filter(population %in% c(\"Male\", \"Female\")) %&gt;%\n    filter(activities %in% c(\"Personal care\", \"Sleep\", \"Eating\", \n                             \"Employment\", \"Household and family care\")) %&gt;%\n    group_by(day) %&gt;%\n    nest()\n\nI only kept two categories, “Male” and “Female” and 5 activities. Then I grouped by day and nested the data. This is how it looks like:\n\ntime_use\n## # A tibble: 3 x 2\n##   day                         data             \n##   &lt;chr&gt;                       &lt;list&gt;           \n## 1 Year 2014_Monday til Friday &lt;tibble [10 × 4]&gt;\n## 2 Year 2014_Saturday          &lt;tibble [10 × 4]&gt;\n## 3 Year 2014_Sunday            &lt;tibble [10 × 4]&gt;\n\nAs shown, time_use is a tibble with 2 columns, the first day contains the days, and the second data, is of type list, and each element of these lists are tibbles themselves. Let’s take a look inside one:\n\ntime_use$data[1]\n## [[1]]\n## # A tibble: 10 x 4\n##    population activities                time  time_in_minutes\n##    &lt;chr&gt;      &lt;chr&gt;                     &lt;chr&gt;           &lt;int&gt;\n##  1 Male       Personal care             11:00             660\n##  2 Male       Sleep                     08:24             504\n##  3 Male       Eating                    01:46             106\n##  4 Male       Employment                08:11             491\n##  5 Male       Household and family care 01:59             119\n##  6 Female     Personal care             11:15             675\n##  7 Female     Sleep                     08:27             507\n##  8 Female     Eating                    01:48             108\n##  9 Female     Employment                06:54             414\n## 10 Female     Household and family care 03:49             229\n\nI can now create plots for each of the days with the following code:\n\nmy_plots &lt;- time_use %&gt;%\n    mutate(plots = map2(.y = day, .x = data, ~ggplot(data = .x) + theme_minimal() +\n                       geom_col(aes(y = time_in_minutes, x = activities, fill = population), \n                                position = \"dodge\") +\n                       ggtitle(.y) +\n                       ylab(\"Time in minutes\") +\n                       xlab(\"Activities\")))\n\nThese steps are all detailled in my blog post Make ggplot2 purrr. Let’s take a look at my_plots:\n\nmy_plots\n## # A tibble: 3 x 3\n##   day                         data              plots \n##   &lt;chr&gt;                       &lt;list&gt;            &lt;list&gt;\n## 1 Year 2014_Monday til Friday &lt;tibble [10 × 4]&gt; &lt;gg&gt;  \n## 2 Year 2014_Saturday          &lt;tibble [10 × 4]&gt; &lt;gg&gt;  \n## 3 Year 2014_Sunday            &lt;tibble [10 × 4]&gt; &lt;gg&gt;\n\nThe last column, called plots is a list where each element is a plot! We can take a look at one:\n\nmy_plots$plots[1]\n## [[1]]\n\n\n\n\nNow, this is where I could export these plots as pdfs or pngs. But this is not what I need. I need to export these plots as editable charts for Powerpoint. To do this for one image, I would do the following (as per {officer}’s documentation):\n\nread_pptx() %&gt;%\n    add_slide(layout = \"Title and Content\", master = \"Office Theme\") %&gt;%\n    ph_with_vg(code = print(one_plot), type = \"body\") %&gt;% \n    print(target = path)\n\nTo map this over a list of arguments, I wrote a wrapper:\n\ncreate_pptx &lt;- function(plot, path){\n    if(!file.exists(path)) {\n        out &lt;- read_pptx()\n    } else {\n        out &lt;- read_pptx(path)\n    }\n    \n    out %&gt;%\n        add_slide(layout = \"Title and Content\", master = \"Office Theme\") %&gt;%\n        ph_with_vg(code = print(plot), type = \"body\") %&gt;% \n        print(target = path)\n}\n\nThis function takes two arguments, plot and path. plot must be an plot object such as the ones contained inside the plots column of my_plots tibble. path is the path of where I want to save the pptx.\n\n\nThe first lines check if the file exists, if yes, the slides get added to the existing file, if not a new pptx gets created. The rest of the code is very similar to the one from the documentation. Now, to create my pptx I simple need to map over the plots column and provide a path:\n\nmap(my_plots$plots, create_pptx, path = \"test.pptx\")\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n## [[1]]\n## [1] \"/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx\"\n## \n## [[2]]\n## [1] \"/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx\"\n## \n## [[3]]\n## [1] \"/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx\"\n\nHere is the end result:\n\n\n\n\n\nInside Powerpoint (or in this case Libreoffice), the plots are geometric shapes that can now be edited!"
  },
  {
    "objectID": "posts/2021-02-20-covid_paper.html#packages-as-by-products-of-papers",
    "href": "posts/2021-02-20-covid_paper.html#packages-as-by-products-of-papers",
    "title": "R makes it too easy to write papers",
    "section": "\nPackages as by-products of papers\n",
    "text": "Packages as by-products of papers\n\n\nThe first thing I did was download data from the various open data portals, make sense of it and then plot it. At first, I did so in a very big a messy script file. As time went on, I felt more and more disgusted with this script and wanted to make something cleaner out of it. This is how the package I already mentioned above came to be. It took some time to prepare, but now it simplifies the process of updating my plots and machine learning models much faster. It also makes the paper more “interesting”; not everyone is interesting in the paper itself, but might be interested in the data, or in the process of making the package itself. I think that there are many examples of such packages as by-products of papers, especially papers that present and discuss new methods are very often accompanied by a package to make it easy for readers of the paper to use this new method. Package development is made easy with {usethis}."
  },
  {
    "objectID": "posts/2021-02-20-covid_paper.html#starting-a-draft-with-rticles",
    "href": "posts/2021-02-20-covid_paper.html#starting-a-draft-with-rticles",
    "title": "R makes it too easy to write papers",
    "section": "\nStarting a draft with {rticles}\n",
    "text": "Starting a draft with {rticles}\n\n\nThe second thing I did was start a draft with {rticles}. This package allows users to start a Rmarkdown draft with a single command. Users can choose among many different drafts for many different journals; I choose the arXiv draft, as I might publish the preprint there. To do so, I used the following command:\n\nrmarkdown::draft(\"paper.Rmd\", template = \"arxiv\", package = \"rticles\")\n\nI can now edit this Rmd file and compile it to a nice looking pdf very easily. But I don’t do so in the “traditional” way of knitting the Rmd file from Rstudio (or rather, from Spacemacs, my editor of choice). No, no, for this I use the magnificent {targets} package."
  },
  {
    "objectID": "posts/2021-02-20-covid_paper.html#setting-up-a-clean-automated-and-reproducible-workflow-with-targets",
    "href": "posts/2021-02-20-covid_paper.html#setting-up-a-clean-automated-and-reproducible-workflow-with-targets",
    "title": "R makes it too easy to write papers",
    "section": "\nSetting up a clean, automated and reproducible workflow with {targets}\n",
    "text": "Setting up a clean, automated and reproducible workflow with {targets}\n\n\n{targets} is the latest package by William Landau, who is also the author of {drake}. I was very impressed by {drake} and even made a video about it but now {targets} will replace {drake} as THE build automation tool for the R programming language. I started using it for this project, and just like {drake} it’s really an amazing package. It allows you to declare your project as a series of steps, each one of them being a call to a function. It’s very neat, and clean. The dependencies between each of the steps and objects that are created at each step are tracked by {targets} and should one of them get updated (for instance, because you changed the code of the underlying function), every object that depends on it will also get updated once you run the pipeline again.\n\n\nThis can get complex very quickly, and here is the network of objects, functions and their dependencies for the preprint I’m writing:"
  },
  {
    "objectID": "posts/2021-02-20-covid_paper.html#machine-learning-and-everything-else",
    "href": "posts/2021-02-20-covid_paper.html#machine-learning-and-everything-else",
    "title": "R makes it too easy to write papers",
    "section": "\nMachine learning, and everything else\n",
    "text": "Machine learning, and everything else\n\n\nLast year I wrote a blog post about {tidymodels}, which you can find here. Since then, the package evolved, and it’s in my opinion definitely one of the best machine learning packages out there. Just like the other tools I discussed in this blog post, it abstracts away many unimportant idiosyncrasies of many other packages and ways of doing things, and let’s you focus on what matters; getting results and presenting them neatly.\n\n\nI think that this is what I really like about the R programming language, and the ecosystem of packages built on top of it. Combining functional programming, build automation tools, markdown, and all the helper packages like {usethis} make it really easy to go from idea, to paper, or interactive app using {shiny} very quickly."
  },
  {
    "objectID": "posts/2017-12-17-teaching_tidyverse.html",
    "href": "posts/2017-12-17-teaching_tidyverse.html",
    "title": "Teaching the tidyverse to beginners",
    "section": "",
    "text": "End October I tweeted this:\n\n\n\nwill teach #rstats soon again but this time following @drob 's suggestion of the tidyverse first as laid out here: https://t.co/js8SsUs8Nv\n\n— Bruno Rodrigues (@brodriguesco) October 24, 2017\n\n\n\nand it generated some discussion. Some people believe that this is the right approach, and some others think that one should first present base R and then show how the tidyverse complements it. This year, I taught three classes; a 12-hour class to colleagues that work with me, a 15-hour class to master’s students and 3 hours again to some of my colleagues. Each time, I decided to focus on the tidyverse(almost) entirely, and must say that I am not disappointed with the results!\n\n\nThe 12 hour class was divided in two 6 hours days. It was a bit intense, especially the last 3 hours that took place Friday afternoon. The crowd was composed of some economists that had experience with STATA, some others that were mostly using Excel and finally some colleagues from the IT department that sometimes need to dig into some data themselves. Apart from 2 people, all the other never had any experience with R.\n\n\nWe went from 0 to being able to do the plot below after the end of the first day (so 6 hours in). Keep in mind that practically none of them even had opened RStudio before. I show the code so you can see the progress made in just a few hours:\n\nlibrary(Ecdat)\nlibrary(tidyverse)\nlibrary(ggthemes)\ndata(Bwages)\nbwages = Bwages %&gt;%\n  mutate(educ_level = case_when(educ == 1 ~ \"Primary School\",\n                                educ == 2 ~ \"High School\",\n                                educ == 3 ~ \"Some university\",\n                                educ == 4 ~ \"Master's degree\",\n                                educ == 5 ~ \"Doctoral degree\"))\n\nggplot(bwages) +\n  geom_smooth(aes(exper, wage, colour = educ_level)) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n## `geom_smooth()` using method = 'loess'\n\n\n\n\nOf course some of them needed some help here and there, and I also gave them hints (for example I told them about case_when() and try to use it inside mutate() instead of nested ifs) but it was mostly due to lack of experience and because they hadn’t had the time to fully digest R’s syntax which was for most people involved completely new.\n\n\nOn the second day I showed purrr::map() and purrr::reduce() and overall it went quite well too. I even showed list-columns, and this is where I started losing some of them; I did not insist too much on it though, only wanted to show them the flexibility of data.frame objects. Some of them were quite impressed by list-columns! Then I started showing (for and while) loops and writing functions. I even showed them tidyeval and again, it went relatively well. Once they had the opportunity to play a bit around with it, I think it clicked (plus they have lots of code examples to go back too).\n\n\nAt the end, people seemed to have enjoyed the course, but told me that Friday was heavy; indeed it was, but I feel that it was mostly because 12 hours spread on 2 days is not the best format for this type of material, but we all had time constraints.\n\n\nThe 15 hour Master’s course was spread over 4 days, and covered basically the same. I just used the last 3 hours to show the students some basic functions for model estimation (linear, count, logit/probit and survival models). Again, the students were quite impressed by how easily they could get descriptive statistics by first grouping by some variables. Through their questions, I even got to show them scoped versions of dplyr verbs, such as select_if() and summarise_at(). I was expecting to lose them there, but actually most of them got these scoped versions quite fast. These students already had some experience with R though, but none with the tidyverse.\n\n\nFinally the 3 hour course was perhaps the most interesting; I only had 100% total beginners. Some just knew R by name and had never heard/seen/opened RStudio (with the exception of one person)! I did not show them any loops, function definitions and no plots. I only showed them how RStudio looked and worked, what were (and how to install) packages (as well as the CRAN Task Views) and then how to import data with rio and do descriptive statistics only with dplyr. They were really interested and quite impressed by rio (“what do you mean I can use the same code for importing any dataset, in any format?”) but also by the simplicity of dplyr.\n\n\nIn all the courses, I did show the $ primitive to refer to columns inside a data.frame. First I showed them lists which is where I introduced $. Then it was easy to explain to them why it was the same for a column inside a data.frame; a data.frame is simply a list! This is also the distinction I made from the previous years; I simply mentioned (and showed really quickly) matrices and focused almost entirely on lists. Most participants, if not all, had learned to program statistics by thinking about linear algebra and matrices. Nothing wrong with that, but I feel that R really shines when you focus on lists and on how to work with them.\n\n\nOverall as the teacher, I think that focusing on the tidyverse might be a very good strategy. I might have to do some adjustments here and there for the future courses, but my hunch is that the difficulties that some participants had were not necessarily due to the tidyverse but simply to lack of time to digest what was shown, as well as a total lack of experience with R. I do not think that these participants would have better understood a more traditional, base, matrix-oriented course."
  },
  {
    "objectID": "posts/2018-06-24-fun_ts.html",
    "href": "posts/2018-06-24-fun_ts.html",
    "title": "Forecasting my weight with R",
    "section": "",
    "text": "I’ve been measuring my weight almost daily for almost 2 years now; I actually started earlier, but not as consistently. The goal of this blog post is to get re-acquaiented with time series; I haven’t had the opportunity to work with time series for a long time now and I have seen that quite a few packages that deal with time series have been released on CRAN. In this blog post, I will explore my weight measurements using some functions from the {tsibble} and {tibbletime} packages, and then do some predictions with the {forecast} package.\n\n\nFirst, let’s load the needed packages, read in the data and convert it to a tsibble:\n\nlibrary(\"tidyverse\")\nlibrary(\"readr\")\nlibrary(\"forecast\")\nlibrary(\"tsibble\")\nlibrary(\"tibbletime\")\nlibrary(\"mice\")\nweight &lt;- read_csv(\"https://gist.githubusercontent.com/b-rodrigues/ea60679135f8dbed448ccf66a216811f/raw/18b469f3b0720f76ce5ee2715d0f9574b615f170/gistfile1.txt\") %&gt;% \n    as_tsibble()\n## Parsed with column specification:\n## cols(\n##   Date = col_date(format = \"\"),\n##   Poids = col_double()\n## )\n## The `index` is `Date`.\n\nYou can read more about {tsibble} here. Here, I use {tsibble} mostly for the next step, which is using the function fill_na() on the tsibble. fill_na() turns implicit missing values into explicit missing values. These are implicit missing values:\n\n          Date Poids\n1   2013-01-01 84.10\n2   2013-01-04 85.60\n\nand this is the same view, but with explicit missing values:\n\n          Date Poids\n1   2013-01-01 84.10\n2   2013-01-02 NA\n3   2013-01-03 NA\n4   2013-01-04 85.60\n\nThis is useful to do, because I want to impute the missing values using the {mice} package. Let’s do this:\n\nweight &lt;- weight %&gt;% \n    fill_na()\n\nimp_weight &lt;- mice(data = weight) %&gt;% \n    mice::complete(\"long\")\n## \n##  iter imp variable\n##   1   1  Poids\n##   1   2  Poids\n##   1   3  Poids\n##   1   4  Poids\n##   1   5  Poids\n##   2   1  Poids\n##   2   2  Poids\n##   2   3  Poids\n##   2   4  Poids\n##   2   5  Poids\n##   3   1  Poids\n##   3   2  Poids\n##   3   3  Poids\n##   3   4  Poids\n##   3   5  Poids\n##   4   1  Poids\n##   4   2  Poids\n##   4   3  Poids\n##   4   4  Poids\n##   4   5  Poids\n##   5   1  Poids\n##   5   2  Poids\n##   5   3  Poids\n##   5   4  Poids\n##   5   5  Poids\n\nLet’s take a look at imp_weight:\n\nhead(imp_weight)\n##   .imp .id       Date Poids\n## 1    1   1 2013-10-28  84.1\n## 2    1   2 2013-10-29  84.4\n## 3    1   3 2013-10-30  83.5\n## 4    1   4 2013-10-31  84.1\n## 5    1   5 2013-11-01  85.6\n## 6    1   6 2013-11-02  85.2\n\nLet’s select the relevant data. I filter from the 11th of July 2016, which is where I started weighing myself almost every day, to the 31st of May 2018. I want to predict my weight for the month of June (you might think of the month of June 2018 as the test data, and the rest as training data):\n\nimp_weight_train &lt;- imp_weight %&gt;% \n    filter(Date &gt;= \"2016-07-11\", Date &lt;= \"2018-05-31\")\n\nIn the next lines, I create a column called imputation which is simply the same as the column .imp but of character class, remove unneeded columns and rename some other columns (“Poids” is French for weight):\n\nimp_weight_train &lt;- imp_weight_train %&gt;% \n    mutate(imputation = as.character(.imp)) %&gt;% \n    select(-.id, -.imp) %&gt;% \n    rename(date = Date) %&gt;% \n    rename(weight = Poids)\n\nLet’s take a look at the data:\n\nggplot(imp_weight_train, aes(date, weight, colour = imputation)) +\n    geom_line() + \n    theme(legend.position = \"bottom\")\n\n\n\n\nThis plots gives some info, but it might be better to smooth the lines. This is possible by computing a rolling mean. For this I will use the rollify() function of the {tibbletime} package:\n\nmean_roll_5 &lt;- rollify(mean, window = 5)\nmean_roll_10 &lt;- rollify(mean, window = 10)\n\nrollify() can be seen as an adverb, pretty much like purrr::safely(); rollify() is a higher order function that literally rollifies a function, in this case mean() which means that rollifying the mean creates a function that returns the rolling mean. The window argument lets you decide how smooth you want the curve to be: the higher the smoother. However, you will lose some observations. Let’s use this functions to add the rolling means to the data frame:\n\nimp_weight_train &lt;- imp_weight_train %&gt;% \n    group_by(imputation) %&gt;% \n    mutate(roll_5 = mean_roll_5(weight),\n           roll_10 = mean_roll_10(weight))\n\nNow, let’s plot these new curves:\n\nggplot(imp_weight_train, aes(date, roll_5, colour = imputation)) +\n    geom_line() + \n    theme(legend.position = \"bottom\")\n## Warning: Removed 20 rows containing missing values (geom_path).\n\n\n\nggplot(imp_weight_train, aes(date, roll_10, colour = imputation)) +\n    geom_line() + \n    theme(legend.position = \"bottom\")\n## Warning: Removed 45 rows containing missing values (geom_path).\n\n\n\n\nThat’s easier to read, isn’t it?\n\n\nNow, I will use the auto.arima() function to train a model on the data to forecast my weight for the month of June. However, my data, imp_weight_train is a list of datasets. auto.arima() does not take a data frame as an argument, much less so a list of datasets. I’ll create a wrapper around auto.arima() that works on a dataset, and then map it to the list of datasets:\n\nauto.arima.df &lt;- function(data, y, ...){\n\n    y &lt;- enquo(y)\n\n    yts &lt;- data %&gt;% \n        pull(!!y) %&gt;% \n        as.ts()\n\n    auto.arima(yts, ...)\n}\n\nauto.arima.df() takes a data frame as argument, and then y, which is the column that contains the univariate time series. This column then gets pulled out of the data frame, converted to a time series object with as.ts(), and then passed down to auto.arima(). I can now use this function on my list of data sets. The first step is to nest the data:\n\nnested_data &lt;- imp_weight_train %&gt;% \n    group_by(imputation) %&gt;% \n    nest() \n\nLet’s take a look at nested_data:\n\nnested_data\n## # A tibble: 5 x 2\n##   imputation data              \n##   &lt;chr&gt;      &lt;list&gt;            \n## 1 1          &lt;tibble [690 × 4]&gt;\n## 2 2          &lt;tibble [690 × 4]&gt;\n## 3 3          &lt;tibble [690 × 4]&gt;\n## 4 4          &lt;tibble [690 × 4]&gt;\n## 5 5          &lt;tibble [690 × 4]&gt;\n\nnested_data is a tibble with a column called data, which is a so-called list-column. Each element of data is itself a tibble. This is a useful structure, because now I can map auto.arima.df() to the data frame:\n\nmodels &lt;- nested_data %&gt;% \n    mutate(model = map(data, auto.arima.df, y = weight))\n\nThis trick can be a bit difficult to follow the first time you see it. The idea is the following: nested_data is a tibble. Thus, I can add a column to it using mutate(). So far so good. Now that I am “inside” the mutate call, I can use purrr::map(). Why? purrr::map() takes a list and then a function as arguments. Remember that data is a list column; you can see it above, the type of the column data is list. So data is a list, and thus can be used inside purrr::map(). Great. Now, what is inside data? tibbles, where inside each of them is a column called weight. This is the column that contains my univariate time series I want to model. Let’s take a look at models:\n\nmodels\n## # A tibble: 5 x 3\n##   imputation data               model      \n##   &lt;chr&gt;      &lt;list&gt;             &lt;list&gt;     \n## 1 1          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;\n## 2 2          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;\n## 3 3          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;\n## 4 4          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;\n## 5 5          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;\n\nmodels is a tibble with a column called model, where each element is a model of type ARIMA.\n\n\nAdding forecasts is based on the same trick as above, and we use the forecast() function:\n\nforecasts &lt;- models %&gt;% \n    mutate(predictions = map(model, forecast, h = 24)) %&gt;% \n    mutate(predictions = map(predictions, as_tibble)) %&gt;% \n    pull(predictions) \n\nI forecast 24 days (I am writing this on the 24th of June), and convert the predictions to tibbles, and then pull only the predictions tibble:\n\nforecasts\n## [[1]]\n## # A tibble: 24 x 5\n##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`\n##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1             71.5    70.7    72.3    70.2    72.8\n##  2             71.5    70.7    72.4    70.3    72.8\n##  3             71.5    70.6    72.3    70.1    72.8\n##  4             71.5    70.6    72.4    70.1    72.9\n##  5             71.4    70.5    72.4    70.0    72.9\n##  6             71.5    70.5    72.4    70.0    72.9\n##  7             71.4    70.5    72.4    69.9    72.9\n##  8             71.4    70.4    72.4    69.9    72.9\n##  9             71.4    70.4    72.4    69.9    72.9\n## 10             71.4    70.4    72.4    69.8    73.0\n## # ... with 14 more rows\n## \n## [[2]]\n## # A tibble: 24 x 5\n##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`\n##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1             71.6    70.8    72.3    70.3    72.8\n##  2             71.6    70.8    72.5    70.3    72.9\n##  3             71.5    70.6    72.4    70.2    72.9\n##  4             71.5    70.6    72.5    70.1    72.9\n##  5             71.5    70.5    72.5    70.0    73.0\n##  6             71.5    70.5    72.5    70.0    73.0\n##  7             71.5    70.5    72.5    69.9    73.0\n##  8             71.5    70.4    72.5    69.9    73.1\n##  9             71.5    70.4    72.5    69.8    73.1\n## 10             71.4    70.3    72.6    69.7    73.1\n## # ... with 14 more rows\n## \n## [[3]]\n## # A tibble: 24 x 5\n##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`\n##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1             71.6    70.8    72.4    70.4    72.8\n##  2             71.5    70.7    72.4    70.2    72.8\n##  3             71.5    70.6    72.4    70.2    72.9\n##  4             71.5    70.6    72.4    70.1    72.9\n##  5             71.5    70.5    72.4    70.0    72.9\n##  6             71.5    70.5    72.4    70.0    73.0\n##  7             71.5    70.5    72.5    69.9    73.0\n##  8             71.4    70.4    72.5    69.9    73.0\n##  9             71.4    70.4    72.5    69.8    73.0\n## 10             71.4    70.4    72.5    69.8    73.1\n## # ... with 14 more rows\n## \n## [[4]]\n## # A tibble: 24 x 5\n##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`\n##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1             71.5    70.8    72.3    70.3    72.8\n##  2             71.5    70.7    72.4    70.3    72.8\n##  3             71.5    70.7    72.4    70.2    72.8\n##  4             71.5    70.6    72.4    70.1    72.9\n##  5             71.5    70.6    72.4    70.1    72.9\n##  6             71.5    70.5    72.5    70.0    73.0\n##  7             71.5    70.5    72.5    69.9    73.0\n##  8             71.5    70.4    72.5    69.9    73.0\n##  9             71.4    70.4    72.5    69.8    73.1\n## 10             71.4    70.3    72.5    69.8    73.1\n## # ... with 14 more rows\n## \n## [[5]]\n## # A tibble: 24 x 5\n##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`\n##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1             71.5    70.8    72.3    70.3    72.8\n##  2             71.5    70.7    72.4    70.3    72.8\n##  3             71.5    70.7    72.4    70.2    72.8\n##  4             71.5    70.6    72.4    70.1    72.9\n##  5             71.5    70.6    72.4    70.1    72.9\n##  6             71.5    70.5    72.4    70.0    73.0\n##  7             71.5    70.5    72.5    69.9    73.0\n##  8             71.5    70.4    72.5    69.9    73.0\n##  9             71.4    70.4    72.5    69.8    73.1\n## 10             71.4    70.3    72.5    69.8    73.1\n## # ... with 14 more rows\n\nSo forecasts is a list of tibble, each containing a forecast. Remember that I have 5 tibbles, because I imputed the data 5 times. I will merge this list of data sets together into one, but before I need to add a column that indices the forecasts:\n\nforecasts &lt;- map2(.x = forecasts, .y = as.character(seq(1, 5)), \n     ~mutate(.x, id = .y)) %&gt;% \n    bind_rows() %&gt;% \n    select(-c(`Lo 80`, `Hi 80`))\n\ncolnames(forecasts) &lt;- c(\"point_forecast\", \"low_95\", \"hi_95\", \"id\")\n\nLet’s take a look again at forecasts:\n\nforecasts\n## # A tibble: 120 x 4\n##    point_forecast low_95 hi_95 id   \n##             &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n##  1           71.5   70.2  72.8 1    \n##  2           71.5   70.3  72.8 1    \n##  3           71.5   70.1  72.8 1    \n##  4           71.5   70.1  72.9 1    \n##  5           71.4   70.0  72.9 1    \n##  6           71.5   70.0  72.9 1    \n##  7           71.4   69.9  72.9 1    \n##  8           71.4   69.9  72.9 1    \n##  9           71.4   69.9  72.9 1    \n## 10           71.4   69.8  73.0 1    \n## # ... with 110 more rows\n\nI now select the true values for the month of June. I also imputed this data, but here I will simply keep the average of the imputations:\n\nweight_june &lt;- imp_weight %&gt;% \n    filter(Date &gt;= \"2018-06-01\") %&gt;% \n    select(-.id) %&gt;% \n    group_by(Date) %&gt;% \n    summarise(true_weight = mean(Poids)) %&gt;% \n    rename(date = Date)\n\nLet’s take a look at weight_june:\n\nweight_june\n## # A tibble: 24 x 2\n##    date       true_weight\n##    &lt;date&gt;           &lt;dbl&gt;\n##  1 2018-06-01        71.8\n##  2 2018-06-02        70.8\n##  3 2018-06-03        71.2\n##  4 2018-06-04        71.4\n##  5 2018-06-05        70.9\n##  6 2018-06-06        70.8\n##  7 2018-06-07        70.5\n##  8 2018-06-08        70.1\n##  9 2018-06-09        70.3\n## 10 2018-06-10        71.0\n## # ... with 14 more rows\n\nLet’s repeat weight_june 5 times, and add the index 1 to 5. Why? Because I want to merge the true data with the forecasts, and having the data in this form makes things easier:\n\nweight_june &lt;- modify(list_along(1:5), ~`&lt;-`(., weight_june)) %&gt;% \n    map2(.y = as.character(seq(1, 5)), \n         ~mutate(.x, id = .y)) %&gt;% \n    bind_rows()\n\nThe first line:\n\nmodify(list_along(1:5), ~`&lt;-`(., weight_june)) \n\nlooks quite complicated, but you will see that it is not, once we break it apart. modify() modifies a list. The list to modify is list_along(1:5), which create a list of NULLs:\n\nlist_along(1:5)\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n\nThe second argument of modify() is either a function or a formula. I created the following formula:\n\n~`&lt;-`(., weight_june)\n\nWe all know the function &lt;-(), but are not used to see it that way. But consider the following:\n\na &lt;- 3\n`&lt;-`(a, 3)\n\nThese two formulations are equivalent. So these lines fill the empty element of the list of NULLs with the data frame weight_june. Then I add the id column and then bind the rows together: bind_rows().\n\n\nLet’s bind the columns of weight_june and forecasts and take a look at it:\n\nforecasts &lt;- bind_cols(weight_june, forecasts) %&gt;% \n    select(-id1)\n\nforecasts\n## # A tibble: 120 x 6\n##    date       true_weight id    point_forecast low_95 hi_95\n##    &lt;date&gt;           &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n##  1 2018-06-01        71.8 1               71.5   70.2  72.8\n##  2 2018-06-02        70.8 1               71.5   70.3  72.8\n##  3 2018-06-03        71.2 1               71.5   70.1  72.8\n##  4 2018-06-04        71.4 1               71.5   70.1  72.9\n##  5 2018-06-05        70.9 1               71.4   70.0  72.9\n##  6 2018-06-06        70.8 1               71.5   70.0  72.9\n##  7 2018-06-07        70.5 1               71.4   69.9  72.9\n##  8 2018-06-08        70.1 1               71.4   69.9  72.9\n##  9 2018-06-09        70.3 1               71.4   69.9  72.9\n## 10 2018-06-10        71.0 1               71.4   69.8  73.0\n## # ... with 110 more rows\n\nNow, for the last plot:\n\nggplot(forecasts, aes(x = date, colour = id)) +\n    geom_line(aes(y = true_weight), size = 2) + \n    geom_line(aes(y = hi_95)) + \n    geom_line(aes(y = low_95)) + \n    theme(legend.position = \"bottom\")\n\n\n\n\nThe true data fall within all the confidence intervals, but I am a bit surprised by the intervals, especially the upper confidence intervals; they all are way above 72kg, however my true weight has been fluctuating around 71kg for quite some months now. I think I have to refresh my memory on time series, because I am certainly missing something!\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2022-02-18-loudly.html#update-loud-has-been-superseded-by-chronicle-read-about-it-here",
    "href": "posts/2022-02-18-loudly.html#update-loud-has-been-superseded-by-chronicle-read-about-it-here",
    "title": "Add logging to your functions using my newest package {loud}",
    "section": "\nUPDATE: {loud} has been superseded by {chronicle}, read about it here\n",
    "text": "UPDATE: {loud} has been superseded by {chronicle}, read about it here\n\n\nThis is a short blog post to announce the early alpha, hyper unstable, use at your own peril, package I’ve been working on for the past 6 hours or so (actually longer if I add all the research/study time). This package provides the function loudly() which allows you to do cool stuff like:\n\n# First two lines install the package\n# install.packages(\"devtools\")\n# devtools::install_github(\"b-rodrigues/loud\")\nlibrary(loud)\n## Loading required package: rlang\nloud_sqrt &lt;- loudly(sqrt)\n\nloud_sqrt(1:10)\n## $result\n##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n##  [9] 3.000000 3.162278\n## \n## $log\n## [1] \"Log start...\"                                                                \n## [2] \"✔ sqrt(1:10) started at 2022-04-01 21:20:00 and ended at 2022-04-01 21:20:00\"\n\nAs you can see, I start by applying loudly() to a function, and then I can use this function as usual. Not only do I get the result, but also a logging message telling me which function and which arguments got used, and when the computation started and ended.\n\n\nIt is also possible to chain operations:\n\nloud_sqrt &lt;- loudly(sqrt)\nloud_exp &lt;- loudly(exp)\nloud_mean &lt;- loudly(mean)\n\n1:10 |&gt;\n  loud_sqrt() |&gt;\n  bind_loudly(loud_exp) |&gt;\n  bind_loudly(loud_mean)\n## $result\n## [1] 11.55345\n## \n## $log\n## [1] \"Log start...\"                                                                     \n## [2] \"✔ sqrt(1:10) started at 2022-04-01 21:20:00 and ended at 2022-04-01 21:20:00\"     \n## [3] \"✔ exp(.l$result) started at 2022-04-01 21:20:00 and ended at 2022-04-01 21:20:00\" \n## [4] \"✔ mean(.l$result) started at 2022-04-01 21:20:00 and ended at 2022-04-01 21:20:00\"\n\nYou’ll notice that here I have to use another function called bind_loudly(). The reason is because loud functions return a list. The first element of that list is the result of the function applied to the inputs, and the second element is the log message. So bind_loudly() passes the first element of the output of loud_sqrt() to the actual function exp() and also passes the second element, this time the log message, to the part of the function that concatenates the log messages.\n\n\nThis works with any function:\n\nlibrary(dplyr)\nloud_group_by &lt;- loudly(group_by)\nloud_select &lt;- loudly(select)\nloud_summarise &lt;- loudly(summarise)\nloud_filter &lt;- loudly(filter)\n\nstarwars %&gt;%\n  loud_select(height, mass, species, sex) %&gt;%\n  bind_loudly(loud_group_by, species, sex) %&gt;%\n  bind_loudly(loud_filter, sex != \"male\") %&gt;%\n  bind_loudly(loud_summarise,\n              mass = mean(mass, na.rm = TRUE)\n              )\n## $result\n## # A tibble: 9 × 3\n## # Groups:   species [9]\n##   species    sex              mass\n##   &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n## 1 Clawdite   female           55  \n## 2 Droid      none             69.8\n## 3 Human      female           56.3\n## 4 Hutt       hermaphroditic 1358  \n## 5 Kaminoan   female          NaN  \n## 6 Mirialan   female           53.1\n## 7 Tholothian female           50  \n## 8 Togruta    female           57  \n## 9 Twi'lek    female           55  \n## \n## $log\n## [1] \"Log start...\"                                                                                                   \n## [2] \"✔ select(.,height,mass,species,sex) started at 2022-04-01 21:20:00 and ended at 2022-04-01 21:20:00\"            \n## [3] \"✔ group_by(.l$result,species,sex) started at 2022-04-01 21:20:00 and ended at 2022-04-01 21:20:00\"              \n## [4] \"✔ filter(.l$result,sex != \\\"male\\\") started at 2022-04-01 21:20:00 and ended at 2022-04-01 21:20:00\"            \n## [5] \"✔ summarise(.l$result,mean(mass, na.rm = TRUE)) started at 2022-04-01 21:20:00 and ended at 2022-04-01 21:20:00\"\n\nThis is not perfect however. You’ll notice that the last log message states:\n\nsummarise(.l$result,mean(mass, na.rm = TRUE)) ....\n\nideally I would like for it to say:\n\nsummarise(.l$result,mass = mean(mass, na.rm = TRUE)) ....\n\nAlso, I’ve added a pipe operator so you don’t need to use bind_loudly() if you don’t want to:\n\n1:10 |&gt;\n  loud_sqrt() %&gt;=%\n  loud_exp() %&gt;=%\n  loud_mean()\n## $result\n## [1] 11.55345\n## \n## $log\n## [1] \"Log start...\"                                                                     \n## [2] \"✔ sqrt(1:10) started at 2022-04-01 21:20:00 and ended at 2022-04-01 21:20:00\"     \n## [3] \"✔ exp(.l$result) started at 2022-04-01 21:20:00 and ended at 2022-04-01 21:20:00\" \n## [4] \"✔ mean(.l$result) started at 2022-04-01 21:20:00 and ended at 2022-04-01 21:20:00\"\n\nHowever, this operator does not work well with {dplyr} functions. See here:\n\nstarwars %&gt;%\n  loud_select(height, mass, species, sex) %&gt;=%\n  loud_group_by(species, sex) %&gt;=%\n  loud_filter(sex != \"male\") %&gt;=%\n  loud_summarise(mass = mean(mass, na.rm = TRUE))\n## $result\n## # A tibble: 9 × 3\n## # Groups:   species [9]\n##   species    sex              mass\n##   &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n## 1 Clawdite   female           55  \n## 2 Droid      none             69.8\n## 3 Human      female           56.3\n## 4 Hutt       hermaphroditic 1358  \n## 5 Kaminoan   female          NaN  \n## 6 Mirialan   female           53.1\n## 7 Tholothian female           50  \n## 8 Togruta    female           57  \n## 9 Twi'lek    female           55  \n## \n## $log\n## [1] \"Log start...\"                                                                                                   \n## [2] \"✔ select(.,height,mass,species,sex) started at 2022-04-01 21:20:00 and ended at 2022-04-01 21:20:00\"            \n## [3] \"✔ group_by(.l$result,species,sex) started at 2022-04-01 21:20:00 and ended at 2022-04-01 21:20:00\"              \n## [4] \"✔ filter(.l$result,sex != \\\"male\\\") started at 2022-04-01 21:20:00 and ended at 2022-04-01 21:20:00\"            \n## [5] \"✔ summarise(.l$result,mean(mass, na.rm = TRUE)) started at 2022-04-01 21:20:00 and ended at 2022-04-01 21:20:00\"\n\nIf you look at the result, you’ll see that it is not equal to the obtained with bind_loudly(), and if you look at the last logging message you’ll see why. Instead of\n\nsummarise(.l$result,mean(mass, na.rm = TRUE)) ....\n\nthe message states:\n\nsummarise(.l$result,mass,TRUE) started at\n\nI know where the problem is (it’s due to some regex fuckery) so I think that I should be able to correct this in the coming days. Ideally, in the future, I would also like for the users to provide their own log messages.\n\n\nThe package has a website with a vignette that shows another interesting example here. Source code can be found here.\n\n\nIt is almost certain that function names will change, maybe even the package name itself. Contributions, bug reports, suggestions, etc, welcome of course.\n\n\nA final word: this is the result of me exploring more advanced functional programming concepts and discussing with really nice people like Andrew R Mcneil, Laszlo Kupcsik. Andrew wrote a cool package called maybe and Laszlo a super cool blog post explaining what monads are here.\n\n\nI’ll be writing a blog post on monads, in particular the maybe monad soonish."
  },
  {
    "objectID": "posts/2017-12-27-build_formulae.html",
    "href": "posts/2017-12-27-build_formulae.html",
    "title": "Building formulae",
    "section": "",
    "text": "This Stackoverflow question made me think about how to build formulae. For example, you might want to programmatically build linear model formulae and then map these models on data. For example, suppose the following (output suppressed):\n\ndata(mtcars)\n\nlm(mpg ~ hp, data = mtcars)\nlm(mpg ~I(hp^2), data = mtcars)\nlm(mpg ~I(hp^3), data = mtcars)\nlm(mpg ~I(hp^4), data = mtcars)\nlm(mpg ~I(hp^5), data = mtcars)\nlm(mpg ~I(hp^6), data = mtcars)\n\nTo avoid doing this, one can write a function that builds the formulae:\n\ncreate_form = function(power){\n  rhs = substitute(I(hp^pow), list(pow=power))\n  rlang::new_formula(quote(mpg), rhs)\n}\n\nIf you are not familiar with substitute(), try the following to understand what it does:\n\nsubstitute(y ~ x, list(x = 1))\n## y ~ 1\n\nThen using rlang::new_formula() I build a formula by providing the left hand side, which is quote(mpg) here, and the right hand side, which I built using substitute(). Now I can create a list of formulae:\n\nlibrary(tidyverse)\n\nlist_formulae = map(seq(1, 6), create_form)\n\nstr(list_formulae)\n## List of 6\n##  $ :Class 'formula'  language mpg ~ I(hp^1L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605f897ca0&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^2L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605f891418&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^3L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605da76098&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^4L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605da6a600&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^5L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605da68980&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^6L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605da66d38&gt;\n\nAs you can see, power got replaced by 1, 2, 3,… and each element of the list is a nice formula. Exactly what lm() needs. So now it’s easy to map lm() to this list of formulae:\n\ndata(mtcars)\n\nmap(list_formulae, lm, data = mtcars)\n## [[1]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^1)  \n##    30.09886     -0.06823  \n## \n## \n## [[2]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^2)  \n##  24.3887252   -0.0001649  \n## \n## \n## [[3]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^3)  \n##   2.242e+01   -4.312e-07  \n## \n## \n## [[4]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^4)  \n##   2.147e+01   -1.106e-09  \n## \n## \n## [[5]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^5)  \n##   2.098e+01   -2.801e-12  \n## \n## \n## [[6]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^6)  \n##   2.070e+01   -7.139e-15\n\nThis is still a new topic for me there might be more elegant ways to do that, using tidyeval to remove the hardcoding of the columns in create_form(). I might continue exploring this."
  },
  {
    "objectID": "posts/2018-07-01-tidy_ive.html",
    "href": "posts/2018-07-01-tidy_ive.html",
    "title": "Missing data imputation and instrumental variables regression: the tidy approach",
    "section": "",
    "text": "In this blog post I will discuss missing data imputation and instrumental variables regression. This is based on a short presentation I will give at my job. You can find the data used here on this website: http://eclr.humanities.manchester.ac.uk/index.php/IV_in_R\n\n\nThe data is used is from Wooldridge’s book, Econometrics: A modern Approach. You can download the data by clicking here.\n\n\nThis is the variable description:\n\n 1. inlf                     =1 if in labor force, 1975\n 2. hours                    hours worked, 1975\n 3. kidslt6                  # kids &lt; 6 years\n 4. kidsge6                  # kids 6-18\n 5. age                      woman's age in yrs\n 6. educ                     years of schooling\n 7. wage                     estimated wage from earns., hours\n 8. repwage                  reported wage at interview in 1976\n 9. hushrs                   hours worked by husband, 1975\n10. husage                   husband's age\n11. huseduc                  husband's years of schooling\n12. huswage                  husband's hourly wage, 1975\n13. faminc                   family income, 1975\n14. mtr                      fed. marginal tax rate facing woman\n15. motheduc                 mother's years of schooling\n16. fatheduc                 father's years of schooling\n17. unem                     unem. rate in county of resid.\n18. city                     =1 if live in SMSA\n19. exper                    actual labor mkt exper\n20. nwifeinc                 (faminc - wage*hours)/1000\n21. lwage                    log(wage)\n22. expersq                  exper^2\n\nThe goal is to first impute missing data in the data set, and then determine the impact of one added year of education on wages. If one simply ignores missing values, bias can be introduced depending on the missingness mechanism. The second problem here is that education is likely to be endogeneous (and thus be correlated to the error term), as it is not randomly assigned. This causes biased estimates and may lead to seriously wrong conclusions. So missingness and endogeneity should be dealt with, but dealing with both issues is more of a programming challenge than an econometrics challenge. Thankfully, the packages contained in the {tidyverse} as well as {mice} will save the day!\n\n\nIf you inspect the data, you will see that there are no missing values. So I will use the {mice} package to first ampute the data (which means adding missing values). This, of course, is done for education purposes. If you’re lucky enough to not have missing values in your data, you shouldn’t add them!\n\n\nLet’s load all the packages needed:\n\nlibrary(tidyverse)\nlibrary(AER)\nlibrary(naniar)\nlibrary(mice)\n\nSo first, let’s read in the data, and ampute it:\n\nwages_data &lt;- read_csv(\"http://eclr.humanities.manchester.ac.uk/images/5/5f/Mroz.csv\")\n## Parsed with column specification:\n## cols(\n##   .default = col_integer(),\n##   wage = col_character(),\n##   repwage = col_double(),\n##   huswage = col_double(),\n##   mtr = col_double(),\n##   unem = col_double(),\n##   nwifeinc = col_double(),\n##   lwage = col_character()\n## )\n## See spec(...) for full column specifications.\n\nFirst, I only select the variables I want to use and convert them to the correct class:\n\nwages_data &lt;- wages_data %&gt;% \n    select(wage, educ, fatheduc, motheduc, inlf, hours, \n               kidslt6, kidsge6, age, huswage, \n               mtr, unem, city, exper) %&gt;% \n    mutate_at(vars(kidslt6, kidsge6, hours, educ, age, wage, huswage, mtr,\n                    motheduc, fatheduc, unem, exper), as.numeric) %&gt;% \n    mutate_at(vars(city, inlf), as.character)\n## Warning in evalq(as.numeric(wage), &lt;environment&gt;): NAs introduced by\n## coercion\n\nIn the data, some women are not in the labour force, and thus do not have any wages; meaning they should have a 0 there. Instead, this is represented with the following symbol: “.”. So I convert these dots to 0. One could argue that the wages should not be 0, but that they’re truly missing. This is true, and there are ways to deal with such questions (Heckman’s selection model for instance), but this is not the point here.\n\nwages_data &lt;- wages_data %&gt;% \n    mutate(wage = ifelse(is.na(wage), 0, wage))\n\nLet’s double check if there are any missing values in the data, using naniar::vis_miss():\n\nvis_miss(wages_data)\n\n\n\n\nNope! Let’s ampute it:\n\nwages_mis &lt;- ampute(wages_data)$amp\n## Warning: Data is made numeric because the calculation of weights requires\n## numeric data\n\nampute() returns an object where the amp element is the amputed data. This is what I save into the new variable wages_mis.\n\n\nLet’s take a look:\n\nvis_miss(wages_mis)\n\n\n\n\nOk, so now we have missing values. Let’s use the recently added mice::parlmice() function to impute the dataset, in parallel:\n\nimp_wages &lt;- parlmice(data = wages_mis, m = 10, maxit = 20, cl.type = \"FORK\")\n\nFor reproducibility, I save these objects to disk:\n\nwrite_csv(wages_mis, \"wages_mis.csv\")\n\nsaveRDS(imp_wages, \"imp_wages.rds\")\n\nAs a sanity check, let’s look at the missingness pattern for the first completed dataset:\n\nvis_miss(complete(imp_wages))\n\n\n\n\nmice::parlmice() was able to impute the dataset. I imputed it 10 times, so now I have 10 imputed datasets. If I want to estimate a model using this data, I will need to do so 10 times. This is where the tidyverse comes into play. First, let’s combine all the 10 imputed datasets into one long dataset, with an index to differentiate them. This is done easily with mice::complete():\n\nimp_wages_df &lt;- mice::complete(imp_wages, \"long\")\n\nLet’s take a look at the data:\n\nhead(imp_wages_df)\n##   .imp .id   wage educ fatheduc motheduc inlf hours kidslt6 kidsge6 age\n## 1    1   1 3.3540   12        7       12    1  1610       1       0  32\n## 2    1   2 1.3889   12        7        7    1  1656       0       2  30\n## 3    1   3 4.5455   12        7       12    1  1980       0       3  35\n## 4    1   4 1.0965   12        7        7    1   456       0       3  34\n## 5    1   5 4.5918   14       14       12    1  1568       1       2  31\n## 6    1   6 4.7421   12        7       14    1  2032       0       0  54\n##   huswage    mtr unem city exper\n## 1  4.0288 0.7215  5.0    0    14\n## 2  8.4416 0.6615 11.0    1     5\n## 3  3.5807 0.6915  5.0    0    15\n## 4  3.5417 0.7815  5.0    0     6\n## 5 10.0000 0.6215  9.5    1    14\n## 6  4.7364 0.6915  7.5    1    33\n\nAs you can see, there are two new columns, .id and .imp. .imp equals i means that it is the ith imputed dataset.\n\n\nBecause I have 0’s in my dependent variable, I will not log the wages but instead use the Inverse Hyperbolic Sine transformation. Marc F. Bellemare wrote a nice post about it here.\n\nihs &lt;- function(x){\n    log(x + sqrt(x**2 + 1))\n}\n\nI can now apply this function, but first I have to group by .imp. Remember, these are 10 separated datasets. I also create the experience squared:\n\nimp_wages_df &lt;- imp_wages_df %&gt;% \n    group_by(.imp) %&gt;% \n    mutate(ihs_wage = ihs(wage),\n           exper2 = exper**2)\n\nNow comes some tidyverse magic. I will create a new dataset by using the nest() function from tidyr.\n\n(imp_wages &lt;- imp_wages_df %&gt;% \n    group_by(.imp) %&gt;% \n    nest())\n## # A tibble: 10 x 2\n##     .imp data               \n##    &lt;int&gt; &lt;list&gt;             \n##  1     1 &lt;tibble [753 × 17]&gt;\n##  2     2 &lt;tibble [753 × 17]&gt;\n##  3     3 &lt;tibble [753 × 17]&gt;\n##  4     4 &lt;tibble [753 × 17]&gt;\n##  5     5 &lt;tibble [753 × 17]&gt;\n##  6     6 &lt;tibble [753 × 17]&gt;\n##  7     7 &lt;tibble [753 × 17]&gt;\n##  8     8 &lt;tibble [753 × 17]&gt;\n##  9     9 &lt;tibble [753 × 17]&gt;\n## 10    10 &lt;tibble [753 × 17]&gt;\n\nAs you can see, imp_wages is now a dataset with two columns: .imp, indexing the imputed datasets, and a column called data, where each element is itself a tibble! data is a so-called list-column. You can read more about it on the purrr tutorial written by Jenny Bryan.\n\n\nEstimating a model now is easy, if you’re familiar with purrr. This is how you do it:\n\nimp_wages_reg = imp_wages %&gt;% \n    mutate(lin_reg = map(data, \n                         ~lm(ihs_wage ~ educ + inlf + hours + \n                                 kidslt6 + kidsge6 + age + huswage + \n                                 mtr + unem + city + exper + exper2, \n                             data = .)))\n\nOk, so what happened here? imp_wages is a data frame, so it’s possible to add a column to it with mutate(). I call that column lin_reg and use map() on the column called data (remember, this column is actually a list of data frame objects, and map() takes a list as an argument, and then a function or formula) with the following formula:\n\n~lm(ihs_wage ~ educ + inlf + hours + \n        kidslt6 + kidsge6 + age + huswage + \n        mtr + unem + city + exper + exper2, \n    data = .)\n\nThis formula is nothing more that a good old linear regression. The last line data = . means that the data to be used inside lm() should be coming from the list called data, which is the second column of imp_wages. As I’m writing these lines, I realize it is confusing as hell. But I promise you that learning to use purrr is a bit like learning how to use a bicycle. Very difficult to explain, but once you know how to do it, it feels super natural. Take some time to play with the lines above to really understand what happened.\n\n\nNow, let’s take a look at the result:\n\nimp_wages_reg\n## # A tibble: 10 x 3\n##     .imp data                lin_reg\n##    &lt;int&gt; &lt;list&gt;              &lt;list&gt; \n##  1     1 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  2     2 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  3     3 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  4     4 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  5     5 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  6     6 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  7     7 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  8     8 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  9     9 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n## 10    10 &lt;tibble [753 × 17]&gt; &lt;lm&gt;\n\nimp_wages_reg now has a third column called lin_reg where each element is a linear model, estimated on the data from the data column! We can now pool the results of these 10 regressions using mice::pool():\n\npool_lin_reg &lt;- pool(imp_wages_reg$lin_reg)\n\nsummary(pool_lin_reg)\n##                  estimate    std.error  statistic       df      p.value\n## (Intercept)  1.2868701172 3.214473e-01  4.0033628 737.9337 6.876133e-05\n## educ         0.0385310276 8.231906e-03  4.6806931 737.9337 3.401935e-06\n## inlf         1.8845418354 5.078235e-02 37.1101707 737.9337 0.000000e+00\n## hours       -0.0001164143 3.011378e-05 -3.8658143 737.9337 1.204773e-04\n## kidslt6     -0.0438925013 3.793152e-02 -1.1571510 737.9337 2.475851e-01\n## kidsge6     -0.0117978229 1.405226e-02 -0.8395678 737.9337 4.014227e-01\n## age         -0.0030084595 2.666614e-03 -1.1281946 737.9337 2.596044e-01\n## huswage     -0.0231736955 5.607364e-03 -4.1327255 737.9337 3.995866e-05\n## mtr         -2.2109176781 3.188827e-01 -6.9333267 737.9337 8.982592e-12\n## unem         0.0028775444 5.462973e-03  0.5267360 737.9337 5.985352e-01\n## city         0.0157414671 3.633755e-02  0.4332011 737.9337 6.649953e-01\n## exper        0.0164364027 6.118875e-03  2.6861806 737.9337 7.389936e-03\n## exper2      -0.0002022602 1.916146e-04 -1.0555575 737.9337 2.915159e-01\n\nThis function averages the results from the 10 regressions and computes correct standard errors. This is based on Rubin’s rules (Rubin, 1987, p. 76). As you can see, the linear regression indicates that one year of added education has a positive, significant effect of log wages (they’re not log wages, I used the IHS transformation, but log wages just sounds better than inverted hyperbolic sined wages). This effect is almost 4%.\n\n\nBut education is not randomly assigned, and as such might be endogenous. This is where instrumental variables come into play. An instrument is a variables that impacts the dependent variable only through the endogenous variable (here, education). For example, the education of the parents do not have a direct impact over one’s wage, but having college-educated parents means that you are likely college-educated yourself, and thus have a higher wage that if you only have a high school diploma.\n\n\nI am thus going to instrument education with both parents’ education:\n\nimp_wages_reg = imp_wages_reg %&gt;% \n    mutate(iv_reg = map(data, \n                         ~ivreg(ihs_wage ~ educ + inlf + hours + \n                                 kidslt6 + kidsge6 + age + huswage + \n                                 mtr + unem + city + exper + exper2 |.-educ + fatheduc + motheduc, \n                             data = .)))\n\nThe only difference from before is the formula:\n\n~ivreg(ihs_wage ~ educ + inlf + hours + \n           kidslt6 + kidsge6 + age + huswage + \n           mtr + unem + city + exper + exper2 |.-educ + fatheduc + motheduc, \n       data = .)\n## ~ivreg(ihs_wage ~ educ + inlf + hours + kidslt6 + kidsge6 + age + \n##     huswage + mtr + unem + city + exper + exper2 | . - educ + \n##     fatheduc + motheduc, data = .)\n\nInstead of lm() I use AER::ivreg() and the formula has a second part, after the | symbol. This is where I specify that I instrument education with the parents’ education.\n\n\nimp_wages_reg now looks like this:\n\nimp_wages_reg\n## # A tibble: 10 x 4\n##     .imp data                lin_reg iv_reg \n##    &lt;int&gt; &lt;list&gt;              &lt;list&gt;  &lt;list&gt; \n##  1     1 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  2     2 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  3     3 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  4     4 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  5     5 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  6     6 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  7     7 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  8     8 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  9     9 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n## 10    10 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n\nLet’s take a look at the results:\n\npool_iv_reg &lt;- pool(imp_wages_reg$iv_reg)\n\nsummary(pool_iv_reg)\n##                  estimate    std.error  statistic       df      p.value\n## (Intercept)  2.0091904157 5.146812e-01  3.9037568 737.9337 1.033832e-04\n## educ         0.0038859137 2.086592e-02  0.1862326 737.9337 8.523136e-01\n## inlf         1.9200207113 5.499457e-02 34.9129122 737.9337 0.000000e+00\n## hours       -0.0001313866 3.157375e-05 -4.1612608 737.9337 3.537881e-05\n## kidslt6     -0.0234593391 4.000689e-02 -0.5863824 737.9337 5.577979e-01\n## kidsge6     -0.0123239220 1.422241e-02 -0.8665145 737.9337 3.864897e-01\n## age         -0.0040874625 2.763340e-03 -1.4791748 737.9337 1.395203e-01\n## huswage     -0.0242737100 5.706497e-03 -4.2536970 737.9337 2.373189e-05\n## mtr         -2.6385172445 3.998419e-01 -6.5989008 737.9337 7.907430e-11\n## unem         0.0047331976 5.622137e-03  0.8418859 737.9337 4.001246e-01\n## city         0.0255647706 3.716783e-02  0.6878197 737.9337 4.917824e-01\n## exper        0.0180917073 6.258779e-03  2.8906127 737.9337 3.957817e-03\n## exper2      -0.0002291007 1.944599e-04 -1.1781381 737.9337 2.391213e-01\n\nAs you can see, education is not statistically significant anymore! This is why it is quite important to think about endogeneity issues. However, it is not always very easy to find suitable instruments. A series of tests exist to determine if you have relevant and strong instruments, but this blog post is already long enough. I will leave this for a future blog post."
  },
  {
    "objectID": "posts/2018-01-03-lists_all_the_way.html",
    "href": "posts/2018-01-03-lists_all_the_way.html",
    "title": "It’s lists all the way down",
    "section": "",
    "text": "There’s a part 2 to this post: read it here.\n\n\nToday, I had the opportunity to help someone over at the R for Data Science Slack group (read more about this group here) and I thought that the question asked could make for an interesting blog post, so here it is!\n\n\nDisclaimer: the way I’m doing things here is totally not optimal, but I want to illustrate how to map functions over nested lists. But I show the optimal way at the end, so for the people that are familiar with purrr don’t get mad at me.\n\n\nSuppose you have to do certain data transformation tasks on a data frame, and you write a nice function that does that for you:\n\nlibrary(tidyverse)\ndata(mtcars)\n\nnice_function = function(df, param1, param2){\n  df = df %&gt;%\n    filter(cyl == param1, am == param2) %&gt;%\n    mutate(result = mpg * param1 * (2 - param2))\n\n  return(df)\n}\n\nnice_function(mtcars, 4, 0)\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\n\nThis might seem like a silly function and not a nice function, but it will illustrate the point I want to make (and the question that was asked) very well. This function is completely useless, but bear with me. Now, suppose that you want to do these operations for each value of cyl and am (of course you can do that without using nice_function()…). First, you might want to fix the value of am to 0, and then loop over the values of cyl. But as I have explained in this other blog post I prefer using the map() functions included in purrr. For example:\n\nvalues_cyl = c(4, 6, 8)\n\n(result = map(values_cyl, nice_function, df = mtcars, param2 = 0))\n## [[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\n## \n## [[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## \n## [[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n\nWhat you get here is a list for each value in values_cyl; so one list for 4, one for 6 and one for 8. Suppose now that you are feeling adventurous, and want to loop over the values of am too:\n\nvalues_am = c(0, 1)\n\nSo first, we need to map a function to each element of values_am. But which function? Well, for given value of am, our problem is the same as before; we need to map nice_function() to each value of cyl. So, that’s what we’re going to do:\n\n(result = map(values_am, ~map(values_cyl, nice_function, df = mtcars, param2 = .)))\n## [[1]]\n## [[1]][[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\n## \n## [[1]][[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## \n## [[1]][[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## \n## \n## [[2]]\n## [[2]][[1]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## \n## [[2]][[2]]\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2\n## \n## [[2]][[3]]\n##    mpg cyl disp  hp drat   wt qsec vs am gear carb result\n## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4\n## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0\n\nWe now have a list of size 2 (for each value of am) where each element is itself a list of size 3 (for each value of cyl) where each element is a data frame. Are you still with me? Also, notice that the second map is given as a formula (notice the ~ in front of the second map). This creates an anonymous function, where the parameter is given by the . (think of the . as being the x in f(x)). So the . is the stand-in for the values contained inside values_am.\n\n\nThe people that are familiar with the map() functions must be fuming right now; there is a way to avoid this nested hell. I will talk about it soon, but first I want to play around with this list of lists.\n\n\nIf you have a list of data frames, you can bind their rows together with reduce(list_of_dfs, rbind). You would like to this here, but because your lists of data frames are contained inside another list… you guessed it, you have to map over it!\n\n(result2 = map(result, ~reduce(., rbind)))\n## [[1]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## \n## [[2]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 2  32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 3  30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 4  33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 5  27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 6  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 7  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 8  21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 9  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 10 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 11 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 12 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 13 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nHere again, I pass reduce() as a formula to map() to create an anonymous function. Again, the . is used as the stand-in for each element contained in result; a list of data frames, where reduce(., rbind) knows what to do. Now that we have this we can use reduce() with rbind() again to get a single data frame:\n\n(result3 = reduce(result2, rbind))\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## 20 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 21 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 22 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 23 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 24 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 25 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 26 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 27 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 28 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 29 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 31 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 32 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nOf course, since reduce(list_of_dfs, rbind) is such a common operation, you could have simply used dplyr::bind_rows, which does exactly this:\n\n(result2 = map(result, bind_rows))\n## [[1]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## \n## [[2]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 2  32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 3  30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 4  33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 5  27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 6  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 7  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 8  21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 9  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 10 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 11 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 12 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 13 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nand then:\n\n(result3 = bind_rows(result2))\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## 20 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 21 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 22 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 23 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 24 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 25 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 26 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 27 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 28 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 29 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 31 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 32 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nOf course, things are even simpler: you can avoid this deeply nested monstrosity by using map_df() instead of map()! map_df() works just like map() but return a data frame (hence the _df in the name) instead of a list:\n\n(result_df = map_df(values_am, ~map_df(values_cyl, nice_function, df = mtcars, param2 = .)))\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## 20 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 21 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 22 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 23 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 24 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 25 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 26 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 27 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 28 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 29 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 31 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 32 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nIf you look at the source code of map_df() you see that dplyr::bind_rows gets called at the end:\n\nmap_df\n## function (.x, .f, ..., .id = NULL) \n## {\n##     if (!is_installed(\"dplyr\")) {\n##         abort(\"`map_df()` requires dplyr\")\n##     }\n##     .f &lt;- as_mapper(.f, ...)\n##     res &lt;- map(.x, .f, ...)\n##     dplyr::bind_rows(res, .id = .id)\n## }\n## &lt;bytecode: 0x55dad486e6a0&gt;\n## &lt;environment: namespace:purrr&gt;\n\nSo moral of the story? There are a lot of variants of the common purrr::map() functions (as well as of dplyr verbs, such as filter_at, select_if, etc…) and learning about them can save you from a lot of pain! However, if you need to apply a function to nested lists this is still possible; you just have to think about the structure of the nested list for a bit. There is also another function that you might want to study, modify_depth() which solves related issues but I will end the blog post here. I might talk about it in a future blog post.\n\n\nAlso, if you want to learn more about R and the tidyverse, do read the link I posted in the introduction of the post and join the R4ds slack group! There are a lot of very nice people there that want to help you get better with your R-fu. Also, this is where I got the inspiration to write this blog post and I am thankful to the people there for the discussions; I feel comfortable with R, but I still learn new tips and tricks every day!\n\n\nIf you enjoy these blog posts, you can follow me on twitter. And happy new yeaR!"
  },
  {
    "objectID": "posts/2013-01-16-simulated-maximum-likelihood-with-r.html",
    "href": "posts/2013-01-16-simulated-maximum-likelihood-with-r.html",
    "title": "Simulated Maximum Likelihood with R",
    "section": "",
    "text": "This document details section 12.4.5. Unobserved Heterogeneity Example from Cameron and Trivedi’s book - MICROECONOMETRICS: Methods and Applications. The original source code giving the results from table 12.2 are available from the authors' site here and written for Stata. This is an attempt to translate the code to R. I’d like to thank Reddit user anonemouse2010 for his advice which helped me write the function.\n\n\nConsult the original source code if you want to read the authors' comments. If you want the R source code without all the commentaries, grab it here. This is not guaranteed to work, nor to be correct. It could set your pet on fire and/or eat your first born. Use at your own risk. I may, or may not, expand this example. Corrections, constructive criticism are welcome.\n\n\nThe model is \\(y=\\theta+u+\\varepsilon\\) where \\(\\theta\\) is a scalar parameter equal to 1. \\(u\\) is extreme value type 1 (Gumbel distribution), \\(\\varepsilon \\leadsto \\mathbb{N}(0,1)\\). For more details, consult the book.\n\n\nImport the data\n\n\nYou can consult the original source code to see how the authors simulated the data. To get the same results, and verify that I didn't make mistakes I prefer importing their data directly from their website.\n\n\ndata &lt;- read.table(\"http://cameron.econ.ucdavis.edu/mmabook/mma12p2mslmsm.asc\")\nu &lt;- data[, 1]\ne &lt;- data[, 2]\ny &lt;- data[, 3]\nnumobs &lt;- length(u)\nsimreps &lt;- 10000\n\n\nSimulation\n\n\nIn the code below, the following likelihood function:\n\\[\\log{\\hat{L}_N(\\theta)} = \\dfrac{1}{N} \\sum_{i=1}^N\\log{\\big( \\dfrac{1}{S}\\sum_{s=1}^S \\dfrac{1}{\\sqrt{2\\pi}} \\exp \\{ -(-y_i-\\theta-u_i^s)^2/2 \\}\\big)}\\]\nwhich can be found on page 397 is programmed using the function sapply.\n\n\ndenssim &lt;- function(theta) {\n    loglik &lt;- mean(sapply(y, function(y) log(mean((1/sqrt(2 * pi)) * exp(-(y - theta + log(-log(runif(simreps))))^2/2)))))\n    return(-loglik)\n}\n\n\nThis likelihood is then maximized:\n\n\nsystem.time(res &lt;- optim(0.1, denssim, method = \"BFGS\", control = list(maxit = simreps)))\n\n   user  system elapsed \n  1.780   0.231   2.010 \n\n\n\nConvergence is achieved pretty rapidly, to\n\n\nres\n\n$par\n[1] 1.180305\n\n$value\n[1] 1.920577\n\n$counts\nfunction gradient \n      76        8 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\nwhich is close to the true value of the parameter 1 (which was used to generate the data).\n\n\nLet's try again with another parameter value, for example ( ). We have to generate y again:\n\n\ny2 &lt;- 2.5 + u + e\n\n\nand slightly modify the likelihood:\n\n\ndenssim2 &lt;- function(theta) {\n  loglik &lt;- mean(\n    sapply(\n      y2,\n      function(y2) log(mean((1/sqrt(2 * pi)) * exp(-(y2 - theta + log(-log(runif(simreps))))^2/2)))))\n  return(-loglik)\n}\n\n\nwhich can then be maximized:\n\n\nsystem.time(res2 &lt;- optim(0.1, denssim2, method = \"BFGS\", control = list(maxit = simreps)))\n\n   user  system elapsed \n  3.164   0.337   3.500 \n\n\n\nThe value that maximizes the likelihood is:\n\n\nres2\n\n$par\n[1] 2.654767\n\n$value\n[1] 1.920779\n\n$counts\nfunction gradient \n     129       18 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\nwhich is close to the true value of the parameter 2.5 (which was used to generate the data)."
  },
  {
    "objectID": "posts/2017-10-26-margins_r.html",
    "href": "posts/2017-10-26-margins_r.html",
    "title": "Easy peasy STATA-like marginal effects with R",
    "section": "",
    "text": "Model interpretation is essential in the social sciences. If one wants to know the effect of variable x on the dependent variable y, marginal effects are an easy way to get the answer. STATA includes a margins command that has been ported to R by Thomas J. Leeper of the London School of Economics and Political Science. You can find the source code of the package on github. In this short blog post, I demo some of the functionality of margins.\n\n\nFirst, let’s load some packages:\n\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(broom)\nlibrary(margins)\nlibrary(Ecdat)\n\nAs an example, we are going to use the Participation data from the Ecdat package:\n\ndata(Participation)\n?Participation\nLabor Force Participation\n\nDescription\n\na cross-section\n\nnumber of observations : 872\n\nobservation : individuals\n\ncountry : Switzerland\n\nUsage\n\ndata(Participation)\nFormat\n\nA dataframe containing :\n\nlfp\nlabour force participation ?\n\nlnnlinc\nthe log of nonlabour income\n\nage\nage in years divided by 10\n\neduc\nyears of formal education\n\nnyc\nthe number of young children (younger than 7)\n\nnoc\nnumber of older children\n\nforeign\nforeigner ?\n\nSource\n\nGerfin, Michael (1996) “Parametric and semiparametric estimation of the binary response”, Journal of Applied Econometrics, 11(3), 321-340.\n\nReferences\n\nDavidson, R. and James G. MacKinnon (2004) Econometric Theory and Methods, New York, Oxford University Press, http://www.econ.queensu.ca/ETM/, chapter 11.\n\nJournal of Applied Econometrics data archive : http://qed.econ.queensu.ca/jae/.\n\nThe variable of interest is lfp: whether the individual participates in the labour force or not. To know which variables are relevant in the decision to participate in the labour force, one could estimate a logit model, using glm().\n\nlogit_participation = glm(lfp ~ ., data = Participation, family = \"binomial\")\n\nNow that we ran the regression, we can take a look at the results. I like to use broom::tidy() to look at the results of regressions, as tidy() returns a nice data.frame, but you could use summary() if you’re only interested in reading the output:\n\ntidy(logit_participation)\n##          term    estimate  std.error  statistic      p.value\n## 1 (Intercept) 10.37434616 2.16685216  4.7877499 1.686617e-06\n## 2     lnnlinc -0.81504064 0.20550116 -3.9661122 7.305449e-05\n## 3         age -0.51032975 0.09051783 -5.6378920 1.721444e-08\n## 4        educ  0.03172803 0.02903580  1.0927211 2.745163e-01\n## 5         nyc -1.33072362 0.18017027 -7.3859224 1.514000e-13\n## 6         noc -0.02198573 0.07376636 -0.2980454 7.656685e-01\n## 7  foreignyes  1.31040497 0.19975784  6.5599678 5.381941e-11\n\nFrom the results above, one can only interpret the sign of the coefficients. To know how much a variable influences the labour force participation, one has to use margins():\n\neffects_logit_participation = margins(logit_participation) \n\nprint(effects_logit_participation)\n## Average marginal effects\n## glm(formula = lfp ~ ., family = \"binomial\", data = Participation)\n##  lnnlinc     age     educ     nyc       noc foreignyes\n##  -0.1699 -0.1064 0.006616 -0.2775 -0.004584     0.2834\n\nUsing summary() on the object returned by margins() provides more details:\n\nsummary(effects_logit_participation)\n##      factor     AME     SE       z      p   lower   upper\n##         age -0.1064 0.0176 -6.0494 0.0000 -0.1409 -0.0719\n##        educ  0.0066 0.0060  1.0955 0.2733 -0.0052  0.0185\n##  foreignyes  0.2834 0.0399  7.1102 0.0000  0.2053  0.3615\n##     lnnlinc -0.1699 0.0415 -4.0994 0.0000 -0.2512 -0.0887\n##         noc -0.0046 0.0154 -0.2981 0.7656 -0.0347  0.0256\n##         nyc -0.2775 0.0333 -8.3433 0.0000 -0.3426 -0.2123\n\nAnd it is also possible to plot the effects with base graphics:\n\nplot(effects_logit_participation)\n\n\n\n\nThis uses the basic R plotting capabilities, which is useful because it is a simple call to the function plot() but if you’ve been using ggplot2 and want this graph to have the same look as the others made with ggplot2 you first need to save the summary in a variable. Let’s overwrite this effects_logit_participation variable with its summary:\n\neffects_logit_participation = summary(effects_logit_participation)\n\nAnd now it is possible to use ggplot2 to create the same plot:\n\nggplot(data = effects_logit_participation) +\n  geom_point(aes(factor, AME)) +\n  geom_errorbar(aes(x = factor, ymin = lower, ymax = upper)) +\n  geom_hline(yintercept = 0) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45))\n\n\n\n\nSo an infinitesimal increase, in say, non-labour income (lnnlinc) of 0.001 is associated with a decrease of the probability of labour force participation by 0.001*17 percentage points.\n\n\nYou can also extract the marginal effects of a single variable, with dydx():\n\nhead(dydx(Participation, logit_participation, \"lnnlinc\"))\n##   dydx_lnnlinc\n## 1  -0.15667764\n## 2  -0.20014487\n## 3  -0.18495109\n## 4  -0.05377262\n## 5  -0.18710476\n## 6  -0.19586986\n\nWhich makes it possible to extract the effects for a list of individuals that you can create yourself:\n\nmy_subjects = tribble(\n    ~lfp,  ~lnnlinc, ~age, ~educ, ~nyc, ~noc, ~foreign,\n    \"yes\",   10.780,  7.0,     4,    1,    1,    \"yes\",\n     \"no\",     1.30,  9.0,     1,    4,    1,    \"yes\"\n)\n\ndydx(my_subjects, logit_participation, \"lnnlinc\")\n##   dydx_lnnlinc\n## 1  -0.09228119\n## 2  -0.17953451\n\nI used the tribble() function from the tibble package to create this test data set, row by row. Then, using dydx(), I get the marginal effect of variable lnnlinc for these two individuals. No doubt that this package will be a huge help convincing more social scientists to try out R and make a potential transition from STATA easier."
  },
  {
    "objectID": "posts/2019-03-05-historical_vowpal_part2.html",
    "href": "posts/2019-03-05-historical_vowpal_part2.html",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2",
    "section": "",
    "text": "In part 1 of this series I set up Vowpal Wabbit to classify newspapers content. Now, let’s use the model to make predictions and see how and if we can improve the model. Then, let’s train the model on the whole data."
  },
  {
    "objectID": "posts/2019-03-05-historical_vowpal_part2.html#step-1-prepare-the-data",
    "href": "posts/2019-03-05-historical_vowpal_part2.html#step-1-prepare-the-data",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2",
    "section": "\nStep 1: prepare the data\n",
    "text": "Step 1: prepare the data\n\n\nThe first step consists in importing the test data and preparing it. The test data need not be large and thus can be imported and worked on in R.\n\n\nI need to remove the target column from the test set, or else it will be used to make predictions. If you do not remove this column the accuracy of the model will be very high, but it will be wrong since, of course, you do not have the target column at running time… because it is the column that you want to predict!\n\nlibrary(\"tidyverse\")\nlibrary(\"yardstick\")\n\nsmall_test &lt;- read_delim(\"data_split/small_test.txt\", \"|\",\n                      escape_double = FALSE, col_names = FALSE,\n                      trim_ws = TRUE)\n\nsmall_test %&gt;%\n    mutate(X1= \" \") %&gt;%\n    write_delim(\"data_split/small_test2.txt\", col_names = FALSE, delim = \"|\")\n\nI wrote the data in a file called small_test2.txt and can now use my model to make predictions:\n\nsystem2(\"/home/cbrunos/miniconda3/bin/vw\", args = \"-t -i vw_models/small_oaa.model data_split/small_test2.txt -p data_split/small_oaa.predict\")\n\nThe predictions get saved in the file small_oaa.predict, which is a plain text file. Let’s add these predictions to the original test set:\n\nsmall_predictions &lt;- read_delim(\"data_split/small_oaa.predict\", \"|\",\n                          escape_double = FALSE, col_names = FALSE,\n                          trim_ws = TRUE)\n\nsmall_test &lt;- small_test %&gt;%\n    rename(truth = X1) %&gt;%\n    mutate(truth = factor(truth, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")))\n\nsmall_predictions &lt;- small_predictions %&gt;%\n    rename(predictions = X1) %&gt;%\n    mutate(predictions = factor(predictions, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")))\n\nsmall_test &lt;- small_test %&gt;%\n    bind_cols(small_predictions)"
  },
  {
    "objectID": "posts/2019-03-05-historical_vowpal_part2.html#step-2-use-the-model-and-test-data-to-evaluate-performance",
    "href": "posts/2019-03-05-historical_vowpal_part2.html#step-2-use-the-model-and-test-data-to-evaluate-performance",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2",
    "section": "\nStep 2: use the model and test data to evaluate performance\n",
    "text": "Step 2: use the model and test data to evaluate performance\n\n\nWe can use the several metrics included in {yardstick} to evaluate the model’s performance:\n\nconf_mat(small_test, truth = truth, estimate = predictions)\n\naccuracy(small_test, truth = truth, estimate = predictions)\n          Truth\nPrediction  1  2  3  4  5\n         1 51 15  2 10  1\n         2 11  6  3  1  0\n         3  0  0  0  0  0\n         4  0  0  0  0  0\n         5  0  0  0  0  0\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.570\n\nWe can see that the model never predicted class 3, 4 or 5. Can we improve by adding some regularization? Let’s find out!"
  },
  {
    "objectID": "posts/2019-03-05-historical_vowpal_part2.html#step-3-adding-regularization",
    "href": "posts/2019-03-05-historical_vowpal_part2.html#step-3-adding-regularization",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2",
    "section": "\nStep 3: adding regularization\n",
    "text": "Step 3: adding regularization\n\n\nBefore trying regularization, let’s try changing the cost function from the logistic function to the hinge function:\n\n# Train the model\nhinge_oaa_fit &lt;- system2(\"/home/cbrunos/miniconda3/bin/vw\", args = \"--oaa 5 -d data_split/small_train.txt --loss_function hinge -f vw_models/hinge_oaa.model\", stderr = TRUE)\n\nsystem2(\"/home/cbrunos/miniconda3/bin/vw\", args = \"-i vw_models/hinge_oaa.model -t -d data_split/small_test2.txt -p data_split/hinge_oaa.predict\")\n\n\npredictions &lt;- read_delim(\"data_split/hinge_oaa.predict\", \"|\",\n                          escape_double = FALSE, col_names = FALSE,\n                          trim_ws = TRUE)\n\ntest &lt;- test %&gt;%\n    select(-predictions)\n\npredictions &lt;- predictions %&gt;%\n    rename(predictions = X1) %&gt;%\n    mutate(predictions = factor(predictions, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")))\n\ntest &lt;- test %&gt;%\n    bind_cols(predictions)\nconf_mat(test, truth = truth, estimate = predictions)\n\naccuracy(test, truth = truth, estimate = predictions)\n          Truth\nPrediction   1   2   3   4   5\n         1 411 120  45  92   1\n         2 355 189  12  17   0\n         3  11   2   0   0   0\n         4  36   4   0   1   0\n         5   3   0   3   0   0\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.462\n\nWell, didn’t work out so well, but at least we now know how to change the loss function. Let’s go back to the logistic loss and add some regularization. First, let’s train the model:\n\nregul_oaa_fit &lt;- system2(\"/home/cbrunos/miniconda3/bin/vw\", args = \"--oaa 5 --l1 0.005 --l2 0.005 -d data_split/small_train.txt -f vw_models/small_regul_oaa.model\", stderr = TRUE)\n\nNow we can use it for prediction:\n\nsystem2(\"/home/cbrunos/miniconda3/bin/vw\", args = \"-i vw_models/small_regul_oaa.model -t -d data_split/test2.txt -p data_split/small_regul_oaa.predict\")\n\n\npredictions &lt;- read_delim(\"data_split/small_regul_oaa.predict\", \"|\",\n                          escape_double = FALSE, col_names = FALSE,\n                          trim_ws = TRUE)\n\ntest &lt;- test %&gt;%\n    select(-predictions)\n\npredictions &lt;- predictions %&gt;%\n    rename(predictions = X1) %&gt;%\n    mutate(predictions = factor(predictions, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")))\n\ntest &lt;- test %&gt;%\n    bind_cols(predictions)\n\nWe can now use it for predictions:\n\nconf_mat(test, truth = truth, estimate = predictions)\n\naccuracy(test, truth = truth, estimate = predictions)\n          Truth\nPrediction   1   2   3   4   5\n         1 816 315  60 110   1\n         2   0   0   0   0   0\n         3   0   0   0   0   0\n         4   0   0   0   0   0\n         5   0   0   0   0   0\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.627\n\nSo accuracy improved, but the model only predicts class 1 now… let’s try with other hyper-parameters values:\n\nregul_oaa_fit &lt;- system2(\"/home/cbrunos/miniconda3/bin/vw\", args = \"--oaa 5 --l1 0.00015 --l2 0.00015 -d data_split/small_train.txt -f vw_models/small_regul_oaa.model\", stderr = TRUE)\nconf_mat(test, truth = truth, estimate = predictions)\n\naccuracy(test, truth = truth, estimate = predictions)\n          Truth\nPrediction   1   2   3   4   5\n         1 784 300  57 108   1\n         2  32  14   3   2   0\n         3   0   1   0   0   0\n         4   0   0   0   0   0\n         5   0   0   0   0   0\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.613\n\nSo accuracy is lower than previously, but at least more categories get correctly predicted. Depending on your needs, you should consider different metrics. Especially for classification problems, you might not be interested in accuracy, in particular if the data is severely unbalanced.\n\n\nAnyhow, to finish this blog post, let’s train the model on the whole data and measure the time it takes to run the full model."
  },
  {
    "objectID": "posts/2019-03-05-historical_vowpal_part2.html#step-4-training-on-the-whole-data",
    "href": "posts/2019-03-05-historical_vowpal_part2.html#step-4-training-on-the-whole-data",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2",
    "section": "\nStep 4: Training on the whole data\n",
    "text": "Step 4: Training on the whole data\n\n\nLet’s first split the whole data into a training and a testing set:\n\nnb_lines &lt;- system2(\"cat\", args = \"text_fr.txt | wc -l\", stdout = TRUE)\n\nsystem2(\"split\", args = paste0(\"-l\", floor(as.numeric(nb_lines)*0.995), \" text_fr.txt data_split/\"))\n\nsystem2(\"mv\", args = \"data_split/aa data_split/train.txt\")\nsystem2(\"mv\", args = \"data_split/ab data_split/test.txt\")\n\nThe whole data contains 260247 lines, and the training set weighs 667MB, which is quite large. Let’s train the simple multiple classifier on the data and see how long it takes:\n\ntic &lt;- Sys.time()\noaa_fit &lt;- system2(\"/home/cbrunos/miniconda3/bin/vw\", args = \"--oaa 5 -d data_split/train.txt -f vw_models/oaa.model\", stderr = TRUE)\nSys.time() - tic\nTime difference of 4.73266 secs\n\nYep, you read that right. Training the classifier on 667MB of data took less than 5 seconds!\n\n\nLet’s take a look at the final object:\n\noaa_fit\n [1] \"final_regressor = vw_models/oaa.model\"                                   \n [2] \"Num weight bits = 18\"                                                    \n [3] \"learning rate = 0.5\"                                                     \n [4] \"initial_t = 0\"                                                           \n [5] \"power_t = 0.5\"                                                           \n [6] \"using no cache\"                                                          \n [7] \"Reading datafile = data_split/train.txt\"                                 \n [8] \"num sources = 1\"                                                         \n [9] \"average  since         example        example  current  current  current\"\n[10] \"loss     last          counter         weight    label  predict features\"\n[11] \"1.000000 1.000000            1            1.0        2        1      253\"\n[12] \"0.500000 0.000000            2            2.0        2        2      499\"\n[13] \"0.250000 0.000000            4            4.0        2        2        6\"\n[14] \"0.250000 0.250000            8            8.0        1        1     2268\"\n[15] \"0.312500 0.375000           16           16.0        1        1      237\"\n[16] \"0.250000 0.187500           32           32.0        1        1      557\"\n[17] \"0.171875 0.093750           64           64.0        1        1      689\"\n[18] \"0.179688 0.187500          128          128.0        2        2      208\"\n[19] \"0.144531 0.109375          256          256.0        1        1      856\"\n[20] \"0.136719 0.128906          512          512.0        4        4        4\"\n[21] \"0.122070 0.107422         1024         1024.0        1        1     1353\"\n[22] \"0.106934 0.091797         2048         2048.0        1        1      571\"\n[23] \"0.098633 0.090332         4096         4096.0        1        1       43\"\n[24] \"0.080566 0.062500         8192         8192.0        1        1      885\"\n[25] \"0.069336 0.058105        16384        16384.0        1        1      810\"\n[26] \"0.062683 0.056030        32768        32768.0        2        2      467\"\n[27] \"0.058167 0.053650        65536        65536.0        1        1       47\"\n[28] \"0.056061 0.053955       131072       131072.0        1        1      495\"\n[29] \"\"                                                                        \n[30] \"finished run\"                                                            \n[31] \"number of examples = 258945\"                                             \n[32] \"weighted example sum = 258945.000000\"                                    \n[33] \"weighted label sum = 0.000000\"                                           \n[34] \"average loss = 0.054467\"                                                 \n[35] \"total feature number = 116335486\"  \n\nLet’s use the test set and see how the model fares:\n\nconf_mat(test, truth = truth, estimate = predictions)\n\naccuracy(test, truth = truth, estimate = predictions)\n          Truth\nPrediction   1   2   3   4   5\n         1 537 175  52 100   1\n         2 271 140   8   9   0\n         3   1   0   0   0   0\n         4   7   0   0   1   0\n         5   0   0   0   0   0\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.521\n\nBetter accuracy can certainly be achieved with hyper-parameter tuning… maybe the subject for a future blog post? In any case I am very impressed with Vowpal Wabbit and am certainly looking forward to future developments of {RVowpalWabbit}!"
  },
  {
    "objectID": "posts/2024-09-27-nix_part_13.html",
    "href": "posts/2024-09-27-nix_part_13.html",
    "title": "Reproducible data science with Nix, part 13 – {rix} is on CRAN!",
    "section": "",
    "text": "Simplifies the creation of reproducible data science environments using the ‘Nix’ package manager, as described in Dolstra (2006) &lt;ISBN 90-393-4130-3&gt;. The included ‘rix()’ function generates a complete description of the environment as a ‘default.nix’ file, which can then be built using ‘Nix’. This results in project specific software environments with pinned versions of R, packages, linked system dependencies, and other tools. Additional helpers make it easy to run R code in ‘Nix’ software environments for testing and production.\n\n\nAfter 15 months of coding, 1364 commits, 143 closed issues, 175 closed PRs, an rOpenSci pre-review, an rOpenSci review, {rix} is finally on CRAN!\n\n\nYou can now install {rix} using good old install.packages(). Soon, {rix} will also be included into the nixpkgs collection of packages, meaning that you will be able to install {rix} with Nix.\n\n\nImportant sidenote: as it so happened, there is currently a bug in the released CRAN version that we thought we had solved, which we did, but only partially. When running rix::rix() two files should be generated: a default.nix and an .Rprofile for your project. It turns out that this file can be empty. If it is, run rix::rix_init(rprofile_action = “overwrite”) to generate a proper .Rprofile. This is important, especially on Mac or if you have a system-wide library of packages! We will submit a fix asap.\n\n\nIf you want to watch a 5-Minute video introduction:\n\n\n\n\n\n\nBtw, here is what scc has to say about the estimated cost of the project:\n\n\nscc –format=html-table –avg-wage 100000 .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage\n\n\nFiles\n\n\nLines\n\n\nBlank\n\n\nComment\n\n\nCode\n\n\nComplexity\n\n\nBytes\n\n\n\n\n\n\nYAML\n\n\n61\n\n\n2798\n\n\n320\n\n\n174\n\n\n2304\n\n\n0\n\n\n69187\n\n\n\n\nR\n\n\n33\n\n\n4515\n\n\n483\n\n\n1225\n\n\n2807\n\n\n389\n\n\n153288\n\n\n\n\nNix\n\n\n10\n\n\n781\n\n\n95\n\n\n0\n\n\n686\n\n\n32\n\n\n18644\n\n\n\n\nMarkdown\n\n\n5\n\n\n1371\n\n\n339\n\n\n0\n\n\n1032\n\n\n0\n\n\n63758\n\n\n\n\nJSON\n\n\n1\n\n\n147\n\n\n0\n\n\n0\n\n\n147\n\n\n0\n\n\n4637\n\n\n\n\nPlain Text\n\n\n1\n\n\n41\n\n\n0\n\n\n0\n\n\n41\n\n\n0\n\n\n2269\n\n\n\n\nTotal\n\n\n111\n\n\n9653\n\n\n1237\n\n\n1399\n\n\n7017\n\n\n421\n\n\n311783\n\n\n\n\n\n\nEstimated Cost to Develop (organic) $371,264 - Estimated Schedule Effort (organic) 7.59 months - Estimated People Required (organic) 2.45\n\n\nDon’t hesitate to give {rix} a try and let us know how it goes!"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "Build reproducible analytical pipelines that will output consistent, high-quality analyses using the R programming language, Git, Github and Docker. Learn about functional and literate programming to keep your code concise, easier to test, easier to share and easily understandable by others.\n\n\n\n\nRead it for free: https://raps-with-r.dev/\n\n\nBuy the DRM free Epub or PDF: https://leanpub.com/raps-with-r/\n\n\nBuy a physical copy on Amazon: https://www.amazon.com/Building-reproducible-analytical-pipelines-R/dp/B0C87H6MGF\n\n\n\n\n\n\nThis book focuses on teaching you how to use R in a modern way. Why modern? Because you will learn about the tidyverse collection of packages, which enable you to write efficient, legible code. You will also learn about functional programming, some statistical methods, writing your own packages… By the end of the book, you will have all the knowledge and tools on hand to use R for your day-to-day data science tasks!\n\n\n\n\nRead it for free: https://modern-rstats.eu/\n\n\n\nBuy the DRM free Epub or PDF (very out of date, update coming Q4-2025): https://leanpub.com/modern_tidyverse/\n\n\nBuy a physical copy (coming one day)"
  },
  {
    "objectID": "books.html#building-reproducible-analytical-pipelines-with-r",
    "href": "books.html#building-reproducible-analytical-pipelines-with-r",
    "title": "Books",
    "section": "",
    "text": "Build reproducible analytical pipelines that will output consistent, high-quality analyses using the R programming language, Git, Github and Docker. Learn about functional and literate programming to keep your code concise, easier to test, easier to share and easily understandable by others.\n\n\n\n\nRead it for free: https://raps-with-r.dev/\n\n\nBuy the DRM free Epub or PDF: https://leanpub.com/raps-with-r/\n\n\nBuy a physical copy on Amazon: https://www.amazon.com/Building-reproducible-analytical-pipelines-R/dp/B0C87H6MGF\n\n\n\n\n\n\nThis book focuses on teaching you how to use R in a modern way. Why modern? Because you will learn about the tidyverse collection of packages, which enable you to write efficient, legible code. You will also learn about functional programming, some statistical methods, writing your own packages… By the end of the book, you will have all the knowledge and tools on hand to use R for your day-to-day data science tasks!\n\n\n\n\nRead it for free: https://modern-rstats.eu/\n\n\n\nBuy the DRM free Epub or PDF (very out of date, update coming Q4-2025): https://leanpub.com/modern_tidyverse/\n\n\nBuy a physical copy (coming one day)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Bruno Rodrigues and hold a PhD in Economics from the University of Strasbourg.\n\n\n\nI’m currently employed as a statistician for the Ministry of Research and Higher Education in Luxembourg. Before that I was senior data scientist and then manager in the data science team at PwC Luxembourg, and before that I was a research assistant at STATEC Research.\nMy hobbies are boxing, lifting weights, cycling, cooking and reading or listening to audiobooks, which is more compatible with the life of a young father. I started this blog to share my enthusiasm for statistics. My blog posts are reshared on R-bloggers and RWeekly. I also enjoy learning about the R programming language and sharing my knowledge. That’s why I made this blog and write ebooks. I also have a youtube channel, where I show some tips and tricks with R, or rant about stuff."
  },
  {
    "objectID": "posts/2023-03-03-quarto_books.html",
    "href": "posts/2023-03-03-quarto_books.html",
    "title": "What I’ve learned making an .epub Ebook with Quarto",
    "section": "",
    "text": "I’ve been working on an ebook (that you can read over here) made using Quarto. Since I’m also selling a DRM-free Epub and PDF on Leanpub I wanted to share some tips and tricks I’ve learned to generate an Epub that passes epubcheck using Quarto.\nQuarto is a tool made by Posit and is an open-source scientific and technical publishing tool. If you know what LaTeX is, then it should be easy for you to grok Quarto. The idea of Quarto is that you write documents using Markdown, and then compile these source files into either PDFs, Word documents, but also books, web-sites, ebooks (in the Epub format) and so on… It’s quite powerful, and you can also use programming language code chunks for literate programming. Quarto support R, Python, Julia and ObsevableJS chunks.\nSo, as I said, I’ve been using Quarto to write an ebook, and from a single set of Markdown source files I can generate the website (linked above), the PDF of the book and the Epub of the book. But you see, if you want to sell an Epub on platforms like Leanpub, the generated Epub must pass epubcheck. epubcheck is a command line application that verifies that your Epub satisfies certain quality checks. If these quality standards are not satisfied, there is no guarantee that Epub readers can successfully open your Epub. Leanpub actually allows you to upload an Epub that does not pass epubcheck, but they warn you that you really should strive for publishing an Epub without any errors or warnings raised by epubcheck. For example, the first version of my Epub did not pass epubcheck and I couldn’t upload it to my Kindle.\nIn this blog post I’ll show you what you should do to generate an Epub that passes epubcheck using Quarto."
  },
  {
    "objectID": "posts/2023-03-03-quarto_books.html#starting-from-the-default-template",
    "href": "posts/2023-03-03-quarto_books.html#starting-from-the-default-template",
    "title": "What I’ve learned making an .epub Ebook with Quarto",
    "section": "\nStarting from the default template\n",
    "text": "Starting from the default template\n\n\nStart by installing Quarto by downloading the right package for your operating system here. To start from a book template open a terminal and run:\n\nquarto create-project example_book --type book\n\nLet’s open the _quarto.yml file that is inside the newly created example_book/. This is your book’s configuration file. It should look like this:\n\nproject:\n  type: book\n\nbook:\n  title: \"example_book\"\n  author: \"Jane Doe\"\n  date: \"3/3/2023\"\n  chapters:\n    - index.qmd\n    - intro.qmd\n    - summary.qmd\n    - references.qmd\n\nbibliography: references.bib\n\nformat:\n  html:\n    theme: cosmo\n  pdf:\n    documentclass: scrreprt\n\nYou can change whatever you like, but for our purposes, we are going to add the epub output format all the way at the bottom of the file. So change these lines:\n\nformat:\n  html:\n    theme: cosmo\n  pdf:\n    documentclass: scrreprt\n\ninto these lines:\n\nformat:\n  html:\n    theme: cosmo\n  epub:\n    toc: true\n\nI’ve added the epub format as an output, as well as the toc: true option, which builds a table of contents. I’ve also removed the pdf output because you need to have a LaTeX distribution installed for this, and the point of this blog post is not to talk about the PDF output (which works flawlessly by the way). Before compiling, let’s open one of the .qmd files. These files are the Markdown source files that we need to edit in order to fill our book with content. Let’s open intro.qmd and change these lines from:\n\n# Introduction\n\nThis is a book created from markdown and executable code.\n\nSee @knuth84 for additional discussion of literate programming.\n\nto:\n\n# Introduction\n\nThis is a book created from markdown and executable code.\n\nSee @knuth84 for additional discussion of literate programming.\n\n![By Boaworm - Own work, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=10649477](images/640px-Eyjafjallajokull_Gigjokull_in_ash.jpg)\n\n\nDownload the image from this link and create the images/ folder at the root of the book, right next to the .qmd files.\n\n\nThis syntax is the default syntax for adding pictures in a Markdown document. If you’re an R user, you could also use an R code chunk and the function knitr::include_graphics() to achieve the same thing.\n\n\nLet’s compile our little example book, and then use epubcheck to see what’s wrong! Use these commands to render the book in all the formats:\n\ncd example_book/\n\nquarto render\n\nYou should see a folder called _book appear on the root of your project. Inside this folder, you will see a bunch of .html files: these constitute the web-site of your book. You can right click on index.html and open it with a web browser and see how your book, as a web-site, looks like. You could host this book on Github pages for free!\n\n\nBut what interests us is the .epub file. If your PDF reader supports this format, you can open it and see how it looks. On Windows you could use SumatraPDF. I use Okular on Linux to open PDF and Epub documents. Anyways, there doesn’t seem to be anything wrong with it. You can open it, you can read it, it seems to be working just fine. But let’s see if epubcheck thinks the same. You can download epubcheck from here. Save the downloaded file on the root directory of the book and decompress it. Inside the decompressed folder, you’ll see a file called epubcheck.jar. Put your epub file right next to it, in the same folder. Now, open a terminal and navigate to the right folder and run the following command to check the epub file:\n\ncd epubcheck-5.0.0 # or whatever version it is you downloaded\n\njava -jar epubcheck.jar example_book.epub\n\nYou should see this output:\n\nValidating using EPUB version 3.3 rules.\nERROR(RSC-005): example_book.epub/EPUB/content.opf(6,39): Error while parsing file: character content of element \"dc:date\" invalid; must \nbe a string with length at least 1 (actual length was 0)\nWARNING(OPF-053): example_book.epub/EPUB/content.opf(6,39): Date value \"\" does not follow recommended syntax as per http://www.w3.org/TR/NOTE-datetime:zero-length string.\nERROR(RSC-005): example_book.epub/EPUB/text/ch002.xhtml(354,16): Error while parsing file: element \"figcaption\" not allowed here; expected the element end-tag, text, element \"a\", \"abbr\", \"area\", \"audio\", \"b\", \"bdi\", \"bdo\", \"br\", \"button\", \"canvas\", \"cite\", \"code\", \"data\", \"datalist\", \"del\", \"dfn\", \"em\", \"embed\", \"epub:switch\", \"i\", \"iframe\", \"img\", \"input\", \"ins\", \"kbd\", \"label\", \"link\", \"map\", \"mark\", \"meta\", \"meter\", \"ns1:math\", \"ns2:svg\", \"object\", \"output\", \"picture\", \"progress\", \"q\", \"ruby\", \"s\", \"samp\", \"script\", \"select\", \"small\", \"span\", \"strong\", \"sub\", \"sup\", \"template\", \"textarea\", \"time\", \"u\", \"var\", \"video\" or \"wbr\" (with xmlns:ns1=\"http://www.w3.org/1998/Math/MathML\" xmlns:ns2=\"http://www.w3.org/2000/svg\") or an element from another namespace\n\nCheck finished with errors\nMessages: 0 fatals / 2 errors / 1 warning / 0 infos\n\nEPUBCheck completed\n\nSo we get 2 errors and 1 warning! Let’s look at the first error:\n\nERROR(RSC-005): example_book.epub/EPUB/content.opf(6,39): Error while parsing file: character content of element \"dc:date\" invalid; must \nbe a string with length at least 1 (actual length was 0)\n\nThe first error message states that our epub does not have a valid dc:date attribute. The warning is also related to this. We can correct this by adding this attribute in the _quarto.yml file:\n\nformat:\n  epub:\n    toc:\n      true\n    date:\n      \"2023-03-01\"\n\nHowever this is not enough. There is a bug in the current release of Quarto that prevents this from working, even though we did what we should. However, this bug is already corrected in the development version of the next release. But until the next version of Quarto, 1.3, gets released, here is the workaround; you need to also specify the language of the book:\n\nformat:\n  html:\n    theme: cosmo\n  epub:\n    toc:\n      true\n    lang:\n      en-GB\n    date:\n      \"2023-03-01\"\n\nAnd now epubcheck does not complain about the date anymore!\n\n\nThe next error:\n\nERROR(RSC-005): example_book.epub/EPUB/text/ch002.xhtml(354,16): Error while parsing file: element \"figcaption\" not allowed here; expected the element end-tag, text, element \"a\", \"abbr\", \"area\", \"audio\", \"b\", \"bdi\", \"bdo\", \"br\", \"button\", \"canvas\", \"cite\", \"code\", \"data\", \"datalist\", \"del\", \"dfn\", \"em\", \"embed\", \"epub:switch\", \"i\", \"iframe\", \"img\", \"input\", \"ins\", \"kbd\", \"label\", \"link\", \"map\", \"mark\", \"meta\", \"meter\", \"ns1:math\", \"ns2:svg\", \"object\", \"output\", \"picture\", \"progress\", \"q\", \"ruby\", \"s\", \"samp\", \"script\", \"select\", \"small\", \"span\", \"strong\", \"sub\", \"sup\", \"template\", \"textarea\", \"time\", \"u\", \"var\", \"video\" or \"wbr\" (with xmlns:ns1=\"http://www.w3.org/1998/Math/MathML\" xmlns:ns2=\"http://www.w3.org/2000/svg\") or an element from another namespace\n\nis related to the image. It turns out that including the image like we did generates code that is not quite correct from the point of view of the standard that Epubs should follow. You should know that Epubs are actually a collection of HTML files, so you can include images by using HTML code in the source Markdown files.\n\n\nIf you insert the image like so, the error should disappear:\n\n&lt;figure&gt;\n    &lt;img src=\"images/640px-Eyjafjallajokull_Gigjokull_in_ash.jpg\"\n         alt=\"By Boaworm - Own work, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=10649477\"&gt;&lt;/img&gt;\n    &lt;figcaption&gt;By Boaworm - Own work, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=10649477&lt;/figcaption&gt;\n&lt;/figure&gt;\n\nIf you re-render the Epub, and try epubcheck again, you should see this:\n\njava -jar epubcheck.jar example_book.epub\nValidating using EPUB version 3.3 rules.\nNo errors or warnings detected.\nMessages: 0 fatals / 0 errors / 0 warnings / 0 infos"
  },
  {
    "objectID": "posts/2023-03-03-quarto_books.html#using-github-actions-to-build-the-book",
    "href": "posts/2023-03-03-quarto_books.html#using-github-actions-to-build-the-book",
    "title": "What I’ve learned making an .epub Ebook with Quarto",
    "section": "\nUsing Github Actions to build the book\n",
    "text": "Using Github Actions to build the book\n\n\nFinally, as a bonus, if you’re using Github, you can also use Github Actions to generate the web-site, as well as the Epub (and the PDF if you want). If you go to this repository, which contains the example book from this post, you can find the workflow to automatically build the Epub and web-site from your Quarto source in the .github/workflows/ folder. Create the same folder structure in your repository and copy the .yml file that is in it to these folders. You should then create a gh-pages branch and make sure that Github Actions has the required permissions to push. For this, go to the Settings menu of your repository, then Actions (listed in the menu on the left), then General, and then under Workflow permissions make sure that Read and write permissions is checked.\n\n\n\n\n\n\n\nNow, each time you push, you should see your Epub get built in the gh-pages branch! If you use R code chunks, you also need to set up an action to set up R. Take a look at the repo of my book for an example."
  },
  {
    "objectID": "posts/2015-01-12-introduction-to-programming-econometrics-with-r.html",
    "href": "posts/2015-01-12-introduction-to-programming-econometrics-with-r.html",
    "title": "Introduction to programming econometrics with R",
    "section": "",
    "text": "This semester, I’ll be teaching an introduction to applied econometrics with R, so I’ve decided to write a very small book called “Introduction to programming Econometrics with R”. This is primarily intended for bachelor students and the focus is not much on econometric theory, but more on how to implement econometric theory into computer code, using the R programming language. It’s very basic and doesn’t cover any advanced topics in econometrics and is intended for people with 0 previous programming knowledge. It is still very rough around the edges, and it’s missing the last chapter about reproducible research, and the references, but I think it’s time to put it out there; someone else than my students may find it useful. The book’s probably full of typos and mistakes, so don’t hesitate to drop me an e-mail if you find something fishy: contact@brodrigues.co\nAlso there might be some sections at the beginning that only concern my students. Just ignore that.\nGet it here: download\n\nUpdate (2017-01-22)\nYou might find the book useful as it is now, but I never had a chance to finish it. I might get back to it once I’ll have more time, and port it to bookdown."
  },
  {
    "objectID": "posts/2015-05-03-update-introduction-r-programming.html",
    "href": "posts/2015-05-03-update-introduction-r-programming.html",
    "title": "Update to Introduction to programming econometrics with R",
    "section": "",
    "text": "This semester I taught a course on applied econometrics with the R programming language. For this, I created a document that I gave to my students and shared online. This is the kind of document I would have liked to read when I first started using R. I already had some programming experience in C and Pascal but this is not necessarily the case for everyone that is confronted to R when they start learning about econometrics.\nThis is why the beginning of the document focuses more on general programming knowledge and techniques, and then only on econometrics. People online seemed to like the document, as I’ve received some positive comments by David Smith from Revolution R (read his blog post about the document here) and Dave Giles which links to David’s blog post here. People on twitter have also retweeted David’s and Dave’s tweets to their blog posts and I’ve received some requests by people to send them the PDF by email (because they live in places where Dropbox is not accessible unfortunately).\nThe document is still a work in progress (and will probably remain so for a long time), but I’ve added some new sections about reproducible research and thought that this update could warrant a new blog post.\nFor now, only linear models are reviewed, but I think I’ll start adding some chapters about non-linear models soonish. The goal for these notes, however, is not to re-invent the wheel: there are lots of good books about econometrics with R out there and packages that estimate a very wide range of models. What I want for these notes, is to focus more on the programming knowledge an econometrician needs, in a very broad and general sense. I want my students to understand that R is a true programming language, and that they need to use every feature offered by such a language, and not think of R as a black box into which you only type pre-programmed commands, but also be able to program their own routines.\nAlso, I’ve made it possible to create the PDF using a Makefile. This may be useful for people that do not have access to Dropbox, but are familiar with git.\nYou can compile the book in two ways: first download the whole repository:\ngit clone git@bitbucket.org:b-rodrigues/programmingeconometrics.git\nand then, with Rstudio, open the file appliedEconometrics.Rnw and knit it. Another solution is to use the Makefile. Just type:\nmake\ninside a terminal (should work on GNU+Linux and OSX systems) and it should compile the document. You may get some message about “additional information” for some R packages. When these come up, just press Q on your keyboard to continue the compilation process.\nGet the notes here.\nAs always, if you have questions or suggestions, do not hesitate to send me an email and I sure hope you’ll find these notes useful!"
  },
  {
    "objectID": "posts/2022-04-11-monads.html",
    "href": "posts/2022-04-11-monads.html",
    "title": "Why you should(n’t) care about Monads if you’re an R programmer",
    "section": "",
    "text": "Update: I also made a video out of this blog post; watch it on youtube."
  },
  {
    "objectID": "posts/2022-04-11-monads.html#introduction-functions",
    "href": "posts/2022-04-11-monads.html#introduction-functions",
    "title": "Why you should(n’t) care about Monads if you’re an R programmer",
    "section": "\nIntroduction: functions\n",
    "text": "Introduction: functions\n\n\nTo understand Monads, I think it’s useful to first think about functions; why do we use functions? Why don’t we simply write scripts with the required operations one after the other? For instance, to compute the average height by species in a data set of individuals from the famous space opera “Star Wars”, we could very well write this code:\n\nsuppressPackageStartupMessages(library(dplyr))\n\ndata(starwars)\n\nsum_humans &lt;- 0\nsum_others &lt;- 0\nn_humans &lt;- 0\nn_others &lt;- 0\n\nfor(i in seq_along(1:nrow(starwars))){\n\n  if(!is.na(unlist(starwars[i, \"species\"])) &\n     unlist(starwars[i, \"species\"]) == \"Human\"){\n    if(!is.na(unlist(starwars[i, \"height\"]))){\n      sum_humans &lt;- sum_humans + unlist(starwars[i, \"height\"])\n      n_humans &lt;- n_humans + 1\n    } else {\n\n      0\n\n    }\n\n  } else {\n    if(!is.na(unlist(starwars[i, \"height\"]))){\n      sum_others &lt;- sum_others + unlist(starwars[i, \"height\"])\n      n_others &lt;- n_others + 1\n    } else {\n      0\n    }\n  }\n}\n\nmean_height_humans &lt;- sum_humans/n_humans\nmean_height_others &lt;- sum_others/n_others\n\nWell, we could do it like this, but we definitely shouldn’t:\n\n\n\nwhat this code does is not immediately obvious. If the code blocks aren’t commented, readers of this code will have to read line by line to understand what is going on;\n\n\nthis code is not reusable. If now I need the average height by species and sex, I need to copy and paste the code, and modify it, and in some cases modify it substantially;\n\n\nthis code handles missing values in a cumbersome way, with nested if…else…s;\n\n\nthis code is not easy to test;\n\n\nthis code cannot be composed (meaning, chained) with other code without substantially altering it (to be precise, chaining and composing are two different things, strictly speaking, but for simplicity’s sake, let’s just assume it is the same. Whenever I’m talking about “composing” something, I mean “chaining” something.)\n\n\n\nBut it’s not just shortcomings, this imperative code has one advantage; it uses only some very fundamental building blocks: if…else…, for loops and that’s almost it (it does use some functions provided by a base installation of R, namely is.na(), !(), unlist() and [(), so strictly speaking, the code above is not purely imperative, but maybe closer to being procedural?).\n\n\nUsing functions solves all the issues from imperative programming. Here is a base solution to the problem above, using a declarative, or functional, approach:\n\naggregate(starwars$height,\n          by = list(starwars$species == \"Human\"),\n          FUN = \\(x)(mean(x, na.rm = TRUE)))\n##   Group.1        x\n## 1   FALSE 172.4043\n## 2    TRUE 176.6452\n\nThis code has many advantages:\n\n\n\nwhat this code does is obvious, but only if you know what aggregate() does. But if you read its documentation you’ll know, and you’ll know every time you’ll see aggregate() unlike a loop like the loop above where you’ll have to read it each time to understand;\n\n\nthis code is reusable. Replace the data frame by another, and that’s it;\n\n\nMissing values are now ignored easily using the na.rm argument of mean();\n\n\nthis code is easy to test (using unit tests);\n\n\nthis code can be composed, for instance like this:\n\n\naggregate(starwars$height,\n          by = list(starwars$species == \"Human\"),\n          FUN = \\(x)(mean(x, na.rm = TRUE))) |&gt;\n  setNames(c(\"is_human\", \"mean_height\"))\n##   is_human mean_height\n## 1    FALSE    172.4043\n## 2     TRUE    176.6452\n\nThe issue with the functional approach (at least that’s the issue that many people I spoke to about this raise) is that… in some way people that don’t like this approach feel like they “lose” control over what’s going on. You don’t know what happens inside these functions. I remember, while working my first job, that my boss required that I don’t use any functions nor packages, but instead write all the loops explicitely, because she wanted to understand what was going on (of course, I completely ignored this request and just did as I pleased). As discussed above, the imperative approach requires minimum knowledge of the language, and almost anyone with an ounce of programming experience can understand imperative code. That’s not the case with a functional approach. Readers will have to be familiar with the individual functions like aggregate(), but also anonymous functions (I had to use (x)(mean(x, na.rm = TRUE)) to set na.rm = TRUE, which is FALSE by default) and also |&gt; for composition/chaining.\n\n\nIt may same more complex, and maybe it is, but the advantages far outweigh the shortcoming.\n\n\nFor completeness, here is a {dplyr} version:\n\nstarwars %&gt;%\n  group_by(is_human = species == \"Human\") %&gt;%\n  summarise(mean_height = mean(height, na.rm = TRUE))\n## # A tibble: 3 × 2\n##   is_human mean_height\n##   &lt;lgl&gt;          &lt;dbl&gt;\n## 1 FALSE           172.\n## 2 TRUE            177.\n## 3 NA              181.\n\n{dplyr} code is even more concise than base functional code. Here again, users will have to know about the individual functions and %&gt;%. But personally, I think that the only hurdle is understanding what %&gt;% does, and once you know this, {dplyr} code can be understood quite easily, thanks to very explicit function names.\n\n\nSo functions are great. They’re easy to test, easy to document, easy to package, easy to reuse, and easy to compose. Composition is really important. For example, let’s go back to the imperative code, and put the result in a neat data frame object, like the functional solutions do:\n\nsum_humans &lt;- 0\nsum_others &lt;- 0\nn_humans &lt;- 0\nn_others &lt;- 0\n\nfor(i in seq_along(1:nrow(starwars))){\n\n  if(!is.na(unlist(starwars[i, \"species\"])) &\n     unlist(starwars[i, \"species\"]) == \"Human\"){\n    if(!is.na(unlist(starwars[i, \"height\"]))){\n      sum_humans &lt;- sum_humans + unlist(starwars[i, \"height\"])\n      n_humans &lt;- n_humans + 1\n    } else {\n\n      0\n\n    }\n\n  } else {\n    if(!is.na(unlist(starwars[i, \"height\"]))){\n      sum_others &lt;- sum_others + unlist(starwars[i, \"height\"])\n      n_others &lt;- n_others + 1\n    } else {\n      0\n    }\n  }\n}\n\nmean_height_humans &lt;- sum_humans/n_humans\nmean_height_others &lt;- sum_others/n_others\n\n# These two lines are new\ndata.frame(list(\"is_human\" = c(TRUE, FALSE),\n           \"mean_height\" = c(mean_height_others, mean_height_humans)))\n##   is_human mean_height\n## 1     TRUE    172.9400\n## 2    FALSE    176.6452\n\nIt’s just two lines (right at the end), but the implications are huge; because imperative code cannot be composed, I had to write separate code to put the result into a data frame. More code that I need to write, more opportunities for mistakes. I actually did a mistake, did you notice? This kind of mistake could go unnoticed for eons. But if you use functions, you don’t have this problem, and can focus on getting (even complex) things done:\n\nstarwars %&gt;%\n  filter(skin_color == \"light\") %&gt;%\n  select(species, sex, mass) %&gt;%\n  group_by(sex, species) %&gt;%\n  summarise(\n    total_individuals = n(),\n    min_mass = min(mass, na.rm = TRUE),\n    mean_mass = mean(mass, na.rm = TRUE),\n    sd_mass = sd(mass, na.rm = TRUE),\n    max_mass = max(mass, na.rm = TRUE)\n  ) %&gt;%\n  select(-species) %&gt;%\n  tidyr::pivot_longer(-sex, names_to = \"statistic\", values_to = \"value\")\n## `summarise()` has grouped output by 'sex'. You can override using the `.groups`\n## argument.\n## # A tibble: 10 × 3\n## # Groups:   sex [2]\n##    sex    statistic         value\n##    &lt;chr&gt;  &lt;chr&gt;             &lt;dbl&gt;\n##  1 female total_individuals   6  \n##  2 female min_mass           45  \n##  3 female mean_mass          56.3\n##  4 female sd_mass            16.3\n##  5 female max_mass           75  \n##  6 male   total_individuals   5  \n##  7 male   min_mass           79  \n##  8 male   mean_mass          90.5\n##  9 male   sd_mass            19.8\n## 10 male   max_mass          120\n\nNeedless to say, trying to write the above code using only for loops and if…else… is not something I’d wish to do, especially passing the result of all the {dplyr} calls to pivot_longer(). Creating that last data frame by hand is error prone, and there would definitely be mistakes in there.\n\n\nI hope I don’t need to convince you any more that functions are great, and that one of the great things they offer is their ability to be chained, or composed. But strictly speaking, you don’t need them. You could write your code without any function whatsoever, and use the most basic building blocks there are (loops and if…else… and little more). However, doing this would result in much messier code. It’s the same with monads. You can live without them. But there will be situations where not using them will result in messier code.\n\n\nOne more thing: as I was writing this blog post, I happened on this tweet:\n\n\n{{% tweet “1513080736785604611” %}}\n\n\nThis is a fine example of all that I’ve been discussing until now. The person who wrote this code was very likely trying to get the diagonal elements of a matrix. That person was likely a beginner in R and used for loops to try to get the answer. We have all been there; what I’m trying to articulate is this: imperative programming can be useful, but it can get messy very quickly…"
  },
  {
    "objectID": "posts/2022-04-11-monads.html#when-functions-are-not-enough",
    "href": "posts/2022-04-11-monads.html#when-functions-are-not-enough",
    "title": "Why you should(n’t) care about Monads if you’re an R programmer",
    "section": "\nWhen functions are not enough\n",
    "text": "When functions are not enough\n\n\nFunctions are awesome, but there are situations which functions simply can’t easily deal with. Situations in which you would like your functions to do a little extra more, and the only way forward you see is to rewrite them to do something totally unrelated. For example, suppose you would like to time your code. Most people would to something such as:\n\ntic &lt;- Sys.time()\nstarwars %&gt;%\n  filter(skin_color == \"light\") %&gt;%\n  select(species, sex, mass) %&gt;%\n  group_by(sex, species) %&gt;%\n  summarise(\n    total_individuals = n(),\n    min_mass = min(mass, na.rm = TRUE),\n    mean_mass = mean(mass, na.rm = TRUE),\n    sd_mass = sd(mass, na.rm = TRUE),\n    max_mass = max(mass, na.rm = TRUE)\n  ) %&gt;%\n  select(-species) %&gt;%\n  tidyr::pivot_longer(-sex, names_to = \"statistic\", values_to = \"value\")\n## `summarise()` has grouped output by 'sex'. You can override using the `.groups`\n## argument.\n## # A tibble: 10 × 3\n## # Groups:   sex [2]\n##    sex    statistic         value\n##    &lt;chr&gt;  &lt;chr&gt;             &lt;dbl&gt;\n##  1 female total_individuals   6  \n##  2 female min_mass           45  \n##  3 female mean_mass          56.3\n##  4 female sd_mass            16.3\n##  5 female max_mass           75  \n##  6 male   total_individuals   5  \n##  7 male   min_mass           79  \n##  8 male   mean_mass          90.5\n##  9 male   sd_mass            19.8\n## 10 male   max_mass          120\ntoc &lt;- Sys.time()\n\n(running_time &lt;- toc - tic)\n## Time difference of 0.04228544 secs\n\nYou could totally do that. But now you’re back to square one. You have to deal with this tic-toc nonsense separately, have to keep track it, overburdening you mentally and polluting your code. To keep track of it, you’ll want to add the running times in a separate data frame, in which you could have all the running times of all your operations you need to run:\n\ndata.frame(list(\"operations\" = seq(1:3),\n                \"running_time\" = c(running_time, running_time * 2, running_time * 3)))\n##   operations    running_time\n## 1          1 0.04228544 secs\n## 2          2 0.08457088 secs\n## 3          3 0.12685633 secs\n\nThis data frame is the consequence of this tic-toc nonsense not being composable and now you have to deal with it, but you don’t want to. So what now? You might be tempted to do something like this:\n\ntic_filter &lt;- function(...){\n\n  tic &lt;- Sys.time()\n\n  result &lt;- filter(...)\n\n  toc &lt;- Sys.time()\n\n  message(\"Running time: \", toc - tic)\n\n  return(result)\n\n}\n\nstarwars %&gt;%\n  tic_filter(species == \"Human\")\n## Running time: 0.00481176376342773\n## # A tibble: 35 × 14\n##    name     height  mass hair_color skin_color eye_color birth_year sex   gender\n##    &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n##  1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n##  2 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n##  3 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n##  4 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n##  5 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n##  6 Biggs D…    183    84 black      light      brown           24   male  mascu…\n##  7 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n##  8 Anakin …    188    84 blond      fair       blue            41.9 male  mascu…\n##  9 Wilhuff…    180    NA auburn, g… fair       blue            64   male  mascu…\n## 10 Han Solo    180    80 brown      fair       brown           29   male  mascu…\n## # … with 25 more rows, and 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;,\n## #   films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt;\n\nBut that’s actually worse: not only do you have to change all the functions you need, and wrap them around tic-toc, but the running time is only shown as a message, so you can’t reuse it. You could then try to rewrite the function like this:\n\ntic_filter &lt;- function(...){\n\n  tic &lt;- Sys.time()\n\n  result &lt;- filter(...)\n\n  toc &lt;- Sys.time()\n\n  running_time &lt;- toc - tic\n\n  list(\"result\" = result,\n       \"running_time\" = running_time)\n\n}\n\nstarwars %&gt;%\n  tic_filter(species == \"Human\")\n## $result\n## # A tibble: 35 × 14\n##    name     height  mass hair_color skin_color eye_color birth_year sex   gender\n##    &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n##  1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n##  2 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n##  3 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n##  4 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n##  5 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n##  6 Biggs D…    183    84 black      light      brown           24   male  mascu…\n##  7 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n##  8 Anakin …    188    84 blond      fair       blue            41.9 male  mascu…\n##  9 Wilhuff…    180    NA auburn, g… fair       blue            64   male  mascu…\n## 10 Han Solo    180    80 brown      fair       brown           29   male  mascu…\n## # … with 25 more rows, and 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;,\n## #   films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt;\n## \n## $running_time\n## Time difference of 0.004878759 secs\n\nAt least now you save the running time along with the object. But the problem of rewriting many functions remains, and these rewritten {dplyr} functions now return a list, and not a data frame anymore so something like this:\n\nstarwars %&gt;%\n  tic_filter(species == \"Human\") %&gt;%\n  tic_select(species, sex)\n\nwouldn’t work, because tic_select() expects a data frame, not a list where the first element is a data frame and the second a double.\n\n\nSo what else can be done? Perhaps you’d be tempted to use a global variable for this:\n\ntic_filter &lt;- function(..., running_time = 0){\n\n  tic &lt;- Sys.time()\n\n  result &lt;- filter(...)\n\n  toc &lt;- Sys.time()\n\n  running_time &lt;&lt;- toc - tic + running_time\n\n  result\n\n}\n\nFunctions written like this would save the running time in a global variable called running_time and each of them would take turns overwriting it:\n\nrunning_time &lt;- 0\n\none &lt;- starwars %&gt;%\n  tic_filter(species == \"Human\", running_time = running_time)\n\nrunning_time\n## Time difference of 0.00490284 secs\ntwo &lt;- one %&gt;%\n  tic_select(species, sex, running_time = running_time)\n\nrunning_time\n## Time difference of 0.007258415 secs\n\n(I defined tic_select() but am not showing it here.)\n\n\nThis has the advantage that the wrapped functions now return data frames as well, and can thus be composed/chained. But these functions are not pure functions, because they change something (the global variable running_time) outside their scope. Impure functions can be tricky; for instance here, because the code keeps overwriting the same variable, if you run the whole script and then separate chunks to try some things, running_time will keep getting incremented. Once again, you have to be extra careful and keep track of it, once again overburdening you mentally."
  },
  {
    "objectID": "posts/2022-04-11-monads.html#the-solution",
    "href": "posts/2022-04-11-monads.html#the-solution",
    "title": "Why you should(n’t) care about Monads if you’re an R programmer",
    "section": "\nThe solution\n",
    "text": "The solution\n\n\nThe solution to this problem looks like one of the previous things we tried, namely:\n\ntic_filter &lt;- function(...){\n\n  tic &lt;- Sys.time()\n\n  result &lt;- filter(...)\n\n  toc &lt;- Sys.time()\n\n  running_time &lt;- toc - tic\n\n  list(\"result\" = result,\n       \"running_time\" = running_time)\n\n}\n\nWhile it is true that it returns a list, this function has the yuge advantage of being pure. But still, we need to solve two problems:\n\n\n\nhow to avoid having to rewrite every function;\n\n\nhow to compose these functions so that the output of one function can be ingested as the input of the next.\n\n\n\nSolving the first problem consists in writing a new function that builds functions, what Hadley Wickham calls function factories. Let’s try:\n\ntimeit &lt;- function(.f, ..., running_time = 0){\n\n  function(..., .running_time = running_time){\n\n    tic &lt;- Sys.time()\n\n    result &lt;- .f(...)\n\n    toc &lt;- Sys.time()\n\n    list(result = result,\n         running_time = toc - tic + .running_time)\n  }\n\n\n}\n\ntimeit() is a function that takes a function (and its arguments as an input), and returns a new function. This function returns the result of the original function (.f) evaluated on its arguments (…) as well as the time it took to run as a list. You’ll notice as well that this function takes another argument, called running_time with a default value of 0. This will become useful below, for now, ignore it.\n\nt_sqrt &lt;- timeit(sqrt)\n\nt_sqrt(10)\n## $result\n## [1] 3.162278\n## \n## $running_time\n## Time difference of 8.34465e-06 secs\n\nThat’s great, but we can’t compose these functions. This fails:\n\nt_log &lt;- timeit(log)\n\n10 |&gt;\n  t_sqrt() |&gt;\n  t_log()\nError in .f(...) : non-numeric argument to mathematical function\n\nbecause t_log() expects a number, not a list. The solution? Write another functions to help! Let’s call this function bind:\n\nbind &lt;- function(.l, .f, ...){\n\n  .f(.l$result, ..., .running_time = .l$running_time)\n\n}\n\nbind() takes a list object returned by a timed function (.l, with elements $result and $running_time) and applies another timed function .f() to the $result element of .l as well as any further arguments … and finally sets the running_time argument of .f equal to .l$running_time. .l$running_time is the running time of the previous timed function call, so now this running time gets added to the running time of .f (see the definition of the list of timeit()).\n\n\nAn example might help:\n\nt_log &lt;- timeit(log)\n\n10 |&gt;\n  t_sqrt() |&gt;\n  bind(t_log)\n## $result\n## [1] 1.151293\n## \n## $running_time\n## Time difference of 8.368492e-05 secs\n\nWhat’s nice with this solution, is that it works with any function:\n\nt_filter &lt;- timeit(filter)\nt_select &lt;- timeit(select)\nt_group_by &lt;- timeit(group_by)\nt_summarise &lt;- timeit(summarise)\nt_p_longer &lt;- timeit(tidyr::pivot_longer)\n\nstarwars %&gt;%\n  t_filter(skin_color == \"light\") %&gt;% # no need to use bind here\n  bind(t_select, species, sex, mass) %&gt;%\n  bind(t_group_by, sex, species) %&gt;%\n  bind(t_summarise,\n    total_individuals = n(),\n    min_mass = min(mass, na.rm = TRUE),\n    mean_mass = mean(mass, na.rm = TRUE),\n    sd_mass = sd(mass, na.rm = TRUE),\n    max_mass = max(mass, na.rm = TRUE)\n  ) %&gt;%\n  bind(t_select, -species) %&gt;%\n  bind(t_p_longer, -sex, names_to = \"statistic\", values_to = \"value\")\n## `summarise()` has grouped output by 'sex'. You can override using the `.groups`\n## argument.\n## $result\n## # A tibble: 10 × 3\n## # Groups:   sex [2]\n##    sex    statistic         value\n##    &lt;chr&gt;  &lt;chr&gt;             &lt;dbl&gt;\n##  1 female total_individuals   6  \n##  2 female min_mass           45  \n##  3 female mean_mass          56.3\n##  4 female sd_mass            16.3\n##  5 female max_mass           75  \n##  6 male   total_individuals   5  \n##  7 male   min_mass           79  \n##  8 male   mean_mass          90.5\n##  9 male   sd_mass            19.8\n## 10 male   max_mass          120  \n## \n## $running_time\n## Time difference of 0.09293914 secs\n\nThere is some overhead compared to the solution that simply calls tic at the beginning of all the {dplyr} calls and then toc at the end, but this overhead becomes negligible the longer the base operations run for. And now the advantage is that you don’t have to think about keeping track of running times. Re-running separate chunks will also not interfere with the running time of any other chunk."
  },
  {
    "objectID": "posts/2022-04-11-monads.html#monads",
    "href": "posts/2022-04-11-monads.html#monads",
    "title": "Why you should(n’t) care about Monads if you’re an R programmer",
    "section": "\nMonads\n",
    "text": "Monads\n\n\nSo here we are, ready to learn what monads are, or rather, we’re done, because you already know what monads are. The solution described before is a monad:\n\n\n\na function factory to create functions that return a special, wrapped value (here it simply was a list of elements $result and $running_time). This wrapped value is also called a monadic value.\n\n\na function to compose, or chain, these special functions together.\n\n\n\nSome other pieces can be added to the list, and one would need to check so-called monadic laws to make extra sure we’re dealing with a monad, but that’s outside the scope of this blog post.\n\n\nThere are many monads, for instance the so-called Maybe monad, available on R thanks to Andrew McNeil who implemented this monad as an R package. I have also developed a monad for logging (which also logs execution time), which I called {chronicler}, read more about it here.\n\n\nTo conclude, why did I title this post why you should(n’t) care about Monads if you’re an R programmer? The reason is that you can live without monads. However, certain things will be more complex if you don’t know about monads or if you don’t want to use them, just like functions. If for some reason you don’t use functions in your code, your life will be more complicated. So should you go ahead and start using monads in your code? Well, maybe (hehe) you should, especially if you’re doing the same thing over and over again, like timing your code. Maybe using a monad to time your code could be a nice solution, especially if you’ve been burned in the past by using the other, sub-optimal solutions?"
  },
  {
    "objectID": "posts/2022-04-11-monads.html#extra-reading",
    "href": "posts/2022-04-11-monads.html#extra-reading",
    "title": "Why you should(n’t) care about Monads if you’re an R programmer",
    "section": "\nExtra reading\n",
    "text": "Extra reading\n\n\nIf this blog post was not enough to satiate your curiosity, here are some more nice resources:\n\n\n\nLaszlo Kupcsik great blog post on the maybe monad,\n\n\nAndrew McNeil implementation of the Maybe monad as a package\n\n\nthis nice video by Studying With Alex\n\n\nand of course, the GOAT, Bartosz Milewski’s Category Theory For Programmers on YouTube if you really want to go into the nitty-gritty theoretical details of functional programming.\n\n\nThere’s also this very accessible and nice blog post, Functors, applicatives and monads in pictures which I highly recommend."
  },
  {
    "objectID": "posts/2023-07-30-nix_for_r_part3.html",
    "href": "posts/2023-07-30-nix_for_r_part3.html",
    "title": "Reproducible data science with Nix, part 3 – frictionless {plumber} api deployments with Nix",
    "section": "",
    "text": "This is the third post in a series of posts about Nix. Disclaimer: I’m a super beginner with Nix. So this series of blog posts is more akin to notes that I’m taking while learning than a super detailed tutorial. So if you’re a Nix expert and read something stupid in here, that’s normal. This post is going to focus on R (obviously) but the ideas are applicable to any programming language.\nThis blog post is part tutorial on creating an api using the {plumber} R package, part an illustration of how Nix makes developing and deploying a breeze."
  },
  {
    "objectID": "posts/2023-07-30-nix_for_r_part3.html#part-1-getting-it-to-work-locally",
    "href": "posts/2023-07-30-nix_for_r_part3.html#part-1-getting-it-to-work-locally",
    "title": "Reproducible data science with Nix, part 3 – frictionless {plumber} api deployments with Nix",
    "section": "\nPart 1: getting it to work locally\n",
    "text": "Part 1: getting it to work locally\n\n\nSo in part 1 I explained what Nix was and how you could use it to build reproducible development environments. In part 2 I talked about running a {targets} pipeline in a reproducible environment set up with Nix, and in this blog post I’ll talk about how I made an api using {plumber} and how Nix made going from my development environment to the production environment (on Digital Ocean) the simplest ever. Originally I wanted to focus on interactive work using Nix, but that’ll be very likely for part 4, maybe even part 5 (yes, I really have a lot to write about).\n\n\nLet me just first explain what {plumber} is before continuing. I already talked about {plumber} here, but in summary, {plumber} allows you to build an api. What is an api? Essentially a service that you can call in different ways and which returns something to you. For example, you could send a Word document to this api and get back the same document converted in PDF. Or you could send some English text and get back a translation. Or you could send some data and get a prediction from a machine learning model. It doesn’t matter: what’s important is that apis completely abstract the programming language that is being used to compute whatever should be computed. With {plumber}, you can create such services using R. This is pretty awesome, because it means that whatever it is you can make with R, you could build a service around it and make it available to anyone. Of course you need a server that actually has R installed and that gets and processes the requests it receives, and this is where the problems start. And by problems I mean THE single biggest problem that you have to deal with whenever you develop something on your computer, and then have to make it work somewhere else: deployment. If you’ve had to deal with deployments you might not understand why it’s so hard. I certainly didn’t really get it until I’ve wanted to deploy my first Shiny app, many moons ago. And this is especially true whenever you don’t want to use any “off the shelf” services like shinyapps.io. In the blog post I mentioned above, I used Docker to deploy the api. But Docker, while an amazing tool, is also quite heavy to deal with. Nix offers an alternative to Docker which I think you should know and think about. Let me try to convince you.\n\n\nSo let’s make a little {plumber} api and deploy that in the cloud. For this, I’m using Digital Ocean, but any other service that allows you to spin a virtual machine (VM) with Ubuntu on it will do. If you don’t have a Digital Ocean account, you can use my referral link to get 200$ in credit for 60 days, more than enough to experiment. A VM serving a {plumber} api needs at least 1 gig of RAM, and the cheapest one with 1 gig of ram is 6$ a month (if you spend 25$ of that credit, I’ll get 25$ too, so don’t hesitate to experiment, you’ll be doing me a solid as well).\n\n\nI won’t explain what my api does, this doesn’t really matter for this blog post. But I’ll have to explain it in a future blog post, because it’s related to a package I’m working on, called {rix} which I’m writing to ease the process of building reproducible environments for R using Nix. So for this blog post, let’s make something very simple: let’s take the classic machine learning task of predicting survival of the passengers of the Titanic (which was not that long ago in the news again…) and make a service out of it.\n\n\nWhat’s going to happen is this: users will make a request to the api giving some basic info about themselves: a simple ML model (I’ll go with logistic regression and call it “machine learning” just to make the statisticians reading this seethe lmao), the machine learning model is going to use this to compute a prediction and the result will be returned to the user. Now to answer a question that comes up often when I explain this stuff: why not use Shiny? Users can enter their data and get a prediction and there’s a nice UI and everything?!. Well yes, but it depends on what it is you actually want to do. An api is useful mostly in situations where you need that request to be made by another machine and then that machine will do something else with that prediction it got back. It could be as simple as showing it in a nice interface, or maybe the machine that made the request will then use that prediction and insert it somewhere for archiving for example. So think of it this way: use an api when machines need to interact with other machines, a Shiny app for when humans need to interact with a machine.\n\n\nOk so first, because I’m using Nix, I’ll create an environment that will contain everything I need to build this api. I’m doing that in the most simple way possible, simply by specifying an R version and the packages I need inside a file called default.nix. Writing this file if you’re not familiar with Nix can be daunting, so I’ve developed a package, called {rix} to write these files for you. Calling this:\n\nrix::rix(r_ver = \"4.2.2\",\n         r_pkgs = c(\"plumber\", \"tidymodels\"),\n         other_pkgs = NULL,\n         git_pkgs = NULL,\n         ide = \"other\",\n         path = \"titanic_api/\", # you might need to create this folder\n         overwrite = TRUE)\n\ngenerates this file for me:\n\n# This file was generated by the {rix} R package on Sat Jul 29 15:50:41 2023\n# It uses nixpkgs' revision 8ad5e8132c5dcf977e308e7bf5517cc6cc0bf7d8 for reproducibility purposes\n# which will install R version 4.2.2\n# Report any issues to https://github.com/b-rodrigues/rix\n{ pkgs ? import (fetchTarball \"https://github.com/NixOS/nixpkgs/archive/8ad5e8132c5dcf977e308e7bf5517cc6cc0bf7d8.tar.gz\") {} }:\n\n  with pkgs;\n\n  let\n  my-r = rWrapper.override {\n    packages = with rPackages; [\n      plumber tidymodels\n    ];\n  };\n  in\n  mkShell {\n    buildInputs = [\n      my-r\n      ];\n  }\n\n(for posterity’s sake: this is using this version of {rix}. Also, if you want to learn more about {rix} take a look at its website. It’s still in very early development, comments and PR more than welcome!)\n\n\nTo build my api I’ll have to have {plumber} installed. I also install the {tidymodels} package. I actually don’t need {tidymodels} for what I’m doing (base R can fit logistic regressions just fine), but the reason I’m installing it is to mimic a “real-word example” as closely as possible (a project with some dependencies).\n\n\nWhen I called rix::rix() to generate the default.nix file, I specified that I wanted R version 4.2.2 (because let’s say that this is the version I need. It’s also possible to get the current version of R by passing “current” to r_ver). You don’t see any reference to this version of R in the default.nix file, but this is the version that will get installed because it’s the version that comes with that particular revision of the nixpkgs repository:\n\n\"https://github.com/NixOS/nixpkgs/archive/8ad5e8132c5dcf977e308e7bf5517cc6cc0bf7d8.tar.gz\"\n\nThis url downloads that particular revision on nixpkgs containing R version 4.2.2. {rix} finds the right revision for you (using this handy service).\n\n\nWhile {rix} doesn’t require your system to have Nix installed, if you want to continue you’ll have to install Nix. To install Nix, I recommend you don’t use the official installer, even if it’s quite simple to use. Instead, the Determinate Systems installer seems better to me. On Windows, you will need to enable WSL2. An alternative is to run all of this inside a Docker container (but more on this later if you’re thinking something along the lines of isn’t the purpose of Nix to not have to use Docker? then see you in the conclusion). Once you have Nix up and running, go inside the titanic_api/ folder (which contains the default.nix file above) and run the following command inside a terminal:\n\nnix-build\n\nThis will build the environment according to the instructions in the default.nix file. Depending on what you want/need, this can take some time. Once the environment is done building, you can “enter” into it by typing:\n\nnix-shell\n\nNow this is where you would use this environment to work on your api. As I stated above, I’ll discuss interactive work using a Nix environment in a future blog post. Leave the terminal with this Nix shell open and create an empty text wile next to default.nix and call it titanic_api.R and put this in there using any text editor of your choice:\n\n#* Would you have survived the Titanic sinking?\n#* @param sex Character. \"male\" or \"female\"\n#* @param age Integer. Your age.\n#* @get /prediction\nfunction(sex, age) {\n\n  trained_logreg &lt;- readRDS(\"trained_logreg.rds\")\n\n  dataset &lt;- data.frame(sex = sex, age = as.numeric(age))\n\n  parsnip::predict.model_fit(trained_logreg,\n                             new_data = dataset)\n\n}\n\nThis script is a {plumber} api. It’s a simple function that uses an already trained logistic regression (lol) by loading it into its scope using the readRDS() function. It then returns a prediction. The script that I wrote to train the model is this one:\n\nlibrary(parsnip)\n\ntitanic_raw &lt;- read.csv(\"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\")\n\ntitanic &lt;- titanic_raw |&gt;\n  subset(select = c(Survived,\n                    Sex,\n                    Age))\n\nnames(titanic) &lt;- c(\"survived\", \"sex\", \"age\")\n\ntitanic$survived = as.factor(titanic$survived)\n\nlogreg_spec &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\")\n\ntrained_logreg &lt;- logreg_spec |&gt;\n  fit(survived ~ ., data = titanic)\n\nsaveRDS(trained_logreg, \"trained_logreg.rds\")\n\nIf you’re familiar with this Titanic prediction task, you will have noticed that the script above is completely stupid. I only kept two variables to fit the logistic regression. But the reason I did this is because this blog post is not about fitting models, but about apis. So bear with me. Anyways, once you’re run the script above to generate the file trained_logreg.rds containing the trained model, you can locally test the api using {plumber}. Go back to the terminal that is running your Nix shell, and now type R to start R in that session. You can then run your api inside that session using:\n\nplumber::pr(\"titanic_api.R\") |&gt;\n  plumber::pr_run(port = \"8000\")\n\nOpen your web browser and visit http://localhost:8000/docs/ to see the Swagger interface to your api (Swagger is a nice little tool that makes testing your apis way easier).\n\n\n\n\n\n\n\nUsing Swagger you can try out your api, click on (1) then on (2). You can enter some mock data in (3) and (4) and then run the computation by clicking on “Execute” (5). You’ll see the result in (7). (6) gives you a curl command to run exactly this example from a terminal. Congrats, your {plumber} api is running on your computer! Now we need to deploy it online and make it available to the world."
  },
  {
    "objectID": "posts/2023-07-30-nix_for_r_part3.html#deploying-your-api",
    "href": "posts/2023-07-30-nix_for_r_part3.html#deploying-your-api",
    "title": "Reproducible data science with Nix, part 3 – frictionless {plumber} api deployments with Nix",
    "section": "\nDeploying your api\n",
    "text": "Deploying your api\n\n\nSo if you have a Digital Ocean account log in (and if you don’t, use my referral link to get 200$ to test things out) and click on the top-right corner on the “Create” button, and then select “Droplet” (a fancy name for a VM):\n\n\n\n\n\n\n\nIn the next screen, select the region closest to you and then select Ubuntu as the operating system, “Regular” for the CPU options, and then the 4$ (or the 6(, it doesn't matter at this stage) a month Droplet. We will need to upgrade it immediately after having created it in order to actually build the environment. This is because building the environment requires some more RAM than what the 6) option offers, but starting from the cheapest option ensures that we will then be able to downsize back to it, after the build process is done.\n\n\n\n\n\n\n\nNext comes how you want to authenticate to your VM. There are two options, one using an SSH key, another using a password. If you’re already using Git, you can use the same SSH key. Click on “New SSH Key” and paste the public key in the box (you should find the key under ~/.ssh/id_rsa.pub if you’re using Linux). If you’re not using Git and have no idea what SSH keys are, my first piece of advice is to start using Git and then to generate an SSH key and login using it. This is much more secure than a password. Finally, click on “Create Droplet”. This will start building your VM. Once the Droplet is done building, you can check out its IP address:\n\n\n\n\n\n\n\nLet’s immediately resize the Droplet to a larger size. As I said before, this is only required to build our production environment using Nix. Once the build is done, we can downsize again to the cheapest Droplet:\n\n\n\n\n\n\n\nChoose a Droplet with 2 gigs of RAM to be on the safe side, and also enable the reserved IP option (this is a static IP that will never change):\n\n\n\n\n\n\n\nFinally, turn on your Droplet, it’s time to log in to it using SSH.\n\n\nOpen a terminal on your computer and connect to your Droplet using SSH (starting now, user@local_computer refers to a terminal opened on your computer and root@droplet to an active ssh session inside your Droplet):\n\nuser@local_computer &gt; ssh root@IP_ADDRESS_OF_YOUR_DROPLET\n\nand add a folder that will contain the project’s files:\n\nroot@droplet &gt; mkdir titanic_api\n\nGreat, let’s now copy our files to the Droplet using scp. Open a terminal on your computer, and navigate to where the default.nix file is. If you prefer doing this graphically, you can use Filezilla. Run the following command to copy the default.nix file to the Droplet:\n\nuser@local_computer &gt; scp default.nix root@IP_ADDRESS_OF_YOUR_DROPLET:/root/titanic_api/\n\nNow go back to the terminal that is logged into your Droplet. We now need to install Nix. For this, follow the instructions from the Determinate Systems installer, and run this line in the Droplet:\n\nroot@droplet &gt; curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install\n\nPay attention to the final message once the installation is done:\n\nNix was installed successfully!\nTo get started using Nix, open a new shell or run `. /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh`\n\nSo run . /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh to start the Nix daemon. Ok so now comes the magic of Nix. You can now build the exact same environment that you used to build the pipeline on your computer in this Droplet. Simply run nix-build for the build process to start. I don’t really know how to describe how easy and awesome this is. You may be thinking well installing R and a couple of packages is not that hard, but let me remind you that we are using a Droplet that is running Ubuntu, which is likely NOT the operating system that you are running. Maybe you are on Windows, maybe you are on macOS, or maybe you’re running another Linux distribution. Whatever it is you’re using, it will be different from that Droplet. Even if you’re running Ubuntu on your computer, chances are that you’ve changed the CRAN repositories from the default Ubuntu ones to the Posit ones, or maybe you’re using r2u. Basically, the chances that you will have the exact same environment in that Droplet than the one running on your computer is basically 0. And if you’re already familiar with Docker, I think that you will admit that this is much, much easier than dockerizing your {plumber} api. If you don’t agree, please shoot me an email and tell me why, I’m honestly curious. Also, let me stress again that if you needed to install a package like {xlsx} that requires Java to be installed, Nix would install the right version of Java for you.\n\n\nOnce the environment is done building, you can downsize your Droplet. Go back to your Digital Ocean account, select that Droplet and choose “Resize Droplet”, and go back to the 6$ a month plan.\n\n\nSSH back into the Droplet and copy the trained model trained_logreg.rds and the api file, titanic_api.R to the Droplet using scp or Filezilla. It’s time to run the api. To do so, the obvious way would be simply to start an R session and to execute the code to run the api. However, if something happens and the R session dies, the api won’t restart. Instead, I’m using a CRON job and an utility called run-one. This utility, pre-installed in Ubuntu, runs one (1) script at a time, and ensures that only one instance of said script is running. So by putting this in a CRON job (CRON is a scheduler, so it executes a script as often as you specify), run-one will try to run the script. If it’s still running, nothing happens, if the script is not running, it runs it.\n\n\nSo go back to your local computer, and create a new text file, call it run_api.sh and write the following text in it:\n\n#!/bin/bash\nwhile true\ndo\nnix-shell /root/titanic_api/default.nix --run \"Rscript -e 'plumber::pr_run(plumber::pr(\\\"/root/titanic_api/titanic_api.R\\\"), host = \\\"0.0.0.0\\\", port=80)'\"\n sleep 10\ndone\n\nthen copy this to your VM using scp or Filezilla, to /root/titanic_api/run_api.sh. Then SSH back into your Droplet, go to where the script is using cd:\n\nroot@droplet &gt; cd /root/titanic_api/\n\nand make the script executable:\n\nroot@droplet &gt; chmod +x run_api.sh\n\nWe’re almost done. Now, let’s edit the crontab, to specify that we want this script to be executed every hour using run-one (so if it’s running, nothing happens, if it died, it gets restarted). To edit the crontab, type crontab -e and select the editor you’re most comfortable with. If you have no idea, select the first option, nano. Using your keyboard keys, navigate all the way down and type:\n\n*/60 * * * * run-one /root/titanic_api/run_api.sh\n\nsave the file by typing CTRL-X, and then type Y when asked Save modified buffer?, and then type the ENTER key when prompted for File name to write.\n\n\nWe are now ready to start the api. Make sure CRON restarts by running:\n\nroot@droplet &gt; service cron reload\n\nand then run the script using nohup followed by run-one:\n\nroot@droplet &gt; nohup run-one /root/titanic_api/run_api.sh &\n\nrun-one will now run the script and will ensure that only one instance of the script is running (the & character at the end means “run this in the background” an nohup, which stands for “no hang-up”, ensures the command will continue running even when you close the terminal). If for any reason the process dies, CRON will restart an instance of the script. We can now call our api using this curl command:\n\nuser@local_computer &gt; curl -X GET \"http://IP_ADDRESS_OF_YOUR_DROPLET/prediction?sex=female&age=45\" -H \"accept: */*\"\n\nIf you don’t have curl installed, you can use this webservice. You should see this answer:\n\n[{\n    \".pred_class\": \"1\"\n}]\n\nI’ll leave my Droplet running for a few days after I post this, so if you want you can try it out run this:\n\ncurl -X GET \"http://142.93.164.182/prediction?sex=female&age=45\" -H \"accept: */*\"\n\nThe answer is in the JSON format, and can now be ingested by some other script which can now process it further."
  },
  {
    "objectID": "posts/2020-11-21-guis_mistake.html",
    "href": "posts/2020-11-21-guis_mistake.html",
    "title": "Graphical User Interfaces were a mistake but you can still make things right",
    "section": "",
    "text": "Some weeks ago I tweeted this:\n\n\n\nGUIs were a mistake\n\n— Bruno Rodrigues (@brodriguesco) October 9, 2020\n\n\n\nyou might think that I tweeted this as an unfunny joke, but it’s not. GUIs were one of the worst things to happen for statisticians. Clickable interfaces for data analysis is probably one of the greatest source of mistakes and errors in data processing, very likely costing many millions to companies worldwide and is a source of constant embarassment when mistakes happen which cost the reputation, and money, of institutions or people.\n\n\nRemember the infamous Excel mistake by Reinhard and Rogoff? If you don’t know what I’m talking about, you can get up to speed by reading this. I think the most interesting sentence is this:\n\n\n\nThe most serious was that, in their Excel spreadsheet, Reinhart and Rogoff had not selected the entire row when averaging growth figures: they omitted data from Australia, Austria, Belgium, Canada and Denmark.\n\n\n\nThis is a typical mistake that happens when a mouse is used to select data in a GUI, instead of typing whatever you need in a scripting language. Many other mistakes like that happen, and they remain hidden, potentially for years, or go unreported.\n\n\nRecently there was another Excel-related problem in England where positive Covid tests got lost. For some obscure reason, the raw data, which was encoded in a CSV file, got converted into an Excel spreadsheet, most likely for further analysis. The problem is that the format that was used was the now obsolete XLS format, instead of the latest XLSX format, which can handle millions of rows. Because the data was converted in the XLS format, up to 15841 cases were lost. You can get all the details from this BBC article. Again, not entirely Excel’s fault, as it was misused. The problem is that when all you have is a hammer, everything looks like a nail, and Excel is that data analytics hammer. So to the uncultured, everything looks like an Excel problem.\n\n\nNow don’t misunderstand me; I’m not blaming Excel specifically, or any other specific GUI application for this. In many cases, the problem lies between the keyboard and the chair. But GUI applications have a part of responsibility, as they allow users to implement GUI-based workflows. I think that complex GUI based workflows were an unintended consequence of developing GUIs. Who could have expected, 40 years ago, that office jobs would evolve so much and that they would require such complex workflows to generate an output? Consider the life-cycle of a shared Excel file in your typical run-of-the-mill financial advisory firm. In many cases, it starts with an already existing file that was made for another client and that is now used as a starting point. The first thing to do, is to assign a poor junior to update the file and adapt it for the current assignment. He or she will spend hours trying to reverse engineer this Excel file and then update it. This file will at some point go to more senior members that will continue working on it, until it gets send off for review to a manager. This manager, already overworked and with little time between meetings to review the file correctly, just gives it a cursory glance and might find some mistakes here and there. As a review method, colours and comments will be used. The file goes back for a round of updates and reviews. As time goes by, and as the file gets more and more complex, it starts to become impossible to manage and review properly. It eventually gets used to give advice to a client, which might be totally wrong, because just as in the case of Reinhard and Rogoff, someone, at some point, somewhere, did not select the right cells for the right formula. Good luck ever finding this mistake, and who did it. During my consulting years, I have been involved with very, very, big clients that were completely overwhelmed because all their workflows were GUI based. They had been working like that for years, and kept recruiting highly educated people en masse just to manage Excel and Word files. They were looking for a magic, AI-based solution, because in their minds, if AIs could drive fricking cars, they should also be able to edit and send Excel files around for review. Well, we’re not quite there yet, so we told them, after our review of their processes and data sources (which in many cases were Excel AND Word files), that what they needed was for their company to go through an in-depth optimisation process “journey”. They weren’t interested so they kept hiring very intelligent people to be office drones. I don’t think that business model can remain sustainable.\n\n\nNow how much are situations like that the fault of Excel and how much personal responsibility do the people involved have? I don’t know, but my point is that if, by magic, GUIs were made to disappear, problems like that would also not exist. The reason is that if you’re forced to write code to reach the results you want, you avoid a lot of these pitfalls I just described. Working with scripts and the command line forces a discipline unto you; you cannot be lazy and click around. For example, reverse engineering a source code file is much easier that a finished Excel spreadsheet. Even poorly written and undocumented code is always much better than an Excel spreadsheet. If you throw a version control system in the mix, you have the whole history of the file and the ability to know exactly what happened and when. Add unit tests on the pile, and you start to get something that is very robust, transparent, and much easier to audit.\n\n\n“But Bruno, not everyone is a programmer!” I hear you scream at your monitor.\n\n\nMy point, again, is that if GUIs did not exist, people would have enough knowledge of these tools to be able to work. What other choice would they have?\n\n\nOf course, GUIs have been invented, and they’re going nowhere. So what can you do?\n\n\nWhen it comes to statistics and data analysis/processing, you can at least not be part of the problem and avoid using Excel altogether. If we go back to our previous scenario from the financial advisory firm, the first step, which consisted in reverse engineering an Excel file, can be done using {tidyxl}. Let’s take a quick look; the spreadsheet I used as the header image for this blog post comes from the Enron corpus , which is mostly know for being a database of over 600000 emails from the US company Enron. But it also contains spreadsheets, which are delightful. You can download the one from the picture here (8mb xlsx warning). Opening it in your usual spreadsheet application will probably cause your heart rate to increase to dangerous levels, so avoid that. Instead, let’s take a look at what {tidyxl} does with it:\n\nlibrary(tidyxl)\n## Warning: package 'tidyxl' was built under R version 4.0.3\nlibrary(tidyverse)\n## Warning: replacing previous import 'vctrs::data_frame' by 'tibble::data_frame'\n## when loading 'dplyr'\n## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──\n## ✔ ggplot2 3.3.2     ✔ purrr   0.3.4\n## ✔ tibble  3.0.1     ✔ dplyr   1.0.0\n## ✔ tidyr   1.1.2     ✔ stringr 1.4.0\n## ✔ readr   1.3.1     ✔ forcats 0.5.0\n## Warning: package 'tidyr' was built under R version 4.0.2\n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\ndutch_quigley_9378 &lt;- xlsx_cells(\"~/six_to/spreadsheets/dutch_quigley__9378__modeldutch.xlsx\")\n\n\nhead(dutch_quigley_9378)\n## Warning: `...` is not empty.\n## \n## We detected these problematic arguments:\n## * `needs_dots`\n## \n## These dots only exist to allow future extensions and should be empty.\n## Did you misspecify an argument?\n## # A tibble: 6 x 21\n##   sheet address   row   col is_blank data_type error logical numeric\n##   &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;lgl&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;     &lt;dbl&gt;\n## 1 Swap… A1          1     1 FALSE    character &lt;NA&gt;  NA           NA\n## 2 Swap… D2          2     4 FALSE    character &lt;NA&gt;  NA           NA\n## 3 Swap… E2          2     5 FALSE    character &lt;NA&gt;  NA           NA\n## 4 Swap… F2          2     6 FALSE    character &lt;NA&gt;  NA           NA\n## 5 Swap… G2          2     7 FALSE    character &lt;NA&gt;  NA           NA\n## 6 Swap… D3          3     4 FALSE    character &lt;NA&gt;  NA           NA\n## # … with 12 more variables: date &lt;dttm&gt;, character &lt;chr&gt;,\n## #   character_formatted &lt;list&gt;, formula &lt;chr&gt;, is_array &lt;lgl&gt;,\n## #   formula_ref &lt;chr&gt;, formula_group &lt;int&gt;, comment &lt;chr&gt;, height &lt;dbl&gt;,\n## #   width &lt;dbl&gt;, style_format &lt;chr&gt;, local_format_id &lt;int&gt;\n\nThat whole Excel workbook is inside a neat data frame. Imagine that you want to quickly know where all the formulas are:\n\ndutch_quigley_9378 %&gt;%\n  filter(!is.na(formula)) %&gt;%\n  count(sheet, address)\n## Warning: `...` is not empty.\n## \n## We detected these problematic arguments:\n## * `needs_dots`\n## \n## These dots only exist to allow future extensions and should be empty.\n## Did you misspecify an argument?\n## # A tibble: 18,776 x 3\n##    sheet address     n\n##    &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;\n##  1 Front B22         1\n##  2 Front C13         1\n##  3 Front C2          1\n##  4 Front C22         1\n##  5 Front C25         1\n##  6 Front C26         1\n##  7 Front C27         1\n##  8 Front C28         1\n##  9 Front C30         1\n## 10 Front C31         1\n## # … with 18,766 more rows\n\nWith the code above, you can quickly find, for each sheet, where the formulas are. This workbook contains 18776 formulas. If Hell is a real place, it’s probably an office building full of cubicles where you’ll sit for eternity looking at these spreadsheets and trying to make sense of them.\n\n\nNow imagine that you’d like to know what these formulas are, let’s say, for the Swap sheet:\n\ndutch_quigley_9378 %&gt;%\n  filter(sheet == \"Swap\", !is.na(formula)) %&gt;%\n  select(address, formula)\n## Warning: `...` is not empty.\n## \n## We detected these problematic arguments:\n## * `needs_dots`\n## \n## These dots only exist to allow future extensions and should be empty.\n## Did you misspecify an argument?\n## # A tibble: 6,773 x 2\n##    address formula           \n##    &lt;chr&gt;   &lt;chr&gt;             \n##  1 F1      DAY(EOMONTH(G1,0))\n##  2 G1      A11               \n##  3 E2      BE9               \n##  4 A3      BQ5               \n##  5 E3      BF9               \n##  6 F3      SUM(G3:K3)        \n##  7 H3      $F$1*H2           \n##  8 I3      $F$1*I2           \n##  9 J3      $F$1*J2           \n## 10 K3      $F$1*K2           \n## # … with 6,763 more rows\n\nBrilliant! Maybe you’re interested to find all the “SUM” formulas? Easy!\n\ndutch_quigley_9378 %&gt;%\n  filter(sheet == \"Swap\", !is.na(formula)) %&gt;%\n  filter(grepl(\"SUM\", formula)) %&gt;%\n  select(address, formula)\n## Warning: `...` is not empty.\n## \n## We detected these problematic arguments:\n## * `needs_dots`\n## \n## These dots only exist to allow future extensions and should be empty.\n## Did you misspecify an argument?\n## # A tibble: 31 x 2\n##    address formula        \n##    &lt;chr&gt;   &lt;chr&gt;          \n##  1 F3      SUM(G3:K3)     \n##  2 E4      SUM(D11:D309)  \n##  3 F5      SUM(G5:K5)     \n##  4 E6      SUM(F6:H6)     \n##  5 BF8     SUM(BF11:BF242)\n##  6 B9      SUM(B47:B294)  \n##  7 AB9     SUM(AB11:AB253)\n##  8 AC9     SUM(AC11:AC253)\n##  9 AD9     SUM(AD11:AD253)\n## 10 AE9     SUM(AE11:AE253)\n## # … with 21 more rows\n\nYou get the idea. There are many more things that you can extract such as the formatting, the contents of the cells, the comments (and where to find them) and much, much more. This will make making sense of a complex Excel file a breeze.\n\n\nThe other thing that you can also do, once you’re done understanding this monster Excel file, is not to perform the analysis inside Excel. Don’t fall into the temptation of continuing this bad habit. As one on the data experts in your team/company, you have a responsibility to bring the light to your colleagues. Be their Prometheus and decouple the data from the code. Let the data be in Excel, but write all the required code to create whatever is expected from you inside R. You can then export your finalized results back to Excel if needed. If management tells you to do it in Excel, tell them that you’re the professional statistician/data scientist, and that they shouldn’t tell you how to do your job. Granted, this is not always possible, but you should plead your case as much as possible. In general, a good manager will be all ears if you explain that not using GUIs like Excel makes it easier to spot and correct mistakes, with the added benefit of being much easily audited and with huge time savings in the long run. This is of course easier for completely new projects, and if you have an open-minded manager. If you’re the manager, then you should ask your IT department to uninstall Excel from your team member’s computers.\n\n\nBe brave, and ditch the GUI."
  },
  {
    "objectID": "posts/2023-07-13-nix_for_r_part1.html",
    "href": "posts/2023-07-13-nix_for_r_part1.html",
    "title": "Reproducible data science with Nix, part 1 – what is Nix",
    "section": "",
    "text": "This is the first of a (hopefully) series of posts about Nix. Disclaimer: I’m a super beginner with Nix. So this series of blog posts is more akin to notes that I’m taking while learning than a super detailed tutorial. So if you’re a Nix expert and read something stupid in here, that’s normal. This post is going to focus on R (obviously) but the ideas are applicable to any programming language.\n\n\nTo ensure that a project is reproducible you need to deal with at least four things:\n\n\n\nMake sure that the required/correct version of R (or any other language) is installed;\n\n\nMake sure that the required versions of packages are installed;\n\n\nMake sure that system dependencies are installed (for example, you’d need a working Java installation to install the {rJava} R package on Linux);\n\n\nMake sure that you can install all of this for the hardware you have on hand.\n\n\n\nFor the three first bullet points, the consensus seems to be a mixture of Docker to deal with system dependencies, {renv} for the packages (or {groundhog}, or a fixed CRAN snapshot like those Posit provides) and the R installation manager to install the correct version of R (unless you use a Docker image as base that already ships the required version by default). As for the last point, the only way out is to be able to compile the software for the target architecture. There’s a lot of moving pieces, and knowledge that you need to know and I even wrote a whole 522 pages book about all of this.\n\n\nBut it turns out that this is not the only solution. Docker + {renv} (or some other way to deal with packages) is likely the most popular way to ensure reproducibility of your projects, but there are other tools to achieve this. One such tool is called Nix.\n\n\nNix is a package manager for Linux distributions, macOS and apparently it even works on Windows if you enable WSL2. What’s a package manager? If you’re not a Linux user, you may not be aware. Let me explain it this way: in R, if you want to install a package to provide some functionality not included with a vanilla installation of R, you’d run this:\n\ninstall.packages(\"dplyr\")\n\nIt turns out that Linux distributions, like Ubuntu for example, work in a similar way, but for software that you’d usually install using an installer (at least on Windows). For example you could install Firefox on Ubuntu using:\n\nsudo apt-get install firefox\n\n(there’s also graphical interfaces that make this process “more user-friendly”). In Linux jargon, packages are simply what normies call software (or I guess it’s all “apps” these days). These packages get downloaded from so-called repositories (think of CRAN, the repository of R packages) but for any type of software that you might need to make your computer work: web browsers, office suites, multimedia software and so on.\n\n\nSo Nix is just another package manager that you can use to install software.\n\n\nBut what interests us is not using Nix to install Firefox, but instead to install R and the R packages that we require for our analysis (or any other programming language that we need). But why use Nix instead of the usual ways to install software on our operating systems?\n\n\nThe first thing that you should know is that Nix’s repository, nixpkgs, is huge. Humongously huge. As I’m writing these lines, there’s more than 80’000 pieces of software available, and the entirety of CRAN is also available through nixpkgs. So instead of installing R as you usually do and then use install.packages() to install packages, you could use Nix to handle everything. But still, why use Nix at all?\n\n\nNix has an interesting feature: using Nix, it is possible to install software in (relatively) isolated environments. So using Nix, you can install as many versions of R and R packages that you need. Suppose that you start working on a new project. As you start the project, with Nix, you would install a project-specific version of R and R packages that you would only use for that particular project. If you switch projects, you’d switch versions of R and R packages. If you are familiar with {renv}, you should see that this is exactly the same thing: the difference is that not only will you have a project-specific library of R packages, you will also have a project-specific R version. So if you start a project now, you’d have R version 4.2.3 installed (the latest version available in nixpkgs but not the latest version available, more on this later), with the accompagnying versions of R packages, for as long as the project lives (which can be a long time). If you start a project next year, then that project will have its own R, maybe R version 4.4.2 or something like that, and the set of required R packages that would be current at that time. This is because Nix always installs the software that you need in separate, (isolated) environments on your computer. So you can define an environment for one specific project.\n\n\nBut Nix even goes even further: not only can you install R and R packages using Nix (in isolated) project-specific environments, Nix even installs the required system dependencies. So for example if I need {rJava}, Nix will make sure to install the correct version of Java as well, always in that project-specific environment (so if you already some Java version installed on your system, there won’t be any interference).\n\n\nWhat’s also pretty awesome, is that you can use a specific version of nixpkgs to always get exactly the same versions of all the software whenever you build that environment to run your project’s code. The environment gets defined in a simple plain-text file, and anyone using that file to build the environment will get exactly, byte by byte, the same environment as you when you initially started the project. And this also regardless of the operating system that is used.\n\n\nSo let me illustrate this. After installing Nix, I can define an environment by writing a file called default.nix that looks like this:\n\n{ pkgs ? import (fetchTarball \"https://github.com/NixOS/nixpkgs/archive/e11142026e2cef35ea52c9205703823df225c947.tar.gz\") {} }:\n\nwith pkgs;\n\nlet\n  my-pkgs = rWrapper.override {\n    packages = with rPackages; [ dplyr ggplot2 R];\n  };\nin\nmkShell {\n  buildInputs = [my-pkgs];\n}\n\nNow this certainly looks complicated! And it is. The entry cost to Nix is quite high, because, actually, Nix is more than a package manager. It is also a programming language, and this programming language gets used to configure environments. I won’t go too much into detail, but you’ll see in the first line that I’m using a specific version of nixpkgs that gets downloaded directly from Github. This means that all the software that I will install with that specific version of nixpkgs will always install the same software. This is what ensures that R and R packages are versioned. Basically, by using a specific version of nixpkgs, I pin all the versions of all the software that this particular version of Nix will ever install. I then define a variable called my-pkgs which lists the packages I want to install ({dplyr}, {ggplot2} and R).\n\n\nBy the way, this may look like it would take a lot of time to install because, after all, you need to install R, R packages and underlying system dependencies, but thankfully there is an online cache of binaries that gets automatically used by Nix (cache.nixos.org) for fast installations. If binaries are not available, sources get compiled.\n\n\nI can now create an environment with these exact specifications using (in the directory where default.nix is):\n\nnix-build\n\nor I could use the R version from this environment to run some arbitrary code:\n\nnix-shell /home/renv/default.nix --run \"Rscript -e 'sessionInfo()'\" &gt;&gt; /home/renv/sessionInfo.txt\n\n(assuming my default.nix file is available in the /home/renv/ directory). This would build the environment on the fly and run sessionInfo() inside of it. Here are the contents of this sessionInfo.txt file:\n\nR version 4.2.3 (2023-03-15)\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nMatrix products: default\nBLAS/LAPACK: /nix/store/pbfs53rcnrzgjiaajf7xvwrfqq385ykv-blas-3/lib/libblas.so.3\n\nlocale:\n[1] C\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n[1] compiler_4.2.3\n\nThis looks like any other output of the sessionInfo() function, but there is something quite unusual: the BLAS/LAPACK line:\n\nBLAS/LAPACK: /nix/store/pbfs53rcnrzgjiaajf7xvwrfqq385ykv-blas-3/lib/libblas.so.3\n\nBLAS is a library that R uses for linear algebra, matrix multiplication and vector operations. R usually ships with its own version of BLAS and LAPACK, but it’s also possible to use external ones. Here, we see that the path to the shared object libblas.so.3 is somewhere in /nix/store/….. /nix/store/ is where all the software gets installed. The long chain of seemingly random characters is a hash, essentially the unique identifier of that particular version of BLAS. This means that unlike Docker, if you’re using Nix you are also certain than these types of dependencies, that may have an impact on your results, also get handled properly, and that the exact same version you used will keep getting installed in the future. Docker images also evolve, and even if you use an LTS release of Ubuntu as a base, the underlying system packages will evolve through time as well. And there will be a point in time where this release will be abandoned (LTS releases receive 5 years of support), so if you need to rebuild a Docker images based on an LTS that doesn’t get supported anymore, you’re out of luck.\n\n\nIf you don’t want to install Nix just yet on your computer, you should know that there’s also a complete operating system called NixOS, that uses Nix as its package manager, and that there are Docker images that use NixOS as a base. So this means that you could use such an image and then build the environment (that is 100% completely reproducible) inside and run a container that will always produce the same output. To see an example of this, check out this Github repo. I’m writing a Dockerfile as I usually do, but actually I could even use Nix to define the Docker image for me, it’s that powerful!\n\n\nNix seems like a very powerful tool to me. But there are some “issues”:\n\n\n\nAs I stated above, the entry cost is quite high, because Nix is not “just a tool”, it’s a complete programming language that can even run pipelines, so you could technically even replace something like {targets} with it;\n\n\nIf you need to install specific versions of R packages, that are not pinned to dates, then Nix is not for you. Nix will always create a coherent environment with R and R packages that go together for a particular release of nixpkgs. If for some reason you need a very old version of {ggplot2} but a much more recent version of {dplyr}, using Nix won’t make this any easier than other methods;\n\n\nThere is no easy way (afaik) to find the version of nixpkgs that you need to download to find the version of R that you may need; UPDATE: turns out that there is such a simple tool, thanks to @shane@hachyderm.io for the telling me!\n\n\nR packages (and I guess others for other programming languages as well) that are available on the stable channel of nixpkgs lag a bit behind their counterparts on CRAN. These usually all get updated whenever there’s a new release of R. Currently however, R is at version 4.2.3, but R should be at version 4.3.1 on the stable branch of nixpkgs. This can sometimes happen due to various reasons (there are actual human beings behind this that volunteer their time and they also have a life). There is however an “unstable” nixpkgs channel that contains bleeding edge versions of R packages (and R itself) if you really need the latest versions of packages (don’t worry about the “unstable” label, from my understanding this simply means that package have not been thoroughly tested yet, but is still pretty much rock-solid);\n\n\nIf you need something that is not on CRAN (or Bioconductor) then it’s still possible to use Nix to install these packages, but you’ll have to perform some manual configuration.\n\n\n\nI will keep exploring Nix, and this is essentially my todo:\n\n\n\nusing my environment that I installed with Nix to work interactively;\n\n\nwrite some tool that lets me specify an R version, a list of packages and it generates a default.nix file automagically (ideally it should also deal with packages only available on Github);\n\n\n????\n\n\nProfit!\n\n\n\n\nResources\n\n\nHere are some of the resources I’ve been using:\n\n\n\nnix.dev tutorials\n\n\nINRIA’s Nix tutorial\n\n\nNix pills\n\n\nNix for Data Science\n\n\nNixOS explained: NixOS is an entire Linux distribution that uses Nix as its package manager.\n\n\nBlog post: Nix with R and devtools\n\n\nBlog post: Statistical Rethinking and Nix\n\n\nBlog post: Searching and installing old versions of Nix packages\n\n\n\n\n\nThanks\n\n\nMany thanks to Justin Bedő, maintainer of the R package for Nix, for answering all my questions on Nix!"
  },
  {
    "objectID": "posts/2018-03-12-keep_trying.html",
    "href": "posts/2018-03-12-keep_trying.html",
    "title": "Keep trying that api call with purrr::possibly()",
    "section": "",
    "text": "Sometimes you need to call an api to get some result from a web service, but sometimes this call might fail. You might get an error 500 for example, or maybe you’re making too many calls too fast. Regarding this last point, I really encourage you to read Ethics in Web Scraping.\n\n\nIn this blog post I will show you how you can keep trying to make this api call using purrr::possibly().\n\n\nFor this, let’s use this function that will simulate an api call:\n\nget_data = function(){\n  number = rbinom(1, 1, 0.9)\n  ifelse(number == 0, \"OK\", stop(\"Error: too many calls!\"))\n}\n\nThis function simply returns a random draw from a binomial distribution. If this number equals 0 with probability 0.1, the function returns “OK”, if not, it throws an error. Because the probability of success is only 10%, your api call might be unsuccessful:\n\nget_data()\nError in ifelse(number == 0, \"OK\", stop(\"Error: too many calls!\")) :\n  Error: too many calls!\n\nHow to keep trying until it works? For this, we’re going to use purrr::possibly(); this function takes another function as argument and either returns the result, or another output in case of error, that the user can define:\n\npossibly_get_data = purrr::possibly(get_data, otherwise = NULL)\n\nLet’s try it:\n\nset.seed(12)\npossibly_get_data()\n## NULL\n\nWith set.seed(12), the function returns a number different from 0, and thus throws an error: but because we’re wrapping the function around purrr::possibly(), the function now returns NULL. The first step is done; now we can use this to our advantage:\n\ndefinitely_get_data = function(func, n_tries, sleep, ...){\n\n  possibly_func = purrr::possibly(func, otherwise = NULL)\n\n  result = NULL\n  try_number = 1\n\n  while(is.null(result) && try_number &lt;= n_tries){\n    print(paste(\"Try number: \", try_number))\n    try_number = try_number + 1\n    result = possibly_func(...)\n    Sys.sleep(sleep)\n  }\n\n  return(result)\n}\n\ndefinitely_get_data() is a function that takes any function as argument, as well as a user provided number of tries (as well as … to pass further arguments to func()). Remember, if func() fails, it will return NULL; the while loop ensures that while the result is NULL, and the number of tries is below what you provided, the function will keep getting called. I didn’t talk about sleep; this argument is provided to Sys.sleep() which introduces a break between calls that is equal to sleep seconds. This ensures you don’t make too many calls too fast. Let’s try it out:\n\nset.seed(123)\ndefinitely_get_data(get_data, 10, 1)\n## [1] \"Try number:  1\"\n## [1] \"Try number:  2\"\n## [1] \"Try number:  3\"\n## [1] \"Try number:  4\"\n## [1] \"Try number:  5\"\n## [1] \"OK\"\n\nIt took 5 tries to get the result! However, if after 10 tries get_data() fails to return what you need it will stop (but you can increase the number of tries…).\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions.html",
    "href": "posts/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions.html",
    "title": "Data frame columns as arguments to dplyr functions",
    "section": "",
    "text": "Suppose that you would like to create a function which does a series of computations on a data frame. You would like to pass a column as this function’s argument. Something like:\n\ndata(cars)\nconvertToKmh &lt;- function(dataset, col_name){\n  dataset$col_name &lt;- dataset$speed * 1.609344\n  return(dataset)\n}\n\nThis example is obviously not very interesting (you don’t need a function for this), but it will illustrate the point. You would like to append a column called speed_in_kmh with the speed in kilometers per hour to this dataset, but this is what happens:\n\nhead(convertToKmh(cars, \"speed_in_kmh\"))\n##   speed dist  col_name\n1     4    2  6.437376\n2     4   10  6.437376\n3     7    4 11.265408\n4     7   22 11.265408\n5     8   16 12.874752\n6     9   10 14.484096\n\nYour column is not called speed_in_kmh but col_name! It turns out that there is a very simple solution:\n\nconvertToKmh &lt;- function(dataset, col\\_name){\n  dataset[col_name] &lt;- dataset$speed * 1.609344\n  return(dataset)\n}\n\nhead(convertToKmh(cars, \"speed\\_in\\_kmh\"))\n##   speed dist speed\\_in\\_kmh\n1     4    2     6.437376\n2     4   10     6.437376\n3     7    4    11.265408\n4     7   22    11.265408\n5     8   16    12.874752\n6     9   10    14.484096\n\nYou can access columns with [] instead of $.\n\n\nBut sometimes you want to do more complex things and for example have a function that groups by a variable and then computes new variables, filters by another and so on. You would like to avoid having to hard code these variables in your function, because then why write a function and of course you would like to use dplyr to do it.\n\n\nI often use dplyr functions in my functions. For illustration purposes, consider this very simple function:\n\nsimpleFunction &lt;- function(dataset, col_name){\n  require(\"dplyr\")\n  dataset %&gt;% \n    group\\_by(col\\_name) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\nsimpleFunction(cars, \"dist\")\n\nThis function takes a dataset as an argument, as well as a column name. However, this does not work. You get this error:\n\nError: unknown variable to group by : col_name \n\nThe variable col_name is passed to simpleFunction() as a string, but group_by() requires a variable name. So why not try to convert col_name to a name?\n\nsimpleFunction &lt;- function(dataset, col_name){\n  require(\"dplyr\")\n  col\\_name &lt;- as.name(col_name)\n  dataset %&gt;% \n    group\\_by(col\\_name) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\nsimpleFunction(cars, \"dist\")\n\nYou get the same error as before:\n\nError: unknown variable to group by : col_name \n\nSo how can you pass a column name to group_by()? Well, there is another version of group_by() called group_by_() that uses standard evaluation. You can learn more about it here. Let’s take a look at what happens when we use group_by_():\n\nsimpleFunction &lt;- function(dataset, col_name){\n  require(\"dplyr\")\n  dataset %&gt;% \n    group\\_by\\_(col_name) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\nsimpleFunction(cars, \"dist\")\nA tibble: 35 x 2\n dist mean\\_speed\n&lt;dbl&gt;      &lt;dbl&gt;\n1      2        4.0\n2      4        7.0\n3     10        6.5\n4     14       12.0\n5     16        8.0\n6     17       11.0\n7     18       10.0\n8     20       13.5\n9     22        7.0\n10    24       12.0\n ... with 25 more rows\n\nWe can even use a formula instead of a string:\n\nsimpleFunction(cars, ~dist)\n A tibble: 35 x 2\n    dist mean_speed\n   &lt;dbl&gt;      &lt;dbl&gt;\n1      2        4.0\n2      4        7.0\n3     10        6.5\n4     14       12.0\n5     16        8.0\n6     17       11.0\n7     18       10.0\n8     20       13.5\n9     22        7.0\n10    24       12.0\n... with 25 more rows\n\nWhat if you want to pass column names and constants, for example to filter without hardcoding anything?\n\n\nTrying to do it naively will only yield pain and despair:\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  require(\"dplyr\")\n  dataset %&gt;% \n    filter\\_(col\\_name == value) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n&gt; simpleFunction(cars, \"dist\", 10)\n\n  mean_speed\n1        NaN\n\n&gt; simpleFunction(cars, dist, 10)\n\n Error in col_name == value : \n  comparison (1) is possible only for atomic and list types \n  \n&gt; simpleFunction(cars, ~dist, 10)\n\n  mean_speed\n1        NaN\n\n\nTo solve this issue, we need to know a little bit about two concepts, lazy evaluation and non-standard evaluation. I recommend you read the following document from Hadley Wickham’s book Advanced R as well as the part on lazy evaluation here.\n\n\nA nice package called lazyeval can help us out. We would like to make R understand that the column name is not col_name but the string inside it \"dist\", and now we would like to use filter() for dist equal to 10.\n\n\nIn the lazyeval package, you’ll find the function interp(). interp() allows you to\n\n\n\nbuild an expression up from a mixture of constants and variables.\n\n\n\nTake a look at this example:\n\nlibrary(lazyeval)\ninterp(~x+y, x = 2)\n## ~2 + y\n\nWhat you get back is this nice formula that you can then use within functions. To see why this is useful, let’s look at the above example again, and make it work using interp():\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  require(\"dplyr\")\n  require(\"lazyeval\")\n  filter\\_criteria &lt;- interp(~y == x, .values=list(y = as.name(col_name), x = value))\n  dataset %&gt;% \n    filter\\_(filter_criteria) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(cars, \"dist\", 10)\n  mean\\_speed\n1        6.5\n\nAnd now it works! For some reason, you have to pass the column name as a string though.\n\n\nSources: apart from the documents above, the following stackoverflow threads helped me out quite a lot: In R: pass column name as argument and use it in function with dplyr::mutate() and lazyeval::interp() and Non-standard evaluation (NSE) in dplyr’s filter_ & pulling data from MySQL."
  },
  {
    "objectID": "posts/2023-05-08-dock_dev_env.html",
    "href": "posts/2023-05-08-dock_dev_env.html",
    "title": "Why you should consider working on a dockerized development environment",
    "section": "",
    "text": "Last year I wrote a post about dockerizing {targets}’s pipelines (link to post) and between that blog post and this one, I’ve written a whole book on building reproducible analytical pipelines that you can read here (for free!). In the book I explain how you can build projects that will stay reproducible thanks to Docker. With Docker, you don’t only ship the code to your project, but ship a whole computer with it, and your project will be executed inside that computer. By whole computer I mean the whole computational environment: so a version of R, the required packages your project depends on, all of it running on a Linux distribution (usually Ubuntu). The whole project can then be executed like any program from your computer (whether you’re running Windows, macOS or Linux) or even on the cloud with a single command.\n\n\nIn this blog post, I’ll discuss something that I’ve been trying for some time now: working directly from a dockerized environment. The idea is to have a Docker image that comes with R, all the usual packages I use for development, Quarto, a LaTeX distribution (that I installed with {tinytex}) and finally, my IDE of choice, Spacemacs (if you use RStudio, just read on, I’ll explain how you can achieve the same thing but with RStudio instead). Why do this? Well because this way I can deploy the exact same environment anywhere. If I get a new computer, I’m only one command line away from a functioning environment. If I want to dockerize a {targets} pipeline, I can write a new Dockerfile that builds upon this image which ensures that there are no discrepancies between the development environment and the production environment. And because I’m building the image on top of a Rocker image, everything just work. If I need to install a package that might be tricky to install (for example, a package that depends on {rJava}, using Docker might be the simplest way to get it to work.\n\n\nSo, here’s the Dockerfile:\n\n# This builds upon the Rocker project's versioned image for R version 4.3\nFROM rocker/r-ver:4.3\n\n# install `gpg-agent` and `software-properties-common` which are needed to add an Ubuntu ppa to install Emacs\nRUN apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n    gpg-agent software-properties-common\n\n# add the ppa which includes the latest version of Emacs\nRUN add-apt-repository ppa:kelleyk/emacs\n\n# Install `git`, `wget` and the latest `Emacs`\nRUN apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n    git \\\n    wget \\\n    emacs28-nox\n\n# Install spacemacs by cloning its repository\nRUN git clone -b develop https://github.com/syl20bnr/spacemacs ~/.emacs.d\n\n# Download my .spacemacs config file\nRUN wget https://raw.githubusercontent.com/b-rodrigues/dotfiles/master/dotfiles/.spacemacs -O ~/.spacemacs\n\n# This launches emacs in daemon mode. This is needed to initialize spacemacs.\n# Running it in daemon mode is required because a Dockerfile must be setup non-interactively\nRUN emacs --daemon\n\n# Install a bunch of Ubuntu dependencies. These are typical dependencies required to use some\n# R packages on Linux.\nRUN apt-get update \\\n   && apt-get install -y --no-install-recommends \\\n   aspell \\\n   aspell-en \\\n   aspell-fr \\\n   aspell-pt-pt \\\n   libfontconfig1-dev \\\n   libglpk-dev \\\n   libxml2-dev \\\n   libcairo2-dev \\\n   libgit2-dev \\\n   default-libmysqlclient-dev \\\n   libpq-dev \\\n   libsasl2-dev \\\n   libsqlite3-dev \\\n   libssh2-1-dev \\\n   libxtst6 \\\n   libcurl4-openssl-dev \\\n   libharfbuzz-dev \\\n   libfribidi-dev \\\n   libfreetype6-dev \\\n   libpng-dev \\\n   libtiff5-dev \\\n   libjpeg-dev \\\n   libxt-dev \\\n   unixodbc-dev \\\n   pandoc\n\n# Download the latest version of Quarto\nRUN wget https://github.com/quarto-dev/quarto-cli/releases/download/v1.3.340/quarto-1.3.340-linux-amd64.deb -O ~/quarto.deb\n\n# Install the latest version of Quarto\nRUN apt-get install --yes ~/quarto.deb\n\n# Remove the installer\nRUN rm ~/quarto.deb\n\n# Create a directory to host my projects\nRUN mkdir /root/projects/\n\n# Write a bunch of lines to the .Rprofile\n# This makes sure that the httpgd server runs on localhost and on the port 8888\nRUN echo 'options(httpgd.host = \"0.0.0.0\", httpgd.port = 8888, httpgd.token = \"aaaaaaaa\")' &gt;&gt; /root/.Rprofile\n\n# This option clones renv cache folders inside the root folder of the projects. This makes\n# sure that they stay persistent across reboots.\nRUN echo 'options(renv.config.cache.symlinks = FALSE)' &gt;&gt; /root/.Rprofile\n\n# Serve shiny apps through localhost and port 8888\nRUN echo 'options(shiny.host = \"0.0.0.0\", shiny.port = 8888)' &gt;&gt; /root/.Rprofile\n\n# Set the CRAN package repositories to the PPPM at the 28th of April\nRUN echo 'options(repos = c(REPO_NAME = \"https://packagemanager.rstudio.com/cran/__linux__/jammy/2023-04-28\"))' &gt;&gt; /root/.Rprofile\n\n# Install the usual packages I use\nRUN R -e \"install.packages(c('quarto', 'remotes', 'tinytex', 'tidyverse', 'arrow', 'chronicler', 'janitor', 'targets', 'tarchetypes', 'openxlsx', 'shiny', 'flexdashboard', 'data.table', 'httpgd', 'blogdown', 'bookdown'))\" \n\n# Install the g2r package (not yet available on CRAN)\nRUN R -e \"remotes::install_github('devOpifex/g2r')\"\n\n# Install a LaTeX distro using tinytex\nRUN R -e \"tinytex::install_tinytex()\"\n\n# Install hugo for blogging\nRUN R -e \"blogdown::install_hugo()\"\n\n# Expose port 8888\nEXPOSE 8888\n\n(and here’s the repository where you can find it).\n\n\nI’ve explained each line of the Dockerfile using comments in the Dockerfile itself. But before explaining it in more detail, here’s a word from this blog post’s sponsor: me, I’m this post’s sponsor.\n\n\nIf you have read until here dear reader, let me express my gratitude by offering you a discount code to purchase a DRM-free Epub and PDF version of my book, Building reproducible analytical pipelines with R (that you can also read for free here by the way). Using the discount code you can get a DRM-free epub and PDF version of the book for 14.99 instead of 19.99! If you want a good old physical book instead, you’ll need to wait some more, I still need to get the formatting right before making it available through Amazon self-publishing service.\n\n\nNow back to our Dockerfile. There are several decisions that I took that I need explain: first, why use a versioned image, and why use the PPPM at a specific date? I did this so that it doesn’t matter when I build the image, I always know which version of R and packages I get. Then, what’s with all the options that I write to the .Rprofile? Well, don’t forget that when I will be running the Docker container defined by this image, I will be using Emacs inside a terminal. So if I want to see plots for example, I need to use the {httpgd} package. This package provides a graphics device that runs on a web server, so if I tell {httpgd} to serve the images over port 8888, and then expose this port in the Dockerfile, I can access {httpgd} from my web browser by pointing it to http://localhost:8888. Here’s how this looks like:\n\n\n\n\n\n\n\nThe terminal on top of the image is running my dockerized environment, and below you see my web browser on to the http://localhost:8888/live?token=aaaaaaaa url to access the {httpgd} web server that is running inside the Docker container. And it’s the same logic with Shiny: if I’m working on a Shiny app from inside the container, I can access it by going to http://localhost:8888/. Now, I have to do all of this because I’m running Emacs, but if you’re developing with RStudio, you could instead run RStudio server, access it on http://localhost:8888/, and then no need to think about configuring on which ports {httpgd} serves images, or on which port Shiny apps should run. Everything will be directly visible from within RStudio. Here is the Dockerfile to run R version 4.3 with RStudio as the IDE. If you want to use this, you could simply start from the above Dockerfile and then add the stuff you need, for example:\n\nFROM rocker/rstudio:4.3.0\n\n# and add what you want below like installing R packages and whatnot\n\nThere is still one important thing that you should know before using a dockerized development environment: a running Docker container can be changed (for example, you could install new R packages), but once you shut it down and restart it, any change will be lost. So how do you save your work? Well, you need to run the Docker image with a volume. A volume is nothing more than a folder on your computer that is linked to a folder inside the Docker container. Anything that gets saved there from the Docker container will be available on your computer, and vice-versa. Here is the command that I use to run my container:\n\ndocker run --rm -it --name ess_dev_env -p 8888:8888 -v /home/path_to_volume/folder:/root/projects:rw brodriguesco/ess_dev_env:main-cdcb1719d emacs\n\nTake note of the -v flag, especially what comes after: /home/path_to_volume/folder:/root/projects:rw. /home/path_to_volume/folder is the folder on my computer, and it is linked to the /root/projects folder inside the Docker container. When I run the above command inside a terminal, Spacemacs starts and I can get to work! If you build a development environment based on RStudio, you would essentially use the same command, you would only need to set a password to login first (read the instructions here).\n\n\nAlso, if you forgot to add a package and want to install it and make this change permanent, the best way is to add it to the Dockerfile and rebuild the image. I’ve streamlined this process by using Github Actions to build images and push them to Docker Hub. Take a look at the Github repository where my Dockerfile is hosted, and if you are familiar with Github Actions, take a look at my workflow file. You’ll see that I’ve set up Github Actions to build the Docker image and push it to Docker Hub each time I commit, and name the Docker image ess_dev_env:main-xxxxx where xxxxx is the corresponding commit hash on Github (so I can easily know which image was built with which commit).\n\n\nI’ll be using this dockerized image for some time, and see how it feels. For now, it works quite well!"
  },
  {
    "objectID": "posts/2022-10-31-optim_shiny.html",
    "href": "posts/2022-10-31-optim_shiny.html",
    "title": "How to deal with annoying medium sized data inside a Shiny app",
    "section": "",
    "text": "This blog post is taken from a chapter of my ebook on building reproducible analytical pipelines, which you can read here\n\n\nIf you want to follow along, you can start by downloading the data I use here. This is a smaller dataset made from the one you can get here.\n\n\nUncompressed it’ll be a 2.4GB file. Not big data in any sense, but big enough to be annoying to handle without the use of some optimization strategies (I’ve seen such data described as medium sized data before.).\n\n\nOne such strategy is only letting the computations run once the user gives the green light by clicking on an action button. The next obvious strategy is to use packages that are optimized for speed. It turns out that the functions we have seen until now (note from the author: the functions we have seen until now if you’re on of my students that’s sitting in the course where I teach this), from packages like {dplyr} and the like, are not the fastest. Their ease of use and expressiveness come at a speed cost. So we will need to switch to something faster. We will do the same to read in the data.\n\n\nThis faster solution is the {arrow} package, which is an interface to the Arrow software developed by Apache.\n\n\nThe final strategy is to enable caching in the app.\n\n\nSo first, install the {arrow} package by running install.packages(“arrow”). This will compile libarrow from source on Linux and might take some time, so perhaps go grab a coffee. On other operating systems, I guess that a binary version gets installed.\n\n\nBefore building the app, let me perform a very simple benchmark. The script below reads in the data, then performs some aggregations. This is done using standard {tidyverse} functions, but also using {arrow}:\n\nstart_tidy &lt;- Sys.time()\n  # {vroom} is able to read in larger files than {readr}\n  # I could not get this file into R using readr::read_csv\n  # my RAM would get maxed out\n  air &lt;- vroom::vroom(\"data/combined\")\n\n  mean_dep_delay &lt;- air |&gt;\n    dplyr::group_by(Year, Month, DayofMonth) |&gt;\n    dplyr::summarise(mean_delay = mean(DepDelay, na.rm = TRUE))\nend_tidy &lt;- Sys.time()\n\ntime_tidy &lt;- end_tidy - start_tidy\n\n\nstart_arrow &lt;- Sys.time()\n  air &lt;- arrow::open_dataset(\"data/combined\", format = \"csv\")\n\n  mean_dep_delay &lt;- air |&gt;\n    dplyr::group_by(Year, Month, DayofMonth) |&gt;\n    dplyr::summarise(mean_delay = mean(DepDelay, na.rm = TRUE))\nend_arrow &lt;- Sys.time()\n\nend_tidy - start_tidy\nend_arrow - start_arrow\n\nThe “tidy” approach took 17 seconds, while the arrow approach took 6 seconds. This is an impressive improvement, but put yourself in the shoes of a user who has to wait 6 seconds for each query. That would get very annoying, very quickly. So the other strategy that we will use is to provide some visual cue that computations are running, and then we will go one step further and use caching of results in the Shiny app.\n\n\nBut before we continue, you may be confused by the code above. After all, I told you before that functions from {dplyr} and the like were not the fastest, and yet, I am using them in the arrow approach as well, and they now run almost 3 times as fast. What’s going on? What’s happening here, is that the air object that we read using arrow::open_dataset is not a dataframe, but an arrow dataset. These are special, and work in a different way. But that’s not what’s important: what’s important is that the {dplyr} api can be used to work with these arrow datasets. This means that functions from {dplyr} change the way they work depending on the type of the object their dealing with. If it’s a good old regular data frame, some C++ code gets called to perform the computations. If it’s an arrow dataset, libarrow and its black magic get called instead to perform the computations. If you’re familiar with the concept of polymorphism this is it (think of + in Python: 1+1 returns 2, “a”+“b” returns “a+b”. A different computation gets performed depending on the type of the function’s inputs).\n\n\nLet’s now build a basic version of the app, only using {arrow} functions for speed. This is the global file:\n\nlibrary(arrow)\nlibrary(dplyr)\nlibrary(rlang)\nlibrary(DT)\n\nair &lt;- arrow::open_dataset(\"data/combined\", format = \"csv\")\n\nThe ui will be quite simple:\n\nui &lt;- function(request){\n  fluidPage(\n\n    titlePanel(\"Air On Time data\"),\n\n    sidebarLayout(\n\n      sidebarPanel(\n        selectizeInput(\"group_by_selected\", \"Variables to group by:\",\n                       choices = c(\"Year\", \"Month\", \"DayofMonth\", \"Origin\", \"Dest\"),\n                       multiple = TRUE,\n                       selected = c(\"Year\", \"Month\"),\n                       options = list(\n                         plugins = list(\"remove_button\"),\n                         create = TRUE,\n                         persist = FALSE # keep created choices in dropdown\n                       )\n                       ),\n        hr(),\n        selectizeInput(\"var_to_average\", \"Select variable to average by groups:\",\n                       choices = c(\"ArrDelay\", \"DepDelay\", \"Distance\"),\n                       multiple = FALSE,\n                       selected = \"DepDelay\",\n                       ),\n        hr(),\n        actionButton(inputId = \"run_aggregation\",\n                     label = \"Click here to run aggregation\"),\n        hr(),\n        bookmarkButton()\n      ),\n\n      mainPanel(\n        DTOutput(\"result\")\n      )\n    )\n  )\n\n}\n\nAnd finally the server:\n\nserver &lt;- function(session, input, output) {\n\n  # Numbers get crunched only when the user clicks on the action button\n  grouped_data &lt;- eventReactive(input$run_aggregation, {\n    air %&gt;%\n      group_by(!!!syms(input$group_by_selected)) %&gt;%\n      summarise(result = mean(!!sym(input$var_to_average),\n                              na.rm = TRUE)) %&gt;%\n      as.data.frame()\n  })\n\n  output$result &lt;- renderDT({\n    grouped_data()\n  })\n\n}\n\nBecause group_by() and mean() expect bare variable names, I convert them from strings to symbols using rlang::syms() and rlang::sym(). The difference between the two is that rlang::syms() is required when a list of strings gets passed down to the function (remember that the user must select several variables to group by), and this is also why !!! are needed (to unquote the list of symbols). Finally, the computed data must be converted back to a data frame using as.data.frame(). This is actually when the computations happen. {arrow} collects all the aggregations but does not perform anything until absolutely required. Let’s see the app in action:\n\n\n\n\n\n\n\nAs you can see, in terms of User Experience (UX) this is quite poor. When the user clicks on the button nothing seems to be going on for several seconds, until the table appears. Then, when the user changes some options and clicks again on the action button, it looks like the app is crashing.\n\n\nLet’s add some visual cues to indicate to the user that something is happening when the button gets clicked. For this, we are going to use the {shinycssloaders} package:\n\ninstall.packages(\"shinycssloaders\")\n\nand simply change the ui to this (and don’t forget to load {shinycssloaders} in the global script!):\n\nui &lt;- function(request){\n  fluidPage(\n\n    titlePanel(\"Air On Time data\"),\n\n    sidebarLayout(\n\n      sidebarPanel(\n        selectizeInput(\"group_by_selected\", \"Variables to group by:\",\n                       choices = c(\"Year\", \"Month\", \"DayofMonth\", \"Origin\", \"Dest\"),\n                       multiple = TRUE,\n                       selected = c(\"Year\", \"Month\"),\n                       options = list(\n                         plugins = list(\"remove_button\"),\n                         create = TRUE,\n                         persist = FALSE # keep created choices in dropdown\n                       )\n                       ),\n        hr(),\n        selectizeInput(\"var_to_average\", \"Select variable to average by groups:\",\n                       choices = c(\"ArrDelay\", \"DepDelay\", \"Distance\"),\n                       multiple = FALSE,\n                       selected = \"DepDelay\",\n                       ),\n        hr(),\n        actionButton(inputId = \"run_aggregation\",\n                     label = \"Click here to run aggregation\"),\n        hr(),\n        bookmarkButton()\n      ),\n\n      mainPanel(\n        # We add a tabsetPanel with two tabs. The first tab show the plot made using ggplot\n        # the second tab shows the plot using g2r\n        DTOutput(\"result\") |&gt;\n          withSpinner()\n      )\n    )\n  )\n\n}\n\nThe only difference with before is that now the DTOutput() right at the end gets passed down to withSpinner(). There are several spinners that you can choose, but let’s simply use the default one. This is how the app looks now:\n\n\n\n\n\n\n\nNow the user gets a visual cue that something is happening. This makes waiting more bearable, but even better than waiting with a spinner is no waiting at all. For this, we are going to enable caching of results. There are several ways that you can cache results inside your app. You can enable the cache on a per-user and per-session basis, or only on a per-user basis. But I think that in our case here, the ideal caching strategy is to keep the cache persistent, and available across sessions. This means that each computation done by any user will get cached and available to any other user. In order to achieve this, you simply have to install the {cachem} packages add the following lines to the global script:\n\nshinyOptions(cache = cachem::cache_disk(\"./app-cache\",\n                                        max_age = Inf))\n\nBy setting the max_age argument to Inf, the cache will never get pruned. The maximum size of the cache, by default is 1GB. You can of course increase it.\n\n\nNow, you must also edit the server file like so:\n\nserver &lt;- function(session, input, output) {\n\n  # Numbers get crunched only when the user clicks on the action button\n  grouped_data &lt;- reactive({\n    air %&gt;%\n      group_by(!!!syms(input$group_by_selected)) %&gt;%\n      summarise(result = mean(!!sym(input$var_to_average),\n                              na.rm = TRUE)) %&gt;%\n      as.data.frame()\n  }) %&gt;%\n    bindCache(input$group_by_selected,\n              input$var_to_average) %&gt;%\n    bindEvent(input$run_aggregation)\n\n  output$result &lt;- renderDT({\n    grouped_data()\n  })\n\n}\n\nWe’ve had to change eventReactive() to reactive(), just like in the app where we don’t use an action button to run computations (note of the author: in the ebook, there is an example of an app with this action button. This is what I’m referring to here). Then, we pass the reactive object to bindCache(). bindCache() also takes the inputs as arguments. These are used to generate cache keys to retrieve the correct objects from cache. Finally, we pass all this to bindEvent(). This function takes the input referencing the action button. This is how we can now bind the computations to the button once again. Let’s test our app now. You will notice that the first time we choose certain options, the computations will take time, as before. But if we perform the same computations again, then the results will be shown instantly:\n\n\n\n\n\n\n\nAs you can see, once I go back to a computation that was done in the past, the table appears instantly. At the end of the video I open a terminal and navigate to the directory of the app, and show you the cache. There are several .Rds objects, these are the final data frames that get computed by the app. If the user wants to rerun a previous computation, the correct data frame gets retrieved, making it look like the computation happened instantly, and with another added benefit: as discussed above, the cache is persistent between sessions, so even if the user closes the browser and comes back later, the cache is still there, and other users will also benefit from the cache."
  },
  {
    "objectID": "posts/2018-04-10-brotools_describe.html",
    "href": "posts/2018-04-10-brotools_describe.html",
    "title": "Get basic summary statistics for all the variables in a data frame",
    "section": "",
    "text": "I have added a new function to my {brotools} package, called describe(), which takes a data frame as an argument, and returns another data frame with descriptive statistics. It is very much inspired by the {skmir} package but also by assist::describe() (click on the packages to be redirected to the respective Github repos) but I wanted to write my own for two reasons: first, as an exercice, and second I really only needed the function skim_to_wide() from {skimr}. So instead of installing a whole package for a single function, I decided to write my own (since I use {brotools} daily).\n\n\nBelow you can see it in action:\n\nlibrary(dplyr)\ndata(starwars)\nbrotools::describe(starwars)\n## # A tibble: 10 x 13\n##    variable  type   nobs  mean    sd mode     min   max   q25 median   q75\n##    &lt;chr&gt;     &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n##  1 birth_ye… Nume…    87  87.6 155.  19         8   896  35       52  72  \n##  2 height    Nume…    87 174.   34.8 172       66   264 167      180 191  \n##  3 mass      Nume…    87  97.3 169.  77        15  1358  55.6     79  84.5\n##  4 eye_color Char…    87  NA    NA   blue      NA    NA  NA       NA  NA  \n##  5 gender    Char…    87  NA    NA   male      NA    NA  NA       NA  NA  \n##  6 hair_col… Char…    87  NA    NA   blond     NA    NA  NA       NA  NA  \n##  7 homeworld Char…    87  NA    NA   Tatoo…    NA    NA  NA       NA  NA  \n##  8 name      Char…    87  NA    NA   Luke …    NA    NA  NA       NA  NA  \n##  9 skin_col… Char…    87  NA    NA   fair      NA    NA  NA       NA  NA  \n## 10 species   Char…    87  NA    NA   Human     NA    NA  NA       NA  NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n\nAs you can see, the object that is returned by describe() is a tibble.\n\n\nFor now, this function does not handle dates, but it’s in the pipeline.\n\n\nYou can also only describe certain columns:\n\nbrotools::describe(starwars, height, mass, name)\n## # A tibble: 3 x 13\n##   variable type    nobs  mean    sd mode      min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 height   Numer…    87 174.   34.8 172        66   264 167      180 191  \n## 2 mass     Numer…    87  97.3 169.  77         15  1358  55.6     79  84.5\n## 3 name     Chara…    87  NA    NA   Luke S…    NA    NA  NA       NA  NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n\nIf you want to try it out, you can install {brotools} from Github:\n\ndevtools::install_github(\"b-rodrigues/brotools\")\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2022-05-26-safer_programs.html#learning-number-1-make-functions-fail-early",
    "href": "posts/2022-05-26-safer_programs.html#learning-number-1-make-functions-fail-early",
    "title": "Some learnings from functional programming you can use to write safer programs",
    "section": "\nLearning number 1: make functions fail early\n",
    "text": "Learning number 1: make functions fail early\n\n\nWhen writing your own functions, avoid conversion of types without warning. For example, this function only works on characters:\n\nmy_nchar &lt;- function(x, result = 0){\n\n  if(x == \"\"){\n    result\n  } else {\n    result &lt;- result + 1\n    split_x &lt;- strsplit(x, split = \"\")[[1]]\n    my_nchar(paste0(split_x[-1],\n                    collapse = \"\"), result)\n  }\n\n}\nmy_nchar(\"100000000\")\n## [1] 9\nmy_nchar(100000000)\nError in strsplit(x, split = \"\") : non-character argument\n\nIt may tempting to write functions that accept a lot of different types of inputs, because it seems convenient and you’re a lazy ding-dong:\n\nmy_nchar2 &lt;- function(x, result = 0){\n\n  # What could go wrong?\n  x &lt;- as.character(x)\n\n  if(x == \"\"){\n    result\n  } else {\n    result &lt;- result + 1\n    split_x &lt;- strsplit(x, split = \"\")[[1]]\n    my_nchar2(paste0(split_x[-1],\n                    collapse = \"\"), result)\n  }\n\n}\n\nYou should avoid doing this, because this can have unforseen consequences:\n\nmy_nchar2(10000000)\n## [1] 5\n\nIf you think that this example is far-fetched, you’d be surprised to learn that this is exactly what nchar(), the built-in function to count characters, does:\n\nnchar(\"10000000\")\n## [1] 8\n\nto this:\n\nnchar(10000000)\n## [1] 5\n\n(thanks to @cararthompson for pointing this out on twitter)\n\n\nYou can also add guards to be extra safe:\n\nmy_nchar2 &lt;- function(x, result = 0){\n\n  if(!isTRUE(is.character(x))){\n    stop(paste0(\"x should be of type 'character', but is of type '\",\n                typeof(x), \"' instead.\"))\n  } else if(x == \"\"){\n    result\n  } else {\n    result &lt;- result + 1\n    split_x &lt;- strsplit(x, split = \"\")[[1]]\n    my_nchar2(paste0(split_x[-1],\n                     collapse = \"\"), result)\n  }\n}\nmy_nchar2(\"10000000\")\n## [1] 8\n\ncompare to this:\n\nmy_nchar2(10000000)\nError in my_nchar2(1000):\nx should be of type 'character', but is of type 'double' instead.\n\nNow this doesn’t really help here, because our function is already safe (it only handles characters, since strsplit() only handles characters), but in other situations this could be helpful (and at least we customized the error message). Since it can be quite tedious to write all these if…else… statements, you might want to take a look at purrr::safely() (and purrr::possibly()), the {maybe} package, or the {typed} package, or even my package for that matter."
  },
  {
    "objectID": "posts/2022-05-26-safer_programs.html#learning-number-2-make-your-functions-referentially-transparent-and-as-pure-as-possible",
    "href": "posts/2022-05-26-safer_programs.html#learning-number-2-make-your-functions-referentially-transparent-and-as-pure-as-possible",
    "title": "Some learnings from functional programming you can use to write safer programs",
    "section": "\nLearning number 2: Make your functions referentially transparent (and as pure as possible)\n",
    "text": "Learning number 2: Make your functions referentially transparent (and as pure as possible)\n\n\nAny variable used by a function should be one of its parameters. Don’t do this:\n\nf &lt;- function(x){\n  x + y\n}\n\nThis function has only one parameter, x, and so depends on y outside of this scope. This function is unpredictable, because the result it provides depends on the value of y.\n\n\nSee what happens:\n\nf(10)\n## [1] 20\nf(10)\n## [1] 10\n\nI called f twice with 10 and got two results (because I changed the value of y without showing you). In very long scripts, having functions like this depending on values in the global environment is a recipe for disaster. It’s better to make this function referentially transparent; some very complicated words to describe a very simple concept:\n\nf &lt;- function(x, y){\n  x + y\n}\n\nJust give f a second parameter, and you’re good to go.\n\n\nSomething else your functions shouldn’t do is changing stuff outside of its scope:\n\nf &lt;- function(x, y){\n  result &lt;&lt;- x + y\n}\n\nLet’s take a look at variables in global environment before calling f:\n\nls()\n## [1] \"f\"         \"my_nchar\"  \"my_nchar2\" \"view\"      \"view_xl\"   \"y\"\n\nNow let’s call it:\n\nf(1, 2)\n\nAnd let’s have a good look at the global environment again:\n\nls()\n## [1] \"f\"         \"my_nchar\"  \"my_nchar2\" \"result\"    \"view\"      \"view_xl\"  \n## [7] \"y\"\n\nWe now see that result has been defined in the global environment:\n\nresult\n## [1] 3\n\nJust like before, if your functions change stuff outside their scope, this is a recipe for disaster. You have to be very careful and know exactly what you’re doing if you want to use &lt;&lt;-.\n\n\nSo it’s better to write your function like this, and call it like this:\n\nf &lt;- function(x, y){\n  x + y\n}\n\nresult &lt;- f(1, 2)"
  },
  {
    "objectID": "posts/2022-05-26-safer_programs.html#learning-number-3-make-your-functions-do-one-thing",
    "href": "posts/2022-05-26-safer_programs.html#learning-number-3-make-your-functions-do-one-thing",
    "title": "Some learnings from functional programming you can use to write safer programs",
    "section": "\nLearning number 3: make your functions do one thing\n",
    "text": "Learning number 3: make your functions do one thing\n\n\nTry to write small functions that do just one thing. This make them easier to document, test and simply wrap your head around. You can then pipe your function one after the other to get stuff done:\n\na |&gt;\n  f() |&gt;\n  g() |&gt;\n  h()\n\nYou have of course to make sure that the output of f() is of the correct type, so that g() then knows how to handle it. In some cases, you really need a function to do several things to get the output you want. In that case, still write small functions to handle every aspect of the whole algorithm, and then write a function that calls each function. And if needed, you can even provide functions as arguments to other functions:\n\nh &lt;- function(x, y, f, g){\n  f(x) + g(y)\n}\n\nThis makes h() a higher-order function."
  },
  {
    "objectID": "posts/2022-05-26-safer_programs.html#learning-number-4-use-higher-order-functions-to-abstract-loops-away",
    "href": "posts/2022-05-26-safer_programs.html#learning-number-4-use-higher-order-functions-to-abstract-loops-away",
    "title": "Some learnings from functional programming you can use to write safer programs",
    "section": "\nLearning number 4: use higher-order functions to abstract loops away\n",
    "text": "Learning number 4: use higher-order functions to abstract loops away\n\n\nLoops are hard to write. Higher order function are really cool though:\n\nReduce(`+`, seq(1:100))\n## [1] 5050\n\nReduce() is a higher-order function that takes a function (here +) and a list of inputs compatible with the function. So Reduce() performs this operation:\n\nReduce(`+`, seq(1:100))\n\n100 + Reduce(`+`, seq(2:100))\n100 + 99 + Reduce(`+`, seq(3:100))\n100 + 99 + 98 + Reduce(`+`, seq(4:100))\n\nThis avoids having to write a loop, which can go wrong for many reasons (typos, checking input types, depending on variables outside the global environment… basically anything I mentioned already).\n\n\nThere’s also purrr::reduce() if you prefer the tidyverse ecosystem. Higher-order functions are super flexible; all that matters is that the function you give to reduce() knows what the do with the elements in the list.\n\n\nAnother higher-order function you should know about is purrr::map() (or lapply() if your prefer base functions):\n\npurrr::map(list(mtcars, iris), nrow)\n## [[1]]\n## [1] 32\n## \n## [[2]]\n## [1] 150\n\nThis loops a function (here nrow()) over a list of whatevers (here data frames). Super flexible once again."
  },
  {
    "objectID": "posts/2022-05-26-safer_programs.html#optional-learning-number-5-use-recursion-to-avoid-loops-further",
    "href": "posts/2022-05-26-safer_programs.html#optional-learning-number-5-use-recursion-to-avoid-loops-further",
    "title": "Some learnings from functional programming you can use to write safer programs",
    "section": "\n(Optional) Learning number 5: use recursion to avoid loops further\n",
    "text": "(Optional) Learning number 5: use recursion to avoid loops further\n\n\nThe following function calls itself and reverses a string:\n\nrev_char &lt;- function(x){\n\n  try({\n    if(x == \"\"){\n      \"\"\n    } else {\n      split_x &lt;- strsplit(x, split = \"\")[[1]]\n\n      len_x &lt;- length(split_x)\n\n      paste0(split_x[len_x],\n             rev_char(paste0(split_x[1:len_x-1],\n                             collapse = \"\")))\n    }\n  }, stop(paste0(\"x should be of type 'character', but is of type '\",\n                 typeof(x), \"' instead.\")))\n\n}\n\nrev_char(\"abc\")\n## [1] \"cba\"\n\nI say that this is optional, because while it might sometimes be easier to use recursion to define a functions, this is not always the case, and (in the case of R) runs slower than using a loop. If you’re interested in learning more about map() and reduce(), I wrote several blog posts on it here, here and here and some youtube videos as well:\n\n\n\nhttps://www.youtube.com/watch?v=3xIKZbZKCWQ\n\n\nhttps://www.youtube.com/watch?v=WjtXc4OXZuk\n\n\nhttps://www.youtube.com/watch?v=vxaKamox_CQ\n\n\nhttps://www.youtube.com/watch?v=H3ao7LzcvW8\n\n\nhttps://www.youtube.com/watch?v=vtxb1j0aqJM\n\n\nhttps://www.youtube.com/watch?v=F2U-l3IcCtc\n\n\nhttps://www.youtube.com/watch?v=gVW9KfkJIrQ\n\n\nhttps://www.youtube.com/watch?v=FanU60pjmt0\n\n\nhttps://www.youtube.com/watch?v=DERMZi3Ck20"
  },
  {
    "objectID": "posts/2017-01-07-my-free-book-has-a-cover.html",
    "href": "posts/2017-01-07-my-free-book-has-a-cover.html",
    "title": "My free book has a cover!",
    "section": "",
    "text": "I’m currently writing a book as a hobby. It’s titled Functional programming and unit testing for data munging with R and you can get it for free here. You can also read it online for free on my webpage What’s the book about?\nHere’s the teaser text:\n\nLearn the basics of functional programming, unit testing and package development for the R programming language in order to make your data tidy!\n\nThe book now has a beautiful cover thanks to @putosaure. Putosaure is a Paris based graphic designer who also reviews video games. He is also a very good friend of mine and I am very happy he made this beautiful cover for my book:\n\n\n\nIn it, we see a guy holding a shield with the Greek letter lambda, which also happens to be the letter to designate functional programming. I’ve added the title with the Komika Title font.\nConsider this cover in beta, it’ll probably evolve some more. But I couldn’t wait to use it!\nI love it. Hope you’ll love it too!"
  },
  {
    "objectID": "posts/2020-09-27-golemDemo.html",
    "href": "posts/2020-09-27-golemDemo.html",
    "title": "Building apps with {shinipsum} and {golem}",
    "section": "",
    "text": "In my previous blog post I showed you how I set up my own Shiny server using a Raspberry Pi 4B. If you visited the following link you’ll be connecting to my Raspberry Pi and can play around with a Shiny app that I called golemDemo. It’s been quite a few months that I wanted to discuss this app:\n\n{{% tweet “1277671383573704706” %}}\n\nSo the tweet mentions that a video was coming in the following week and you’ll notice that the tweet was made on… June 29th, and still no video. As I said in my previous blog post, I’ve been busy. Anyways, here’s already a blog post, and I might still do a video where I’ll go into greater detail. I think that videos are quite nice to walk an audience through an app, but it works best with an accompanying blog post where I can comment some more complicated snippets of code.\n\n\n\nWhy {golem}?\n\n\nWhy should you consider the {golem} package to develop your Shiny apps? For me, there are two main reasons. First of all, I’m already familiar with package development in R, having made some little packages that I have on my Github account, and one out on CRAN (with the complete texts of Luxembourguish author Michel Rodange) so using {golem} came at no additional costs. This is because a Shiny app built with {golem} is actually an R package! This has many advantages; all the steps of documenting, testing and sharing the app are greatly simplified. Another reason to use {golem} is that it forces on you a certain way of working. Now this might seem like a pretty bad thing, but I find that it is quite helpful. When you start working on a Shiny app, you might get very quickly overwhelmed with both thinking about your server logic and your UI. You might spend much time tinkering with getting the server functions working, while still not having no UI to speak of, or you might work on one part of the server and then go to the UI, then back on the server… You’ll spend hours working on the app without a clear approach, and probably waste much time because of this back and forth. The first recommended step when building a shiny app (with or without {golem}) is a “UI first” approach. For this, we’re going to use {shinipsum}\n\n\n\n\nLorem ipsum dolor server amet (it’s Latin for “don’t bother with the server logic until it’s time”)\n\n\nThe developers of {golem}, French company ThinkR suggest an “UI” first approach. The idea is to focus on the UI, and to do so using their other package called {shinipsum} to randomly generate elements on the server side which you can then later replace with your actual server logic. For instance, imagine that somewhere on your app, you want to show a bar plot using the {ggplot2} package. Using {shinipsum}, you can generate a random bar plot with the following line:\n\nshinipsum::random_ggplot(\"bar\")\n\n\n\n\nand that’s it! Now simply ignore this bit on the server, and continue focusing on the UI. You need to show a random table? No problem:\n\nshinipsum::random_table(ncol = 7, nrow = 10)\n##    conc rate   state conc.1 rate.1 state.1 conc.2\n## 1  0.02   76 treated   0.02     76 treated   0.02\n## 2  0.02   47 treated   0.02     47 treated   0.02\n## 3  0.06   97 treated   0.06     97 treated   0.06\n## 4  0.06  107 treated   0.06    107 treated   0.06\n## 5  0.11  123 treated   0.11    123 treated   0.11\n## 6  0.11  139 treated   0.11    139 treated   0.11\n## 7  0.22  159 treated   0.22    159 treated   0.22\n## 8  0.22  152 treated   0.22    152 treated   0.22\n## 9  0.56  191 treated   0.56    191 treated   0.56\n## 10 0.56  201 treated   0.56    201 treated   0.56\n\nYour app might now look something like this (actually, it won’t because the little demo below is not a {golem} app, but it illustrates {shinipsum} well):\n\nlibrary(shiny)\nlibrary(reactable)\nlibrary(shinipsum)\nlibrary(ggiraph)\n\nui &lt;- pageWithSidebar(\n  \n  headerPanel(\"This is a shinipsum demo\"),\n  \n  sidebarPanel(\n    sliderInput(\"rows\",\n                \"Number of rows:\",\n                min = 1,\n                max = 50,\n                value = 5)\n  ),\n  \n  mainPanel(\n    reactableOutput(\"table\"),\n    girafeOutput(\"graph\")\n  )\n)\n\n\nserver &lt;- function(input, output) {\n\n  output$table &lt;- renderReactable({\n    reactable(random_table(ncol = 10, nrow = input$rows))\n  })\n\n  output$graph &lt;- renderGirafe({\n    girafe(ggobj = random_ggplot(\"bar\"))\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\nIf you have the required packages, running this on a fresh R session should start a little app.\n\n\nYou see that the server is only a call to shinipsum::random_table, and shinipsum::random_ggplot. Because I want a reactable and an interactive plot using the {ggiraph} package, I have already written the minimum amount of code on the server side to get things working. Now I can focus on my UI and then, when I’m done, I can start replacing the random objects from {shinipsum} with the actual code.\n\n\nNow proceeding in this way is not a requirement of {golem}, but it helps to structure your thoughts and your app, and you can use this approach for any type of app. The example above, after all, is not a {golem} app.\n\n\n\n\nGet modular with {golem}\n\n\nThis is now where we get to some more interesting, and {golem} specific things. If you’ve been using R and Shiny for the past years, you’ve probably have heard a lot about functional programming. Functional programming is a programming paradigm that encourages, and in some languages forces, the use of functions. The idea is that everything you do should be a call to a function, and functions should be chained together to achieve whatever it is you want to do; cleaning data, visualizing data, modeling data… R has many functional tools out of the box, which can be complemented using the {purrr} package. What does all of this have to do with Shiny and {golem}? Well, {golem} forces you to write modules to build your apps, and modules are very similar to functions (they’re actually functions). They’re bits of code that can be decoupled from your app, used in any other app, they can be linked together, they can be easily documented and tested… If you are familiar with R’s functional programming approach, modules should not be totally new to you. But if you’ve been using Shiny without module, they’ll require some getting used to.\n\n\nTo illustrate how a simple app can be written using modules, I have built golemDemo, which, as implied by its name, is a demonstration of a {golem} app which I hope is simple enough for anyone to start using. The app is quite simple and does only three things:\n\n\n\nit allows you to choose between two datasets;\n\n\nit shows a table of the selected dataset;\n\n\nit shows a map of Luxembourg with the data points;\n\n\n\nEach of these things is a module, which means that if I were to create another app with a map of Luxembourg, I could simply reuse it. But remember, the app is actually an R package. Here is the root of the app on my computer:\n\nsystem2(\"ls\", args = \"-lFR ~/Documents/golemDemo\", stdout = TRUE)\n##  [1] \"/home/cbrunos/Documents/golemDemo:\"                                    \n##  [2] \"total 56\"                                                              \n##  [3] \"-rw-r--r-- 1 cbrunos users  302 Sep 19 11:28 app.R\"                    \n##  [4] \"drwxr-xr-x 2 cbrunos users 4096 Jun 29 17:49 data-raw/\"                \n##  [5] \"-rw-r--r-- 1 cbrunos users  729 Sep 19 21:27 DESCRIPTION\"              \n##  [6] \"drwxr-xr-x 2 cbrunos users 4096 Sep 11 23:39 dev/\"                     \n##  [7] \"-rw-r--r-- 1 cbrunos users 2723 Sep 12 15:04 Dockerfile\"               \n##  [8] \"drwxr-xr-x 3 cbrunos users 4096 Jun 28 11:33 inst/\"                    \n##  [9] \"-rw-r--r-- 1 cbrunos users  483 Apr  8 21:38 LICENSE.md\"               \n## [10] \"drwxr-xr-x 2 cbrunos users 4096 Sep 19 21:27 man/\"                     \n## [11] \"-rw-r--r-- 1 cbrunos users 1420 Sep 19 21:27 NAMESPACE\"                \n## [12] \"drwxr-xr-x 2 cbrunos users 4096 Sep 19 21:27 R/\"                       \n## [13] \"-rw-r--r-- 1 cbrunos users 1056 Jun 28 11:38 README.Rmd\"               \n## [14] \"drwxr-xr-x 3 cbrunos users 4096 Sep 11 17:12 rsconnect/\"               \n## [15] \"drwxr-xr-x 3 cbrunos users 4096 Jun 28 11:48 tests/\"                   \n## [16] \"drwxr-xr-x 2 cbrunos users 4096 Jun 28 11:48 vignettes/\"               \n## [17] \"\"                                                                      \n## [18] \"/home/cbrunos/Documents/golemDemo/data-raw:\"                           \n## [19] \"total 1168\"                                                            \n## [20] \"-rw-r--r-- 1 cbrunos users 1176106 Jun 11 09:52 communes_df.csv\"       \n## [21] \"-rw-r--r-- 1 cbrunos users      99 Jun 28 11:48 my_dataset.R\"          \n## [22] \"-rw-r--r-- 1 cbrunos users    1998 Jun 28 17:00 radars.csv\"            \n## [23] \"-rw-r--r-- 1 cbrunos users    6390 Jun 28 12:31 rettungspunkte.csv\"    \n## [24] \"\"                                                                      \n## [25] \"/home/cbrunos/Documents/golemDemo/dev:\"                                \n## [26] \"total 16\"                                                              \n## [27] \"-rw-r--r-- 1 cbrunos users 1935 Jun 28 11:33 01_start.R\"               \n## [28] \"-rw-r--r-- 1 cbrunos users 2011 Sep 11 23:39 02_dev.R\"                 \n## [29] \"-rw-r--r-- 1 cbrunos users 1012 Jun 28 11:33 03_deploy.R\"              \n## [30] \"-rw-r--r-- 1 cbrunos users  318 Jun 28 11:33 run_dev.R\"                \n## [31] \"\"                                                                      \n## [32] \"/home/cbrunos/Documents/golemDemo/inst:\"                               \n## [33] \"total 8\"                                                               \n## [34] \"drwxr-xr-x 3 cbrunos users 4096 Jun 28 11:33 app/\"                     \n## [35] \"-rw-r--r-- 1 cbrunos users  140 Jun 28 11:38 golem-config.yml\"         \n## [36] \"\"                                                                      \n## [37] \"/home/cbrunos/Documents/golemDemo/inst/app:\"                           \n## [38] \"total 4\"                                                               \n## [39] \"drwxr-xr-x 2 cbrunos users 4096 Jun 28 11:48 www/\"                     \n## [40] \"\"                                                                      \n## [41] \"/home/cbrunos/Documents/golemDemo/inst/app/www:\"                       \n## [42] \"total 12\"                                                              \n## [43] \"-rw-r--r-- 1 cbrunos users    0 Jun 28 11:48 custom.css\"               \n## [44] \"-rw-r--r-- 1 cbrunos users 3774 Jun 28 11:33 favicon.ico\"              \n## [45] \"-rw-r--r-- 1 cbrunos users  100 Jun 28 11:48 handlers.js\"              \n## [46] \"-rw-r--r-- 1 cbrunos users   40 Jun 28 11:48 script.js\"                \n## [47] \"\"                                                                      \n## [48] \"/home/cbrunos/Documents/golemDemo/man:\"                                \n## [49] \"total 8\"                                                               \n## [50] \"-rw-r--r-- 1 cbrunos users 261 Sep 19 21:27 pipe.Rd\"                   \n## [51] \"-rw-r--r-- 1 cbrunos users 291 Jun 28 11:33 run_app.Rd\"                \n## [52] \"\"                                                                      \n## [53] \"/home/cbrunos/Documents/golemDemo/R:\"                                  \n## [54] \"total 48\"                                                              \n## [55] \"-rw-r--r-- 1 cbrunos users  783 Jun 28 11:33 app_config.R\"             \n## [56] \"-rw-r--r-- 1 cbrunos users  654 Jun 29 18:34 app_server.R\"             \n## [57] \"-rw-r--r-- 1 cbrunos users 1790 Sep 12 15:00 app_ui.R\"                 \n## [58] \"-rw-r--r-- 1 cbrunos users    0 Jun 28 11:48 fct_helpers.R\"            \n## [59] \"-rw-rw-r-- 1 cbrunos users  997 Jun 28 11:38 golem_utils_server.R\"     \n## [60] \"-rw-rw-r-- 1 cbrunos users 5849 Jun 28 11:38 golem_utils_ui.R\"         \n## [61] \"-rw-r--r-- 1 cbrunos users  549 Jun 28 11:48 mod_filter_data.R\"        \n## [62] \"-rw-r--r-- 1 cbrunos users 3118 Sep 19 11:16 mod_load_data.R\"          \n## [63] \"-rw-r--r-- 1 cbrunos users 2088 Jun 29 18:30 mod_map_data.R\"           \n## [64] \"-rw-r--r-- 1 cbrunos users  910 Jun 29 18:17 mod_table_data.R\"         \n## [65] \"-rw-r--r-- 1 cbrunos users  337 Jun 28 11:33 run_app.R\"                \n## [66] \"-rw-r--r-- 1 cbrunos users    0 Jun 28 11:48 utils_helpers.R\"          \n## [67] \"-rw-r--r-- 1 cbrunos users  207 Sep 19 21:27 utils-pipe.R\"             \n## [68] \"\"                                                                      \n## [69] \"/home/cbrunos/Documents/golemDemo/rsconnect:\"                          \n## [70] \"total 4\"                                                               \n## [71] \"drwxr-xr-x 3 cbrunos users 4096 Sep 11 17:12 shinyapps.io/\"            \n## [72] \"\"                                                                      \n## [73] \"/home/cbrunos/Documents/golemDemo/rsconnect/shinyapps.io:\"             \n## [74] \"total 4\"                                                               \n## [75] \"drwxr-xr-x 2 cbrunos users 4096 Sep 11 17:12 brodriguesco/\"            \n## [76] \"\"                                                                      \n## [77] \"/home/cbrunos/Documents/golemDemo/rsconnect/shinyapps.io/brodriguesco:\"\n## [78] \"total 4\"                                                               \n## [79] \"-rw-r--r-- 1 cbrunos users 219 Sep 19 21:30 golemdemo.dcf\"             \n## [80] \"\"                                                                      \n## [81] \"/home/cbrunos/Documents/golemDemo/tests:\"                              \n## [82] \"total 8\"                                                               \n## [83] \"drwxr-xr-x 2 cbrunos users 4096 Jun 28 11:48 testthat/\"                \n## [84] \"-rw-r--r-- 1 cbrunos users   62 Jun 28 11:48 testthat.R\"               \n## [85] \"\"                                                                      \n## [86] \"/home/cbrunos/Documents/golemDemo/tests/testthat:\"                     \n## [87] \"total 4\"                                                               \n## [88] \"-rw-r--r-- 1 cbrunos users 64 Jun 28 11:48 test-app.R\"                 \n## [89] \"\"                                                                      \n## [90] \"/home/cbrunos/Documents/golemDemo/vignettes:\"                          \n## [91] \"total 4\"                                                               \n## [92] \"-rw-r--r-- 1 cbrunos users 298 Jun 28 11:48 golemDemo.Rmd\"\n\nThe first 16 lines show the root of the folder, and then we see what’s inside each subfolder, starting with data-raw/, then dev/ etc (this is done via a call to the ls -lFR Linux command, invoked here with R’s system2() function).\n\n\nIf you’ve already developed a package in the past, you’ll recognize the structure. What’s important here is the dev/ folder, which is {golem} specific. This folder contains for files, 01_start.R, 02_dev.R, 03_deploy.R and run_dev.R. These files are the ones that will help you develop your shiny app and you should follow the instructions contained in each of them. Let’s take a look at 01_start.R:\n\nsystem2(\"cat\", args = \"~/Documents/golemDemo/dev/01_start.R\", stdout = TRUE)\n##  [1] \"# Building a Prod-Ready, Robust Shiny Application.\"                                     \n##  [2] \"# \"                                                                                     \n##  [3] \"# README: each step of the dev files is optional, and you don't have to \"               \n##  [4] \"# fill every dev scripts before getting started. \"                                      \n##  [5] \"# 01_start.R should be filled at start. \"                                               \n##  [6] \"# 02_dev.R should be used to keep track of your development during the project.\"        \n##  [7] \"# 03_deploy.R should be used once you need to deploy your app.\"                         \n##  [8] \"# \"                                                                                     \n##  [9] \"# \"                                                                                     \n## [10] \"########################################\"                                               \n## [11] \"#### CURRENT FILE: ON START SCRIPT #####\"                                               \n## [12] \"########################################\"                                               \n## [13] \"\"                                                                                       \n## [14] \"## Fill the DESCRIPTION ----\"                                                           \n## [15] \"## Add meta data about your application\"                                                \n## [16] \"golem::fill_desc(\"                                                                      \n## [17] \"  pkg_name = \\\"golemDemo\\\", # The Name of the package containing the App \"              \n## [18] \"  pkg_title = \\\"PKG_TITLE\\\", # The Title of the package containing the App \"            \n## [19] \"  pkg_description = \\\"PKG_DESC.\\\", # The Description of the package containing the App \"\n## [20] \"  author_first_name = \\\"AUTHOR_FIRST\\\", # Your First Name\"                              \n## [21] \"  author_last_name = \\\"AUTHOR_LAST\\\", # Your Last Name\"                                 \n## [22] \"  author_email = \\\"AUTHOR@MAIL.COM\\\", # Your Email\"                                     \n## [23] \"  repo_url = NULL # The URL of the GitHub Repo (optional) \"                             \n## [24] \")     \"                                                                                 \n## [25] \"\"                                                                                       \n## [26] \"## Set {golem} options ----\"                                                            \n## [27] \"golem::set_golem_options()\"                                                             \n## [28] \"\"                                                                                       \n## [29] \"## Create Common Files ----\"                                                            \n## [30] \"## See ?usethis for more information\"                                                   \n## [31] \"usethis::use_mit_license( name = \\\"Golem User\\\" )  # You can set another license here\"  \n## [32] \"usethis::use_readme_rmd( open = FALSE )\"                                                \n## [33] \"usethis::use_code_of_conduct()\"                                                         \n## [34] \"usethis::use_lifecycle_badge( \\\"Experimental\\\" )\"                                       \n## [35] \"usethis::use_news_md( open = FALSE )\"                                                   \n## [36] \"\"                                                                                       \n## [37] \"## Use git ----\"                                                                        \n## [38] \"usethis::use_git()\"                                                                     \n## [39] \"\"                                                                                       \n## [40] \"## Init Testing Infrastructure ----\"                                                    \n## [41] \"## Create a template for tests\"                                                         \n## [42] \"golem::use_recommended_tests()\"                                                         \n## [43] \"\"                                                                                       \n## [44] \"## Use Recommended Packages ----\"                                                       \n## [45] \"golem::use_recommended_deps()\"                                                          \n## [46] \"\"                                                                                       \n## [47] \"## Favicon ----\"                                                                        \n## [48] \"# If you want to change the favicon (default is golem's one)\"                           \n## [49] \"golem::remove_favicon()\"                                                                \n## [50] \"golem::use_favicon() # path = \\\"path/to/ico\\\". Can be an online file. \"                 \n## [51] \"\"                                                                                       \n## [52] \"## Add helper functions ----\"                                                           \n## [53] \"golem::use_utils_ui()\"                                                                  \n## [54] \"golem::use_utils_server()\"                                                              \n## [55] \"\"                                                                                       \n## [56] \"# You're now set! ----\"                                                                 \n## [57] \"\"                                                                                       \n## [58] \"# go to dev/02_dev.R\"                                                                   \n## [59] \"rstudioapi::navigateToFile( \\\"dev/02_dev.R\\\" )\"                                         \n## [60] \"\"\n\nThis script is a series of calls to {usethis} functions; you can remove whatever you don’t need and adapt the others that you need. As you can see, I did not change much here. Execute it line by line when you’re done editing it. Once you’re done, you can go to 02_dev.R and this is probably the script that you’ll change the most:\n\nsystem2(\"cat\", args = \"~/Documents/golemDemo/dev/02_dev.R\", stdout = TRUE)\n##  [1] \"# Building a Prod-Ready, Robust Shiny Application.\"                             \n##  [2] \"# \"                                                                             \n##  [3] \"# README: each step of the dev files is optional, and you don't have to \"       \n##  [4] \"# fill every dev scripts before getting started. \"                              \n##  [5] \"# 01_start.R should be filled at start. \"                                       \n##  [6] \"# 02_dev.R should be used to keep track of your development during the project.\"\n##  [7] \"# 03_deploy.R should be used once you need to deploy your app.\"                 \n##  [8] \"# \"                                                                             \n##  [9] \"# \"                                                                             \n## [10] \"###################################\"                                            \n## [11] \"#### CURRENT FILE: DEV SCRIPT #####\"                                            \n## [12] \"###################################\"                                            \n## [13] \"\"                                                                               \n## [14] \"# Engineering\"                                                                  \n## [15] \"\"                                                                               \n## [16] \"## Dependencies ----\"                                                           \n## [17] \"## Add one line by package you want to add as dependency\"                       \n## [18] \"usethis::use_package( \\\"shiny\\\" )\"                                              \n## [19] \"usethis::use_package( \\\"shinydashboard\\\" )\"                                     \n## [20] \"usethis::use_package(\\\"data.table\\\") \"                                          \n## [21] \"usethis::use_package(\\\"DT\\\")\"                                                   \n## [22] \"usethis::use_package(\\\"dplyr\\\")\"                                                \n## [23] \"usethis::use_package(\\\"rlang\\\")\"                                                \n## [24] \"usethis::use_package(\\\"ggiraph\\\")\"                                              \n## [25] \"usethis::use_package(\\\"ggplot2\\\")\"                                              \n## [26] \"usethis::use_package(\\\"htmlwidgets\\\")\"                                          \n## [27] \"usethis::use_package(\\\"dplyr\\\")\"                                                \n## [28] \"usethis::use_package(\\\"colorspace\\\")\"                                           \n## [29] \"usethis::use_package(\\\"shinycssloaders\\\")\"                                      \n## [30] \"usethis::use_package(\\\"lubridate\\\")\"                                            \n## [31] \"\"                                                                               \n## [32] \"## Add modules ----\"                                                            \n## [33] \"## Create a module infrastructure in R/\"                                        \n## [34] \"golem::add_module( name = \\\"name_of_module1\\\" ) # Name of the module\"           \n## [35] \"golem::add_module( name = \\\"name_of_module2\\\" ) # Name of the module\"           \n## [36] \"\"                                                                               \n## [37] \"## Add helper functions ----\"                                                   \n## [38] \"## Creates ftc_* and utils_*\"                                                   \n## [39] \"golem::add_fct( \\\"helpers\\\" ) \"                                                 \n## [40] \"golem::add_utils( \\\"helpers\\\" )\"                                                \n## [41] \"\"                                                                               \n## [42] \"## External resources\"                                                          \n## [43] \"## Creates .js and .css files at inst/app/www\"                                  \n## [44] \"golem::add_js_file( \\\"script\\\" )\"                                               \n## [45] \"golem::add_js_handler( \\\"handlers\\\" )\"                                          \n## [46] \"golem::add_css_file( \\\"custom\\\" )\"                                              \n## [47] \"\"                                                                               \n## [48] \"## Add internal datasets ----\"                                                  \n## [49] \"## If you have data in your package\"                                            \n## [50] \"usethis::use_data_raw( name = \\\"my_dataset\\\", open = FALSE ) \"                  \n## [51] \"\"                                                                               \n## [52] \"## Tests ----\"                                                                  \n## [53] \"## Add one line by test you want to create\"                                     \n## [54] \"usethis::use_test( \\\"app\\\" )\"                                                   \n## [55] \"\"                                                                               \n## [56] \"# Documentation\"                                                                \n## [57] \"\"                                                                               \n## [58] \"## Vignette ----\"                                                               \n## [59] \"usethis::use_vignette(\\\"golemDemo\\\")\"                                           \n## [60] \"devtools::build_vignettes()\"                                                    \n## [61] \"\"                                                                               \n## [62] \"## Code coverage ----\"                                                          \n## [63] \"## (You'll need GitHub there)\"                                                  \n## [64] \"usethis::use_github()\"                                                          \n## [65] \"usethis::use_travis()\"                                                          \n## [66] \"usethis::use_appveyor()\"                                                        \n## [67] \"\"                                                                               \n## [68] \"# You're now set! ----\"                                                         \n## [69] \"# go to dev/03_deploy.R\"                                                        \n## [70] \"rstudioapi::navigateToFile(\\\"dev/03_deploy.R\\\")\"                                \n## [71] \"\"\n\nThis is where you will list the dependencies of your package (lines 18 to 30) as well as the modules (lines 34 to 35). I have mostly used this file for the dependencies, as I already had the modules from another app, so I didn’t bother listing them here. But if I would have started from scratch, I would changed the line:\n\ngolem::add_module( name = \\\"name_of_module1\\\" ) # Name of the module\n\nto something like:\n\ngolem::add_module( name = \\\"import_data\\\" ) # Name of the module\n\nand executing it would have generated the needed files to start creating the module at the right spot. Let’s go see how such a module looks like (I’m skipping the third script for now, as it is only useful once you want to deploy).\n\n\nYou can find the modules in the R/ folder. Let’s take a look at the module that allows the user to load the data:\n\nsystem2(\"cat\", args = \"~/Documents/golemDemo/R/mod_load_data.R\", stdout = TRUE)\n##   [1] \"#' load_data UI Function\"                                                                                                    \n##   [2] \"#'\"                                                                                                                          \n##   [3] \"#' @description A shiny Module.\"                                                                                             \n##   [4] \"#'\"                                                                                                                          \n##   [5] \"#' @param id,input,output,session Internal parameters for {shiny}.\"                                                          \n##   [6] \"#'\"                                                                                                                          \n##   [7] \"#' @noRd \"                                                                                                                   \n##   [8] \"#'\"                                                                                                                          \n##   [9] \"#' @importFrom shiny NS tagList \"                                                                                            \n##  [10] \"#' @importFrom data.table fread\"                                                                                             \n##  [11] \"#' @importFrom DT renderDataTable dataTableOutput\"                                                                           \n##  [12] \"#' @importFrom dplyr filter\"                                                                                                 \n##  [13] \"#' @importFrom rlang quo `!!` as_name\"                                                                                       \n##  [14] \"mod_load_data_ui &lt;- function(id){\"                                                                                           \n##  [15] \"  ns &lt;- NS(id)\"                                                                                                              \n##  [16] \"  tagList(\"                                                                                                                  \n##  [17] \"    box(title = \\\"Select dataset\\\",\"                                                                                         \n##  [18] \"        radioButtons(ns(\\\"select_dataset\\\"),\"                                                                                \n##  [19] \"                    label = \\\"Select dataset\\\",\"                                                                             \n##  [20] \"                    choices = c(\\\"Rescue points\\\", \\\"Radars\\\"),\"                                                             \n##  [21] \"                    selected = c(\\\"Rescue points\\\")),\"                                                                       \n##  [22] \"        conditionalPanel(\"                                                                                                   \n##  [23] \"          condition = paste0('input[\\\\'', ns('select_dataset'), \\\"\\\\'] == \\\\'Rescue points\\\\'\\\"),\"                           \n##  [24] \"          selectInput(ns(\\\"selector_place\\\"), \\\"Place\\\",\"                                                                    \n##  [25] \"                      choices = c(\\\"test\\\"),\"                                                                                \n##  [26] \"                      #choices = c(unique(output$dataset$place)),\"                                                           \n##  [27] \"                      selected = c(\\\"Luxembourg, Ville (G)\\\"),\"                                                              \n##  [28] \"                      multiple = TRUE)),\"                                                                                    \n##  [29] \"        conditionalPanel(\"                                                                                                   \n##  [30] \"          condition = paste0('input[\\\\'', ns('select_dataset'), \\\"\\\\'] == \\\\'Radars\\\\'\\\"),\"                                  \n##  [31] \"          selectInput(ns(\\\"selector_radar\\\"), \\\"Radar\\\",\"                                                                    \n##  [32] \"                      choices = c(\\\"test\\\"),\"                                                                                \n##  [33] \"                      #choices = c(\\\"huhu\\\"),\"                                                                               \n##  [34] \"                      selected = c(\\\"National road\\\"),\"                                                                      \n##  [35] \"                      multiple = TRUE)),\"                                                                                    \n##  [36] \"        width = NULL),\"                                                                                                      \n##  [37] \"  )\"                                                                                                                         \n##  [38] \"}\"                                                                                                                           \n##  [39] \"\"                                                                                                                            \n##  [40] \"#' load_data Server Function\"                                                                                                \n##  [41] \"#'\"                                                                                                                          \n##  [42] \"#' @noRd \"                                                                                                                   \n##  [43] \"mod_load_data_server &lt;- function(input, output, session){\"                                                                   \n##  [44] \"  ns &lt;- session$ns\"                                                                                                          \n##  [45] \" \"                                                                                                                           \n##  [46] \"  \"                                                                                                                          \n##  [47] \"  read_dataset &lt;- reactive({\"                                                                                                \n##  [48] \"    if(input$select_dataset == \\\"Rescue points\\\") {\"                                                                         \n##  [49] \"\"                                                                                                                            \n##  [50] \"      dataset &lt;- fread(\\\"data-raw/rettungspunkte.csv\\\")\"                                                                     \n##  [51] \"      variable &lt;- quo(place)\"                                                                                                \n##  [52] \"      filter_values &lt;- unique(dataset[, place])\"                                                                             \n##  [53] \"    } else {\"                                                                                                                \n##  [54] \"      dataset &lt;- fread(\\\"data-raw/radars.csv\\\")\"                                                                             \n##  [55] \"      variable &lt;- quo(type_road)\"                                                                                            \n##  [56] \"      filter_values &lt;- unique(dataset[, type_road])\"                                                                         \n##  [57] \"    }\"                                                                                                                       \n##  [58] \"    cat(\\\"reading data\\\\n\\\")\"                                                                                                \n##  [59] \"    list(dataset = dataset,\"                                                                                                 \n##  [60] \"         variable = variable,\"                                                                                               \n##  [61] \"         filter_values = filter_values)\"                                                                                     \n##  [62] \"  })\"                                                                                                                        \n##  [63] \"\"                                                                                                                            \n##  [64] \"\"                                                                                                                            \n##  [65] \"  observe({\"                                                                                                                 \n##  [66] \"    updateSelectInput(session, \\\"selector_place\\\", label = \\\"Select place:\\\", choices = read_dataset()$filter_values,\"       \n##  [67] \"                      selected = \\\"Luxembourg, Ville (G)\\\")\"                                                                 \n##  [68] \"  })\"                                                                                                                        \n##  [69] \"\"                                                                                                                            \n##  [70] \"  observe({\"                                                                                                                 \n##  [71] \"    updateSelectInput(session, \\\"selector_radar\\\", label = \\\"Select type of road:\\\", choices = read_dataset()$filter_values,\"\n##  [72] \"                      selected = \\\"National road\\\")\"                                                                         \n##  [73] \"  })\"                                                                                                                        \n##  [74] \"\"                                                                                                                            \n##  [75] \"  result &lt;- reactive({\"                                                                                                      \n##  [76] \"    return_dataset &lt;- read_dataset()$dataset\"                                                                                \n##  [77] \"\"                                                                                                                            \n##  [78] \"    if(\\\"place\\\" %in% colnames(return_dataset)){\"                                                                            \n##  [79] \"      return_dataset &lt;- return_dataset %&gt;%\"                                                                                  \n##  [80] \"        filter((!!read_dataset()$variable) %in% input$selector_place)\"                                                       \n##  [81] \"\"                                                                                                                            \n##  [82] \"      result &lt;- list(\"                                                                                                       \n##  [83] \"        return_dataset = return_dataset,\"                                                                                    \n##  [84] \"        variable = quo(place)\"                                                                                               \n##  [85] \"      )\"                                                                                                                     \n##  [86] \"    } else {\"                                                                                                                \n##  [87] \"      return_dataset &lt;- return_dataset %&gt;%\"                                                                                  \n##  [88] \"        filter((!!read_dataset()$variable) %in% input$selector_radar)\"                                                       \n##  [89] \"\"                                                                                                                            \n##  [90] \"      result &lt;- list(\"                                                                                                       \n##  [91] \"        return_dataset = return_dataset,\"                                                                                    \n##  [92] \"        variable = quo(type_road)\"                                                                                           \n##  [93] \"      )\"                                                                                                                     \n##  [94] \"    }\"                                                                                                                       \n##  [95] \"  })\"                                                                                                                        \n##  [96] \"\"                                                                                                                            \n##  [97] \"  result\"                                                                                                                    \n##  [98] \"}\"                                                                                                                           \n##  [99] \"    \"                                                                                                                        \n## [100] \"## To be copied in the UI\"                                                                                                   \n## [101] \"# mod_load_data_ui(\\\"load_data_ui_1\\\")\"                                                                                      \n## [102] \"    \"                                                                                                                        \n## [103] \"## To be copied in the server\"                                                                                               \n## [104] \"# callModule(mod_load_data_server, \\\"load_data_ui_1\\\")\"                                                                      \n## [105] \" \"\n\nThis scripts looks like a mini Shiny app; there’s a UI defined at the top of the script, and then a server defined at the bottom (I’m not describing what the module does here, I’ll do that in the video). What’s important here, is that this is a module and as such it can be reused in any app, by simply copying the right lines of code at the right spot. See lines 100 to 104 for this, which tells you exactly where to copy the lines to use this module. All the modules will look the same, and have this little explanation at the bottom to tell you where you need to copy the lines to use the modules. While building each module, you can use {shinipsum} instead of having to bother about the server logic, just to get things going, as explained above.\n\n\nNow, finally, let’s take a look at the actual UI of the app:\n\nsystem2(\"cat\", args = \"~/Documents/golemDemo/R/app_ui.R\", stdout = TRUE)\n##  [1] \"#' The application User-Interface\"                                                          \n##  [2] \"#' \"                                                                                        \n##  [3] \"#' @param request Internal parameter for `{shiny}`. \"                                       \n##  [4] \"#'     DO NOT REMOVE.\"                                                                      \n##  [5] \"#' @import shiny\"                                                                           \n##  [6] \"#' @import shinydashboard\"                                                                  \n##  [7] \"#' @noRd\"                                                                                   \n##  [8] \"app_ui &lt;- function(request) {\"                                                              \n##  [9] \"  tagList(\"                                                                                 \n## [10] \"                                        # Leave this function for adding external resources\"\n## [11] \"    golem_add_external_resources(),\"                                                        \n## [12] \"                                        # List the first level UI elements here\"            \n## [13] \"    dashboardPage(\"                                                                         \n## [14] \"      dashboardHeader(title = \\\"Prototype: dashboard ecoles\\\"),\"                            \n## [15] \"      dashboardSidebar(\"                                                                    \n## [16] \"        sidebarMenu(\"                                                                       \n## [17] \"          menuItem(\\\"Carte\\\", tabName = \\\"Carte\\\", icon = icon(\\\"map\\\")),\"                  \n## [18] \"          menuItem(\\\"Tab 2\\\", tabName = \\\"tab_2\\\", icon = icon(\\\"chart-line\\\"))\"            \n## [19] \"        )\"                                                                                  \n## [20] \"      ),\"                                                                                   \n## [21] \"      dashboardBody(\"                                                                       \n## [22] \"        tabItems(\"                                                                          \n## [23] \"          tabItem(tabName = \\\"Carte\\\",\"                                                     \n## [24] \"                  fluidRow(\"                                                                \n## [25] \"                    column(\"                                                                \n## [26] \"                      width = 4,\"                                                           \n## [27] \"                      mod_load_data_ui(\\\"load_data_ui_1\\\"),\"                                \n## [28] \"                      mod_table_data_ui(\\\"table_data_ui_1\\\")\"                               \n## [29] \"                    ),\"                                                                     \n## [30] \"                    column(\"                                                                \n## [31] \"                      width = 6, offset = 2,\"                                               \n## [32] \"                      mod_map_data_ui(\\\"map_data_ui_1\\\")\"                                   \n## [33] \"                    )\"                                                                      \n## [34] \"                  ))\"                                                                       \n## [35] \"        )\"                                                                                  \n## [36] \"      )\"                                                                                    \n## [37] \"    )\"                                                                                      \n## [38] \"  )\"                                                                                        \n## [39] \"}\"                                                                                          \n## [40] \"\"                                                                                           \n## [41] \"#' Add external Resources to the Application\"                                               \n## [42] \"#' \"                                                                                        \n## [43] \"#' This function is internally used to add external \"                                       \n## [44] \"#' resources inside the Shiny application. \"                                                \n## [45] \"#' \"                                                                                        \n## [46] \"#' @import shiny\"                                                                           \n## [47] \"#' @importFrom golem add_resource_path activate_js favicon bundle_resources\"                \n## [48] \"#' @noRd\"                                                                                   \n## [49] \"golem_add_external_resources &lt;- function(){\"                                                \n## [50] \"  \"                                                                                         \n## [51] \"  add_resource_path(\"                                                                       \n## [52] \"    'www', app_sys('app/www')\"                                                              \n## [53] \"  )\"                                                                                        \n## [54] \" \"                                                                                          \n## [55] \"  tags$head(\"                                                                               \n## [56] \"    favicon(),\"                                                                             \n## [57] \"    bundle_resources(\"                                                                      \n## [58] \"      path = app_sys('app/www'),\"                                                           \n## [59] \"      app_title = 'golemDemo'\"                                                              \n## [60] \"    )\"                                                                                      \n## [61] \"    # Add here other external resources\"                                                    \n## [62] \"    # for example, you can add shinyalert::useShinyalert() \"                                \n## [63] \"  )\"                                                                                        \n## [64] \"}\"                                                                                          \n## [65] \"\"\n\nthis is the “global” UI of the app. This looks like any other Shiny UI, but instead of having many many lines of code, there’s basically only calls to the UIs of each modules (see lines 27 and 28). And that’s it! It keeps your code quite small and much easier to reason about. You’ll find something even simpler for the server:\n\nsystem2(\"cat\", args = \"~/Documents/golemDemo/R/app_server.R\", stdout = TRUE)\n##  [1] \"#' The application server-side\"                                              \n##  [2] \"#' \"                                                                         \n##  [3] \"#' @param input,output,session Internal parameters for {shiny}. \"            \n##  [4] \"#'     DO NOT REMOVE.\"                                                       \n##  [5] \"#' @import shiny\"                                                            \n##  [6] \"#' @noRd\"                                                                    \n##  [7] \"app_server &lt;- function( input, output, session ) {\"                          \n##  [8] \"  # List the first level callModules here\"                                   \n##  [9] \"\"                                                                            \n## [10] \"  result &lt;- callModule(mod_load_data_server, \\\"load_data_ui_1\\\")\"            \n## [11] \"\"                                                                            \n## [12] \"  callModule(mod_table_data_server, \\\"table_data_ui_1\\\", result)\"            \n## [13] \"  \"                                                                          \n## [14] \"\"                                                                            \n## [15] \"  selected_lines &lt;- reactive({\"                                              \n## [16] \"    if(is.null(input$`table_data_ui_1-dataset_rows_selected`)){\"             \n## [17] \"      return(TRUE)\"                                                          \n## [18] \"    } else {\"                                                                \n## [19] \"      as.numeric(input$`table_data_ui_1-dataset_rows_selected`)\"             \n## [20] \"    }\"                                                                       \n## [21] \"  })\"                                                                        \n## [22] \"\"                                                                            \n## [23] \"  callModule(mod_map_data_server, \\\"map_data_ui_1\\\", result, selected_lines)\"\n## [24] \"\"                                                                            \n## [25] \"}\"\n\nLine 10 calls the server side of the “load data” module, and saves the result (a data frame) into a variable called result. This result is then passed as an argument to the server side of table data module, which simply shows a table of the data. From lines 15 to 21, I define a variable called selected-lines in which the lines that the user selects in the data table are saved. This gave me some headaches, because I needed to find the right syntax. I was able to find it thanks to a Stackoverflow post that I have now lost since then… but the idea is that the indices of the selected rows are saved into a variable called dataset_rows_selected and this variable name must be appended to the name of the UI of the table where the table is. If no row is selected, then this object should be TRUE; why? Because if you filter a data frame with a condition that simply evaluates always to TRUE, you get all the rows back, and thus, all of the data frame. If you start selecting rows, say, rows number 2, 8 and 12, then dataset_rows_selected will be equal to c(2, 8, 12), and the filter will return these rows.\n\n\nFinally, I call the module that returns a map of Luxembourg, and pass both the data frame, saved in the result variable, and the selected_lines objects as arguments. And that’s how you make modules communicate and share data with each other, just like you would chain functions together. I won’t go through each module, but there’s several other interesting tricks that I’ll discuss during the video; for instance, I’m quite happy with the module that loads the data; the user can choose between two different dataset, and the select input will update with the right columns. This also wasn’t so easy to do, but it’ll be easier to explain during a video, so stay tuned!"
  },
  {
    "objectID": "posts/2020-12-30-year_review.html",
    "href": "posts/2020-12-30-year_review.html",
    "title": "A year in review",
    "section": "",
    "text": "This blog post just contains the links I mention in my video that you can watch here.\n\n\nI mention the following books, packages, and people in my video:\n\n\n\necharts4r\n\n\ntargets\n\n\neasystats\n\n\nApplied Economics with R\n\n\ncoolbutuseless\n\n\nData Science ZJ and disk.frame\n\n\nStatistical Rethinking\n\n\nGolem and Engineering production-grade shiny apps\n\n\nModernDive\n\n\n\nMany others created and shared amazing content during the year, so sorry I could not mention everyone!\n\n\nHappy new year to all and thank you for the ongoing support!"
  },
  {
    "objectID": "posts/2023-07-19-nix_for_r_part2.html",
    "href": "posts/2023-07-19-nix_for_r_part2.html",
    "title": "Reproducible data science with Nix, part 2 – running {targets} pipelines with Nix",
    "section": "",
    "text": "This is the second post in a series of posts about Nix. Disclaimer: I’m a super beginner with Nix. So this series of blog posts is more akin to notes that I’m taking while learning than a super detailed tutorial. So if you’re a Nix expert and read something stupid in here, that’s normal. This post is going to focus on R (obviously) but the ideas are applicable to any programming language.\nSo in part 1 I explained what Nix was and how you could use it to build reproducible development environments. Now, let’s go into more details and actually set up some environments and run a {targets} pipeline using it.\nObviously the first thing you should do is install Nix. A lot of what I’m showing here comes from the Nix.dev so if you want to install Nix, then look at the instructions here. If you’re using Windows, you’ll have to have WSL2 installed. If you don’t want to install Nix just yet, you can also play around with a NixOS Docker image. NixOS is a Linux distribution that uses the concepts of Nix for managing the whole operating system, and obviously comes with the Nix package manager installed. But if you’re using Nix inside Docker you won’t be able to work interactively with graphical applications like RStudio, due to how Docker works (but more on working interactively with IDEs in part 3 of this series, which I’m already drafting).\nAssuming you have Nix installed, you should be able to run the following command in a terminal:\nThis will launch a Nix shell with the sl package installed. Because sl is not available, it’ll get installed on the fly, and you will get “dropped” into a Nix shell:\nYou can now run sl and marvel at what it does (I won’t spoil you). You can quit the Nix shell by typing exit and you’ll go back to your usual terminal. If you try now to run sl it won’t work (unless you installed on your daily machine). So if you need to go back to that Nix shell and rerun sl, simply rerun:\nThis time you’ll be dropped into the Nix shell immediately and can run sl. So if you need to use R, simply run the following:\nand you’ll be dropped in a Nix shell with R. This version of R will be different than the one potentially already installed on your system, and it won’t have access to any R packages that you might have installed. This is because Nix environment are isolated from the rest of your system (well, not quite, but again, more on this in part 3). So you’d need to add packages as well (exit the Nix shell and run this command to add packages):\nYou can now start R in that Nix shell and load the {dplyr} and {janitor} packages. You might be wondering how I knew that I needed to type rPackages.dplyr to install {dplyr}. You can look for this information online. By the way, if a package uses the . character in its name, you should replace that . character by _ so to install {data.table} write rPackages.data_table.\nSo that’s nice and dandy, but not quite what we want. Instead, what we want is to be able to declare what we need in terms of packages, dependencies, etc, inside a file, and have Nix build an environment according to these specifications which we can then use for our daily needs. To do so, we need to write a so-called default.nix file. This is what such a file looks like:\nI wont discuss the intricate details of writing such a file just yet, because it’ll take too much time and I’ll be repeating what you can find on the Nix.dev website. I’ll give some pointers though. But for now, let’s assume that we already have such a default.nix file that we defined for our project, and see how we can use it to run a {targets} pipeline. I’ll explain how I write such files."
  },
  {
    "objectID": "posts/2023-07-19-nix_for_r_part2.html#running-a-targets-pipeline-using-nix",
    "href": "posts/2023-07-19-nix_for_r_part2.html#running-a-targets-pipeline-using-nix",
    "title": "Reproducible data science with Nix, part 2 – running {targets} pipelines with Nix",
    "section": "\nRunning a {targets} pipeline using Nix\n",
    "text": "Running a {targets} pipeline using Nix\n\n\nLet’s say I have this, more complex, default.nix file:\n\n{ pkgs ? import (fetchTarball \"https://github.com/NixOS/nixpkgs/archive/8ad5e8132c5dcf977e308e7bf5517cc6cc0bf7d8.tar.gz\") {} }:\n\nwith pkgs;\n\nlet\n  my-pkgs = rWrapper.override {\n    packages = with rPackages; [\n      targets\n      tarchetypes\n      rmarkdown\n    (buildRPackage {\n      name = \"housing\";\n      src = fetchgit {\n        url = \"https://github.com/rap4all/housing/\";\n        branchName = \"fusen\";\n        rev = \"1c860959310b80e67c41f7bbdc3e84cef00df18e\";\n        sha256 = \"sha256-s4KGtfKQ7hL0sfDhGb4BpBpspfefBN6hf+XlslqyEn4=\";\n      };\n    propagatedBuildInputs = [\n        dplyr\n        ggplot2\n        janitor\n        purrr\n        readxl\n        rlang\n        rvest\n        stringr\n        tidyr\n        ];\n      })\n    ];\n  };\nin\nmkShell {\n  buildInputs = [my-pkgs];\n}\n\nSo the file above defines an environment that contains all the required packages to run a pipeline that you can find on this Github repository. What’s interesting is that I need to install a package that’s only been released on Github, the {housing} package that I wrote for the purposes of my book, and I can do so in that file as well, using the fetchgit() function. Nix has many such functions, called fetchers that simplify the process of downloading files from the internet, see here. This function takes some self-explanatory inputs as arguments, and two other arguments that might not be that self-explanatory: rev and sha256. rev is actually the commit on the Github repository. This commit is the one that I want to use for this particular project. So if I keep working on this package, then building an environment with this default.nix will always pull the source code as it was at that particular commit. sha256 is the hash of the downloaded repository. It makes sure that the files weren’t tampered with. How did I obtain that? Well, the simplest way is to set it to the empty string ““ and then try to build the environment. This error message will pop-up:\n\nerror: hash mismatch in fixed-output derivation '/nix/store/449zx4p6x0yijym14q3jslg55kihzw66-housing-1c86095.drv':\n         specified: sha256-AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\n            got:    sha256-s4KGtfKQ7hL0sfDhGb4BpBpspfefBN6hf+XlslqyEn4=\n\nSo simply copy the hash from the last line, and rebuild! Then if in the future something happens to the files, you’ll know. Another interesting input is propagatedBuildInputs. These are simply the dependencies of the {housing} package. To find them, see the Imports: section of the DESCRIPTION file. There’s also the fetchFromGithub fetcher that I could have used, but unlike fetchgit, it is not possible to specify the branch name we want to use. Since here I wanted to get the code from the branch called fusen, I had to use fetchgit. The last thing I want to explain is the very first line:\n\n{ pkgs ? import (fetchTarball \"https://github.com/NixOS/nixpkgs/archive/8ad5e8132c5dcf977e308e7bf5517cc6cc0bf7d8.tar.gz\") {} }:\n\nIn particular the url. This url points to a specific release of nixpkgs, that ships the required version of R for this project, R version 4.2.2. How did I find this release of nixpkgs? There’s a handy service for that here. So using this service, I get the right commit hash for the release that install R version 4.2.2.\n\n\nOk, but before building the environment defined by this file, let me just say that I know what you’re thinking. Probably something along the lines of: damn it Bruno, this looks complicated and why should I care? Let me just use {renv}!! and I’m not going to lie, writing the above file from scratch didn’t take me long in typing, but it took me long in reading. I had to read quite a lot (look at part 1 for some nice references) before being comfortable enough to write it. But I’ll just say this:\n\n\n\ncontinue reading, because I hope to convince you that Nix is really worth the effort\n\n\nI’m working on a package that will help R users generate default.nix files like the one from above with minimal effort (more on this at the end of the blog post)\n\n\n\nIf you’re following along, instead of typing this file, you can clone this repository. This repository contains the default.nix file from above, and a {targets} pipeline that I will run in that environment.\n\n\nOk, so now let’s build the environment by running nix-build inside a terminal in the folder that contains this file. It should take a bit of time, because many of the packages will need to be built from source. But they will get built. Then, you can drop into a Nix shell using nix-shell and then type R, which will start the R session in that environment. You can then simply run targets::tar_make(), and you’ll see the file analyse.html appear, which is the output of the {targets} pipeline.\n\n\nBefore continuing, let me just make you realize three things:\n\n\n\nwe just ran a targets pipeline with all the needed dependencies which include not only package dependencies, but the right version of R (version 4.2.2) as well, and all required system dependencies;\n\n\nwe did so WITHOUT using any containerization tool like Docker;\n\n\nthe whole thing is completely reproducible; the exact same packages will forever be installed, regardless of when we build this environment, because I’m using a particular release of nixpkgs (8ad5e8132c5dcf977e308e7bf5517cc6cc0bf7d8) so each piece of software this release of Nix installs is going to stay constant.\n\n\n\nAnd I need to stress completely reproducible. Because using {renv}+Docker, while providing a very nice solution, still has some issues. First of all, with Docker, the underlying operating system (often Ubuntu) evolves and changes through time. So lower level dependencies might change. And at some point in the future, that version of Ubuntu will not be supported anymore. So it won’t be possible to rebuild the image, because it won’t be possible to download any software into it. So either we build our Docker image and really need to make sure to keep it forever, or we need to port our pipeline to newer versions of Ubuntu, without any guarantee that it’s going to work exactly the same. Also, by defining Dockerfiles that build upon Dockerfiles that build upon Dockerfiles, it’s difficult to know what is actually installed in a particular image. This situation can of course be avoided by writing Dockerfiles in such a way that it doesn’t rely on any other Dockerfile, but that’s also a lot of effort. Now don’t get me wrong: I’m not saying Docker should be canceled. I still think that it has its place and that its perfectly fine to use it (I’ll take a project that uses {renv}+Docker any day over one that doesn’t!). But you should be aware of alternative ways of running pipelines in a reproducible way, and Nix is such a way.\n\n\nGoing back to our pipeline, we could also run the pipeline with this command:\n\nnix-shell /path/to/default.nix --run \"Rscript -e 'setwd(\\\"/path/to\\\");targets::tar_make()'\"\n\nbut it’s a bit of a mouthful. What you could do instead is running the pipeline each time you drop into the nix shell by adding a so-called shellHook. For this, we need to change the default.nix file again. Add these lines in the mkShell function:\n\n...\nmkShell {\n  buildInputs = [my-pkgs];\n  shellHook = ''\n     Rscript -e \"targets::tar_make()\"\n  '';\n}\n\nNow, each time you drop into the Nix shell in the folder containing that default.nix file, targets::tar_make() get automatically executed. You can then inspect the results.\n\n\nIn the next blog post, I’ll show how we can use that environment with IDEs like RStudio, VS Code and Emacs to work interactively. But first, let me quickly talk about a package I’ve been working on to ease the process of writing default.nix files."
  },
  {
    "objectID": "posts/2023-07-19-nix_for_r_part2.html#rix-reproducible-environments-with-nix",
    "href": "posts/2023-07-19-nix_for_r_part2.html#rix-reproducible-environments-with-nix",
    "title": "Reproducible data science with Nix, part 2 – running {targets} pipelines with Nix",
    "section": "\nRix: Reproducible Environments with Nix\n",
    "text": "Rix: Reproducible Environments with Nix\n\n\nI wrote a very early, experimental package called {rix} which will help write these default.nix files for us. {rix} is an R package that hopefully will make R users want to try out Nix for their development purposes. It aims to mimic the workflow of {renv}, or to be more exact, the workflow of what Python users do when starting a new project. Usually what they do is create a completely fresh environment using pyenv (or another similar tool). Using pyenv, Python developers can install a per project version of Python and Python packages, but unlike Nix, won’t install system-level dependencies as well.\n\n\nIf you want to install {rix}, run the following line in an R session:\n\ndevtools::install_github(\"b-rodrigues/rix\")\n\nYou can then using the rix() function to create a default.nix file like so:\n\nrix::rix(r_ver = \"current\",\n         pkgs = c(\"dplyr\", \"janitor\"),\n         ide = \"rstudio\",\n         path = \".\")\n\nThis will create a default.nix file that Nix can use to build an environment that includes the current versions of R, {dplyr} and {janitor}, and RStudio as well. Yes you read that right: you need to have a per-project RStudio installation. The reason is that RStudio modifies environment variables and so your “locally” installed RStudio would not find the R version installed with Nix. This is not the case with other IDEs like VS Code or Emacs. If you want to have an environment with another version of R, simply run:\n\nrix::rix(r_ver = \"4.2.1\",\n         pkgs = c(\"dplyr\", \"janitor\"),\n         ide = \"rstudio\",\n         path = \".\")\n\nand you’ll get an environment with R version 4.2.1. To see which versions are available, you can run rix::available_r(). Learn more about {rix} on its website. It’s in very early stages, and doesn’t handle packages that have only been released on Github, yet. And the interface might change. I’m thinking of making it possible to list the packages in a yaml file and then have rix() generate the default.nix file from the yaml file. This might be cleaner. There is already something like this called Nixml, so maybe I don’t even need to rewrite anything!\n\n\nBut I’ll discuss this is more detail next time, where I’ll explain how you can use development environments built with Nix using an IDE."
  },
  {
    "objectID": "posts/2023-07-19-nix_for_r_part2.html#references",
    "href": "posts/2023-07-19-nix_for_r_part2.html#references",
    "title": "Reproducible data science with Nix, part 2 – running {targets} pipelines with Nix",
    "section": "\nReferences\n",
    "text": "References\n\n\n\nThe great Nix.dev tutorials.\n\n\nThis blog post: Statistical Rethinking and Nix I referenced in part 1 as well, it helped me install my {housing} package from Github.\n\n\nNixml."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html",
    "title": "How to use jailbreakr",
    "section": "",
    "text": "The jailbreakr package is probably one of the most interesting packages I came across recently. This package makes it possible to extract messy data from spreadsheets. What is meant by messy? I am sure you already had to deal with spreadsheets that contained little tables inside a single sheet for example. As far as I know, there is no simple way of extracting these tables without having to fiddle around a lot. This is now over with jailbreakr. Well not entirely, because jailbreakr is still in development, but it works well already. If you want to know more about the planned features, you can watch the following video by Jenny Bryan, one of the package’s authors."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html#what-is-jailbreakr",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html#what-is-jailbreakr",
    "title": "How to use jailbreakr",
    "section": "",
    "text": "The jailbreakr package is probably one of the most interesting packages I came across recently. This package makes it possible to extract messy data from spreadsheets. What is meant by messy? I am sure you already had to deal with spreadsheets that contained little tables inside a single sheet for example. As far as I know, there is no simple way of extracting these tables without having to fiddle around a lot. This is now over with jailbreakr. Well not entirely, because jailbreakr is still in development, but it works well already. If you want to know more about the planned features, you can watch the following video by Jenny Bryan, one of the package’s authors."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html#installation-and-data",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html#installation-and-data",
    "title": "How to use jailbreakr",
    "section": "\nInstallation and data\n",
    "text": "Installation and data\n\n\nYou will have to install the package from Github, as it is not on CRAN yet. Here is the Github link. To install the package, just run the following commands in an R console:\n\ndevtools::install_github(c(\"hadley/xml2\",\n                           \"rsheets/linen\",\n                           \"rsheets/cellranger\",\n                           \"rsheets/rexcel\",\n                           \"rsheets/jailbreakr\"))\n\nIf you get the following error:\n\ndevtools::install_github(\"hadley/xml2\")\nDownloading GitHub repo hadley/xml2@master\nfrom URL https://api.github.com/repos/hadley/xml2/zipball/master\nError in system(full, intern = quiet, ignore.stderr = quiet, ...) :\n    error in running command\n\nand if you’re on a GNU+Linux distribution try to run the following command:\n\noptions(unzip = \"internal\")\n\nand then run github_install() again.\n\n\nAs you can see, you need some other packages to make it work. Now we are going to get some data. We are going to download some time series from the European Commission, data I had to deal with recently. Download the data by clicking here and look for the spreadsheet titled Investment_total_factors_nace2.xlsx. The data we are interested in is on the second sheet, named TOT. You cannot import this sheet easily into R because there are four tables on the same sheet. Let us use jailbreakr to get these tables out of the sheet and into nice, tidy, data frames."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html#jailbreakr-to-the-rescue",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html#jailbreakr-to-the-rescue",
    "title": "How to use jailbreakr",
    "section": "\njailbreakr to the rescue\n",
    "text": "jailbreakr to the rescue\n\n\nThe first step is to read the data in. For this, we are going to use the rexcel package, which is also part of the rsheets organization on Github that was set up by Jenny Brian and Rich Fitzjohn, the authors of these packages. rexcel imports the sheet you want but not in a way that is immediately useful to you. It just gets the sheet into R, which makes it then possible to use jailbreakr’s magic on it. First, let’s import the packages we need:\n\nlibrary(\"rexcel\")\nlibrary(\"jailbreakr\")\n\nWe need to check which sheet to import. There are two sheets, and we want to import the one called TOT, the second one. But is it really the second one? I have noticed that sometimes, there are hidden sheets which makes importing the one you want impossible. So first, let use use another package, readxl and its function excel_sheets() to make sure we are extracting the sheet we really need:\n\nsheets &lt;- readxl::excel_sheets(path_to_data)\n\ntot_sheet &lt;- which(sheets == \"TOT\")\n\nprint(tot_sheet)\n## [1] 3\n\nAs you can see, the sheet we want is not the second, but the third! Let us import this sheet into R now (this might take more time than you think; on my computer it takes around 10 seconds):\n\nmy_sheet &lt;- rexcel_read(path_to_data, sheet = tot_sheet)\n\nNow we can start using jailbreakr. The function split_sheet() is the one that splits the sheet into little tables:\n\ntables &lt;- split_sheet(my_sheet)\nstr(tables)\n## List of 4\n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 34 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list \n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 33 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list \n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 32 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list \n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 33 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list\n\ntables is actually a list containing worksheet_view objects. Take a look at the dim attribute: you see the dimensions of the tables there. When I started using jailbreakr I was stuck here. I was looking for the function that would extract the data frames and could not find it. Then I watched the video and I understood what I had to do: a worksheet_view object has a values() method that does the extraction for you. This is a bit unusual in R (it made me feel like I was using Python); maybe in future versions this values() method will become a separate function of its own in the package. What happens when we use values()?\n\nlibrary(\"purrr\")\nlist_of_data &lt;-  map(tables, (function(x)(x$values())))\nmap(list_of_data, head)\n## [[1]]\n##      [,1]     [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]\n## [1,] \"TOT\"    NA      NA    NA    NA    NA    NA    NA    NA    NA   \n## [2,] \"DEMAND\" 33603   33969 34334 34699 35064 35430 35795 36160 36525\n## [3,] \"FDEMT\"  \"FDEMN\" NA    NA    NA    NA    NA    NA    NA    NA   \n## [4,] \"EU\"     \":\"     16.9  -1.4  20.2  34.5  31.4  37.5  39    37.3 \n## [5,] \"EA\"     \":\"     15.5  -13.1 14.8  30.9  25.1  35.2  39.2  37.1 \n## [6,] \"BE\"     \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   42.3  43.1 \n##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n## [1,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [2,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543\n## [3,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [4,] 39.2  27.5  20.6  21.4  29.8  26.4  32.5  47.1  19    -1.3  23.5 \n## [5,] 39.5  25.3  18.2  18.9  27.4  23    28.2  46.1  12.3  -9.3  19.3 \n## [6,] 45.8  42.2  42.9  43.8  45.8  47.4  49.1  50.9  48.2  46.9  46.3 \n##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] \n## [1,] NA    NA    NA    NA    NA    NA    NA    \n## [2,] 40908 41274 41639 42004 42369 42735 43100 \n## [3,] NA    NA    NA    NA    NA    NA    NA    \n## [4,] 29    22    21.1  25.6  31.8  22.9  \"30.7\"\n## [5,] 26.2  18.6  15.7  21.7  28.8  17.3  26.6  \n## [6,] 46.8  47.1  48.2  50.1  49.2  34.5  34.4  \n## \n## [[2]]\n##      [,1]        [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]\n## [1,] \"FINANCIAL\" 33603   33969 34334 34699 35064 35430 35795 36160 36525\n## [2,] \"FFINT\"     \"FFINN\" NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] \"EU\"        \":\"     -5.1  -6.2  2.7   6.7   9     14.4  13.9  14   \n## [4,] \"EA\"        \":\"     -8.8  -13.5 -3.4  2.6   5.7   12.5  13.2  13.1 \n## [5,] \"BE\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   21.5  22.4 \n## [6,] \"BG\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"  \n##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n## [1,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543\n## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] 16.4  9.4   7.4   8.1   12.4  8.4   13.6  23.4  4.1   -4    10.9 \n## [4,] 16.5  8     6.8   5.1   9.9   4.8   8.4   24.3  -2.8  -10.5 9.3  \n## [5,] 20.9  22.3  32.2  33.5  33.8  34.8  35    34.5  37.2  33.5  32.7 \n## [6,] \":\"   \":\"   20.8  24    27.1  28.3  33.4  37.5  37.7  26.6  30.4 \n##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] \n## [1,] 40908 41274 41639 42004 42369 42735 43100 \n## [2,] NA    NA    NA    NA    NA    NA    NA    \n## [3,] 12.4  10.2  8.8   13.4  17.4  6.2   \"12.3\"\n## [4,] 9     7.2   5     11    13.1  -1    6.5   \n## [5,] 31.5  32.3  33    31.7  32.2  19.9  20.5  \n## [6,] 33.8  35.6  36    41.5  41.6  44.2  43.8  \n## \n## [[3]]\n##      [,1]        [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]\n## [1,] \"TECHNICAL\" 33603   33969 34334 34699 35064 35430 35795 36160 36525\n## [2,] \"FTECT\"     \"FTECN\" NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] \"EU\"        \":\"     39.2  37.6  38.3  40    40.7  42.8  43.5  43.8 \n## [4,] \"EA\"        \":\"     39.7  36.2  37.5  41.2  40    44    44.8  44.9 \n## [5,] \"BE\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   58.8  58.5 \n## [6,] \"BG\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"  \n##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n## [1,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543\n## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] 37    31.1  27.2  30.9  30.4  30.3  27.4  40.5  25.8  23.1  27.4 \n## [4,] 37    30.3  27.4  31    29.9  29.7  24.8  41    23.4  19.5  26.4 \n## [5,] 58.3  58.4  57.7  59.2  59.6  59.4  60.2  59.5  60.5  57.9  56.3 \n## [6,] \":\"   \":\"   17.3  17.5  21.1  21.5  25.3  28.2  26.1  21    25.3 \n##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] \n## [1,] 40908 41274 41639 42004 42369 42735 43100 \n## [2,] NA    NA    NA    NA    NA    NA    NA    \n## [3,] 28.9  26.3  31.3  32.1  32.1  30.2  \"34.6\"\n## [4,] 28.5  25.9  32.1  32.4  33.1  30.2  36    \n## [5,] 56.7  57.7  57.9  58.6  59.1  13.1  13.1  \n## [6,] 24.6  26.8  30.4  31.9  34.1  34.8  33.7  \n## \n## [[4]]\n##      [,1]    [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10] [,11]\n## [1,] \"OTHER\" 33603   33969 34334 34699 35064 35430 35795 36160 36525 36891\n## [2,] \"FOTHT\" \"FOTHN\" NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] \"EU\"    \":\"     2.9   -0.5  3.9   3.9   1     4.1   4.7   7     7.2  \n## [4,] \"EA\"    \":\"     2.3   -4.9  1.4   1.3   -2.4  1.1   3.2   5.8   7    \n## [5,] \"BE\"    \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   14    14.9  15.9 \n## [6,] \"BG\"    \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"  \n##      [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]\n## [1,] 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543 40908\n## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] -1.5  6.2   8.1   7.6   1.4   2.4   13.7  -1.9  -3.2  1.1   1.1  \n## [4,] -3.7  5.5   7.1   7.2   -2.2  0.4   15.5  -4.6  -8.4  0.3   -3.3 \n## [5,] 16.3  22.8  23.1  22.4  24.5  25.3  25.5  26.6  26.6  24.7  24.6 \n## [6,] \":\"   -2.3  -0.8  2.4   2.9   3.5   4.8   5.5   2.2   3.3   3.2  \n##      [,23] [,24] [,25] [,26] [,27] [,28]\n## [1,] 41274 41639 42004 42369 42735 43100\n## [2,] NA    NA    NA    NA    NA    NA   \n## [3,] -1.6  0.9   2.7   1.9   -3.3  \"2.1\"\n## [4,] -2.3  0.6   2.5   2.1   -5.4  1.7  \n## [5,] 26.4  25.9  25    25.3  4.7   5.2  \n## [6,] 5.9   7     8.2   9.6   9.4   9.1\n\nWe are getting really close to something useful! Now we can get the first table and do some basic cleaning to have a tidy dataset:\n\ndataset1 &lt;- list_of_data[[1]]\n\ndataset1 &lt;- dataset1[-c(1:3), ]\ndataset1[dataset1 == \":\"] &lt;- NA\ncolnames(dataset1) &lt;- c(\"country\", seq(from = 1991, to = 2017))\n\nhead(dataset1)\n##      country 1991 1992 1993  1994 1995 1996 1997 1998 1999 2000 2001 2002\n## [1,] \"EU\"    NA   16.9 -1.4  20.2 34.5 31.4 37.5 39   37.3 39.2 27.5 20.6\n## [2,] \"EA\"    NA   15.5 -13.1 14.8 30.9 25.1 35.2 39.2 37.1 39.5 25.3 18.2\n## [3,] \"BE\"    NA   NA   NA    NA   NA   NA   NA   42.3 43.1 45.8 42.2 42.9\n## [4,] \"BG\"    NA   NA   NA    NA   NA   NA   NA   NA   NA   NA   NA   39.6\n## [5,] \"CZ\"    NA   NA   NA    NA   NA   NA   NA   NA   NA   NA   NA   54.9\n## [6,] \"DK\"    49.5 45   50    59.5 62.5 55.5 60.5 57.5 56   61.5 57.5 59.5\n##      2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016\n## [1,] 21.4 29.8 26.4 32.5 47.1 19   -1.3 23.5 29   22   21.1 25.6 31.8 22.9\n## [2,] 18.9 27.4 23   28.2 46.1 12.3 -9.3 19.3 26.2 18.6 15.7 21.7 28.8 17.3\n## [3,] 43.8 45.8 47.4 49.1 50.9 48.2 46.9 46.3 46.8 47.1 48.2 50.1 49.2 34.5\n## [4,] 43   42.8 45.5 49.1 52.6 50.7 39.5 45.5 47.4 45.6 50.5 51.4 49.9 53.2\n## [5,] 37   48.5 67.9 66.4 66.8 69.3 64.7 61   56   47.5 53   53.5 67.5 58  \n## [6,] 53.5 50   59   64   63   56   33.5 57   47   48   52   45.5 40.5 36.5\n##      2017  \n## [1,] \"30.7\"\n## [2,] 26.6  \n## [3,] 34.4  \n## [4,] 52.8  \n## [5,] 59.5  \n## [6,] 37.5\n\nEt voilà! We went from a messy spreadsheet to a tidy dataset in a matter of minutes. Even though this package is still in early development and not all the features that are planned are available, the basics are there and can save you a lot of pain!"
  },
  {
    "objectID": "posts/2021-02-06-echarts_map.html#introduction",
    "href": "posts/2021-02-06-echarts_map.html#introduction",
    "title": "How to draw a map of arbitrary contiguous regions, or visualizing the spread of COVID-19 in the Greater Region",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nI was able to blog during the year 2020 without mentioning the ongoing pandemic once. It’s not that I made any conscious effort not to talk about it, but I did not really want to do something that had already been done a 1000 times. This changed this year, when I wanted to look at the spread of COVID-19, not only in the Grand-Duchy of Luxembourg, the country I live in, but also among our neighbours. You see, the Grand-Duchy of Luxembourg is like an island, but instead of being surrounded by water, it’s surrounded by Belgians, Germans and Frenchmen. Many of them commute every day to Luxembourg to work, and even though they technically don’t live inside the country, many aspects of their lives happen inside Luxembourguish borders. Their children might even come to school here, and sometimes they live so close by the border, that they can catch Luxembourguish public transportation in their towns. 200k commuters from Belgium, Germany and France work here every day. That’s half our workforce! So that’s why I thought that it would make sense to look at the spread of the disease at the level of the so-called Greater Region. This Greater Region is made up of the Grand-Duchy of Luxembourg, the Provinces of Liège and Luxembourg in Belgium (hence why I keep writing the Grand-Duchy of Luxembourg to refer to the country, and the Province of Luxembourg to refer to the Belgian province of the same name), and two German Länders, the Saarland and the Rhineland-Palatinate. Confused? Welcome to Europe, where supranational institutions literally have to have a page entitled Do not get confused so that citizens don’t get lost (we still do).\n\n\nSo the Greater Region is not a state, but facilitates collaboration between the regions comprising it. To me, technically a citizen of the Greater Region, it feels like there was a want to peacefully correct for the randomness of history, where German-speaking regions ended up in both France and Belgium, and where Belgium and Luxembourg, well, somehow became independent countries.\n\n\nAnyways, what I wanted to do was to first of all get the COVID-19 daily cases data for each of these regions. I did that, and even created a package called {covidGrandeRegion} hosted here that makes it very easy to download the latest data for the Greater Region. I will write another blog post about it, I have something in mind that I wanted to try for some time, and this was the first step. Then I thought that adding a function that would create a map could also be nice. And this is where the technical aspect of this blog post starts."
  },
  {
    "objectID": "posts/2021-02-06-echarts_map.html#the-problems-to-map-the-greater-region",
    "href": "posts/2021-02-06-echarts_map.html#the-problems-to-map-the-greater-region",
    "title": "How to draw a map of arbitrary contiguous regions, or visualizing the spread of COVID-19 in the Greater Region",
    "section": "\nThe problems to map the Greater Region\n",
    "text": "The problems to map the Greater Region\n\n\nSo how do you draw a map for an arbitrary landmass like the Greater Region? I wanted to draw the maps using {echarts4r}, and there’s a very easy guide you can read. If you want to draw a map for one, or several, countries, this guide is all you need. But I wanted a map with only parts of France, Belgium and Germany. The only complete country was Luxembourg. So the first problem was how to get only parts of a country. The second problem, is that I had daily covid cases for the lowest administrative levels for France (which are Départements), Belgium (the Provinces) and Germany (Land- and Stadtkreise). But for the Grand-Duchy of Luxembourg, there’s only data at the level of the country. So this would be another problem. How to draw a map with unequal levels of precision? One final problem: the names of the administrative divisions in my covid datasets are not the same than the ones that get downloaded if you follow the guide I linked before. So I had to rename them as well."
  },
  {
    "objectID": "posts/2021-02-06-echarts_map.html#the-solutions",
    "href": "posts/2021-02-06-echarts_map.html#the-solutions",
    "title": "How to draw a map of arbitrary contiguous regions, or visualizing the spread of COVID-19 in the Greater Region",
    "section": "\nThe solutions\n",
    "text": "The solutions\n\n\nLet’s first start by following the guide, so loading the packages, and getting the maps I need:\n\nlibrary(echarts4r)\nlibrary(sp)\nlibrary(raster)\nlibrary(geojsonio)\nfrance_dep &lt;- getData(\"GADM\", country = \"FRANCE\", level = 2)\n\nger_kreise &lt;- getData(\"GADM\", country = \"GERMANY\", level = 2)\n\nbe_province &lt;- getData(\"GADM\", country = \"BELGIUM\", level = 2)\n\nThe above lines of code load the required packages, and download the maps for France, Belgium and Germany with the required administrative level I need. I’ll leave Luxembourg for last.\n\n\nLet’s take a look at what type of object we’re dealing with:\n\nclass(france_dep)\n## [1] \"SpatialPolygonsDataFrame\"\n## attr(,\"package\")\n## [1] \"sp\"\n\nSo it seems to be something like a data frame, but probably more complex. Looking for some help online, I saw that you can coerce it to a data frame:\n\nas.data.frame(be_province)\n##    GID_0  NAME_0   GID_1     NAME_1 NL_NAME_1     GID_2          NAME_2\n## 1    BEL Belgium BEL.1_1  Bruxelles      &lt;NA&gt; BEL.1.1_1       Bruxelles\n## 2    BEL Belgium BEL.2_1 Vlaanderen      &lt;NA&gt; BEL.2.1_1       Antwerpen\n## 3    BEL Belgium BEL.2_1 Vlaanderen      &lt;NA&gt; BEL.2.2_1         Limburg\n## 4    BEL Belgium BEL.2_1 Vlaanderen      &lt;NA&gt; BEL.2.3_1 Oost-Vlaanderen\n## 5    BEL Belgium BEL.2_1 Vlaanderen      &lt;NA&gt; BEL.2.4_1  Vlaams Brabant\n## 6    BEL Belgium BEL.2_1 Vlaanderen      &lt;NA&gt; BEL.2.5_1 West-Vlaanderen\n## 7    BEL Belgium BEL.3_1   Wallonie      &lt;NA&gt; BEL.3.1_1  Brabant Wallon\n## 8    BEL Belgium BEL.3_1   Wallonie      &lt;NA&gt; BEL.3.2_1         Hainaut\n## 9    BEL Belgium BEL.3_1   Wallonie      &lt;NA&gt; BEL.3.3_1           Liège\n## 10   BEL Belgium BEL.3_1   Wallonie      &lt;NA&gt; BEL.3.4_1      Luxembourg\n## 11   BEL Belgium BEL.3_1   Wallonie      &lt;NA&gt; BEL.3.5_1           Namur\n##                                                                                                             VARNAME_2\n## 1  Brussel Hoofstadt|Brusselse Hoofdstedelijke Gewest|Brüssel|Bruxelas|Région de Bruxelles-Capitale|Brussels|Bruselas\n## 2                                                                            Amberes|Antuérpia|Antwerp|Anvers|Anversa\n## 3                                                                                                   Limbourg|Limburgo\n## 4                   Flandres Oriental|Fiandra Orientale|Flandes Oriental|Flandre orientale|East Flanders|Ost Flandern\n## 5                                                 Brabant Flamand|Brabante Flamenco|Brabante Flamengo|Flemish Brabant\n## 6           Fiandra Occidentale|Flandes Occidental|Flandre occidentale|Flandres Ocidental|West Flandern|West Flanders\n## 7                                                                                       Waals Brabant|Walloon Brabant\n## 8                                                                                                 Henegouwen|Hennegau\n## 9                                                                                            Luik|Liegi|Lieja|Lüttich\n## 10                                                                                   Lussemburgo|Luxemburg|Luxemburgo\n## 11                                                                                                              Namen\n##    NL_NAME_2                                TYPE_2      ENGTYPE_2 CC_2 HASC_2\n## 1       &lt;NA&gt; Hoofdstedelijk Gewest|Région Capitale Capital Region &lt;NA&gt;  BE.BU\n## 2       &lt;NA&gt;                             Provincie       Province &lt;NA&gt;  BE.AN\n## 3       &lt;NA&gt;                             Provincie       Province &lt;NA&gt;  BE.LI\n## 4       &lt;NA&gt;                             Provincie       Province &lt;NA&gt;  BE.OV\n## 5       &lt;NA&gt;                             Provincie       Province &lt;NA&gt;  BE.VB\n## 6       &lt;NA&gt;                             Provincie       Province &lt;NA&gt;  BE.WV\n## 7       &lt;NA&gt;                              Province      Provincie &lt;NA&gt;  BE.BW\n## 8       &lt;NA&gt;                              Province      Provincie &lt;NA&gt;  BE.HT\n## 9       &lt;NA&gt;                              Province      Provincie &lt;NA&gt;  BE.LG\n## 10      &lt;NA&gt;                              Province      Provincie &lt;NA&gt;  BE.LX\n## 11      &lt;NA&gt;                              Province      Provincie &lt;NA&gt;  BE.NA\n\nWe’re not going to convert them to data frames however; but this is an interesting clue; these SpatialPolygonsDataFrame objects share common methods with data frames. What this means is that we can use the usual, base R way of manipulating these objects.\n\n\nSo to get only the French départements I need, I can slice them like so:\n\nlorraine &lt;- france_dep[`%in%`(france_dep$NAME_2, c(\"Meurthe-et-Moselle\", \"Meuse\", \"Moselle\", \"Vosges\")),]\n\nSame for the German kreise, here I select the Länder which are a higher administrative division than the Kreise, which makes it faster (so I don’t need to type all the 40+ Kreise):\n\nger_kreise &lt;- ger_kreise[`%in%`(ger_kreise$NAME_1, c(\"Rheinland-Pfalz\", \"Saarland\")),]\n\nFor Germany, many Kreise had a name which was different than on my covid data, so I had to rename them. So here again, the base R way of doing things works:\n\nger_kreise$NAME_2[ger_kreise$NAME_2 == \"Eifelkreis Bitburg-Prüm\"]  &lt;- \"Bitburg-Prüm\"\nger_kreise$NAME_2[ger_kreise$NAME_2 == \"St. Wendel\"]  &lt;- \"Sankt Wendel\"\nger_kreise$NAME_2[ger_kreise$NAME_2 == \"Altenkirchen (Westerwald)\"]  &lt;- \"Altenkirchen\"\nger_kreise$NAME_2[ger_kreise$NAME_2 == \"Neustadt an der Weinstraße\"]  &lt;- \"Neustadt a.d.Weinstraße\"\nger_kreise$NAME_2[ger_kreise$NAME_2 == \"Landau in der Pfalz\"]  &lt;- \"Landau i.d.Pfalz\"\nger_kreise$NAME_2[ger_kreise$NAME_2 == \"Ludwigshafen am Rhein\"]  &lt;- \"Ludwigshafen\"\nger_kreise$NAME_2[ger_kreise$NAME_2 == \"Frankenthal (Pfalz)\"]  &lt;- \"Frankenthal\"\n\nFinally, I do the same for Belgium, and rename their province of Luxembourg, which was simply called “Luxembourg”, to “Province de Luxembourg”:\n\nbe_wallonia &lt;- be_province[be_province$NAME_1 == \"Wallonie\", ]\nbe_wallonia$NAME_2[be_wallonia$NAME_2 == \"Luxembourg\"]  &lt;- \"Province de Luxembourg\"\n\nI rename the province because the Grand-Duchy of Luxembourg is also only called “Luxembourg” in the data, and this would cause issues when mapping.\n\n\nNow, comes Luxembourg. As I’ve written above, I only have data at the level of the country, so I download the country map:\n\nlu_map_0 &lt;- getData(\"GADM\", country = \"LUXEMBOURG\", level = 0)\n\nLet’s also see how it looks like as a data frame:\n\nas.data.frame(lu_map_0)\n##   GID_0     NAME_0\n## 1   LUX Luxembourg\n\nUnlike the previous SpatialPolygonsDataFrames, there are much less columns and this will cause an issue. Indeed, in order to have a single SpatialPolygonsDataFrame object to draw my map, I will need to combine them. This will be very easy, by simple using the rbind() function. Again, simply using base R functions. However, this only works if the data frames have the same columns. Another issue, is that I will be using the names of the regions which are in the SpatialPolygonsDataFrames’ column called NAME_2, but for Luxembourg, the name of the region (in this case the whole country) is in the column called NAME_0. So I need to add this columns to the SpatialPolygonsDataFrame object for Luxembourg:\n\nlu_map_0$GID_1 &lt;- NA\nlu_map_0$NAME_1 &lt;- NA\nlu_map_0$NL_NAME_1 &lt;- NA\nlu_map_0$GID_2 &lt;- NA\nlu_map_0$NAME_2 &lt;- \"Luxembourg\"\nlu_map_0$VARNAME_2 &lt;- NA\nlu_map_0$NL_NAME_2 &lt;- NA\nlu_map_0$TYPE_2 &lt;- NA\nlu_map_0$ENGTYPE_2 &lt;- NA\nlu_map_0$CC_2 &lt;- NA\nlu_map_0$HASC_2 &lt;- NA\n\nAaaand… that’s it! Wasn’t that hard, but a bit convoluted nonetheless. Now I can bind all the SpatialPolygonsDataFrame objects in one and use that for mapping:\n\ngrande_region &lt;- do.call(rbind, list(lorraine, ger_kreise, be_wallonia, lu_map_0))\n\nas.data.frame(grande_region)\n##     GID_0     NAME_0    GID_1          NAME_1 NL_NAME_1       GID_2\n## 76    FRA     France  FRA.6_1       Grand Est      &lt;NA&gt;   FRA.6.7_1\n## 77    FRA     France  FRA.6_1       Grand Est      &lt;NA&gt;   FRA.6.8_1\n## 78    FRA     France  FRA.6_1       Grand Est      &lt;NA&gt;   FRA.6.9_1\n## 70    FRA     France  FRA.6_1       Grand Est      &lt;NA&gt;  FRA.6.10_1\n## 99    DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt;  DEU.11.1_1\n## 110   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt;  DEU.11.2_1\n## 121   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt;  DEU.11.3_1\n## 129   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt;  DEU.11.4_1\n## 130   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt;  DEU.11.5_1\n## 131   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt;  DEU.11.6_1\n## 132   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt;  DEU.11.7_1\n## 133   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt;  DEU.11.8_1\n## 134   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt;  DEU.11.9_1\n## 100   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.10_1\n## 101   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.11_1\n## 102   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.12_1\n## 104   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.14_1\n## 103   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.13_1\n## 105   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.15_1\n## 106   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.16_1\n## 107   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.17_1\n## 108   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.18_1\n## 111   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.20_1\n## 109   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.19_1\n## 112   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.21_1\n## 113   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.22_1\n## 114   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.23_1\n## 115   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.24_1\n## 116   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.25_1\n## 117   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.26_1\n## 118   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.27_1\n## 119   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.28_1\n## 120   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.29_1\n## 122   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.30_1\n## 124   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.32_1\n## 123   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.31_1\n## 125   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.33_1\n## 126   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.34_1\n## 127   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.35_1\n## 128   DEU    Germany DEU.11_1 Rheinland-Pfalz      &lt;NA&gt; DEU.11.36_1\n## 135   DEU    Germany DEU.12_1        Saarland      &lt;NA&gt;  DEU.12.1_1\n## 136   DEU    Germany DEU.12_1        Saarland      &lt;NA&gt;  DEU.12.2_1\n## 137   DEU    Germany DEU.12_1        Saarland      &lt;NA&gt;  DEU.12.3_1\n## 138   DEU    Germany DEU.12_1        Saarland      &lt;NA&gt;  DEU.12.4_1\n## 139   DEU    Germany DEU.12_1        Saarland      &lt;NA&gt;  DEU.12.5_1\n## 140   DEU    Germany DEU.12_1        Saarland      &lt;NA&gt;  DEU.12.6_1\n## 7     BEL    Belgium  BEL.3_1        Wallonie      &lt;NA&gt;   BEL.3.1_1\n## 8     BEL    Belgium  BEL.3_1        Wallonie      &lt;NA&gt;   BEL.3.2_1\n## 9     BEL    Belgium  BEL.3_1        Wallonie      &lt;NA&gt;   BEL.3.3_1\n## 10    BEL    Belgium  BEL.3_1        Wallonie      &lt;NA&gt;   BEL.3.4_1\n## 11    BEL    Belgium  BEL.3_1        Wallonie      &lt;NA&gt;   BEL.3.5_1\n## 1     LUX Luxembourg     &lt;NA&gt;            &lt;NA&gt;      &lt;NA&gt;        &lt;NA&gt;\n##                                NAME_2                        VARNAME_2\n## 76                 Meurthe-et-Moselle                             &lt;NA&gt;\n## 77                              Meuse                             &lt;NA&gt;\n## 78                            Moselle                       Lothringen\n## 70                             Vosges                             &lt;NA&gt;\n## 99                          Ahrweiler                             &lt;NA&gt;\n## 110                      Altenkirchen                             &lt;NA&gt;\n## 121                       Alzey-Worms                             &lt;NA&gt;\n## 129                      Bad Dürkheim                             &lt;NA&gt;\n## 130                     Bad Kreuznach                             &lt;NA&gt;\n## 131               Bernkastel-Wittlich                             &lt;NA&gt;\n## 132                        Birkenfeld                             &lt;NA&gt;\n## 133                       Cochem-Zell                             &lt;NA&gt;\n## 134                  Donnersbergkreis                             &lt;NA&gt;\n## 100                      Bitburg-Prüm                             &lt;NA&gt;\n## 101                       Frankenthal                             &lt;NA&gt;\n## 102                       Germersheim                             &lt;NA&gt;\n## 104                    Kaiserslautern                             &lt;NA&gt;\n## 103 Kaiserslautern (Kreisfreie Stadt)                             &lt;NA&gt;\n## 105                           Koblenz                             &lt;NA&gt;\n## 106                             Kusel                             &lt;NA&gt;\n## 107                  Landau i.d.Pfalz                             &lt;NA&gt;\n## 108                      Ludwigshafen                             &lt;NA&gt;\n## 111                             Mainz                             &lt;NA&gt;\n## 109                      Mainz-Bingen                             &lt;NA&gt;\n## 112                     Mayen-Koblenz                             &lt;NA&gt;\n## 113           Neustadt a.d.Weinstraße                             &lt;NA&gt;\n## 114                           Neuwied                             &lt;NA&gt;\n## 115                         Pirmasens                             &lt;NA&gt;\n## 116              Rhein-Hunsrück-Kreis                             &lt;NA&gt;\n## 117                  Rhein-Lahn-Kreis                             &lt;NA&gt;\n## 118                 Rhein-Pfalz-Kreis                             &lt;NA&gt;\n## 119                            Speyer                             &lt;NA&gt;\n## 120               Südliche Weinstraße                             &lt;NA&gt;\n## 122                      Südwestpfalz                             &lt;NA&gt;\n## 124                             Trier                             &lt;NA&gt;\n## 123                    Trier-Saarburg                             &lt;NA&gt;\n## 125                       Vulkaneifel                             &lt;NA&gt;\n## 126                   Westerwaldkreis                             &lt;NA&gt;\n## 127                             Worms                             &lt;NA&gt;\n## 128                       Zweibrücken                             &lt;NA&gt;\n## 135                     Merzig-Wadern                             &lt;NA&gt;\n## 136                       Neunkirchen                             &lt;NA&gt;\n## 137       Regionalverband Saarbrücken                             &lt;NA&gt;\n## 138                         Saarlouis                             &lt;NA&gt;\n## 139                   Saarpfalz-Kreis                             &lt;NA&gt;\n## 140                      Sankt Wendel                             &lt;NA&gt;\n## 7                      Brabant Wallon    Waals Brabant|Walloon Brabant\n## 8                             Hainaut              Henegouwen|Hennegau\n## 9                               Liège         Luik|Liegi|Lieja|Lüttich\n## 10             Province de Luxembourg Lussemburgo|Luxemburg|Luxemburgo\n## 11                              Namur                            Namen\n## 1                          Luxembourg                             &lt;NA&gt;\n##     NL_NAME_2           TYPE_2  ENGTYPE_2  CC_2   HASC_2\n## 76       &lt;NA&gt;      Département Department    54    FR.MM\n## 77       &lt;NA&gt;      Département Department    55    FR.MS\n## 78       &lt;NA&gt;      Département Department    57    FR.MO\n## 70       &lt;NA&gt;      Département Department    88    FR.VG\n## 99       &lt;NA&gt;        Landkreis   District 07131 DE.RP.AR\n## 110      &lt;NA&gt;        Landkreis   District 07132 DE.RP.AT\n## 121      &lt;NA&gt;        Landkreis   District 07331 DE.RP.AW\n## 129      &lt;NA&gt;        Landkreis   District 07332 DE.RP.BD\n## 130      &lt;NA&gt;        Landkreis   District 07133 DE.RP.BK\n## 131      &lt;NA&gt;        Landkreis   District 07231 DE.RP.BW\n## 132      &lt;NA&gt;        Landkreis   District 07134 DE.RP.BR\n## 133      &lt;NA&gt;        Landkreis   District 07135 DE.RP.CZ\n## 134      &lt;NA&gt;        Landkreis   District 07333 DE.RP.DN\n## 100      &lt;NA&gt;        Landkreis   District 07232 DE.RP.EB\n## 101      &lt;NA&gt; Kreisfreie Stadt   District 07311 DE.RP.FA\n## 102      &lt;NA&gt;        Landkreis   District 07334 DE.RP.GR\n## 104      &lt;NA&gt;        Landkreis   District 07335 DE.RP.KL\n## 103      &lt;NA&gt; Kreisfreie Stadt   District 07312 DE.RP.KL\n## 105      &lt;NA&gt; Kreisfreie Stadt   District 07111 DE.RP.KO\n## 106      &lt;NA&gt;        Landkreis   District 07336 DE.RP.KU\n## 107      &lt;NA&gt; Kreisfreie Stadt   District 07313 DE.RP.LP\n## 108      &lt;NA&gt; Kreisfreie Stadt   District 07314 DE.RP.LR\n## 111      &lt;NA&gt; Kreisfreie Stadt   District 07315 DE.RP.MI\n## 109      &lt;NA&gt;        Landkreis   District 07339 DE.RP.MB\n## 112      &lt;NA&gt;        Landkreis   District 07137 DE.RP.MK\n## 113      &lt;NA&gt; Kreisfreie Stadt   District 07316 DE.RP.NW\n## 114      &lt;NA&gt;        Landkreis   District 07138 DE.RP.NU\n## 115      &lt;NA&gt; Kreisfreie Stadt   District 07317 DE.RP.PR\n## 116      &lt;NA&gt;        Landkreis   District 07140 DE.RP.RH\n## 117      &lt;NA&gt;        Landkreis   District 07141 DE.RP.RN\n## 118      &lt;NA&gt;        Landkreis   District 07338 DE.RP.RZ\n## 119      &lt;NA&gt; Kreisfreie Stadt   District 07318 DE.RP.SE\n## 120      &lt;NA&gt;        Landkreis   District 07337 DE.RP.SW\n## 122      &lt;NA&gt;        Landkreis   District 07340 DE.RP.SD\n## 124      &lt;NA&gt; Kreisfreie Stadt   District 07211 DE.RP.TI\n## 123      &lt;NA&gt;        Landkreis   District 07235 DE.RP.TS\n## 125      &lt;NA&gt;        Landkreis   District 07233 DE.RP.VL\n## 126      &lt;NA&gt;        Landkreis   District 07143 DE.RP.WS\n## 127      &lt;NA&gt; Kreisfreie Stadt   District 07319 DE.RP.WR\n## 128      &lt;NA&gt; Kreisfreie Stadt   District 07320 DE.RP.ZE\n## 135      &lt;NA&gt;        Landkreis   District 10042 DE.SL.MW\n## 136      &lt;NA&gt;        Landkreis   District 10043 DE.SL.NU\n## 137      &lt;NA&gt;        Landkreis   District 10041 DE.SL.SB\n## 138      &lt;NA&gt;        Landkreis   District 10044 DE.SL.SA\n## 139      &lt;NA&gt;        Landkreis   District 10045 DE.SL.SP\n## 140      &lt;NA&gt;        Landkreis   District 10046 DE.SL.SW\n## 7        &lt;NA&gt;         Province  Provincie  &lt;NA&gt;    BE.BW\n## 8        &lt;NA&gt;         Province  Provincie  &lt;NA&gt;    BE.HT\n## 9        &lt;NA&gt;         Province  Provincie  &lt;NA&gt;    BE.LG\n## 10       &lt;NA&gt;         Province  Provincie  &lt;NA&gt;    BE.LX\n## 11       &lt;NA&gt;         Province  Provincie  &lt;NA&gt;    BE.NA\n## 1        &lt;NA&gt;             &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;     &lt;NA&gt;\n\nAnd now I can continue following the tutorial from the {echarts4r} website, by converting this SpatialPolygonsDataFrame object for the Greater Region into a geojson file which can now be used to draw maps! You can take a look at the final result here.\n\n\nI don’t post the code to draw the map here, because it would require some more tinkering by joining the COVID data. But you can find my raw script here (lines 51 to 61) or you could also take a look at the draw_map() function from the package I made, which you can find here.\n\n\nI really like the end result, {echarts4r} is really a fantastic package! Stay tuned part 2 of the project, which will deal with machine learning."
  },
  {
    "objectID": "posts/2022-11-19-raps.html",
    "href": "posts/2022-11-19-raps.html",
    "title": "Reproducibility with Docker and Github Actions for the average R enjoyer",
    "section": "",
    "text": "This blog post is a summary of Chapters 9 and 10 of this ebook I wrote for a course\nThe goal is the following: we want to write a pipeline that produces some plots. We want the code to be executed inside a Docker container for reproducibility, and we want this container to get executed on Github Actions. Github Actions is a Continuous Integration and Continuous Delivery service from Github that allows you to execute arbitrary code on events (like pushing code to a repo). It’s pretty neat. For example, you could be writing a paper using Latex and get the pdf compiled on Github Actions each time you push, without needing to have to do it yourself. Or if you are developing an R package, unit tests could get executed each time you push code, so you don’t have to do it manually.\nThis blog post will assume that you are familiar with R and are comfortable with it, as well as Git and Github.\nIt will also assume that you’ve at least heard of Docker and have it already installed on your computer, but ideally, you’ve already played a bit around with Docker. If you’re a total Docker beginner, this tutorial might be a bit too esoteric.\nLet’s start by writing a pipeline that works on our machines using the {targets} package."
  },
  {
    "objectID": "posts/2022-11-19-raps.html#getting-something-working-on-your-machine",
    "href": "posts/2022-11-19-raps.html#getting-something-working-on-your-machine",
    "title": "Reproducibility with Docker and Github Actions for the average R enjoyer",
    "section": "\nGetting something working on your machine\n",
    "text": "Getting something working on your machine\n\n\nSo, let’s say that you got some nice code that you need to rerun every month, week, day, or even hour. Or let’s say that you’re a researcher that is concerned with reproducibility. Let’s also say that you want to make sure that this code always produces the same result (let’s say it’s some plots that need to get remade once some data is refreshed).\n\n\nOk, so first of all, you really want your workflow to be defined using the {targets} package. If you’re not familiar with {targets}, this will serve as a micro introduction, but you really should read the {targets} manual, at least the walkthrough (watch the 4 minute video). {targets} is a build automation tool that you should definitely add to your toolbox.\n\n\nLet’s define a workflow that does the following: data gets read, data gets filtered, data gets plotted. What’s the data about? Unemployment in Luxembourg. Luxembourg is a little Western European country that looks like a shoe and is about the size of .98 Rhode Islands from which yours truly hails from. Did you know that Luxembourg was a monarchy, and the last Grand-Duchy in the World? I bet you did not know that. Also, what you should know to understand the script below is that the country of Luxembourg is divided into Cantons, and each Cantons into Communes. Basically, if Luxembourg was the USA, Cantons would be States and Communes would be Counties (or Parishes or Boroughs). What’s confusing is that “Luxembourg” is also the name of a Canton, and of a Commune (which also has the status of a city).\n\n\nAnyways, here’s how my script looks like:\n\nlibrary(targets)\nlibrary(dplyr)\nlibrary(ggplot2)\nsource(\"functions.R\")\n\n\nlist(\n    tar_target(\n        unemp_data,\n        get_data()\n    ),\n\n    tar_target(\n        lux_data,\n        clean_unemp(unemp_data,\n                    place_name_of_interest = \"Luxembourg\",\n                    level_of_interest = \"Country\",\n                    col_of_interest = active_population)\n    ),\n\n    tar_target(\n        canton_data,\n        clean_unemp(unemp_data,\n                    level_of_interest = \"Canton\",\n                    col_of_interest = active_population)\n    ),\n\n    tar_target(\n        commune_data,\n        clean_unemp(unemp_data,\n                    place_name_of_interest = c(\"Luxembourg\",\n                                               \"Dippach\",\n                                               \"Wiltz\",\n                                               \"Esch/Alzette\",\n                                               \"Mersch\"),\n                    col_of_interest = active_population)\n    ),\n\n    tar_target(\n        lux_plot,\n        make_plot(lux_data)\n    ),\n\n    tar_target(\n        canton_plot,\n        make_plot(canton_data)\n    ),\n\n    tar_target(\n        commune_plot,\n        make_plot(commune_data)\n    ),\n\n    tar_target(\n        luxembourg_saved_plot,\n        save_plot(\"fig/luxembourg.png\", lux_plot),\n        format = \"file\"\n    ),\n\n    tar_target(\n        canton_saved_plot,\n        save_plot(\"fig/canton.png\", canton_plot),\n        format = \"file\"\n    ),\n\n    tar_target(\n        commune_saved_plot,\n        save_plot(\"fig/commune.png\", commune_plot),\n        format = \"file\"\n    )\n\n\n)\n\nBecause this is a {targets} script, this needs to be saved inside a file called _targets.R. Each tar_target() object defines a target that will get built once we run the pipeline. The first element of tar_target() is the name of the target, the second line a call to a function that returns the first element and in the last three targets format = “file” is used to indicate that this target saves an output to disk (as a file).\n\n\nThe fourth line of the script sources a script called functions.R. This script should be placed next to the _targets.R script and should look like this:\n\n# clean_unemp() is a function inside a package I made. Because I don't want you to install\n# the package if you're following along, I'm simply sourcing it:\n\nsource(\"https://raw.githubusercontent.com/b-rodrigues/myPackage/main/R/functions.R\")\n\n# The cleaned data is also available in that same package. But again, because I don't want you\n# to install a package just for a blog post, here is the script to clean it.\n# Don't waste time trying to understand it, it's very specific to the data I'm using\n# to illustrate the concept of reproducible analytical pipelines. Just accept this data \n# as given.\n\n# This is a helper function to clean the data\nclean_data &lt;- function(x){\n  x %&gt;%\n    janitor::clean_names() %&gt;%\n    mutate(level = case_when(\n             grepl(\"Grand-D.*\", commune) ~ \"Country\",\n             grepl(\"Canton\", commune) ~ \"Canton\",\n             !grepl(\"(Canton|Grand-D.*)\", commune) ~ \"Commune\"\n           ),\n           commune = ifelse(grepl(\"Canton\", commune),\n                            stringr::str_remove_all(commune, \"Canton \"),\n                            commune),\n           commune = ifelse(grepl(\"Grand-D.*\", commune),\n                            stringr::str_remove_all(commune, \"Grand-Duche de \"),\n                            commune),\n           ) %&gt;%\n    select(year,\n           place_name = commune,\n           level,\n           everything())\n}\n\n# This reads in the data.\nget_data &lt;- function(){\n  list(\n    \"https://raw.githubusercontent.com/b-rodrigues/modern_R/master/datasets/unemployment/unemp_2013.csv\",\n    \"https://raw.githubusercontent.com/b-rodrigues/modern_R/master/datasets/unemployment/unemp_2014.csv\",\n    \"https://raw.githubusercontent.com/b-rodrigues/modern_R/master/datasets/unemployment/unemp_2015.csv\",\n    \"https://raw.githubusercontent.com/b-rodrigues/modern_R/master/datasets/unemployment/unemp_2016.csv\",\n  ) |&gt;\n    purrr::map_dfr(readr::read_csv) %&gt;%\n    purrr::map_dfr(clean_data)\n}\n\n# This plots the data\nmake_plot &lt;- function(data){\n  ggplot(data) +\n    geom_col(\n      aes(\n        y = active_population,\n        x = year,\n        fill = place_name\n      )\n    ) +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank())\n}\n\n# This saves plots to disk\nsave_plot &lt;- function(save_path, plot){\n  ggsave(save_path, plot)\n  save_path\n}\n\nWhat you could do instead of having a functions.R script that you source like this, is put everything inside a package that you then host on Github. But that’s outside the scope of this blog post. Put these scripts inside a folder, open an R session inside that folder, and run the pipeline using targets::tar_make():\n\ntargets::tar_make()\n/\nAttaching package: ‘dplyr’\n\nThe following objects are masked from ‘package:stats’:\n\n    filter, lag\n\nThe following objects are masked from ‘package:base’:\n\n    intersect, setdiff, setequal, union\n\n• start target unemp_data\n• built target unemp_data [1.826 seconds]\n• start target canton_data\n• built target canton_data [0.038 seconds]\n• start target lux_data\n• built target lux_data [0.034 seconds]\n• start target commune_data\n• built target commune_data [0.043 seconds]\n• start target canton_plot\n• built target canton_plot [0.007 seconds]\n• start target lux_plot\n• built target lux_plot [0.006 seconds]\n• start target commune_plot\n• built target commune_plot [0.003 seconds]\n• start target canton_saved_plot\nSaving 7 x 7 in image\n• built target canton_saved_plot [0.425 seconds]\n• start target luxembourg_saved_plot\nSaving 7 x 7 in image\n• built target luxembourg_saved_plot [0.285 seconds]\n• start target commune_saved_plot\nSaving 7 x 7 in image\n• built target commune_saved_plot [0.291 seconds]\n• end pipeline [3.128 seconds]\n\nYou can now see a fig/ folder in the root of your project with the plots. Sweet."
  },
  {
    "objectID": "posts/2022-11-19-raps.html#making-sure-this-is-reproducible",
    "href": "posts/2022-11-19-raps.html#making-sure-this-is-reproducible",
    "title": "Reproducibility with Docker and Github Actions for the average R enjoyer",
    "section": "\nMaking sure this is reproducible\n",
    "text": "Making sure this is reproducible\n\n\nNow what we would like to do is make sure that this pipeline will, for the same inputs, returns the same outputs FOREVER. If I’m running this in 10 years on R version 6.9, I want the exact same plots back. So the idea is to actually never run this on whatever version of R will be available in 10 years, but keep rerunning it, ad vitam æternam on whatever environment I’m using now to type this blog post. So for this, I’m going to use Docker.\n\n\n(If, like me, you’re an average functional programming enjoyer, then this means getting rid of the hidden state of our pipeline. The hidden global state is the version of R and packages used to run the pipeline.)\n\n\nWhat’s Docker? Docker is a way to run a Linux computer inside your computer (Linux or not). That computer is not real, but real enough for our purposes. Ever heard of virtual machines? Basically the same thing, but without the overhead of actually setting up and running a virtual machine.\n\n\nYou can write a simple text file that defines what your machine is, and what it should run. Thankfully, we don’t need to start from scratch and can use the amazing Rocker project that provides many, many, images for us to start playing with Docker. What’s a Docker image? A definition of a computer/machine. Which is a text file. Don’t ask why it’s called an image. Turns out the Rocker project has a page specifically on reproducibility. Their advice can be summarised as follows: if you’re aiming at setting up a reproducible pipeline, use a version-stable image. This means that if you start from such an image, the exact same R version will always be used to run your pipeline. Plus, the RStudio Public Package Manager (RSPM), frozen at a specific date, will be used to fetch the packages needed for your pipeline. So, not only is the R version frozen, but the exact same packages will always get installed (as long as the RSPM exists, hopefully for a long time).\n\n\nNow, I’ve been talking about a script that defines an image for some time. This script is called a Dockerfile, and you can find the versioned Dockerfiles here. As you can see there are many Dockerfiles, each defining a Linux machine and with several things pre-installed. Let’s take a look at the image r-ver_4.2.1.Dockerfile. What’s interesting here are the following lines (let’s ignore the others):\n\n8 ENV R_VERSION=4.2.1\n\n16 ENV CRAN=https://packagemanager.rstudio.com/cran/__linux__/focal/2022-10-28\n\nThe last characters of that link are a date. This means that if you use this for your project, packages will be downloaded as they were on the October 28th, 2022, and the R version used will always be version 4.2.1.\n\n\nOk so, how do we use this?\n\n\nLet’s add a Dockerfile to our project. Simply create a text file called Dockerfile and add the following lines in it:\n\nFROM rocker/r-ver:4.2.1\n\nRUN R -e \"install.packages(c('dplyr', 'purrr', 'readr', 'stringr', 'ggplot2', 'janitor', 'targets'))\"\n\nRUN mkdir /home/fig\n\nCOPY _targets.R /_targets.R\n\nCOPY functions.R /functions.R\n\nCMD R -e \"targets::tar_make()\"\n\nBefore continuing, I should explain what the first line does:\n\nFROM rocker/r-ver:4.2.1\n\nThis simply means that we are using the image from before as a base. This image is itself based on Ubuntu Focal, see its first line:\n\nFROM ubuntu:focal\n\nUbuntu is a very popular, likely the most popular, Linux distribution. So the versioned image is built on top of Ubuntu 20.04 codenamed Focal Fossa (which is a long term support release), and our image is built on top of that. To make sense of all this, you can take a look at the table here.\n\n\nSo now that we’ve written this Dockerfile, we need to build the image. This can be done inside a terminal with the following line:\n\ndocker build -t my_pipeline .\n\nThis tells Docker to build an image called my_pipeline using the Dockerfile in the current directory (hence the .).\n\n\nBut, here’s what happens when we try to run the pipeline (I’ll be showing the command to run the pipeline below):\n\n&gt; targets::tar_make()\nError in dyn.load(file, DLLpath = DLLpath, ...) : \n  unable to load shared object '/usr/local/lib/R/site-library/igraph/libs/igraph.so':\n  libxml2.so.2: cannot open shared object file: No such file or directory\nCalls: loadNamespace ... asNamespace -&gt; loadNamespace -&gt; library.dynam -&gt; dyn.load\nExecution halted\n\nWe get a nasty error message; apparently some library, libxml2.so cannot be found. So we need to change our Dockerfile, and add the following lines:\n\nFROM rocker/r-ver:4.2.1\n\nRUN apt-get update && apt-get install -y \\\n    libxml2-dev \\\n    libglpk-dev \\\n    libxt-dev\n\nRUN R -e \"install.packages(c('dplyr', 'purrr', 'readr', 'stringr', 'ggplot2', 'janitor', 'targets'))\"\n\nRUN mkdir /home/fig\n\nCOPY _targets.R /_targets.R\n\nCOPY functions.R /functions.R\n\nCMD R -e \"targets::tar_make()\"\n\nI’ve added these lines:\n\nRUN apt-get update && apt-get install -y \\\n    libxml2-dev \\\n    libglpk-dev \\\n    libxt-dev\n\nthis runs the apt-get update and apt-get install commands. Aptitude is Ubuntu’s package manager and is used to install software. The three pieces of software I installed will avoid further issues. libxml2-dev is for the error message I’ve pasted here, while the other two avoid further error messages. One last thing before we rebuild th image: we actually need to change the _targets.R file a bit. Let’s take a look at our Dockerfile again, there’s three lines I haven’t commented:\n\nRUN mkdir /home/fig\n\nCOPY _targets.R /_targets.R\n\nCOPY functions.R /functions.R\n\nThe first line creates the fig/ folder in the home/ directory, and the COPY statements copy the files into the Docker image, so that they’re actually available inside the Docker. I also need to tell _targets to save the figures into the home/fig folder. So simply change the last three targets from this:\n\ntar_target(\n        luxembourg_saved_plot,\n        save_plot(\"fig/luxembourg.png\", lux_plot),\n        format = \"file\"\n    ),\n\n    tar_target(\n        canton_saved_plot,\n        save_plot(\"fig/canton.png\", canton_plot),\n        format = \"file\"\n    ),\n\n    tar_target(\n        commune_saved_plot,\n        save_plot(\"fig/commune.png\", commune_plot),\n        format = \"file\"\n    )\n\nto this:\n\ntar_target(\n        luxembourg_saved_plot,\n        save_plot(\"/home/fig/luxembourg.png\", lux_plot),\n        format = \"file\"\n    ),\n\n    tar_target(\n        canton_saved_plot,\n        save_plot(\"/home/fig/canton.png\", canton_plot),\n        format = \"file\"\n    ),\n\n    tar_target(\n        commune_saved_plot,\n        save_plot(\"/home/fig/commune.png\", commune_plot),\n        format = \"file\"\n    )\n\nOk, so now we’re ready to rebuild the image:\n\ndocker build -t my_pipeline .\n\nand we can now run it:\n\ndocker run --rm --name my_pipeline_container -v /path/to/fig:/home/fig my_pipeline\n\ndocker run runs a container based on the image you defined. –rm means that the container should be removed once it stops, –name gives it a name, here my_pipeline_container (this is not really needed here, because the container stops and gets removed once it’s done running), and -v mounts a volume, which is a fancy way of saying that the folder /path/to/fig/, which is a real folder on your computer, is a portal to the folder /home/fig/ (which we created in the Dockerfile). This means that whatever gets saved inside home/fig/ inside the Docker container gets also saved inside /path/to/fig on your computer. The last argument my_pipeline is simply the Docker image you built before. You should see the three plots magically appearing in /path/to/fig once the container is done running. The other neat thing is that you can upload this image to Docker Hub, for free (to know how to do this, check out this section of the course I teach on this). This way, if other people want to run it, they could do so by running the same command as above, but replacing my_pipeline by your_username_on_docker_hub/image_name_on_docker_hub. People could even create new images based on this image, by using FROM your_username_on_docker_hub/image_name_on_docker_hub at the beginning of their Dockerfiles. If you want an example of a pipeline that starts off from such an image, you can check out this repository. This repository tells you how can run a reproducible pipeline by simply cloning it, building the image (which only takes a few seconds because all software is already installed in the image that I start from) and then running it."
  },
  {
    "objectID": "posts/2022-11-19-raps.html#running-this-on-github-actions",
    "href": "posts/2022-11-19-raps.html#running-this-on-github-actions",
    "title": "Reproducibility with Docker and Github Actions for the average R enjoyer",
    "section": "\nRunning this on Github Actions\n",
    "text": "Running this on Github Actions\n\n\nOk, so now, let’s suppose that we got an image on Docker Hub that contains all the dependencies required for our pipeline, and let’s say that we create a Github repository containing a Dockerfile that pulls from this image, as well as the required scripts for our pipeline. Basically, this is what I did here (the same repository that I linked above already). If you take a look at the first line of the Dockerfile in it, you will see this:\n\nFROM brodriguesco/r421_rap:version1\n\nThis means that the image that gets built from this Dockerfile starts off from this image I’ve uploaded on Docker Hub, this way each time the image gets rebuilt, because the dependencies are already installed, it’s going to be fast. Ok, so now what I want is the following: each time I change a file, be it the Dockerfile, or the _targets.R script, commit my changes and push them, I want Github Actions to rebuild the image, run the container, and give me the plots back.\n\n\nThis means that I can focus on coding, Github Actions will take care of the boring stuff.\n\n\nTo do this, start by creating a .github/ directory on the root of your Github repo, and inside of it, add a .workflows directory, and add a file in it called something like docker-build-run.yml. What matters is that this file ends in .yml. This is what the file I use to define the actions I’ve described above looks like:\n\nname: Docker Image CI\n\non:\n  push:\n    branches: [ \"main\" ]\n  pull_request:\n    branches: [ \"main\" ]\n\njobs:\n\n  build:\n\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Build the Docker image\n      run: docker build -t my-image-name .\n    - name: Docker Run Action\n      run: docker run --rm --name my_pipeline_container -v /github/workspace/fig/:/home/graphs/:rw my-image-name\n    - uses: actions/upload-artifact@v3\n      with:\n        name: my-figures\n        path: /github/workspace/fig/\n\nThe first line defines the name of the job, here Docker Image CI. The lines state when this should get executed: whenever there’s a push on or pull request on main. The job itself runs on an Ubuntu VM (so Github Actions starts an Ubuntu VM that will pull a Docker image itself running Ubuntu…). Then, there’s the steps statement. For now, let’s focus on the run statements inside steps, because these should be familiar:\n\nrun: docker build -t my-image-name .\n\nand:\n\nrun: docker run --rm --name my_pipeline_container -v /github/workspace/fig/:/home/graphs/:rw my-image-name\n\nThe only new thing here, is that the path “on our machine” has been changed to /github/workspace/. This is the home directory of your repository, so to speak. Now there’s the uses keyword that’s new:\n\nuses: actions/checkout@v3\n\nThis action checkouts your repository inside the VM, so the files in the repo are available inside the VM. Then, there’s this action here:\n\n- uses: actions/upload-artifact@v3\n  with:\n    name: my-figures\n    path: /github/workspace/fig/\n\nThis action takes what’s inside /github/workspace/fig/ (which will be the output of our pipeline) and makes the contents available as so-called “artifacts”. Artifacts are the outputs of your workflow, and will be made available as zip files for download. In our case, as stated, the output of the pipeline. It is thus possible to rerun our workflow in the cloud. This has the advantage that we can now focus on simply changing the code, and not have to bother with useless manual steps. For example, let’s change this target in the _targets.R file:\n\ntar_target(\n    commune_data,\n    clean_unemp(unemp_data,\n                place_name_of_interest = c(\"Luxembourg\", \"Dippach\", \n                                           \"Wiltz\", \"Esch/Alzette\", \n                                           \"Mersch\", \"Dudelange\"),\n                col_of_interest = active_population)\n)\n\n\nI’ve added “Dudelange” to the list of communes to plot. Let me push this change to the repo now, and let’s take a look at the artifacts. The video below summarises the process:\n\n\n\n\n\n\n\n\n\nAs you can see in the video, the _targets.R script was changed, and the changes pushed to Github. This triggered the action we’ve defined before. The plots (artifacts) get refreshed, and we can download them. We see then that Dudelange was added in the communes.png plot!\n\n\nIf you enjoyed this blog post and want more of this, I wrote a whole ebook on it."
  },
  {
    "objectID": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html",
    "href": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html",
    "title": "Historical newspaper scraping with {tesseract} and R",
    "section": "",
    "text": "I have been playing around with historical newspapers data for some months now. The “obvious” type of analysis to do is NLP, but there is also a lot of numerical data inside historical newspapers. For instance, you can find these tables that show the market prices of the day in the L’Indépendance Luxembourgeoise:\nI wanted to see how easy it was to extract these tables from the newspapers and then make it available. It was a bit more complicated than anticipated."
  },
  {
    "objectID": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html#download-data",
    "href": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html#download-data",
    "title": "Historical newspaper scraping with {tesseract} and R",
    "section": "\nDownload data\n",
    "text": "Download data\n\n\nThe first step is to download the data. For this, I have used the code @yvesmaurer which you can find here. This code makes it easy to download individual pages of certain newspapers, for instance this one. The pages I am interested in are pages 3, which contain the tables I need, for example here. @yvesmaurer’s code makes it easy to find the download links, which look like this: https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F1drtkj%2Fpages%2F3/full/full/0/default.jpg. It is also possible to crop the image by changing some parameters like so. This is helpful, because it makes the image smaller. The tables I’m interested in are always in the last column, so I can can use this feature to get smaller images. However, not every issue contains these tables, and I only want to download the ones that have these tables. So I wrote the following code to download the images I’m interested in:\n\nlibrary(tidyverse)\nlibrary(magick)\nlibrary(tesseract)\nlibrary(furrr)\n\ndownload_image &lt;- function(link){\n\n    print(link)\n\n    isok &lt;- image_read(link) %&gt;%\n        ocr(engine = \"fra\") %&gt;%\n        str_to_lower() %&gt;%\n        str_detect(\"marché de luxembourg\")\n\n    if(isok){\n        date_link &lt;- link %&gt;%\n            str_replace(\"pages%2f3\", \"pages%2f1\") %&gt;%\n            str_replace(\"pct:74,0,100,100\", \"pct:76,1,17,5\")\n\n        paper_date &lt;- image_read(date_link) %&gt;%\n            ocr(engine = \"fra\") %&gt;%\n            str_squish() %&gt;%\n            str_remove(\"%\") %&gt;%\n            str_remove(\"&\") %&gt;%\n            str_remove(\"/\")\n\n        ark &lt;- link %&gt;%\n            str_sub(53, 60)\n\n        download.file(link, paste0(\"indep_pages/\", ark, \"-\", paper_date, \".jpg\"))\n    } else {\n        NULL\n        }\n}\n\nThis code only downloads an image if the ocr() from the {tesseract} (which does, you guessed it, OCR) detects the string “marché de luxembourg” which is the title of the tables. This is a bit extreme, because if a single letter cannot be correctly detected by the OCR, the page will not be downloaded. But I figured that if this string could not be easily recognized, this would be a canary telling me that the text inside the table would also not be easily recognized. So it might be extreme, but my hope was that it would make detecting the table itself easier. Turned out it wasn’t so easy, but more on this later."
  },
  {
    "objectID": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html#preparing-images",
    "href": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html#preparing-images",
    "title": "Historical newspaper scraping with {tesseract} and R",
    "section": "\nPreparing images\n",
    "text": "Preparing images\n\n\nNow that I have the images, I will prepare them to make character recognition easier. To do this, I’m using the {magick} package:\n\nlibrary(tidyverse)\nlibrary(magick)\nlibrary(tesseract)\nlibrary(furrr)\n\nprepare_image &lt;- function(image_path){\n    image &lt;- image_read(image_path)\n\n    image &lt;- image %&gt;%\n        image_modulate(brightness = 150) %&gt;%\n        image_convolve('DoG:0,0,2', scaling = '1000, 100%') %&gt;%\n        image_despeckle(times = 10)\n\n    image_write(image, paste0(getwd(), \"/edited/\", str_remove(image_path, \".jpg\"), \"edited.jpg\"))\n}\n\n\nimage_paths &lt;- dir(path = \"indep_pages\", pattern = \"*.jpg\", full.names = TRUE)\n\nplan(multiprocess, workers = 8)\n\nimage_paths %&gt;%\n    future_map(prepare_image)\n\nThe picture below shows the result:\n\n\n\n\n\nNow comes the complicated part, which is going from the image above, to the dataset below:\n\ngood_fr,good_en,unit,market_date,price,source_url\nFroment,Wheat,hectolitre,1875-08-28,23,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg\nMétail,Meslin,hectolitre,1875-08-28,21,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg\nSeigle,Rye,hectolitre,1875-08-28,15,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg\nOrge,Barley,hectolitre,1875-08-28,16,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg\nOrge mondé,Pot Barley,kilogram,1875-08-28,0.85,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg\nOrge perlé,Pearl barley,kilogram,1875-08-28,0.8,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg\nAvoine,Oats,hectolitre,1875-08-28,8.5,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg\nPois,Peas,hectolitre,1875-08-28,NA,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg"
  },
  {
    "objectID": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html#ocr-with-tesseract",
    "href": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html#ocr-with-tesseract",
    "title": "Historical newspaper scraping with {tesseract} and R",
    "section": "\nOCR with {tesseract}\n",
    "text": "OCR with {tesseract}\n\n\nThe first step was to get the date. For this, I have used the following function, which will then be used inside another function, which will extract the data and prices.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(magick)\nlibrary(tesseract)\nlibrary(furrr)\nlibrary(janitor)\n\nis_empty_line &lt;- function(line){\n    ifelse(line == \"\", TRUE, FALSE)\n}\n\nSys.setlocale('LC_TIME', \"fr_FR\")\n\nget_date &lt;- function(string, annee){\n\n    liste_mois &lt;- c(\"janvier\", \"février\", \"mars\", \"avril\", \"mai\", \"juin\", \"juillet\",\n                    \"août\", \"septembre\", \"octobre\", \"novembre\", \"décembre\")\n\n    raw_date &lt;- string %&gt;%\n      str_to_lower() %&gt;%\n        str_remove_all(\"\\\\.\") %&gt;%\n        str_extract(\"\\\\d{1,2} .{3,9}(\\\\s+)?\\\\d{0,4}\") %&gt;%\n        str_split(\"\\\\s+\", simplify = TRUE)\n\n    if(ncol(raw_date) == 2){\n        raw_date &lt;- cbind(raw_date, \"annee\")\n    }\n\n    raw_date[1, 3] &lt;- annee\n\n    raw_date &lt;- str_to_lower(raw_date[1:1, 1:3])\n\n    long_month &lt;- case_when(\n      raw_date[2] == \"janv\" ~ \"janvier\",\n      raw_date[2] == \"févr\" ~ \"février\",\n      raw_date[2] == \"sept\" ~ \"septembre\",\n      raw_date[2] == \"oct\" ~ \"octobre\",\n      raw_date[2] == \"nov\" ~ \"novembre\",\n      raw_date[2] == \"dec\" ~ \"décembre\",\n      TRUE ~ as.character(raw_date[2]))\n\n    raw_date[2] &lt;- long_month\n\n    is_it_date &lt;- as.Date(paste0(raw_date, collapse = \"-\"), format = \"%d-%b-%Y\") %&gt;%\n        is.na() %&gt;% `!`()\n\n    if(is_it_date){\n        return(as.Date(paste0(raw_date, collapse = \"-\"), format = \"%d-%b-%Y\"))\n    } else {\n        if(!(raw_date[2] %in% liste_mois)){\n            raw_date[2] &lt;- liste_mois[stringdist::amatch(raw_date[2], liste_mois, maxDist = 2)]\n            return(as.Date(paste0(raw_date, collapse = \"-\"), format = \"%d-%b-%Y\"))\n        }\n    }\n}\n\nThis function is more complicated than I had hoped. This is because dates come in different formats. For example, there are dates written like this “21 Janvier 1872”, or “12 Septembre” or “12 sept.”. The biggest problem here is that sometimes the year is missing. I deal with this in the next function, which is again, more complicated than what I had hoped. I won’t go into details and explain every step of the function above, but the idea is to extract the data from the raw text, replace abbreviated months with the full month name if needed, and then check if I get a valid date. If not, I try my luck with stringdist::amatch(), to try to match, say “jonvier” with “janvier”. This is in case the OCR made a mistake. I am not very happy with this solution, because it is very approximative, but oh well.\n\n\nThe second step is to get the data. I noticed that the rows stay consistent, but do change after June 1st 1876. So I simply hardcoded the goods names, and was only concerned with extracting the prices. I also apply some manual corrections inside the function; mainly dates that were wrongly recognized by the OCR engine, and which were causing problems. Again, not an optimal solution, the other alternative was to simply drop this data, which I did not want to do. Here is the function:\n\nextract_table &lt;- function(image_path){\n\n  image &lt;- image_read(image_path)\n\n  annee &lt;- image_path %&gt;%\n    str_extract(\"187\\\\d\")\n\n  ark &lt;- image_path %&gt;%\n    str_sub(22, 27)\n\n  source_url &lt;- str_glue(\"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F{ark}%2Fpages%2F1/full/full/0/default.jpg\",\n                         ark = ark)\n\n  text &lt;- ocr(image, engine = \"fra\")\n\n    text &lt;- text %&gt;%\n      str_split(\"\\n\") %&gt;%\n      unlist %&gt;%\n      str_squish() %&gt;%\n      str_remove_all(\"^.{1,10}$\") %&gt;%\n      discard(is_empty_line) %&gt;%\n      str_replace(\"Mercuriale du \\\\+ Nov. 1831.\", \"Mercuriale du 4 Nov. 1831.\") %&gt;%\n      str_replace(\"….u .T juillet.\", \"du 7 juillet\") %&gt;%\n      str_replace(\"octobré\", \"octobre\") %&gt;%\n      str_replace(\"AT octobre\", \"17 octobre\") %&gt;% # correction for \"f8g6kq8-18  LUNDI 19 OCTOBRÉ 1874. BUREAUX de fa RÉDACTIGedited.jpg\"\n      str_replace(\"T norembre\", \"7 novembre\") %&gt;%  # correction for fcrhrn5-LE 8  LUNDI 9 NOVEMBRE 1874 BUREAUX de la RÉDedited.jpg\n      str_replace(\"À oc demain 5\", \"27 mai\") %&gt;% # correction for fd61vzp-MARDI 50. MAI 1876 BUREAUX de la. RED, n VE DE L’ADMINISTRAedited.jpg\n      str_replace(\"G\", \"6\") %&gt;%\n      str_replace(\"Hercariale du 80 nov. 1872,\", \"du 30 novembre 1872\") %&gt;%\n      str_replace(\"….u .T juillet.\", \"du 7 juillet\") %&gt;%\n      str_replace(\"Rs ne its du 28-octobré.: :!: :\", \"28 octobre\") %&gt;%\n      str_replace(\"De routes due 98-juilléle. à eat\", \"28 juillet\") %&gt;%\n      str_replace(\"\\\\| Mereariale dn 14 dre. 1872,\", \"14 décembre 1872\")\n\n\n  start &lt;- text %&gt;%\n    str_which(\"MARCH(É|E).*D(E|É).*LUXEMBOUR(G|6)\") + 2\n\n  start &lt;- ifelse(is_empty(start), str_which(text, \".*D.*UXEM.*\") + 2, start)\n\n  end &lt;- start + 40\n\n  pricing_date &lt;- text[start - 1] %&gt;%\n    str_remove(\"%\") %&gt;%\n    str_remove(\"er\") %&gt;%\n    str_remove(\"\\\\.+\") %&gt;%\n    str_remove(\"\\\\*\") %&gt;%\n    str_remove(\"®\") %&gt;%\n    str_remove(\":\") %&gt;%\n    str_remove(\"\\\\?\") %&gt;%\n    str_replace(\"\\\\$\", \"9\") %&gt;%\n    str_remove(\"°\") %&gt;%\n    str_replace(\"‘du 14août.. - ; En\", \"14 août\") %&gt;%\n    str_replace(\"OP PE CN AP PP\", \"du 28 juin\") %&gt;%\n    str_replace(\"‘ du 81 janvi Le\", \"31 janvier\") %&gt;%\n    str_replace(\"\\\\| \\\\| du AT août\", \"17 août\") %&gt;%\n    str_replace(\"Su”  du 81 juillet. L\", \"31 juillet\") %&gt;%\n    str_replace(\"0 du 29 avril \\\" \\\\|\", \"29 avril\") %&gt;%\n    str_replace(\"LU 0 du 28 ail\", \"28 avril\") %&gt;%\n    str_replace(\"Rs ne its du 28-octobre :!: :\", \"23 octobre\") %&gt;%\n    str_replace(\"7 F \\\\|  du 13 octobre LA LOTS\", \"13 octobre\") %&gt;%\n    str_replace(\"À. du 18 juin UT ET\", \"13 juin\")\n\n\n  market_date &lt;- get_date(pricing_date, annee)\n\n  items &lt;- c(\"Froment\", \"Métail\", \"Seigle\", \"Orge\", \"Orge mondé\", \"Orge perlé\", \"Avoine\", \"Pois\", \"Haricots\",\n             \"Lentilles\", \"Pommes de terre\", \"Bois de hêtre\", \"Bois de chêne\", \"Beurre\", \"Oeufs\", \"Foin\",\n             \"Paille\", \"Viande de boeuf\", \"Viande de vache\", \"Viande de veau\", \"Viande de mouton\",\n             \"Viande fraîche de cochon\", \"Viande fumée de cochon\", \"Haricots\", \"Pois\", \"Lentilles\",\n             \"Farines de froment\", \"Farines de méteil\", \"Farines de seigle\")\n\n  items_en &lt;- c(\"Wheat\", \"Meslin\", \"Rye\", \"Barley\", \"Pot Barley\", \"Pearl barley\", \"Oats\", \"Peas\", \"Beans\",\n    \"Lentils\", \"Potatoes\", \"Beech wood\", \"Oak wood\", \"Butter\", \"Eggs\", \"Hay\", \"Straw\", \"Beef meat\",\n    \"Cow meat\", \"Veal meat\", \"Sheep meat\", \"Fresh pig meat\", \"Smoked pig meat\", \"Beans\", \"Peas\",\n    \"Lentils\", \"Wheat flours\", \"Meslin flours\", \"Rye flours\")\n\n\n  unit &lt;- c(\"hectolitre\", \"hectolitre\", \"hectolitre\", \"hectolitre\", \"kilogram\", \"kilogram\", \"hectolitre\",\n            \"hectolitre\", \"hectolitre\", \"hectolitre\", \"hectolitre\", \"stere\", \"stere\", \"kilogram\", \"dozen\",\n            \"500 kilogram\", \"500 kilogram\", \"kilogram\", \"kilogram\", \"kilogram\", \"kilogram\", \"kilogram\",\n            \"kilogram\", \"litre\", \"litre\", \"litre\", \"kilogram\", \"kilogram\", \"kilogram\")\n\n  # starting with june 1876, the order of the items changes\n  items_06_1876 &lt;- c(\"Froment\", \"Métail\", \"Seigle\", \"Orge\", \"Avoine\", \"Pois\", \"Haricots\", \"Lentilles\",\n                     \"Pommes de terre\", \"Farines de froment\", \"Farines de méteil\", \"Farines de seigle\", \"Orge mondé\",\n                     \"Beurre\", \"Oeufs\", \"Foins\", \"Paille\", \"Bois de hêtre\", \"Bois de chêne\", \"Viande de boeuf\", \"Viande de vache\",\n                     \"Viande de veau\", \"Viande de mouton\", \"Viande fraîche de cochon\", \"Viande fumée de cochon\")\n\n  items_06_1876_en &lt;- c(\"Wheat\", \"Meslin\", \"Rye\", \"Barley\", \"Oats\", \"Peas\", \"Beans\", \"Lentils\",\n                        \"Potatoes\", \"Wheat flours\", \"Meslin flours\", \"Rye flours\", \"Pot barley\",\n                        \"Butter\", \"Eggs\", \"Hay\", \"Straw\", \"Beechwood\", \"Oakwood\", \"Beef meat\", \"Cow meat\",\n                        \"Veal meat\", \"Sheep meat\", \"Fresh pig meat\", \"Smoked pig meat\")\n\n  units_06_1876 &lt;- c(rep(\"hectolitre\", 9), rep(\"kilogram\", 5), \"douzaine\", rep(\"500 kilogram\", 2),\n                     \"stere\", \"stere\", rep(\"kilogram\", 6))\n\n  raw_data &lt;- text[start:end]\n\n  prices &lt;- raw_data %&gt;%\n    str_replace_all(\"©\", \"0\") %&gt;%\n    str_extract(\"\\\\d{1,2}\\\\s\\\\d{2}\") %&gt;%\n    str_replace(\"\\\\s\", \"\\\\.\") %&gt;%\n    as.numeric\n\n  if(is.na(prices[1])){\n    prices &lt;- tail(prices, -1)\n  } else {\n    prices &lt;- prices\n  }\n\n  if(market_date &lt; as.Date(\"01-06-1876\", format = \"%d-%m-%Y\")){\n    prices &lt;- prices[1:length(items)]\n    tibble(\"good_fr\" = items, \"good_en\" = items_en, \"unit\" = unit, \"market_date\" = market_date,\n           \"price\" = prices, \"source_url\" = source_url)\n  } else {\n    prices &lt;- prices[1:length(items_06_1876_en)]\n    tibble(\"good_fr\" = items_06_1876, \"good_en\" = items_06_1876_en, \"unit\" = units_06_1876,\n           \"market_date\" = market_date, \"price\" = prices, \"source_url\" = source_url)\n  }\n}\n\nAs I wrote previously, I had to deal with the missing year in the date inside this function. To do that, I extracted the year from the name of the file, and pasted it then into the date. The file name contains the data because the function in the function that downloads the files I also performed OCR on the first page, to get the date of the newspaper issue. The sole purpose of this was to get the year. Again, the function is more complex than what I hoped, but it did work well overall. There are still mistakes in the data, for example sometimes the prices are in the wrong order; meaning that they’re “shifted”, for example instead of the prices for eggs, I have the prices of the good that comes next. So obviously be careful if you decide to analyze the data, and double-check if something seems weird. I have made the data available on Luxembourg Open Data Portal, here."
  },
  {
    "objectID": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html#analyzing-the-data",
    "href": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html#analyzing-the-data",
    "title": "Historical newspaper scraping with {tesseract} and R",
    "section": "\nAnalyzing the data\n",
    "text": "Analyzing the data\n\n\nAnd now, to the fun part. I want to know what was the price of smoked pig meat, and how it varied through time:\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(brotools)\nmarket_price &lt;- read_csv(\"https://download.data.public.lu/resources/digitised-luxembourg-historical-newspapers-journaux-historiques-luxembourgeois-numerises/20190407-183605/market-price.csv\")\n## Parsed with column specification:\n## cols(\n##   good_fr = col_character(),\n##   good_en = col_character(),\n##   unit = col_character(),\n##   market_date = col_date(format = \"\"),\n##   price = col_double(),\n##   source_url = col_character()\n## )\nmarket_price %&gt;%\n    filter(good_en == \"Smoked pig meat\") %&gt;%\n    ggplot(aes(x = market_date, y = price)) +\n    geom_line(aes(group = 1), colour = \"#82518c\") + \n    theme_blog() + \n    labs(title = \"Prices of smoked pig meat at the Luxembourg-City market in the 19th century\")\n## Warning: Removed 2 rows containing missing values (geom_path).\n\n\n\n\nAs you can see, there is a huge spike somewhere in 1874. Maybe there was a very severe smoked pig meat shortage that caused the prices to increase dramatically, but the more likely explanation is that there was some sort of mistake, either in the OCR step, or when I extracted the prices, and somehow that particular price of smoked pig meat is actually the price of another, more expensive good.\n\n\nSo let’s only consider prices that are below, say, 20 franks, which is already very high:\n\nmarket_price %&gt;%\n    filter(good_en == \"Smoked pig meat\") %&gt;%\n    filter(price &lt; 20) %&gt;% \n    ggplot(aes(x = market_date, y = price)) +\n    geom_line(aes(group = 1), colour = \"#82518c\") + \n    theme_blog() + \n    labs(title = \"Prices of smoked pig meat at the Luxembourg-City market in the 1870s\")\n\n\n\n\nNow, some prices are very high. Let’s check if it’s a mistake:\n\nmarket_price %&gt;% \n    filter(good_en == \"Smoked pig meat\") %&gt;% \n    filter(between(price, 5, 20)) %&gt;% \n    pull(source_url)\n## [1] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fbs2fs6%2Fpages%2F1/full/full/0/default.jpg\"\n## [2] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fd61vzp%2Fpages%2F1/full/full/0/default.jpg\"\n## [3] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fjdwb6m%2Fpages%2F1/full/full/0/default.jpg\"\n## [4] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fng14m3%2Fpages%2F1/full/full/0/default.jpg\"\n## [5] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fw9jdrb%2Fpages%2F1/full/full/0/default.jpg\"\n\nIf you go to the first url, you will land on the first page of the newspaper. To check the table, you need to check the third page, by changing this part of the url “pages%2F1” to this “pages%2F3”.\n\n\nYou will then find the following:\n\n\n\n\n\nAs you can see, the price was 2.5, but the OCR returned 7.5. This is a problem that is unavoidable with OCR; there is no way of knowing a priori if characters were not well recognized. It is actually quite interesting how the price for smoked pig meat stayed constant through all these years. A density plot shows that most prices were around 2.5:\n\nmarket_price %&gt;% \n    filter(good_en == \"Smoked pig meat\") %&gt;% \n    filter(price &lt; 20) %&gt;% \n    ggplot() + \n    geom_density(aes(price), colour = \"#82518c\") + \n    theme_blog()\n\n\n\n\nWhat about another good, say, barley?\n\nmarket_price %&gt;%\n    filter(good_en == \"Barley\") %&gt;%\n    ggplot(aes(x = market_date, y = price)) +\n    geom_line(aes(group = 1), colour = \"#82518c\") + \n    theme_blog() + \n    labs(title = \"Prices of barley at the Luxembourg-City market in the 1870s\")\n\n\n\n\nHere again, we see some very high spikes, most likely due to errors. Let’s try to limit the prices to likely values:\n\nmarket_price %&gt;%\n    filter(good_en == \"Barley\") %&gt;%\n    filter(between(price, 10, 40)) %&gt;%\n    ggplot(aes(x = market_date, y = price)) +\n    geom_line(aes(group = 1), colour = \"#82518c\") + \n    theme_blog() + \n    labs(title = \"Prices of barley at the Luxembourg-City market in the 1870s\")\n\n\n\nmarket_price %&gt;% \n    filter(good_en == \"Barley\") %&gt;% \n    ggplot() + \n    geom_density(aes(price), colour = \"#82518c\") + \n    theme_blog()\n## Warning: Removed 39 rows containing non-finite values (stat_density).\n\n\n\n\nLet’s finish this with one of my favourite legume, lentils:\n\nmarket_price %&gt;%\n    filter(good_en == \"Lentils\") %&gt;%\n    ggplot(aes(x = market_date, y = price)) +\n    geom_line(aes(group = 1), colour = \"#82518c\") + \n    theme_blog() + \n    labs(title = \"Prices of lentils at the Luxembourg-City market in the 1870s\")\n\n\n\nmarket_price %&gt;% \n    filter(good_en == \"Lentils\") %&gt;% \n    ggplot() + \n    geom_density(aes(price), colour = \"#82518c\") + \n    theme_blog()\n## Warning: Removed 79 rows containing non-finite values (stat_density).\n\n\n\n\nAll these 0’s might be surprising, but in most cases, they are actually true zeros! For example, you can check this issue. This very likely means that no lentils were available that day at the market. Let’s get rid of the 0s and other extreme values:\n\nmarket_price %&gt;%\n    filter(good_en == \"Lentils\") %&gt;%\n    filter(between(price, 1, 40)) %&gt;% \n    ggplot(aes(x = market_date, y = price)) +\n    geom_line(aes(group = 1), colour = \"#82518c\") + \n    theme_blog() + \n    labs(title = \"Prices of lentils at the Luxembourg-City market in the 1870s\")\n\n\n\n\nI would like to see if the spikes above 30 are errors or not:\n\nmarket_price %&gt;% \n    filter(good_en == \"Lentils\") %&gt;% \n    filter(between(price, 30, 40)) %&gt;% \n    pull(source_url)\n## [1] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F04mb5t%2Fpages%2F1/full/full/0/default.jpg\"\n## [2] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fb8zp31%2Fpages%2F1/full/full/0/default.jpg\"\n## [3] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fkzrj53%2Fpages%2F1/full/full/0/default.jpg\"\n## [4] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fs8sw2v%2Fpages%2F1/full/full/0/default.jpg\"\n## [5] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fsjptsk%2Fpages%2F1/full/full/0/default.jpg\"\n## [6] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fwk65b6%2Fpages%2F1/full/full/0/default.jpg\"\n\nThe price was recognized as being 35, and turns out it was correct as you can see here. This is quite interesting, because the average price was way lower than that:\n\nmarket_price %&gt;%\n    filter(good_en == \"Lentils\") %&gt;%\n    filter(between(price, 1, 40)) %&gt;% \n    summarise(mean_price = mean(price), \n              sd_price = sd(price))\n## # A tibble: 1 x 2\n##   mean_price sd_price\n##        &lt;dbl&gt;    &lt;dbl&gt;\n## 1       20.8     5.82\n\nI’m going to finish here; it was an interesting project, and I can’t wait for more newspapers to be digitized and OCR to work even better. There is a lot more historical data trapped in these newspapers that could provide a lot insights on Luxembourg’s society in the 19th century."
  },
  {
    "objectID": "posts/2022-05-21-heavy_syntax.html",
    "href": "posts/2022-05-21-heavy_syntax.html",
    "title": "Get packages that introduce unique syntax adopted less?",
    "section": "",
    "text": "I have this hypothesis that packages that introduce a unique syntax, or a workflow change, get adopted less by users, even if what these packages do is super useful. I’m going to discuss two examples of packages that I think are really, really useful, but sometimes I wonder how many R users use them, or would use them if they were aware these packages existed. I myself, only use one of them!\n\n\nThe first package is {typed} which introduces a type system for R. No more silent conversion to and from types without your knowing! If you don’t know what a type system is, consider the following:\n\nnchar(\"100000000\")\n## [1] 9\n\nyou get “9” back, no problem. But if you do:\n\nnchar(100000000)\n## [1] 5\n\nYou get 5 back… what in the Lord’s name happened here? What happened is that the number 100000000 was converted to a character implicitly. But because of all these 0’s, this is what happened:\n\nas.character(100000000)\n## [1] \"1e+08\"\n\nIt gets converted to a character alright, but scientific notation gets used! So yes, 1e+08 is 5 characters long… Ideally nchar() would at least warn you that this conversion is happening, or maybe even error. After all, it’s called nchar() not nnumeric() or whatever. (Thanks to @cararthompson for this!)\n\n\nA solution could be to write a wrapper around it:\n\nnchar2 &lt;- function(x, ...){\n  stopifnot(\"x is not a character\" = is.character(x))\n\n  nchar(x, ...)\n}\n\nNow this function is safe:\n\nnchar2(123456789)\n## [1] Error in nchar2(123456789) : x is not a character\n\n{typed} makes writing safe functions like this easier. Using {typed} you can write the wrapper like this:\n\nlibrary(typed, warn.conflicts = FALSE) \n\nstrict_nchar &lt;- ? function(x = ? Character(), ...){\n\n  nchar(x, ...)\n\n}\n\n{typed} introduces ? (masking the base ? function to read a function’s docs) allowing you to set the type the function’s arguments. It’s also possible to set the return type of the function:\n\nstrict_nchar &lt;- Integer() ? function(x = ? Character(), ...){\n\n  nchar(x, ...)\n\n}\nstrict_nchar(\"10000000\")\n## [1] 8\n\nThis is very useful if you want to write safe functions in a very concise and clean way.\n\n\nThe second kind of package I was thinking about are packages like {targets}, which force users to structure their projects in a very specific way. I really like {targets} and have been using it for quite some time. {targets} takes inspiration from build automation tools from the software development world and introduces the concept of build automation in R. If you’re a linux user, you’ve probably dealt with Makefiles (especially if you’ve been using linux for more than 10 years), and {targets} works in a similar way; by writing a script in which you define targets, these get built in a reproducible way. If you’d like to see it in action, take a look at this video of mine. As useful as it is, I can imagine that some potential users will end up not adopting it, because {targets} really does things in a very unique and different way. Most people do not know what build automation tools are, and the cost of adopting {targets} seems disproportionally higher to its benefits (but believe me, it is well worth the cost!).\n\n\nNow here’s the meat of the post: I think that packages like these, even though they’re very useful, get adopted less by users than other packages, that either:\n\n\n\ndo not introduce a unique way of doing things;\n\n\nfor which alternatives are available.\n\n\n\nThe reason, I believe, is that users do not feel comfortable adopting a unique syntax and way of doing things that impact their code so much, because if these libraries get abandoned, users will need to completely rewrite their scripts. And this is especially true when the two conditions above are not verified.\n\n\nTake {dplyr}: one could argue that it introduces both a unique syntax, and a very specific workflow/way of doing things:\n\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nmtcars %&gt;%\n filter(am == 0) %&gt;%\n group_by(cyl) %&gt;%\n summarise(mean_hp = mean(hp))\n## # A tibble: 3 × 2\n##     cyl mean_hp\n##   &lt;dbl&gt;   &lt;dbl&gt;\n## 1     4    84.7\n## 2     6   115. \n## 3     8   194.\n\nBut there are alternatives to it (a lot of {dplyr} functionality is covered by base functions already, and there’s also {data.table}), so IF {dplyr} would get abandoned by Rstudio (which will never happen, but let’s assume for the sake of argument), users could switch to {data.table}. Not so with more niche packages like the ones discussed above. Also, even {dplyr}’s unique syntax making heavy use of %&gt;% is not so unique anymore, since the release of R 4.1. A base approach to the above snippet would be:\n\nmtcars |&gt;\n  subset(am == 0) |&gt;\n  with(aggregate(hp, by = list(cyl), mean))\n##   Group.1         x\n## 1       4  84.66667\n## 2       6 115.25000\n## 3       8 194.16667\n\nBefore R 4.1, looking at {dplyr} chains felt like looking at a completely different language than base R, but now with the introduction of |&gt; not so anymore. The other thing packages like {dplyr} have going for them, even when they introduce a completely new syntax, and do not have any alternative like {ggplot2} (I don’t consider base plotting an alternative to {ggplot2}, because it works in a completely different way) is that they have big teams and/or companies behind them, like Rstudio. So users feel much more confident adopting such packages, than if they’re written by a very small team (sometimes even just one person).\n\n\nThe reason I’m thinking about all this, is because I recently released a package that raises all of the above red flags:\n\n\n\nnew syntax (makes heavy use of a new pipe %&gt;=%);\n\n\nforces a new workflow on users;\n\n\ndeveloped by a single dude in his free time who isn’t even a programmer (me).\n\n\n\nIf I was a potential interested user, I honestly don’t know if I’d adopt this package for anything critical. I might play around with it a bit, but using that in production? What if the author (me) gets sick of it after a few months/years? Even I, as the author, cannot guarantee today that this package will still be maintained in 2 years. So users that might have important stuff running which uses my package are now screwed. I think that the only way for such packages to succeed, is if a sizeable community gathers around it and if the team of developers expands, and ideally, if it gets backed by a company (like Rstudio with all their packages, or rOpenSci does for {targets}.\n\n\nTo be clear, I am NOT complaining about free and open source software: these problems also exist with proprietary software. If a company builds something and they decide to abandon it, that’s it, it’s over. If there are no alternatives to it, users are screwed just as well. And companies can also go bankrupt or change focus on other more profitable projects. At least with free and open source software, if the author of a package has had enough and decides to not maintain anymore, there is still the possibility of someone else taking it over, and this someone else might be a user! There is also the possibility of running old R version with older versions of packages, even if they’re abandoned, using Docker. So maybe it’s not so bad.\n\n\nWhat do you think? I’d be curious to hear your thoughts. Tell me what you think on this github issue I opened.\n\n\nOh and by the way, IF you’re using {chronicler} after reading this, really, thank you."
  },
  {
    "objectID": "posts/2020-09-05-tidytable.html",
    "href": "posts/2020-09-05-tidytable.html",
    "title": "Gotta go fast with “{tidytable}”",
    "section": "",
    "text": "I’m back in business! After almost 5 months of hiatus, during which I was very busy with my new job, and new house, I’m in a position where I can write again. To celebrate my comeback, I’ll introduce to you the {tidytable} package, which I learned about this week on Twitter.\n\n\n{tidytable} is a package that allows you to manipulate data.table objects with the speed of {data.table} and the convenience of the {tidyverse} syntax. My first reaction when I heard about {tidytable} was how is that different from {dtplyr}? Well, {dtplyr} focuses on providing a {data.table} backend for {dplyr}, while {tidytable} also allows you to use other {tidyverse} verbs on data.table objects, for instance some {tidyr} and {purrr} verbs.\n\n\nAnother very interesting feature of {tidytable} is that it supports {rlang}, which means that you can program with {tidytable}, which, as far as I know, is not possible with {dtplyr} (but fact-check me on that please).\n\n\nSo to summarise, the speed of {data.table} and the syntax of the {tidyverse}, plus verbs for {tidyr} and {purrr}? Sign me up!\n\n\nTo illustrate, I have downloaded a data set and wrote a function in both a {tidyverse} version and a {tidytable} version. Even though it is true that {tidytable}’s syntax is very much, almost the same as the regular {tidyverse} syntax, there are some minor differences. But more on that later. First, let’s get the data, which you can find here. Then, let’s load the needed packages:\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(tidytable)\nlibrary(readr)\n\nand let’s take a look at the data a little bit:\n\nenergy &lt;- read.csv(\"~/Downloads/energydata_complete.csv\")\n\nhead(energy)\n##                  date Appliances lights    T1     RH_1   T2     RH_2    T3\n## 1 2016-01-11 17:00:00         60     30 19.89 47.59667 19.2 44.79000 19.79\n## 2 2016-01-11 17:10:00         60     30 19.89 46.69333 19.2 44.72250 19.79\n## 3 2016-01-11 17:20:00         50     30 19.89 46.30000 19.2 44.62667 19.79\n## 4 2016-01-11 17:30:00         50     40 19.89 46.06667 19.2 44.59000 19.79\n## 5 2016-01-11 17:40:00         60     40 19.89 46.33333 19.2 44.53000 19.79\n## 6 2016-01-11 17:50:00         50     40 19.89 46.02667 19.2 44.50000 19.79\n##       RH_3       T4     RH_4       T5  RH_5       T6     RH_6       T7     RH_7\n## 1 44.73000 19.00000 45.56667 17.16667 55.20 7.026667 84.25667 17.20000 41.62667\n## 2 44.79000 19.00000 45.99250 17.16667 55.20 6.833333 84.06333 17.20000 41.56000\n## 3 44.93333 18.92667 45.89000 17.16667 55.09 6.560000 83.15667 17.20000 41.43333\n## 4 45.00000 18.89000 45.72333 17.16667 55.09 6.433333 83.42333 17.13333 41.29000\n## 5 45.00000 18.89000 45.53000 17.20000 55.09 6.366667 84.89333 17.20000 41.23000\n## 6 44.93333 18.89000 45.73000 17.13333 55.03 6.300000 85.76667 17.13333 41.26000\n##     T8     RH_8       T9  RH_9    T_out Press_mm_hg RH_out Windspeed Visibility\n## 1 18.2 48.90000 17.03333 45.53 6.600000       733.5     92  7.000000   63.00000\n## 2 18.2 48.86333 17.06667 45.56 6.483333       733.6     92  6.666667   59.16667\n## 3 18.2 48.73000 17.00000 45.50 6.366667       733.7     92  6.333333   55.33333\n## 4 18.1 48.59000 17.00000 45.40 6.250000       733.8     92  6.000000   51.50000\n## 5 18.1 48.59000 17.00000 45.40 6.133333       733.9     92  5.666667   47.66667\n## 6 18.1 48.59000 17.00000 45.29 6.016667       734.0     92  5.333333   43.83333\n##   Tdewpoint      rv1      rv2\n## 1       5.3 13.27543 13.27543\n## 2       5.2 18.60619 18.60619\n## 3       5.1 28.64267 28.64267\n## 4       5.0 45.41039 45.41039\n## 5       4.9 10.08410 10.08410\n## 6       4.8 44.91948 44.91948\n\nAs you can see, this data is wide, and not long. Variables, or features, T1 to T9 provide the temperature of 9 rooms, and RH_1 to RH_9 provide the humidity of the same 9 rooms.\n\n\nWhat if I’d like to make a plot of each room’s temperature throughout the year? In this format, it is not possible. So let’s reshape this a little bit:\n\nflat_energy &lt;- energy %&gt;% \n  pivot_longer(cols = matches(\"T\\\\d{1}\"), names_to = \"temperature\", values_to = \"temp_value\") %&gt;% \n  pivot_longer(cols = matches(\"RH_\\\\d{1}\"), names_to = \"humidity\", values_to = \"hum_value\") %&gt;%\n  mutate(temperature = case_when(temperature == \"T1\" ~ \"kitchen\",\n                                 temperature == \"T2\" ~ \"living\",\n                                 temperature == \"T3\" ~ \"laundry\",\n                                 temperature == \"T4\" ~ \"office\",\n                                 temperature == \"T5\" ~ \"bathroom\",\n                                 temperature == \"T6\" ~ \"north\",\n                                 temperature == \"T7\" ~ \"ironing\",\n                                 temperature == \"T8\" ~ \"teenager\",\n                                 temperature == \"T9\" ~ \"parents\")) %&gt;%  \n  mutate(humidity = case_when(humidity == \"RH_1\" ~ \"kitchen\",\n                                 humidity == \"RH_2\" ~ \"living\",\n                                 humidity == \"RH_3\" ~ \"laundry\",\n                                 humidity == \"RH_4\" ~ \"office\",\n                                 humidity == \"RH_5\" ~ \"bathroom\",\n                                 humidity == \"RH_6\" ~ \"north\",\n                                 humidity == \"RH_7\" ~ \"ironing\",\n                                 humidity == \"RH_8\" ~ \"teenager\",\n                              humidity == \"RH_9\" ~ \"parents\"))\n\nAs explained above, there are two variables that need this treatment; the temperature, and the humidity levels. In order to plot the average monthly temperature in each room, I need to use tidyr::pivot_longer() (a little side note, I could have used names_to = “room”, instead of “temperature” and “humidity”, but there’s a reason for that. More on it below).\n\n\nNow let’s plot it:\n\nflat_energy %&gt;%\n  mutate(month = month(date)) %&gt;%  \n  group_by(month, temperature) %&gt;%\n  summarise(avg_temp = mean(temp_value)) %&gt;%  \n  ggplot() +\n  geom_line(aes(y = avg_temp, x = month, col = temperature)) +\n  brotools::theme_blog()\n## `summarise()` regrouping output by 'month' (override with `.groups` argument)\n\n Ok great. But what if I had such a dataset per house for a whole city? How many datasets would that be? And how long would these operations take? The first step I would take if I were in this situation, would be to write a function. I would make it general enough to work with temperature or humidity. Below is this function:\n\nprepare_data &lt;- function(energy, variable){\n\n  variable &lt;- enquo(variable)\n\n  variable_label &lt;- as_label(variable)\n\n  regex_selector &lt;- ifelse(variable_label == \"temperature\",\n                           \"T\\\\d{1}\",\n                           \"RH_\\\\d{1}\")\nenergy %&gt;%\n  pivot_longer(cols = matches(regex_selector),\n               names_to = variable_label,\n               values_to = paste0(variable_label, \"_value\")) %&gt;%\n    mutate(!!(variable) := case_when(grepl(\"1$\", !!(variable)) ~ \"kitchen\",\n                                    grepl(\"2$\", !!(variable)) ~ \"living\",\n                                    grepl(\"3$\", !!(variable)) ~ \"laundry\",\n                                    grepl(\"4$\", !!(variable)) ~ \"office\",\n                                    grepl(\"5$\", !!(variable)) ~ \"bathroom\",\n                                    grepl(\"6$\", !!(variable)) ~ \"outside\",\n                                    grepl(\"7$\", !!(variable)) ~ \"ironing\",\n                                    grepl(\"8$\", !!(variable)) ~ \"teenager\",\n                                    grepl(\"9$\", !!(variable)) ~ \"parents\")) %&gt;%\n  mutate(month = month(date)) %&gt;%  \n  group_by(month, !!(variable)) %&gt;%\n  summarise(across(.cols = ends_with(\"_value\"),\n                   .fns = mean),\n            .groups = \"drop\")\n}\n\nThis function does exactly the same thing as above:\n\nprepare_data(energy, temperature) %&gt;%\n  ggplot() +\n  geom_line(aes(y = temperature_value, x = month, col = temperature)) +\n  brotools::theme_blog()\n\n\n\n\nAs you can see, I have the exact same plot. What’s nice with this function, is that it uses many verbs from the {tidyverse} as well as the tidy eval framework for non-standard evaluation ( which is why I did not use names_to = “room”, I wanted to use the variable label defined with as_label() and see if it works with {tidytable} as well). Ok, so now let’s imagine that I’m happy with this function, but I’d like it to run faster, and because I’m lazy, the less I have to modify it, the happier I am. This is where {tidytable} looks very promising. Let’s rewrite the function to make it work with {tidytable}:\n\nprepare_data_dt &lt;- function(energy, variable){\n\n  variable &lt;- enquo(variable)\n\n  variable_label &lt;- as_label(variable)\n\n  regex_selector &lt;- ifelse(variable_label == \"temperature\",\n                           \"T\\\\d{1}\",\n                           \"RH_\\\\d{1}\")\nenergy %&gt;%\n  pivot_longer.(cols = matches(regex_selector),\n               names_to = variable_label,\n               values_to = paste0(variable_label, \"_value\")) %&gt;%\n    mutate.(!!(variable) := case_when(grepl(\"1$\", !!(variable)) ~ \"kitchen\",\n                                    grepl(\"2$\", !!(variable)) ~ \"living\",\n                                    grepl(\"3$\", !!(variable)) ~ \"laundry\",\n                                    grepl(\"4$\", !!(variable)) ~ \"office\",\n                                    grepl(\"5$\", !!(variable)) ~ \"bathroom\",\n                                    grepl(\"6$\", !!(variable)) ~ \"outside\",\n                                    grepl(\"7$\", !!(variable)) ~ \"ironing\",\n                                    grepl(\"8$\", !!(variable)) ~ \"teenager\",\n                                    grepl(\"9$\", !!(variable)) ~ \"parents\")) %&gt;%  \n  mutate.(month = month(date)) %&gt;%  \n  summarise_across.(.cols = ends_with(\"_value\"),\n                    .fns = mean,\n                    .by = c(month, !!(variable))) %&gt;%  \n  ungroup()\n}\n\nAs you can see, it’s almost the same thing. {tidytable} verbs end with a ‘.’ and that’s it. Well almost (again), the biggest difference is how {tidytable} groups by a variable. It’s very similar to how it’s done in {data.table}, by using a .by = argument to verbs that support it, such as summarise_across() (which is also, by the way, another difference with standard {tidyverse} syntax). While I’ll have to remember these, I’d argue that they’re minor differences and if it can make my function run faster, I don’t mind!\n\n\nNow let’s run a little benchmark. But first, let’s define our data as a tidytable object:\n\nenergy_tidytable &lt;- as_tidytable(energy)\n\nNow we’re good to go:\n\nmicrobenchmark::microbenchmark(\n                  energy %&gt;%\n                  prepare_data(temperature),\n                  energy_tidytable %&gt;%\n                  prepare_data_dt(temperature),\n                  times = 10\n                )\n## Unit: milliseconds\n##                                               expr      min       lq     mean\n##               energy %&gt;% prepare_data(temperature) 847.9709 849.6671 868.6524\n##  energy_tidytable %&gt;% prepare_data_dt(temperature) 820.2051 838.6647 861.9685\n##    median       uq      max neval\n##  861.0652 880.8200 914.4685    10\n##  858.9454 873.3268 936.0147    10\n\nThat is nice! It does indeed run faster, and with only some minor changes to the function! And how about using some more cores to run this function? This can be done using data.table::setDTthreads(n_cores) where n_cores is the number of cores you want to use:\n\ndata.table::setDTthreads(12)\nmicrobenchmark::microbenchmark(\n                  energy %&gt;%\n                  prepare_data(temperature),\n                  energy_tidytable %&gt;%\n                  prepare_data_dt(temperature),\n                  times = 10\n                )\n## Unit: milliseconds\n##                                               expr      min       lq     mean\n##               energy %&gt;% prepare_data(temperature) 832.9876 840.8000 874.3047\n##  energy_tidytable %&gt;% prepare_data_dt(temperature) 829.7937 831.2868 866.4383\n##    median       uq      max neval\n##  889.2684 898.6861 914.7178    10\n##  836.8712 893.0613 997.8511    10\n\nMaybe surprisingly, it did not run faster. It could very well be that my function does not really lend itself to running in parallel, and the overhead induced by distributing the work to the cpu cores cancels out the gains from running it in parallel. But in any case, this is really looking very interesting. I have not tested the whole package yet, but since the syntax is so similar to the {tidyverse}, you can try really quickly to see if the {tidytable} version of the function runs faster, and if yes, I don’t really see a reason not to use it!\n\n\nCheck out the project’s website here, and follow the author’s twitter here."
  },
  {
    "objectID": "posts/2018-10-21-lux_elections.html",
    "href": "posts/2018-10-21-lux_elections.html",
    "title": "Getting the data from the Luxembourguish elections out of Excel",
    "section": "",
    "text": "In this blog post, similar to a previous blog post I am going to show you how we can go from an Excel workbook that contains data to flat file. I will taking advantage of the structure of the tables inside the Excel sheets by writing a function that extracts the tables and then mapping it to each sheet!\n\n\nLast week, October 14th, Luxembourguish nationals went to the polls to elect the Grand Duke! No, actually, the Grand Duke does not get elected. But Luxembourguish citizen did go to the polls to elect the new members of the Chamber of Deputies (a sort of parliament if you will). The way the elections work in Luxembourg is quite interesting; you can vote for a party, or vote for individual candidates from different parties. The candidates that get the most votes will then seat in the parliament. If you vote for a whole party, each of the candidates get a vote. You get as many votes as there are candidates to vote for. So, for example, if you live in the capital city, also called Luxembourg, you get 21 votes to distribute. You could decide to give 10 votes to 10 candidates of party A and 11 to 11 candidates of party B. Why 21 votes? The chamber of Deputies is made up 60 deputies, and the country is divided into four legislative circonscriptions. So each voter in a circonscription gets an amount of votes that is proportional to the population size of that circonscription.\n\n\nNow you certainly wonder why I put the flag of Gambia on top of this post? This is because the government that was formed after the 2013 elections was made up of a coalition of 3 parties; the Luxembourg Socialist Worker’s Party, the Democratic Party and The Greens. The LSAP managed to get 13 seats in the Chamber, while the DP got 13 and The Greens 6, meaning 32 seats out of 60. So because they made this coalition, they could form the government, and this coalition was named the Gambia coalition because of the colors of these 3 parties: red, blue and green. If you want to take a look at the ballot from 2013 for the southern circonscription, click here.\n\n\nNow that you have the context, we can go back to some data science. The results of the elections of last week can be found on Luxembourg’s Open Data portal, right here. The data is trapped inside Excel sheets; just like I explained in a previous blog post the data is easily read by human, but not easily digested by any type of data analysis software. So I am going to show you how we are going from this big Excel workbook to a flat file.\n\n\nFirst of all, if you open the Excel workbook, you will notice that there are a lot of sheets; there is one for the whole country, named “Le Grand-Duché de Luxembourg”, one for the four circonscriptions, “Centre”, “Nord”, “Sud”, “Est” and 102 more for each commune of the country (a commune is an administrative division). However, the tables are all very similarly shaped, and roughly at the same position.\n\n\n\n\n\nThis is good, because we can write a function to extracts the data and then map it over all the sheets. First, let’s load some packages and the data for the country:\n\nlibrary(\"tidyverse\")\nlibrary(\"tidyxl\")\nlibrary(\"brotools\")\n# National Level 2018\nelections_raw_2018 &lt;- xlsx_cells(\"leg-2018-10-14-22-58-09-737.xlsx\",\n                        sheets = \"Le Grand-Duché de Luxembourg\")\n\n{brotools} is my own package. You can install it with:\n\ndevtools::install_github(\"b-rodrigues/brotools\")\n\nit contains a function that I will use down below. The function I wrote to extract the tables is not very complex, but requires that you are familiar with how {tidyxl} imports Excel workbooks. So if you are not familiar with it, study the imported data frame for a few minutes. It will make understanding the next function easier:\n\nextract_party &lt;- function(dataset, starting_col, target_rows){\n\n    almost_clean &lt;- dataset %&gt;%\n        filter(row %in% target_rows) %&gt;%\n        filter(col %in% c(starting_col, starting_col + 1)) %&gt;%\n        select(character, numeric) %&gt;%\n        fill(numeric, .direction = \"up\") %&gt;%\n        filter(!is.na(character))\n\n    party_name &lt;- almost_clean$character[1] %&gt;%\n        str_split(\"-\", simplify = TRUE) %&gt;%\n        .[2] %&gt;%\n        str_trim()\n\n    almost_clean$character[1] &lt;- \"Pourcentage\"\n\n    almost_clean$party &lt;- party_name\n\n    colnames(almost_clean) &lt;- c(\"Variables\", \"Values\", \"Party\")\n\n    almost_clean %&gt;%\n        mutate(Year = 2018) %&gt;%\n        select(Party, Year, Variables, Values)\n\n}\n\nThis function has three arguments, dataset, starting_col and target_rows. dataset is the data I loaded with xlsx_cells from the {tidyxl} package. I think the following picture illustrates easily what the function does:\n\n\n\n\n\nSo the function first filters only the rows we are interested in, then the cols. I then select the columns I want which are called character and numeric (if the Excel cell contains characters then you will find them in the character column, if it contains numbers you will them in the numeric column), then I fill the empty cells with the values from the numeric column and the I remove the NA’s. These two last steps might not be so clear; this is how the data looks like up until the select() function:\n\n&gt; elections_raw_2018 %&gt;%\n+     filter(row %in% seq(11,19)) %&gt;%\n+     filter(col %in% c(1, 2)) %&gt;%\n+     select(character, numeric)\n# A tibble: 18 x 2\n   character                       numeric\n   &lt;chr&gt;                             &lt;dbl&gt;\n 1 1 - PIRATEN - PIRATEN           NA     \n 2 NA                               0.0645\n 3 Suffrage total                  NA     \n 4 NA                          227549     \n 5 Suffrages de liste              NA     \n 6 NA                          181560     \n 7 Suffrage nominatifs             NA     \n 8 NA                           45989     \n 9 Pourcentage pondéré             NA     \n10 NA                               0.0661\n11 Suffrage total pondéré          NA     \n12 NA                           13394.    \n13 Suffrages de liste pondéré      NA     \n14 NA                           10308     \n15 Suffrage nominatifs pondéré     NA     \n16 NA                            3086.    \n17 Mandats attribués               NA     \n18 NA                               2  \n\nSo by filling the NA’s in the numeric the data now looks like this:\n\n&gt; elections_raw_2018 %&gt;%\n+     filter(row %in% seq(11,19)) %&gt;%\n+     filter(col %in% c(1, 2)) %&gt;%\n+     select(character, numeric) %&gt;%\n+     fill(numeric, .direction = \"up\")\n# A tibble: 18 x 2\n   character                       numeric\n   &lt;chr&gt;                             &lt;dbl&gt;\n 1 1 - PIRATEN - PIRATEN            0.0645\n 2 NA                               0.0645\n 3 Suffrage total              227549     \n 4 NA                          227549     \n 5 Suffrages de liste          181560     \n 6 NA                          181560     \n 7 Suffrage nominatifs          45989     \n 8 NA                           45989     \n 9 Pourcentage pondéré              0.0661\n10 NA                               0.0661\n11 Suffrage total pondéré       13394.    \n12 NA                           13394.    \n13 Suffrages de liste pondéré   10308     \n14 NA                           10308     \n15 Suffrage nominatifs pondéré   3086.    \n16 NA                            3086.    \n17 Mandats attribués                2     \n18 NA                               2 \n\nAnd then I filter out the NA’s from the character column, and that’s almost it! I simply need to add a new column with the party’s name and rename the other columns. I also add a “Year” colmun.\n\n\nNow, each party will have a different starting column. The table with the data for the first party starts on column 1, for the second party it starts on column 4, column 7 for the third party… So the following vector contains all the starting columns:\n\nposition_parties_national &lt;- seq(1, 24, by = 3)\n\n(If you study the Excel workbook closely, you will notice that I do not extract the last two parties. This is because these parties were not present in all of the 4 circonscriptions and are very, very, very small.)\n\n\nThe target rows are always the same, from 11 to 19. Now, I simply need to map this function to this list of positions and I get the data for all the parties:\n\nelections_national_2018 &lt;- map_df(position_parties_national, extract_party, \n                         dataset = elections_raw_2018, target_rows = seq(11, 19)) %&gt;%\n    mutate(locality = \"Grand-Duchy of Luxembourg\", division = \"National\")\n\nI also added the locality and division columns to the data.\n\n\nLet’s take a look:\n\nglimpse(elections_national_2018)\n## Observations: 72\n## Variables: 6\n## $ Party     &lt;chr&gt; \"PIRATEN\", \"PIRATEN\", \"PIRATEN\", \"PIRATEN\", \"PIRATEN\",…\n## $ Year      &lt;dbl&gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, …\n## $ Variables &lt;chr&gt; \"Pourcentage\", \"Suffrage total\", \"Suffrages de liste\",…\n## $ Values    &lt;dbl&gt; 6.446204e-02, 2.275490e+05, 1.815600e+05, 4.598900e+04…\n## $ locality  &lt;chr&gt; \"Grand-Duchy of Luxembourg\", \"Grand-Duchy of Luxembour…\n## $ division  &lt;chr&gt; \"National\", \"National\", \"National\", \"National\", \"Natio…\n\nVery nice.\n\n\nNow we need to do the same for the 4 electoral circonscriptions. First, let’s load the data:\n\n# Electoral districts 2018\ndistricts &lt;- c(\"Centre\", \"Nord\", \"Sud\", \"Est\")\n\nelections_district_raw_2018 &lt;- xlsx_cells(\"leg-2018-10-14-22-58-09-737.xlsx\",\n                                      sheets = districts)\n\nNow things get trickier. Remember I said that the number of seats is proportional to the population of each circonscription? We simply can’t use the same target rows as before. For example, for the “Centre” circonscription, the target rows go from 12 to 37, but for the “Est” circonscription only from 12 to 23. Ideally, we would need a function that would return the target rows.\n\n\nThis is that function:\n\n# The target rows I need to extract are different from district to district\nget_target_rows &lt;- function(dataset, sheet_to_extract, reference_address){\n\n    last_row &lt;- dataset %&gt;%\n        filter(sheet == sheet_to_extract) %&gt;%\n        filter(address == reference_address) %&gt;%\n        pull(numeric)\n\n    seq(12, (11 + 5 + last_row))\n}\n\nThis function needs a dataset, a sheet_to_extract and a reference_address. The reference address is a cell that actually contains the number of seats in that circonscription, in our case “B5”. We can easily get the list of target rows now:\n\n# Get the target rows\nlist_targets &lt;- map(districts, get_target_rows, dataset = elections_district_raw_2018, \n                    reference_address = \"B5\")\n\nlist_targets\n## [[1]]\n##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34\n## [24] 35 36 37\n## \n## [[2]]\n##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n## \n## [[3]]\n##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34\n## [24] 35 36 37 38 39\n## \n## [[4]]\n##  [1] 12 13 14 15 16 17 18 19 20 21 22 23\n\nNow, let’s split the data we imported into a list, where each element of the list is a dataframe with the data from one circonscription:\n\nlist_data_districts &lt;- map(districts, ~filter(.data = elections_district_raw_2018, sheet == .)) \n\nNow I can easily map the function I defined above, extract_party to this list of datasets. Well, I say easily, but it’s a bit more complicated than before because I have now a list of datasets and a list of target rows:\n\nelections_district_2018 &lt;- map2(.x = list_data_districts, .y = list_targets,\n     ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = .y))\n\nThe way to understand this is that for each element of list_data_districts and list_targets, I have to map extract_party to each element of position_parties_national. This gives the intented result:\n\nelections_district_2018\n## [[1]]\n## # A tibble: 208 x 4\n##    Party    Year Variables               Values\n##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;\n##  1 PIRATEN  2018 Pourcentage             0.0514\n##  2 PIRATEN  2018 CLEMENT Sven (1)     8007     \n##  3 PIRATEN  2018 WEYER Jerry (2)      3446     \n##  4 PIRATEN  2018 CLEMENT Pascal (3)   3418     \n##  5 PIRATEN  2018 KUNAKOVA Lucie (4)   2860     \n##  6 PIRATEN  2018 WAMPACH Jo (14)      2693     \n##  7 PIRATEN  2018 LAUX Cynthia (6)     2622     \n##  8 PIRATEN  2018 ISEKIN Christian (5) 2610     \n##  9 PIRATEN  2018 SCHWEICH Georges (9) 2602     \n## 10 PIRATEN  2018 LIESCH Mireille (8)  2551     \n## # … with 198 more rows\n## \n## [[2]]\n## # A tibble: 112 x 4\n##    Party    Year Variables                             Values\n##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                                  &lt;dbl&gt;\n##  1 PIRATEN  2018 Pourcentage                           0.0767\n##  2 PIRATEN  2018 COLOMBERA Jean (2)                 5074     \n##  3 PIRATEN  2018 ALLARD Ben (1)                     4225     \n##  4 PIRATEN  2018 MAAR Andy (3)                      2764     \n##  5 PIRATEN  2018 GINTER Joshua (8)                  2536     \n##  6 PIRATEN  2018 DASBACH Angelika (4)               2473     \n##  7 PIRATEN  2018 GRÜNEISEN Sam (6)                  2408     \n##  8 PIRATEN  2018 BAUMANN Roy (5)                    2387     \n##  9 PIRATEN  2018 CONRAD Pierre (7)                  2280     \n## 10 PIRATEN  2018 TRAUT ép. MOLITOR Angela Maria (9) 2274     \n## # … with 102 more rows\n## \n## [[3]]\n## # A tibble: 224 x 4\n##    Party    Year Variables                    Values\n##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt;\n##  1 PIRATEN  2018 Pourcentage                  0.0699\n##  2 PIRATEN  2018 GOERGEN Marc (1)          9818     \n##  3 PIRATEN  2018 FLOR Starsky (2)          6737     \n##  4 PIRATEN  2018 KOHL Martine (3)          6071     \n##  5 PIRATEN  2018 LIESCH Camille (4)        6025     \n##  6 PIRATEN  2018 KOHL Sylvie (6)           5628     \n##  7 PIRATEN  2018 WELTER Christian (5)      5619     \n##  8 PIRATEN  2018 DA GRAÇA DIAS Yanick (10) 5307     \n##  9 PIRATEN  2018 WEBER Jules (7)           5301     \n## 10 PIRATEN  2018 CHMELIK Libor (8)         5247     \n## # … with 214 more rows\n## \n## [[4]]\n## # A tibble: 96 x 4\n##    Party    Year Variables                           Values\n##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                                &lt;dbl&gt;\n##  1 PIRATEN  2018 Pourcentage                         0.0698\n##  2 PIRATEN  2018 FRÈRES Daniel (1)                4152     \n##  3 PIRATEN  2018 CLEMENT Jill (7)                 1943     \n##  4 PIRATEN  2018 HOUDREMONT Claire (2)            1844     \n##  5 PIRATEN  2018 BÖRGER Nancy (3)                 1739     \n##  6 PIRATEN  2018 MARTINS DOS SANTOS Catarina (6)  1710     \n##  7 PIRATEN  2018 BELLEVILLE Tatjana (4)           1687     \n##  8 PIRATEN  2018 CONTRERAS Gerald (5)             1687     \n##  9 PIRATEN  2018 Suffrages total                 14762     \n## 10 PIRATEN  2018 Suffrages de liste              10248     \n## # … with 86 more rows\n\nI now need to add the locality and division columns:\n\nelections_district_2018 &lt;- map2(.y = elections_district_2018, .x = districts, \n     ~mutate(.y, locality = .x, division = \"Electoral district\")) %&gt;%\n    bind_rows()\n\nWe’re almost done! Now we need to do the same for the 102 remaining sheets, one for each commune of Luxembourg. This will now go very fast, because we got all the building blocks from before:\n\ncommunes &lt;- xlsx_sheet_names(\"leg-2018-10-14-22-58-09-737.xlsx\")\n\ncommunes &lt;- communes %-l% \n    c(\"Le Grand-Duché de Luxembourg\", \"Centre\", \"Est\", \"Nord\", \"Sud\", \"Sommaire\")\n\nLet me introduce the following function: %-l%. This function removes elements from lists:\n\nc(\"a\", \"b\", \"c\", \"d\") %-l% c(\"a\", \"d\")\n## [1] \"b\" \"c\"\n\nYou can think of it as “minus for lists”. This is called an infix operator.\n\n\nSo this function is very useful to get the list of communes, and is part of my package, {brotools}.\n\n\nAs before, I load the data:\n\nelections_communes_raw_2018 &lt;- xlsx_cells(\"leg-2018-10-14-22-58-09-737.xlsx\",\n                                 sheets = communes)\n\nThen get my list of targets, but I need to change the reference address. It’s “B8” now, not “B7”.\n\n# Get the target rows\nlist_targets &lt;- map(communes, get_target_rows, \n                    dataset = elections_communes_raw_2018, reference_address = \"B8\")\n\nI now create a list of communes by mapping a filter function to the data:\n\nlist_data_communes &lt;- map(communes, ~filter(.data = elections_communes_raw_2018, sheet == .)) \n\nAnd just as before, I get the data I need by using extract_party, and adding the “locality” and “division” columns:\n\nelections_communes_2018 &lt;- map2(.x = list_data_communes, .y = list_targets,\n                                ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = .y))\n\nelections_communes_2018 &lt;- map2(.y = elections_communes_2018, .x = communes,\n                                ~mutate(.y, locality = .x, division = \"Commune\")) %&gt;%\n    bind_rows()\n\nThe steps are so similar for the four circonscriptions and for the 102 communes that I could have write a big wrapper function and the use it for the circonscription and communes at once. But I was lazy.\n\n\nFinally, I bind everything together and have a nice, tidy, flat file:\n\n# Final results\n\nelections_2018 &lt;- bind_rows(list(elections_national_2018, elections_district_2018, elections_communes_2018))\n\nglimpse(elections_2018)\n## Observations: 15,544\n## Variables: 6\n## $ Party     &lt;chr&gt; \"PIRATEN\", \"PIRATEN\", \"PIRATEN\", \"PIRATEN\", \"PIRATEN\",…\n## $ Year      &lt;dbl&gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, …\n## $ Variables &lt;chr&gt; \"Pourcentage\", \"Suffrage total\", \"Suffrages de liste\",…\n## $ Values    &lt;dbl&gt; 6.446204e-02, 2.275490e+05, 1.815600e+05, 4.598900e+04…\n## $ locality  &lt;chr&gt; \"Grand-Duchy of Luxembourg\", \"Grand-Duchy of Luxembour…\n## $ division  &lt;chr&gt; \"National\", \"National\", \"National\", \"National\", \"Natio…\n\nThis blog post is already quite long, so I will analyze the data now that R can easily ingest it in a future blog post."
  },
  {
    "objectID": "posts/2020-03-10-exp_tidymodels.html",
    "href": "posts/2020-03-10-exp_tidymodels.html",
    "title": "Explainbility of {tidymodels} models with {iml}",
    "section": "",
    "text": "In my previous blog post, I have shown how you could use {tidymodels} to train several machine learning models. Now, let’s take a look at getting some explanations out of them, using the {iml} package. Originally I did not intend to create a separate blog post, but I have encountered… an issue, or bug, when using both {iml} and {tidymodels} and I felt that it was important that I write about it. Maybe it’s just me that’s missing something, and you, kind reader, might be able to give me an answer. But let’s first reload the models from last time (the same packages as on the previous blog post are loaded):\n\ntrained_models_list\n## [[1]]\n## #  10-fold cross-validation \n## # A tibble: 10 x 4\n##    splits               id     .metrics          .notes          \n##  * &lt;list&gt;               &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n##  1 &lt;split [23.6K/2.6K]&gt; Fold01 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  2 &lt;split [23.6K/2.6K]&gt; Fold02 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  3 &lt;split [23.6K/2.6K]&gt; Fold03 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  4 &lt;split [23.6K/2.6K]&gt; Fold04 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  5 &lt;split [23.6K/2.6K]&gt; Fold05 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  6 &lt;split [23.6K/2.6K]&gt; Fold06 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  7 &lt;split [23.6K/2.6K]&gt; Fold07 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  8 &lt;split [23.6K/2.6K]&gt; Fold08 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  9 &lt;split [23.6K/2.6K]&gt; Fold09 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n## 10 &lt;split [23.6K/2.6K]&gt; Fold10 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n## \n## [[2]]\n## #  10-fold cross-validation \n## # A tibble: 10 x 4\n##    splits               id     .metrics          .notes          \n##  * &lt;list&gt;               &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n##  1 &lt;split [23.6K/2.6K]&gt; Fold01 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  2 &lt;split [23.6K/2.6K]&gt; Fold02 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  3 &lt;split [23.6K/2.6K]&gt; Fold03 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  4 &lt;split [23.6K/2.6K]&gt; Fold04 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  5 &lt;split [23.6K/2.6K]&gt; Fold05 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  6 &lt;split [23.6K/2.6K]&gt; Fold06 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  7 &lt;split [23.6K/2.6K]&gt; Fold07 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  8 &lt;split [23.6K/2.6K]&gt; Fold08 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  9 &lt;split [23.6K/2.6K]&gt; Fold09 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n## 10 &lt;split [23.6K/2.6K]&gt; Fold10 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n## \n## [[3]]\n## #  10-fold cross-validation \n## # A tibble: 10 x 4\n##    splits               id     .metrics          .notes          \n##  * &lt;list&gt;               &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n##  1 &lt;split [23.6K/2.6K]&gt; Fold01 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  2 &lt;split [23.6K/2.6K]&gt; Fold02 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  3 &lt;split [23.6K/2.6K]&gt; Fold03 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  4 &lt;split [23.6K/2.6K]&gt; Fold04 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  5 &lt;split [23.6K/2.6K]&gt; Fold05 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  6 &lt;split [23.6K/2.6K]&gt; Fold06 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  7 &lt;split [23.6K/2.6K]&gt; Fold07 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  8 &lt;split [23.6K/2.6K]&gt; Fold08 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  9 &lt;split [23.6K/2.6K]&gt; Fold09 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n## 10 &lt;split [23.6K/2.6K]&gt; Fold10 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n## \n## [[4]]\n## #  10-fold cross-validation \n## # A tibble: 10 x 4\n##    splits               id     .metrics          .notes          \n##  * &lt;list&gt;               &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n##  1 &lt;split [23.6K/2.6K]&gt; Fold01 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n##  2 &lt;split [23.6K/2.6K]&gt; Fold02 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n##  3 &lt;split [23.6K/2.6K]&gt; Fold03 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n##  4 &lt;split [23.6K/2.6K]&gt; Fold04 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n##  5 &lt;split [23.6K/2.6K]&gt; Fold05 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n##  6 &lt;split [23.6K/2.6K]&gt; Fold06 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n##  7 &lt;split [23.6K/2.6K]&gt; Fold07 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n##  8 &lt;split [23.6K/2.6K]&gt; Fold08 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n##  9 &lt;split [23.6K/2.6K]&gt; Fold09 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n## 10 &lt;split [23.6K/2.6K]&gt; Fold10 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n## \n## [[5]]\n## #  10-fold cross-validation \n## # A tibble: 10 x 4\n##    splits               id     .metrics          .notes          \n##  * &lt;list&gt;               &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n##  1 &lt;split [23.6K/2.6K]&gt; Fold01 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  2 &lt;split [23.6K/2.6K]&gt; Fold02 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  3 &lt;split [23.6K/2.6K]&gt; Fold03 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  4 &lt;split [23.6K/2.6K]&gt; Fold04 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  5 &lt;split [23.6K/2.6K]&gt; Fold05 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  6 &lt;split [23.6K/2.6K]&gt; Fold06 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  7 &lt;split [23.6K/2.6K]&gt; Fold07 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  8 &lt;split [23.6K/2.6K]&gt; Fold08 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  9 &lt;split [23.6K/2.6K]&gt; Fold09 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n## 10 &lt;split [23.6K/2.6K]&gt; Fold10 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n\nLet’s see which of the models performed best (in cross-validation):\n\ntrained_models_list %&gt;%\n  map(show_best, metric = \"accuracy\", n = 1)\n## [[1]]\n## # A tibble: 1 x 7\n##    penalty mixture .metric  .estimator  mean     n std_err\n##      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n## 1 6.57e-10  0.0655 accuracy binary     0.916    10 0.00179\n## \n## [[2]]\n## # A tibble: 1 x 7\n##    mtry trees .metric  .estimator  mean     n std_err\n##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n## 1    13  1991 accuracy binary     0.929    10 0.00172\n## \n## [[3]]\n## # A tibble: 1 x 7\n##   num_terms prune_method .metric  .estimator  mean     n std_err\n##       &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n## 1         5 backward     accuracy binary     0.904    10 0.00186\n## \n## [[4]]\n## # A tibble: 1 x 9\n##    mtry trees tree_depth learn_rate .metric  .estimator  mean     n std_err\n##   &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n## 1    12  1245         12     0.0770 accuracy binary     0.929    10 0.00175\n## \n## [[5]]\n## # A tibble: 1 x 7\n##   hidden_units    penalty .metric  .estimator  mean     n std_err\n##          &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n## 1           10 0.00000307 accuracy binary     0.917    10 0.00209\n\nSeems like the second model, the random forest performed the best (highest mean accuracy with lowest standard error). So let’s retrain the model on the whole training set and see how it fares on the testing set:\n\nrf_specs &lt;- trained_models_list[[2]]\n\nLet’s save the best model specification in a variable:\n\nbest_rf_spec &lt;- show_best(rf_specs, \"accuracy\", 1)\n\nLet’s now retrain this model, using a workflow:\n\nbest_rf_model &lt;- rand_forest(mode = \"classification\", mtry = best_rf_spec$mtry,\n                           trees = best_rf_spec$trees) %&gt;%\n  set_engine(\"ranger\")\n\npreprocess &lt;- recipe(job_search ~ ., data = pra) %&gt;%\n  step_dummy(all_predictors())\n\npra_wflow_best &lt;- workflow() %&gt;%\n  add_recipe(preprocess) %&gt;%\n  add_model(best_rf_model)\n\nbest_model_fitted &lt;- fit(pra_wflow_best, data = pra_train)\n## Warning: The following variables are not factor vectors and will be ignored:\n## `hours`\n\nand let’s take a look at the confusion matrix:\n\npredictions &lt;- predict(best_model_fitted, new_data = pra_test) %&gt;%\n  bind_cols(pra_test)\n\npredictions %&gt;%\n  mutate(job_search = as.factor(job_search)) %&gt;%  \n  accuracy(job_search, .pred_class)\n## # A tibble: 1 x 3\n##   .metric  .estimator .estimate\n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n## 1 accuracy binary         0.924\npredictions %&gt;%\n  mutate(job_search = as.factor(job_search)) %&gt;%  \n  conf_mat(job_search, .pred_class) \n##           Truth\n## Prediction    N    S\n##          N 2539  156\n##          S   64  149\n\nWe see that predicting class S (“Si”, meaning, “yes” in Spanish) is tricky. One would probably need to use techniques such as SMOTE to deal with this (see this blog post for more info). Anyways, this is not today’s topic.\n\n\nLet’s say that we are satisfied with the model and want some explanations out of it. I have already blogged about it in the past, so if you want more details, you can read this blog post.\n\n\nNow, what is important, is that I have defined a complete workflow to deal with the data preprocessing and then the training of the model. So I’ll be using this workflow as well to get my explainability. What I mean with this is the following: to get explanations, we need a model, and a way to get predictions out of it. As I have shown before, my fitted workflow is able to give me predictions. So I should have every needed ingredient; {iml}, the package that I am using for explainability provides several functions that work all the same; you first define an object that takes as an input the fitted model, the design matrix, the target variable and the prediction function. Let’s start with defining the design matrix and the target variable:\n\nlibrary(\"iml\")\n\nfeatures &lt;- pra_test %&gt;%\n  select(-job_search)\n\ntarget &lt;- pra_test %&gt;%\n  mutate(job_search = as.factor(job_search)) %&gt;%  \n  select(job_search)\n\nNow, let’s define the predict function:\n\npredict_wrapper &lt;- function(model, newdata){\n  workflows:::predict.workflow(object = model, new_data = newdata)\n}\n\nBecause a workflow is a bit special, I need to define this wrapper function that wraps the workflows:::predict.workflow() function. Normally, users should not have to deal with this function; as you can see, to access it I had to use the very special ::: function. ::: permits users to access private functions (not sure if this is the right term; what I mean is that private functions are used internally by the package and should not be available to users. AFAIK, this is how these functions are called in Python). I tried simply using the predict() function, which works interactively but I was getting issues with it when I was providing it to the constructor below:\n\npredictor &lt;- Predictor$new(\n                         model = best_model_fitted,\n                         data = features, \n                         y = target,\n                         predict.fun = predict_wrapper\n                       )\n\nThis creates a Predictor object from which I am now able to get explanations. For example, for feature importance, I would write the following:\n\nfeature_importance &lt;- FeatureImp$new(predictor, loss = \"ce\")\n\nplot(feature_importance)\n\n\n\n\nAnd this is where I noticed that something was wrong; the variables we are looking at are categorical variables. So why am I not seeing the categories? Why is the most important variable the contract type, without the category of the contract type that is the most important? Remember that I created dummy variables using a recipe. So I was expecting something like type_of_contract_type_1, type_of_contract_type_2, etc… as variables.\n\n\nThis made me want to try to fit the model “the old way”, without using workflows. So for this I need to use the prep(), juice() and bake() functions, which are included in the {recipes} package. I won’t go into much detail, but the idea is that prep() is used to train the recipe, and compute whatever is needed to preprocess the data (such as means and standard deviations for normalization). For this, you should use the training data only. juice() returns the preprocessed training set, and bake() is then used to preprocessed a new data set, for instance the test set, using the same estimated parameters that were obtained with prep().\n\n\nUsing workflows avoids having to do these steps manually, but what I am hoping is that doing this manually will solve my issue. So let’s try:\n\n# without workflows\ntrained_recipe &lt;- prep(preprocess, training = pra_train)\n## Warning: The following variables are not factor vectors and will be ignored:\n## `hours`\npra_train_prep &lt;- juice(trained_recipe)\n\n\nbest_model_fit &lt;- fit(best_rf_model, job_search ~ ., data = pra_train_prep)\n\n\npra_test_bake_features &lt;- bake(trained_recipe, pra_test) %&gt;%\n  select(-job_search)\n\n\npredict_wrapper2 &lt;- function(model, newdata){\n  predict(object = model, new_data = newdata)\n}\n\npredictor2 &lt;- Predictor$new(\n                          model = best_model_fit,\n                          data = pra_test_bake_features, \n                          y = target,\n                          predict.fun = predict_wrapper2\n                        )\n\nfeature_importance2 &lt;- FeatureImp$new(predictor2, loss = \"ce\")\n\nplot(feature_importance2)\n\n\n\n\nEureka! As you can see, the issue is now solved; we now have all the variables that were used for training the model, also in our explanations. I don’t know exactly what’s going on; is this a bug? Is it because the {workflows} package makes this process too streamlined that it somehow rebuilds the features and then returns the results? I have no idea. In any case, it would seem that for the time being, doing the training and explanations without the {workflows} package is the way to go if you require explanations as well."
  },
  {
    "objectID": "posts/2022-05-18-cran_0_2_0.html",
    "href": "posts/2022-05-18-cran_0_2_0.html",
    "title": "chronicler is now available on CRAN",
    "section": "",
    "text": "I am very happy to annouce that the {chronicler} package, which I’ve been working on for the past 3 months has been released on CRAN. Install it with:\n{chronicler} allows you to create objects that carry a log with them. Here is an example of an object that has been created using {chronicler}, and saved using saveRDS() (which we now load back into our session using readRDS()):\nPrinting my_df shows the following output:\nmy_df is made up of two parts, one is a data set, and the other is the log. If you wish to know how this data set was created, you can call read_log(my_df) (this function will be renamed to read.log() in the next release, to avoid clashing with readr::read_log()):\nif you want to get the dataset out of the {chronicler} “box”, you can do so with pick(my_df, “value”):\nTo know more about all the package has to offer, read the readme and the vignettes on the package’s website. I’m already working on the next release, where I plan to add the following features:\nI’m really looking forward to see how people are going to use this package for their work, personally I’ve been mixing {chronicler} with {targets} to build very robust pipelines to build chronicle objects!"
  },
  {
    "objectID": "posts/2022-05-18-cran_0_2_0.html#thanks",
    "href": "posts/2022-05-18-cran_0_2_0.html#thanks",
    "title": "chronicler is now available on CRAN",
    "section": "\nThanks\n",
    "text": "Thanks\n\n\nI’d like to thank armcn, Kupac for their blog posts (here) and packages (maybe) which inspired me to build this package. Thank you as well to TimTeaFan for his help with writing the %&gt;=% infix operator, nigrahamuk for showing me a nice way to catch errors, and finally Mwavu for pointing me towards the right direction with an issue I’ve had as I started working on this package. Thanks to Putosaure for designing the hex logo, and of course to every single person that makes free and open source software possible."
  },
  {
    "objectID": "posts/2022-10-01-why_js_shiny.html#the-snake-biting-its-own-tail",
    "href": "posts/2022-10-01-why_js_shiny.html#the-snake-biting-its-own-tail",
    "title": "Why and how to use JS in your Shiny app",
    "section": "\nThe snake biting its own tail\n",
    "text": "The snake biting its own tail\n\n\nDisclaimer: I’m a beginner at JS, so don’t ask me about the many intricacies of JS.\n\n\nI’ve been working on a Shiny app for work these past few weeks, and had to use Javascript to solve a very specific issue I encountered. Something for which, as far as I know, there is no other solution than using Javascript. The problem had to do with dynamically changing the UI of an app. The way to usually achieve this is using renderUI()/uiOutput(). For example, consider the following little app (if you don’t want to run it, watch the video below):\n\nlibrary(shiny)\nlibrary(ggplot2)\n\ndata(mtcars)\n\nui &lt;- fluidPage(\n  selectInput(\"var\", \"Select variable:\", choices = colnames(mtcars)),\n  uiOutput(\"welcome\"),\n  plotOutput(\"my_plot\")\n)\n\nserver &lt;- function(input, output) {\n\n  output$welcome &lt;- renderUI({\n      tags$div(paste0(\"Welcome to my award-winning app! Currently showing variable: \", input$var))\n  })\n\n  output$my_plot &lt;- renderPlot({\n        ggplot(data = mtcars) +\n          geom_bar(aes_string(y = input$var))\n      })\n}\n\nshinyApp(ui, server)\n\n\n\n\n\n\nAs you can see, when the user chooses a new variable, the plot gets updated of course, but the welcome message changes as well. Normally, the UI of a Shiny app gets rendered once, at startup, and stays fixed. But thanks to renderUI()/uiOutput(), it is possible to change UI elements on the fly, and anything can go inside of renderUI()/uiOutput(), it can be something much more complex than a simple message like in my example above.\n\n\nSo, why did I need to use Javascript to basically achieve the same thing? The reason is that I am currently using {bs4Dash}, an amazing package to build Shiny dashboard using Bootstrap 4. {bs4Dash} comes with many neat features, one of them being improved box()es (improved when compared to the box()es from {shinydashboard}). These improved boxes allow you to do something like this (if you don’t want to run it, watch the video below):\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bs4Dash)\n\ndata(mtcars)\n\nshinyApp(\n  ui = dashboardPage(\n    header = dashboardHeader(\n      title = dashboardBrand(\n        title = \"Welcome to my award-winning dashboard!\",\n        color = \"primary\"\n      )\n    ),\n    sidebar = dashboardSidebar(),\n    body = dashboardBody(\n      box(\n        plotOutput(\"my_plot\"),\n        title = \"This is where I will put the title, but bear with me.\",\n        width = 12,\n        sidebar = boxSidebar(\n          id = \"sidebarid\",\n          startOpen = TRUE,\n          selectInput(\"var\", \"Select variable:\", choices = colnames(mtcars))\n          ))\n    ),\n    controlbar = dashboardControlbar(),\n    title = \"DashboardPage\"\n  ),\n  server = function(input, output, session) {\n\n    output$my_plot &lt;- renderPlot({\n      ggplot(data = mtcars) +\n        geom_bar(aes_string(y = input$var))\n    })\n\n  }\n)\n\n\n\n\n\n\nEach box can have a side bar, and these side bars can contain toggles specific to the graph. If you click outside the side bar, the side bar closes; to show the side bar, click on the little gears in the top right corner of the side bar. Ok we’re almost done with the setup: see how the box can have a title? Let’s make it change like before; for this, because the title is part of the box() function, I need to re-render the whole box (if you don’t want to run it, watch the video below):\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bs4Dash)\n\ndata(mtcars)\n\nshinyApp(\n  ui = dashboardPage(\n    header = dashboardHeader(\n      title = dashboardBrand(\n        title = \"Welcome to my award-winning dashboard!\",\n        color = \"primary\"\n      )\n    ),\n    sidebar = dashboardSidebar(),\n    body = dashboardBody(\n      uiOutput(\"my_dynamic_box\")\n    ),\n    controlbar = dashboardControlbar(),\n    title = \"DashboardPage\"\n  ),\n  server = function(input, output, session) {\n\n    output$my_plot &lt;- renderPlot({\n      ggplot(data = mtcars) +\n        geom_bar(aes_string(y = input$var))\n    })\n\n    output$my_dynamic_box &lt;- renderUI({\n      box(\n        plotOutput(\"my_plot\"),\n        title = paste0(\"Currently showing variable:\", input$var),\n        width = 12,\n        sidebar = boxSidebar(\n          id = \"sidebarid\",\n          startOpen = TRUE,\n          selectInput(\"var\", \"Select variable:\", choices = colnames(mtcars))\n        ))\n    })\n  }\n)\n\n\n\n\n\n\nNow try changing variables and see what happens… as soon as you change the value in the selectInput(), it goes back to selecting mpg! The reason is because the whole box gets re-rendered, including the selectInput(), and its starting, default, value (even if we did not specify one, this value is simply the first element of colnames(mtcars) which happens to be mpg). So now you see the problem; I have to re-render part of the UI, but doing so puts the selectInput() on its default value… so I need to be able to only to re-render the title, not the whole box (or move the selectInput() outside the boxes, but that was not an acceptable solution in my case).\n\n\nSo there we have it, we’re done with the problem statement. Now on to the solution.\n\n\n\nUPDATE\n\n\nIt turns out that it’s not needed to use JS for this special use case! {bs4Dash} comes with a function, called updateBox() which updates a targeted box. You can read about it here. Thanks to {bs4Dash}’s author, David Granjon for the heads-up!\n\n\nWell, even though my specific use case does not actually need Javascript, you can continue reading, because in case your use case does not have an happy ending like mine, the blog post is still relevant!"
  },
  {
    "objectID": "posts/2022-10-01-why_js_shiny.html#javascript-to-the-rescue",
    "href": "posts/2022-10-01-why_js_shiny.html#javascript-to-the-rescue",
    "title": "Why and how to use JS in your Shiny app",
    "section": "\nJavascript to the rescue\n",
    "text": "Javascript to the rescue\n\n\nLet me be very clear: I know almost nothing about Javascript. I just knew a couple of things: Javascript can be used for exactly what I needed to do (change part of the UI), and it does so by making use of the DOM (which I also knew a little bit about). The DOM is a tree-like representation of a webpage. So you have your webpage’s header, body, footer, and inside of the body, for example, in my case here, we have a box with a title. That title has an address, if you will, represented by one of the branches of the DOM. At least, that’s the way I understand it.\n\n\nIn any case, it is possible to integrate JS scripts inside any Shiny app. So here’s what I thought I would do: I would create the title of my box as a reactive value inside the server part of my app, and would then pass this title to a JS script which would then, using the DOM, knock at the door of the box and give it its new title. Easier written in plain English than in R/JS though. But surprisingly enough, it didn’t turn out to be that complicated, and even someone (me) with only a very, very, shallow knowledge of JS could do it in less than an hour. First thing’s first, we need to read this documentation: Communicating with Shiny via JavaScript, especially the second part, From R to JavaScript.\n\n\nBecause we won’t re-render the whole box, let’s simply reuse the app from before, in which the box is static. The script is below, but first read the following lines, then take a look at the script:\n\n\n\nI have defined a JS script outside the app, called box_title_js;\n\n\nRead the title of the box;\n\n\nIn the server, there is now an observeEvent(),\n\n\nIn the UI you’ll see the following line (inside the box’s definition): tags\\(script(box_title_js)&lt;/code&gt;, which executes the JS script &lt;code&gt;box_title_js&lt;/code&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;The script knows which element to change thanks to &lt;code&gt;\\)(“#box_plot h3”). That’s a bit of jQuery, which comes bundled with Shiny. jQuery allows you to query elements of the DOM. If you know nothing about it, like me, you should read this. This should give you the basic knowledge such that you’ll eventually somehow manage to select the element you actually want to change.\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bs4Dash)\n\n# This is the bit of JS that will update the title\n# From what I could gather, $(bla bla) references the object,\n# here the title, and `.html()` is a getter/setter.\n# So $(\"#box_plot h3\").html() means \"take whatever is called #box_plot h3\n# (h3 is the class of the title, meaning, it’s a header3 bit of text)\n# and set its html to whatever string is inside `html()`\"\nbox_title_js &lt;- '\n  Shiny.addCustomMessageHandler(\"box_title\", function(title) {\n    $(\"#box_plot h3\").html(title)\n  });\n'\n\ndata(mtcars)\n\nshinyApp(\n  ui = dashboardPage(\n    header = dashboardHeader(\n      title = dashboardBrand(\n        title = \"Welcome to my award-winning dashboard!\",\n        color = \"primary\"\n      )\n    ),\n    sidebar = dashboardSidebar(),\n    body = dashboardBody(\n      box(id = \"box_plot\", #We need to give the box an ID now, to help query it\n        plotOutput(\"my_plot\"),\n        tags$script(box_title_js), #Integration of the JS script into the app\n        title = \"This title will change dynamically. You won’t even see this sentence!\",\n        width = 12,\n        sidebar = boxSidebar(\n          id = \"sidebarid\",\n          startOpen = TRUE,\n          selectInput(\"var\", \"Select variable:\", choices = colnames(mtcars))\n        ))\n    ),\n    controlbar = dashboardControlbar(),\n    title = \"DashboardPage\"\n  ),\n  server = function(input, output, session) {\n\n    # The following lines put the title together, and send them to the JS script\n    observe({\n      session$sendCustomMessage(\n                \"box_title\",\n                paste0(\"Currently showing variable:\", input$var)\n              )\n    })\n\n    output$my_plot &lt;- renderPlot({\n      ggplot(data = mtcars) +\n        geom_bar(aes_string(y = input$var))\n    })\n\n  }\n)\n\nThe video below shows how the app works:\n\n\n\n\n\n\n\nThe idea is as follows: a bit of code puts the title together in the server part of your app. This title gets sent to a JS script that you define somewhere where the UI and the server part know about it (for example, in your global.R file). In the UI you can now integrate the JS script using tags$script(). And you’re done!\n\n\nJust for fun, let’s have a more complex example; I’ll change the background color of the box using JS as well, but depending on the selected column, the color will be different. For this, I only need to change the JS script. Using a simple if-then-else statement, I set the background color of the box to red if the selected column is mpg, else I set it to blue. The way I do this, is by using jQuery again to target the element I want to change, in this case, the object with the id “box_plot” and of class “.card-body”. Take a look at the script:\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bs4Dash)\n\n# This is the bit of JS that will update the title\n# From what I could gather, $(bla bla) references the object,\n# here the title, and `.html()` is a getter/setter.\n# So $(\"#box_plot h3\").html() means \"take whatever is called #box_plot h3\n# (h3 is the class of the title, meaning, it’s a header3 bit of text)\n# and set its html to whatever string is inside `html()`\"\nbox_title_js &lt;- '\n  Shiny.addCustomMessageHandler(\"box_title\", function(title) {\n  if(title.includes(\"mpg\")){\n    colour = \"red\"\n  } else {\n    colour = \"blue\"\n  }\n    $(\"#box_plot h3\").html(title)\n    $(\"#box_plot .card-body\").css(\"background-color\", colour)\n  });\n'\n\ndata(mtcars)\n\nshinyApp(\n  ui = dashboardPage(\n    header = dashboardHeader(\n      title = dashboardBrand(\n        title = \"Welcome to my award-winning dashboard!\",\n        color = \"primary\"\n      )\n    ),\n    sidebar = dashboardSidebar(),\n    body = dashboardBody(\n      box(id = \"box_plot\", #We need to give the box an ID now, to help query it\n        plotOutput(\"my_plot\"),\n        tags$script(box_title_js), #Integration of the JS script into the app\n        title = \"This title will change dynamically. You won’t even see this sentence!\",\n        width = 12,\n        sidebar = boxSidebar(\n          id = \"sidebarid\",\n          startOpen = TRUE,\n          selectInput(\"var\", \"Select variable:\", choices = colnames(mtcars))\n        ))\n    ),\n    controlbar = dashboardControlbar(),\n    title = \"DashboardPage\"\n  ),\n  server = function(input, output, session) {\n\n    # The following lines put the title together, and send them to the JS script\n    observe({\n      session$sendCustomMessage(\n                \"box_title\",\n                paste0(\"Currently showing variable:\", input$var)\n              )\n    })\n\n    output$my_plot &lt;- renderPlot({\n      ggplot(data = mtcars) +\n        geom_bar(aes_string(y = input$var))\n    })\n\n  }\n)\n\n\n\n\n\n\nHow did I know that I needed to target card-body? To find out, go to your browser, right click on the box and select Inspect (sometimes inspect element). Navigating through the source of your app in this way allows you to find the classes and ids of things you need to target, which you then can use as a query. You can even try changing stuff in real time, as the video below shows:\n\n\n\n\n\n\n\nIt’s actually scary what you can achieve with only some cursory knowledge of JS. I’m sure nothing bad ever happens because clueless beginners like me start playing around with JS."
  },
  {
    "objectID": "posts/2023-03-07-dry_wit.html",
    "href": "posts/2023-03-07-dry_wit.html",
    "title": "Software engineering techniques that non-programmers who write a lot of code can benefit from — the DRY WIT approach",
    "section": "",
    "text": "Data scientists, statisticians, analysts, researchers, and many other professionals write a lot of code.\nNot only do they write a lot of code, but they must also read and review a lot of code as well. They either work in teams and need to review each other’s code, or need to be able to reproduce results from past projects, be it for peer review or auditing purposes. And yet, they never, or very rarely, get taught the tools and techniques that would make the process of writing, collaborating, reviewing and reproducing projects possible.\nWhich is truly unfortunate because software engineers face the same challenges and solved them decades ago. Software engineers developed a set of project management techniques and tools that non-programmers who write a lot of code could benefit from as well.\nThese tools and techniques can be used right from the start of a project at a minimal cost, such that the analysis is well-tested, well-documented, trustworthy and reproducible by design. Projects are going to be reproducible simply because they were engineered, from the start, to be reproducible.\nBut all these tools, frameworks and techniques boil down to two acronyms that I like to keep in my head at all times:\nDRY WIT: by systematically avoiding not to repeat yourself and by writing everything down, projects become well-tested, well-documented, trustworthy and reproducible by design. Why is that?"
  },
  {
    "objectID": "posts/2023-03-07-dry_wit.html#dry-dont-repeat-yourself",
    "href": "posts/2023-03-07-dry_wit.html#dry-dont-repeat-yourself",
    "title": "Software engineering techniques that non-programmers who write a lot of code can benefit from — the DRY WIT approach",
    "section": "\nDRY: Don’t Repeat Yourself\n",
    "text": "DRY: Don’t Repeat Yourself\n\n\nLet’s start with DRY: what does it mean not having to repeat oneself? It means:\n\n\n\nusing functions instead of copy-and-pasting bits of code here and there;\n\n\nusing literate programming, to avoid having to copy and paste graphs and tables into word or pdf documents;\n\n\ntreating code as data and making use of templating.\n\n\n\nThe most widely used programming languages for data science/statistics, Python and R, both have first-class functions. This means that functions can be manipulated like any other object. So something like:\n\nReduce(`+`, seq(1:100))\n## [1] 5050\n\nwhere the function +() gets used as an argument of the higher-order Reduce() function is absolutely valid (and so is Python’s equivalent reduce from functools) and avoids having to use a for-loop which can lead to other issues. Generally speaking, the functional programming paradigm lends itself very naturally to data analysis tasks, and in my opinion data scientists and statisticians would benefit a lot from adopting this paradigm.\n\n\nLiterate programming is another tool that needs to be in the toolbox of any person analysing data. This is because at the end of the day, the results of an analysis need to be in some form of document. Without literate programming, this is how you would draft reports:\n\n\n\n\n\n\n\nBut with literate programming, this is how this loop would look like:\n\n\n\n\n\n\n\nQuarto is the latest open-source scientific and technical publishing system that leverages Pandoc and supports R, Python, Julia and ObservableJs right out of the box.\n\n\nBelow is a little Quarto Hello World:\n\n---\noutput: pdf\n\n---\n\nIn this example we embed parts of the examples from the\n\\texttt{kruskal.test} help page into a LaTeX document:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata (airquality)\nkruskal.test(Ozone ~ Month, data = airquality)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n    Kruskal-Wallis rank sum test\n\ndata:  Ozone by Month\nKruskal-Wallis chi-squared = 29.267, df = 4, p-value = 6.901e-06\n```\n\n\n:::\n:::\n\n\n\nwhich shows that the location parameter of the Ozone\ndistribution varies significantly from month to month.\nFinally we include a boxplot of the data:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2023-03-07-dry_wit_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\nCompiling this document results in the following:\n\n\n\n\nExample from Leisch’s 2002 paper.\n\n\n\nOf course, you could use Python code chunks instead of R, you could also compile this document to Word, or HTML, or anything else really. By combining code and prose, the process of data analysis gets streamlined and we don’t need to repeat ourselves copy and pasting images and tables into Word documents.\n\n\nFinally, treating code as data is also quite useful. This means that it is possible to compute on the language itself. This is a more advanced topic, but definitely worth the effort. As an illustration, consider the following R toy example:\n\nshow_and_eval &lt;- function(f, ...){\n  f &lt;- deparse(substitute(f))\n  dots &lt;- list(...)\n  message(\"Evaluating: \", f, \"() with arguments: \", deparse(dots))\n  do.call(f, dots)\n}\n\nRunning this function does the following:\n\nshow_and_eval(sqrt, 2)\n## Evaluating: sqrt() with arguments: list(2)\n## [1] 1.414214\nshow_and_eval(mean, x = c(NA, 1, 2))\n## Evaluating: mean() with arguments: list(x = c(NA, 1, 2))\n## [1] NA\nshow_and_eval(mean, x = c(NA, 1, 2), na.rm = TRUE)\n## Evaluating: mean() with arguments: list(x = c(NA, 1, 2), na.rm = TRUE)\n## [1] 1.5\n\nThis is incredibly useful when writing packages (to know more about these techniques in the R programming language, read the chapter Metaprogramming from Advanced R)."
  },
  {
    "objectID": "posts/2023-03-07-dry_wit.html#wit-write-it-down",
    "href": "posts/2023-03-07-dry_wit.html#wit-write-it-down",
    "title": "Software engineering techniques that non-programmers who write a lot of code can benefit from — the DRY WIT approach",
    "section": "\nWIT: Write IT down\n",
    "text": "WIT: Write IT down\n\n\nNow on the WIT bit: write it down. You’ve just written a function. To see if it works correctly, you test it in the interactive console. You execute the test, see that it works, and move on. But wait! What you just did is called a unit test. Instead of writing that in the console and then never use it ever again, write it down in a script. Now you’ve got a unit test for that function that you can execute each time you update that function’s code, and make sure that it keeps working as expected. There are many unit testing frameworks that can help you how to write unit tests consistently and run them automatically.\n\n\nDocumentation: write it down! How does the function work? What are its inputs? Its outputs? What else should the user know to make it work? Very often, documentation is but a series of comments in your scripts. That’s already nice, but using literate programming, you could also turn these comments into proper documentation. You could use docstrings in Python or {roxygen2} style comments in R.\n\n\nAnother classic: you correct some data manually in the raw dataset (very often a .csv or .xlsx file). For example, when dealing with data on people, sex is sometimes “M” or “F”, sometimes “Male” or “Female”, sometimes “1” or “0”. You spot a couple of inconsistencies and decide to quickly correct them by hand. Maybe only 3 men were coded as “Male” so you simply erase the “ale” and go on with your project. Stop!\n\n\nWrite IT down!\n\n\nWrite a couple of lines of code that does the replacement for you. Not only will this leave a trace, it will ensure that when you get an update to that data in the future you don’t have to remember to have to change it by hand.\n\n\nYou should aim at completely eliminating any required manual intervention when building your project. A project that can be fully run by a machine is easier to debug, its execution can be scheduled and can be iterated over very quickly.\n\n\nSomething else that you should write down, or rather, let another tool do it for you: how you collaborate with your teammates. For this, you should be using Git. Who changed what part of what function when? If the project’s code is versioned, Git writes it down for you. You want to experiment with a new feature? Write it down by creating a new branch and going nuts. There’s something wrong in the code? Write it down as an issue on your versioning platform (usually Github).\n\n\nThere are many more topics that us disciplines of the data could learn from software engineers. I’m currently working on a free ebook that you can read here that teaches these techniques. If this post opened your appetite, give the book a go!"
  },
  {
    "objectID": "posts/2015-11-11-bootstrapping-did-with-r.html",
    "href": "posts/2015-11-11-bootstrapping-did-with-r.html",
    "title": "Bootstrapping standard errors for difference-in-differences estimation with R",
    "section": "",
    "text": "I’m currently working on a paper (with my colleague Vincent Vergnat who is also a Phd candidate at BETA) where I want to estimate the causal impact of the birth of a child on hourly and daily wages as well as yearly worked hours. For this we are using non-parametric difference-in-differences (henceforth DiD) and thus have to bootstrap the standard errors. In this post, I show how this is possible using the function boot.\n\n\nFor this we are going to replicate the example from Wooldridge’s Econometric Analysis of Cross Section and Panel Data and more specifically the example on page 415. You can download the data for R here. The question we are going to try to answer is how much does the price of housing decrease due to the presence of an incinerator in the neighborhood?\n\n\nFirst put the data in a folder and set the correct working directory and load the boot library.\n\nlibrary(boot)\nsetwd(\"/home/path/to/data/kiel data/\")\nload(\"kielmc.RData\")\n\nNow you need to write a function that takes the data as an argument, as well as an indices argument. This argument is used by the boot function to select samples. This function should return the statistic you’re interested in, in our case, the DiD estimate.\n\nrun_DiD &lt;- function(my_data, indices){\n    d &lt;- my_data[indices,]\n    return(\n        mean(d$rprice[d$year==1981 & d$nearinc==1]) - \n        mean(d$rprice[d$year==1981 & d$nearinc==0]) - \n        (mean(d$rprice[d$year==1978 & d$nearinc==1]) - \n        mean(d$rprice[d$year==1978 & d$nearinc==0]))\n    )\n}\n\nYou’re almost done! To bootstrap your DiD estimate you just need to use the boot function. If you have cpu with multiple cores (which you should, single core machines are quite outdated by now) you can even parallelize the bootstrapping.\n\nboot_est &lt;- boot(data, run_DiD, R=1000, parallel=\"multicore\", ncpus = 2)\n\nNow you should just take a look at your estimates:\n\nboot_est\n \nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = data, statistic = run_DiD, R = 1000, parallel = \"multicore\", \n ncpus = 2)\n\n\nBootstrap Statistics :\n original    bias    std. error\nt1* -11863.9 -553.3393    8580.435\n\nThese results are very similar to the ones in the book, only the standard error is higher.\n\n\nYou can get confidence intervals like this:\n\nquantile(boot_est$t, c(0.025, 0.975))\n##       2.5%      97.5% \n## -30186.397   3456.133\n\nor a t-statistic:\n\nboot_est$t0/sd(boot_est$t)\n## [1] -1.382669\n\nOr the density of the replications:\n\nplot(density(boot_est$t))\n\n&lt;img src=\"/img/density_did.png\" width=\"80%\" height=\"auto\" / width=\"80%\" height=\"auto\"&gt;&lt;/a&gt;\n\n\nJust as in the book, we find that the DiD estimate is not significant to the 5% level."
  },
  {
    "objectID": "posts/2016-06-21-careful-with-trycatch.html",
    "href": "posts/2016-06-21-careful-with-trycatch.html",
    "title": "Careful with tryCatch",
    "section": "",
    "text": "tryCatch is one of the functions that allows the users to handle errors in a simple way. With it, you can do things like: if(error), then(do this).\n\n\nTake the following example:\n\nsqrt(\"a\")\nError in sqrt(\"a\") : non-numeric argument to mathematical function\n\nNow maybe you’d want something to happen when such an error happens. You can achieve that with tryCatch:\n\ntryCatch(sqrt(\"a\"), error=function(e) print(\"You can't take the square root of a character, silly!\"))\n## [1] \"You can't take the square root of a character, silly!\"\n\nWhy am I interested in tryCatch?\n\n\nI am currently working with dates, specifically birthdays of people in my data sets. For a given mother, the birthday of her child is given in three distinct columns: a column for the child’s birth year, birth month and birth day respectively. I’ve wanted to put everything in a single column and convert the birthday to unix time (I have a very good reason to do that, but I won’t bore you with the details).\n\n\nLet’s create some data:\n\nmother &lt;- as.data.frame(list(month=12, day=1, year=1988))\n\nIn my data, there’s a lot more columns of course, such as the mother’s wage, education level, etc, but for illustration purposes, this is all that’s needed.\n\n\nNow, to create this birthday column:\n\nmother$birth1 &lt;- as.POSIXct(paste0(as.character(mother$year), \n                                   \"-\", as.character(mother$month), \n                                   \"-\", as.character(mother$day)), \n                            origin=\"1970-01-01\")\n\nand to convert it to unix time:\n\nmother$birth1 &lt;- as.numeric(as.POSIXct(paste0(as.character(mother$year), \n                                              \"-\", as.character(mother$month), \n                                              \"-\", as.character(mother$day)),\n                                       origin=\"1970-01-01\"))\n\nprint(mother)\n##   month day year    birth1\n## 1    12   1 1988 596934000\n\nNow let’s see what happens in this other example here:\n\nmother2 &lt;- as.data.frame(list(month=2, day=30, year=1988))\n\nmother2$birth1 &lt;- as.POSIXct(paste0(as.character(mother2$year), \n                                    \"-\", as.character(mother2$month), \n                                    \"-\", as.character(mother2$day)), \n                             origin=\"1970-01-01\")\n\nThis is what happens:\n\nError in as.POSIXlt.character(x, tz, ...) : \n  character string is not in a standard unambiguous format\n\nThis error is to be expected; there is no 30th of February! It turns out that in some rare cases, weird dates like this exist in my data. Probably some encoding errors. Not a problem I thought, I could use tryCatch and return NA in the case of an error.\n\nmother2 &lt;- as.data.frame(list(month=2, day=30, year=1988))\n\nmother2$birth1 &lt;- tryCatch(as.POSIXct(paste0(as.character(mother2$year), \n                                    \"-\", as.character(mother2$month), \n                                    \"-\", as.character(mother2$day)), \n                             origin=\"1970-01-01\"), error=function(e) NA)\n\nprint(mother2)\n##   month day year birth1\n## 1     2  30 1988     NA\n\nPretty great, right? Well, no. Take a look at what happens in this case:\n\nmother &lt;- as.data.frame(list(month=c(12, 2), day=c(1, 30), year=c(1988, 1987)))\nprint(mother)\n##   month day year\n## 1    12   1 1988\n## 2     2  30 1987\n\nWe’d expect to have a correct date for the first mother and an NA for the second. However, this is what happens\n\nmother$birth1 &lt;- tryCatch(as.POSIXct(paste0(as.character(mother$year), \n                                    \"-\", as.character(mother$month), \n                                    \"-\", as.character(mother$day)), \n                             origin=\"1970-01-01\"), error=function(e) NA)\n\nprint(mother)\n##   month day year birth1\n## 1    12   1 1988     NA\n## 2     2  30 1987     NA\n\nAs you can see, we now have an NA for both mothers! That’s actually to be expected. Indeed, this little example illustrates it well:\n\nsqrt(c(4, 9, \"haha\"))\nError in sqrt(c(4, 9, \"haha\")) : \n  non-numeric argument to mathematical function\n\nBut you’d like to have this:\n\n[1]  2  3 NA\n\nSo you could make the same mistake as myself and use tryCatch:\n\ntryCatch(sqrt(c(4, 9, \"haha\")), error=function(e) NA)\n## [1] NA\n\nBut you only get NA in return. That’s actually completely normal, but it took me off-guard and I spent quite some time to figure out what was happening. Especially because I had written unit tests to test my function create_birthdays() that was doing the above computations and all tests were passing! The problem was that in my tests, I only had a single individual, so for a wrong date, having NA for this individual was expected behaviour. But in a panel, only some individuals have a weird date like the 30th of February, but because of those, the whole column was filled with NA’s! What I’m doing now is trying to either remove these weird birthdays (there are mothers whose children were born on the 99-99-9999. Documentation is lacking, but this probably means missing value), or tyring to figure out how to only get NA’s for the “weird” dates. I guess that the answer lies with dplyr’s group_by() and mutate() to compute this birthdays for each individual separately."
  },
  {
    "objectID": "posts/2023-03-18-docxtractr.html",
    "href": "posts/2023-03-18-docxtractr.html",
    "title": "Automating checks of handcrafted Word tables with {docxtractr}",
    "section": "",
    "text": "Unfortunately not everyone knows about literate programming so many tables in Word documents are “generated” by hand (generated is really too strong a word) and what very often happens is that these handcrafted tables have typos. Usually it’s totals that are wrong. Checking the totals in these tables by hand with a pocket calculator is a tedious, long and boring job.\nSo as my job’s statistician but also kinda automation dude that types all day in a weird black box, I’ve been asked if it were possible to automate these checks on tables in a Word document. And of course, the answer is yes, because whatever you need to get done, there’s an R package for that!\nThere are, to my knowledge, 2 packages that we can use to get tables from a Word document into R (an activity that I will now refer to as office-scraping).\nThese packages are {officer} and {docxtractr}. For his particular task I’ve used {docxtractr}. The reason is that {docxtractr} returns the table “as-is”, while {officer} returns a tibble where each cell of the table gets a row in the tibble. {officer}’s representation of the scraped tables might be useful in some cases, but in this particular case, {docxtractr}’s approach was exactly what I needed.\nFirst of all, we need a Word document with some tables.Here’s one I’ve prepared that contains two tables that look close to the ones I had to deal with. In the actual document, there were hundreds of such tables. Here’s a picture of the tables in case you can’t open the document:\nThe first table is divided by departments of the company, and each section of the table has its own sub-total. As stated in the beginning, the goal is to check for typos by recomputing the sub-totals and totals and then comparing the original tables with the ones where the totals were recomputed.\nThe problem we will face with each table are the merged cells; if there were no merged cells, scraping them with {docxtractr} would be trivially simple, but because of these merged cells, we will have to write quite a lot of code to get them in a format that we can actually use."
  },
  {
    "objectID": "posts/2023-03-18-docxtractr.html#extracting-the-tables-using-docxtractr",
    "href": "posts/2023-03-18-docxtractr.html#extracting-the-tables-using-docxtractr",
    "title": "Automating checks of handcrafted Word tables with {docxtractr}",
    "section": "\nExtracting the tables using {docxtractr}\n",
    "text": "Extracting the tables using {docxtractr}\n\n\n{docxtractr} has a very handy function that gets all the tables from a Word document and puts them into a list (it’s also possible to extract other stuff like comments). Let’s start by loading {dplyr} (for the rest of the functions, I’ll use the package::function() notation to make it clear where the functions come from):\n\nlibrary(dplyr)\n\nLet’s now read the document using {docxtractr}:\n\ndoc_raw &lt;- docxtractr::read_docx(\"report.docx\")\n\nAnd let’s get all the tables:\n\nlist_tables &lt;- docxtractr::docx_extract_all_tbls(doc_raw)\n\nLet’s now take a look at the second element of the list, which corresponds to the second table (I’m starting with the second table because it’s the smallest of the two):\n\ntable_1 &lt;- list_tables[[1]] %&gt;%\n  janitor::clean_names()\n\ntable_2 &lt;- list_tables[[2]] %&gt;%\n  janitor::clean_names()\n\ntable_2\n## # A tibble: 8 × 11\n##   company            x2020 x2021 x2022 na    na_2  na_3  na_4  na_5  na_6  na_7 \n##   &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n## 1 \"\"                 M     F     Total M     F     Total M     F     Total &lt;NA&gt; \n## 2 \"Luxembourg branc… Work… 92    124   210   87    129   216   99    129   228  \n## 3 \"\"                 Mana… 22    81    103   28    81    109   26    85    112  \n## 4 \"\"                 Assi… 1     0     10    1     0     1     1     0     1    \n## 5 \"\"                 Dire… 3     1     4     0     0     0     0     1     0    \n## 6 \"External consult… 38    55    95    35    64    99    42    70    112   &lt;NA&gt; \n## 7 \"Statisticians\"    0     0     0     0     0     0     0     0     0     &lt;NA&gt; \n## 8 \"Total\"            156   263   419   151   274   425   168   285   453   &lt;NA&gt;\n\nAs you can see, because of the merged cells, the rows are not all aligned with the columns. So we need to split the table, and treat the two parts separately. I’m starting with the part of the table where the rows are correctly aligned with the columns. This is just a matter of renaming some columns, and converting the numbers (that are represented as characters) into numerics:\n\ntable_2_1 &lt;- table_2 %&gt;%\n  select(-company) %&gt;%\n  filter(!is.na(na_7)) %&gt;%\n  purrr::set_names(\n    c(\"worker_type\",\n      \"m_2020\",\n      \"f_2020\",\n      \"t_2020\",\n      \"m_2021\",\n      \"f_2021\",\n      \"t_2021\",\n      \"m_2022\",\n      \"f_2022\",\n      \"t_2022\"\n      )\n    ) %&gt;%\n  mutate(across(!starts_with(\"worker\"),\n                as.numeric))\n\ntable_2_1\n## # A tibble: 4 × 10\n##   worker_type m_2020 f_2020 t_2020 m_2021 f_2021 t_2021 m_2022 f_2022 t_2022\n##   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n## 1 Workers         92    124    210     87    129    216     99    129    228\n## 2 Managers        22     81    103     28     81    109     26     85    112\n## 3 Assistants       1      0     10      1      0      1      1      0      1\n## 4 Directors        3      1      4      0      0      0      0      1      0\n\nLet’s now deal with the second part of the table. This is the part of the table where the rows were not aligned with the columns, due to the merged cells. The operations are essentially the same as before, the difference is that we need to remove a different column (here we remove na_7, before it was company):\n\ntable_2_2 &lt;- table_2 %&gt;%\n  filter(is.na(na_7)) %&gt;%\n  select(-na_7) %&gt;%\n  rename(worker_type = company) %&gt;%\n  filter(worker_type != \"\") %&gt;%\n  purrr::set_names(\n           c(\"worker_type\",\n             \"m_2020\",\n             \"f_2020\",\n             \"t_2020\",\n             \"m_2021\",\n             \"f_2021\",\n             \"t_2021\",\n             \"m_2022\",\n             \"f_2022\",\n             \"t_2022\"\n             )\n         ) %&gt;%\n  mutate(across(!starts_with(\"worker\"),\n                as.numeric))\n\ntable_2_2\n## # A tibble: 3 × 10\n##   worker_type     m_2020 f_2020 t_2020 m_2021 f_2021 t_2021 m_2022 f_2022 t_2022\n##   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n## 1 External consu…     38     55     95     35     64     99     42     70    112\n## 2 Statisticians        0      0      0      0      0      0      0      0      0\n## 3 Total              156    263    419    151    274    425    168    285    453\n\nI didn’t comment the operations, but if you’re following along, take some time to see what each line does.\n\n\nNow we can bind the rows and we end up with the table from the Word document as a flat and easy to manipulate data frame:\n\ntable_2_clean &lt;- bind_rows(\n  table_2_1,\n  table_2_2\n)\n\ntable_2_clean\n## # A tibble: 7 × 10\n##   worker_type     m_2020 f_2020 t_2020 m_2021 f_2021 t_2021 m_2022 f_2022 t_2022\n##   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n## 1 Workers             92    124    210     87    129    216     99    129    228\n## 2 Managers            22     81    103     28     81    109     26     85    112\n## 3 Assistants           1      0     10      1      0      1      1      0      1\n## 4 Directors            3      1      4      0      0      0      0      1      0\n## 5 External consu…     38     55     95     35     64     99     42     70    112\n## 6 Statisticians        0      0      0      0      0      0      0      0      0\n## 7 Total              156    263    419    151    274    425    168    285    453\n\nAll of this because of these merged cells! This may seem like a lot of work, but imagine that you need to check 50 such tables. You could put all the previous operations into a function and then simply apply that function over all the tables (which is exactly what I did at my job). So you end up with 50 cleaned tables in a matter of seconds. Now let’s not forget our original objective, we wanted to recompute the totals to check if everything was alright. In the operations below I remove the columns that represent the totals and remove the row with the grand totals as well. I then simply recompute the totals:\n\ntable_2_totals &lt;- table_2_clean %&gt;%\n  select(-starts_with(\"t_\")) %&gt;%\n  filter(worker_type != \"Total\") %&gt;%\n  mutate(\n    t_2020 = m_2020 + f_2020,\n    t_2021 = m_2021 + f_2021,\n    t_2022 = m_2022 + f_2022,\n    ) %&gt;%\n  select(\n    worker_type,\n    m_2020,\n    f_2020,\n    t_2020,\n    m_2021,\n    f_2021,\n    t_2021,\n    m_2022,\n    f_2022,\n    t_2022,\n    ) %&gt;%\n  janitor::adorn_totals()\n\nWe can now compare both data frames and see if there were mistakes:\n\ntable_2_clean == table_2_totals\n##      worker_type m_2020 f_2020 t_2020 m_2021 f_2021 t_2021 m_2022 f_2022 t_2022\n## [1,]        TRUE   TRUE   TRUE  FALSE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE\n## [2,]        TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE  FALSE\n## [3,]        TRUE   TRUE   TRUE  FALSE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE\n## [4,]        TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE  FALSE\n## [5,]        TRUE   TRUE   TRUE  FALSE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE\n## [6,]        TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE\n## [7,]        TRUE   TRUE  FALSE  FALSE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE\n\nWe do see a bunch of FALSE statements, so we need to check those! This is where some typos where found.\n\n\nLet’s now deal with table 1. The way we will handle this one will be very similar to the one before. It’s just that we have subtotals to deal with as well.\n\ntable_1\n## # A tibble: 32 × 8\n##    by_department   fte        persons na    na_2  na_3  na_4  na_5 \n##    &lt;chr&gt;           &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n##  1 \"\"              M          F       Total M     F     Total &lt;NA&gt; \n##  2 \"Dep A\"         Workers    12,30   33,40 55,70 13    36    59   \n##  3 \"\"              Managers   3,80    19,90 15,70 4     19    19   \n##  4 \"\"              Assistants 0,00    0,00  0,00  0     0     0    \n##  5 \"\"              Directors  0,20    0,00  0,20  1     0     1    \n##  6 \"Total – Dep A\" 26,30      45,30   71,60 28,00 51,00 79,00 &lt;NA&gt; \n##  7 \"Dep B\"         Workers    31,80   39,60 71,40 32    41    73   \n##  8 \"\"              Managers   3,00    13,50 16,50 3     15    18   \n##  9 \"\"              Assistants 0,00    0,00  0,00  0     0     0    \n## 10 \"\"              Directors  0,20    0,20  0,40  1     1     2    \n## # … with 22 more rows\n\nHere as well, we have a problem with merged cells. But only the rows with the totals are affected. So just like before, we can split that into two tables and deal with the two parts separately:\n\ntable_1_1 &lt;- table_1 %&gt;%\n  filter(!grepl(\"(t|T)otal\", by_department),\n         fte != \"M\") %&gt;%\n  purrr::set_names(\n           c(\"department\",\n             \"worker_type\",\n             \"m_fte\",\n             \"f_fte\",\n             \"t_fte\",\n             \"m_hc\",\n             \"f_hc\",\n             \"t_hc\"\n             )\n         ) %&gt;%\n  mutate(department = ifelse(department == \"\",\n                              NA,\n                              department)) %&gt;%\n  tidyr::fill(department, .direction = \"down\") %&gt;%\n  mutate(across(contains(\"fte\"),\n                \\(x)(gsub(pattern = \",\", replacement = \".\", x = x))),\n         across(-c(\"department\", \"worker_type\"),\n                as.numeric)) %&gt;%\n  as.data.frame()\n\nHere again, it’s really worth it to take your time going through all the different commands.\n\n\nLet’s now clean the totals:\n\ntable_1_2 &lt;- table_1 %&gt;%\n  filter(grepl(\"(t|T)otal\", by_department),\n         fte != \"M\") %&gt;%\n  select(by_department, na_5, everything()) %&gt;%\n  purrr::set_names(\n           c(\"department\",\n             \"worker_type\",\n             \"m_fte\",\n             \"f_fte\",\n             \"t_fte\",\n             \"m_hc\",\n             \"f_hc\",\n             \"t_hc\"\n             )\n         ) %&gt;%\n  tidyr::fill(department, .direction = \"down\") %&gt;%\n  mutate(across(-c(\"department\", \"worker_type\"),\n                \\(x)(gsub(pattern = \",\", replacement = \".\", x = x))),\n         across(-c(\"department\", \"worker_type\"),\n                as.numeric)) %&gt;%\n  as.data.frame()\n\nFinally, we can bind the rows and we end up with a clean data frame:\n\ntable_1 &lt;- bind_rows(\n  table_1_1,\n  table_1_2\n)\n\ntable_1\n##       department worker_type m_fte  f_fte  t_fte m_hc f_hc t_hc\n## 1          Dep A     Workers  12.3  33.40  55.70   13   36   59\n## 2          Dep A    Managers   3.8  19.90  15.70    4   19   19\n## 3          Dep A  Assistants   0.0   0.00   0.00    0    0    0\n## 4          Dep A   Directors   0.2   0.00   0.20    1    0    1\n## 5          Dep B     Workers  31.8  39.60  71.40   32   41   73\n## 6          Dep B    Managers   3.0  13.50  16.50    3   15   18\n## 7          Dep B  Assistants   0.0   0.00   0.00    0    0    0\n## 8          Dep B   Directors   0.2   0.20   0.40    1    1    2\n## 9          Dep C     Workers  19.0  24.20  43.20   20   26   46\n## 10         Dep C    Managers   1.0   8.95   9.95    1   11   12\n## 11         Dep C  Assistants   0.0   0.00   0.00    0    0    0\n## 12         Dep C   Directors   0.0   0.00   0.00    0    0    0\n## 13         Dep D     Workers   7.5   5.00  12.50    8    5   13\n## 14         Dep D    Managers   0.5   1.60   2.10    1    2    3\n## 15         Dep D  Assistants   1.0   0.00   1.60    1    0    1\n## 16         Dep D   Directors   0.4   0.00   0.40    1    0    1\n## 17         Dep E     Workers  11.8  13.75  27.55   14   16   30\n## 18         Dep E    Managers  16.0  38.20  54.20   17   42   59\n## 19         Dep E  Assistants   0.0   0.00   0.00    0    0    0\n## 20         Dep E   Directors   0.0   0.00   0.00    0    0    0\n## 21         Dep F     Workers   0.2   0.00   0.20    1    0    1\n## 22         Dep F    Managers   0.0   0.00   0.00    0    0    0\n## 23         Dep F  Assistants   0.0   0.00   0.00    0    0    0\n## 24         Dep F   Directors   0.2   0.00   0.20    1    0    1\n## 25 Total – Dep A        &lt;NA&gt;  26.3  45.30  71.60   28   51   79\n## 26 Total – Dep B        &lt;NA&gt;  35.0  53.30  98.30   36   57   93\n## 27 Total – Dep C        &lt;NA&gt;  20.0  33.15  53.15   21   37   58\n## 28 Total – Dep D        &lt;NA&gt;   9.4   6.60  16.00   11    7   18\n## 29 Total – Dep E        &lt;NA&gt;  29.8  51.95  81.75   31   58   89\n## 30 Total – Dep F        &lt;NA&gt;   1.0   1.00   0.20    1    1    1\n## 31   Grand total        &lt;NA&gt; 101.5 195.40 316.90  129  216  345\n\nAgain, let’s not forget our objective: recomputing the totals to see if everything is alright. So because we need each sub-total, one per department, we will simply group by departments and use janitor::adorn_totals(). But janitor::adorn_totals() does not work on grouped data frames. So instead I use group_nest() to create a tibble with a list column, and then map janitor::adorn_totals:\n\ntable_1_subtotals &lt;- table_1 %&gt;%\n  filter(!grepl(\"(t|T)otal\", department)) %&gt;%\n  group_nest(department) %&gt;%\n  mutate(data = purrr::map(data, janitor::adorn_totals)) %&gt;%\n  tidyr::unnest(cols = data) %&gt;%\n  arrange(department) %&gt;%\n  as.data.frame()\n\nOk so in the table above I have the subtotals per department. Now, I need to compute the grand total:\n\ntable_1_total &lt;- table_1_subtotals %&gt;%\n  filter(grepl(\"Total\", worker_type)) %&gt;%\n  janitor::adorn_totals()\n\nNow I just need to bind the grand total to the table from before:\n\ntable_1_clean &lt;- bind_rows(\n  table_1_subtotals,\n  filter(\n    table_1_total,\n    department == \"Total\")\n)\n\ntable_1_clean\n##    department worker_type m_fte  f_fte  t_fte m_hc f_hc t_hc\n## 1       Dep A     Workers  12.3  33.40  55.70   13   36   59\n## 2       Dep A    Managers   3.8  19.90  15.70    4   19   19\n## 3       Dep A  Assistants   0.0   0.00   0.00    0    0    0\n## 4       Dep A   Directors   0.2   0.00   0.20    1    0    1\n## 5       Dep A       Total  16.3  53.30  71.60   18   55   79\n## 6       Dep B     Workers  31.8  39.60  71.40   32   41   73\n## 7       Dep B    Managers   3.0  13.50  16.50    3   15   18\n## 8       Dep B  Assistants   0.0   0.00   0.00    0    0    0\n## 9       Dep B   Directors   0.2   0.20   0.40    1    1    2\n## 10      Dep B       Total  35.0  53.30  88.30   36   57   93\n## 11      Dep C     Workers  19.0  24.20  43.20   20   26   46\n## 12      Dep C    Managers   1.0   8.95   9.95    1   11   12\n## 13      Dep C  Assistants   0.0   0.00   0.00    0    0    0\n## 14      Dep C   Directors   0.0   0.00   0.00    0    0    0\n## 15      Dep C       Total  20.0  33.15  53.15   21   37   58\n## 16      Dep D     Workers   7.5   5.00  12.50    8    5   13\n## 17      Dep D    Managers   0.5   1.60   2.10    1    2    3\n## 18      Dep D  Assistants   1.0   0.00   1.60    1    0    1\n## 19      Dep D   Directors   0.4   0.00   0.40    1    0    1\n## 20      Dep D       Total   9.4   6.60  16.60   11    7   18\n## 21      Dep E     Workers  11.8  13.75  27.55   14   16   30\n## 22      Dep E    Managers  16.0  38.20  54.20   17   42   59\n## 23      Dep E  Assistants   0.0   0.00   0.00    0    0    0\n## 24      Dep E   Directors   0.0   0.00   0.00    0    0    0\n## 25      Dep E       Total  27.8  51.95  81.75   31   58   89\n## 26      Dep F     Workers   0.2   0.00   0.20    1    0    1\n## 27      Dep F    Managers   0.0   0.00   0.00    0    0    0\n## 28      Dep F  Assistants   0.0   0.00   0.00    0    0    0\n## 29      Dep F   Directors   0.2   0.00   0.20    1    0    1\n## 30      Dep F       Total   0.4   0.00   0.40    2    0    2\n## 31      Total           - 108.9 198.30 311.80  119  214  339\n\nWe’re almost done! We now need to make sure that the rows are in the same order across the two tables. So we need to transform the original table from the Word document a little bit:\n\ntable_1 &lt;- table_1 %&gt;%\n  mutate(worker_type = ifelse(is.na(worker_type),\n                              \"Total\",\n                              worker_type)) %&gt;%\n  mutate(department = stringr::str_remove_all(department, \"Total – \"),\n         worker_type = ifelse(department == \"Grand total\",\n                              \"-\",\n                              worker_type),\n         department = ifelse(department == \"Grand total\",\n                             \"Total\",\n                             department))\n\nWe can now order them the same way, and finally compare them!\n\narrange(table_1, worker_type) == arrange(table_1_clean, worker_type)\n##       department worker_type m_fte f_fte t_fte  m_hc  f_hc  t_hc\n##  [1,]       TRUE        TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n##  [2,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n##  [3,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n##  [4,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n##  [5,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n##  [6,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n##  [7,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n##  [8,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n##  [9,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [10,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [11,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [12,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [13,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [14,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [15,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [16,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [17,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [18,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [19,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [20,]       TRUE        TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n## [21,]       TRUE        TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n## [22,]       TRUE        TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n## [23,]       TRUE        TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n## [24,]       TRUE        TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [25,]       TRUE        TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n## [26,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [27,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [28,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [29,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [30,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n## [31,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nLet’s see where all the FALSEs are:\n\narrange(table_1, worker_type)\n##    department worker_type m_fte  f_fte  t_fte m_hc f_hc t_hc\n## 1       Total           - 101.5 195.40 316.90  129  216  345\n## 2       Dep A  Assistants   0.0   0.00   0.00    0    0    0\n## 3       Dep B  Assistants   0.0   0.00   0.00    0    0    0\n## 4       Dep C  Assistants   0.0   0.00   0.00    0    0    0\n## 5       Dep D  Assistants   1.0   0.00   1.60    1    0    1\n## 6       Dep E  Assistants   0.0   0.00   0.00    0    0    0\n## 7       Dep F  Assistants   0.0   0.00   0.00    0    0    0\n## 8       Dep A   Directors   0.2   0.00   0.20    1    0    1\n## 9       Dep B   Directors   0.2   0.20   0.40    1    1    2\n## 10      Dep C   Directors   0.0   0.00   0.00    0    0    0\n## 11      Dep D   Directors   0.4   0.00   0.40    1    0    1\n## 12      Dep E   Directors   0.0   0.00   0.00    0    0    0\n## 13      Dep F   Directors   0.2   0.00   0.20    1    0    1\n## 14      Dep A    Managers   3.8  19.90  15.70    4   19   19\n## 15      Dep B    Managers   3.0  13.50  16.50    3   15   18\n## 16      Dep C    Managers   1.0   8.95   9.95    1   11   12\n## 17      Dep D    Managers   0.5   1.60   2.10    1    2    3\n## 18      Dep E    Managers  16.0  38.20  54.20   17   42   59\n## 19      Dep F    Managers   0.0   0.00   0.00    0    0    0\n## 20      Dep A       Total  26.3  45.30  71.60   28   51   79\n## 21      Dep B       Total  35.0  53.30  98.30   36   57   93\n## 22      Dep C       Total  20.0  33.15  53.15   21   37   58\n## 23      Dep D       Total   9.4   6.60  16.00   11    7   18\n## 24      Dep E       Total  29.8  51.95  81.75   31   58   89\n## 25      Dep F       Total   1.0   1.00   0.20    1    1    1\n## 26      Dep A     Workers  12.3  33.40  55.70   13   36   59\n## 27      Dep B     Workers  31.8  39.60  71.40   32   41   73\n## 28      Dep C     Workers  19.0  24.20  43.20   20   26   46\n## 29      Dep D     Workers   7.5   5.00  12.50    8    5   13\n## 30      Dep E     Workers  11.8  13.75  27.55   14   16   30\n## 31      Dep F     Workers   0.2   0.00   0.20    1    0    1\n\nWe see that the totals for department A and F are all wrong, and some others for other departments as well. Obviously the grand total is this completely wrong!"
  },
  {
    "objectID": "posts/2023-03-18-docxtractr.html#conclusion",
    "href": "posts/2023-03-18-docxtractr.html#conclusion",
    "title": "Automating checks of handcrafted Word tables with {docxtractr}",
    "section": "\nConclusion\n",
    "text": "Conclusion\n\n\nIf this looked complicated, let me assure you that, yes, it was. That’s quite typical with tasks like these: if the data is not in a tidy format, you really have to type a lot of code to make it tidy. But the advantage now is that I could put all this code into two functions, and apply them to as many tables as I need. This is what I did, and what I will be doing in the future as well. Now that the code is written, I can simply keep applying it to future reports that use the same table format."
  },
  {
    "objectID": "posts/2019-05-19-spacemacs.html",
    "href": "posts/2019-05-19-spacemacs.html",
    "title": "The never-ending editor war (?)",
    "section": "",
    "text": "The creation of this blog post was prompted by this tweet, asking an age-old question:\n\n{{% tweet “1128981852558123008” %}}\n\nThis is actually a very important question, that I have been asking myself for a long time. An IDE, and plain text editors, are a very important tools to anyone writing code. Most working hours are spent within such a program, which means that one has to be careful about choosing the right one, and once a choice is made, one has, in my humble opinion, learn as many features of this program as possible to become as efficient as possible.\n\n\nAs you can notice from the tweet above, I suggested the use of Spacemacs… and my tweet did not get any likes or retweets (as of the 19th of May, sympathetic readers of this blog have liked the tweet). It is to set this great injustice straight that I decided to write this blog post.\n\n\nSpacemacs is a strange beast; if vi and Emacs had a baby, it would certainly look like Spacemacs. So first of all, to understand what is Spacemacs, one has to know a bit about vi and Emacs.\n\n\n\n\n\nvi is a text editor with 43 years of history now. You might have heard of Vim (Vi IMproved) which is a modern clone of vi, from 1991. More recently, another clone has been getting popular, Neovim, started in 2014. Whatever version of vi however, its basic way of functioning remains the same. vi is a modal editor, meaning that the user has to switch between different modes to work on a text file. When vi is first started, the program will be in Normal mode. In this mode, trying to type a word will likely result in nothing, or unexpected behaviour; unexpected, if you’re not familiar with vi. For instance, in Normal mode, typing j will not show the character j on your screen. Instead, this will move the cursor down one line. Typing p will paste, u will undo the last action, y will yank (copy) etc…\n\n\nTo type text, first, one has to enter Insert mode, by typing i while in Normal mode. Only then is it possible to write text. To go back to Normal mode, type ESC. Other modes are Visual mode (from Normal mode press v), which allows the user to select text and Command-line mode which can be entered by keying : from Normal mode and allows to enter commands.\n\n\nNow you might be wondering why anyone would use such a convoluted way to type text. Well, this is because one can chain these commands quite easily to perform repetitive tasks very quickly. For instance, to delete a word, one types daw (in Normal mode), delete a word. To delete the next 3 words, you can type 3daw. To edit the text between, for instance, () you would type ci( (while in Normal mode and anywhere between the braces containing the text to edit), change in (. Same logic applies for ci[ for instance. Can you guess what ciw does? If you are in Normal mode, and you want to change the word the cursor is on, this command will erase the word and put you in Insert mode so that you can write the new word.\n\n\nThese are just basic reasons why vi (or its clones) are awesome. It is also possible to automate very long and complex tasks using macros. One starts a macro by typing q and then any letter of the alphabet to name it, for instance a. The user then performs the actions needed, types q again to stop the recording of the macro, and can then execute the macro with @a. If the user needs to execute the macro say, 10 times, 10@‌‌a does the trick. It is possible to extend vi’s functionalities by using plugins, but more on that down below.\n\n\nvi keybindings have inspired a lot of other programs. For instance, you can get extensions for popular web browsers that mimick vi keybindings, such as Tridayctl for Firefox, or Vivium for Chromium (or Google Chrome). There are even browsers that are built from scratch with support for vi keybinds, such as my personal favorite, qutebrowser. You can even go further and use a tiling window manager on GNU-Linux, for instance i3, which I use, or xmonad. You might need to configure those to behave more like vi, but it is possible. This means that by learning one set of keyboard shortcuts, (and the logic behind chaining the keystrokes to achieve what you want), you can master several different programs. This blog post only deals with the editor part, but as you can see, if you go down the rabbit hole enough, a new exciting world opens up.\n\n\nI will show some common vi operations below, but before that let’s discuss Emacs.\n\n\n\n\n\nI am not really familiar with Emacs; I know that Emacs users only swear by it (just like vi users only swear by vi), and that Emacs is not a modal editor. However, it contains a lot of functions that you can use by pressing ESC, CTRL, ALT or META (META is the Windows key on a regular PC keyboard) followed by regular keys. So the approach is different, but it is widely accepted that productivity of proficient Emacs users is very high too. Emacs was started in 1985, and the most popular clone is GNU Emacs. Emacs also features modes, but not in the same sense as vi. There are major and minor modes. For instance, if you’re editing a Python script, Emacs will be in Python mode, or if editing a Markdown file Emacs will be in Markdown mode. This will change the available functions to the user, as well as provide other niceties, such as auto-completion. Emacs is also easily extensible, which is another reason why it is so popular. Users can install packages for Emacs, just like R users would do for R, to extend Emacs’ capabilities. For instance, a very important package if you plan to use Emacs for statistics or data science is ESS, Emacs Speaks Statistics. Emacs contains other very high quality packages, and it seems to me (but don’t quote me on that) that Emacs’ packages are more mature and feature-rich than vi’s plugins. However, vi keybindings are really awesome. This is, I believe, what Sylvain Benner was thinking when he developed Spacemacs.\n\n\n\n\n\nSpacemacs’ motto is that The best editor is neither Emacs nor Vim, it’s Emacs and Vim!. Spacemacs is a version, or distribution of Emacs, that has a very specific way of doing things. However, since it’s built on top of Emacs, all of Emacs’ packages are available to the user, notably Evil, which is a package that makes Emacs mimick vi’s modal mode and keybindings (the name of this package tells you everything you need to know about what Emacs users think of vi users 😀)\n\n\nNot only does Spacemacs support Emacs packages, but Spacemacs also features so-called layers, which are configuration files that integrate one, or several packages, seamlessly into Spacemacs particular workflow. This particular workflow is what gave Spacemacs its name. Instead of relying on ESC, CTRL, ALT or META like Emacs, users can launch functions by typing Space in Normal mode and then a sequence of letters. For instance, Spaceqr restarts Spacemacs. And what’s more, you don’t actually need to learn these new key sequences. When you type Space, the minibuffer, a little popup window at the bottom of Spacemacs, appears and shows you all the options that you can type. For instance, typing b after Space opens up the buffer menu. Buffers are what could be called tabs in Rstudio. Here you can chose to delete a buffer, with d, create a new buffer with N, and many more options.\n\n\n\n\n\nEnough text, let’s get into the videos. But keep in mind the following: the videos below show the keystrokes I am typing to perform the actions. However, because I use the BÉPO keyboard layout, which is the french equivalent of the DVORAK layout, the keystrokes will be different than those in a regular vi guide, which are mainly written for the QWERTY layout. Also, to use Spacemacs for R, you need to enable the ESS layer, which I show how to do at the end. Enabling this layer will turn on auto-completion, as well as provide documentation in real time for your function in the minibuffer:\n\n\n\n\n\n\n\n\nThe first video shows Spacemacs divided into two windows. On the left, I am navigating around code using the T (move down) and S (move up) keys. To execute a region that I select, I type Spacemrr (this stands for Major mode Run Region). Then around second 5, I key O which switches to Insert mode one line below the line I was, type head(mtcars) and then ESC to switch back to Normal mode and run the line with Spacemrl (Major mode Run Line).\n\n\n\n\nYour browser does not support the video tag. \n\n\nIn this video, I show you how to switch between windows. Type SpaceN to switch to window N. At the end, I key dd which deletes a whole line.\n\n\n\n\nYour browser does not support the video tag. \n\n\nIn the video below, I show how to use the pipe operator with Spacemm. This is a keyboard shortcut that I have defined myself. You can also spot the auto-completion at work in this video. To run the code, I first select it with V, which selects the whole line the cursor is currently at and enters Visual mode. I then select the lines below with T and run the region with Spacemrr.\n\n\n\n\nYour browser does not support the video tag. \n\n\nHere I show how plotting behaves. When a plot is created, a new window is opened with the plot. This is a major shortcoming of using Spacemacs for R programming; there is not a dedicated buffer for plots, and it only shows the very last one created, so there is no way to keep all the plots created in the current session in a neat, dedicated buffer. It seems to be possible using Org-mode, which is an Emacs mode for writing notes, todos, and authoring documents. But I haven’t explored this option yet, mainly because in my case, only looking at one plot at a time is ok.\n\n\n\n\nYour browser does not support the video tag. \n\n\nHere I show how to quickly add text to the top of the document when at the cursor is at the bottom: I try to use the tabyl() function found in the {janitor} package, which I forgot to load. I quickly go all the way up with gg, then key yy to copy the first line, then P to paste it on the line below (p would paste it on the same line), type fv, to find the letter v from the word “tidyverse”, then type liw (which is the BÉPO equivalent of ciw for Change In Word) and finally change “tidyverse” to “janitor”. This seems overly complex, but once you get used to this way of working, you will wonder why you hadn’t tried vi sooner.\n\n\n\n\nYour browser does not support the video tag. \n\n\nHere I show how to do block comment. 8gg jumps to the 8th line, CTRLv starts block visual mode, which allows me to select a block of text. I select the first column of the text, G to jump all the way down, then A to enter insert mode at the end of the selection (actually, it would have been more logical to use I, which enters insert mode at the beginning of the selection) of the line and then add “#” to comment.\n\n\n\n\nYour browser does not support the video tag. \n\n\nHere I show how to delete a block of text:\n\n\n\n\nYour browser does not support the video tag. \n\n\nSearch and replace, by entering command-line mode (look at the very bottom of the window):\n\n\n\n\nYour browser does not support the video tag. \n\n\nI forgot to add “,” characters on a bunch of lines. I add the first “,” to the first line, go down and press ESC to exit Insert mode. Now in Normal mode, I type . to execute the last command, which is inserting a “,” character and going down a line. This dot command is a feature of vi, and it will always redo the last performed change.\n\n\n\n\nYour browser does not support the video tag. \n\n\nBut instead of typing . six times, just type 6. and be done with it:\n\n\n\n\nYour browser does not support the video tag. \n\n\nWhat if you want to do something more complex, involving several commands? Here the dot command won’t be enough, since it only replicates the last command, not more. For this you can define macros with **@**. I look for the “,” character, twice, and put the rest of the characters in the next line with enter. I then repeat this operation by executing the macro using @‌‌a repeatedly (@‌‌a because I saved the actions in a, but it could have been any other letter). I then undo my changes and execute the macro 5 times with 5@‌‌a.\n\n\n\n\nYour browser does not support the video tag. \n\n\nHere I show the undo tree (by typing Spaceua), which is a feature Spacemacs inherited from Emacs: it makes undoing changes and going back to a previous version of your script very easily:\n\n\n\n\nYour browser does not support the video tag. \n\n\nFinally, I show my Spacemacs configuration file. I show where one needs to specify the layers one wishes to use. For R, the ESS layer (which is a configuration file for the ESS Emacs package) is mandatory. As I explained above, it is also possible to use Emacs packages for which no layer is available. These are the packages under dotspacemacs-additional-packages. In my case I use:\n\ndotspacemacs-additional-packages '(polymode\n                                  poly-R\n                                  poly-noweb\n                                  poly-markdown)\n\nwhich makes working with RMarkdown possible. polymode enables simultaneous Major modes, which is needed for RMarkdown (because RMarkdown files mix Markdown and R).\n\n\n\n\nYour browser does not support the video tag. \n\n\nThat’s the end of this long post. Spacemacs is really a joy to use, but the learning curve is quite steep. However, it is definitely worth it. There are so many packages available for Emacs (and hence Spacemacs) that allow you to browse the web, play games, listen to music, send and read emails… that a recurrent joke is that Emacs is a very nice operating system, but it lacks a decent editor. If that’s the case, Spacemacs is the perfect operating system, because it includes the greatest editor, vi.\n\n\nIf you’re interested and and want to learn more about vi, I advise you to read the following book Vim Recipes (pdf warning, free) or Practical Vim, Edit Text at the Speed of thought (not free, but worth every cent), and Use Vim Like a Pro, which I have not read, but it looks quite good, and is free too if you want. Now this only covers the vi part, not the Emacs aspects of Spacemacs, but you don’t really need to know about Emacs to use Spacemacs. I had 0 experience with Emacs, and still have 0 experience with it. I only learned how to configure Spacemacs, which does not require any previous experience. To find the packages you need, as usual, use any search engine of your liking.\n\n\nThe last point I want to address is the built-in Vim mode of Rstudio. While it works, it does not work 100% as regular Vim, and worst of all, does not support, as far as I know, any other keyboard layout than QWERTY, which is a nogo for me.\n\n\nIn any case, if you’re looking to learn something new that you can use for many programs, including Rstudio, learn Vim, and then give Spacemacs a try. Chaining keystrokes to edit text gets addictive very quickly.\n\n\nFor reference, here is my dotspacemacs/user-config, which is where I defined the shortcut for the %&gt;% operator.\n\n(defun dotspacemacs/user-config ()\n  \"Configuration for user code:\nThis function is called at the very end of Spacemacs startup, after layer\nconfiguration.\nPut your configuration code here, except for variables that should be set\nbefore packages are loaded.\"\n;;; R modes\n  (add-to-list 'auto-mode-alist '(\"\\\\.md\" . poly-markdown-mode))\n  (add-to-list 'auto-mode-alist '(\"\\\\.Snw\" . poly-noweb+r-mode))\n  (add-to-list 'auto-mode-alist '(\"\\\\.Rnw\" . poly-noweb+r-mode))\n  (add-to-list 'auto-mode-alist '(\"\\\\.Rmd\" . poly-markdown+r-mode))\n\n  ;; (require 'poly-R)\n  ;; (require 'poly-markdown)\n  ;; (add-to-list 'auto-mode-alist '(\"\\\\.Rmd\" . poly-markdown+r-mode))\n\n  (global-company-mode t)\n  (global-hl-line-mode 1) ; Enable/Disable current line highlight\n  (setq-default fill-column 99)\n  (setq-default auto-fill-mode t)\n  ;; ESS shortcuts\n  (spacemacs/set-leader-keys \"mdt\" 'ess-r-devtools-test-package)\n  (spacemacs/set-leader-keys \"mrl\" 'ess-eval-line)\n  (spacemacs/set-leader-keys \"mrr\" 'ess-eval-region)\n  (spacemacs/set-leader-keys \"mdb\" 'ess-r-devtools-build-package)\n  (spacemacs/set-leader-keys \"mdd\" 'ess-r-devtools-document-package)\n  (spacemacs/set-leader-keys \"mdl\" 'ess-r-devtools-load-package)\n  (spacemacs/set-leader-keys \"mdc\" 'ess-r-devtools-check-package)\n  (spacemacs/set-leader-keys \"mdp\" 'ess-r-package-mode)\n  (add-hook 'ess-mode-hook\n            (lambda ()\n              (ess-toggle-underscore nil)))\n  (define-key evil-normal-state-map (kbd \"SPC mm\")\n            (lambda ()\n              (interactive)\n              (insert \" %&gt;% \")\n              (evil-insert-state)\n              ))\n  ;; Move lines around\n  (spacemacs/set-leader-keys \"MS\" 'move-text-line-up)\n  (spacemacs/set-leader-keys \"MT\" 'move-text-line-down)\n  (setq-default whitespace-mode t)\n  (setq-default whitespace-style (quote (spaces tabs newline space-mark tab-mark newline-mark)))\n  (setq-default whitespace-display-mappings\n        ;; all numbers are Unicode codepoint in decimal. try (insert-char 182 ) to see it\n        '(\n          (space-mark 32 [183] [46]) ; 32 SPACE, 183 MIDDLE DOT 「·」, 46 FULL STOP 「.」\n          (newline-mark 10 [9226 10]) ; 10 LINE FEED\n          (tab-mark 9 [9655 9] [92 9]) ; 9 TAB, 9655 WHITE RIGHT-POINTING TRIANGLE 「▷」\n          ))\n  (setq-default TeX-view-program-selection\n         '((output-pdf \"PDF Viewer\")))\n  (setq-default TeX-view-program-list\n        '((\"PDF Viewer\" \"okular %o\")))\n  (setq-default indent-tabs-mode nil)\n  (setq-default tab-width 2)\n   ;; (setq org-default-notes-file (concat org-directory \"/agenda/notes.org\"))\n   (add-hook 'prog-mode-hook 'spacemacs/toggle-fill-column-indicator-on)\n   (add-hook 'text-mode-hook 'spacemacs/toggle-fill-column-indicator-on)\n   (add-hook 'markdown-mode-hook 'spacemacs/toggle-fill-column-indicator-on)\n  )"
  },
  {
    "objectID": "posts/2021-07-30-worth_weight.html#intro",
    "href": "posts/2021-07-30-worth_weight.html#intro",
    "title": "Is it worth the weight?",
    "section": "\nIntro\n",
    "text": "Intro\n\n\nOh man, I did it again. Grab a coffee, this is going to be a long one.\n\n\nWeights got me confused. The justification for using weights seems simple enough; if you’re working with a sample in which one (or more) strata are over(under)-represented, you should compute weighted univariate statistics. I’ve discussed this already here.\n\n\nBut what about regression and prediction? There does not seem to be a consensus in the literature. So I wanted to experiment with some data and see if it would help.\n\n\nSpoiler alert: I’m more confused now than before, so maybe stop reading here. But maybe, by reading this blog post, dear reader, you might spot where I am confused and help me? Any help, comments, etc. more than welcome.\n\n\nAnyway, let’s start by loading the required packages:\n\nlibrary(\"dplyr\")\nlibrary(\"rsample\")\nlibrary(\"yardstick\")\nlibrary(\"readr\")\nlibrary(\"janitor\")\nlibrary(\"lubridate\")\nlibrary(\"broom\")\nlibrary(\"purrr\")\n\nand also the required dataset. This is a dataset that I have already featured in one of my previous blog posts here, a blog post about synthetic datasets. I’ll reuse the description from this other blog post here:\n\n\nThe Survey on the Population in Relation to Activity operation is a continuous source of information on the characteristics and dynamics of the labour force of the Basque Country. It records the relation to productive activity of the population resident in family households, as well as the changes produced in labour situations; it produces indicators of conjunctural variations in the evolution of the active population; it also estimates the degree of participation of the population in economically non-productive activities. It offers information on the province and capital level.\n\n\nTo make it easy for you to follow along, I have re-uploaded the data here. For the purposes of my analysis, I’ll be focusing on the “Hours Worked” variable. I’ll also assume that the dataset is the entire, complete population, and that I will have to deal with unbiased, randomly sampled individuals, but also with samples that are not randomly sampled.\n\n\nLet’s read in the data, rename the columns and do some basic data cleaning:\n\npopulation &lt;- read_csv2(\"https://raw.githubusercontent.com/rbind/b-rodrigues.github.com/master/public/assets/MICRO_PRA_2021_1.csv\")\n## ℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n## Rows: 12757 Columns: 33\n## ── Column specification ──────────────────────────────────────────────────────────────────────────────────────────────────────────────\n## Delimiter: \";\"\n## chr (10): TERH, EDAD, ENRE, FOCU, BUSQ, GBUSQ, FBUSQ, DISP, PRA2, RACT\n## dbl (23): NUMH, AENC, TENC, MUNI, SEXO, LNAC, NACI, LEST, SJUB, SILH, EMPTP,...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ncol_names_english &lt;- c(\n  \"Household number\",\n  \"Year of survey\",\n  \"Reference quarter\",\n  \"Province\",\n  \"Capital\",\n  \"Sex\",\n  \"Place of birth\",\n  \"Age\",\n  \"Nationality\",\n  \"Level of education\",\n  \"Formal education system\",\n  \"Professional training\",\n  \"Retirement situation\",\n  \"Household duties situation\",\n  \"Part-time employment\",\n  \"Reason for reduced worknig hours\",\n  \"Job search\",\n  \"Reasons for seeking employment\",\n  \"Working hours sought\",\n  \"Carry out employment seeking activities\",\n  \"Main employment seeking method\",\n  \"Months seeking employment\",\n  \"Availability\",\n  \"Relation to activity (ILO)\",\n  \"Relation to activity\",\n  \"Main occupation\",\n  \"Main activity\",\n  \"Main professional situation\",\n  \"Main institutional sector\",\n  \"Type of contract\",\n  \"Hours worked\",\n  \"Relationship\",\n  \"Elevator\")\n\n colnames(population) &lt;- col_names_english\n\npopulation &lt;- population %&gt;%\n  clean_names() %&gt;%\n  filter(!is.na(hours_worked)) %&gt;%\n  filter(!is.na(part_time_employment)) %&gt;%\n  mutate(part_time_employment = ifelse(part_time_employment == 1, \"Working full time\", \"Working part time\")) %&gt;%\n  mutate(type_of_contract = ifelse(is.na(type_of_contract), \"Unknown\", type_of_contract)) %&gt;%\n  mutate(sex = ifelse(sex == 1, \"Male\", \"Female\")) %&gt;%\n  mutate(age_group = case_when(between(age, 4, 7) ~ \"1\",\n                               between(age, 8, 12) ~ \"2\",\n                               age &gt; 12 ~ \"3\")) %&gt;%\n  mutate(type_of_contract = ifelse(type_of_contract %in% c(seq(2, 4), 6), \"Other\", type_of_contract)) %&gt;%  \n  select(capital,\n         sex,\n         age_group,\n         level_of_education,\n         part_time_employment,\n         type_of_contract,\n         hours_worked) %&gt;%\n  mutate(across(-hours_worked, as.factor)) %&gt;%\n  mutate(id = row_number())\n\nLet’s put some data on the side, for later:\n\nholdout &lt;- population %&gt;%\n  sample_n(300)\n\npopulation &lt;- population %&gt;%\n  filter(!(id %in% holdout$id))\n\nThis holdout set will be useful later on. I’m now going to compute some sampling weights. This weights will make it easy for me to select biased samples, where part-time workers are over-represented:\n\nset.seed(1234)\nbeta0 &lt;- -3.6\nbeta1 &lt;- 2.63\npopulation &lt;- population %&gt;%\n  mutate(pi_x = exp(beta0 + beta1 * I(part_time_employment == \"Working part time\")) / (1 + exp(beta0 + beta1 * I(part_time_employment == \"Working part time\"))))\n\nBy the way, I’ve found this code here.\n\n\nLet’s see what happens when I randomly sample from the population and compute some basic frequencies, and then what happens when I sample using the weights. First, the true frequencies of part-time and full-time workers, on the complete population:\n\npopulation %&gt;%\n  tabyl(part_time_employment)\n##  part_time_employment    n   percent\n##     Working full time 4107 0.8204155\n##     Working part time  899 0.1795845\n\nNow, on a random sample:\n\nsample_n(population, 1000) %&gt;%\n  tabyl(part_time_employment)\n##  part_time_employment   n percent\n##     Working full time 823   0.823\n##     Working part time 177   0.177\n\nPretty much the same value, now what happens when I don’t have a random sample:\n\nsample_n(population, 1000, weight = pi_x) %&gt;%\n  tabyl(part_time_employment)\n##  part_time_employment   n percent\n##     Working full time 409   0.409\n##     Working part time 591   0.591\n\nThis might seem obvious, since I have computed the weights such as to over-represent part-time workers. But this problem also affects other variables:\n\nsample_n(population, 1000) %&gt;%\n  tabyl(sex)\n##     sex   n percent\n##  Female 471   0.471\n##    Male 529   0.529\nsample_n(population, 1000, weight = pi_x) %&gt;%\n  tabyl(sex)\n##     sex   n percent\n##  Female 633   0.633\n##    Male 367   0.367\n\nBecause more women work part-time than men, women are now over-represented. The age structure is also different:\n\nsample_n(population, 1000) %&gt;%\n  tabyl(age_group)\n##  age_group   n percent\n##          1 181   0.181\n##          2 726   0.726\n##          3  93   0.093\nsample_n(population, 1000, weight = pi_x) %&gt;%\n  tabyl(age_group)\n##  age_group   n percent\n##          1 215   0.215\n##          2 662   0.662\n##          3 123   0.123\n\nAnd what about what interests us, the hours worked?\n\nsample_n(population, 1000) %&gt;%\n  summarise(mean(hours_worked))\n## # A tibble: 1 × 1\n##   `mean(hours_worked)`\n##                  &lt;dbl&gt;\n## 1                 29.9\nsample_n(population, 1000, weight = pi_x) %&gt;%\n  summarise(mean(hours_worked))\n## # A tibble: 1 × 1\n##   `mean(hours_worked)`\n##                  &lt;dbl&gt;\n## 1                 23.1\n\nOk, so this is bad, and the way to deal with it would be to computed post-stratification weights.\n\n\nBut let’s go a bit further and see what happens if I rerun this a 1000 times. Maybe I just got very unlucky with my non-random sample? With another sample, maybe things wouldn’t be so bad?\n\ntrue_mean &lt;- mean(population$hours_worked)\n\nrandom_samples &lt;- rerun(1000, sample_n(population, 1000))\n\nhours_worked_random_samples &lt;- map_df(.x = random_samples,\n                                      ~summarise(.x, mean_hours_worked = mean(hours_worked)))\n\nhours_worked_random_samples %&gt;%\n  summarise(mean(mean_hours_worked), sd(mean_hours_worked))\n## # A tibble: 1 × 2\n##   `mean(mean_hours_worked)` `sd(mean_hours_worked)`\n##                       &lt;dbl&gt;                   &lt;dbl&gt;\n## 1                      29.8                   0.393\nhours_worked_random_samples %&gt;%\n  ggplot() +\n  geom_density(aes(x = mean_hours_worked)) +\n  geom_vline(xintercept = true_mean)\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): X11 used font\n## size 25 when 29 was requested\n\n\n\n\nWe see that the distribution is centered around the true mean. What about a 1000 biased samples?\n\nbiased_samples &lt;- rerun(1000, sample_n(population, 1000, weight = pi_x))\n\nhours_worked_biased_samples &lt;- map_df(.x = biased_samples,\n                                      ~summarise(.x, mean_hours_worked = mean(hours_worked)))\n\nhours_worked_biased_samples %&gt;%\n  summarise(mean(mean_hours_worked), sd(mean_hours_worked))\n## # A tibble: 1 × 2\n##   `mean(mean_hours_worked)` `sd(mean_hours_worked)`\n##                       &lt;dbl&gt;                   &lt;dbl&gt;\n## 1                      23.4                   0.355\nhours_worked_biased_samples %&gt;%\n  ggplot() +\n  geom_density(aes(x = mean_hours_worked)) +\n  geom_vline(xintercept = true_mean)\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : X11\n## used font size 25 when 29 was requested\n\n\n\n\nClearly, the average hours worked are consistently under-estimated. So it’s not a matter of being unlucky with one particular sample.\n\n\nBut what about other tasks, such as prediction and regression? What is the impact there? This is where I started getting confused."
  },
  {
    "objectID": "posts/2021-07-30-worth_weight.html#regression-and-prediction-with-weights",
    "href": "posts/2021-07-30-worth_weight.html#regression-and-prediction-with-weights",
    "title": "Is it worth the weight?",
    "section": "\nRegression and prediction (with weights?)\n",
    "text": "Regression and prediction (with weights?)\n\n\nLet me first write a function that will do a bunch of things:\n\n\n\nsplit the data into training and testing sets\n\n\nrun a linear regression\n\n\npredict on the testing set\n\n\nreturn the rmse, the coefficients and the model\n\n\nrun_regression &lt;- function(dataset){\n\n  split_unbiased_data &lt;- initial_split(dataset, prop = 0.9)\n\n  training_unbiased_data &lt;- training(split_unbiased_data)\n\n  testing_unbiased_data &lt;- testing(split_unbiased_data)\n\n  linear_model &lt;- lm(hours_worked ~ capital +\n                       sex +\n                       age_group +\n                       level_of_education +\n                       part_time_employment +\n                       type_of_contract,\n                     data = training_unbiased_data)\n\n  lm_predictions &lt;- predict(linear_model,\n                            newdata = testing_unbiased_data)\n\n  testing_data_lm_predictions &lt;- testing_unbiased_data %&gt;%\n    mutate(lm_pred = lm_predictions)\n\n  lm_rmse &lt;- testing_data_lm_predictions %&gt;%\n    rmse(hours_worked, lm_pred)\n\n  lm_result &lt;- broom::tidy(linear_model)\n\n  tribble(~rmse, ~tidy_coeffs, ~model,\n          lm_rmse$.estimate, lm_result, linear_model)\n\n}\n\nLet’s now run this on the 1000 random samples and on the 1000 non-random samples:\n\nmany_lms &lt;- map_df(.x = random_samples, ~run_regression(.x))\n\nmany_biased_lms &lt;- map_df(.x = biased_samples, ~run_regression(.x))\n\nLet’s take a look at the RMSE of both models:\n\nmany_lms %&gt;%\n  summarise(mean(rmse), sd(rmse))\n## # A tibble: 1 × 2\n##   `mean(rmse)` `sd(rmse)`\n##          &lt;dbl&gt;      &lt;dbl&gt;\n## 1         13.3       1.18\nmany_biased_lms %&gt;%\n  summarise(mean(rmse), sd(rmse))\n## # A tibble: 1 × 2\n##   `mean(rmse)` `sd(rmse)`\n##          &lt;dbl&gt;      &lt;dbl&gt;\n## 1         12.1       1.08\n\nSo… both models perform the same? Hum. What about the coefficients? Well I don’t expect much difference there now, but let’s see:\n\nrandom_sample_coefs &lt;- many_lms %&gt;%\n  pull(tidy_coeffs) %&gt;%\n  bind_rows() %&gt;%\n  mutate(tidy_coeffs = \"random_sample\")\n\nbiased_sample_coefs &lt;- many_biased_lms %&gt;%\n  pull(tidy_coeffs) %&gt;%\n  bind_rows() %&gt;%\n  mutate(tidy_coeffs = \"biased_sample\")\n\ntrue_lm &lt;- lm(hours_worked ~ capital +\n                       sex +\n                       age_group +\n                       level_of_education +\n                       part_time_employment +\n                       type_of_contract,\n                     data = population)\n\ntrue_lm_coefs &lt;- broom::tidy(true_lm) %&gt;%\n  mutate(tidy_coeffs = \"true\")\n\nsimulations &lt;- bind_rows(random_sample_coefs,\n          biased_sample_coefs) \n\nLet’s plot the 1000 coefficients for each variable in a nice violin plot:\n\nggplot() +\n  geom_violin(data = simulations, aes(y = estimate, x = term, fill = tidy_coeffs),\n              draw_quantiles = c(0.05, 0.5, 0.95)) +\n  geom_point(data = true_lm_coefs, aes(y = estimate, x = term), size = 2) +\n  scale_x_discrete(guide = guide_axis(n.dodge = 4)) +\n  theme(legend.position = \"bottom\")\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : X11\n## used font size 25 when 29 was requested\n\n\n\n\nThe dots are the true coefficients (obtained from a linear regression on the whole data). The coefficients from the random sample are “more often” closer to the true coefficients, but it doesn’t seem to be a lot (the bars in the violins are the 5th, 50th and 95th percentile).\n\n\nLet’s now see what happens on the holdout set (using the best performing models):\n\nbest_unbiased_model &lt;- many_lms %&gt;%\n  filter(rmse == min(rmse)) %&gt;%\n  pull(model) %&gt;%\n  .[[1]]\n\nholdout &lt;- holdout %&gt;%\n  mutate(unbiased = predict(best_unbiased_model, newdata = holdout))\n\nbest_biased_model &lt;- many_biased_lms %&gt;%\n  filter(rmse == min(rmse)) %&gt;%\n  pull(model) %&gt;%\n  .[[1]]\n\nholdout &lt;- holdout %&gt;%\n  mutate(biased = predict(best_biased_model, newdata = holdout))\n\nholdout %&gt;%\n  rmse(hours_worked, unbiased)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rmse    standard        13.3\nholdout %&gt;%\n  rmse(hours_worked, biased)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rmse    standard        13.3\n\nAgain, pretty much no difference… What about hours worked?\n\nholdout %&gt;%\n  summarise(mean_true = mean(hours_worked),\n            mean_unbiased = mean(unbiased),\n            mean_biased = mean(biased))\n## # A tibble: 1 × 3\n##   mean_true mean_unbiased mean_biased\n##       &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n## 1      30.4          29.9        29.9\n\nSame…??? What about coefficients?\n\nbind_cols(broom::tidy(best_unbiased_model),\n          broom::tidy(best_biased_model)) %&gt;%\n  select(term...1, estimate...2, std.error...3, estimate...7, std.error...8)\n## New names:\n## * term -&gt; term...1\n## * estimate -&gt; estimate...2\n## * std.error -&gt; std.error...3\n## * statistic -&gt; statistic...4\n## * p.value -&gt; p.value...5\n## * ...\n## # A tibble: 13 × 5\n##    term...1                estimate...2 std.error...3 estimate...7 std.error...8\n##    &lt;chr&gt;                          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n##  1 (Intercept)                   30.6           2.11        36.4           1.95 \n##  2 capital2                       0.317         1.91        -3.35          1.72 \n##  3 capital3                       0.501         1.90        -2.66          1.78 \n##  4 capital9                       0.258         1.40        -3.45          1.32 \n##  5 sexMale                        3.54          0.946       -0.649         0.915\n##  6 age_group2                     0.295         1.29        -0.467         1.09 \n##  7 age_group3                    -3.42          1.82        -5.55          1.45 \n##  8 level_of_education2           -0.506         1.21         0.439         1.06 \n##  9 level_of_education3            0.636         1.20         0.545         1.06 \n## 10 part_time_employmentWo…      -13.3           1.23       -14.3           0.960\n## 11 type_of_contract5             -0.646         1.20        -1.86          0.982\n## 12 type_of_contractOther         -5.74          2.60        -4.98          1.63 \n## 13 type_of_contractUnknown        0.378         1.18         3.17          1.25\n\nAgain, some differences here (especially for significant coefficients, which makes sense). So I guess you should use weights if you’re interested in the coefficients (and especially their standard deviation). I definitely need to explore this more, and read some more."
  },
  {
    "objectID": "posts/2019-06-12-intermittent.html",
    "href": "posts/2019-06-12-intermittent.html",
    "title": "Intermittent demand, Croston and Die Hard",
    "section": "",
    "text": "I have recently been confronted to a kind of data set and problem that I was not even aware existed: intermittent demand data. Intermittent demand arises when the demand for a certain good arrives sporadically. Let’s take a look at an example, by analyzing the number of downloads for the {RDieHarder} package:\n\nlibrary(tidyverse)\nlibrary(tsintermittent)\nlibrary(nnfor)\nlibrary(cranlogs)\nlibrary(brotools)\nrdieharder &lt;- cran_downloads(\"RDieHarder\", from = \"2017-01-01\")\n\nggplot(rdieharder) +\n  geom_line(aes(y = count, x = date), colour = \"#82518c\") +\n  theme_blog()\n\n\n\n\nLet’s take a look at just one month of data, because the above plot is not very clear, because of the outlier just before 2019… I wonder now, was that on Christmas day?\n\nrdieharder %&gt;%\n  filter(count == max(count))\n##         date count    package\n## 1 2018-12-21   373 RDieHarder\n\nNot exactly on Christmas day, but almost! Anyways, let’s look at one month of data:\n\njanuary_2018 &lt;- rdieharder %&gt;%\n  filter(between(date, as.Date(\"2018-01-01\"), as.Date(\"2018-02-01\")))\n\nggplot(january_2018) +\n  geom_line(aes(y = count, x = date), colour = \"#82518c\") +\n  theme_blog()\n\n\n\n\nNow, it is clear that this will be tricky to forecast. There is no discernible pattern, no trend, no seasonality… nothing that would make it “easy” for a model to learn how to forecast such data.\n\n\nThis is typical intermittent demand data. Specific methods have been developed to forecast such data, the most well-known being Croston, as detailed in this paper. A function to estimate such models is available in the {tsintermittent} package, written by Nikolaos Kourentzes who also wrote another package, {nnfor}, which uses Neural Networks to forecast time series data. I am going to use both to try to forecast the intermittent demand for the {RDieHarder} package for the year 2019.\n\n\nLet’s first load these packages:\n\nlibrary(tsintermittent)\nlibrary(nnfor)\n\nAnd as usual, split the data into training and testing sets:\n\ntrain_data &lt;- rdieharder %&gt;%\n  filter(date &lt; as.Date(\"2019-01-01\")) %&gt;%\n  pull(count) %&gt;%\n  ts()\n\ntest_data &lt;- rdieharder %&gt;%\n  filter(date &gt;= as.Date(\"2019-01-01\"))\n\nLet’s consider three models; a naive one, which simply uses the mean of the training set as the forecast for all future periods, Croston’s method, and finally a Neural Network from the {nnfor} package:\n\nnaive_model &lt;- mean(train_data)\n\ncroston_model &lt;- crost(train_data, h = 163)\n\nnn_model &lt;- mlp(train_data, reps = 1, hd.auto.type = \"cv\")\n## Warning in preprocess(y, m, lags, keep, difforder, sel.lag,\n## allow.det.season, : No inputs left in the network after pre-selection,\n## forcing AR(1).\nnn_model_forecast &lt;- forecast(nn_model, h = 163)\n\nThe crost() function estimates Croston’s model, and the h argument produces the forecast for the next 163 days. mlp() trains a multilayer perceptron, and the hd.auto.type = \"cv\" argument means that 5-fold cross-validation will be used to find the best number of hidden nodes. I then obtain the forecast using the forecast() function. As you can read from the Warning message above, the Neural Network was replaced by an auto-regressive model, AR(1), because no inputs were left after pre-selection… I am not exactly sure what that means, but if I remove the big outlier from before, this warning message disappears, and a Neural Network is successfully trained.\n\n\nIn order to rank the models, I follow this paper from Rob J. Hyndman, who wrote a very useful book titled Forecasting: Principles and Practice, and use the Mean Absolute Scaled Error, or MASE. You can also read this shorter pdf which also details how to use MASE to measure the accuracy for intermittent demand. Here is the function:\n\nmase &lt;- function(train_ts, test_ts, outsample_forecast){\n\n  naive_insample_forecast &lt;- stats::lag(train_ts)\n\n  insample_mae &lt;- mean(abs(train_ts - naive_insample_forecast), na.rm = TRUE)\n  error_outsample &lt;- test_ts - outsample_forecast\n\n  ase &lt;- error_outsample / insample_mae\n  mean(abs(ase), na.rm = TRUE)\n}\n\nIt is now easy to compute the models’ accuracies:\n\nmase(train_data, test_data$count, naive_model)\n## [1] 1.764385\nmase(train_data, test_data$count, croston_model$component$c.out[1])\n## [1] 1.397611\nmase(train_data, test_data$count, nn_model_forecast$mean)\n## [1] 1.767357\n\nCroston’s method is the one that performs best from the three. Maybe surprisingly, the naive method performs just as well as the Neural Network! (or rather, the AR(1) model) Let’s also plot the predictions with the true values from the test set:\n\ntest_data &lt;- test_data %&gt;%\n  mutate(naive_model_forecast = naive_model,\n         croston_model_forecast = croston_model$component$c.out[1],\n         nn_model_forecast = nn_model_forecast$mean) %&gt;%\n  select(-package) %&gt;%\n  rename(actual_value = count)\n\n\ntest_data_longer &lt;- test_data %&gt;%\n  gather(models, value,\n         actual_value, naive_model_forecast, croston_model_forecast, nn_model_forecast)\n## Warning: attributes are not identical across measure variables;\n## they will be dropped\nggplot(test_data_longer) +\n  geom_line(aes(y = value, x = date, colour = models)) +\n  theme_blog()\n\n\n\n\nJust to make sure I didn’t make a mistake when writing the mase() function, let’s use the accuracy() function from the {forecast} package and compare the result for the Neural Network:\n\nlibrary(forecast)\naccuracy(nn_model_forecast, x = test_data$actual_value)\n##                       ME     RMSE      MAE  MPE MAPE      MASE       ACF1\n## Training set 0.001929409 14.81196 4.109577  NaN  Inf 0.8437033 0.05425074\n## Test set     8.211758227 12.40199 8.635563 -Inf  Inf 1.7673570         NA\n\nThe result is the same, so it does seem like the naive method is not that bad, actually! Now, in general, intermittent demand series have a lot of 0 values, which is not really the case here. I still think that the methodology fits to this particular data set.\n\n\nHow else would you have forecast this data? Let me know via twitter!"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html",
    "href": "posts/2017-03-24-lesser_known_purrr.html",
    "title": "Lesser known purrr tricks",
    "section": "",
    "text": "purrr is a package that extends R’s functional programming capabilities. It brings a lot of new stuff to the table and in this post I show you some of the most useful (at least to me) functions included in purrr."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#getting-rid-of-loops-with-map",
    "href": "posts/2017-03-24-lesser_known_purrr.html#getting-rid-of-loops-with-map",
    "title": "Lesser known purrr tricks",
    "section": "\nGetting rid of loops with map()\n",
    "text": "Getting rid of loops with map()\n\nlibrary(purrr)\n\nnumbers &lt;- list(11, 12, 13, 14)\n\nmap_dbl(numbers, sqrt)\n## [1] 3.316625 3.464102 3.605551 3.741657\n\nYou might wonder why this might be preferred to a for loop? It’s a lot less verbose, and you do not need to initialise any kind of structure to hold the result. If you google “create empty list in R” you will see that this is very common. However, with the map() family of functions, there is no need for an initial structure. map_dbl() returns an atomic list of real numbers, but if you use map() you will get a list back. Try them all out!"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#map-conditionally",
    "href": "posts/2017-03-24-lesser_known_purrr.html#map-conditionally",
    "title": "Lesser known purrr tricks",
    "section": "\nMap conditionally\n",
    "text": "Map conditionally\n\n\n\nmap_if()\n\n# Create a helper function that returns TRUE if a number is even\nis_even &lt;- function(x){\n  !as.logical(x %% 2)\n}\n\nmap_if(numbers, is_even, sqrt)\n## [[1]]\n## [1] 11\n## \n## [[2]]\n## [1] 3.464102\n## \n## [[3]]\n## [1] 13\n## \n## [[4]]\n## [1] 3.741657\n\n\n\nmap_at()\n\nmap_at(numbers, c(1,3), sqrt)\n## [[1]]\n## [1] 3.316625\n## \n## [[2]]\n## [1] 12\n## \n## [[3]]\n## [1] 3.605551\n## \n## [[4]]\n## [1] 14\n\nmap_if() and map_at() have a further argument than map(); in the case of map_if(), a predicate function ( a function that returns TRUE or FALSE) and a vector of positions for map_at(). This allows you to map your function only when certain conditions are met, which is also something that a lot of people google for."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#map-a-function-with-multiple-arguments",
    "href": "posts/2017-03-24-lesser_known_purrr.html#map-a-function-with-multiple-arguments",
    "title": "Lesser known purrr tricks",
    "section": "\nMap a function with multiple arguments\n",
    "text": "Map a function with multiple arguments\n\nnumbers2 &lt;- list(1, 2, 3, 4)\n\nmap2(numbers, numbers2, `+`)\n## [[1]]\n## [1] 12\n## \n## [[2]]\n## [1] 14\n## \n## [[3]]\n## [1] 16\n## \n## [[4]]\n## [1] 18\n\nYou can map two lists to a function which takes two arguments using map_2(). You can even map an arbitrary number of lists to any function using pmap().\n\n\nBy the way, try this in: +(1,3) and see what happens."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#dont-stop-execution-of-your-function-if-something-goes-wrong",
    "href": "posts/2017-03-24-lesser_known_purrr.html#dont-stop-execution-of-your-function-if-something-goes-wrong",
    "title": "Lesser known purrr tricks",
    "section": "\nDon’t stop execution of your function if something goes wrong\n",
    "text": "Don’t stop execution of your function if something goes wrong\n\npossible_sqrt &lt;- possibly(sqrt, otherwise = NA_real_)\n\nnumbers_with_error &lt;- list(1, 2, 3, \"spam\", 4)\n\nmap(numbers_with_error, possible_sqrt)\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 1.414214\n## \n## [[3]]\n## [1] 1.732051\n## \n## [[4]]\n## [1] NA\n## \n## [[5]]\n## [1] 2\n\nAnother very common issue is to keep running your loop even when something goes wrong. In most cases the loop simply stops at the error, but you would like it to continue and see where it failed. Try to google “skip error in a loop” or some variation of it and you’ll see that a lot of people really just want that. This is possible by combining map() and possibly(). Most solutions involve the use of tryCatch() which I personally do not find very easy to use."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#dont-stop-execution-of-your-function-if-something-goes-wrong-and-capture-the-error",
    "href": "posts/2017-03-24-lesser_known_purrr.html#dont-stop-execution-of-your-function-if-something-goes-wrong-and-capture-the-error",
    "title": "Lesser known purrr tricks",
    "section": "\nDon’t stop execution of your function if something goes wrong and capture the error\n",
    "text": "Don’t stop execution of your function if something goes wrong and capture the error\n\nsafe_sqrt &lt;- safely(sqrt, otherwise = NA_real_)\n\nmap(numbers_with_error, safe_sqrt)\n## [[1]]\n## [[1]]$result\n## [1] 1\n## \n## [[1]]$error\n## NULL\n## \n## \n## [[2]]\n## [[2]]$result\n## [1] 1.414214\n## \n## [[2]]$error\n## NULL\n## \n## \n## [[3]]\n## [[3]]$result\n## [1] 1.732051\n## \n## [[3]]$error\n## NULL\n## \n## \n## [[4]]\n## [[4]]$result\n## [1] NA\n## \n## [[4]]$error\n## \n\nsafely() is very similar to possibly() but it returns a list of lists. An element is thus a list of the result and the accompagnying error message. If there is no error, the error component is NULL if there is an error, it returns the error message."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#transpose-a-list",
    "href": "posts/2017-03-24-lesser_known_purrr.html#transpose-a-list",
    "title": "Lesser known purrr tricks",
    "section": "\nTranspose a list\n",
    "text": "Transpose a list\n\nsafe_result_list &lt;- map(numbers_with_error, safe_sqrt)\n\ntranspose(safe_result_list)\n## $result\n## $result[[1]]\n## [1] 1\n## \n## $result[[2]]\n## [1] 1.414214\n## \n## $result[[3]]\n## [1] 1.732051\n## \n## $result[[4]]\n## [1] NA\n## \n## $result[[5]]\n## [1] 2\n## \n## \n## $error\n## $error[[1]]\n## NULL\n## \n## $error[[2]]\n## NULL\n## \n## $error[[3]]\n## NULL\n## \n## $error[[4]]\n## \n\nHere we transposed the above list. This means that we still have a list of lists, but where the first list holds all the results (which you can then access with safe_result_list$result) and the second list holds all the errors (which you can access with safe_result_list$error). This can be quite useful!"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#apply-a-function-to-a-lower-depth-of-a-list",
    "href": "posts/2017-03-24-lesser_known_purrr.html#apply-a-function-to-a-lower-depth-of-a-list",
    "title": "Lesser known purrr tricks",
    "section": "\nApply a function to a lower depth of a list\n",
    "text": "Apply a function to a lower depth of a list\n\ntransposed_list &lt;- transpose(safe_result_list)\n\ntransposed_list %&gt;%\n    at_depth(2, is_null)\n## Warning: at_depth() is deprecated, please use `modify_depth()` instead\n## $result\n## $result[[1]]\n## [1] FALSE\n## \n## $result[[2]]\n## [1] FALSE\n## \n## $result[[3]]\n## [1] FALSE\n## \n## $result[[4]]\n## [1] FALSE\n## \n## $result[[5]]\n## [1] FALSE\n## \n## \n## $error\n## $error[[1]]\n## [1] TRUE\n## \n## $error[[2]]\n## [1] TRUE\n## \n## $error[[3]]\n## [1] TRUE\n## \n## $error[[4]]\n## [1] FALSE\n## \n## $error[[5]]\n## [1] TRUE\n\nSometimes working with lists of lists can be tricky, especially when we want to apply a function to the sub-lists. This is easily done with at_depth()!"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#set-names-of-list-elements",
    "href": "posts/2017-03-24-lesser_known_purrr.html#set-names-of-list-elements",
    "title": "Lesser known purrr tricks",
    "section": "\nSet names of list elements\n",
    "text": "Set names of list elements\n\nname_element &lt;- c(\"sqrt()\", \"ok?\")\n\nset_names(transposed_list, name_element)\n## $`sqrt()`\n## $`sqrt()`[[1]]\n## [1] 1\n## \n## $`sqrt()`[[2]]\n## [1] 1.414214\n## \n## $`sqrt()`[[3]]\n## [1] 1.732051\n## \n## $`sqrt()`[[4]]\n## [1] NA\n## \n## $`sqrt()`[[5]]\n## [1] 2\n## \n## \n## $`ok?`\n## $`ok?`[[1]]\n## NULL\n## \n## $`ok?`[[2]]\n## NULL\n## \n## $`ok?`[[3]]\n## NULL\n## \n## $`ok?`[[4]]\n##"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#reduce-a-list-to-a-single-value",
    "href": "posts/2017-03-24-lesser_known_purrr.html#reduce-a-list-to-a-single-value",
    "title": "Lesser known purrr tricks",
    "section": "\nReduce a list to a single value\n",
    "text": "Reduce a list to a single value\n\nreduce(numbers, `*`)\n## [1] 24024\n\nreduce() applies the function * iteratively to the list of numbers. There’s also accumulate():\n\naccumulate(numbers, `*`)\n## [1]    11   132  1716 24024\n\nwhich keeps the intermediary results.\n\n\nThis function is very general, and you can reduce anything:\n\n\nMatrices:\n\nmat1 &lt;- matrix(rnorm(10), nrow = 2)\nmat2 &lt;- matrix(rnorm(10), nrow = 2)\nmat3 &lt;- matrix(rnorm(10), nrow = 2)\nlist_mat &lt;- list(mat1, mat2, mat3)\n\nreduce(list_mat, `+`)\n##             [,1]       [,2]       [,3]     [,4]      [,5]\n## [1,] -2.48530177  1.0110049  0.4450388 1.280802 1.3413979\n## [2,]  0.07596679 -0.6872268 -0.6579242 1.615237 0.8231933\n\neven data frames:\n\ndf1 &lt;- as.data.frame(mat1)\ndf2 &lt;- as.data.frame(mat2)\ndf3 &lt;- as.data.frame(mat3)\n\nlist_df &lt;- list(df1, df2, df3)\n\nreduce(list_df, dplyr::full_join)\n## Joining, by = c(\"V1\", \"V2\", \"V3\", \"V4\", \"V5\")\n## Joining, by = c(\"V1\", \"V2\", \"V3\", \"V4\", \"V5\")\n##           V1         V2          V3          V4         V5\n## 1 -0.6264538 -0.8356286  0.32950777  0.48742905  0.5757814\n## 2  0.1836433  1.5952808 -0.82046838  0.73832471 -0.3053884\n## 3 -0.8969145  1.5878453 -0.08025176  0.70795473  1.9844739\n## 4  0.1848492 -1.1303757  0.13242028 -0.23969802 -0.1387870\n## 5 -0.9619334  0.2587882  0.19578283  0.08541773 -1.2188574\n## 6 -0.2925257 -1.1521319  0.03012394  1.11661021  1.2673687\n\nHope you enjoyed this list of useful functions! If you enjoy the content of my blog, you can follow me on twitter."
  },
  {
    "objectID": "posts/2018-12-27-fun_gganimate.html",
    "href": "posts/2018-12-27-fun_gganimate.html",
    "title": "Some fun with {gganimate}",
    "section": "",
    "text": "Your browser does not support the video tag.\nIn this short blog post I show you how you can use the {gganimate} package to create animations from {ggplot2} graphs with data from UNU-WIDER."
  },
  {
    "objectID": "posts/2018-12-27-fun_gganimate.html#wiid-data",
    "href": "posts/2018-12-27-fun_gganimate.html#wiid-data",
    "title": "Some fun with {gganimate}",
    "section": "\nWIID data\n",
    "text": "WIID data\n\n\nJust before Christmas, UNU-WIDER released a new edition of their World Income Inequality Database:\n\n\n\nNEW #DATAWe’ve just released a new version of the World Income Inequality Database.WIID4 includes #data from 7 new countries, now totalling 189, and reaches the year 2017. All data is freely available for download on our website: https://t.co/XFxuLvyKTC pic.twitter.com/rCf9eXN8D5\n\n— UNU-WIDER (@UNUWIDER) December 21, 2018\n\n\n\nThe data is available in Excel and STATA formats, and I thought it was a great opportunity to release it as an R package. You can install it with:\n\ndevtools::install_github(\"b-rodrigues/wiid4\")\n\nHere a short description of the data, taken from UNU-WIDER’s website:\n\n\n“The World Income Inequality Database (WIID) presents information on income inequality for developed, developing, and transition countries. It provides the most comprehensive set of income inequality statistics available and can be downloaded for free.\n\n\nWIID4, released in December 2018, covers 189 countries (including historical entities), with over 11,000 data points in total. With the current version, the latest observations now reach the year 2017.”\n\n\nIt was also a good opportunity to play around with the {gganimate} package. This package makes it possible to create animations and is an extension to {ggplot2}. Read more about it here."
  },
  {
    "objectID": "posts/2018-12-27-fun_gganimate.html#preparing-the-data",
    "href": "posts/2018-12-27-fun_gganimate.html#preparing-the-data",
    "title": "Some fun with {gganimate}",
    "section": "\nPreparing the data\n",
    "text": "Preparing the data\n\n\nTo create a smooth animation, I need to have a cylindrical panel data set; meaning that for each country in the data set, there are no missing years. I also chose to focus on certain variables only; net income, all the population of the country (instead of just focusing on the economically active for instance) as well as all the country itself (and not just the rural areas). On this link you can find a codebook (pdf warning), so you can understand the filters I defined below better.\n\n\nLet’s first load the packages, data and perform the necessary transformations:\n\nlibrary(wiid4)\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(gganimate)\nlibrary(brotools)\n\nsmall_wiid4 &lt;- wiid4 %&gt;%\n    mutate(eu = as.character(eu)) %&gt;%\n    mutate(eu = case_when(eu == \"1\" ~ \"EU member state\",\n                          eu == \"0\" ~ \"Non-EU member state\")) %&gt;%\n    filter(resource == 1, popcovr == 1, areacovr == 1, scale == 2) %&gt;%\n    group_by(country) %&gt;%\n    group_by(country, year) %&gt;%\n    filter(quality_score == max(quality_score)) %&gt;%\n    filter(source == min(source)) %&gt;%\n    filter(!is.na(bottom5)) %&gt;%\n    group_by(country) %&gt;%\n    mutate(flag = ifelse(all(seq(2004, 2016) %in% year), 1, 0)) %&gt;%\n    filter(flag == 1, year &gt; 2003) %&gt;%\n    mutate(year = lubridate::ymd(paste0(year, \"-01-01\")))\n\nFor some country and some years, there are several sources of data with varying quality. I only keep the highest quality sources with:\n\n    group_by(country, year) %&gt;%\n    filter(quality_score == max(quality_score)) %&gt;%\n\nIf there are different sources of equal quality, I give priority to the sources that are the most comparable across country (Luxembourg Income Study, LIS data) to less comparable sources with (at least that’s my understanding of the source variable):\n\n    filter(source == min(source)) %&gt;%\n\nI then remove missing data with:\n\n    filter(!is.na(bottom5)) %&gt;%\n\nbottom5 and top5 give the share of income that is controlled by the bottom 5% and top 5% respectively. These are the variables that I want to plot.\n\n\nFinally I keep the years 2004 to 2016, without any interruption with the following line:\n\n    mutate(flag = ifelse(all(seq(2004, 2016) %in% year), 1, 0)) %&gt;%\n    filter(flag == 1, year &gt; 2003) %&gt;%\n\nifelse(all(seq(2004, 2016) %in% year), 1, 0)) creates a flag that equals 1 only if the years 2004 to 2016 are present in the data without any interruption. Then I only keep the data from 2004 on and only where the flag variable equals 1.\n\n\nIn the end, I ended up only with European countries. It would have been interesting to have countries from other continents, but apparently only European countries provide data in an annual basis."
  },
  {
    "objectID": "posts/2018-12-27-fun_gganimate.html#creating-the-animation",
    "href": "posts/2018-12-27-fun_gganimate.html#creating-the-animation",
    "title": "Some fun with {gganimate}",
    "section": "\nCreating the animation\n",
    "text": "Creating the animation\n\n\nTo create the animation I first started by creating a static ggplot showing what I wanted; a scatter plot of the income by bottom and top 5%. The size of the bubbles should be proportional to the GDP of the country (another variable provided in the data). Once the plot looked how I wanted I added the lines that are specific to {gganimate}:\n\n    labs(title = 'Year: {frame_time}', x = 'Top 5', y = 'Bottom 5') +\n    transition_time(year) +\n    ease_aes('linear')\n\nI took this from {gganimate}’s README.\n\nanimation &lt;- ggplot(small_wiid4) +\n    geom_point(aes(y = bottom5, x = top5, colour = eu, size = log(gdp_ppp_pc_usd2011))) +\n    xlim(c(10, 20)) +\n    geom_label_repel(aes(y = bottom5, x = top5, label = country), hjust = 1, nudge_x = 20) +\n    theme(legend.position = \"bottom\") +\n    theme_blog() +\n    scale_color_blog() +\n    labs(title = 'Year: {frame_time}', x = 'Top 5', y = 'Bottom 5') +\n    transition_time(year) +\n    ease_aes('linear')\n\nI use geom_label_repel to place the countries’ labels on the right of the plot. If I don’t do this, the labels of the countries would be floating around and the animation would be unreadable.\n\n\nI then spent some time trying to render a nice webm instead of a gif. It took some trial and error and I am still not entirely satisfied with the result, but here is the code to render the animation:\n\nanimate(animation, renderer = ffmpeg_renderer(options = list(s = \"864x480\", \n                                                             vcodec = \"libvpx-vp9\",\n                                                             crf = \"15\",\n                                                             b = \"1600k\", \n                                                             vf = \"setpts=5*PTS\")))\n\nThe option vf = “setpts=5*PTS” is important because it slows the video down, so we can actually see something. crf = “15” is the quality of the video (lower is better), b = “1600k” is the bitrate, and vcodec = “libvpx-vp9” is the codec I use. The video you saw at the top of this post is the result. You can also find the video here, and here’s a gif if all else fails:\n\n\n\n \n\n\n\nI would have preferred if the video was smoother, which should be possible by creating more frames. I did not find such an option in {gganimate}, and perhaps there is none, at least for now.\n\n\nIn any case {gganimate} is pretty nice to play with, and I’ll definitely use it more!"
  },
  {
    "objectID": "posts/2018-11-15-tidy_gridsearch.html#introduction",
    "href": "posts/2018-11-15-tidy_gridsearch.html#introduction",
    "title": "Searching for the optimal hyper-parameters of an ARIMA model in parallel: the tidy gridsearch approach",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nIn this blog post, I’ll use the data that I cleaned in a previous blog post, which you can download here. If you want to follow along, download the monthly data.\n\n\nIn the previous blog post, I used the auto.arima() function to very quickly get a “good-enough” model to predict future monthly total passengers flying from LuxAirport. “Good-enough” models can be all you need in a lot of situations, but perhaps you’d like to have a better model. I will show here how you can get a better model by searching through a grid of hyper-parameters.\n\n\nThis blog post was partially inspired by: https://drsimonj.svbtle.com/grid-search-in-the-tidyverse"
  },
  {
    "objectID": "posts/2018-11-15-tidy_gridsearch.html#the-problem",
    "href": "posts/2018-11-15-tidy_gridsearch.html#the-problem",
    "title": "Searching for the optimal hyper-parameters of an ARIMA model in parallel: the tidy gridsearch approach",
    "section": "\nThe problem\n",
    "text": "The problem\n\n\nSARIMA models have a lot of hyper-parameters, 7 in total! Three trend hyper-parameters, p, d, q, same as for an ARIMA model, and four seasonal hyper-parameters, P, D, Q, S. The traditional way t o search for these hyper-parameters is the so-called Box-Jenkins method. You can read about it here. This method was described in a 1970 book, Time series analysis: Forecasting and control by Box and Jenkins. The method requires that you first prepare the data by logging it and differencing it, in order to make the time series stationary. You then need to analyze ACF and PACF plots, in order to determine the right amount of lags… It take some time, but this method made sense in a time were computing power was very expensive. Today, we can simply let our computer search through thousands of models, check memes on the internet, and come back to the best fit. This blog post is for you, the busy data scientist meme connoisseurs who cannot waste time with theory and other such useless time drains, when there are literally thousands of new memes being created and shared every day. Every second counts. To determine what model is best, I will do pseudo out-of-sample forecasting and compute the RMSE for each model. I will then choose the model that has the lowest RMSE."
  },
  {
    "objectID": "posts/2018-11-15-tidy_gridsearch.html#setup",
    "href": "posts/2018-11-15-tidy_gridsearch.html#setup",
    "title": "Searching for the optimal hyper-parameters of an ARIMA model in parallel: the tidy gridsearch approach",
    "section": "\nSetup\n",
    "text": "Setup\n\n\nLet’s first load some libraries:\n\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(lubridate)\nlibrary(furrr)\nlibrary(tsibble)\nlibrary(brotools)\n\nihs &lt;- function(x){\n    log(x + sqrt(x**2 + 1))\n}\n\nNow, let’s load the data:\n\navia_clean_monthly &lt;- read_csv(\"https://raw.githubusercontent.com/b-rodrigues/avia_par_lu/master/avia_clean_monthy.csv\")\n## Parsed with column specification:\n## cols(\n##   destination = col_character(),\n##   date = col_date(format = \"\"),\n##   passengers = col_double()\n## )\n\nLet’s split the data into a training set and into a testing set:\n\navia_clean_train &lt;- avia_clean_monthly %&gt;%\n    select(date, passengers) %&gt;%\n    filter(year(date) &lt; 2015) %&gt;%\n    group_by(date) %&gt;%\n    summarise(total_passengers = sum(passengers)) %&gt;%\n    pull(total_passengers) %&gt;%\n    ts(., frequency = 12, start = c(2005, 1))\n\navia_clean_test &lt;- avia_clean_monthly %&gt;%\n    select(date, passengers) %&gt;%\n    filter(year(date) &gt;= 2015) %&gt;%\n    group_by(date) %&gt;%\n    summarise(total_passengers = sum(passengers)) %&gt;%\n    pull(total_passengers) %&gt;%\n    ts(., frequency = 12, start = c(2015, 1))\n\nlogged_train_data &lt;- ihs(avia_clean_train)\n\nlogged_test_data &lt;- ihs(avia_clean_test)\n\nI also define a helper function:\n\nto_tibble &lt;- function(forecast_object){\n    point_estimate &lt;- forecast_object$mean %&gt;%\n        as_tsibble() %&gt;%\n        rename(point_estimate = value,\n               date = index)\n\n    upper &lt;- forecast_object$upper %&gt;%\n        as_tsibble() %&gt;%\n        spread(key, value) %&gt;%\n        rename(date = index,\n               upper80 = `80%`,\n               upper95 = `95%`)\n\n    lower &lt;- forecast_object$lower %&gt;%\n        as_tsibble() %&gt;%\n        spread(key, value) %&gt;%\n        rename(date = index,\n               lower80 = `80%`,\n               lower95 = `95%`)\n\n    reduce(list(point_estimate, upper, lower), full_join)\n}\n\nThis function takes a forecast object as argument, and returns a nice tibble. This will be useful later, and is based on the code I already used in my previous blog post.\n\n\nNow, let’s take a closer look at the arima() function:\n\nARIMA Modelling of Time Series\n\nDescription\n\nFit an ARIMA model to a univariate time series.\n\nUsage\n\narima(x, order = c(0L, 0L, 0L),\n      seasonal = list(order = c(0L, 0L, 0L), period = NA),\n      xreg = NULL, include.mean = TRUE,\n      transform.pars = TRUE,\n      fixed = NULL, init = NULL,\n      method = c(\"CSS-ML\", \"ML\", \"CSS\"), n.cond,\n      SSinit = c(\"Gardner1980\", \"Rossignol2011\"),\n      optim.method = \"BFGS\",\n      optim.control = list(), kappa = 1e6)\n\nThe user is supposed to enter the hyper-parameters as two lists, one called order for p, d, q and one called seasonal for P, D, Q, S. So what we need is to define these lists:\n\norder_list &lt;- list(\"p\" = seq(0, 3),\n                   \"d\" = seq(0, 2),\n                   \"q\" = seq(0, 3)) %&gt;%\n    cross() %&gt;%\n    map(lift(c))\n\nI first start with order_list. This list has 3 elements, “p”, “d” and “q”. Each element is a sequence from 0 to 3 (2 in the case of “d”). When I pass this list to purrr::cross() I get the product set of the starting list, so in this case a list of 434 = 48 elements. However, this list looks pretty bad:\n\nlist(\"p\" = seq(0, 3),\n     \"d\" = seq(0, 2),\n     \"q\" = seq(0, 3)) %&gt;%\n    cross() %&gt;%\n    head(3)\n## [[1]]\n## [[1]]$p\n## [1] 0\n## \n## [[1]]$d\n## [1] 0\n## \n## [[1]]$q\n## [1] 0\n## \n## \n## [[2]]\n## [[2]]$p\n## [1] 1\n## \n## [[2]]$d\n## [1] 0\n## \n## [[2]]$q\n## [1] 0\n## \n## \n## [[3]]\n## [[3]]$p\n## [1] 2\n## \n## [[3]]$d\n## [1] 0\n## \n## [[3]]$q\n## [1] 0\n\nI would like to have something like this instead:\n\n[[1]]\np d q \n0 0 0 \n\n[[2]]\np d q \n1 0 0 \n\n[[3]]\np d q \n2 0 0 \n\nThis is possible with the last line, map(lift(c)). There’s a lot going on in this very small line of code. First of all, there’s map(). map() iterates over lists, and applies a function, in this case lift(c). purrr::lift() is a very interesting function that lifts the domain of definition of a function from one type of input to another. The function whose input I am lifting is c(). So now, c() can take a list instead of a vector. Compare the following:\n\n# The usual\n\nc(\"a\", \"b\")\n## [1] \"a\" \"b\"\n# Nothing happens\nc(list(\"a\", \"b\"))\n## [[1]]\n## [1] \"a\"\n## \n## [[2]]\n## [1] \"b\"\n# Magic happens\nlift(c)(list(\"a\", \"b\"))\n## [1] \"a\" \"b\"\n\nSo order_list is exactly what I wanted:\n\nhead(order_list)\n## [[1]]\n## p d q \n## 0 0 0 \n## \n## [[2]]\n## p d q \n## 1 0 0 \n## \n## [[3]]\n## p d q \n## 2 0 0 \n## \n## [[4]]\n## p d q \n## 3 0 0 \n## \n## [[5]]\n## p d q \n## 0 1 0 \n## \n## [[6]]\n## p d q \n## 1 1 0\n\nI do the same for season_list:\n\nseason_list &lt;- list(\"P\" = seq(0, 3),\n                    \"D\" = seq(0, 2),\n                    \"Q\" = seq(0, 3),\n                    \"period\" = 12)  %&gt;%\n    cross() %&gt;%\n    map(lift(c))\n\nI now coerce these two lists of vectors to tibbles:\n\norderdf &lt;- tibble(\"order\" = order_list)\n\nseasondf &lt;- tibble(\"season\" = season_list)\n\nAnd I can now finally create the grid of hyper-parameters:\n\nhyper_parameters_df &lt;- crossing(orderdf, seasondf)\n\nnrows &lt;- nrow(hyper_parameters_df)\n\nhead(hyper_parameters_df)\n## # A tibble: 6 x 2\n##   order     season   \n##   &lt;list&gt;    &lt;list&gt;   \n## 1 &lt;int [3]&gt; &lt;dbl [4]&gt;\n## 2 &lt;int [3]&gt; &lt;dbl [4]&gt;\n## 3 &lt;int [3]&gt; &lt;dbl [4]&gt;\n## 4 &lt;int [3]&gt; &lt;dbl [4]&gt;\n## 5 &lt;int [3]&gt; &lt;dbl [4]&gt;\n## 6 &lt;int [3]&gt; &lt;dbl [4]&gt;\n\nThe hyper_parameters_df data frame has 2304 rows, meaning, I will now estimate 2304 models, and will do so in parallel. Let’s just take a quick look at the internals of hyper_parameters_df:\n\nglimpse(hyper_parameters_df)\n## Observations: 2,304\n## Variables: 2\n## $ order  &lt;list&gt; [&lt;0, 0, 0&gt;, &lt;0, 0, 0&gt;, &lt;0, 0, 0&gt;, &lt;0, 0, 0&gt;, &lt;0, 0, 0&gt;, …\n## $ season &lt;list&gt; [&lt;0, 0, 0, 12&gt;, &lt;1, 0, 0, 12&gt;, &lt;2, 0, 0, 12&gt;, &lt;3, 0, 0, …\n\nSo in the order column, the vector 0, 0, 0 is repeated as many times as there are combinations of P, D, Q, S for season. Same for all the other vectors of the order column."
  },
  {
    "objectID": "posts/2018-11-15-tidy_gridsearch.html#training-the-models",
    "href": "posts/2018-11-15-tidy_gridsearch.html#training-the-models",
    "title": "Searching for the optimal hyper-parameters of an ARIMA model in parallel: the tidy gridsearch approach",
    "section": "\nTraining the models\n",
    "text": "Training the models\n\n\nBecause training these models might take some time, I will use the fantastic {furrr} package by Davis Vaughan to train the arima() function in parallel. For this, I first define 8 workers:\n\nplan(multiprocess, workers = 8)\n\nAnd then I run the code:\n\ntic &lt;- Sys.time()\nmodels_df &lt;- hyper_parameters_df %&gt;%\n    mutate(models = future_map2(.x = order,\n                         .y = season,\n                         ~possibly(arima, otherwise = NULL)(x = logged_train_data,\n                                                                           order = .x, seasonal = .y)))\nrunning_time &lt;- Sys.time() - tic\n\nI use future_map2(), which is just like map2() but running in parallel. I add a new column to the data called models, which will contain the models trained over all the different combinations of order and season. The models are trained on the logged_train_data.\n\n\nTraining the 2304 models took 18 minutes, which is plenty of time to browse the latest memes, but still quick enough that it justifies the whole approach. Let’s take a look at the models_df object:\n\nhead(models_df)\n## # A tibble: 6 x 3\n##   order     season    models \n##   &lt;list&gt;    &lt;list&gt;    &lt;list&gt; \n## 1 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;\n## 2 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;\n## 3 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;\n## 4 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;\n## 5 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;\n## 6 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;\n\nAs you can see, the models column contains all the trained models. The model on the first row, was trained with the hyperparameters of row 1, and so on. But, our work is not over! We now need to find the best model. First, I add a new column to the tibble, which contains the forecast. From the forecast, I extract the point estimate:\n\nmodels_df %&gt;%\n    mutate(forecast = map(models, ~possibly(forecast, otherwise = NULL)(., h = 39))) %&gt;%\n    mutate(point_forecast = map(forecast, ~.$`mean`)) %&gt;%\n    ....\n\nYou have to be familiar with a forecast object to understand the last line: a forecast object is a list with certain elements, the point estimates, the confidence intervals, and so on. To get the point estimates, I have to extract the “mean” element from the list. Hence the weird ~.$mean. Then I need to add a new list-column, where each element is the vector of true values, meaning the data from 2015 to 2018. Because I have to add it as a list of size 2304, I do that with purrr::rerun():\n\nrerun(5, c(\"a\", \"b\", \"c\"))\n## [[1]]\n## [1] \"a\" \"b\" \"c\"\n## \n## [[2]]\n## [1] \"a\" \"b\" \"c\"\n## \n## [[3]]\n## [1] \"a\" \"b\" \"c\"\n## \n## [[4]]\n## [1] \"a\" \"b\" \"c\"\n## \n## [[5]]\n## [1] \"a\" \"b\" \"c\"\n\nIt is then easy to compute the RMSE, which I add as a column to the original data:\n\n... %&gt;%\n    mutate(true_value = rerun(nrows, logged_test_data)) %&gt;%\n    mutate(rmse = map2_dbl(point_forecast, true_value,\n                           ~sqrt(mean((.x - .y) ** 2))))\n\nThe whole workflow is here:\n\nmodels_df &lt;- models_df %&gt;%\n    mutate(forecast = map(models, ~possibly(forecast, otherwise = NULL)(., h = 39))) %&gt;%\n    mutate(point_forecast = map(forecast, ~.$`mean`)) %&gt;%\n    mutate(true_value = rerun(nrows, logged_test_data)) %&gt;%\n    mutate(rmse = map2_dbl(point_forecast, true_value,\n                           ~sqrt(mean((.x - .y) ** 2))))\n\nThis is how models_df looks now:\n\nhead(models_df)\n## # A tibble: 6 x 7\n##   order     season    models  forecast   point_forecast true_value  rmse\n##   &lt;list&gt;    &lt;list&gt;    &lt;list&gt;  &lt;list&gt;     &lt;list&gt;         &lt;list&gt;     &lt;dbl&gt;\n## 1 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.525\n## 2 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.236\n## 3 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.235\n## 4 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.217\n## 5 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.190\n## 6 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.174\n\nNow, I can finally select the best performing model. I select the model with minimum RMSE:\n\nbest_model &lt;- models_df %&gt;%\n    filter(rmse == min(rmse, na.rm = TRUE))\n\nAnd save the forecast into a new variable, as a tibble, using my to_tibble() function:\n\n(best_model_forecast &lt;- to_tibble(best_model$forecast[[1]]))\n## Joining, by = \"date\"\n## Joining, by = \"date\"\n## # A tsibble: 39 x 6 [1M]\n##        date point_estimate upper80 upper95 lower80 lower95\n##       &lt;mth&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1 2015 Jan           11.9    12.1    12.1    11.8    11.7\n##  2 2015 Feb           11.9    12.0    12.1    11.7    11.6\n##  3 2015 Mar           12.1    12.3    12.3    11.9    11.9\n##  4 2015 Apr           12.2    12.3    12.4    12.0    11.9\n##  5 2015 May           12.2    12.4    12.5    12.1    12.0\n##  6 2015 Jun           12.3    12.4    12.5    12.1    12.0\n##  7 2015 Jul           12.2    12.3    12.4    12.0    11.9\n##  8 2015 Aug           12.3    12.5    12.6    12.2    12.1\n##  9 2015 Sep           12.3    12.5    12.6    12.2    12.1\n## 10 2015 Oct           12.2    12.4    12.5    12.1    12.0\n## # … with 29 more rows\n\nAnd now, I can plot it:\n\navia_clean_monthly %&gt;%\n    group_by(date) %&gt;%\n    summarise(total = sum(passengers)) %&gt;%\n    mutate(total_ihs = ihs(total)) %&gt;%\n    ggplot() +\n    ggtitle(\"Logged data\") +\n    geom_line(aes(y = total_ihs, x = date), colour = \"#82518c\") +\n    scale_x_date(date_breaks = \"1 year\", date_labels = \"%m-%Y\") +\n    geom_ribbon(data = best_model_forecast, aes(x = date, ymin = lower95, ymax = upper95), \n                fill = \"#666018\", alpha = 0.2) +\n    geom_line(data = best_model_forecast, aes(x = date, y = point_estimate), linetype = 2, colour = \"#8e9d98\") +\n    theme_blog()\n\n\n\n\nCompared to the previous blog post, the dotted line now seems to follow the true line even better! However, this is not suprising, as I am using the test set as a validation set, which might lead to overfitting the hyperparameters to the test set. Also, I am not saying that you should always do a gridsearch whenever you have a problem like this one. In the case of univariate time series, I am still doubtful that a gridsearch like this is really necessary. The goal of this blog post was not to teach you how to look for hyperparameters per se, but more to show you how to do a grid search the tidy way. I’ll be writing about proper hyperparameter optimization in a future blog post. Also, the other thing I wanted to show was the power of {furrr}."
  },
  {
    "objectID": "posts/2018-07-08-rob_stderr.html",
    "href": "posts/2018-07-08-rob_stderr.html",
    "title": "Dealing with heteroskedasticity; regression with robust standard errors using R",
    "section": "",
    "text": "First of all, is it heteroskedasticity or heteroscedasticity? According to McCulloch (1985), heteroskedasticity is the proper spelling, because when transliterating Greek words, scientists use the Latin letter k in place of the Greek letter κ (kappa). κ sometimes is transliterated as the Latin letter c, but only when these words entered the English language through French, such as scepter.\n\n\nNow that this is out of the way, we can get to the meat of this blogpost (foreshadowing pun). A random variable is said to be heteroskedastic, if its variance is not constant. For example, the variability of expenditures may increase with income. Richer families may spend a similar amount on groceries as poorer people, but some rich families will sometimes buy expensive items such as lobster. The variability of expenditures for rich families is thus quite large. However, the expenditures on food of poorer families, who cannot afford lobster, will not vary much. Heteroskedasticity can also appear when data is clustered; for example, variability of expenditures on food may vary from city to city, but is quite constant within a city.\n\n\nTo illustrate this, let’s first load all the packages needed for this blog post:\n\nlibrary(robustbase)\nlibrary(tidyverse)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(modelr)\nlibrary(broom)\n\nFirst, let’s load and prepare the data:\n\ndata(\"education\")\n\neducation &lt;- education %&gt;% \n    rename(residents = X1,\n           per_capita_income = X2,\n           young_residents = X3,\n           per_capita_exp = Y,\n           state = State) %&gt;% \n    mutate(region = case_when(\n        Region == 1 ~ \"northeast\",\n        Region == 2 ~ \"northcenter\",\n        Region == 3 ~ \"south\",\n        Region == 4 ~ \"west\"\n    )) %&gt;% \n    select(-Region)\n\nI will be using the education data set from the {robustbase} package. I renamed some columns and changed the values of the Region column. Now, let’s do a scatterplot of per capita expenditures on per capita income:\n\nggplot(education, aes(per_capita_income, per_capita_exp)) + \n    geom_point() +\n    theme_dark()\n\n\n\n\nIt would seem that, as income increases, variability of expenditures increases too. Let’s look at the same plot by region:\n\nggplot(education, aes(per_capita_income, per_capita_exp)) + \n    geom_point() + \n    facet_wrap(~region) + \n    theme_dark()\n\n\n\n\nI don’t think this shows much; it would seem that observations might be clustered, but there are not enough observations to draw any conclusion from this plot (in any case, drawing conclusions from only plots is dangerous).\n\n\nLet’s first run a good ol’ linear regression:\n\nlmfit &lt;- lm(per_capita_exp ~ region + residents + young_residents + per_capita_income, data = education)\n\nsummary(lmfit)\n## \n## Call:\n## lm(formula = per_capita_exp ~ region + residents + young_residents + \n##     per_capita_income, data = education)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -77.963 -25.499  -2.214  17.618  89.106 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)       -467.40283  142.57669  -3.278 0.002073 ** \n## regionnortheast     15.72741   18.16260   0.866 0.391338    \n## regionsouth          7.08742   17.29950   0.410 0.684068    \n## regionwest          34.32416   17.49460   1.962 0.056258 .  \n## residents           -0.03456    0.05319  -0.650 0.519325    \n## young_residents      1.30146    0.35717   3.644 0.000719 ***\n## per_capita_income    0.07204    0.01305   5.520 1.82e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 39.88 on 43 degrees of freedom\n## Multiple R-squared:  0.6292, Adjusted R-squared:  0.5774 \n## F-statistic: 12.16 on 6 and 43 DF,  p-value: 6.025e-08\n\nLet’s test for heteroskedasticity using the Breusch-Pagan test that you can find in the {lmtest} package:\n\nbptest(lmfit)\n## \n##  studentized Breusch-Pagan test\n## \n## data:  lmfit\n## BP = 17.921, df = 6, p-value = 0.006432\n\nThis test shows that we can reject the null that the variance of the residuals is constant, thus heteroskedacity is present. To get the correct standard errors, we can use the vcovHC() function from the {sandwich} package (hence the choice for the header picture of this post):\n\nlmfit %&gt;% \n    vcovHC() %&gt;% \n    diag() %&gt;% \n    sqrt()\n##       (Intercept)   regionnortheast       regionsouth        regionwest \n##      311.31088691       25.30778221       23.56106307       24.12258706 \n##         residents   young_residents per_capita_income \n##        0.09184368        0.68829667        0.02999882\n\nBy default vcovHC() estimates a heteroskedasticity consistent (HC) variance covariance matrix for the parameters. There are several ways to estimate such a HC matrix, and by default vcovHC() estimates the “HC3” one. You can refer to Zeileis (2004) for more details.\n\n\nWe see that the standard errors are much larger than before! The intercept and regionwest variables are not statistically significant anymore.\n\n\nYou can achieve the same in one single step:\n\ncoeftest(lmfit, vcov = vcovHC(lmfit))\n## \n## t test of coefficients:\n## \n##                      Estimate  Std. Error t value Pr(&gt;|t|)  \n## (Intercept)       -467.402827  311.310887 -1.5014  0.14056  \n## regionnortheast     15.727405   25.307782  0.6214  0.53759  \n## regionsouth          7.087424   23.561063  0.3008  0.76501  \n## regionwest          34.324157   24.122587  1.4229  0.16198  \n## residents           -0.034558    0.091844 -0.3763  0.70857  \n## young_residents      1.301458    0.688297  1.8908  0.06540 .\n## per_capita_income    0.072036    0.029999  2.4013  0.02073 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIt’s is also easy to change the estimation method for the variance-covariance matrix:\n\ncoeftest(lmfit, vcov = vcovHC(lmfit, type = \"HC0\"))\n## \n## t test of coefficients:\n## \n##                      Estimate  Std. Error t value  Pr(&gt;|t|)    \n## (Intercept)       -467.402827  172.577569 -2.7084  0.009666 ** \n## regionnortheast     15.727405   20.488148  0.7676  0.446899    \n## regionsouth          7.087424   17.755889  0.3992  0.691752    \n## regionwest          34.324157   19.308578  1.7777  0.082532 .  \n## residents           -0.034558    0.054145 -0.6382  0.526703    \n## young_residents      1.301458    0.387743  3.3565  0.001659 ** \n## per_capita_income    0.072036    0.016638  4.3296 8.773e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAs I wrote above, by default, the type argument is equal to “HC3”.\n\n\nAnother way of dealing with heteroskedasticity is to use the lmrob() function from the {robustbase} package. This package is quite interesting, and offers quite a lot of functions for robust linear, and nonlinear, regression models. Running a robust linear regression is just the same as with lm():\n\nlmrobfit &lt;- lmrob(per_capita_exp ~ region + residents + young_residents + per_capita_income, \n                  data = education)\n\nsummary(lmrobfit)\n## \n## Call:\n## lmrob(formula = per_capita_exp ~ region + residents + young_residents + per_capita_income, \n##     data = education)\n##  \\--&gt; method = \"MM\"\n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -57.074 -14.803  -0.853  24.154 174.279 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)       -156.37169  132.73828  -1.178  0.24526   \n## regionnortheast     20.64576   26.45378   0.780  0.43940   \n## regionsouth         10.79695   29.42746   0.367  0.71549   \n## regionwest          45.22589   33.07950   1.367  0.17867   \n## residents            0.03406    0.04412   0.772  0.44435   \n## young_residents      0.57896    0.25512   2.269  0.02832 * \n## per_capita_income    0.04328    0.01442   3.000  0.00447 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Robust residual standard error: 26.4 \n## Multiple R-squared:  0.6235, Adjusted R-squared:  0.571 \n## Convergence in 24 IRWLS iterations\n## \n## Robustness weights: \n##  observation 50 is an outlier with |weight| = 0 ( &lt; 0.002); \n##  7 weights are ~= 1. The remaining 42 ones are summarized as\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n## 0.05827 0.85200 0.93870 0.85250 0.98700 0.99790 \n## Algorithmic parameters: \n##        tuning.chi                bb        tuning.psi        refine.tol \n##         1.548e+00         5.000e-01         4.685e+00         1.000e-07 \n##           rel.tol         scale.tol         solve.tol       eps.outlier \n##         1.000e-07         1.000e-10         1.000e-07         2.000e-03 \n##             eps.x warn.limit.reject warn.limit.meanrw \n##         1.071e-08         5.000e-01         5.000e-01 \n##      nResample         max.it       best.r.s       k.fast.s          k.max \n##            500             50              2              1            200 \n##    maxit.scale      trace.lev            mts     compute.rd fast.s.large.n \n##            200              0           1000              0           2000 \n##                   psi           subsampling                   cov \n##            \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \n## compute.outlier.stats \n##                  \"SM\" \n## seed : int(0)\n\nThis however, gives you different estimates than when fitting a linear regression model. The estimates should be the same, only the standard errors should be different. This is because the estimation method is different, and is also robust to outliers (at least that’s my understanding, I haven’t read the theoretical papers behind the package yet).\n\n\nFinally, it is also possible to bootstrap the standard errors. For this I will use the bootstrap() function from the {modelr} package:\n\nresamples &lt;- 100\n\nboot_education &lt;- education %&gt;% \n modelr::bootstrap(resamples)\n\nLet’s take a look at the boot_education object:\n\nboot_education\n## # A tibble: 100 x 2\n##    strap      .id  \n##    &lt;list&gt;     &lt;chr&gt;\n##  1 &lt;resample&gt; 001  \n##  2 &lt;resample&gt; 002  \n##  3 &lt;resample&gt; 003  \n##  4 &lt;resample&gt; 004  \n##  5 &lt;resample&gt; 005  \n##  6 &lt;resample&gt; 006  \n##  7 &lt;resample&gt; 007  \n##  8 &lt;resample&gt; 008  \n##  9 &lt;resample&gt; 009  \n## 10 &lt;resample&gt; 010  \n## # … with 90 more rows\n\nThe column strap contains resamples of the original data. I will run my linear regression from before on each of the resamples:\n\n(\n    boot_lin_reg &lt;- boot_education %&gt;% \n        mutate(regressions = \n                   map(strap, \n                       ~lm(per_capita_exp ~ region + residents + \n                               young_residents + per_capita_income, \n                           data = .))) \n)\n## # A tibble: 100 x 3\n##    strap      .id   regressions\n##    &lt;list&gt;     &lt;chr&gt; &lt;list&gt;     \n##  1 &lt;resample&gt; 001   &lt;lm&gt;       \n##  2 &lt;resample&gt; 002   &lt;lm&gt;       \n##  3 &lt;resample&gt; 003   &lt;lm&gt;       \n##  4 &lt;resample&gt; 004   &lt;lm&gt;       \n##  5 &lt;resample&gt; 005   &lt;lm&gt;       \n##  6 &lt;resample&gt; 006   &lt;lm&gt;       \n##  7 &lt;resample&gt; 007   &lt;lm&gt;       \n##  8 &lt;resample&gt; 008   &lt;lm&gt;       \n##  9 &lt;resample&gt; 009   &lt;lm&gt;       \n## 10 &lt;resample&gt; 010   &lt;lm&gt;       \n## # … with 90 more rows\n\nI have added a new column called regressions which contains the linear regressions on each bootstrapped sample. Now, I will create a list of tidied regression results:\n\n(\n    tidied &lt;- boot_lin_reg %&gt;% \n        mutate(tidy_lm = \n                   map(regressions, broom::tidy))\n)\n## # A tibble: 100 x 4\n##    strap      .id   regressions tidy_lm         \n##    &lt;list&gt;     &lt;chr&gt; &lt;list&gt;      &lt;list&gt;          \n##  1 &lt;resample&gt; 001   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  2 &lt;resample&gt; 002   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  3 &lt;resample&gt; 003   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  4 &lt;resample&gt; 004   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  5 &lt;resample&gt; 005   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  6 &lt;resample&gt; 006   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  7 &lt;resample&gt; 007   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  8 &lt;resample&gt; 008   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  9 &lt;resample&gt; 009   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n## 10 &lt;resample&gt; 010   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n## # … with 90 more rows\n\nbroom::tidy() creates a data frame of the regression results. Let’s look at one of these:\n\ntidied$tidy_lm[[1]]\n## # A tibble: 7 x 5\n##   term              estimate std.error statistic  p.value\n##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)       -571.     109.        -5.22  4.92e- 6\n## 2 regionnortheast    -48.0     17.2       -2.80  7.71e- 3\n## 3 regionsouth        -21.3     15.1       -1.41  1.66e- 1\n## 4 regionwest           1.88    13.9        0.135 8.93e- 1\n## 5 residents           -0.134    0.0608    -2.21  3.28e- 2\n## 6 young_residents      1.50     0.308      4.89  1.47e- 5\n## 7 per_capita_income    0.100    0.0125     8.06  3.85e-10\n\nThis format is easier to handle than the standard lm() output:\n\ntidied$regressions[[1]]\n## \n## Call:\n## lm(formula = per_capita_exp ~ region + residents + young_residents + \n##     per_capita_income, data = .)\n## \n## Coefficients:\n##       (Intercept)    regionnortheast        regionsouth  \n##         -571.0568           -48.0018           -21.3019  \n##        regionwest          residents    young_residents  \n##            1.8808            -0.1341             1.5042  \n## per_capita_income  \n##            0.1005\n\nNow that I have all these regression results, I can compute any statistic I need. But first, let’s transform the data even further:\n\nlist_mods &lt;- tidied %&gt;% \n    pull(tidy_lm)\n\nlist_mods is a list of the tidy_lm data frames. I now add an index and bind the rows together (by using map2_df() instead of map2()):\n\nmods_df &lt;- map2_df(list_mods, \n                   seq(1, resamples), \n                   ~mutate(.x, resample = .y))\n\nLet’s take a look at the final object:\n\nhead(mods_df, 25)\n## # A tibble: 25 x 6\n##    term              estimate std.error statistic  p.value resample\n##    &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;\n##  1 (Intercept)       -571.     109.        -5.22  4.92e- 6        1\n##  2 regionnortheast    -48.0     17.2       -2.80  7.71e- 3        1\n##  3 regionsouth        -21.3     15.1       -1.41  1.66e- 1        1\n##  4 regionwest           1.88    13.9        0.135 8.93e- 1        1\n##  5 residents           -0.134    0.0608    -2.21  3.28e- 2        1\n##  6 young_residents      1.50     0.308      4.89  1.47e- 5        1\n##  7 per_capita_income    0.100    0.0125     8.06  3.85e-10        1\n##  8 (Intercept)        -97.2    145.        -0.672 5.05e- 1        2\n##  9 regionnortheast     -1.48    10.8       -0.136 8.92e- 1        2\n## 10 regionsouth         12.5     11.4        1.09  2.82e- 1        2\n## # … with 15 more rows\n\nNow this is a very useful format, because I now can group by the term column and compute any statistics I need, in the present case the standard deviation:\n\n(\n    r.std.error &lt;- mods_df %&gt;% \n        group_by(term) %&gt;% \n        summarise(r.std.error = sd(estimate))\n)\n## # A tibble: 7 x 2\n##   term              r.std.error\n##   &lt;chr&gt;                   &lt;dbl&gt;\n## 1 (Intercept)          220.    \n## 2 per_capita_income      0.0197\n## 3 regionnortheast       24.5   \n## 4 regionsouth           21.1   \n## 5 regionwest            22.7   \n## 6 residents              0.0607\n## 7 young_residents        0.498\n\nWe can append this column to the linear regression model result:\n\nlmfit %&gt;% \n    broom::tidy() %&gt;% \n    full_join(r.std.error) %&gt;% \n    select(term, estimate, std.error, r.std.error)\n## Joining, by = \"term\"\n## # A tibble: 7 x 4\n##   term               estimate std.error r.std.error\n##   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n## 1 (Intercept)       -467.      143.        220.    \n## 2 regionnortheast     15.7      18.2        24.5   \n## 3 regionsouth          7.09     17.3        21.1   \n## 4 regionwest          34.3      17.5        22.7   \n## 5 residents           -0.0346    0.0532      0.0607\n## 6 young_residents      1.30      0.357       0.498 \n## 7 per_capita_income    0.0720    0.0131      0.0197\n\nAs you see, using the whole bootstrapping procedure is longer than simply using either one of the first two methods. However, this procedure is very flexible and can thus be adapted to a very large range of situations. Either way, in the case of heteroskedasticity, you can see that results vary a lot depending on the procedure you use, so I would advise to use them all as robustness tests and discuss the differences."
  },
  {
    "objectID": "posts/2019-07-19-statmatch.html",
    "href": "posts/2019-07-19-statmatch.html",
    "title": "Statistical matching, or when one single data source is not enough",
    "section": "",
    "text": "I was recently asked how to go about matching several datasets where different samples of individuals were interviewed. This sounds like a big problem; say that you have dataset A and B, and that A contain one sample of individuals, and B another sample of individuals, then how could you possibly match the datasets? Matching datasets requires a common identifier, for instance, suppose that A contains socio-demographic information on a sample of individuals I, while B, contains information on wages and hours worked on the same sample of individuals I, then yes, it will be possible to match/merge/join both datasets.\n\n\nBut that was not what I was asked about; I was asked about a situation where the same population gets sampled twice, and each sample answers to a different survey. For example the first survey is about labour market information and survey B is about family structure. Would it be possible to combine the information from both datasets?\n\n\nTo me, this sounded a bit like missing data imputation problem, but where all the information about the variables of interest was missing! I started digging a bit, and found that not only there was already quite some literature on it, there is even a package for this, called {StatMatch} with a very detailed vignette. The vignette is so detailed, that I will not write any code, I just wanted to share this package!"
  },
  {
    "objectID": "posts/2018-02-16-importing_30gb_of_data.html",
    "href": "posts/2018-02-16-importing_30gb_of_data.html",
    "title": "Importing 30GB of data into R with sparklyr",
    "section": "",
    "text": "Disclaimer: the first part of this blog post draws heavily from Working with CSVs on the Command Line, which is a beautiful resource that lists very nice tips and tricks to work with CSV files before having to load them into R, or any other statistical software. I highly recommend it! Also, if you find this interesting, read also Data Science at the Command Line another great resource!\n\n\nIn this blog post I am going to show you how to analyze 30GB of data. 30GB of data does not qualify as big data, but it’s large enough that you cannot simply import it into R and start working on it, unless you have a machine with a lot of RAM.\n\n\nLet’s start by downloading some data. I am going to import and analyze (very briefly) the airline dataset that you can download from Microsoft here. I downloaded the file AirOnTimeCSV.zip from AirOnTime87to12. Once you decompress it, you’ll end up with 303 csv files, each around 80MB. Before importing them into R, I will use command line tools to bind the rows together. But first, let’s make sure that the datasets all have the same columns. I am using Linux, and if you are too, or if you are using macOS, you can follow along. Windows users that installed the Linux Subsystem can also use the commands I am going to show! First, I’ll use the head command in bash. If you’re familiar with head() from R, the head command in bash works exactly the same:\n\n[18-02-15 21:12] brodriguesco in /Documents/AirOnTimeCSV ➤ head -5 airOT198710.csv\n\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\",\"FL_DATE\",\"UNIQUE_CARRIER\",\"TAIL_NUM\",\"FL_NUM\",\n1987,10,1,4,1987-10-01,\"AA\",\"\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0901\",1.00,\n1987,10,2,5,1987-10-02,\"AA\",\"\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0901\",1.00\n1987,10,3,6,1987-10-03,\"AA\",\"\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0859\",-1.00\n1987,10,4,7,1987-10-04,\"AA\",\"\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0900\",0.00,\n\nlet’s also check the 5 first lines of the last file:\n\n[18-02-15 21:13] cbrunos in brodriguesco in /Documents/AirOnTimeCSV ➤ head -5 airOT201212.csv\n\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\",\"FL_DATE\",\"UNIQUE_CARRIER\",\"TAIL_NUM\",\"FL_NUM\",\n2012,12,1,6,2012-12-01,\"AA\",\"N322AA\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0852\",\n2012,12,2,7,2012-12-02,\"AA\",\"N327AA\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0853\",\n2012,12,3,1,2012-12-03,\"AA\",\"N319AA\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0856\"\n2012,12,4,2,2012-12-04,\"AA\",\"N329AA\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"1006\"\n\nWhy do that in bash instead of R? This way, I don’t need to import the data into R before checking its contents!\n\n\nIt does look like the structure did not change. Before importing the data into R, I am going to bind the rows of the datasets using other command line tools. Again, the reason I don’t import all the files into R is because I would need around 30GB of RAM to do so. So it’s easier to do it with bash:\n\nhead -1 airOT198710.csv &gt; combined.csv\nfor file in $(ls airOT*); do cat $file | sed \"1 d\" &gt;&gt; combined.csv; done\n\nOn the first line I use head again to only copy the column names (the first line of the first file) into a new file called combined.csv.\n\n\nThis &gt; operator looks like the now well known pipe operator in R, %&gt;%, but in bash, %&gt;% is actually |, not &gt;. &gt; redirects the output of the left hand side to a file on the right hand side, not to another command. On the second line, I loop over the files. I list the files with ls, and because I want only to loop over those that are named airOTxxxxx I use a regular expression, airOT* to only list those. The second part is do cat $file. do is self-explanatory, and cat stands for catenate. Think of it as head, but on all rows instead of just 5; it prints $file to the terminal. $file one element of the list of files I am looping over. But because I don’t want to see the contents of $file on my terminal, I redirect the output with the pipe, | to another command, sed. sed has an option, \"1 d\", and what this does is filtering out the first line, containing the header, from $file before appending it with &gt;&gt; to combined.csv. If you found this interesting, read more about it here.\n\n\nThis creates a 30GB CSV file that you can then import. But how? There seems to be different ways to import and work with larger than memory data in R using your personal computer. I chose to use {sparklyr}, an R package that allows you to work with Apache Spark from R. Apache Spark is a fast and general engine for large-scale data processing, and {sparklyr} not only offers bindings to it, but also provides a complete {dplyr} backend. Let’s start:\n\nlibrary(sparklyr)\nlibrary(tidyverse)\n\nspark_dir = \"/my_2_to_disk/spark/\"\n\nI first load {sparklyr} and the {tidyverse} and also define a spark_dir. This is because Spark creates a lot of temporary files that I want to save there instead of my root partition, which is on my SSD. My root partition only has around 20GO of space left, so whenever I tried to import the data I would get the following error:\n\njava.io.IOException: No space left on device\n\nIn order to avoid this error, I define this directory on my 2TO hard disk. I then define the temporary directory using the two lines below:\n\nconfig = spark_config()\n\nconfig$`sparklyr.shell.driver-java-options` &lt;-  paste0(\"-Djava.io.tmpdir=\", spark_dir)\n\nThis is not sufficient however; when I tried to read in the data, I got another error:\n\njava.lang.OutOfMemoryError: Java heap space\n\nThe solution for this one is to add the following lines to your config():\n\nconfig$`sparklyr.shell.driver-memory` &lt;- \"4G\"\nconfig$`sparklyr.shell.executor-memory` &lt;- \"4G\"\nconfig$`spark.yarn.executor.memoryOverhead` &lt;- \"512\"\n\nFinally, I can load the data. Because I am working on my machine, I connect to a \"local\" Spark instance. Then, using spark_read_csv(), I specify the Spark connection, sc, I give a name to the data that will be inside the database and the path to it:\n\nsc = spark_connect(master = \"local\", config = config)\n\nair = spark_read_csv(sc, name = \"air\", path = \"combined.csv\")\n\nOn my machine, this took around 25 minutes, and RAM usage was around 6GO.\n\n\nIt is possible to use standard {dplyr} verbs with {sparklyr} objects, so if I want the mean delay at departure per day, I can simply write:\n\ntic = Sys.time()\nmean_dep_delay = air %&gt;%\n  group_by(YEAR, MONTH, DAY_OF_MONTH) %&gt;%\n  summarise(mean_delay = mean(DEP_DELAY))\n(toc = Sys.time() - tic)\nTime difference of 0.05634999 secs\n\nThat’s amazing, only 0.06 seconds to compute these means! Wait a minute, that’s weird… I mean my computer is brand new and quite powerful but still… Let’s take a look at mean_dep_delay:\n\nhead(mean_dep_delay)\n# Source:   lazy query [?? x 4]\n# Database: spark_connection\n# Groups:   YEAR, MONTH\n   YEAR MONTH DAY_OF_MONTH mean_delay\n  &lt;int&gt; &lt;int&gt;        &lt;int&gt;      &lt;dbl&gt;\n1  1987    10            9       6.71\n2  1987    10           10       3.72\n3  1987    10           12       4.95\n4  1987    10           14       4.53\n5  1987    10           23       6.48\n6  1987    10           29       5.77\nWarning messages:\n1: Missing values are always removed in SQL.\nUse `AVG(x, na.rm = TRUE)` to silence this warning\n2: Missing values are always removed in SQL.\nUse `AVG(x, na.rm = TRUE)` to silence this warning\n\nSurprisingly, this takes around 5 minutes to print? Why? Look at the class of mean_dep_delay: it’s a lazy query that only gets evaluated once I need it. Look at the first line; lazy query [?? x 4]. This means that I don’t even know how many rows are in mean_dep_delay! The contents of mean_dep_delay only get computed once I explicitly ask for them. I do so with the collect() function, which transfers the Spark object into R’s memory:\n\ntic = Sys.time()\nr_mean_dep_delay = collect(mean_dep_delay)\n(toc = Sys.time() - tic)\nTime difference of 5.2399 mins\n\nAlso, because it took such a long time to compute: I save it to disk:\n\nsaveRDS(r_mean_dep_delay, \"mean_dep_delay.rds\")\n\nSo now that I transferred this sparklyr table to a standard tibble in R, I can create a nice plot of departure delays:\n\nlibrary(lubridate)\n\ndep_delay =  r_mean_dep_delay %&gt;%\n  arrange(YEAR, MONTH, DAY_OF_MONTH) %&gt;%\n  mutate(date = ymd(paste(YEAR, MONTH, DAY_OF_MONTH, sep = \"-\")))\n\nggplot(dep_delay, aes(date, mean_delay)) + geom_smooth()\n## `geom_smooth()` using method = 'gam'\n\n\n\n\nThat’s it for now, but in a future blog post I will continue to explore this data!\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2017-07-27-spread_rename_at.html",
    "href": "posts/2017-07-27-spread_rename_at.html",
    "title": "tidyr::spread() and dplyr::rename_at() in action",
    "section": "",
    "text": "I was recently confronted to a situation that required going from a long dataset to a wide dataset, but with a small twist: there were two datasets, which I had to merge into one. You might wonder what kinda crappy twist that is, right? Well, let’s take a look at the data:\n\ndata1; data2\n## # A tibble: 20 x 4\n##    country date       variable_1       value\n##    &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;            &lt;int&gt;\n##  1 lu      01/01/2005 maybe               22\n##  2 lu      01/07/2005 maybe               13\n##  3 lu      01/01/2006 maybe               40\n##  4 lu      01/07/2006 maybe               25\n##  5 lu      01/01/2005 totally_agree       42\n##  6 lu      01/07/2005 totally_agree       17\n##  7 lu      01/01/2006 totally_agree       25\n##  8 lu      01/07/2006 totally_agree       16\n##  9 lu      01/01/2005 totally_disagree    39\n## 10 lu      01/07/2005 totally_disagree    17\n## 11 lu      01/01/2006 totally_disagree    23\n## 12 lu      01/07/2006 totally_disagree    21\n## 13 lu      01/01/2005 kinda_disagree      69\n## 14 lu      01/07/2005 kinda_disagree      12\n## 15 lu      01/01/2006 kinda_disagree      10\n## 16 lu      01/07/2006 kinda_disagree       9\n## 17 lu      01/01/2005 kinda_agree         38\n## 18 lu      01/07/2005 kinda_agree         31\n## 19 lu      01/01/2006 kinda_agree         19\n## 20 lu      01/07/2006 kinda_agree         12\n## # A tibble: 20 x 4\n##    country date       variable_2       value\n##    &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;            &lt;int&gt;\n##  1 lu      01/01/2005 kinda_agree         22\n##  2 lu      01/07/2005 kinda_agree         13\n##  3 lu      01/01/2006 kinda_agree         40\n##  4 lu      01/07/2006 kinda_agree         25\n##  5 lu      01/01/2005 totally_agree       42\n##  6 lu      01/07/2005 totally_agree       17\n##  7 lu      01/01/2006 totally_agree       25\n##  8 lu      01/07/2006 totally_agree       16\n##  9 lu      01/01/2005 totally_disagree    39\n## 10 lu      01/07/2005 totally_disagree    17\n## 11 lu      01/01/2006 totally_disagree    23\n## 12 lu      01/07/2006 totally_disagree    21\n## 13 lu      01/01/2005 maybe               69\n## 14 lu      01/07/2005 maybe               12\n## 15 lu      01/01/2006 maybe               10\n## 16 lu      01/07/2006 maybe                9\n## 17 lu      01/01/2005 kinda_disagree      38\n## 18 lu      01/07/2005 kinda_disagree      31\n## 19 lu      01/01/2006 kinda_disagree      19\n## 20 lu      01/07/2006 kinda_disagree      12\n\nAs explained in Hadley (2014), this is how you should keep your data… But for a particular purpose, I had to transform these datasets. What I was asked to do was to merge these into a single wide data frame. Doing this for one dataset is easy:\n\ndata1 %&gt;%\n  spread(variable_1, value)\n## # A tibble: 4 x 7\n##   country date       kinda_agree kinda_disagree maybe totally_agree\n##   &lt;chr&gt;   &lt;chr&gt;            &lt;int&gt;          &lt;int&gt; &lt;int&gt;         &lt;int&gt;\n## 1 lu      01/01/2005          38             69    22            42\n## 2 lu      01/01/2006          19             10    40            25\n## 3 lu      01/07/2005          31             12    13            17\n## 4 lu      01/07/2006          12              9    25            16\n## # ... with 1 more variable: totally_disagree &lt;int&gt;\n\nBut because data1 and data2 have the same levels for variable_1 and variable_2, this would not work. So the solution I found online, in this SO thread was to use tidyr::spread() with dplyr::rename_at() like this:\n\ndata1 &lt;- data1 %&gt;%\n  spread(variable_1, value) %&gt;%\n  rename_at(vars(-country, -date), funs(paste0(\"variable1:\", .)))\n\nglimpse(data1)\n## Observations: 4\n## Variables: 7\n## $ country                      &lt;chr&gt; \"lu\", \"lu\", \"lu\", \"lu\"\n## $ date                         &lt;chr&gt; \"01/01/2005\", \"01/01/2006\", \"01/0...\n## $ `variable1:kinda_agree`      &lt;int&gt; 38, 19, 31, 12\n## $ `variable1:kinda_disagree`   &lt;int&gt; 69, 10, 12, 9\n## $ `variable1:maybe`            &lt;int&gt; 22, 40, 13, 25\n## $ `variable1:totally_agree`    &lt;int&gt; 42, 25, 17, 16\n## $ `variable1:totally_disagree` &lt;int&gt; 39, 23, 17, 21\ndata2 &lt;- data2 %&gt;%\n  spread(variable_2, value) %&gt;%\n  rename_at(vars(-country, -date), funs(paste0(\"variable2:\", .)))\n\nglimpse(data2)\n## Observations: 4\n## Variables: 7\n## $ country                      &lt;chr&gt; \"lu\", \"lu\", \"lu\", \"lu\"\n## $ date                         &lt;chr&gt; \"01/01/2005\", \"01/01/2006\", \"01/0...\n## $ `variable2:kinda_agree`      &lt;int&gt; 22, 40, 13, 25\n## $ `variable2:kinda_disagree`   &lt;int&gt; 38, 19, 31, 12\n## $ `variable2:maybe`            &lt;int&gt; 69, 10, 12, 9\n## $ `variable2:totally_agree`    &lt;int&gt; 42, 25, 17, 16\n## $ `variable2:totally_disagree` &lt;int&gt; 39, 23, 17, 21\n\nrename_at() needs variables which you pass to vars(), a helper function to select variables, and a function that will do the renaming, passed to funs(). The function I use is simply paste0(), which pastes a string, for example “variable1:” with the name of the columns, given by the single ‘.’, a dummy argument. Now these datasets can be merged:\n\ndata1 %&gt;%\n  full_join(data2) %&gt;%\n  glimpse()\n## Joining, by = c(\"country\", \"date\")\n## Observations: 4\n## Variables: 12\n## $ country                      &lt;chr&gt; \"lu\", \"lu\", \"lu\", \"lu\"\n## $ date                         &lt;chr&gt; \"01/01/2005\", \"01/01/2006\", \"01/0...\n## $ `variable1:kinda_agree`      &lt;int&gt; 38, 19, 31, 12\n## $ `variable1:kinda_disagree`   &lt;int&gt; 69, 10, 12, 9\n## $ `variable1:maybe`            &lt;int&gt; 22, 40, 13, 25\n## $ `variable1:totally_agree`    &lt;int&gt; 42, 25, 17, 16\n## $ `variable1:totally_disagree` &lt;int&gt; 39, 23, 17, 21\n## $ `variable2:kinda_agree`      &lt;int&gt; 22, 40, 13, 25\n## $ `variable2:kinda_disagree`   &lt;int&gt; 38, 19, 31, 12\n## $ `variable2:maybe`            &lt;int&gt; 69, 10, 12, 9\n## $ `variable2:totally_agree`    &lt;int&gt; 42, 25, 17, 16\n## $ `variable2:totally_disagree` &lt;int&gt; 39, 23, 17, 21\n\nHope this post helps you understand the difference between long and wide datasets better, as well as dplyr::rename_at()!"
  },
  {
    "objectID": "posts/2016-07-26-read-a-lot-of-datasets-at-once-with-r.html",
    "href": "posts/2016-07-26-read-a-lot-of-datasets-at-once-with-r.html",
    "title": "Read a lot of datasets at once with R",
    "section": "",
    "text": "I often have to read a lot of datasets at once using R. So I’ve wrote the following function to solve this issue:\n\nread_list &lt;- function(list_of_datasets, read_func){\n\n        read_and_assign &lt;- function(dataset, read_func){\n                dataset_name &lt;- as.name(dataset)\n                dataset_name &lt;- read_func(dataset)\n        }\n\n        # invisible is used to suppress the unneeded output\n        output &lt;- invisible(\n                sapply(list_of_datasets,\n                           read_and_assign, read_func = read_func, simplify = FALSE, USE.NAMES = TRUE))\n\n        # Remove the extension at the end of the data set names\n        names_of_datasets &lt;- c(unlist(strsplit(list_of_datasets, \"[.]\"))[c(T, F)])\n        names(output) &lt;- names_of_datasets\n        return(output)\n}\n\nYou need to supply a list of datasets as well as the function to read the datasets to read_list. So for example to read in .csv files, you could use read.csv() (or read_csv() from the readr package, which I prefer to use), or read_dta() from the package haven for STATA files, and so on.\n\n\nNow imagine you have some data in your working directory. First start by saving the name of the datasets in a variable:\n\ndata_files &lt;- list.files(pattern = \".csv\")\n\nprint(data_files)\n## [1] \"data_1.csv\" \"data_2.csv\" \"data_3.csv\"\n\nNow you can read all the data sets and save them in a list with read_list():\n\nlibrary(\"readr\")\nlibrary(\"tibble\")\n\nlist_of_data_sets &lt;- read_list(data_files, read_csv)\n\n\nglimpse(list_of_data_sets)\n## List of 3\n##  $ data_1:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,018930679\" \"0,8748013128\" \"0,1025635934\" \"0,6246140983\" ...\n##   ..$ col2: chr [1:19] \"0,0377725807\" \"0,5959457638\" \"0,4429121533\" \"0,558387159\" ...\n##   ..$ col3: chr [1:19] \"0,6241767189\" \"0,031324594\" \"0,2238059868\" \"0,2773350732\" ...\n##  $ data_2:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9098418493\" \"0,1127788509\" \"0,5818891392\" \"0,1011773532\" ...\n##   ..$ col2: chr [1:19] \"0,7455905887\" \"0,4015039612\" \"0,6625796605\" \"0,029955339\" ...\n##   ..$ col3: chr [1:19] \"0,327232932\" \"0,2784035673\" \"0,8092386735\" \"0,1216045306\" ...\n##  $ data_3:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9236124896\" \"0,6303271761\" \"0,6413583054\" \"0,5573887416\" ...\n##   ..$ col2: chr [1:19] \"0,2114708388\" \"0,6984538266\" \"0,0469865249\" \"0,9271510226\" ...\n##   ..$ col3: chr [1:19] \"0,4941919971\" \"0,7391538511\" \"0,3876723797\" \"0,2815014394\" ...\n\nIf you prefer not to have the datasets in a list, but rather import them into the global environment, you can change the above function like so:\n\nread_list &lt;- function(list_of_datasets, read_func){\n\n        read_and_assign &lt;- function(dataset, read_func){\n                assign(dataset, read_func(dataset), envir = .GlobalEnv)\n        }\n\n        # invisible is used to suppress the unneeded output\n        output &lt;- invisible(\n                sapply(list_of_datasets,\n                           read_and_assign, read_func = read_func, simplify = FALSE, USE.NAMES = TRUE))\n\n}\n\nBut I personnally don’t like this second option, but I put it here for completeness."
  },
  {
    "objectID": "posts/2019-03-31-tesseract.html",
    "href": "posts/2019-03-31-tesseract.html",
    "title": "Get text from pdfs or images using OCR: a tutorial with {tesseract} and {magick}",
    "section": "",
    "text": "In this blog post I’m going to show you how you can extract text from scanned pdf files, or pdf files where no text recognition was performed. (For pdfs where text recognition was performed, you can read my other blog post).\n\n\nThe pdf I’m going to use can be downloaded from here. It’s a poem titled, D’Léierchen (Dem Léiweckerche säi Lidd), written by Michel Rodange, arguably Luxembourg’s most well known writer and poet. Michel Rodange is mostly known for his fable, Renert oder De Fuuß am Frack an a Ma’nsgrëßt, starring a central European trickster anthropomorphic red fox.\n\n\n\n\n\nAnyway, back to the point of this blog post. How can we get data from a pdf where no text recognition was performed (or, how can we get text from an image)? The pdf we need the text from looks like this:\n\n\n\n\n\nTo get the text from the pdf, we can use the {tesseract} package, which provides bindings to the tesseract program. tesseract is an open source OCR engine developed by Google. This means that first you will need to install the tesseract program on your system. You can follow the intructions from tesseract’s github page. tesseract is currently at version 4.\n\n\nBefore applying OCR to a pdf, let’s first use the {pdftools} package to convert the pdf to png. This is because {tesseract} requires images as input (if you provide a pdf file, it will converted on the fly). Let’s first load the needed packages:\n\nlibrary(tidyverse)\nlibrary(tesseract)\nlibrary(pdftools)\nlibrary(magick)\n\nAnd now let’s convert the pdf to png files (in plural, because we’ll get one image per page of the pdf):\n\npngfile &lt;- pdftools::pdf_convert(\"path/to/pdf\", dpi = 600)\n\nThis will generate 14 png files. I erase the ones that are not needed, such as the title page. Now, let’s read in all the image files:\n\npath &lt;- dir(path = \"path/to/pngs\", pattern = \"*.png\", full.names = TRUE)\n\nimages &lt;- map(path, magick::image_read)\n\nThe images object is a list of magick-images, which we can parse. BUUUUUT! There’s a problem. The text is laid out in two columns. Which means that the first line after performing OCR will be the first line of the first column, and the first line of the second column joined together. Same for the other lines of course. So ideally, I’d need to split the file in the middle, and then perform OCR. This is easily done with the {magick} package:\n\nfirst_half &lt;- map(images, ~image_crop(., geometry = \"2307x6462\"))\n\nsecond_half &lt;- map(images, ~image_crop(., geometry = \"2307x6462+2307+0\"))\n\nBecause the pngs are 4614 by 6962 pixels, I can get the first half of the png by cropping at “2307x6462” (I decrease the height a bit to get rid of the page number), and the second half by applying the same logic, but starting the cropping at the “2307+0” position. The result looks like this:\n\n\n\n\n\nMuch better! Now I need to join these two lists together. I cannot simply join them. Consider the following example:\n\none &lt;- list(1, 3, 5)\n\ntwo &lt;- list(2, 4, 6)\n\nThis is the setup I currently have; first_half contains odd pages, and second_half contains even pages. The result I want would look like this:\n\nlist(1, 2, 3, 4, 5, 6)\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 2\n## \n## [[3]]\n## [1] 3\n## \n## [[4]]\n## [1] 4\n## \n## [[5]]\n## [1] 5\n## \n## [[6]]\n## [1] 6\n\nThere is a very elegant solution, with reduce2() from the {purrr} package. reduce() takes one list and a function, and … reduces the list to a single element. For instance:\n\nreduce(list(1, 2, 3), paste)\n## [1] \"1 2 3\"\n\nreduce2() is very similar, but takes in two lists, but the second list must be one element shorter:\n\nreduce2(list(1, 2, 3), list(\"a\", \"b\"), paste)\n## [1] \"1 2 a 3 b\"\n\nSo we cannot simply use reduce2() on lists one and two, because they’re the same length. So let’s prepend a value to one, using the prepend() function of {purrr}:\n\nprepend(one, 0) %&gt;% \n    reduce2(two, c)\n## [1] 0 1 2 3 4 5 6\n\nExactly what we need! Let’s apply this trick to our lists:\n\nmerged_list &lt;- prepend(first_half, NA) %&gt;% \n    reduce2(second_half, c) %&gt;% \n    discard(is.na)\n\nI’ve prepended NA to the first list, and then used reduce2() and then used discard(is.na) to remove the NA I’ve added at the start. Now, we can use OCR to get the text:\n\ntext_list &lt;- map(merged_list, ocr)\n\nocr() uses a model trained on English by default, and even though there is a model trained on Luxembourguish, the one trained on English works better! Very likely because the English model was trained on a lot more data than the Luxembourguish one. I was worried the English model was not going to recognize characters such as é, but no, it worked quite well.\n\n\nThis is how it looks like:\n\ntext_list\n\n[[1]]\n[1] \"Lhe\\n| Kaum huet d’Feld dat fréndlecht Feier\\nVun der Aussentssonn gesunn\\nAs mam Plou aus Stall a Scheier\\n* D’lescht e Bauer ausgezunn.\\nFir de Plou em nach ze dreiwen\\nWar sai Jéngelchen alaert,\\nDeen nét wéllt doheem méi bleiwen\\n8 An esouz um viischte Paerd.\\nOp der Schéllche stoung ze denken\\nD’Léierche mam Hierz voll Lidder\\nFir de Béifchen nach ze zanken\\n12 Duckelt s’an de Som sech nidder.\\nBis e laascht war, an du stémmt se\\nUn e Liddchen, datt et kraacht\\nOp der Nouteleder klémmt se\\n16 Datt dem Béifchen d’Haerz alt laacht.\\nAn du sot en: Papp, ech mengen\\nBal de Vull dee kénnt och schwatzen.\\nLauschter, sot de Papp zum Klengen,\\n20 Ech kann d’Liddchen iwersetzen.\\nI\\nBas de do, mii léiwe Fréndchen\\nMa de Wanter dee war laang!\\nKuck, ech hat keng fréilech Sténnchen\\n24 *T war fir dech a mech mer baang.\\nAn du koum ech dech besichen\\nWell du goungs nét méi eraus\\nMann wat hues jo du eng Kichen\\n28 Wat eng Scheier wat en Haus.\\nWi zerguttster, a wat Saachen!\\nAn déng Frache gouf mer Brout.\\nAn déng Kanner, wi se laachen,\\n32, An hir Backelcher, wi rout!\\nJo, bei dir as Rot nét deier!\\nJo a kuck mer wat eng Méscht.\\nDat gét Saache fir an d’Scheier\\n36 An och Sué fir an d’Késcht.\\nMuerges waars de schuns um Dreschen\\nIr der Daudes d’Schung sech stréckt\\nBas am Do duurch Wis a Paschen\\n40 Laascht all Waassergruef geschréckt.\\n\"\n....\n....\n\nWe still need to split at the “” character:\n\ntext_list &lt;- text_list %&gt;% \n    map(., ~str_split(., \"\\n\"))\n\nThe end result:\n\ntext_list\n\n[[1]]\n[[1]][[1]]\n [1] \"Lhe\"                                      \"| Kaum huet d’Feld dat fréndlecht Feier\" \n [3] \"Vun der Aussentssonn gesunn\"              \"As mam Plou aus Stall a Scheier\"         \n [5] \"* D’lescht e Bauer ausgezunn.\"            \"Fir de Plou em nach ze dreiwen\"          \n [7] \"War sai Jéngelchen alaert,\"               \"Deen nét wéllt doheem méi bleiwen\"       \n [9] \"8 An esouz um viischte Paerd.\"            \"Op der Schéllche stoung ze denken\"       \n[11] \"D’Léierche mam Hierz voll Lidder\"         \"Fir de Béifchen nach ze zanken\"          \n[13] \"12 Duckelt s’an de Som sech nidder.\"      \"Bis e laascht war, an du stémmt se\"      \n[15] \"Un e Liddchen, datt et kraacht\"           \"Op der Nouteleder klémmt se\"             \n[17] \"16 Datt dem Béifchen d’Haerz alt laacht.\" \"An du sot en: Papp, ech mengen\"          \n[19] \"Bal de Vull dee kénnt och schwatzen.\"     \"Lauschter, sot de Papp zum Klengen,\"     \n[21] \"20 Ech kann d’Liddchen iwersetzen.\"       \"I\"                                       \n[23] \"Bas de do, mii léiwe Fréndchen\"           \"Ma de Wanter dee war laang!\"             \n[25] \"Kuck, ech hat keng fréilech Sténnchen\"    \"24 *T war fir dech a mech mer baang.\"    \n[27] \"An du koum ech dech besichen\"             \"Well du goungs nét méi eraus\"            \n[29] \"Mann wat hues jo du eng Kichen\"           \"28 Wat eng Scheier wat en Haus.\"         \n[31] \"Wi zerguttster, a wat Saachen!\"           \"An déng Frache gouf mer Brout.\"          \n[33] \"An déng Kanner, wi se laachen,\"           \"32, An hir Backelcher, wi rout!\"         \n[35] \"Jo, bei dir as Rot nét deier!\"            \"Jo a kuck mer wat eng Méscht.\"           \n[37] \"Dat gét Saache fir an d’Scheier\"          \"36 An och Sué fir an d’Késcht.\"          \n[39] \"Muerges waars de schuns um Dreschen\"      \"Ir der Daudes d’Schung sech stréckt\"     \n[41] \"Bas am Do duurch Wis a Paschen\"           \"40 Laascht all Waassergruef geschréckt.\" \n[43] \"\"  \n...\n...\n\nPerfect! Some more cleaning would be needed though. For example, I need to remove the little annotations that are included:\n\n\n\n\n\nI don’t know yet how I’m going to do that.I also need to remove the line numbers at the beginning of every fourth line, but this is easily done with a simple regular expression:\n\nstr_remove_all(c(\"12 bla\", \"blb\", \"123 blc\"), \"^\\\\d{1,}\\\\s+\")\n## [1] \"bla\" \"blb\" \"blc\"\n\nBut this will be left for a future blog post!"
  },
  {
    "objectID": "posts/2018-09-08-steam_linux.html",
    "href": "posts/2018-09-08-steam_linux.html",
    "title": "The year of the GNU+Linux desktop is upon us: using user ratings of Steam Play compatibility to play around with regex and the tidyverse",
    "section": "",
    "text": "I’ve been using GNU+Linux distros for about 10 years now, and have settled for openSUSE as my main operating system around 3 years ago, perhaps even more. If you’re a gamer, you might have heard about SteamOS and how more and more games are available on GNU+Linux. I don’t really care about games, I play the occasional one (currently Tangledeep) when I find the time, but still follow the news about gaming on GNU+Linux. Last week, Valve announced something quite big; it is now possible to run Windows games on GNU+Linux directly from Steam, using a modified version of Wine they call Proton. The feature is still in Beta, and Valve announced that they guarantee around 30 games to work already flawlessly. Of course, people have tried running a lot of other games, and, as was to be expected from Free Software and Open Source fans, GNU+Linux gamers created a Google Sheet that lists which games were tried and how they run. You can take a look at the sheet here.\n\n\nIn this blog post, I will play around with this sheet. This blog post lists some {tidyverse} tricks I find useful and use often. Perhaps these tricks will be useful to you too! Let’s start by loading the needed packages:\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(readxl)\n\nSince I’m lazy and don’t want to type the whole name of the file I’ll be using some little regex:\n\nsteam &lt;- read_excel(Sys.glob(\"Steam*\"), sheet = \"Main\", skip = 2)\n\nglimpse(steam)\n## Observations: 8,570\n## Variables: 9\n## $ SteamDB   &lt;chr&gt; \"LINK\", \"LINK\", \"LINK\", \"LINK\", \"LINK\", \"LINK\", \"LINK\"…\n## $ Game      &lt;chr&gt; \"64\", \"1849\", \"1982\", \"1982\", \"am Weapon: Revival\", \".…\n## $ Submitted &lt;chr&gt; \"5 days ago\", \"12 days ago\", \"11 days ago\", \"11 days a…\n## $ Status    &lt;chr&gt; \"Garbage\", \"Platinum\", \"Gold\", \"Platinum\", \"Platinum\",…\n## $ Notes     &lt;chr&gt; \"Crashes with a debug log\", \"Plays OK.\", \"Gamepad supp…\n## $ Distro    &lt;chr&gt; \"Arch (4.18.5)\", \"Manjaro XFCE\", \"Gentoo AMD64 (Kernel…\n## $ Driver    &lt;chr&gt; \"Nvidia 396.54 / Intel xf86-video-intel (1:2.99.917+83…\n## $ Specs     &lt;chr&gt; \"Intel Core i7-7700HQ / Nvidia GTX 1050 (Mobile)\", \"Ry…\n## $ Proton    &lt;chr&gt; \"3.7 Beta\", \"3.7-4 Beta\", \"3.7-4 Beta\", \"Default\", \"3.…\n\nLet’s count how many unique games are in the data:\n\nsteam %&gt;%\n    count(Game)\n## # A tibble: 3,855 x 2\n##    Game                                                                   n\n##    &lt;chr&gt;                                                              &lt;int&gt;\n##  1 .hack//G.U. Last Recode                                                2\n##  2 $1 Ride                                                                1\n##  3 0rbitalis                                                              1\n##  4 10 Second Ninja                                                        4\n##  5 100% Orange Juice                                                     17\n##  6 1000 Amps                                                              3\n##  7 12 Labours of Hercules VII: Fleecing the Fleece (Platinum Edition)     1\n##  8 16bit trader                                                           1\n##  9 1849                                                                   1\n## 10 1953 - KGB Unleased                                                    1\n## # … with 3,845 more rows\n\nThat’s quite a lot of games! However, not everyone of them is playable:\n\nsteam %&gt;%\n    count(Status)\n## # A tibble: 8 x 2\n##   Status       n\n##   &lt;chr&gt;    &lt;int&gt;\n## 1 Borked     205\n## 2 bronze       1\n## 3 Bronze     423\n## 4 Garbage   2705\n## 5 Gold       969\n## 6 Platinum  2596\n## 7 Primary      1\n## 8 Silver    1670\n\nAround 2500 have the status “Platinum”, but some games might have more than one status:\n\nsteam %&gt;%\n    filter(Game == \"100% Orange Juice\") %&gt;%\n    count(Status)\n## # A tibble: 5 x 2\n##   Status       n\n##   &lt;chr&gt;    &lt;int&gt;\n## 1 Bronze       5\n## 2 Garbage      3\n## 3 Gold         2\n## 4 Platinum     6\n## 5 Silver       1\n\nMore games run like Garbage than Platinum. But perhaps we can dig a little deeper and see if we find some patterns.\n\n\nLet’s take a look at the GNU+Linux distros:\n\nsteam %&gt;%\n    count(Distro) \n## # A tibble: 2,085 x 2\n##    Distro                                         n\n##    &lt;chr&gt;                                      &lt;int&gt;\n##  1 &lt;NA&gt;                                           1\n##  2 ?                                              2\n##  3 \"\\\"Arch Linux\\\" (64 bit)\"                      1\n##  4 \"\\\"Linux Mint 18.3 Sylvia 64bit\"               1\n##  5 \"\\\"Manjaro Stable 64-bit (Kernel 4.14.66)\"     1\n##  6 \"\\\"Solus\\\" (64 bit)\"                           2\n##  7 (K)ubuntu 18.04 64-bit (Kernel 4.15.0)         2\n##  8 (L)Ubuntu 18.04.1 LTS                          1\n##  9 18.04.1                                        1\n## 10 18.04.1 LTS                                    2\n## # … with 2,075 more rows\n\nOk the distro column is pretty messy. Let’s try to bring some order to it:\n\nsteam %&lt;&gt;%\n    mutate(distribution = as_factor(case_when(\n        grepl(\"buntu|lementary|antergos|steam|mint|18.|pop|neon\", Distro, ignore.case = TRUE) ~ \"Ubuntu\",\n        grepl(\"arch|manjaro\", Distro, ignore.case = TRUE) ~ \"Arch Linux\",\n        grepl(\"gentoo\", Distro, ignore.case = TRUE) ~ \"Gentoo\",\n        grepl(\"fedora\", Distro, ignore.case = TRUE) ~ \"Fedora\",\n        grepl(\"suse\", Distro, ignore.case = TRUE) ~ \"openSUSE\",\n        grepl(\"debian|sid|stretch|lmde\", Distro, ignore.case = TRUE) ~ \"Debian\",\n        grepl(\"solus\", Distro, ignore.case = TRUE) ~ \"Solus\",\n        grepl(\"slackware\", Distro, ignore.case = TRUE) ~ \"Slackware\",\n        grepl(\"void\", Distro, ignore.case = TRUE) ~ \"Void Linux\",\n        TRUE ~ \"Other\"\n    )))\n\nThe %&lt;&gt;% operator is shorthand for a &lt;- a %&gt;% f(). It passes a to f() and assigns the result back to a. Anyways, let’s take a look at the distribution column:\n\nsteam %&gt;%\n    count(distribution)\n## # A tibble: 10 x 2\n##    distribution     n\n##    &lt;fct&gt;        &lt;int&gt;\n##  1 Ubuntu        6632\n##  2 Arch Linux     805\n##  3 Solus          175\n##  4 Debian         359\n##  5 Fedora         355\n##  6 Gentoo          42\n##  7 Void Linux      38\n##  8 Other           76\n##  9 openSUSE        66\n## 10 Slackware       22\n\nI will group distributions that have less than 100 occurrences into a single category (meaning I will keep the 5 more common values):\n\nsteam %&lt;&gt;%\n    mutate(distribution = fct_lump(distribution, n = 5, other_level = \"Other\")) \n\nsteam %&gt;%\n    count(distribution)\n## # A tibble: 6 x 2\n##   distribution     n\n##   &lt;fct&gt;        &lt;int&gt;\n## 1 Ubuntu        6632\n## 2 Arch Linux     805\n## 3 Solus          175\n## 4 Debian         359\n## 5 Fedora         355\n## 6 Other          244\n\nLet’s do the same for the CPUs:\n\nsteam %&lt;&gt;%\n    mutate(CPU = as_factor(case_when(\n        grepl(\"intel|i\\\\d|xeon|core2|\\\\d{4}k|q\\\\d{4}|pentium\", Specs, ignore.case = TRUE) ~ \"Intel\",\n        grepl(\"ryzen|threadripper|tr|amd|fx|r\\\\d|\\\\d{4}x|phenom\", Specs, ignore.case = TRUE) ~ \"AMD\",\n        TRUE ~ NA_character_\n    )))\n\nsteam %&gt;%\n    count(CPU)\n## Warning: Factor `CPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n## # A tibble: 3 x 2\n##   CPU       n\n##   &lt;fct&gt; &lt;int&gt;\n## 1 Intel  5768\n## 2 AMD    2319\n## 3 &lt;NA&gt;    483\n\nAnd the same for the GPUs:\n\nsteam %&lt;&gt;%\n    mutate(GPU = as_factor(case_when(\n        grepl(\"nvidia|geforce|3\\\\d{2}|nouveau|gtx|gt\\\\s?\\\\d{1,}|9\\\\d0|1060|1070|1080\", Specs, ignore.case = TRUE) ~ \"Nvidia\",\n        grepl(\"amd|radeon|ati|rx|vega|r9\", Specs, ignore.case = TRUE) ~ \"AMD\",\n        grepl(\"intel|igpu|integrated|hd\\\\d{4}|hd\\\\sgraphics\", Specs, ignore.case = TRUE) ~ \"Intel\",\n        TRUE ~ NA_character_\n    )))\n\nsteam %&gt;%\n    count(GPU)\n## Warning: Factor `GPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n## # A tibble: 4 x 2\n##   GPU        n\n##   &lt;fct&gt;  &lt;int&gt;\n## 1 Nvidia  6086\n## 2 AMD     1374\n## 3 Intel    413\n## 4 &lt;NA&gt;     697\n\nI will also add a rank for the Status column:\n\nsteam %&lt;&gt;%\n    mutate(rank_status = case_when(\n        Status == \"Platinum\" ~ 5,\n        Status == \"Gold\" ~ 4,\n        Status == \"Silver\" ~ 3,\n        Status == \"Bronze\" ~ 2,\n        Status == \"Garbage\" ~ 1\n    ))\n\nNow, what are the top 5 most frequent combinations of Status, distribution, CPU and GPU?\n\nsteam %&gt;%\n    filter(!is.na(CPU), !is.na(GPU)) %&gt;%\n    count(Status, distribution, CPU, GPU) %&gt;%\n    mutate(total = sum(n)) %&gt;%\n    mutate(freq = n / total) %&gt;%\n    top_n(5)\n## Selecting by freq\n## # A tibble: 5 x 7\n##   Status   distribution CPU   GPU        n total   freq\n##   &lt;chr&gt;    &lt;fct&gt;        &lt;fct&gt; &lt;fct&gt;  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n## 1 Garbage  Ubuntu       Intel Nvidia  1025  7443 0.138 \n## 2 Gold     Ubuntu       Intel Nvidia   361  7443 0.0485\n## 3 Platinum Ubuntu       Intel Nvidia  1046  7443 0.141 \n## 4 Platinum Ubuntu       AMD   Nvidia   338  7443 0.0454\n## 5 Silver   Ubuntu       Intel Nvidia   650  7443 0.0873\n\nUnsurprisingly, Ubuntu, or distributions using Ubuntu as a base, are the most popular ones. Nvidia is the most popular GPU, Intel for CPUs and in most cases, this combo of hardware and distribution is associated with positive ratings (even though there are almost as many “Garbage” ratings than “Platinum” ratings).\n\n\nNow let’s compute some dumb averages of Statuses by distribution, CPU and GPU. Since I’m going to run the same computation three times, I’ll write a function to do that.\n\ncompute_avg &lt;- function(dataset, var){\n    var &lt;- enquo(var)\n    dataset %&gt;%\n        select(rank_status, (!!var)) %&gt;%\n        group_by((!!var)) %&gt;%\n        mutate(wt = n()) %&gt;%\n        summarise(average_rating = weighted.mean(rank_status, (!!var), wt, na.rm = TRUE))\n}\n\nLet’s see now if we can rank distribution by Steam play rating:\n\ncompute_avg(steam, distribution)\n## # A tibble: 6 x 2\n##   distribution average_rating\n##   &lt;fct&gt;                 &lt;dbl&gt;\n## 1 Ubuntu                 3.03\n## 2 Arch Linux             3.05\n## 3 Solus                  3.03\n## 4 Debian                 3.01\n## 5 Fedora                 3.07\n## 6 Other                  3.16\n\nHow about for hardware?\n\ncompute_avg(steam, GPU)\n## Warning: Factor `GPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n\n## Warning: Factor `GPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n## # A tibble: 4 x 2\n##   GPU    average_rating\n##   &lt;fct&gt;           &lt;dbl&gt;\n## 1 Nvidia           3.07\n## 2 AMD              2.90\n## 3 Intel            3.01\n## 4 &lt;NA&gt;            NA\ncompute_avg(steam, CPU)\n## Warning: Factor `CPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n\n## Warning: Factor `CPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n## # A tibble: 3 x 2\n##   CPU   average_rating\n##   &lt;fct&gt;          &lt;dbl&gt;\n## 1 Intel           3.03\n## 2 AMD             3.06\n## 3 &lt;NA&gt;           NA\n\nTo wrap this up, what are the games with the most ratings? Perhaps this can give us a hint about which games GNU+Linux users prefer:\n\nsteam %&gt;%\n    count(Game) %&gt;%\n    top_n(10)\n## Selecting by n\n## # A tibble: 10 x 2\n##    Game                              n\n##    &lt;chr&gt;                         &lt;int&gt;\n##  1 Age of Empires II: HD Edition    43\n##  2 Borderlands                      39\n##  3 DiRT 3 Complete Edition          32\n##  4 DOOM                             62\n##  5 Fallout: New Vegas               45\n##  6 Grim Dawn                        34\n##  7 No Man's Sky                     40\n##  8 Path of Exile                    35\n##  9 Quake Champions                  32\n## 10 The Elder Scrolls V: Skyrim      46\n\nI actually laughed out loud when I saw that DOOM was the game with the most ratings! What else was I expecting, really."
  },
  {
    "objectID": "posts/2018-12-21-tidyverse_pi.html",
    "href": "posts/2018-12-21-tidyverse_pi.html",
    "title": "Using the tidyverse for more than data manipulation: estimating pi with Monte Carlo methods",
    "section": "",
    "text": "This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free here. This is taken from Chapter 5, which presents the {tidyverse} packages and how to use them to compute descriptive statistics and manipulate data. In the text below, I show how you can use the {tidyverse} functions and principles for the estimation of () using Monte Carlo simulation."
  },
  {
    "objectID": "posts/2018-12-21-tidyverse_pi.html#going-beyond-descriptive-statistics-and-data-manipulation",
    "href": "posts/2018-12-21-tidyverse_pi.html#going-beyond-descriptive-statistics-and-data-manipulation",
    "title": "Using the tidyverse for more than data manipulation: estimating pi with Monte Carlo methods",
    "section": "\nGoing beyond descriptive statistics and data manipulation\n",
    "text": "Going beyond descriptive statistics and data manipulation\n\n\nThe {tidyverse} collection of packages can do much more than simply data manipulation and descriptive statisics. You can use the principles we have covered and the functions you now know to do much more. For instance, you can use a few {tidyverse} functions to do Monte Carlo simulations, for example to estimate ().\n\n\nDraw the unit circle inside the unit square, the ratio of the area of the circle to the area of the square will be (/4). Then shot K arrows at the square; roughly (K*/4) should have fallen inside the circle. So if now you shoot N arrows at the square, and M fall inside the circle, you have the following relationship (M = N/4). You can thus compute () like so: (= 4M/N).\n\n\nThe more arrows N you throw at the square, the better approximation of () you’ll have. Let’s try to do this with a tidy Monte Carlo simulation. First, let’s randomly pick some points inside the unit square:\n\nlibrary(tidyverse)\nlibrary(brotools)\nn &lt;- 5000\n\nset.seed(2019)\npoints &lt;- tibble(\"x\" = runif(n), \"y\" = runif(n))\n\nNow, to know if a point is inside the unit circle, we need to check wether (x^2 + y^2 &lt; 1). Let’s add a new column to the points tibble, called inside equal to 1 if the point is inside the unit circle and 0 if not:\n\npoints &lt;- points %&gt;% \n    mutate(inside = map2_dbl(.x = x, .y = y, ~ifelse(.x**2 + .y**2 &lt; 1, 1, 0))) %&gt;% \n    rowid_to_column(\"N\")\n\nLet’s take a look at points:\n\npoints\n## # A tibble: 5,000 x 4\n##        N       x      y inside\n##    &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n##  1     1 0.770   0.984       0\n##  2     2 0.713   0.0107      1\n##  3     3 0.303   0.133       1\n##  4     4 0.618   0.0378      1\n##  5     5 0.0505  0.677       1\n##  6     6 0.0432  0.0846      1\n##  7     7 0.820   0.727       0\n##  8     8 0.00961 0.0758      1\n##  9     9 0.102   0.373       1\n## 10    10 0.609   0.676       1\n## # … with 4,990 more rows\n\nThe rowid_to_column() function, from the {tibble} package, adds a new column to the data frame with an id, going from 1 to the number of rows in the data frame. Now, I can compute the estimation of () at each row, by computing the cumulative sum of the 1’s in the inside column and dividing that by the current value of N column:\n\npoints &lt;- points %&gt;% \n    mutate(estimate = 4*cumsum(inside)/N)\n\ncumsum(inside) is the M from the formula. Now, we can finish by plotting the result:\n\nggplot(points) + \n    geom_line(aes(y = estimate, x = N), colour = \"#82518c\") + \n    geom_hline(yintercept = pi) +\n    theme_blog()\n\n\n\n\nIn Chapter 6, we are going to learn all about {ggplot2}.\n\n\nAs the number of tries grows, the estimation of () gets better.\n\n\nUsing a data frame as a structure to hold our simulated points and the results makes it very easy to avoid loops, and thus write code that is more concise and easier to follow. If you studied a quantitative field in u8niversity, you might have done a similar exercise at the time, very likely by defining a matrix to hold your points, and an empty vector to hold whether a particular point was inside the unit circle. Then you wrote a loop to compute whether a point was inside the unit circle, save this result in the before-defined empty vector and then compute the estimation of (). Again, I take this opportunity here to stress that there is nothing wrong with this approach per se, but R, with the {tidyverse} is better suited for a workflow where lists or data frames are the central objects and where the analyst operates over them with functional programming techniques."
  },
  {
    "objectID": "posts/2022-04-04-chron_post.html",
    "href": "posts/2022-04-04-chron_post.html",
    "title": "The {chronicler} package, an implementation of the logger monad in R",
    "section": "",
    "text": "Back in February I discussed a package I was working on which allowed users to add logging to function calls. I named the package {loudly} but decided to rename it to {chronicler}.\nI have been working on it for the past few weeks, and I think that a CRAN release could happen soon."
  },
  {
    "objectID": "posts/2022-04-04-chron_post.html#introduction",
    "href": "posts/2022-04-04-chron_post.html#introduction",
    "title": "The {chronicler} package, an implementation of the logger monad in R",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nSo what does {chronicler} do? {chronicler} allows you do decorate functions, so that they provide enhanced output:\n\nlibrary(chronicler)\n## Loading required package: rlang\nr_sqrt &lt;- record(sqrt)\n\na &lt;- r_sqrt(1:5)\n\nObject a is now an object of class chronicle. Let’s print a to the terminal:\n\na\n## ✔ Value computed successfully:\n## ---------------\n## [1] 1.000000 1.414214 1.732051 2.000000 2.236068\n## \n## ---------------\n## This is an object of type `chronicle`.\n## Retrieve the value of this object with pick(.c, \"value\").\n## To read the log of this object, call read_log().\n\nas the output says, a is an object of type chronicle. Let’s use read_log() as suggested:\n\nread_log(a)\n## [1] \"Complete log:\"                                      \n## [2] \"✔ sqrt(1:5) ran successfully at 2022-04-01 21:14:28\"\n## [3] \"Total running time: 0.000240325927734375 secs\"\n\nThe log tells us how a was built, and it’s especially useful for objects that are the result of many function calls:\n\nr_sqrt &lt;- record(sqrt)\nr_exp &lt;- record(exp)\nr_mean &lt;- record(mean)\n\nb &lt;- 1:10 |&gt;\n  r_sqrt() |&gt;\n  bind_record(r_exp) |&gt;\n  bind_record(r_mean)\n\nThe log gives all the details:\n\nread_log(b)\n## [1] \"Complete log:\"                                           \n## [2] \"✔ sqrt(1:10) ran successfully at 2022-04-01 21:14:28\"    \n## [3] \"✔ exp(.c$value) ran successfully at 2022-04-01 21:14:28\" \n## [4] \"✔ mean(.c$value) ran successfully at 2022-04-01 21:14:28\"\n## [5] \"Total running time: 0.00820255279541016 secs\"\n\nThe end result, or what is called value can be obtained using pick() (you could also use a$value):\n\npick(a, \"value\")\n## [1] 1.000000 1.414214 1.732051 2.000000 2.236068\npick(b, \"value\")\n## [1] 11.55345"
  },
  {
    "objectID": "posts/2022-04-04-chron_post.html#composing-decorated-functions",
    "href": "posts/2022-04-04-chron_post.html#composing-decorated-functions",
    "title": "The {chronicler} package, an implementation of the logger monad in R",
    "section": "\nComposing decorated functions\n",
    "text": "Composing decorated functions\n\n\nbind_record() is used to pass the output from one decorated function to the next:\n\nsuppressPackageStartupMessages(\n  library(dplyr)\n)\n\nr_group_by &lt;- record(group_by)\nr_select &lt;- record(select)\nr_summarise &lt;- record(summarise)\nr_filter &lt;- record(filter)\n\noutput &lt;- starwars %&gt;%\n  r_select(height, mass, species, sex) %&gt;%\n  bind_record(r_group_by, species, sex) %&gt;%\n  bind_record(r_filter, sex != \"male\") %&gt;%\n  bind_record(r_summarise,\n              mass = mean(mass, na.rm = TRUE)\n              )\nread_log(output)\n## [1] \"Complete log:\"                                                                         \n## [2] \"✔ select(.,height,mass,species,sex) ran successfully at 2022-04-01 21:14:28\"           \n## [3] \"✔ group_by(.c$value,species,sex) ran successfully at 2022-04-01 21:14:28\"              \n## [4] \"✔ filter(.c$value,sex != \\\"male\\\") ran successfully at 2022-04-01 21:14:28\"            \n## [5] \"✔ summarise(.c$value,mean(mass, na.rm = TRUE)) ran successfully at 2022-04-01 21:14:28\"\n## [6] \"Total running time: 0.11384654045105 secs\"\n\nThe value can then be saved in a new variable:\n\n(my_df &lt;- pick(output, \"value\"))\n## # A tibble: 9 × 3\n## # Groups:   species [9]\n##   species    sex              mass\n##   &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n## 1 Clawdite   female           55  \n## 2 Droid      none             69.8\n## 3 Human      female           56.3\n## 4 Hutt       hermaphroditic 1358  \n## 5 Kaminoan   female          NaN  \n## 6 Mirialan   female           53.1\n## 7 Tholothian female           50  \n## 8 Togruta    female           57  \n## 9 Twi'lek    female           55\n\nYou can save the output object with saveRDS() and share it; your colleague can then read the log to learn how the object was created.\n\n\nThis package also ships with a dedicated pipe, %&gt;=% which you can use instead of bind_record():\n\noutput_pipe &lt;- starwars %&gt;%\n  r_select(height, mass, species, sex) %&gt;=%\n  r_group_by(species, sex) %&gt;=%\n  r_filter(sex != \"male\") %&gt;=%\n  r_summarise(mass = mean(mass, na.rm = TRUE))\npick(output_pipe, \"value\")\n## # A tibble: 9 × 3\n## # Groups:   species [9]\n##   species    sex              mass\n##   &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n## 1 Clawdite   female           55  \n## 2 Droid      none             69.8\n## 3 Human      female           56.3\n## 4 Hutt       hermaphroditic 1358  \n## 5 Kaminoan   female          NaN  \n## 6 Mirialan   female           53.1\n## 7 Tholothian female           50  \n## 8 Togruta    female           57  \n## 9 Twi'lek    female           55"
  },
  {
    "objectID": "posts/2022-04-04-chron_post.html#condition-handling",
    "href": "posts/2022-04-04-chron_post.html#condition-handling",
    "title": "The {chronicler} package, an implementation of the logger monad in R",
    "section": "\nCondition handling\n",
    "text": "Condition handling\n\n\nBy default, errors and warnings get caught and composed in the log:\n\nerrord_output &lt;- starwars %&gt;%\n  r_select(height, mass, species, sex) %&gt;=%\n  r_group_by(species, sx) %&gt;=% # typo, \"sx\" instead of \"sex\"\n  r_filter(sex != \"male\") %&gt;=%\n  r_summarise(mass = mean(mass, na.rm = TRUE))\nerrord_output\n## ✖ Value computed unsuccessfully:\n## ---------------\n## [1] NA\n## \n## ---------------\n## This is an object of type `chronicle`.\n## Retrieve the value of this object with pick(.c, \"value\").\n## To read the log of this object, call read_log().\n\nReading the log tells you which function failed, and with which error message:\n\nread_log(errord_output)\n## [1] \"Complete log:\"                                                                                                                                                                                    \n## [2] \"✔ select(.,height,mass,species,sex) ran successfully at 2022-04-01 21:14:28\"                                                                                                                      \n## [3] \"✖ group_by(.c$value,species,sx) ran unsuccessfully with following exception: Must group by variables found in `.data`.\\n✖ Column `sx` is not found. at 2022-04-01 21:14:28\"                       \n## [4] \"✖ filter(.c$value,sex != \\\"male\\\") ran unsuccessfully with following exception: no applicable method for 'filter' applied to an object of class \\\"logical\\\" at 2022-04-01 21:14:28\"               \n## [5] \"✖ summarise(.c$value,mean(mass, na.rm = TRUE)) ran unsuccessfully with following exception: no applicable method for 'summarise' applied to an object of class \\\"logical\\\" at 2022-04-01 21:14:28\"\n## [6] \"Total running time: 0.163575887680054 secs\"\n\nIt is also possible to only capture errors, or catpure errors, warnings and messages using the strict parameter of record()\n\n# Only errors:\n\nr_sqrt &lt;- record(sqrt, strict = 1)\n\n# Nothing will be captured here, since sqrt(-10) returns a NA and a warning\nr_sqrt(-10) |&gt;\n  read_log()\n## Warning in .f(...): NaNs produced\n## [1] \"Complete log:\"                                                                     \n## [2] \"✖ sqrt(-10) ran unsuccessfully with following exception: NA at 2022-04-01 21:14:28\"\n## [3] \"Total running time: 0.000255584716796875 secs\"\n# Errors and warnings:\n\nr_sqrt &lt;- record(sqrt, strict = 2)\n\n# The warning gets captured\nr_sqrt(-10) |&gt;\n  read_log()\n## [1] \"Complete log:\"                                                                                \n## [2] \"✖ sqrt(-10) ran unsuccessfully with following exception: NaNs produced at 2022-04-01 21:14:28\"\n## [3] \"Total running time: 0.00019383430480957 secs\"\n# Errors, warnings and messages\n\nmy_f &lt;- function(x){\n  message(\"this is a message\")\n  10\n}\n\nrecord(my_f, strict = 3)(10) |&gt;\n                         read_log()\n## [1] \"Complete log:\"                                                                                     \n## [2] \"✖ my_f(10) ran unsuccessfully with following exception: this is a message\\n at 2022-04-01 21:14:28\"\n## [3] \"Total running time: 0.000336408615112305 secs\""
  },
  {
    "objectID": "posts/2022-04-04-chron_post.html#advanced-logging",
    "href": "posts/2022-04-04-chron_post.html#advanced-logging",
    "title": "The {chronicler} package, an implementation of the logger monad in R",
    "section": "\nAdvanced logging\n",
    "text": "Advanced logging\n\n\nYou can provide a function to record(), which will be evaluated on the output. This makes it possible to, for example, monitor the size of a data frame throughout the pipeline. In the example below I provide dim(), which will return the dimensions of the data frame, as an argument to record():\n\nr_group_by &lt;- record(group_by)\nr_select &lt;- record(select, .g = dim)\nr_summarise &lt;- record(summarise, .g = dim)\nr_filter &lt;- record(filter, .g = dim)\n\noutput_pipe &lt;- starwars %&gt;%\n  r_select(height, mass, species, sex) %&gt;=%\n  r_group_by(species, sex) %&gt;=%\n  r_filter(sex != \"male\") %&gt;=%\n  r_summarise(mass = mean(mass, na.rm = TRUE))\n\nThe $log_df element of a chronicle object contains detailed information. In most cases you don’t need to worry about it:\n\npick(output_pipe, \"log_df\")\n## # A tibble: 4 × 8\n##   outcome   `function` arguments message start_time          end_time           \n##   &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;   &lt;dttm&gt;              &lt;dttm&gt;             \n## 1 ✔ Success select     \".,heigh… NA      2022-04-01 21:14:28 2022-04-01 21:14:28\n## 2 ✔ Success group_by   \".c$valu… NA      2022-04-01 21:14:28 2022-04-01 21:14:28\n## 3 ✔ Success filter     \".c$valu… NA      2022-04-01 21:14:28 2022-04-01 21:14:29\n## 4 ✔ Success summarise  \".c$valu… NA      2022-04-01 21:14:28 2022-04-01 21:14:29\n## # … with 2 more variables: run_time &lt;drtn&gt;, g &lt;list&gt;\n\nbut if you want to look at the output of .g, then you have to grab it and see:\n\n# I coerce it to a data.frame just for the output here on my blog, to make the column `g` readable\nas.data.frame(output_pipe$log_df[, c(\"function\", \"g\")])\n##    function     g\n## 1    select 87, 4\n## 2  group_by    NA\n## 3    filter 23, 4\n## 4 summarise  9, 3\n\nWe can see that the dimension of the dataframe was (87, 4) after the call to select(), (23, 4) after the call to filter() and finally (9, 3) after the call to summarise()."
  },
  {
    "objectID": "posts/2022-04-04-chron_post.html#monads",
    "href": "posts/2022-04-04-chron_post.html#monads",
    "title": "The {chronicler} package, an implementation of the logger monad in R",
    "section": "\nMonads\n",
    "text": "Monads\n\n\nThis package implements a logger monad. I might talk about monads in the future, but probably in a video; if you don’t know what monads are, don’t worry, no one really knows. Legend has it that to truly understand what monads are you have to have a PhD in computer science and have been born in the former Soviet Union. But to make things simple, you can think of a monad as a way to:\n\n\n\nembelish functions to provide additional output without having to touch the function’s core behaviour\n\n\na way to compose these functions and work with the embelished outputs (also called monadic values)\n\n\nmonadic values are basically containers that contain the actual value of the function evaluated on its inputs and something else (here, a log)\n\n\n\nMonads are quite useful in some programming languanges, like Haskell. Not so much in R, but I think that the logger monad I propose here can be quite useful. So let me know if you find it useful or if you have suggestions!\n\n\nYou can install {chronicler} from its github repo."
  },
  {
    "objectID": "posts/2013-12-31-r-cas.html",
    "href": "posts/2013-12-31-r-cas.html",
    "title": "Using R as a Computer Algebra System with Ryacas",
    "section": "",
    "text": "R is used to perform statistical analysis and doesn't focus on symbolic maths. But it is sometimes useful to let the computer derive a function for you (and have the analytic expression of said derivative), but maybe you don't want to leave your comfy R shell. It is possible to turn R into a full-fledged computer algebra system. CASs are tools that perform symbolic operations, such as getting the expression of the derivative of a user-defined (and thus completely arbitrary) function. Popular CASs include the proprietary Mathematica and Maple. There exists a lot of CASs under a Free Software license, Maxima (based on the very old Macsyma), Yacas, Xcas… In this post I will focus on Yacas and the Ryacas libarary. There is also the possibility to use the rSympy library that uses the Sympy Python library, which has a lot more features than Yacas. However, depending on your operating system installation can be tricky as it also requires rJava as a dependency.\n\n\nEven though Ryacas is quite nice to have, there are some issues though. For example, let's say you want the first derivative of a certain function f. If you use Ryacas to get it, the returned object won't be a function. There is a way to “extract” the text from the returned object and make a function out of it. But there are still other issues; I'll discuss them later.\n\n\nInstallation\n\n\nInstallation should be rather painless. On Linux you need to install Yacas first, which should be available in the major distros' repositories. Then you can install Ryacas from within the R shell. On Windows, you need to run these three commands (don't bother installing Yacas first):\n\n\ninstall.packages(Ryacas)\nlibrary(Ryacas)\nyacasInstall()\n\n\nYou can find more information on the project's page.\n\n\nExample session\n\n\nFirst, you must load Ryacas and define symbols that you will use in your functions.\n\n\nlibrary(Ryacas)\n\n## Loading required package: Ryacas Loading required package: XML\n\n\nx &lt;- Sym(\"x\")\n\n\nYou can then define your fonctions:\n\n\nmy_func &lt;- function(x) {\n  return(x/(x^2 + 3))\n}\n\n\nAnd you can get the derivative for instance:\n\n\nmy_deriv &lt;- yacas(deriv(my_func(x), x))\n\n## [1] \"Starting Yacas!\"\n\n\nIf you check the class of my_deriv, you'll see that it is of class yacas, which is not very useful. Let's «convert» it to a function:\n\n\nmy_deriv2 &lt;- function(x) {\n  eval(parse(text = my_deriv$YacasForm))\n}\n\n\nWe can then evaluate it. A lot of different operations are possible. But there are some problems.\n\n\nIssues with Ryacas\n\n\nYou can't use elements of a vector as parameters of your function, i.e.:\n\n\ntheta &lt;- Sym(\"theta\")\nfunc &lt;- function(x) {\n  return(theta[1] * x + theta[2])\n}\n\n\nLet's integrate this\nFunc &lt;- yacas(Integrate(func(x), x)) \n\n\nreturns (x^2theta)/2+NAx; which is not quite what we want…there is a workaround however. Define your functions like this:\n\n\na &lt;- Sym(\"a\")\nb &lt;- Sym(\"b\")\nfunc2 &lt;- function(x) {\n  return(a * x + b)\n}\n\n# Let&#39;s integrate this\nFunc2 &lt;- yacas(Integrate(func2(x), x))\n\n\nwe get the expected result: (x^2a)/2+bx;. Now replace a and b by the thetas:\n\n\nFunc2 &lt;- gsub(\"a\", \"theta[1]\", Func2$YacasForm)\nFunc2 &lt;- gsub(\"b\", \"theta[2]\", Func2)\n\n\nNow we have what we want:\n\n\nFunc2\n\n## [1] \"(x^2*theta[1])/2+theta[2]*x;\"\n\n\nYou can then copy-paste this result into a function.\n\n\nAnother problem is if you use built-in functions that are different between R and Yacas. For example:\n\n\nmy_log &lt;- function(x) {\n    return(sin(log(2 + x)))\n}\n\n\nNow try to differentiate it:\n\n\ndmy_log &lt;- yacas(deriv(my_log(x), x))\n\n\nyou get: Cos(Ln(x+2))/(x+2);. The problem with this, is that R doesn't recognize Cos as the cosine (which is cos in R) and the same goes for Ln. These are valid Yacas functions, but that is not the case in R. So you'll have to use gsub to replace these functions and then copy paste the end result into a function.\n\n\nConclusion\n\n\nWhile it has some flaws, Ryacas can be quite useful if you need to derive or integrate complicated expressions that you then want to use in R. Using some of the tricks I showed here, you should be able to overcome some of its shortcomings. If installation of rJava and thus rSympy becomes easier, I'll probably also do a short blog-post about it, as it has more features than Ryacas."
  },
  {
    "objectID": "posts/2016-11-04-ive-started-writing-a-book-functional-programming-and-unit-testing-for-data-munging-with-r.html",
    "href": "posts/2016-11-04-ive-started-writing-a-book-functional-programming-and-unit-testing-for-data-munging-with-r.html",
    "title": "I’ve started writing a ‘book’: Functional programming and unit testing for data munging with R",
    "section": "",
    "text": "I have started writing a ‘book’ using the awesome bookdown package. In the book I explain and show why using functional programming and putting your functions in your own packages is the way to go when you want to clean, prepare and transform large data sets. It makes testing and documenting your code easier. You don’t need to think about managing paths either. The book is far from complete, but I plan on working on it steadily. For now, you can read an intro to functional programming, unit testing and creating your own packages that will hold your code. I also show you can write documentation for your functions. I am also looking for feedback; so if you have any suggestions, do not hesitate to shoot me an email or a tweet! You can read the book by clicking here."
  },
  {
    "objectID": "posts/2022-05-15-self_doc_ggplot.html",
    "href": "posts/2022-05-15-self_doc_ggplot.html",
    "title": "Self-documenting {ggplot}s thanks to the power of monads!",
    "section": "",
    "text": "Hey kid, fancy some self-documenting {ggplots} like this one:\n\n\n\n\n\nJust read on!\n\n\nI’ve been working hard on a package that I’ve called {chronicler} (read my post on it here) which allows you to attach a log to the objects you create, thus making it easy to know how some data (for example) has been created. Here’s a quick example and intro to the main features:\n\nsuppressPackageStartupMessages(\n  library(dplyr)\n)\nlibrary(chronicler)\n\n# record() decorates functions so they provide enriched output\nr_group_by &lt;- record(group_by)\nr_select &lt;- record(select)\nr_summarise &lt;- record(summarise)\nr_filter &lt;- record(filter)\n\noutput_pipe &lt;- starwars %&gt;%\n  r_select(height, mass, species, sex) %&gt;=% # &lt;- this is a special pipe operator to handle `chronicle` objects\n  r_group_by(species, sex) %&gt;=%\n  r_filter(sex != \"male\") %&gt;=%\n  r_summarise(mass = mean(mass, na.rm = TRUE))\n\noutput_pipe not only has the result of all the {dplyr} operations, but also carries a log with it. Let’s print the object:\n\noutput_pipe\n## OK! Value computed successfully:\n## ---------------\n## Just\n## # A tibble: 9 × 3\n## # Groups:   species [9]\n##   species    sex              mass\n##   &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n## 1 Clawdite   female           55  \n## 2 Droid      none             69.8\n## 3 Human      female           56.3\n## 4 Hutt       hermaphroditic 1358  \n## 5 Kaminoan   female          NaN  \n## 6 Mirialan   female           53.1\n## 7 Tholothian female           50  \n## 8 Togruta    female           57  \n## 9 Twi'lek    female           55  \n## \n## ---------------\n## This is an object of type `chronicle`.\n## Retrieve the value of this object with pick(.c, \"value\").\n## To read the log of this object, call read_log(.c).\n\nAccessing the value is possible with pick(“value”):\n\npick(output_pipe, \"value\")\n## # A tibble: 9 × 3\n## # Groups:   species [9]\n##   species    sex              mass\n##   &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n## 1 Clawdite   female           55  \n## 2 Droid      none             69.8\n## 3 Human      female           56.3\n## 4 Hutt       hermaphroditic 1358  \n## 5 Kaminoan   female          NaN  \n## 6 Mirialan   female           53.1\n## 7 Tholothian female           50  \n## 8 Togruta    female           57  \n## 9 Twi'lek    female           55\n\nand you can read the log with read_log():\n\nread_log(output_pipe)\n## [1] \"Complete log:\"                                                                  \n## [2] \"OK! select(height,mass,species,sex) ran successfully at 2022-05-15 17:10:43\"    \n## [3] \"OK! group_by(species,sex) ran successfully at 2022-05-15 17:10:43\"              \n## [4] \"OK! filter(sex != \\\"male\\\") ran successfully at 2022-05-15 17:10:43\"            \n## [5] \"OK! summarise(mean(mass, na.rm = TRUE)) ran successfully at 2022-05-15 17:10:43\"\n## [6] \"Total running time: 0.0434844493865967 secs\"\n\nIf you want to understand how this works, I suggest you read the blog post I linked above but also this one, where I explain the nitty gritty, theoretical details behind what {chronicler} does. To make a long story short, {chronicler} uses an advanced functional programming concept called a monad. And using the power of monads, I can now make self-documenting {ggplot2} graphs.\n\n\nThe idea was to be able to build a plot in a way similar to how I built that dataset just above, and have a log of how it was created attached to it. The issue is that the function that transforms functions to chronicler functions, record(), does not work on {ggplot2} functions.\n\n\nThis is because the way you create {ggplot2} graphs is by adding layers on top of each other:\n\nlibrary(ggplot2)\n\nggplot(mtcars) +\n  geom_point(aes(mpg, hp))\n\n\n\n\nThe + here acts as a way to “add” the geom_point(mpg, hp) layer on top of the ggplot(mtcars) layer. I remember reading some tweets, quite some time ago, from people asking why %&gt;% couldn’t work with {ggplot2} and if Hadley Wickham, the developer of {ggplot2}, was considering making something like this work:\n\nggplot(mtcars) %&gt;%\n  geom_point(aes(mpg, hp))\n\nbecause people kept forgetting using + and kept using %&gt;%. The thing is, %&gt;% and + do very different things. %&gt;% takes its first argument and passes it as the first argument of its second argument, in other words this:\n\na %&gt;% f(b)\n\nis exactly the same as:\n\nf(a, b)\n\nThis is not what {ggplot2} functions do. When you call + on {ggplot2} objects, this is NOT what happens:\n\ngeom_point(ggplot(mtcars), aes(mpg, hp))\n\nSo that’s why %&gt;% cannot be used with {ggplot2} functions, and that’s also why the functions I developed in {chronicle} could not handle {ggplot2} functions either. So I had to provide new functions. The first function I developed is called ggrecord() and it decorates {ggplot2} functions:\n\nr_ggplot &lt;- ggrecord(ggplot)\nr_geom_point &lt;- ggrecord(geom_point)\nr_labs &lt;- ggrecord(labs)\n\nNow the output of these functions are not ggplot objects anymore, but chronicle objects. So to make layering them possible, I also needed to rewrite +. I called my rewritten + like this: %&gt;+%:\n\na &lt;- r_ggplot(mtcars) %&gt;+%\n  r_geom_point(aes(y = mpg, x = hp)) %&gt;+%\n  r_labs(title = \"Self-documenting ggplot!\\nLook at the bottom right\",\n         caption = \"This is an example caption\")\n\nLet’s first take a look at a:\n\na\n## OK! Ggplot computed successfully:\n## ---------------\n## Just\n\n\n\n## \n## ---------------\n## This is an object of type `chronicle`.\n## Retrieve the value of this object with pick(.c, \"value\").\n## To read the log of this object, call read_log(.c).\n\nAs before expected, a is now an object of type {chronicle}, where its “value” is a ggplot object. But where is the self-documenting part? For this, you use the last piece of the puzzle, document_gg():\n\ndocument_gg(a)\n## OK! Ggplot computed successfully:\n## ---------------\n## Just\n\n\n\n## \n## ---------------\n## This is an object of type `chronicle`.\n## Retrieve the value of this object with pick(.c, \"value\").\n## To read the log of this object, call read_log(.c).\n\nThe caption now contains the log of the plot, making it easily reproducible!\n\n\nThis is still in very early development, but if you want to try it out, you’ll need to try the dev branch of the package.\n\n\nAny feedback, comments, ideas, pull requests, more than welcome."
  },
  {
    "objectID": "posts/2020-03-08-tidymodels.html#intro-what-is-tidymodels",
    "href": "posts/2020-03-08-tidymodels.html#intro-what-is-tidymodels",
    "title": "Machine learning with {tidymodels}",
    "section": "\nIntro: what is {tidymodels}\n",
    "text": "Intro: what is {tidymodels}\n\n\nI have already written about {tidymodels} in the past but since then, the {tidymodels} meta-package has evolved quite a lot. If you don’t know what {tidymodels} is, it is a suite of packages that make machine learning with R a breeze. R has many packages for machine learning, each with their own syntax and function arguments. {tidymodels} aims at providing an unified interface which allows data scientists to focus on the problem they’re trying to solve, instead of wasting time with learning package specificities.\n\n\nThe packages included in {tidymodels} are:\n\n\n\n{parsnip} for model definition\n\n\n{recipes} for data preprocessing and feature engineering\n\n\n{rsample} to resample data (useful for cross-validation)\n\n\n{yardstick} to evaluate model performance\n\n\n{dials} to define tuning parameters of your models\n\n\n{tune} for model tuning\n\n\n{workflows} which allows you to bundle everything together and train models easily\n\n\n\nThere are some others, but I will not cover these. This is a lot of packages, and you might be worried of getting lost; however, in practice I noticed that loading {tidymodels} and then using the functions I needed was good enough. Only rarely did I need to know from which package a certain function came, and the more you use these, the better you know them, obviously. Before continuing, one final and important note: these packages are still in heavy development, so you might not want to use them in production yet. I don’t know how likely it is that the api still evolves, but my guess is that it is likely. However, even though it might be a bit early to use these packages for production code, I think it is important to learn about them as soon as possible and see what is possible with them.\n\n\nAs I will show you, these packages do make the process of training machine learning models a breeze, and of course they integrate very well with the rest of the {tidyverse} packages. The problem we’re going to tackle is to understand which variables play an important role in the probability of someone looking for a job. I’ll use Eustat’s microdata, which I already discussed in my previous blog post. The dataset can be downloaded from here, and is called Population with relation to activity (PRA)."
  },
  {
    "objectID": "posts/2020-03-08-tidymodels.html#the-problem-at-hand",
    "href": "posts/2020-03-08-tidymodels.html#the-problem-at-hand",
    "title": "Machine learning with {tidymodels}",
    "section": "\nThe problem at hand\n",
    "text": "The problem at hand\n\n\nThe dataset contains information on residents from the Basque country, and focuses on their labour supply. Thus, we have information on how many hours people work a week, if they work, in which industry, what is their educational attainment and whether they’re looking for a job. The first step, as usual, is to load the data and required packages:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(readxl)\nlibrary(naniar)\nlibrary(janitor)\nlibrary(furrr)\n\nlist_data &lt;- Sys.glob(\"~/Documents/b-rodrigues.github.com/content/blog/MICRO*.csv\")\n\ndataset &lt;- map(list_data, read_csv2) %&gt;%\n  bind_rows()\n\ndictionary &lt;- read_xlsx(\"~/Documents/b-rodrigues.github.com/content/blog/Microdatos_PRA_2019/diseño_registro_microdatos_pra.xlsx\", sheet=\"Valores\",\n                        col_names = FALSE)\n\ncol_names &lt;- dictionary %&gt;%\n  filter(!is.na(...1)) %&gt;%\n  dplyr::select(1:2)\n\nenglish &lt;- readRDS(\"~/Documents/b-rodrigues.github.com/content/blog/english_col_names.rds\")\n\ncol_names$english &lt;- english\n\ncolnames(dataset) &lt;- col_names$english\n\ndataset &lt;- janitor::clean_names(dataset)\n\nLet’s take a look at the data:\n\nhead(dataset)\n## # A tibble: 6 x 33\n##   household_number survey_year reference_quart… territory capital   sex\n##              &lt;dbl&gt;       &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n## 1                1        2019                1 48              9     6\n## 2                1        2019                1 48              9     1\n## 3                2        2019                1 48              1     1\n## 4                2        2019                1 48              1     6\n## 5                2        2019                1 48              1     6\n## 6                2        2019                1 48              1     1\n## # … with 27 more variables: place_of_birth &lt;dbl&gt;, age &lt;chr&gt;, nationality &lt;dbl&gt;,\n## #   level_of_studies_completed &lt;dbl&gt;, ruled_teaching_system &lt;chr&gt;,\n## #   occupational_training &lt;chr&gt;, retirement_situation &lt;dbl&gt;,\n## #   homework_situation &lt;dbl&gt;, part_time_employment &lt;dbl&gt;,\n## #   short_time_cause &lt;dbl&gt;, job_search &lt;chr&gt;, search_reasons &lt;dbl&gt;,\n## #   day_searched &lt;dbl&gt;, make_arrangements &lt;chr&gt;, search_form &lt;chr&gt;,\n## #   search_months &lt;dbl&gt;, availability &lt;chr&gt;,\n## #   relationship_with_the_activity &lt;dbl&gt;,\n## #   relationship_with_the_activity_2 &lt;chr&gt;, main_occupation &lt;dbl&gt;,\n## #   main_activity &lt;chr&gt;, main_professional_situation &lt;dbl&gt;,\n## #   main_institutional_sector &lt;dbl&gt;, type_of_contract &lt;dbl&gt;, hours &lt;dbl&gt;,\n## #   relationship &lt;dbl&gt;, elevator &lt;dbl&gt;\n\nThere are many columns, most of them are categorical variables and unfortunately the levels in the data are only some non-explicit codes. The excel file I have loaded, which I called dictionary contains the codes and their explanation. I kept the file opened while I was working, especially for missing values imputation. Indeed, there are missing values in the data, and one should always try to understand why before blindly imputing them. Indeed, there might be a very good reason why data might be missing for a particular column. For instance, if children are also surveyed, they would have an NA in the, say, main_occupation column which gives the main occupation of the surveyed person. This might seem very obvious, but sometimes these reasons are not so obvious at all. You should always go back with such questions to the data owners/producers, because if not, you will certainly miss something very important. Anyway, the way I tackled this issue was by looking at the variables with missing data and checking two-way tables with other variables. For instance, to go back to my example from before, I would take a look at the two-way frequency table between age and main_occupation. If all the missing values from main_occupation where only for people 16 or younger, then it would be quite safe to assume that I was right, and I could recode these NAs in main_occupation to \"without occupation\" for instance. I’ll spare you all this exploration, and go straight to the data cleaning:\n\ndataset &lt;- dataset %&gt;%\n  mutate(main_occupation2 = ifelse(is.na(main_occupation),\n                                   \"without_occupation\",\n                                   main_occupation))\n\ndataset &lt;- dataset %&gt;%\n  mutate(main_professional_situation2 = ifelse(is.na(main_professional_situation),\n                                               \"without_occupation\",\n                                               main_professional_situation))\n\n# People with missing hours are actually not working, so I put them to 0\ndataset &lt;- dataset %&gt;%\n  mutate(hours = ifelse(is.na(hours), 0, hours))\n\n# Short time gives the reason why people are working less hours than specified in their contract\ndataset &lt;- dataset %&gt;%\n  mutate(short_time_cause = ifelse(hours == 0 | is.na(short_time_cause), \n                                   \"without_occupation\",\n                                   short_time_cause))\n\ndataset &lt;- dataset %&gt;%\n  mutate(type_of_contract = ifelse(is.na(type_of_contract),\n                                   \"other_contract\",\n                                   type_of_contract))\n\nLet’s now apply some further cleaning:\n\npra &lt;- dataset %&gt;%\n  filter(age %in% c(\"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\")) %&gt;%\n  filter(retirement_situation == 4) %&gt;%    \n  filter(!is.na(job_search)) %&gt;%  \n  select(capital, sex, place_of_birth, age, nationality, level_of_studies_completed,\n         occupational_training, job_search, main_occupation2, type_of_contract,\n         hours, short_time_cause, homework_situation,\n         main_professional_situation2) %&gt;%\n  mutate_at(.vars = vars(-hours), .funs=as.character) %&gt;%\n  mutate(job_search = as.factor(job_search))\n\nI only keep people that are not retired and of ages where they could work. I remove rows where job_search, the target, is missing, mutate all variables but hours to character and job_search to factor. At first, I made every categorical column a factor but I got problems for certain models. I think the issue came from the recipe that I defined (I’ll talk about it below), but the problem was resolved if categorical variables were defined as character variables. However, for certain models, the target (I think it was xgboost) needs to be a factor variable for classification problems.\n\n\nLet’s take a look at the data and check if any more data is missing:\n\nstr(pra)\n## Classes 'spec_tbl_df', 'tbl_df', 'tbl' and 'data.frame': 29083 obs. of  14 variables:\n##  $ capital                     : chr  \"9\" \"9\" \"1\" \"1\" ...\n##  $ sex                         : chr  \"6\" \"1\" \"1\" \"6\" ...\n##  $ place_of_birth              : chr  \"1\" \"1\" \"1\" \"1\" ...\n##  $ age                         : chr  \"09\" \"09\" \"11\" \"10\" ...\n##  $ nationality                 : chr  \"1\" \"1\" \"1\" \"1\" ...\n##  $ level_of_studies_completed  : chr  \"1\" \"2\" \"3\" \"3\" ...\n##  $ occupational_training       : chr  \"N\" \"N\" \"N\" \"N\" ...\n##  $ job_search                  : Factor w/ 2 levels \"N\",\"S\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ main_occupation2            : chr  \"5\" \"7\" \"3\" \"2\" ...\n##  $ type_of_contract            : chr  \"1\" \"other_contract\" \"other_contract\" \"1\" ...\n##  $ hours                       : num  36 40 40 40 0 0 22 38 40 0 ...\n##  $ short_time_cause            : chr  \"2\" \"2\" \"2\" \"2\" ...\n##  $ homework_situation          : chr  \"1\" \"2\" \"2\" \"2\" ...\n##  $ main_professional_situation2: chr  \"4\" \"2\" \"3\" \"4\" ...\nvis_miss(pra)\n\n\n\n\nThe final dataset contains 29083 observations. Look’s like we’re good to go."
  },
  {
    "objectID": "posts/2020-03-08-tidymodels.html#setting-up-the-training-resampling",
    "href": "posts/2020-03-08-tidymodels.html#setting-up-the-training-resampling",
    "title": "Machine learning with {tidymodels}",
    "section": "\nSetting up the training: resampling\n",
    "text": "Setting up the training: resampling\n\n\nIn order to properly train a model, one needs to split the data into two: a part for trying out models with different configuration of hyper-parameters, and another part for final evaluation of the model. This is achieved with rsample::initial_split():\n\npra_split &lt;- initial_split(pra, prop = 0.9)\n\npra_split now contains a training set and a testing set. We can get these by using the rsample::training() and rsample::testing() functions:\n\npra_train &lt;- training(pra_split)\npra_test &lt;- testing(pra_split)\n\nWe can’t stop here though. First we need to split the training set further, in order to perform cross validation. Cross validation will allow us to select the best model; by best I mean a model that has a good hyper-parameter configuration, enabling the model to generalize well to unseen data. I do this by creating 10 splits from the training data (I won’t touch the testing data up until the very end. This testing data is thus sometimes called the holdout set as well):\n\npra_cv_splits &lt;- vfold_cv(pra_train, v = 10)\n\nLet’s take a look at this object:\n\npra_cv_splits\n## #  10-fold cross-validation \n## # A tibble: 10 x 2\n##    splits               id    \n##    &lt;named list&gt;         &lt;chr&gt; \n##  1 &lt;split [23.6K/2.6K]&gt; Fold01\n##  2 &lt;split [23.6K/2.6K]&gt; Fold02\n##  3 &lt;split [23.6K/2.6K]&gt; Fold03\n##  4 &lt;split [23.6K/2.6K]&gt; Fold04\n##  5 &lt;split [23.6K/2.6K]&gt; Fold05\n##  6 &lt;split [23.6K/2.6K]&gt; Fold06\n##  7 &lt;split [23.6K/2.6K]&gt; Fold07\n##  8 &lt;split [23.6K/2.6K]&gt; Fold08\n##  9 &lt;split [23.6K/2.6K]&gt; Fold09\n## 10 &lt;split [23.6K/2.6K]&gt; Fold10"
  },
  {
    "objectID": "posts/2020-03-08-tidymodels.html#preprocessing-the-data",
    "href": "posts/2020-03-08-tidymodels.html#preprocessing-the-data",
    "title": "Machine learning with {tidymodels}",
    "section": "\nPreprocessing the data\n",
    "text": "Preprocessing the data\n\n\nI have already pre-processed the missing values in the dataset, so there is not much more that I can do. I will simply create dummy variables out of the categorical variables using step_dummy():\n\npreprocess &lt;- recipe(job_search ~ ., data = pra) %&gt;%\n  step_dummy(all_predictors())\n\npreprocess is a recipe that defines the transformations that must be applied to the training data before fitting. In this case there is only one step; transforming all the predictors into dummies (hours is a numeric variable and will be ignored by this step). The recipe also defines the formula that will be fitted by the models, job_search ~ ., and takes data as a further argument. This is only to give the data frame specification to recipe(): it could even be an empty data frame with the right column names and types. This is why I give it the original data pra and not the training set pra_train. Because this recipe is very simple, it could be applied to the original raw data pra and then I could do the split into training and testing set, as well as further splitting the training set into 10 cross-validation sets. However, this is not the recommended way of applying pre-processing steps. Pre-processing needs to happen inside the cross-validation loop, not outside of it. Why? Suppose that you are normalizing a numeric variable, meaning, substracting its mean from it and dividing by its standard deviation. If you do this operation outside of cross-validation, and even worse, before splitting the data into training and testing set, you will be leaking information from the testing set into the training set. The mean will contain information from the testing set, which will be picked up by the model. It is much better and “realistic” to first split the data and then apply the pre-processing (remember that hiding the test set from the model is supposed to simulate the fact that new, completely unseen data, is thrown at your model once it’s put into production). The same logic applies to cross-validation splits; each split contains now also a training and a testing set (which I will be calling analysis and assessment sets, following {tidymodels}’s author, Max Kuhn) and thus the pre-processing needs to be applied inside the cross-validation loop, meaning that the analysis set will be processed on the fly."
  },
  {
    "objectID": "posts/2020-03-08-tidymodels.html#model-definition",
    "href": "posts/2020-03-08-tidymodels.html#model-definition",
    "title": "Machine learning with {tidymodels}",
    "section": "\nModel definition\n",
    "text": "Model definition\n\n\nWe come now to the very interesting part: model definition. With {parsnip}, another {tidymodels} package, defining models is always the same, regardless of the underlying package doing the heavy lifting. For instance, to define a logistic regression one would simply write:\n\n# logistic regression \nlogit_tune_pra &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nThis defines a standard logistic regression, powered by the glm() engine or function. The way to do this in vanilla R would be :\n\nglm(y ~ ., data = mydata, family = \"binomial\")\n\nThe difference here is that the formula is contained in the glm() function; in our case it is contained in the recipe, which is why I don’t repeat it in the model definition above. You might wonder what the added value of using {tidymodels} for this is. Well, suppose now that I would like to run a logistic regression but with regularization. I would use {glmnet} for this but would need to know the specific syntax of glmnet() which, as you will see, is very different than the one for glm():\n\n  glmnet(x_vars[train,], y_var[train], alpha = 1, lambda = 1.6)\n\nglmnet(), unlike glm(), does not use a formula as an input, but two matrices, one for the design matrix, and another for the target variable. Using {parsnip}, however, I simply need to change the engine from \"glm\" to \"glmnet\":\n\n# logistic regression \nlogit_tune_pra &lt;- logistic_reg() %&gt;%\n  set_engine(\"glmnet\")\n\nThis makes things much simpler as now users only need to learn how to use {parsnip}. However, it is of course still important to read the documentation of the original packages, because it is were hyper-parameters are discussed. Another advantage of {parsnip} is that the same words are used to speak of the same hyper-parameters . For instance for tree-based methods, the number of trees is sometimes ntree then in another package num_trees, and is again different in yet another package. In {parsnip}’s interface for tree-based methods, this parameter is simply called tree. Users can fix the value of hyper-parameters directly by passing values to, say, tree (as in \"tree\" = 200), or they can tune these hyper-parameters. To do so, one needs to tag them, like so:\n\n# logistic regression \nlogit_tune_pra &lt;- logistic_reg(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(\"glmnet\")\n\nThis defines logit_tune_pra with 2 hyper-parameters that must be tuned using cross-validation, the penalty and the amount of mixture between penalties (this is for elasticnet regularization).\n\n\nNow, I will define 5 different models, with different hyper-parameters to tune, and I will also define a grid of hyper-parameters of size 10 for each model. This means that I will train these 5 models 10 times, each time with a different hyper-parameter configuration. To define the grid, I use the grid_max_entropy() function from the {dials} package. This creates a grid with points that are randomly drawn from the parameter space in a way that ensures that the combination we get covers the whole space, or at least are not too far away from any portion of the space. Of course, the more configuration you try, the better, but the longer the training will run.\n\n# Logistic regression\nlogit_tune_pra &lt;- logistic_reg(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(\"glmnet\")\n\n# Hyperparameter grid\nlogit_grid &lt;- logit_tune_pra %&gt;%\n  parameters() %&gt;%\n  grid_max_entropy(size = 10)\n\n# Workflow bundling every step \nlogit_wflow &lt;- workflow() %&gt;%\n  add_recipe(preprocess) %&gt;%\n  add_model(logit_tune_pra)\n\n# random forest\nrf_tune_pra &lt;- rand_forest(mtry = tune(), trees = tune()) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\nrf_grid &lt;- rf_tune_pra %&gt;%\n  parameters() %&gt;%\n  finalize(select(pra, -job_search)) %&gt;%  \n  grid_max_entropy(size = 10)\n\nrf_wflow &lt;- workflow() %&gt;%\n  add_recipe(preprocess) %&gt;%\n  add_model(rf_tune_pra)\n\n# mars model\nmars_tune_pra &lt;- mars(num_terms = tune(), prod_degree = 2, prune_method = tune()) %&gt;%\n  set_engine(\"earth\") %&gt;%\n  set_mode(\"classification\")\n\nmars_grid &lt;- mars_tune_pra %&gt;%\n  parameters() %&gt;%\n  grid_max_entropy(size = 10)\n\nmars_wflow &lt;- workflow() %&gt;%\n  add_recipe(preprocess) %&gt;%\n  add_model(mars_tune_pra)\n\n#boosted trees\nboost_tune_pra &lt;- boost_tree(mtry = tune(), tree = tune(),\n                             learn_rate = tune(), tree_depth = tune()) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\nboost_grid &lt;- boost_tune_pra %&gt;%\n  parameters() %&gt;%\n  finalize(select(pra, -job_search)) %&gt;%  \n  grid_max_entropy(size = 10)\n\nboost_wflow &lt;- workflow() %&gt;%\n  add_recipe(preprocess) %&gt;%\n  add_model(boost_tune_pra)\n\n#neural nets\nkeras_tune_pra &lt;- mlp(hidden_units = tune(), penalty = tune(), activation = \"relu\") %&gt;%\n  set_engine(\"keras\") %&gt;%\n  set_mode(\"classification\")\n\nkeras_grid &lt;- keras_tune_pra %&gt;%\n  parameters() %&gt;%\n  grid_max_entropy(size = 10)\n\nkeras_wflow &lt;- workflow() %&gt;%\n  add_recipe(preprocess) %&gt;%\n  add_model(keras_tune_pra)\n\nFor each model, I defined three objects; the model itself, for instance keras_tune_pra, then a grid of hyper-parameters, and finally a workflow. To define the grid, I need to extract the parameters to tune using the parameters() function, and for tree based methods, I also need to use finalize() to set the mtry parameter. This is because mtry depends on the dimensions of the data (the value of mtry cannot be larger than the number of features), so I need to pass on this information to…well, finalize the grid. Then I can choose the size of the grid and how I want to create it (randomly, or using max entropy, or regularly spaced…). A workflow bundles the pre-processing and the model definition together, and makes fitting the model very easy. Workflows make it easy to run the pre-processing inside the cross-validation loop. Workflow objects can be passed to the fitting function, as we shall see in the next section."
  },
  {
    "objectID": "posts/2020-03-08-tidymodels.html#fitting-models-with-tidymodels",
    "href": "posts/2020-03-08-tidymodels.html#fitting-models-with-tidymodels",
    "title": "Machine learning with {tidymodels}",
    "section": "\nFitting models with {tidymodels}\n",
    "text": "Fitting models with {tidymodels}\n\n\nFitting one model with {tidymodels} is quite easy:\n\nfitted_model &lt;- fit(model_formula, data = data_train)\n\nand that’s it. If you define a workflow, which bundles pre-processing and model definition in one package, you need to pass it to fit() as well:\n\nfitted_wflow &lt;- fit(model_wflow, data = data_train)\n\nHowever, a single call to fit does not perform cross-validation. This simply trains the model on the training data, and that’s it. To perform cross validation, you can use either fit_resamples():\n\nfitted_resamples &lt;- fit_resamples(model_wflow,\n                               resamples = my_cv_splits,\n                               control = control_resamples(save_pred = TRUE))\n\nor tune_grid():\n\ntuned_model &lt;- tune_grid(model_wflow,\n                         resamples = my_cv_splits,\n                         grid = my_grid,\n                         control = control_resamples(save_pred = TRUE))\n\nAs you probably guessed it, fit_resamples() does not perform tuning; it simply fits a model specification (without varying hyper-parameters) to all the analysis sets contained in the my_cv_splits object (which contains the resampled training data for cross-validation), while tune_grid() does the same, but allows for varying hyper-parameters.\n\n\nWe thus are going to use tune_grid() to fit our models and perform hyper-paramater tuning. However, since I have 5 models and 5 grids, I’ll be using map2() for this. If you’re not familiar with map2(), here’s a quick example:\n\nmap2(c(1, 1, 1), c(2,2,2), `+`)\n## [[1]]\n## [1] 3\n## \n## [[2]]\n## [1] 3\n## \n## [[3]]\n## [1] 3\n\nmap2() maps the +() function to each element of both vectors successively. I’m going to use this to map the tune_grid() function to a list of models and a list of grids. But because this is going to take some time to run, and because I have an AMD Ryzen 5 1600X processor with 6 physical cores and 12 logical cores, I’ll by running this in parallel using furrr::future_map2().\n\n\nfurrr::future_map2() will run one model per core, and the way to do it is to simply define how many cores I want to use, then replace map2() in my code by future_map2():\n\nwflow_list &lt;- list(logit_wflow, rf_wflow, mars_wflow, boost_wflow, keras_wflow)\ngrid_list &lt;- list(logit_grid, rf_grid, mars_grid, boost_grid, keras_grid)\n\nplan(multiprocess, workers = 6)\n\ntrained_models_list &lt;- future_map2(.x = wflow_list,\n                                   .y = grid_list,\n                                   ~tune_grid(.x , resamples = pra_cv_splits, grid = .y))\n\nRunning this code took almost 3 hours. In the end, here is the result:\n\ntrained_models_list\n## [[1]]\n## #  10-fold cross-validation \n## # A tibble: 10 x 4\n##    splits               id     .metrics          .notes          \n##  * &lt;list&gt;               &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n##  1 &lt;split [23.6K/2.6K]&gt; Fold01 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  2 &lt;split [23.6K/2.6K]&gt; Fold02 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  3 &lt;split [23.6K/2.6K]&gt; Fold03 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  4 &lt;split [23.6K/2.6K]&gt; Fold04 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  5 &lt;split [23.6K/2.6K]&gt; Fold05 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  6 &lt;split [23.6K/2.6K]&gt; Fold06 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  7 &lt;split [23.6K/2.6K]&gt; Fold07 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  8 &lt;split [23.6K/2.6K]&gt; Fold08 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  9 &lt;split [23.6K/2.6K]&gt; Fold09 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n## 10 &lt;split [23.6K/2.6K]&gt; Fold10 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n## \n## [[2]]\n## #  10-fold cross-validation \n## # A tibble: 10 x 4\n##    splits               id     .metrics          .notes          \n##  * &lt;list&gt;               &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n##  1 &lt;split [23.6K/2.6K]&gt; Fold01 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  2 &lt;split [23.6K/2.6K]&gt; Fold02 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  3 &lt;split [23.6K/2.6K]&gt; Fold03 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  4 &lt;split [23.6K/2.6K]&gt; Fold04 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  5 &lt;split [23.6K/2.6K]&gt; Fold05 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  6 &lt;split [23.6K/2.6K]&gt; Fold06 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  7 &lt;split [23.6K/2.6K]&gt; Fold07 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  8 &lt;split [23.6K/2.6K]&gt; Fold08 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  9 &lt;split [23.6K/2.6K]&gt; Fold09 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n## 10 &lt;split [23.6K/2.6K]&gt; Fold10 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n## \n## [[3]]\n## #  10-fold cross-validation \n## # A tibble: 10 x 4\n##    splits               id     .metrics          .notes          \n##  * &lt;list&gt;               &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n##  1 &lt;split [23.6K/2.6K]&gt; Fold01 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  2 &lt;split [23.6K/2.6K]&gt; Fold02 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  3 &lt;split [23.6K/2.6K]&gt; Fold03 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  4 &lt;split [23.6K/2.6K]&gt; Fold04 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  5 &lt;split [23.6K/2.6K]&gt; Fold05 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  6 &lt;split [23.6K/2.6K]&gt; Fold06 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  7 &lt;split [23.6K/2.6K]&gt; Fold07 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  8 &lt;split [23.6K/2.6K]&gt; Fold08 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  9 &lt;split [23.6K/2.6K]&gt; Fold09 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n## 10 &lt;split [23.6K/2.6K]&gt; Fold10 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n## \n## [[4]]\n## #  10-fold cross-validation \n## # A tibble: 10 x 4\n##    splits               id     .metrics          .notes          \n##  * &lt;list&gt;               &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n##  1 &lt;split [23.6K/2.6K]&gt; Fold01 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n##  2 &lt;split [23.6K/2.6K]&gt; Fold02 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n##  3 &lt;split [23.6K/2.6K]&gt; Fold03 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n##  4 &lt;split [23.6K/2.6K]&gt; Fold04 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n##  5 &lt;split [23.6K/2.6K]&gt; Fold05 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n##  6 &lt;split [23.6K/2.6K]&gt; Fold06 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n##  7 &lt;split [23.6K/2.6K]&gt; Fold07 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n##  8 &lt;split [23.6K/2.6K]&gt; Fold08 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n##  9 &lt;split [23.6K/2.6K]&gt; Fold09 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n## 10 &lt;split [23.6K/2.6K]&gt; Fold10 &lt;tibble [20 × 7]&gt; &lt;tibble [1 × 1]&gt;\n## \n## [[5]]\n## #  10-fold cross-validation \n## # A tibble: 10 x 4\n##    splits               id     .metrics          .notes          \n##  * &lt;list&gt;               &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n##  1 &lt;split [23.6K/2.6K]&gt; Fold01 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  2 &lt;split [23.6K/2.6K]&gt; Fold02 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  3 &lt;split [23.6K/2.6K]&gt; Fold03 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  4 &lt;split [23.6K/2.6K]&gt; Fold04 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  5 &lt;split [23.6K/2.6K]&gt; Fold05 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  6 &lt;split [23.6K/2.6K]&gt; Fold06 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  7 &lt;split [23.6K/2.6K]&gt; Fold07 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  8 &lt;split [23.6K/2.6K]&gt; Fold08 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n##  9 &lt;split [23.6K/2.6K]&gt; Fold09 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n## 10 &lt;split [23.6K/2.6K]&gt; Fold10 &lt;tibble [20 × 5]&gt; &lt;tibble [1 × 1]&gt;\n\nI now have a list of 5 tibbles containing the analysis/assessment splits, the id identifying the cross-validation fold, a list-column containing information on model performance for that given split and some notes (if everything goes well, notes are empty). Let’s take a look at the column .metrics of the first model and for the first fold:\n\ntrained_models_list[[1]]$.metrics[[1]]\n## # A tibble: 20 x 5\n##     penalty mixture .metric  .estimator .estimate\n##       &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n##  1 4.25e- 3  0.0615 accuracy binary         0.906\n##  2 4.25e- 3  0.0615 roc_auc  binary         0.895\n##  3 6.57e-10  0.0655 accuracy binary         0.908\n##  4 6.57e-10  0.0655 roc_auc  binary         0.897\n##  5 1.18e- 6  0.167  accuracy binary         0.908\n##  6 1.18e- 6  0.167  roc_auc  binary         0.897\n##  7 2.19e-10  0.371  accuracy binary         0.907\n##  8 2.19e-10  0.371  roc_auc  binary         0.897\n##  9 2.73e- 1  0.397  accuracy binary         0.885\n## 10 2.73e- 1  0.397  roc_auc  binary         0.5  \n## 11 1.72e- 6  0.504  accuracy binary         0.907\n## 12 1.72e- 6  0.504  roc_auc  binary         0.897\n## 13 1.25e- 9  0.633  accuracy binary         0.907\n## 14 1.25e- 9  0.633  roc_auc  binary         0.897\n## 15 6.62e- 6  0.880  accuracy binary         0.907\n## 16 6.62e- 6  0.880  roc_auc  binary         0.897\n## 17 6.00e- 1  0.899  accuracy binary         0.885\n## 18 6.00e- 1  0.899  roc_auc  binary         0.5  \n## 19 4.57e-10  0.989  accuracy binary         0.907\n## 20 4.57e-10  0.989  roc_auc  binary         0.897\n\nThis shows how the 10 different configurations of the elasticnet model performed. To see how the model performed on the second fold:\n\ntrained_models_list[[1]]$.metrics[[2]]\n## # A tibble: 20 x 5\n##     penalty mixture .metric  .estimator .estimate\n##       &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n##  1 4.25e- 3  0.0615 accuracy binary         0.913\n##  2 4.25e- 3  0.0615 roc_auc  binary         0.874\n##  3 6.57e-10  0.0655 accuracy binary         0.913\n##  4 6.57e-10  0.0655 roc_auc  binary         0.877\n##  5 1.18e- 6  0.167  accuracy binary         0.913\n##  6 1.18e- 6  0.167  roc_auc  binary         0.878\n##  7 2.19e-10  0.371  accuracy binary         0.913\n##  8 2.19e-10  0.371  roc_auc  binary         0.878\n##  9 2.73e- 1  0.397  accuracy binary         0.901\n## 10 2.73e- 1  0.397  roc_auc  binary         0.5  \n## 11 1.72e- 6  0.504  accuracy binary         0.913\n## 12 1.72e- 6  0.504  roc_auc  binary         0.878\n## 13 1.25e- 9  0.633  accuracy binary         0.913\n## 14 1.25e- 9  0.633  roc_auc  binary         0.878\n## 15 6.62e- 6  0.880  accuracy binary         0.913\n## 16 6.62e- 6  0.880  roc_auc  binary         0.878\n## 17 6.00e- 1  0.899  accuracy binary         0.901\n## 18 6.00e- 1  0.899  roc_auc  binary         0.5  \n## 19 4.57e-10  0.989  accuracy binary         0.913\n## 20 4.57e-10  0.989  roc_auc  binary         0.878\n\nHyper-Parameters are the same; it is only the cross validation fold that is different. To get the best performing model from such objects you can use show_best() which will extract the best performing models across all the cross validation folds:\n\nshow_best(trained_models_list[[1]], metric = \"accuracy\")\n## # A tibble: 5 x 7\n##    penalty mixture .metric  .estimator  mean     n std_err\n##      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n## 1 6.57e-10  0.0655 accuracy binary     0.916    10 0.00179\n## 2 1.18e- 6  0.167  accuracy binary     0.916    10 0.00180\n## 3 1.72e- 6  0.504  accuracy binary     0.916    10 0.00182\n## 4 4.57e-10  0.989  accuracy binary     0.916    10 0.00181\n## 5 6.62e- 6  0.880  accuracy binary     0.916    10 0.00181\n\nThis shows the 5 best configurations for elasticnet when looking at accuracy. Now how to get the best performing elasticnet regression, random forest, boosted trees, etc? Easy, using map():\n\nmap(trained_models_list, show_best, metric = \"accuracy\")\n## [[1]]\n## # A tibble: 5 x 7\n##    penalty mixture .metric  .estimator  mean     n std_err\n##      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n## 1 6.57e-10  0.0655 accuracy binary     0.916    10 0.00179\n## 2 1.18e- 6  0.167  accuracy binary     0.916    10 0.00180\n## 3 1.72e- 6  0.504  accuracy binary     0.916    10 0.00182\n## 4 4.57e-10  0.989  accuracy binary     0.916    10 0.00181\n## 5 6.62e- 6  0.880  accuracy binary     0.916    10 0.00181\n## \n## [[2]]\n## # A tibble: 5 x 7\n##    mtry trees .metric  .estimator  mean     n std_err\n##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n## 1    13  1991 accuracy binary     0.929    10 0.00172\n## 2    13  1180 accuracy binary     0.929    10 0.00168\n## 3    12   285 accuracy binary     0.928    10 0.00168\n## 4     8  1567 accuracy binary     0.927    10 0.00171\n## 5     8   647 accuracy binary     0.927    10 0.00191\n## \n## [[3]]\n## # A tibble: 5 x 7\n##   num_terms prune_method .metric  .estimator  mean     n std_err\n##       &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n## 1         5 backward     accuracy binary     0.904    10 0.00186\n## 2         5 forward      accuracy binary     0.902    10 0.00185\n## 3         4 exhaustive   accuracy binary     0.901    10 0.00167\n## 4         4 seqrep       accuracy binary     0.901    10 0.00167\n## 5         2 backward     accuracy binary     0.896    10 0.00209\n## \n## [[4]]\n## # A tibble: 5 x 9\n##    mtry trees tree_depth learn_rate .metric  .estimator  mean     n std_err\n##   &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n## 1    12  1245         12   7.70e- 2 accuracy binary     0.929    10 0.00175\n## 2     1   239          8   8.23e- 2 accuracy binary     0.927    10 0.00186\n## 3     1   835         14   8.53e-10 accuracy binary     0.913    10 0.00232\n## 4     4  1522         12   2.22e- 5 accuracy binary     0.896    10 0.00209\n## 5     6   313          2   1.21e- 8 accuracy binary     0.896    10 0.00209\n## \n## [[5]]\n## # A tibble: 5 x 7\n##   hidden_units  penalty .metric  .estimator  mean     n std_err\n##          &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n## 1           10 3.07e- 6 accuracy binary     0.917    10 0.00209\n## 2            6 1.69e-10 accuracy binary     0.917    10 0.00216\n## 3            4 2.32e- 7 accuracy binary     0.916    10 0.00194\n## 4            7 5.52e- 5 accuracy binary     0.916    10 0.00163\n## 5            8 1.13e- 9 accuracy binary     0.916    10 0.00173\n\nNow, we need to test these models on the holdout set, but this post is already quite long. In the next blog post, I will retrain the top best performing models for each type of model and see how they fare against the holdout set. I’ll be also looking at explainability, so stay tuned!"
  },
  {
    "objectID": "posts/2018-11-16-rgenoud_arima.html#introduction",
    "href": "posts/2018-11-16-rgenoud_arima.html#introduction",
    "title": "Using a genetic algorithm for the hyperparameter optimization of a SARIMA model",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nIn this blog post, I’ll use the data that I cleaned in a previous blog post, which you can download here. If you want to follow along, download the monthly data. In my last blog post I showed how to perform a grid search the “tidy” way. As an example, I looked for the right hyperparameters of a SARIMA model. However, the goal of the post was not hyperparameter optimization per se, so I did not bother with tuning the hyperparameters on a validation set, and used the test set for both validation of the hyperparameters and testing the forecast. Of course, this is not great because doing this might lead to overfitting the hyperparameters to the test set. So in this blog post I split my data into trainig, validation and testing sets and use a genetic algorithm to look for the hyperparameters. Again, this is not the most optimal way to go about this problem, since the {forecast} package contains the very useful auto.arima() function. I just wanted to see what kind of solution a genetic algorithm would return, and also try different cost functions. If you’re interested, read on!"
  },
  {
    "objectID": "posts/2018-11-16-rgenoud_arima.html#setup",
    "href": "posts/2018-11-16-rgenoud_arima.html#setup",
    "title": "Using a genetic algorithm for the hyperparameter optimization of a SARIMA model",
    "section": "\nSetup\n",
    "text": "Setup\n\n\nLet’s first load some libraries and define some helper functions (the helper functions were explained in the previous blog posts):\n\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(rgenoud)\nlibrary(parallel)\nlibrary(lubridate)\nlibrary(furrr)\nlibrary(tsibble)\nlibrary(brotools)\n\nihs &lt;- function(x){\n    log(x + sqrt(x**2 + 1))\n}\n\nto_tibble &lt;- function(forecast_object){\n    point_estimate &lt;- forecast_object$mean %&gt;%\n        as_tsibble() %&gt;%\n        rename(point_estimate = value,\n               date = index)\n\n    upper &lt;- forecast_object$upper %&gt;%\n        as_tsibble() %&gt;%\n        spread(key, value) %&gt;%\n        rename(date = index,\n               upper80 = `80%`,\n               upper95 = `95%`)\n\n    lower &lt;- forecast_object$lower %&gt;%\n        as_tsibble() %&gt;%\n        spread(key, value) %&gt;%\n        rename(date = index,\n               lower80 = `80%`,\n               lower95 = `95%`)\n\n    reduce(list(point_estimate, upper, lower), full_join)\n}\n\nNow, let’s load the data:\n\navia_clean_monthly &lt;- read_csv(\"https://raw.githubusercontent.com/b-rodrigues/avia_par_lu/master/avia_clean_monthy.csv\")\n## Parsed with column specification:\n## cols(\n##   destination = col_character(),\n##   date = col_date(format = \"\"),\n##   passengers = col_double()\n## )\n\nLet’s split the data into a train set, a validation set and a test set:\n\navia_clean_train &lt;- avia_clean_monthly %&gt;%\n    select(date, passengers) %&gt;%\n    filter(year(date) &lt; 2013) %&gt;%\n    group_by(date) %&gt;%\n    summarise(total_passengers = sum(passengers)) %&gt;%\n    pull(total_passengers) %&gt;%\n    ts(., frequency = 12, start = c(2005, 1))\n\navia_clean_validation &lt;- avia_clean_monthly %&gt;%\n    select(date, passengers) %&gt;%\n    filter(between(year(date), 2013, 2016)) %&gt;%\n    group_by(date) %&gt;%\n    summarise(total_passengers = sum(passengers)) %&gt;%\n    pull(total_passengers) %&gt;%\n    ts(., frequency = 12, start = c(2013, 1))\n\navia_clean_test &lt;- avia_clean_monthly %&gt;%\n    select(date, passengers) %&gt;%\n    filter(year(date) &gt;= 2016) %&gt;%\n    group_by(date) %&gt;%\n    summarise(total_passengers = sum(passengers)) %&gt;%\n    pull(total_passengers) %&gt;%\n    ts(., frequency = 12, start = c(2016, 1))\n\nlogged_test_data &lt;- ihs(avia_clean_test)\n\nlogged_validation_data &lt;- ihs(avia_clean_validation)\n\nlogged_train_data &lt;- ihs(avia_clean_train)\n\nI will train the models on data from 2005 to 2012, look for the hyperparameters on data from 2013 to 2016 and test the accuracy on data from 2016 to March 2018. For this kind of exercise, the ideal situation would be to perform cross-validation. Doing this with time-series data is not obvious because of the autocorrelation between observations, which would be broken by sampling independently which is required by CV. Also, if for example you do leave-one-out CV, you would end up trying to predict a point in, say, 2017, with data from 2018, which does not make sense. So you should be careful about that. {forecast} is able to perform CV for time series and scikit-learn, the Python package, is able to perform cross-validation of time series data too. I will not do it in this blog post and simply focus on the genetic algorithm part.\n\n\nLet’s start by defining the cost function to minimize. I’ll try several, in the first one I will minimize the RMSE:\n\ncost_function_rmse &lt;- function(param, train_data, validation_data, forecast_periods){\n    order &lt;- param[1:3]\n    season &lt;- c(param[4:6], 12)\n    model &lt;- purrr::possibly(arima, otherwise = NULL)(x = train_data, order = order, \n                                                      seasonal = season,\n                                                      method = \"ML\")\n    if(is.null(model)){\n        return(9999999)\n    } else {\n      forecast_model &lt;- forecast::forecast(model, h = forecast_periods)\n      point_forecast &lt;- forecast_model$mean\n      sqrt(mean(point_forecast - validation_data) ** 2)\n    }\n}\n\nIf arima() is not able to estimate a model for the given parameters, I force it to return NULL, and in that case force the cost function to return a very high cost. If a model was successfully estimated, then I compute the RMSE.\n\n\nLet’s also take a look at what auto.arima() says:\n\nstarting_model &lt;- auto.arima(logged_train_data)\nsummary(starting_model)\n## Series: logged_train_data \n## ARIMA(3,0,0)(0,1,1)[12] with drift \n## \n## Coefficients:\n##          ar1     ar2     ar3     sma1   drift\n##       0.2318  0.2292  0.3661  -0.8498  0.0029\n## s.e.  0.1016  0.1026  0.1031   0.2101  0.0010\n## \n## sigma^2 estimated as 0.004009:  log likelihood=107.98\n## AIC=-203.97   AICc=-202.88   BIC=-189.38\n## \n## Training set error measures:\n##                        ME       RMSE        MAE         MPE      MAPE\n## Training set 0.0009924108 0.05743719 0.03577996 0.006323241 0.3080978\n##                   MASE        ACF1\n## Training set 0.4078581 -0.02707016\n\nLet’s compute the cost at this vector of parameters:\n\ncost_function_rmse(c(1, 0, 2, 2, 1, 0),\n              train_data = logged_train_data,\n              validation_data = logged_validation_data,\n              forecast_periods = 65)\n## [1] 0.1731473\n\nOk, now let’s start with optimizing the hyperparameters. Let’s help the genetic algorithm a little bit by defining where it should perform the search:\n\ndomains &lt;- matrix(c(0, 3, 0, 2, 0, 3, 0, 3, 0, 2, 0, 3), byrow = TRUE, ncol = 2)\n\nThis matrix constraints the first parameter to lie between 0 and 3, the second one between 0 and 2, and so on.\n\n\nLet’s call the genoud() function from the {rgenoud} package, and use 8 cores:\n\ncl &lt;- makePSOCKcluster(8)\nclusterExport(cl, c('logged_train_data', 'logged_validation_data'))\n\ntic &lt;- Sys.time()\n\nauto_arima_rmse &lt;- genoud(cost_function_rmse,\n                     nvars = 6,\n                     data.type.int = TRUE,\n                     starting.values = c(1, 0, 2, 2, 1, 0), # &lt;- from auto.arima\n                     Domains = domains,\n                     cluster = cl,\n                     train_data = logged_train_data,\n                     validation_data = logged_validation_data,\n                     forecast_periods = length(logged_validation_data),\n                     hard.generation.limit = TRUE)\ntoc_rmse &lt;- Sys.time() - tic\n\nmakePSOCKcluster() is a function from the {parallel} package. I must also export the global variables logged_train_data or logged_validation_data. If I don’t do that, the workers called by genoud() will not know about these variables and an error will be returned. The option data.type.int = TRUE force the algorithm to look only for integers, and hard.generation.limit = TRUE forces the algorithm to stop after 100 generations.\n\n\nThe process took 7 minutes, which is faster than doing the grid search. What was the solution found?\n\nauto_arima_rmse\n## $value\n## [1] 0.0001863039\n## \n## $par\n## [1] 3 2 1 1 2 1\n## \n## $gradients\n## [1] NA NA NA NA NA NA\n## \n## $generations\n## [1] 11\n## \n## $peakgeneration\n## [1] 1\n## \n## $popsize\n## [1] 1000\n## \n## $operators\n## [1] 122 125 125 125 125 126 125 126   0\n\nLet’s train the model using the arima() function at these parameters:\n\nbest_model_rmse &lt;- arima(logged_train_data, order = auto_arima_rmse$par[1:3], \n                         season = list(order = auto_arima_rmse$par[4:6], period = 12),\n                         method = \"ML\")\n\nsummary(best_model_rmse)\n## \n## Call:\n## arima(x = logged_train_data, order = auto_arima_rmse$par[1:3], seasonal = list(order = auto_arima_rmse$par[4:6], \n##     period = 12), method = \"ML\")\n## \n## Coefficients:\n##           ar1      ar2      ar3      ma1     sar1     sma1\n##       -0.6999  -0.4541  -0.0476  -0.9454  -0.4996  -0.9846\n## s.e.   0.1421   0.1612   0.1405   0.1554   0.1140   0.2193\n## \n## sigma^2 estimated as 0.006247:  log likelihood = 57.34,  aic = -100.67\n## \n## Training set error measures:\n##                         ME       RMSE        MAE          MPE      MAPE\n## Training set -0.0006142355 0.06759545 0.04198561 -0.005408262 0.3600483\n##                   MASE         ACF1\n## Training set 0.4386693 -0.008298546\n\nLet’s extract the forecasts:\n\nbest_model_rmse_forecast &lt;- forecast::forecast(best_model_rmse, h = 65)\n\nbest_model_rmse_forecast &lt;- to_tibble(best_model_rmse_forecast)\n## Joining, by = \"date\"\n## Joining, by = \"date\"\nstarting_model_forecast &lt;- forecast(starting_model, h = 65)\n\nstarting_model_forecast &lt;- to_tibble(starting_model_forecast)\n## Joining, by = \"date\"\n## Joining, by = \"date\"\n\nand plot the forecast to see how it looks:\n\navia_clean_monthly %&gt;%\n    group_by(date) %&gt;%\n    summarise(total = sum(passengers)) %&gt;%\n    mutate(total_ihs = ihs(total)) %&gt;%\n    ggplot() +\n    ggtitle(\"Minimization of RMSE\") +\n    geom_line(aes(y = total_ihs, x = date), colour = \"#82518c\") +\n    scale_x_date(date_breaks = \"1 year\", date_labels = \"%m-%Y\") +\n    geom_ribbon(data = best_model_rmse_forecast, aes(x = date, ymin = lower95, ymax = upper95),\n                fill = \"#666018\", alpha = 0.2) +\n    geom_line(data = best_model_rmse_forecast, aes(x = date, y = point_estimate), \n              linetype = 2, colour = \"#8e9d98\") +\n    geom_ribbon(data = starting_model_forecast, aes(x = date, ymin = lower95, ymax = upper95),\n                fill = \"#98431e\", alpha = 0.2) +\n    geom_line(data = starting_model_forecast, aes(x = date, y = point_estimate), \n              linetype = 2, colour = \"#a53031\") +\n    theme_blog()\n\n\n\n\nThe yellowish line and confidence intervals come from minimizing the genetic algorithm, and the redish from auto.arima(). Interesting; the point estimate is very precise, but the confidence intervals are very wide. Low bias, high variance.\n\n\nNow, let’s try with another cost function, where I minimize the BIC, similar to the auto.arima() function:\n\ncost_function_bic &lt;- function(param, train_data, validation_data, forecast_periods){\n    order &lt;- param[1:3]\n    season &lt;- c(param[4:6], 12)\n    model &lt;- purrr::possibly(arima, otherwise = NULL)(x = train_data, order = order, \n                                                      seasonal = season,\n                                                      method = \"ML\")\n    if(is.null(model)){\n        return(9999999)\n    } else {\n        BIC(model)\n    }\n}\n\nLet’s take a look at the cost at the parameter values returned by auto.arima():\n\ncost_function_bic(c(1, 0, 2, 2, 1, 0),\n              train_data = logged_train_data,\n              validation_data = logged_validation_data,\n              forecast_periods = 65)\n## [1] -184.6397\n\nLet the genetic algorithm run again:\n\ncl &lt;- makePSOCKcluster(8)\nclusterExport(cl, c('logged_train_data', 'logged_validation_data'))\n\ntic &lt;- Sys.time()\n\nauto_arima_bic &lt;- genoud(cost_function_bic,\n                     nvars = 6,\n                     data.type.int = TRUE,\n                     starting.values = c(1, 0, 2, 2, 1, 0), # &lt;- from auto.arima\n                     Domains = domains,\n                     cluster = cl,\n                     train_data = logged_train_data,\n                     validation_data = logged_validation_data,\n                     forecast_periods = length(logged_validation_data),\n                     hard.generation.limit = TRUE)\ntoc_bic &lt;- Sys.time() - tic\n\nThis time, it took 6 minutes, a bit slower than before. Let’s take a look at the solution:\n\nauto_arima_bic\n## $value\n## [1] -201.0656\n## \n## $par\n## [1] 0 1 1 1 0 1\n## \n## $gradients\n## [1] NA NA NA NA NA NA\n## \n## $generations\n## [1] 12\n## \n## $peakgeneration\n## [1] 1\n## \n## $popsize\n## [1] 1000\n## \n## $operators\n## [1] 122 125 125 125 125 126 125 126   0\n\nLet’s train the model at these parameters:\n\nbest_model_bic &lt;- arima(logged_train_data, order = auto_arima_bic$par[1:3], \n                        season = list(order = auto_arima_bic$par[4:6], period = 12),\n                        method = \"ML\")\n\nsummary(best_model_bic)\n## \n## Call:\n## arima(x = logged_train_data, order = auto_arima_bic$par[1:3], seasonal = list(order = auto_arima_bic$par[4:6], \n##     period = 12), method = \"ML\")\n## \n## Coefficients:\n##           ma1    sar1    sma1\n##       -0.6225  0.9968  -0.832\n## s.e.   0.0835  0.0075   0.187\n## \n## sigma^2 estimated as 0.004145:  log likelihood = 109.64,  aic = -211.28\n## \n## Training set error measures:\n##                       ME       RMSE        MAE        MPE      MAPE\n## Training set 0.003710982 0.06405303 0.04358164 0.02873561 0.3753513\n##                   MASE        ACF1\n## Training set 0.4553447 -0.03450603\n\nAnd let’s plot the results:\n\nbest_model_bic_forecast &lt;- forecast::forecast(best_model_bic, h = 65)\n\nbest_model_bic_forecast &lt;- to_tibble(best_model_bic_forecast)\n## Joining, by = \"date\"\n## Joining, by = \"date\"\navia_clean_monthly %&gt;%\n    group_by(date) %&gt;%\n    summarise(total = sum(passengers)) %&gt;%\n    mutate(total_ihs = ihs(total)) %&gt;%\n    ggplot() +\n    ggtitle(\"Minimization of BIC\") +\n    geom_line(aes(y = total_ihs, x = date), colour = \"#82518c\") +\n    scale_x_date(date_breaks = \"1 year\", date_labels = \"%m-%Y\") +\n    geom_ribbon(data = best_model_bic_forecast, aes(x = date, ymin = lower95, ymax = upper95),\n                fill = \"#5160a0\", alpha = 0.2) +\n    geom_line(data = best_model_bic_forecast, aes(x = date, y = point_estimate), \n              linetype = 2, colour = \"#208480\") +\n    geom_ribbon(data = starting_model_forecast, aes(x = date, ymin = lower95, ymax = upper95),\n                fill = \"#98431e\", alpha = 0.2) +\n    geom_line(data = starting_model_forecast, aes(x = date, y = point_estimate), \n              linetype = 2, colour = \"#a53031\") +\n    theme_blog()\n\n\n\n\nThe solutions are very close, both in terms of point estimates and confidence intervals. Bias increased, but variance lowered… This gives me an idea! What if I minimize the RMSE, while keeping the number of parameters low, as a kind of regularization? This is somewhat what minimising BIC does, but let’s try to do it a more “naive” approach:\n\ncost_function_rmse_low_k &lt;- function(param, train_data, validation_data, forecast_periods, max.order){\n    order &lt;- param[1:3]\n    season &lt;- c(param[4:6], 12)\n    if(param[1] + param[3] + param[4] + param[6] &gt; max.order){\n        return(9999999)\n    } else {\n        model &lt;- purrr::possibly(arima, otherwise = NULL)(x = train_data, \n                                                          order = order, \n                                                          seasonal = season,\n                                                          method = \"ML\")\n    }\n    if(is.null(model)){\n        return(9999999)\n    } else {\n        forecast_model &lt;- forecast::forecast(model, h = forecast_periods)\n        point_forecast &lt;- forecast_model$mean\n        sqrt(mean(point_forecast - validation_data) ** 2)\n    }\n}\n\nThis is also similar to what auto.arima() does; by default, the max.order argument in auto.arima() is set to 5, and is the sum of p + q + P + Q. So I’ll try something similar.\n\n\nLet’s take a look at the cost at the parameter values returned by auto.arima():\n\ncost_function_rmse_low_k(c(1, 0, 2, 2, 1, 0),\n              train_data = logged_train_data,\n              validation_data = logged_validation_data,\n              forecast_periods = 65,\n              max.order = 5)\n## [1] 0.1731473\n\nLet’s see what will happen:\n\ncl &lt;- makePSOCKcluster(8)\nclusterExport(cl, c('logged_train_data', 'logged_validation_data'))\n\ntic &lt;- Sys.time()\n\nauto_arima_rmse_low_k &lt;- genoud(cost_function_rmse_low_k,\n                         nvars = 6,\n                         data.type.int = TRUE,\n                         starting.values = c(1, 0, 2, 2, 1, 0), # &lt;- from auto.arima\n                         max.order = 5,\n                         Domains = domains,\n                         cluster = cl,\n                         train_data = logged_train_data,\n                         validation_data = logged_validation_data,\n                         forecast_periods = length(logged_validation_data),\n                         hard.generation.limit = TRUE)\ntoc_rmse_low_k &lt;- Sys.time() - tic\n\nIt took 1 minute to train this one, quite fast! Let’s take a look:\n\nauto_arima_rmse_low_k\n## $value\n## [1] 0.002503478\n## \n## $par\n## [1] 1 2 0 3 1 0\n## \n## $gradients\n## [1] NA NA NA NA NA NA\n## \n## $generations\n## [1] 11\n## \n## $peakgeneration\n## [1] 1\n## \n## $popsize\n## [1] 1000\n## \n## $operators\n## [1] 122 125 125 125 125 126 125 126   0\n\nAnd let’s plot it:\n\nbest_model_rmse_low_k &lt;- arima(logged_train_data, order = auto_arima_rmse_low_k$par[1:3], \n                               season = list(order = auto_arima_rmse_low_k$par[4:6], period = 12),\n                               method = \"ML\")\n\nsummary(best_model_rmse_low_k)\n## \n## Call:\n## arima(x = logged_train_data, order = auto_arima_rmse_low_k$par[1:3], seasonal = list(order = auto_arima_rmse_low_k$par[4:6], \n##     period = 12), method = \"ML\")\n## \n## Coefficients:\n##           ar1     sar1     sar2     sar3\n##       -0.6468  -0.7478  -0.5263  -0.1143\n## s.e.   0.0846   0.1171   0.1473   0.1446\n## \n## sigma^2 estimated as 0.01186:  log likelihood = 57.88,  aic = -105.76\n## \n## Training set error measures:\n##                        ME      RMSE        MAE         MPE      MAPE\n## Training set 0.0005953302 0.1006917 0.06165919 0.003720452 0.5291736\n##                   MASE       ACF1\n## Training set 0.6442205 -0.3706693\nbest_model_rmse_low_k_forecast &lt;- forecast::forecast(best_model_rmse_low_k, h = 65)\n\nbest_model_rmse_low_k_forecast &lt;- to_tibble(best_model_rmse_low_k_forecast)\n## Joining, by = \"date\"\n## Joining, by = \"date\"\navia_clean_monthly %&gt;%\n    group_by(date) %&gt;%\n    summarise(total = sum(passengers)) %&gt;%\n    mutate(total_ihs = ihs(total)) %&gt;%\n    ggplot() +\n    ggtitle(\"Minimization of RMSE + low k\") +\n    geom_line(aes(y = total_ihs, x = date), colour = \"#82518c\") +\n    scale_x_date(date_breaks = \"1 year\", date_labels = \"%m-%Y\") +\n    geom_ribbon(data = best_model_rmse_low_k_forecast, aes(x = date, ymin = lower95, ymax = upper95),\n                fill = \"#5160a0\", alpha = 0.2) +\n    geom_line(data = best_model_rmse_low_k_forecast, aes(x = date, y = point_estimate), \n              linetype = 2, colour = \"#208480\") +\n    geom_ribbon(data = starting_model_forecast, aes(x = date, ymin = lower95, ymax = upper95),\n                fill = \"#98431e\", alpha = 0.2) +\n    geom_line(data = starting_model_forecast, aes(x = date, y = point_estimate), \n              linetype = 2, colour = \"#a53031\") +\n    theme_blog()\n\n\n\n\nLooks like this was not the right strategy. There might be a better cost function than what I have tried, but looks like minimizing the BIC is the way to go."
  },
  {
    "objectID": "posts/2022-10-23-licenses.html",
    "href": "posts/2022-10-23-licenses.html",
    "title": "R, its license and my take on it",
    "section": "",
    "text": "Foreword: This is not a tutorial nor anything like that. I’m going to talk about free software, open source, and their licenses. I’m going to give my (non-)expert opinion on it. You may find, after having finished reading this post, that I wasted your time. So only read if by some miracle the first sentence of the foreword excited you. If not, close this tab and go back now. It’s not too late.\nForeword 2: I’ve updated the post on October 24th with an additional meme, clarifications and a link to an interesting stackexchange discussion."
  },
  {
    "objectID": "posts/2022-10-23-licenses.html#free-software-aint-free",
    "href": "posts/2022-10-23-licenses.html#free-software-aint-free",
    "title": "R, its license and my take on it",
    "section": "\nFree software ain’t free\n",
    "text": "Free software ain’t free\n\n\nLet me first re-iterate that free software and open source are not the same thing. Free software is open source, but not every piece of open source software is free. Open source means that the source code of a piece of software is available and can be consulted without much hurdles. It also means, usually, that you can take these pieces of software, modify them, and redistribute them without much hurdles either.\n\n\nFree software is like open source software, but it’s much more restrictive. That may seem surprising, because there’s the word free in there, so how could it be more restrictive than open source software? Consider the following: I can take a piece of open source software (typically licensed under something like the MIT licenses or the BSD licenses) and re-release it with a proprietary license and sell it. I don’t actually even need to change anything substantial to the source code. I take that piece of software (which I may or may not modify), repackage it under a new name and sell it. Free software allows all of this as well (I literally could sell the Linux kernel on this website if I found people willing to pay me for it), but what it does not allow is only this: I cannot distribute (by selling or for free) the program without its source code. So if I sold the Linux kernel on here, I would need to also give out a copy of the source code with it, and this obviously would also still be necessary if I actually changed something to the source code of the Linux kernel.\n\n\nR is licensed under a Free Software license, the GPL v2, which means that it’s illegal for anyone to rebrand it, package it and sell it without providing the source code of their (modified) version of R. Thanks to something like the GPL, it is impossible for companies to employ what is called Embrace, Extend and Extinguish, which is a strategy that Microsoft used in the past. It consists in embracing a piece of software, extending it with proprietary bits of code and technology, use their dominant position on the desktop to impose their new version that relies on proprietary bits (or which is 100% proprietary) and then extinguish the open source version (in the sense that no one will use it anymore because it virtually became incompatible with the newly imposed Microsoft version).\n\n\nNow some of you may now be thinking that I’m stuck in the 90’s, after all, Microsoft have been the good guys for a decade now. They contribute to open source software (not free software), have bought Github and have not ruined it (yet) and they even included the possibility to run Linux inside Windows using WSL. So what am I afraid of? Why don’t I trust them?"
  },
  {
    "objectID": "posts/2022-10-23-licenses.html#all-licenses-have-their-place-but",
    "href": "posts/2022-10-23-licenses.html#all-licenses-have-their-place-but",
    "title": "R, its license and my take on it",
    "section": "\nAll licenses have their place, but…\n",
    "text": "All licenses have their place, but…\n\n\nThe thing is, I shouldn’t have to trust anyone not to fuck up a piece of free software. Maybe the current management of Microsoft is not hostile to free software, but maybe that’ll change in the future. That’s not really the point. The point is that I don’t need to have to trust them, and I’m happy that a fundamental, crucial piece of software like R uses something like the GPL. But that doesn’t mean that everything should be licensed under the GPL. For example, as far as I know, every package of the {tidyverse} uses an MIT license. So just because R is licensed under the GPL doesn’t mean that its packages all have to be GPL. But I must also admit that while I see why a company like Posit releases their packages under a permissive license, I don’t see why an independent developer would do that. I absolutely do not see what independent developers gain from releasing the code of their packages under anything else than the GPL. (As an aside, go read this… code under a permissive license taken from an independent developer? check. Nothing was given back to the community? check. The code in question was used for nefarious purposes? check. Original developer on massive amounts of copium? check). But, to be fair, I have a grand total of two (2) packages on CRAN that likely get less than 10 downloads a year, so what do I know. One of the arguments I’ve heard is that the GPL is not really free, because it restricts users from taking the code and releasing it under a proprietary license, so akshually the MIT/BSD licenses are really the free ones, and if I like freedom so much I should be using FreeBSD instead of a Linux distro and release my packages under a MIT/BSD license. I want to ask people that make this argument if they would allow the Nazi party to make a come back in their countries legislature, then.\n\n\nThat being said, I do release stuff with permissive licenses. For example the content of this blog or for the courses I teach are under the WTFPL, which is, I would say, the only acceptable permissive license for independent developers. If the name of the license was not explicit enough, the comic below illustrates what the WPTFL is all about:"
  },
  {
    "objectID": "posts/2022-10-23-licenses.html#can-r-be-used-to-write-proprietary-code",
    "href": "posts/2022-10-23-licenses.html#can-r-be-used-to-write-proprietary-code",
    "title": "R, its license and my take on it",
    "section": "\nCan R be used to write proprietary code\n",
    "text": "Can R be used to write proprietary code\n\n\nYes, you can write proprietary code using R. Microsoft has done so, for example their {RevoUtilsMath} package is, as far as I know, proprietary, and I’m sure that it includes some R code. I’m pretty sure it would also be possible to even build a proprietary program that would require the R interpreter to be bundled to run. As long as the developers of this tool would:\n\n\n\nRelease their modified version of R with it (if they modified it);\n\n\nTell their users that their program runs with R, and thus also distribute R and its license;\n\n\n\nR could likely be downloaded at install time in cases like this, again, as long as the users get notified that it’s needed. I doubt that the rest of the program would need to be licensed under the GPL, since no code of R itself has been modified.\n\n\nBut I’m not that certain on this last point, so any comments welcome (on here).\n\n\nEDIT: There’s this interesting discussion on stackexchange here and it would seem that the answer is not clearcut, but, it depends. Hence why companies prefer working using permissive licenses, to avoid these types of discussions.\n\n\nThat’s it, that’s the blog post. Thank GNU for the GPL."
  },
  {
    "objectID": "posts/2019-08-17-modern_R.html",
    "href": "posts/2019-08-17-modern_R.html",
    "title": "Modern R with the tidyverse is available on Leanpub",
    "section": "",
    "text": "Yesterday I released an ebook on Leanpub, called Modern R with the tidyverse, which you can also read for free here.\n\n\nIn this blog post, I want to give some context.\n\n\nModern R with the tidyverse is the second ebook I release on Leanpub. I released the first one, called Functional programming and unit testing for data munging with R around Christmas 2016 (I’ve retired it on Leanpub, but you can still read it for free here) . I just had moved back to my home country of Luxembourg and started a new job as a research assistant at the statistical national institute. Since then, lots of things happened; I’ve changed jobs and joined PwC Luxembourg as a data scientist, was promoted to manager, finished my PhD, and most importantly of all, I became a father.\n\n\nThrough all this, I continued blogging and working on a new ebook, called Modern R with the tidyverse. At first, this was supposed to be a separate book from the first one, but as I continued writing, I realized that updating and finishing the first one, would take a lot of effort, and also, that it wouldn’t make much sense in keeping both separated. So I decided to merge the content from the first ebook with the second, and update everything in one go.\n\n\nMy very first notes were around 50 pages if memory serves, and I used them to teach R at the University of Strasbourg while I employed there as a research and teaching assistant and working on my PhD. These notes were the basis of Functional programming and unit testing for data munging with R and now Modern R. Chapter 2 of Modern R is almost a simple copy and paste from these notes (with more sections added). These notes were first written around 2012-2013ish.\n\n\nModern R is the kind of text I would like to have had when I first started playing around with R, sometime around 2009-2010. It starts from the beginning, but also goes quite into details in the later chapters. For instance, the section on modeling with functional programming is quite advanced, but I believe that readers that read through all the book and reached that part would be armed with all the needed knowledge to follow. At least, this is my hope.\n\n\nNow, the book is still not finished. Two chapters are missing, but it should not take me long to finish them as I already have drafts lying around. However, exercises might still be in wrong places, and more are required. Also, generally, more polishing is needed.\n\n\nAs written in the first paragraph of this section, the book is available on Leanpub. Unlike my previous ebook, this one costs money; a minimum price of 4.99$ and a recommended price of 14.99$, but as mentioned you can read it for free online. I’ve hesitated to give it a minimum price of 0$, but I figured that since the book can be read for free online, and that Leanpub has a 45 days return policy where readers can get 100% reimbursed, no questions asked (and keep the downloaded ebook), readers were not taking a lot of risks by buying it for 5 bucks. I sure hope however that readers will find that this ebook is worth at least 5 bucks!\n\n\nNow why should you read it? There’s already a lot of books on learning how to use R. Well, I don’t really want to convince you to read it. But some people do seem to like my style of writing and my blog posts, so I guess these same people, or similar people, might like the ebook. Also, I think that this ebook covers a lot of different topics, enough of them to make you an efficient R user. But as I’ve written in the introduction of Modern R:\n\n\nSo what you can expect from this book is that this book is not the only one you should read.\n\n\nAnyways, hope you’ll enjoy Modern R, suggestions, criticisms and reviews welcome!\n\n\nBy the way, the cover of the book is a painting by John William Waterhouse, depicting Diogenes of Sinope, an ancient Greek philosopher, an absolute mad lad. Read his Wikipedia page, it’s worth it."
  },
  {
    "objectID": "posts/2018-12-15-lubridate_africa.html",
    "href": "posts/2018-12-15-lubridate_africa.html",
    "title": "Manipulate dates easily with {lubridate}",
    "section": "",
    "text": "This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free here. This is taken from Chapter 5, which presents the {tidyverse} packages and how to use them to compute descriptive statistics and manipulate data. In the text below, I scrape a table from Wikipedia, which shows when African countries gained independence from other countries. Then, using {lubridate} functions I show you how you can answers questions such as Which countries gained independence before 1960?."
  },
  {
    "objectID": "posts/2018-12-15-lubridate_africa.html#set-up-scraping-some-data-from-wikipedia",
    "href": "posts/2018-12-15-lubridate_africa.html#set-up-scraping-some-data-from-wikipedia",
    "title": "Manipulate dates easily with {lubridate}",
    "section": "\nSet-up: scraping some data from Wikipedia\n",
    "text": "Set-up: scraping some data from Wikipedia\n\n\n{lubridate} is yet another tidyverse package, that makes dealing with dates or duration data (and intervals) as painless as possible. I do not use every function contained in the package daily, and as such will only focus on some of the functions. However, if you have to deal with dates often, you might want to explore the package thoroughly.\n\n\nLet’s get some data from a Wikipedia table:\n\nlibrary(tidyverse)\nlibrary(rvest)\npage &lt;- read_html(\"https://en.wikipedia.org/wiki/Decolonisation_of_Africa\")\n\nindependence &lt;- page %&gt;%\n    html_node(\".wikitable\") %&gt;%\n    html_table(fill = TRUE)\n\nindependence &lt;- independence %&gt;%\n    select(-Rank) %&gt;%\n    map_df(~str_remove_all(., \"\\\\[.*\\\\]\")) %&gt;%\n    rename(country = `Country[a]`,\n           colonial_name = `Colonial name`,\n           colonial_power = `Colonial power[b]`,\n           independence_date = `Independence date[c]`,\n           first_head_of_state = `First head of state[d]`,\n           independence_won_through = `Independence won through`)\n\nThis dataset was scraped from the following Wikipedia table. It shows when African countries gained independence from which colonial powers. In Chapter 11, I will show you how to scrape Wikipedia pages using R. For now, let’s take a look at the contents of the dataset:\n\nindependence\n## # A tibble: 54 x 6\n##    country colonial_name colonial_power independence_da… first_head_of_s…\n##    &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;            &lt;chr&gt;           \n##  1 Liberia Liberia       United States  26 July 1847     Joseph Jenkins …\n##  2 South … Cape Colony … United Kingdom 31 May 1910      Louis Botha     \n##  3 Egypt   Sultanate of… United Kingdom 28 February 1922 Fuad I          \n##  4 Eritrea Italian Erit… Italy          10 February 1947 Haile Selassie  \n##  5 Libya   British Mili… United Kingdo… 24 December 1951 Idris           \n##  6 Sudan   Anglo-Egypti… United Kingdo… 1 January 1956   Ismail al-Azhari\n##  7 Tunisia French Prote… France         20 March 1956    Muhammad VIII a…\n##  8 Morocco French Prote… France Spain   2 March 19567 A… Mohammed V      \n##  9 Ghana   Gold Coast    United Kingdom 6 March 1957     Kwame Nkrumah   \n## 10 Guinea  French West … France         2 October 1958   Ahmed Sékou Tou…\n## # … with 44 more rows, and 1 more variable: independence_won_through &lt;chr&gt;\n\nas you can see, the date of independence is in a format that might make it difficult to answer questions such as Which African countries gained independence before 1960 ? for two reasons. First of all, the date uses the name of the month instead of the number of the month (well, this is not such a big deal, but still), and second of all the type of the independence day column is character and not “date”. So our first task is to correctly define the column as being of type date, while making sure that R understands that January is supposed to be “01”, and so on."
  },
  {
    "objectID": "posts/2018-12-15-lubridate_africa.html#using-lubridate",
    "href": "posts/2018-12-15-lubridate_africa.html#using-lubridate",
    "title": "Manipulate dates easily with {lubridate}",
    "section": "\nUsing {lubridate}\n",
    "text": "Using {lubridate}\n\n\nThere are several helpful functions included in {lubridate} to convert columns to dates. For instance if the column you want to convert is of the form “2012-11-21”, then you would use the function ymd(), for “year-month-day”. If, however the column is “2012-21-11”, then you would use ydm(). There’s a few of these helper functions, and they can handle a lot of different formats for dates. In our case, having the name of the month instead of the number might seem quite problematic, but it turns out that this is a case that {lubridate} handles painfully:\n\nlibrary(lubridate)\n## \n## Attaching package: 'lubridate'\n## The following object is masked from 'package:base':\n## \n##     date\nindependence &lt;- independence %&gt;%\n  mutate(independence_date = dmy(independence_date))\n## Warning: 5 failed to parse.\n\nSome dates failed to parse, for instance for Morocco. This is because these countries have several independence dates; this means that the string to convert looks like:\n\n\"2 March 1956\n7 April 1956\n10 April 1958\n4 January 1969\"\n\nwhich obviously cannot be converted by {lubridate} without further manipulation. I ignore these cases for simplicity’s sake.\n\n\nLet’s take a look at the data now:\n\nindependence\n## # A tibble: 54 x 6\n##    country colonial_name colonial_power independence_da… first_head_of_s…\n##    &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;          &lt;date&gt;           &lt;chr&gt;           \n##  1 Liberia Liberia       United States  1847-07-26       Joseph Jenkins …\n##  2 South … Cape Colony … United Kingdom 1910-05-31       Louis Botha     \n##  3 Egypt   Sultanate of… United Kingdom 1922-02-28       Fuad I          \n##  4 Eritrea Italian Erit… Italy          1947-02-10       Haile Selassie  \n##  5 Libya   British Mili… United Kingdo… 1951-12-24       Idris           \n##  6 Sudan   Anglo-Egypti… United Kingdo… 1956-01-01       Ismail al-Azhari\n##  7 Tunisia French Prote… France         1956-03-20       Muhammad VIII a…\n##  8 Morocco French Prote… France Spain   NA               Mohammed V      \n##  9 Ghana   Gold Coast    United Kingdom 1957-03-06       Kwame Nkrumah   \n## 10 Guinea  French West … France         1958-10-02       Ahmed Sékou Tou…\n## # … with 44 more rows, and 1 more variable: independence_won_through &lt;chr&gt;\n\nAs you can see, we now have a date column in the right format. We can now answer questions such as Which countries gained independence before 1960? quite easily, by using the functions year(), month() and day(). Let’s see which countries gained independence before 1960:\n\nindependence %&gt;%\n  filter(year(independence_date) &lt;= 1960) %&gt;%\n  pull(country)\n##  [1] \"Liberia\"                          \"South Africa\"                    \n##  [3] \"Egypt\"                            \"Eritrea\"                         \n##  [5] \"Libya\"                            \"Sudan\"                           \n##  [7] \"Tunisia\"                          \"Ghana\"                           \n##  [9] \"Guinea\"                           \"Cameroon\"                        \n## [11] \"Togo\"                             \"Mali\"                            \n## [13] \"Madagascar\"                       \"Democratic Republic of the Congo\"\n## [15] \"Benin\"                            \"Niger\"                           \n## [17] \"Burkina Faso\"                     \"Ivory Coast\"                     \n## [19] \"Chad\"                             \"Central African Republic\"        \n## [21] \"Republic of the Congo\"            \"Gabon\"                           \n## [23] \"Mauritania\"\n\nYou guessed it, year() extracts the year of the date column and converts it as a numeric so that we can work on it. This is the same for month() or day(). Let’s try to see if countries gained their independence on Christmas Eve:\n\nindependence %&gt;%\n  filter(month(independence_date) == 12,\n         day(independence_date) == 24) %&gt;%\n  pull(country)\n## [1] \"Libya\"\n\nSeems like Libya was the only one! You can also operate on dates. For instance, let’s compute the difference between two dates, using the interval() column:\n\nindependence %&gt;%\n  mutate(today = lubridate::today()) %&gt;%\n  mutate(independent_since = interval(independence_date, today)) %&gt;%\n  select(country, independent_since)\n## # A tibble: 54 x 2\n##    country      independent_since             \n##    &lt;chr&gt;        &lt;S4: Interval&gt;                \n##  1 Liberia      1847-07-26 UTC--2019-02-10 UTC\n##  2 South Africa 1910-05-31 UTC--2019-02-10 UTC\n##  3 Egypt        1922-02-28 UTC--2019-02-10 UTC\n##  4 Eritrea      1947-02-10 UTC--2019-02-10 UTC\n##  5 Libya        1951-12-24 UTC--2019-02-10 UTC\n##  6 Sudan        1956-01-01 UTC--2019-02-10 UTC\n##  7 Tunisia      1956-03-20 UTC--2019-02-10 UTC\n##  8 Morocco      NA--NA                        \n##  9 Ghana        1957-03-06 UTC--2019-02-10 UTC\n## 10 Guinea       1958-10-02 UTC--2019-02-10 UTC\n## # … with 44 more rows\n\nThe independent_since column now contains an interval object that we can convert to years:\n\nindependence %&gt;%\n  mutate(today = lubridate::today()) %&gt;%\n  mutate(independent_since = interval(independence_date, today)) %&gt;%\n  select(country, independent_since) %&gt;%\n  mutate(years_independent = as.numeric(independent_since, \"years\"))\n## # A tibble: 54 x 3\n##    country      independent_since              years_independent\n##    &lt;chr&gt;        &lt;S4: Interval&gt;                             &lt;dbl&gt;\n##  1 Liberia      1847-07-26 UTC--2019-02-10 UTC             172. \n##  2 South Africa 1910-05-31 UTC--2019-02-10 UTC             109. \n##  3 Egypt        1922-02-28 UTC--2019-02-10 UTC              97.0\n##  4 Eritrea      1947-02-10 UTC--2019-02-10 UTC              72  \n##  5 Libya        1951-12-24 UTC--2019-02-10 UTC              67.1\n##  6 Sudan        1956-01-01 UTC--2019-02-10 UTC              63.1\n##  7 Tunisia      1956-03-20 UTC--2019-02-10 UTC              62.9\n##  8 Morocco      NA--NA                                      NA  \n##  9 Ghana        1957-03-06 UTC--2019-02-10 UTC              61.9\n## 10 Guinea       1958-10-02 UTC--2019-02-10 UTC              60.4\n## # … with 44 more rows\n\nWe can now see for how long the last country to gain independence has been independent. Because the data is not tidy (in some cases, an African country was colonized by two powers, see Libya), I will only focus on 4 European colonial powers: Belgium, France, Portugal and the United Kingdom:\n\nindependence %&gt;%\n  filter(colonial_power %in% c(\"Belgium\", \"France\", \"Portugal\", \"United Kingdom\")) %&gt;%\n  mutate(today = lubridate::today()) %&gt;%\n  mutate(independent_since = interval(independence_date, today)) %&gt;%\n  mutate(years_independent = as.numeric(independent_since, \"years\")) %&gt;%\n  group_by(colonial_power) %&gt;%\n  summarise(last_colony_independent_for = min(years_independent, na.rm = TRUE))\n## # A tibble: 4 x 2\n##   colonial_power last_colony_independent_for\n##   &lt;chr&gt;                                &lt;dbl&gt;\n## 1 Belgium                               56.6\n## 2 France                                41.6\n## 3 Portugal                              43.2\n## 4 United Kingdom                        42.6\n\n{lubridate} contains many more functions. If you often work with dates, duration or interval data, {lubridate} is a package that you have to master."
  },
  {
    "objectID": "posts/2021-03-28-survey.html",
    "href": "posts/2021-03-28-survey.html",
    "title": "The link between keyboard layouts and typing speed - Data collection phase",
    "section": "",
    "text": "I’m curious about different keyboard layouts, and how that correlates with typing speed (if at all). I prepared a little 2 minute survey, and would be very grateful if you could take it. You can find it on this link.\n\n\nThere’s no R code in this blog post, but I’ll be analyzing the data using R, promise :)"
  },
  {
    "objectID": "posts/2018-09-11-human_to_machine.html",
    "href": "posts/2018-09-11-human_to_machine.html",
    "title": "Going from a human readable Excel file to a machine-readable csv with {tidyxl}",
    "section": "",
    "text": "I won’t write a very long introduction; we all know that Excel is ubiquitous in business, and that it has a lot of very nice features, especially for business practitioners that do not know any programming. However, when people use Excel for purposes it was not designed for, it can be a hassle. Often, people use Excel as a reporting tool, which it is not; they create very elaborated and complicated spreadsheets that are human readable, but impossible to import within any other tool.\n\n\nIn this blog post (which will probably be part of a series), I show you how you can go from this:\n\n\n\n\n\nto this:\n\n\n\n\n\nYou can find the data I will use here. Click on the “Time use” folder and you can download the workbook.\n\n\nThe Excel workbook contains several sheets (in French and English) of the amount of time Luxembourguish citizens spend from Monday to Sunday. For example, on average, people that are in employment spend almost 8 hours sleeping during the week days, and 8:45 hours on Saturday.\n\n\nAs you can see from the screenshot, each sheet contains several tables that have lots of headers and these tables are next to one another. Trying to import these sheets with good ol’ readxl::read_excel() produces a monster.\n\n\nThis is where {tidyxl} comes into play. Let’s import the workbook with {tidyxl}:\n\nlibrary(tidyverse)\nlibrary(tidyxl)\n\ntime_use_xl &lt;- xlsx_cells(\"time-use.xlsx\")\n\nLet’s see what happened:\n\nhead(time_use_xl)\n## # A tibble: 6 x 21\n##   sheet address   row   col is_blank data_type error logical numeric\n##   &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;lgl&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;     &lt;dbl&gt;\n## 1 Index A1          1     1 FALSE    character &lt;NA&gt;  NA           NA\n## 2 Index B1          1     2 TRUE     blank     &lt;NA&gt;  NA           NA\n## 3 Index C1          1     3 TRUE     blank     &lt;NA&gt;  NA           NA\n## 4 Index D1          1     4 TRUE     blank     &lt;NA&gt;  NA           NA\n## 5 Index E1          1     5 TRUE     blank     &lt;NA&gt;  NA           NA\n## 6 Index F1          1     6 TRUE     blank     &lt;NA&gt;  NA           NA\n## # … with 12 more variables: date &lt;dttm&gt;, character &lt;chr&gt;,\n## #   character_formatted &lt;list&gt;, formula &lt;chr&gt;, is_array &lt;lgl&gt;,\n## #   formula_ref &lt;chr&gt;, formula_group &lt;int&gt;, comment &lt;chr&gt;, height &lt;dbl&gt;,\n## #   width &lt;dbl&gt;, style_format &lt;chr&gt;, local_format_id &lt;int&gt;\n\nAs you can see, the sheet was imported, but the result might be unexpected. Actually, time_use_xl is a tibble object, where each row is one cell of the Excel sheet. This might seem very complicated to handle, but you will see that it actually makes things way easier.\n\n\nI only want to work on the English sheets so I use the following code to ignore the French ones:\n\nsheets &lt;- xlsx_sheet_names(\"time-use.xlsx\") %&gt;%\n    keep(grepl(pattern = \".*day$\", .))\n\nAlso, there’s a sheet that aggregates the results for week days and weekends, which I also ignore.\n\n\nNow, to extract the tables from each sheet I wrote the following function:\n\nextract_data &lt;- function(sheet){\n    activities &lt;- sheet %&gt;%\n        filter(col == 2) %&gt;%\n        select(row, character) %&gt;%\n        filter(row %in% seq(6,58)) %&gt;%\n        rename(activities = character) %&gt;%\n        select(-row)\n    \n    cols_to_extract &lt;- sheet %&gt;% \n        filter(grepl(\"Population who completed.*\", character)) %&gt;% \n        pull(col)\n    \n    headers_pos &lt;- cols_to_extract - 1\n    \n    headers &lt;- sheet %&gt;%\n        filter(col %in% headers_pos, row == 3) %&gt;%\n        pull(character)\n    \n    cols_to_extract %&gt;% \n        map(~filter(sheet, col %in% .)) %&gt;%\n        map(~select(., sheet, address, row, col, character)) %&gt;%\n        map(~filter(., row %in% seq(6,58))) %&gt;%\n        map(~select(., character)) %&gt;%\n        map2(.x = ., .y = headers, ~mutate(.x, \"population\" = .y)) %&gt;%\n        map(., ~bind_cols(activities, .)) %&gt;%\n        bind_rows()\n}\n\nLet’s study it step by step and see how it works. First, there’s the argument, sheet. This function will be mapped to each sheet of the workbook. Then, the first block I wrote, extracts the activities:\n\n    activities &lt;- sheet %&gt;%\n        filter(col == 2) %&gt;%\n        select(row, character) %&gt;%\n        filter(row %in% seq(6,58)) %&gt;%\n        rename(activities = character) %&gt;%\n        select(-row)\n\nI only keep the second column (filter(col == 2)); col is a column of the tibble and if you look inside the workbook, you will notice that the activities are on the second column, or the B column. Then, I select two columns, the row and the character column. row is self-explanatory and character actually contains whatever is written inside the cells. Then, I only keep rows 6 to 58, because that is what interests me; the rest is either empty cells, or unneeded. Finally, I rename the character column to activities and remove the row column.\n\n\nThe second block:\n\n    cols_to_extract &lt;- sheet %&gt;% \n        filter(grepl(\"Population who completed.*\", character)) %&gt;% \n        pull(col)\n\nreturns the index of the columns I want to extract. I am only interested in the people that have completed the activities, so using grepl() inside filter(), I located these columns, and use pull()… to pull them out of the data frame! cols_to_extract is thus a nice atomic vector of columns that I want to keep.\n\n\nIn the third block, I extract the headers:\n\n    headers_pos &lt;- cols_to_extract - 1\n\nWhy - 1? This is because if you look in the Excel, you will see that the headers are one column before the column labeled “People who completed the activity”. For example on column G, I have “People who completed the activity” and on column F I have the header, in this case “Male”.\n\n\nNow I actually extract the headers:\n\n    headers &lt;- sheet %&gt;%\n        filter(col %in% headers_pos, row == 3) %&gt;%\n        pull(character)\n\nHeaders are always on the third row, but on different columns, hence the col %in% headers_pos. I then pull out the values inside the cells with pull(character). So my headers object will be an atomic vector with “All”, “Male”, “Female”, “10 - 19 years”, etc… everything on row 3.\n\n\nFinally, the last block, actually extracts the data:\n\n    cols_to_extract %&gt;% \n        map(~filter(sheet, col %in% .)) %&gt;%\n        map(~select(., sheet, address, row, col, character)) %&gt;%\n        map(~filter(., row %in% seq(6,58))) %&gt;%\n        map(~select(., character)) %&gt;%\n        map2(.x = ., .y = headers, ~mutate(.x, \"population\" = .y)) %&gt;%\n        map(., ~bind_cols(activities, .)) %&gt;%\n        bind_rows()\n\ncols_to_extract is a vector with the positions of the columns that interest me. So for example “4”, “7”, “10” and so on. I map this vector to the sheet, which returns me a list of extracted data frames. I pass this down to a select() (which is inside map()… why? Because the input parameter is a list of data frames). So for each data frame inside the list, I select the columns sheet, address, row, col and character. Then, for each data frame inside the list, I use filter() to only keep the rows from position 6 to 58. Then, I only select the character column, which actually contains the text inside the cell. Then, using map2(), I add the values inside the headers object as a new column, called population. Then, I bind the activities column to the data frame and bind all the rows together.\n\n\nTime to use this function! Let’s see:\n\nclean_data &lt;- sheets %&gt;%\n    map(~filter(time_use_xl, sheet %in% .)) %&gt;%\n    set_names(sheets) %&gt;%\n    map(extract_data) %&gt;%\n    map2(.x = ., .y = sheets, ~mutate(.x, \"day\" = .y)) %&gt;%\n    bind_rows() %&gt;%\n    select(day, population, activities, time = character)\n\nglimpse(clean_data)\n## Observations: 2,968\n## Variables: 4\n## $ day        &lt;chr&gt; \"Year 2014_Monday til Friday\", \"Year 2014_Monday til …\n## $ population &lt;chr&gt; \"All\", \"All\", \"All\", \"All\", \"All\", \"All\", \"All\", \"All…\n## $ activities &lt;chr&gt; \"Personal care\", \"Sleep\", \"Eating\", \"Other personal c…\n## $ time       &lt;chr&gt; \"11:07\", \"08:26\", \"01:47\", \"00:56\", \"07:37\", \"07:47\",…\n\nSo I map my list of sheets to the tibble I imported with readxl, use set_names to name the elements of my list (which is superfluous, but I wanted to show this; might interest you!) and then map this result to my little function. I could stop here, but I then add a new column to each data frame that contains the day on which the data was measured, bind the rows together and reorder the columns. Done!\n\n\nNow, how did I come up with this function? I did not start with a function. I started by writing some code that did what I wanted for one table only, inside one sheet only. Only when I got something that worked, did I start to generalize to several tables and then to several sheets. Most of the time spent was actually in trying to find patterns in the Excel sheet that I could use to write my function (for example noticing that the headers I wanted where always one column before the column I was interested in). This is my advice when working with function programming; always solve the issue for one element, wrap this code inside a function, and then simply map this function to a list of elements!"
  },
  {
    "objectID": "posts/2019-02-04-newspapers_shiny_app_tutorial.html#introduction",
    "href": "posts/2019-02-04-newspapers_shiny_app_tutorial.html#introduction",
    "title": "Building a shiny app to explore historical newspapers: a step-by-step guide",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nI started off this year by exploring a world that was unknown to me, the world of historical newspapers. I did not know that historical newspapers data was a thing, and have been thoroughly enjoying myself exploring the different datasets published by the National Library of Luxembourg. You can find the data here.\n\n\nIn my first blog post, I analyzed data from L’indépendence Luxembourgeoise. I focused on the ads, which were for the most part in the 4th and last page of the newspaper. I did so by extracting the data from the ALTO files. ALTO files contain the content of the newspapers, (basically, the words that make up the article). For this first exercise, I disregarded the METS files, for two reasons. First, I simply wanted to have something quick, and get used to the data. And second, I did not know about ALTO and METS files enough to truly make something out of them. The problem of disregarding the METS file is that I only had a big dump of words, and did not know which words came from which article, or ad in this case.\n\n\nIn the second blog post), I extracted data from the L’Union newspaper, this time by using the metadata from the METS files too. By combining the data from the ALTO files with the metadata from the METS files, I know which words came from which article, which would make further analysis much more interesting.\n\n\nIn the third blog post of this series, I built a Shiny app which makes it easy to explore the 10 years of publications of L’Union. In this blog post, I will explain in great detail how I created this app."
  },
  {
    "objectID": "posts/2019-02-04-newspapers_shiny_app_tutorial.html#part-1-getting-the-data-ready-for-the-shiny-app",
    "href": "posts/2019-02-04-newspapers_shiny_app_tutorial.html#part-1-getting-the-data-ready-for-the-shiny-app",
    "title": "Building a shiny app to explore historical newspapers: a step-by-step guide",
    "section": "\nPart 1: Getting the data ready for the Shiny app\n",
    "text": "Part 1: Getting the data ready for the Shiny app\n\n\n\nStep 1: Extracting the needed data\n\n\nIf you want to follow along with a dataset from a single publication, you can download the following archive on dropbox. Extract this archive, and you will find the data exactly as you would get it from the the big archive you can download from the website of the National Library of Luxembourg. However, to keep the size of the archive small, I removed the .pdf and .jpeg scans.\n\n\nIn the second blog post) I wrote some functions that made extracting the needed data from the files easy. However, after I wrote the article, I noticed that in some cases these functions were not working exactly as intended. I rewrote them a little bit to overcome these issues. You can find the code I used right below. I won’t explain it too much, because you can read the details in the previous blog post. However, should something be unclear, just drop me an email or a tweet!\n\n\n\n\nClick if you want to see the code\n\n\n# This functions will be used within the next functions to extract the relevant pieces\n\nextractor &lt;- function(string, regex, all = FALSE){\n    if(all) {\n        string %&gt;%\n            str_extract_all(regex) %&gt;%\n            flatten_chr() %&gt;%\n            str_remove_all(\"=|\\\\\\\"\") %&gt;%\n            #str_extract_all(\"[:alnum:]+|.|,|\\\\?|!\", simplify = FALSE) %&gt;%\n            map(paste, collapse = \"\") %&gt;%\n            flatten_chr()\n    } else {\n        string %&gt;%\n            str_extract(regex) %&gt;%\n            str_remove_all(\"=|\\\\\\\"\") %&gt;%\n            #str_extract_all(\"[:alnum:]+|.|,|\\\\?|!\", simplify = TRUE) %&gt;%\n            paste(collapse = \" \") %&gt;%\n            tolower()\n    }\n}\n\n# This function extracts the data from the METS files, and returns a tibble:\n\nextract_mets &lt;- function(article){\n    id &lt;- article %&gt;%\n        extractor(\"(?&lt;=ID)(.*?)(?=LABEL)\")\n\n    label &lt;- article %&gt;%\n        extractor(\"(?&lt;=LABEL)(.*?)(?=TYPE)\")\n\n    type &lt;- article %&gt;%\n        extractor(\"(?&lt;=TYPE)(.*?)(?=&gt;)\")\n\n    begins &lt;- article %&gt;%\n        extractor(\"(?&lt;=BEGIN)(.*?)(?=BETYPE)\", all = TRUE)\n\n    tibble::tribble(~label, ~type, ~begins, ~id,\n                    label, type, begins, id) %&gt;%\n        unnest()\n}\n\n# This function extracts the data from the ALTO files, and also returns a tibble:\n\nextract_alto &lt;- function(article){\n    begins &lt;- article[1] %&gt;%\n        extractor(\"(?&lt;=^ID)(.*?)(?=HPOS)\", all = TRUE)\n\n    content &lt;- article %&gt;%\n        extractor(\"(?&lt;=CONTENT)(.*?)(?=WC)\", all = TRUE)\n\n    tibble::tribble(~begins, ~content,\n                    begins, content) %&gt;%\n        unnest()\n}\n\n# This function takes the path to a page as an argument, and extracts the data from \n# each article using the function defined above. It then writes a flat CSV to disk.\n\nalto_csv &lt;- function(page_path){\n\n    page &lt;- read_file(page_path)\n\n    doc_name &lt;- str_extract(page_path, \"(?&lt;=/text/).*\")\n\n    alto_articles &lt;- page %&gt;%\n        str_split(\"TextBlock \") %&gt;%\n        flatten_chr()\n\n    alto_df &lt;- map_df(alto_articles, extract_alto)\n\n    alto_df &lt;- alto_df %&gt;%\n        mutate(document = doc_name)\n\n    write_csv(alto_df, paste0(page_path, \".csv\"))\n}\n\n# Same as above, but for the METS file:\n\nmets_csv &lt;- function(page_path){\n\n    page &lt;- read_file(page_path)\n\n    doc_name &lt;- str_extract(page_path, \"(?&lt;=/).*\")\n\n    mets_articles &lt;- page %&gt;%\n        str_split(\"DMDID\") %&gt;%\n        flatten_chr()\n\n    mets_df &lt;- map_df(mets_articles, extract_mets)\n\n    mets_df &lt;- mets_df %&gt;%\n        mutate(document = doc_name)\n\n    write_csv(mets_df, paste0(page_path, \".csv\"))\n}\n\n# Time to use the above defined functions. First, let's save the path of all the ALTO files\n# into a list:\n\npages_alto &lt;- str_match(list.files(path = \"./\", all.files = TRUE, recursive = TRUE), \".*/text/.*.xml\") %&gt;%\n    discard(is.na)\n\n# I use the {furrr} library to do the extraction in parallel, using 8 cores:\n\nlibrary(furrr)\n\nplan(multiprocess, workers = 8)\n\ntic &lt;- Sys.time()\nfuture_map(pages_alto, alto_csv)\ntoc &lt;- Sys.time()\n\ntoc - tic\n\n#Time difference of 18.64776 mins\n\n\n# Same for the METS files:\n\npages_mets &lt;- str_match(list.files(path = \"./\", all.files = TRUE, recursive = TRUE), \".*mets.xml\") %&gt;%\n    discard(is.na)\n\n\nlibrary(furrr)\n\nplan(multiprocess, workers = 8)\n\ntic &lt;- Sys.time()\nfuture_map(pages_mets, mets_csv)\ntoc &lt;- Sys.time()\n\ntoc - tic\n\n#Time difference of 18.64776 mins\n\n\nIf you want to try the above code for one ALTO and METS files, you can use the following lines (use the download link in the beginning of the blog post to get the required data):\n\n\n\n\nClick if you want to see the code\n\n\nmets &lt;- read_file(\"1533660_newspaper_lunion_1860-11-14/1533660_newspaper_lunion_1860-11-14-mets.xml\")\n\nmets_articles2 &lt;- mets %&gt;%\n    str_split(\"DMDID\") %&gt;%\n    flatten_chr()\n\n\nalto &lt;- read_file(\"1533660_newspaper_lunion_1860-11-14/text/1860-11-14_01-00001.xml\")\n\nalto_articles &lt;- alto %&gt;%\n    str_split(\"TextBlock \") %&gt;%\n    flatten_chr()\n\nmets_df2 &lt;- mets_articles2 %&gt;%\n    map_df(extract_mets)\n\n# Same exercice for ALTO\n\nalto_df &lt;- alto_articles %&gt;%\n    map_df(extract_alto)\n\n\n\n\nStep 2: Joining the data and the metadata\n\n\nNow that I extracted the data from the ALTO files, and the metadata from the METS files, I still need to join both data sets and do some cleaning. What is the goal of joining these two sources? Remember, by doing this I will know which words come from which article, which will make things much easier later on. I explain how the code works as comments in the code block below:\n\n\n\n\nClick if you want to see the code\n\n\nlibrary(tidyverse)\nlibrary(udpipe)\nlibrary(textrank)\nlibrary(tidytext)\n\n# First, I need the path to each folder that contains the ALTO and METS files. Each newspaper\n# data is inside its own folder, one folder per publication. Inside, there's `text` folder that\n# contains the ALTO and METS files. This is also where I saved the .csv files from before.\n\npathdirs &lt;- list.dirs(recursive = FALSE) %&gt;%\n    str_match(\".*lunion.*\") %&gt;%\n    discard(is.na)\n\n# The following function imports the METS and the ALTO csv files, joins them, and does some \n# basic cleaning. I used a trick to detect German articles (even though L'Union is a French publication\n# some articles are in German) and then remove them.\n\ntidy_papers &lt;- function(path){\n    mets_path &lt;- paste0(path, \"/\", list.files(path, \".*.xml.csv\"))\n    mets_csv &lt;- data.table::fread(mets_path)\n\n    alto_path &lt;- paste0(path, \"/text/\", list.files(paste0(path, \"/text/\"), \".*.csv\"))\n    alto_csv &lt;- map_dfr(alto_path, data.table::fread)\n\n    final &lt;- full_join(alto_csv, mets_csv, by = \"begins\") %&gt;%\n        mutate(content = tolower(content)) %&gt;%\n        mutate(content = if_else(str_detect(content, \"hyppart1\"), str_extract_all(content, \"(?&lt;=CONTENT_).*\", simplify = TRUE), content)) %&gt;%\n        mutate(content = if_else(str_detect(content, \"hyppart2\"), NA_character_, content)) %&gt;%\n        # When words are separated by a hyphen and split over two lines, it looks like this in the data.\n        # ex SUBS_TYPEHypPart1 SUBS_CONTENTexceptée\n        # ceptée SUBS_TYPEHypPart2 SUBS_CONTENTexceptée\n        # Here, the word `exceptée` is split over two lines, so using a regular expression, I keep\n        # the string `exceptée`, which comes after the string `CONTENT`,  from the first line and \n        # replace the second line by an NA_character_\n        mutate(content = if_else(str_detect(content, \"superscript\"), NA_character_, content)) %&gt;%\n        mutate(content = if_else(str_detect(content, \"subscript\"), NA_character_, content)) %&gt;%\n        filter(!is.na(content)) %&gt;%\n        filter(type == \"article\") %&gt;%\n        group_by(id) %&gt;%\n        nest %&gt;%\n        # Below I create a list column with all the content of the article in a single string.\n        mutate(article_text = map(data, ~paste(.$content, collapse = \" \"))) %&gt;%\n        mutate(article_text = as.character(article_text)) %&gt;%\n        # Detecting and removing german articles\n        mutate(german = str_detect(article_text, \"wenn|wird|und\")) %&gt;%\n        filter(german == FALSE) %&gt;%\n        select(-german) %&gt;%\n        # Finally, creating the label of the article (the title), and removing things that are \n        # not articles, such as the daily feuilleton.\n        mutate(label = map(data, ~`[`(.$label, 1))) %&gt;%\n        filter(!str_detect(label, \"embranchement|ligne|bourse|abonnés|feuilleton\")) %&gt;%\n        filter(label != \"na\")\n\n    # Save the data in the rds format, as it is not a flat file\n    saveRDS(final, paste0(path, \"/\", str_sub(path, 11, -1), \".rds\"))\n}\n\n# Here again, I do this in parallel\n\nlibrary(furrr)\n\nplan(multiprocess, workers = 8)\n\nfuture_map(pathdirs, tidy_papers)\n\n\nThis is how one of these files looks like, after passing through this function:\n\n\n\n\n\nOne line is one article. The first column is the id of the article, the second column contains a data frame, the text of the article and finally the title of the article. Let’s take a look at the content of the first element of the data column:\n\n\n\n\n\nThis is the result of the merger of the METS and ALTO csv files. The first column is the id of the article, the second column contains each individual word of the article, the label column the label, or title of the article.\n\n\n\n\nStep 3: Part-of-speech annotation\n\n\nPart-of-speech annotation is a technique with the aim of assigning to each word its part of speech. Basically, Pos annotation tells us whether a word is a verb, a noun, an adjective… This will be quite useful for the analysis. To perform Pos annotation, you need to install the {udpipe} package, and download the pre-trained model for the language you want to annotate, in my case French:\n\n\n\n\nClick if you want to see the code\n\n\n# Only run this once. This downloads the model for French\nudpipe_download_model(language = \"french\")\n\n# Load the model\nudmodel_french &lt;- udpipe_load_model(file = 'french-gsd-ud-2.3-181115.udpipe')\n\n# Save the path of the files to annotate in a list:\npathrds &lt;- list.files(path = \"./\", all.files = TRUE, recursive = TRUE) %&gt;% \n  str_match(\".*.rds\") %&gt;%\n  discard(is.na)\n\nannotate_rds &lt;- function(path, udmodel){\n\n    newspaper &lt;- readRDS(path)\n\n    s &lt;- udpipe_annotate(udmodel, newspaper$article_text, doc_id = newspaper$label)\n    x &lt;- data.frame(s)\n\n    saveRDS(x, str_replace(path, \".rds\", \"_annotated.rds\"))\n}\n\nlibrary(furrr)\nplan(multiprocess, workers = 8)\ntic &lt;- Sys.time()\nfuture_map(pathrds, annotate_rds, udmodel = udmodel_french)\ntoc &lt;- Sys.time()\ntoc - tic\n\n\nAnd here is the result:\n\n\n\n\n\nThe upos column contains the tags. Now I know which words are nouns, verbs, adjectives, stopwords… Meaning that I can easily focus on the type of words that interest me. Plus, as an added benefit, I can focus on the lemma of the words. For example, the word viennent, is the conjugated form of the verb venir. venir is thus the lemma of viennent. This means that I can focus my analysis on lemmata. This is useful, because if I compute the frequency of words, viennent would be different from venir, which is not really what we want.\n\n\n\n\nStep 4: tf-idf\n\n\nJust like what I did in my first blog post, I compute the tf-idf of words. The difference, is that here the “document” is the article. This means that I will get the most frequent words inside each article, but who are at the same time rare in the other articles. Doing this ensures that I will only get very relevant words for each article.\n\n\nIn the lines below, I prepare the data to then make the plots. The files that are created using the code below are available in the following Github link.\n\n\nIn the Shiny app, I read the data directly from the repo. This way, I can keep the app small in size.\n\n\n\n\nClick if you want to see the code\n\n\npath_annotatedrds &lt;- list.files(path = \"./\", all.files = TRUE, recursive = TRUE) %&gt;% str_match(\".*_annotated.rds\") %&gt;%\n    discard(is.na)\n\nprepare_tf_idf &lt;- function(path){\n\n    annotated_newspaper &lt;- readRDS(path)\n\n    tf_idf_data &lt;- annotated_newspaper %&gt;%\n        filter(upos %in% c(\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\")) %&gt;%\n        filter(nchar(lemma) &gt; 3) %&gt;%\n        count(doc_id, lemma) %&gt;%\n        bind_tf_idf(lemma, doc_id, n) %&gt;%\n        arrange(desc(tf_idf)) %&gt;%\n        group_by(doc_id)\n\n    name_tf_idf_data &lt;- str_split(path, \"/\", simplify = 1)[1] %&gt;%\n        paste0(\"_tf_idf_data.rds\")  %&gt;%\n        str_sub(start = 9, -1)\n\n    saveRDS(tf_idf_data, paste0(\"tf_idf_data/\", name_tf_idf_data))\n}\n\nlibrary(furrr)\nplan(multiprocess, workers = 8)\n\nfuture_map(path_annotatedrds, prepare_tf_idf)\n\n\n\n\nStep 5: Summarizing articles by extracting the most relevant sentences, using {textrank}\n\n\nThe last step in data preparation is to extract the most relevant sentences of each articles, using the {textrank} package. This packages implements the PageRank algorithm developed by Larry Page and Sergey Brin in 1995. This algorithm ranks pages by the number of links that point to the pages; the most popular and important pages are also the ones with more links to them. A similar approach is used by the implementation of {textrank}. The algorithm is explained in detail in the following paper.\n\n\nHowever, I cannot simply apply {textrank} to the annotated data frame as it is. Because I have several articles, I have to run the textrank_sentences() function, which extracts the relevant sentences, article by article. For this I still need to transform the data set and also need to prepare the data in a way that makes it digestible by the function. I will not explain the code below line by line, since the documentation of the package is quite straightforward. However, keep in mind that I have to run the textrank_sentences() function for each article, which explains that as some point I use the following:\n\ngroup_by(doc_id) %&gt;%\n    nest() %&gt;%\n\nwhich then makes it easy to work by article (doc_id is the id of the articles). This part is definitely the most complex, so if you’re interested in the methodology described here, really take your time to understand this function. Let me know if I can clarify things!\n\n\n\n\nClick if you want to see the code\n\n\nlibrary(textrank)\nlibrary(brotools)\n\npath_annotatedrds &lt;- list.files(path = \"./\", all.files = TRUE, recursive = TRUE) %&gt;% str_match(\".*_annotated.rds\") %&gt;%\n    discard(is.na)\n\nprepare_textrank &lt;- function(path){\n\n    annotated_newspaper &lt;- readRDS(path)\n\n    # sentences summary\n    x_text_rank &lt;- annotated_newspaper %&gt;%\n        group_by(doc_id) %&gt;%\n        nest() %&gt;%\n        mutate(textrank_id = map(data, ~unique_identifier(., c(\"paragraph_id\", \"sentence_id\")))) %&gt;%\n        mutate(cleaned = map2(.x = data, .y = textrank_id, ~cbind(.x, \"textrank_id\" = .y))) %&gt;%\n        select(doc_id, cleaned)\n\n    x_text_rank2 &lt;- x_text_rank %&gt;%\n        mutate(sentences = map(cleaned, ~select(., textrank_id, sentence))) %&gt;%\n        # one_row() is a function from my own package, which eliminates duplicates rows\n        # from a data frame\n        mutate(sentences = map(sentences, ~one_row(., c(\"textrank_id\", \"sentence\"))))\n\n    x_terminology &lt;- x_text_rank %&gt;%\n        mutate(terminology = map(cleaned, ~filter(., upos %in% c(\"NOUN\", \"ADJ\")))) %&gt;%\n        mutate(terminology = map(terminology, ~select(., textrank_id, \"lemma\"))) %&gt;%\n        select(terminology)\n\n    x_final &lt;- bind_cols(x_text_rank2, x_terminology)\n\n    possibly_textrank_sentences &lt;- possibly(textrank_sentences, otherwise = NULL)\n\n    x_final &lt;- x_final %&gt;%\n        mutate(summary = map2(sentences, terminology, possibly_textrank_sentences)) %&gt;%\n        select(doc_id, summary)\n\n    name_textrank_data &lt;- str_split(path, \"/\", simplify = 1)[1] %&gt;%\n        paste0(\"_textrank_data.rds\") %&gt;%\n        str_sub(start = 9, -1)\n\n    saveRDS(x_final, paste0(\"textrank_data/\", name_textrank_data))\n}\n\nlibrary(furrr)\nplan(multiprocess, workers = 8)\n\nfuture_map(path_annotatedrds, prepare_textrank)\n\n\nYou can download the annotated data sets from the following link. This is how the data looks like:\n\n\n\n\n\nUsing the summary() function on an element of the summary column returns the 5 most relevant sentences as extracted by {textrank}."
  },
  {
    "objectID": "posts/2019-02-04-newspapers_shiny_app_tutorial.html#part-2-building-the-shiny-app",
    "href": "posts/2019-02-04-newspapers_shiny_app_tutorial.html#part-2-building-the-shiny-app",
    "title": "Building a shiny app to explore historical newspapers: a step-by-step guide",
    "section": "\nPart 2: Building the shiny app\n",
    "text": "Part 2: Building the shiny app\n\n\nThe most difficult parts are behind us! Building a dashboard is quite easy thanks to the {flexdashboard} package. You need to know Markdown and some Shiny, but it’s way easier than building a complete Shiny app. First of all, install the {fleshdashboard} package, and start from a template, or from this list of layouts.\n\n\nI think that the only trick worth mentioning is that I put the data in a Github repo, and read it directly from the Shiny app. Users choose a date, which I save in a reactive variable. I then build the right url that points towards the right data set, and read it:\n\npath_tf_idf &lt;- reactive({\n    paste0(\"https://raw.githubusercontent.com/b-rodrigues/newspapers_shinyapp/master/tf_idf_data/newspaper_lunion_\", as.character(input$date2), \"_tf_idf_data.rds\")\n})\n\ndfInput &lt;- reactive({\n        read_rds(url(path_tf_idf())) %&gt;%\n        top_n(as.numeric(input$tf_df_words), tf_idf) %&gt;%\n        mutate(word = reorder(lemma, tf_idf)) \n})\n\nBecause I did all the computations beforehand, the app simply reads the data and creates the bar plots for the tf-idf data, or prints the sentences for the textrank data. To print the sentences correcly, I had to use some html tags, using the {htmltools} package. Below you can find the source code of the app:\n\n\n\n\nClick if you want to see the code\n\n\n---\ntitle: \"Exploring 10 years of daily publications of the Luxembourguish newspaper, *L'Union*\"\noutput: \n  flexdashboard::flex_dashboard:\n    theme: yeti\n    orientation: columns\n    vertical_layout: fill\nruntime: shiny\n\n---\n\n`` `{r setup, include=FALSE}\nlibrary(flexdashboard)\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(textrank)\nlibrary(tidytext)\nlibrary(udpipe)\nlibrary(plotly)\nlibrary(ggthemes)\n`` `\n\nSidebar {.sidebar}\n=====================================\n\n`` `{r}\ndateInput('date2',\n      label = paste('Select date'),\n      value = as.character(as.Date(\"1860-11-14\")),\n      min = as.Date(\"1860-11-12\"), max = as.Date(\"1869-12-31\"),\n      format = \"yyyy/mm/dd\",\n      startview = 'year', language = 'en-GB', weekstart = 1\n    )\nselectInput(inputId = \"tf_df_words\", \n            label = \"Select number of unique words for tf-idf\", \n            choices = seq(1:10),\n            selected = 5)\nselectInput(inputId = \"textrank_n_sentences\", \n            label = \"Select the number of sentences for the summary of the article\", \n            choices = seq(1:20), \n            selected = 5)\n`` `\n\n*The BnL has digitised over 800.000 pages of Luxembourg newspapers. From those, more than 700.000 \npages have rich metadata using international XML standards such as METS and ALTO. \nMultiple datasets are available for download. Each one is of different size and contains different\nnewspapers. All the digitised material can also be found on our search platform a-z.lu \n(Make sure to filter by “eluxemburgensia”). All datasets contain XML (METS + ALTO), PDF, original \nTIFF and PNG files for every newspaper issue.* \nSource: https://data.bnl.lu/data/historical-newspapers/\n\nThis Shiny app allows you to get summaries of the 10 years of daily issues of the \"L'Union\" newspaper.\nIn the first tab, a simple word frequency per article is shown, using the tf-idf method. In the \nsecond tab, summary sentences have been extracted using the `{textrank}` package.\n\n\nWord frequency per article\n===================================== \nRow\n-----------------------------------------------------------------------\n\n### Note: there might be days without any publication. In case of an error, select another date.\n    \n`` `{r}\npath_tf_idf &lt;- reactive({\n    paste0(\"https://raw.githubusercontent.com/b-rodrigues/newspapers_shinyapp/master/tf_idf_data/newspaper_lunion_\", as.character(input$date2), \"_tf_idf_data.rds\")\n})\ndfInput &lt;- reactive({\n        read_rds(url(path_tf_idf())) %&gt;%\n        top_n(as.numeric(input$tf_df_words), tf_idf) %&gt;%\n        mutate(word = reorder(lemma, tf_idf)) \n})\nrenderPlotly({\n    df_tf_idf &lt;- dfInput()\n    p1 &lt;- ggplot(df_tf_idf,\n                 aes(word, tf_idf)) +\n                 geom_col(show.legend = FALSE, fill = \"#82518c\") +\n                 labs(x = NULL, y = \"tf-doc_idf\") +\n                 facet_wrap(~doc_id, ncol = 2, scales = \"free\") +\n                 coord_flip() +\n                 theme_dark()\n    ggplotly(p1)\n})\n`` `\n\nSummary of articles {data-orientation=rows}\n===================================== \nRow \n-----------------------------------------------------------------------\n\n### The sentence in bold is the title of the article. You can show more sentences in the summary by using the input in the sidebar.\n    \n`` `{r}\nprint_summary_textrank &lt;- function(doc_id, summary, n_sentences){\n    htmltools::HTML(paste0(\"&lt;b&gt;\", doc_id, \"&lt;/b&gt;\"), paste(\"&lt;p&gt;\", summary(summary, n_sentences), sep = \"\", collapse = \"&lt;br/&gt;\"), \"&lt;/p&gt;\")\n}\npath_textrank &lt;- reactive({\n    paste0(\"https://raw.githubusercontent.com/b-rodrigues/newspapers_shinyapp/master/textrank_data/newspaper_lunion_\", as.character(input$date2), \"_textrank_data.rds\")\n})\ndfInput2 &lt;- reactive({\n        read_rds(url(path_textrank()))\n})\nrenderUI({\n    df_textrank &lt;- dfInput2()\n    \ndf_textrank &lt;- df_textrank %&gt;% \n    mutate(to_print = map2(doc_id, summary, print_summary_textrank, n_sentences = as.numeric(input$textrank_n_sentences)))\ndf_textrank$to_print\n})\n`` `\n\n\n\nI host the app on Shinyapps.io, which is really easy to do from within Rstudio.\n\n\nThat was quite long, I’m not sure that anyone will read this blog post completely, but oh well. Better to put the code online, might help someone one day, that leave it to rot on my hard drive."
  },
  {
    "objectID": "posts/2020-04-20-no_excuse.html",
    "href": "posts/2020-04-20-no_excuse.html",
    "title": "No excuse not to be a Bayesian anymore",
    "section": "",
    "text": "My first encounter with Bayesian statistics was around 10 years ago, when I was doing my econometrics master’s degree. I was immediately very interested by the Bayesian approach to fit econometric models, because, when you’re reading about Bayesian approaches, it just sounds so easy and natural. You have a model, you might have some prior beliefs about the models parameters, and Bayes’ rule tells you how your beliefs should change when confronting the model to data (evidence). It is really appealing, and what I really liked as well was the interpretation of the results. It was very natural as well. Once your software is done estimating/training the model, you don’t actually get a vector of values for the parameters of the model. You get whole distributions for each parameter, so-called posterior distributions. You can then make statements like “there’s a 95% probability that this parameter lies between 0.12 and 0.21” for instance, which is a statement that you cannot make in a frequentist/classical framework.\n\n\nHowever, while this was very appealing to me at the time, there is no free lunch as they say. At the time, and it was not that long ago, doing Bayesian statistics was not as straightforward as it is now, as I will show you in this blog post. At the time, the BUGS language was still the standard way to describe a Bayesian model and the actual estimation procedure. However, using the BUGS language was tricky; there was WinBUGS, a Windows tool that had already been discontinued at the time in favour of OpenBUGS, which was what I was using. The professor teaching this class, who eventually became one of my PhD advisors, was using WinBUGS to teach, but I was using Linux at the time already, so I went with OpenBUGS which worked with WINE, I think (WINE is a compatibility layer that allows running some Windows programs on Linux. It is quite amazing what WINE is able to do, so much so that Valve forked it to create Proton, which enables running Windows games on Linux on their popular Steam platform). Plus, I found out that there was an R package to call OpenBUGS from R and get the results back into R seamlessly! I think that I remember that there was one for WINBUGS as well, but I also think I remember I could not get it to work. Anyways, after some digging, I found an even better alternative called JAGS. JAGS is open source, and is natively available for Linux. It is also able to run in parallel, which is really useful for Bayesian inference. In any case, these were separate languages/programs from R, and as such the user had to learn how to use them. The way they worked was that users needed to write the Bayesian model in a separate text file. I will illustrate this with an example that I worked on during my Master’s, back in 2011. The example is taken from Ioannis Ntzoufras’ book Bayesian Modeling Using WinBUGS. The example can be found in chapter 7, section 7.4.2. The dataset is from Montgomery et al. (2006) and refers to the number of aircarft damages in 30 strike missions during the Vietnam War.\n\n\nHere’s a description of the data, taken from Ntzoufras’ book:\n\n\n\ndamage: the number of damaged locations of the aircraft\n\n\ntype: binary variable which indicates the type of the plane (0 for A4, B for A6)\n\n\nbombload: the aircraft bomb load in tons\n\n\nairexp: the total months of aircrew experience.\n\n\n\nThe goal is to find a model for damage, what variables explain the amount of damage on the aircraft? Now, something quite interesting here, is that we only have 30 observations, and getting more observations is very costly. So what are you to do with this?\n\n\nFirst, let’s write the model down:\n\n\n[ \\[\\begin{align}\n\\text{damage}_i &amp; = \\text{Poisson}(\\lambda_i) \\\\\n\\log(\\lambda_i) &amp; = \\beta_1 + \\beta_2*\\text{type}_i + \\beta_3 * \\text{bombload}_i + \\beta_4*\\text{airexp}_i\n\\end{align}\\] ]\n\n\nwhere (i = 1, 2, 3, …, 30). Since damage is a count variable, we go with a Poisson distribution, which only has one parameter, (). () is defined on the second line. Both these definition form the likelihood, which should be familiar to any statistician. In JAGS and BUGS, this likelihood had to be written in a separate text file, as I mentioned above, with a syntax that was very similar to R’s (at least for JAGS, because if memory serves, BUGS’s syntax was more different):\n\nmodel{\nfor (i in 1:30){damage[i] ~ dpois( lambda[i] )\n        log(lambda[i]) &lt;- b1 + b2 * type[i] \n        + b3 * bombload[i] + b4 * airexp[i]}\nb1 ~ dnorm(0, 1.0E-4)\nb2 ~ dnorm(0, 1.0E-4)\nb3 ~ dnorm(0, 1.0E-4)\nb4 ~ dnorm(0, 1.0E-4)}\n\nThe last four lines are the prior distributions on the parameters. This is something that does not exist in frequentist statistics. Frequentists would maximise the above likelihood and call it a day. However, the Bayesian framework allows the practitioner to add prior knowledge into the model. This prior knowledge can come from domain knowledge or the literature. However, if the practitioner does not have a clue about good priors, then diffuse priors can be used. Diffuse priors do not carry much information, if at all. The priors above are diffuse; they’re normally distributed, centered around 0 with very small precision (in the Bayesian framework, the normal distribution is defined with two parameters, () and (), where (= )). But since my student years I have learned that using such priors is actually not a very good idea, and better alternatives exist (priors that at least provide some regularization for instance). The Bayesian approach to statistics is often criticized for this, because priors are not objective. If you’re not using diffuse priors, then you’re using priors that carry some information. This information is subjective and subjectivity is a big No-No. But should subjectivity be a No-No? After all, if you can defend your priors, either because of domain knowledge, or because of past studies that provide some clue why not use this information? Especially when here is very little data like in this example. Also, you can perform a sensitivity analysis, and show how the posterior distribution of the parameters change when your priors change. What is important is to be fully transparent about the priors you’re using, and have clear justification for them. If these conditions are met, I don’t see why you should not use prior information in your model. Plus, even in frequentist statistics prior knowledge is used as well, for instance by pre-processing the data in a certain way, or by constraining the values the parameters are allowed to take in the optimisation routine (I’m looking at you, L-BFGS-B).\n\n\nNow, let’s continue with the data. To load the data, I had to manually created each variable (but maybe JAGS now uses data frames) to pass it to jags():\n\n# We load the data this way since jags only takes numerical vectors, matrices or lists\n# containing the names as input\ndamage &lt;- c(0, 1, 0, 0, 0, 0, 1, 0, 0, 2, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 0, 1, 1, 2, 5, 1, 1, 5, 5, 7) \ntype &lt;- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) \nbombload &lt;- c(4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 7, 7, 7, 10, 10, 10, 12, 12, 12, 8, 8, 8, 14, 14, 14)\nairexp &lt;- c(91.5, 84, 76.5, 69, 61.5, 80, 72.5, 65, 57.5, 50, 103, 95.5, 88, 80.5, 73, 116.1, 100.6, 85, 69.4, 53.9, \n112.3, 96.7, 81.1, 65.6, 50, 120, 104.4, 88.9, 73.7, 57.8)\n\nNow that we loaded the data, we can fit the model, by first defining a vector of parameters, and a named list for the data:\n\nparameters &lt;- c(\"b1\", \"b2\", \"b3\",\"b4\")\ndata &lt;- list(\"damage\",\"type\",\"bombload\",\"airexp\")\n\n#We don't give inits to jags since it can generate appropriate initial values\n\n#Use this on single core machines, and/or windows machines\nmodel_fit&lt;-jags(data,inits=NULL,parameters,n.iter=50000,\n                model.file=\"bugsjags.txt\",n.chains=4,DIC=T)\n\nNotice that jags() has an argument called model.file, which is the file I showed above. Below, the code to take a look at the result:\n\n#Let's see the results\nmodel_fit$BUGSoutput\n\n\nmodel.mcmc&lt;-as.mcmc(model_fit)\n\ntraceplot(model.mcmc)\n\nxyplot(model.mcmc)\n\nheidel.diag(model.mcmc) \n\npar(mfrow=c(2,3))\nautocorr.plot(model.mcmc[1],auto.layout=F,ask=F)\n\ngeweke.plot(model.mcmc)\n\nWe’re actually not looking at this, because I’m not running the code; I only wanted to show you how this was done 8 years ago. But why? Because now I can show you how this is done nowadays with {rstanarm}:\n\nlibrary(rstanarm)\n\nmodel_fit_stan &lt;- stan_glm(damage ~ ., data = bombs, family = poisson)\n## \n## SAMPLING FOR MODEL 'count' NOW (CHAIN 1).\n## Chain 1: \n## Chain 1: Gradient evaluation took 1.7e-05 seconds\n## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.\n## Chain 1: Adjust your expectations accordingly!\n## Chain 1: \n## Chain 1: \n## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 1: \n## Chain 1:  Elapsed Time: 0.04083 seconds (Warm-up)\n## Chain 1:                0.043647 seconds (Sampling)\n## Chain 1:                0.084477 seconds (Total)\n## Chain 1: \n## \n## SAMPLING FOR MODEL 'count' NOW (CHAIN 2).\n## Chain 2: \n## Chain 2: Gradient evaluation took 5e-06 seconds\n## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\n## Chain 2: Adjust your expectations accordingly!\n## Chain 2: \n## Chain 2: \n## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 2: \n## Chain 2:  Elapsed Time: 0.037986 seconds (Warm-up)\n## Chain 2:                0.041253 seconds (Sampling)\n## Chain 2:                0.079239 seconds (Total)\n## Chain 2: \n## \n## SAMPLING FOR MODEL 'count' NOW (CHAIN 3).\n## Chain 3: \n## Chain 3: Gradient evaluation took 5e-06 seconds\n## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\n## Chain 3: Adjust your expectations accordingly!\n## Chain 3: \n## Chain 3: \n## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 3: \n## Chain 3:  Elapsed Time: 0.041033 seconds (Warm-up)\n## Chain 3:                0.042982 seconds (Sampling)\n## Chain 3:                0.084015 seconds (Total)\n## Chain 3: \n## \n## SAMPLING FOR MODEL 'count' NOW (CHAIN 4).\n## Chain 4: \n## Chain 4: Gradient evaluation took 5e-06 seconds\n## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\n## Chain 4: Adjust your expectations accordingly!\n## Chain 4: \n## Chain 4: \n## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 4: \n## Chain 4:  Elapsed Time: 0.036928 seconds (Warm-up)\n## Chain 4:                0.041124 seconds (Sampling)\n## Chain 4:                0.078052 seconds (Total)\n## Chain 4:\n\nThere is a lot of output, but the input was a single line that should look very familiar to practitioners used to the glm() function. I only used default options, as well as the default priors. Specifying different priors is quite simple, but I won’t discuss this here, because this blot post, while it might look like a tutorial, is not a tutorial. What I wanted to show you is the difference that 9 years make in software development. {stan} is an R package for Bayesian statistics that came out in 2012 and which has been developed ever since. Just like JAGS and BUGS, users can write external files with very detailed models. But for smaller, or more standard problems, {rstanarm}, makes Bayesian inference very easy and feel familiar to the traditional way of doing things and as its name implies, uses {stan} under the hood.\n\n\nNow let’s continue a little bit and take a look at the model summary:\n\nsummary(model_fit_stan)\n## \n## Model Info:\n##  function:     stan_glm\n##  family:       poisson [log]\n##  formula:      damage ~ .\n##  algorithm:    sampling\n##  sample:       4000 (posterior sample size)\n##  priors:       see help('prior_summary')\n##  observations: 30\n##  predictors:   4\n## \n## Estimates:\n##               mean   sd   10%   50%   90%\n## (Intercept) -0.5    0.9 -1.6  -0.5   0.7 \n## type         0.6    0.5 -0.1   0.6   1.2 \n## bombload     0.2    0.1  0.1   0.2   0.3 \n## airexp       0.0    0.0  0.0   0.0   0.0 \n## \n## Fit Diagnostics:\n##            mean   sd   10%   50%   90%\n## mean_PPD 1.5    0.3  1.1   1.5   1.9  \n## \n## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n## \n## MCMC diagnostics\n##               mcse Rhat n_eff\n## (Intercept)   0.0  1.0  2504 \n## type          0.0  1.0  2412 \n## bombload      0.0  1.0  2262 \n## airexp        0.0  1.0  2652 \n## mean_PPD      0.0  1.0  3813 \n## log-posterior 0.0  1.0  1902 \n## \n## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nJust like for Bayesian stats, we get our parameter’s estimates. But wait! In the intro of this blog post, I said that in Bayesian statistics, we estimate full parameter distributions. So why are we getting point estimates? Well, these are statistics from the posterior distribution, the mean, standard deviation and some deciles.\n\n\nTo explore the results, I like to use {bayestestR}:\n\nlibrary(bayestestR)\n\ndescribe_posterior(model_fit_stan)\n## # Description of Posterior Distributions\n## \n## Parameter   | Median | CI | CI_low | CI_high |    pd | ROPE_CI | ROPE_low | ROPE_high | ROPE_Percentage |  Rhat |  ESS\n## ----------------------------------------------------------------------------------------------------------------------\n## (Intercept) | -0.473 | 89 | -1.898 |   0.929 | 0.711 |      89 |   -0.100 |     0.100 |           0.082 | 0.999 | 2504\n## type        |  0.577 | 89 | -0.230 |   1.365 | 0.869 |      89 |   -0.100 |     0.100 |           0.099 | 1.002 | 2412\n## bombload    |  0.169 | 89 |  0.065 |   0.275 | 0.994 |      89 |   -0.100 |     0.100 |           0.108 | 1.001 | 2262\n## airexp      | -0.014 | 89 | -0.028 |  -0.001 | 0.953 |      89 |   -0.100 |     0.100 |           1.000 | 1.000 | 2652\n\nLet’s also actually see the posterior of, say, (_3):\n\nlibrary(insight)\n## Warning: package 'insight' was built under R version 3.6.2\nposteriors &lt;- get_parameters(model_fit_stan)\n\nggplot(posteriors, aes(x = bombload)) +\n  geom_density(fill = \"cyan\") +\n  brotools::theme_blog()\n\n\n\n\nI won’t again go into much detail, because you can read the very detailed Vignettes on {bayestestR}’s website: Get started with Bayesian Analysis and Describing the posterior which explain all of this much better than I would ever do. The code I’m showing here is basically a copy paste of these Vignettes, so if I piqued your interest, go read those Vignettes! I also highly recommend reading {rstanarm}’s Vignettes, and grabbing the second edition of Statistical Rethinking, by Richard McElreath, it is a great intro to Bayesian statistics with {stan}.\n\n\nNow, as the title of this blog post reads, there is no excuse not to use Bayesian statistics; from a software point of view, it’s as simple as ever. And by the way, {stan} models are supported in {tidymodels}’ {parsnip} package as well, which makes things even easier!"
  },
  {
    "objectID": "posts/2025-02-13-testthat.html",
    "href": "posts/2025-02-13-testthat.html",
    "title": "Using options() to inject a function’s internal variable for reproducible testing",
    "section": "",
    "text": "No image this time\nImagine you have a function that does something complicated, and in the middle of its definition it generates a variable. Now suppose that you want to save this variable and then re-use it for tests, what I mean is that you want your function to always reproduce this intermediary variable, regardless of what you give it as inputs. This can be useful for testing, if computing this intermediate variable is costly.\nIn my {rix} package, the rix() function generates valid Nix expressions from R input and these Nix expressions can then be used to build reproducible development environments that include R, R packages, development libraries, and so on. If you want a 5-minute intro to {rix}, click here.\nAnyways, sometimes, computing these expressions can take some time, especially if the users wants to include remote dependencies that have themselves remote dependencies. rix() will try to look for suitable GitHub commits to pin all the packages for reproducibility purposes, and this can imply quite a lot of api calls. Now for my tests, I wanted to use an already generated default.nix file (which contains the generated Nix expression) but I didn’t want to have to recompute it every time I ran the test and I couldn’t simply use it as is for the test either. You see, that default.nix was in an intermediary state, before rix() is supposed to do some post-processing to it, which is what I actually want to test (I want to actually test the argument that makes rix() skip this post-processing step).\nSo suppose rix() looks like this:\n\nrix &lt;- function(a,b,c){\n  ... # lots of code\n  ... # lots of code\n  default.nix_file &lt;- ... # it's generated here\n  # Then a bunch of things happen to it\n  out &lt;- f(default.nix_file)\n  writeLines(out, path) # this is what's written\n}\n\nNow what I want is to be able to “overwrite” the default.nix_file variable on line 4 when testing, to provide what I want. This way, I can call rix() with some “easy” parameters that make the computations up to that point very quick. My goal is essentially to test f() (line 6), which begs the question, why not write f() as a separate function and test it? This would be the best practice, however, I don’t really have such an f(), rather it’s a series of complicated steps that follow and rewriting everything to make it easily testable would just take too much time.\nInstead, I opted for the following:\n\nrix &lt;- function(a,b,c){\n  ... # lots of code\n  ... # lots of code\n\n  stub_default.nix &lt;- getOption(\"TESTTHAT_DEFAULT.NIX\", default = NULL)\n\n  if(!is.null(stub_default.nix)){\n    default.nix_file &lt;- readLines(stub_default.nix)\n  } else {\n    default.nix_file &lt;- ... # it's generated here if not being tested\n  }\n  out &lt;- f(default.nix_file)\n  # Then a bunch of things happen to it\n  writeLines(out, path) # this is what's written\n}\n\nOn line 5, I get the option \"TESTTHAT_DEFAULT.NIX\" and if it doesn’t exist, stub_default.nix will be set to NULL. So if it’s NULL it’s business as usual, if not, then that default.nix file dedicated for testing gets passed further down. In a sense, I injected the variable I needed in the spot I needed.\nThen, my tests looks like this:\n\ntestthat::test_that(\"remove_duplicate_entries(), don't remove duplicates if skip\", {\n\n\n  dups_entries_default.nix &lt;- paste0(\n    testthat::test_path(),\n    \"/testdata/default-nix_samples/dups-entries_default.nix\")\n\n  tmpdir &lt;- tempdir()\n\n  # This copies the file I need in the right path\n  destination_file &lt;- file.path(tempdir(), basename(dups_entries_default.nix))\n  file.copy(dups_entries_default.nix, destination_file, overwrite = TRUE)\n\n  on.exit(\n    unlink(tmpdir, recursive = TRUE, force = TRUE),\n    add = TRUE\n  )\n\n  removed_dups &lt;- function(destination_file) {\n\n    # Set the option to the file path and clean the option afterwards\n    op &lt;- options(\"TESTTHAT_DEFAULT.NIX\" = destination_file)\n    on.exit(options(op), add = TRUE, after = FALSE)\n\n    out &lt;- rix(\n      date = \"2025-02-10\",\n      project_path = tmpdir,\n      overwrite = TRUE,\n      skip_post_processing = TRUE) # &lt;- this is actually want I wanted to test\n    file.path(destination_file)\n  }\n\n\n  testthat::expect_snapshot_file(\n    path = removed_dups(destination_file),\n    name = \"skip-dups-entries_default.nix\",\n  )\n})\n\nOn line 22, I set the option and on line 23 I write code to remove that option once the test is done, to not mess up subsequent tests. This is a snapshot test, so now I can take a look at the resulting file, and indeed make sure that post-processing was skipped, as expected.\nHow would you have done this?"
  },
  {
    "objectID": "posts/2018-03-03-sparklyr_h2o_rsparkling.html",
    "href": "posts/2018-03-03-sparklyr_h2o_rsparkling.html",
    "title": "Getting {sparklyr}, {h2o}, {rsparkling} to work together and some fun with bash",
    "section": "",
    "text": "This is going to be the type of blog posts that would perhaps be better as a gist, but it is easier for me to use my blog as my own personal collection of gists. Plus, someone else might find this useful, so here it is! In this blog post I am going to show a little trick to randomly sample rows from a text file using bash, and then train a model using the {h2o} package. I will also use the {rsparkling} package. From {rsparkling}’s documentation: {rsparkling} is a package that provides an R interface to the H2O Sparkling Water machine learning library. and will be needed to transfer the data from Spark to H2O.\n\n\nIn a previous blog post I used the {sparklyr} package to load a 30GB csv file into R. I created the file by combining around 300 csv files, each around 80MB big. Here, I would like to use the machine learning functions included in the {h2o} packages to train a random forest on this data. However, I only want to have a simple prototype that simply runs, and check if all the packages work well together. If everything is ok, I’ll keep iterating to make the model better (in a possible subsequent post).\n\n\nFor fast prototyping, using 30GB of data is not a good idea, so I am going to sample 500000 from this file using the linux command line (works on macOS too and also on Windows if you installed the linux subsystem). Why not use R to sample 500000 rows? Because on my machine, loading the 30GB file takes 25 minutes. Sampling half a million lines from it would take quite long too. So here are some bash lines that do that directly on the file, without needing to load it into R beforehand:\n\n[18-03-03 21:50] brodriguesco in /Documents/AirOnTimeCSV ➤ get_seeded_random()\n{\n  seed=\"$1\"\n  openssl enc -aes-256-ctr -pass pass:\"$seed\" -nosalt \\\n  &lt;/dev/zero 2&gt;/dev/null\n}\n\n[18-03-03 21:50] brodriguesco in /Documents/AirOnTimeCSV ➤ sed \"1 d\" combined.csv | shuf --random-source=&lt;(get_seeded_random 42) -n 500000 &gt; small_combined_temp.csv\n\n[18-03-03 21:56] brodriguesco in /Documents/AirOnTimeCSV ➤ head -1 combined.csv &gt; colnames.csv\n\n[18-03-03 21:56] brodriguesco in /Documents/AirOnTimeCSV ➤ cat colnames.csv small_combined_temp.csv &gt; small_combined.csv\n\nThe first function I took from the gnu coreutils manual which allows me to fix the random seed to reproduce the same sampling of the file. Then I use \"sed 1 d\" cobmined.csv to remove the first line of combined.csv which is the header of the file. Then, I pipe the result of sed using | to shuf which does the shuffling. The option –random-source=&lt;(get_seeded_random 42) fixes the seed, and -n 500000 only shuffles 500000 and not the whole file. The final bit of the line, &gt; small_combined_temp.csv, saves the result to small_cobmined_temp.csv. Because I need to add back the header, I use head -1 to extract the first line of combined.csv and save it into colnames.csv. Finally, I bind the rows of both files using cat colnames.csv small_combined_temp.csv and save the result into small_combined.cvs. Taken together, all these steps took about 5 minutes (without counting the googling around for finding how to pass a fixed seed to shuf).\n\n\nNow that I have this small dataset, I can write a small prototype:\n\n\nFirst, you need to install {sparklyr}, {rsparkling} and {h2o}. Refer to this to know how to install the packages. I had a mismatch between the version of H2O that was automatically installed when I installed the {h2o} package, and the version of Spark that {sparklyr} installed but thankfully the {h2o} package returns a very helpful error message with the following lines:\n\ndetach(\"package:rsparkling\", unload = TRUE)\n                       if (\"package:h2o\" %in% search()) { detach(\"package:h2o\", unload = TRUE) }\n                       if (isNamespaceLoaded(\"h2o\")){ unloadNamespace(\"h2o\") }\n                       remove.packages(\"h2o\")\n                       install.packages(\"h2o\", type = \"source\", repos = \"https://h2o-release.s3.amazonaws.com/h2o/rel-weierstrass/2/R\")\n\nwhich tells you which version to install.\n\n\nSo now, let’s load everything:\n\nlibrary(sparklyr)\nlibrary(rsparkling)\nlibrary(h2o)\n## \n## ----------------------------------------------------------------------\n## \n## Your next step is to start H2O:\n##     &gt; h2o.init()\n## \n## For H2O package documentation, ask for help:\n##     &gt; ??h2o\n## \n## After starting H2O, you can use the Web UI at http://localhost:54321\n## For more information visit http://docs.h2o.ai\n## \n## ----------------------------------------------------------------------\n## \n## Attaching package: 'h2o'\n## The following objects are masked from 'package:stats':\n## \n##     cor, sd, var\n## The following objects are masked from 'package:base':\n## \n##     &&, %*%, %in%, ||, apply, as.factor, as.numeric, colnames,\n##     colnames&lt;-, ifelse, is.character, is.factor, is.numeric, log,\n##     log10, log1p, log2, round, signif, trunc\nh2o.init()\n## \n## H2O is not running yet, starting it now...\n## \n## Note:  In case of errors look at the following log files:\n##     /tmp/Rtmph48vf9/h2o_cbrunos_started_from_r.out\n##     /tmp/Rtmph48vf9/h2o_cbrunos_started_from_r.err\n## \n## \n## Starting H2O JVM and connecting: .. Connection successful!\n## \n## R is connected to the H2O cluster: \n##     H2O cluster uptime:         1 seconds 944 milliseconds \n##     H2O cluster version:        3.16.0.2 \n##     H2O cluster version age:    4 months and 15 days !!! \n##     H2O cluster name:           H2O_started_from_R_cbrunos_bpn152 \n##     H2O cluster total nodes:    1 \n##     H2O cluster total memory:   6.98 GB \n##     H2O cluster total cores:    12 \n##     H2O cluster allowed cores:  12 \n##     H2O cluster healthy:        TRUE \n##     H2O Connection ip:          localhost \n##     H2O Connection port:        54321 \n##     H2O Connection proxy:       NA \n##     H2O Internal Security:      FALSE \n##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 \n##     R Version:                  R version 3.4.4 (2018-03-15)\n## Warning in h2o.clusterInfo(): \n## Your H2O cluster version is too old (4 months and 15 days)!\n## Please download and install the latest version from http://h2o.ai/download/\n\nI left all the startup messages because they’re quite helpful. Especially that bit telling you to start H2O with h2o.init(). If something’s wrong, h2o.init() will give you helpful information.\n\n\nNow that all this is loaded, I can start working on the data (the steps below are explained in detail in my previous blog post):\n\nspark_dir = \"/my_2_to_disk/spark/\"\n\nconfig = spark_config()\n\nconfig$`sparklyr.shell.driver-memory` &lt;- \"4G\"\nconfig$`sparklyr.shell.executor-memory` &lt;- \"4G\"\nconfig$`spark.yarn.executor.memoryOverhead` &lt;- \"512\"\nconfig$`sparklyr.shell.driver-java-options` = paste0(\"-Djava.io.tmpdir=\", spark_dir)\n\nsc = spark_connect(master = \"local\", config = config)\n\nAnother useful function that allows you to check if everything is alright is h2o_context():\n\nh2o_context(sc)\n&lt;jobj[12]&gt;\n  org.apache.spark.h2o.H2OContext\n\nSparkling Water Context:\n * H2O name: sparkling-water-cbrunos_local-1520111879840\n * cluster size: 1\n * list of used nodes:\n  (executorId, host, port)\n  ------------------------\n  (driver,127.0.0.1,54323)\n  ------------------------\n\n  Open H2O Flow in browser: http://127.0.0.1:54323 (CMD + click in Mac OSX)\n\n\nNow, let’s load the data into R with {sparklyr}:\n\nair = spark_read_csv(sc, name = \"air\", path = \"small_combined.csv\")\n\nOf course, here, using Spark is overkill, because small_combined.csv is only around 100MB big, so no need for {sparklyr} but as stated in the beginning this is only to have a quick and dirty prototype. Once all the pieces are working together, I can iterate on the real data, for which {sparklyr} will be needed. Now, if I needed to use {dplyr} I could use it on air, but I don’t want to do anything on it, so I convert it to a h2o data frame. h2o data frames are needed as arguments for the machine learning algorithms included in the {h2o} package. as_h2o_frame() is a function included in {rsparkling}:\n\nair_hf = as_h2o_frame(sc, air)\n\nThen, I convert the columns I need to factors (I am only using factors here):\n\nair_hf$ORIGIN = as.factor(air_hf$ORIGIN)\nair_hf$UNIQUE_CARRIER = as.factor(air_hf$UNIQUE_CARRIER)\nair_hf$DEST = as.factor(air_hf$DEST)\n\n{h2o} functions need the names of the predictors and of the target columns, so let’s define that:\n\ntarget = \"ARR_DELAY\"\npredictors = c(\"UNIQUE_CARRIER\", \"ORIGIN\", \"DEST\")\n\nNow, let’s train a random Forest, without any hyper parameter tweaking:\n\nmodel = h2o.randomForest(predictors, target, training_frame = air_hf)\n\nNow that this runs, I will in the future split the data into training, validation and test set, and train a model with better hyper parameters. For now, let’s take a look at the summary of model:\n\nsummary(model)\nModel Details:\n==============\n\nH2ORegressionModel: drf\nModel Key:  DRF_model_R_1520111880605_1\nModel Summary:\n  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n1              50                       50            11055998        20\n  max_depth mean_depth min_leaves max_leaves mean_leaves\n1        20   20.00000       1856       6129  4763.42000\n\nH2ORegressionMetrics: drf\n** Reported on training data. **\n** Metrics reported on Out-Of-Bag training samples **\n\nMSE:  964.9246\nRMSE:  31.06324\nMAE:  17.65517\nRMSLE:  NaN\nMean Residual Deviance :  964.9246\n\n\n\n\n\nScoring History:\n             timestamp   duration number_of_trees training_rmse training_mae\n1  2018-03-03 22:52:24  0.035 sec               0\n2  2018-03-03 22:52:25  1.275 sec               1      30.93581     17.78216\n3  2018-03-03 22:52:25  1.927 sec               2      31.36998     17.78867\n4  2018-03-03 22:52:26  2.272 sec               3      31.36880     17.80359\n5  2018-03-03 22:52:26  2.564 sec               4      31.29683     17.79467\n6  2018-03-03 22:52:26  2.854 sec               5      31.31226     17.79467\n7  2018-03-03 22:52:27  3.121 sec               6      31.26214     17.78542\n8  2018-03-03 22:52:27  3.395 sec               7      31.20749     17.75703\n9  2018-03-03 22:52:27  3.666 sec               8      31.19706     17.74753\n10 2018-03-03 22:52:27  3.935 sec               9      31.16108     17.73547\n11 2018-03-03 22:52:28  4.198 sec              10      31.13725     17.72493\n12 2018-03-03 22:52:32  8.252 sec              27      31.07608     17.66648\n13 2018-03-03 22:52:36 12.462 sec              44      31.06325     17.65474\n14 2018-03-03 22:52:38 14.035 sec              50      31.06324     17.65517\n   training_deviance\n1\n2          957.02450\n3          984.07580\n4          984.00150\n5          979.49147\n6          980.45794\n7          977.32166\n8          973.90720\n9          973.25655\n10         971.01272\n11         969.52856\n12         965.72249\n13         964.92530\n14         964.92462\n\nVariable Importances: (Extract with `h2o.varimp`)\n=================================================\n\nVariable Importances:\n        variable relative_importance scaled_importance percentage\n1         ORIGIN    291883392.000000          1.000000   0.432470\n2           DEST    266749168.000000          0.913890   0.395230\n3 UNIQUE_CARRIER    116289536.000000          0.398411   0.172301\n&gt;\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2018-10-27-lux_elections_analysis.html#abstract",
    "href": "posts/2018-10-27-lux_elections_analysis.html#abstract",
    "title": "Maps with pie charts on top of each administrative division: an example with Luxembourg’s elections data",
    "section": "\nAbstract\n",
    "text": "Abstract\n\n\nYou can find the data used in this blog post here: https://github.com/b-rodrigues/elections_lux\n\n\nThis is a follow up to a previous blog post where I extracted data of the 2018 Luxembourguish elections from Excel Workbooks. Now that I have the data, I will create a map of Luxembourg by commune, with pie charts of the results on top of each commune! To do this, I use good ol’ {ggplot2} and another packages called {scatterpie}. As a bonus, I have added the code to extract the data from the 2013 elections from Excel. You’ll find this code in the appendix at the end of the blog post."
  },
  {
    "objectID": "posts/2018-10-27-lux_elections_analysis.html#introduction",
    "href": "posts/2018-10-27-lux_elections_analysis.html#introduction",
    "title": "Maps with pie charts on top of each administrative division: an example with Luxembourg’s elections data",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nBefore importing the data for the elections of 2018, let’s install some packages:\n\ninstall.packages('rgeos', type='source') # Dependency of rgdal\ninstall.packages('rgdal', type='source') # To read in the shapefile\n\nThese packages might be very tricky to install on OSX and Linux, but they’re needed to import the shapefile of the country, which is needed to draw a map. So to make things easier, I have created an rds object, from the shapefile of Luxembourg, that you can import natively in R without needing these two packages. But if you want to use them, here is how:\n\ncommunes &lt;- readOGR(\"Limadmin_SHP/LIMADM_COMMUNES.shp\")\n\nBy the way, you can download the shapefile for Luxembourg here.\n\n\nI’ll use my shapefile though (that you can download from the same github repo as the data):\n\ncommunes_df &lt;- readRDS(\"commune_shapefile.rds\")\n\nHere’s how it looks like:\n\nhead(communes_df)\n##       long      lat order  hole piece      group       id\n## 1 91057.65 101536.6     1 FALSE     1 Beaufort.1 Beaufort\n## 2 91051.79 101487.3     2 FALSE     1 Beaufort.1 Beaufort\n## 3 91043.43 101461.7     3 FALSE     1 Beaufort.1 Beaufort\n## 4 91043.37 101449.8     4 FALSE     1 Beaufort.1 Beaufort\n## 5 91040.42 101432.1     5 FALSE     1 Beaufort.1 Beaufort\n## 6 91035.44 101405.6     6 FALSE     1 Beaufort.1 Beaufort\n\nNow let’s load some packages:\n\nlibrary(\"tidyverse\")\nlibrary(\"tidyxl\")\nlibrary(\"ggplot2\")\nlibrary(\"scatterpie\")\n\nOk, now, let’s import the elections results data, which is the output of last week’s blog post:\n\nelections &lt;- read_csv(\"elections_2018.csv\")\n## Parsed with column specification:\n## cols(\n##   Party = col_character(),\n##   Year = col_double(),\n##   Variables = col_character(),\n##   Values = col_double(),\n##   locality = col_character(),\n##   division = col_character()\n## )\n\nI will only focus on the data at the commune level, and only use the share of votes for each party:\n\nelections_map &lt;- elections %&gt;%\n    filter(division == \"Commune\",\n           Variables == \"Pourcentage\")\n\nNow I need to make sure that the names of the communes are the same between the elections data and the shapefile. Usual suspects are the “Haute-Sûre” and the “Redange-sur-Attert” communes, but let’s take a look:\n\nlocality_elections &lt;- unique(elections_map$locality)\nlocality_shapefile &lt;- unique(communes_df$id)\n\nsetdiff(locality_elections, locality_shapefile)\n## [1] \"Lac de la Haute-Sûre\" \"Redange Attert\"\n\nYep, exactly as expected. I’ve had problems with the names of these two communes in the past already. Let’s rename these two communes in the elections data:\n\nelections_map &lt;- elections_map %&gt;%\n    mutate(commune = case_when(locality == \"Lac de la Haute-Sûre\" ~ \"Lac de la Haute Sûre\",\n                          locality == \"Redange Attert\" ~ \"Redange\",\n                          TRUE ~ locality))\n\nNow, I can select the relevant columns from the shapefile:\n\ncommunes_df &lt;- communes_df %&gt;%\n    select(long, lat, commune = id)\n\nand from the elections data:\n\nelections_map &lt;- elections_map %&gt;%\n    select(commune, Party, Variables, Values)"
  },
  {
    "objectID": "posts/2018-10-27-lux_elections_analysis.html#plotting-the-data-on-a-map",
    "href": "posts/2018-10-27-lux_elections_analysis.html#plotting-the-data-on-a-map",
    "title": "Maps with pie charts on top of each administrative division: an example with Luxembourg’s elections data",
    "section": "\nPlotting the data on a map\n",
    "text": "Plotting the data on a map\n\n\nNow, for the type of plot I want to make, using the {scatterpie} package, I need the data to be in the wide format, not long. For this I will use tidyr::spread():\n\nelections_map &lt;- elections_map %&gt;% \n    spread(Party, Values)\n\nThis is how the data looks now:\n\nglimpse(elections_map)\n## Observations: 102\n## Variables: 10\n## $ commune     &lt;chr&gt; \"Beaufort\", \"Bech\", \"Beckerich\", \"Berdorf\", \"Bertran…\n## $ Variables   &lt;chr&gt; \"Pourcentage\", \"Pourcentage\", \"Pourcentage\", \"Pource…\n## $ ADR         &lt;dbl&gt; 0.12835106, 0.09848661, 0.08596748, 0.16339234, 0.04…\n## $ CSV         &lt;dbl&gt; 0.2426239, 0.2945285, 0.3004751, 0.2604552, 0.290278…\n## $ `déi gréng` &lt;dbl&gt; 0.15695672, 0.21699651, 0.24072721, 0.15619529, 0.15…\n## $ `déi Lénk`  &lt;dbl&gt; 0.04043732, 0.03934808, 0.05435776, 0.02295273, 0.04…\n## $ DP          &lt;dbl&gt; 0.15875393, 0.19394645, 0.12899689, 0.15444466, 0.30…\n## $ KPL         &lt;dbl&gt; 0.015875393, 0.006519208, 0.004385164, 0.011476366, …\n## $ LSAP        &lt;dbl&gt; 0.11771754, 0.11455180, 0.08852549, 0.16592103, 0.09…\n## $ PIRATEN     &lt;dbl&gt; 0.13928411, 0.03562282, 0.09656496, 0.06516242, 0.04…\n\nFor this to work, I need two datasets; one to draw the map (commune_df) and one to draw the pie charts over each commune, with the data to draw the charts, but also the position of where I want the pie charts. For this, I will compute the average of the longitude and latitude, which should be good enough:\n\nscatterpie_data &lt;- communes_df %&gt;%\n    group_by(commune) %&gt;%\n    summarise(long = mean(long),\n              lat = mean(lat))\n\nNow, let’s join the two datasets:\n\nfinal_data &lt;- left_join(scatterpie_data, elections_map, by = \"commune\") \n\nI have all the ingredients to finally plot the data:\n\nggplot() +\n    geom_polygon(data = communes_df, aes(x = long, y = lat, group = commune), colour = \"grey\", fill = NA) +\n    geom_scatterpie(data = final_data, aes(x=long, y=lat, group=commune), \n                    cols = c(\"ADR\", \"CSV\", \"déi gréng\", \"déi Lénk\", \"DP\", \"KPL\", \"LSAP\", \"PIRATEN\")) +\n    labs(title = \"Share of total vote in each commune, 2018 elections\") +\n    theme_void() +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank(),\n          legend.text = element_text(colour = \"white\"),\n          plot.background = element_rect(\"#272b30\"),\n          plot.title = element_text(colour = \"white\")) +\n    scale_fill_manual(values = c(\"ADR\" = \"#009dd1\",\n                                 \"CSV\" = \"#ee7d00\",\n                                 \"déi gréng\" = \"#45902c\",\n                                 \"déi Lénk\" = \"#e94067\",\n                                 \"DP\" = \"#002a54\",\n                                 \"KPL\" = \"#ff0000\",\n                                 \"LSAP\" = \"#ad3648\",\n                                 \"PIRATEN\" = \"#ad5ea9\"))\n\n\n\n\nNot too bad, but we can’t really read anything from the pie charts. I will now make their size proportional to the number of voters in each commune. For this, I need to go back to the Excel sheets, and look for the right cell:\n\n\n\n\n\nIt will be easy to extract this info. It located in cell “E5”:\n\nelections_raw_2018 &lt;- xlsx_cells(\"leg-2018-10-14-22-58-09-737.xlsx\")\n\nelectors_commune &lt;- elections_raw_2018 %&gt;%\n    filter(!(sheet %in% c(\"Le Grand-Duché de Luxembourg\", \"Centre\", \"Est\", \"Nord\", \"Sud\", \"Sommaire\"))) %&gt;%\n    filter(address == \"E5\") %&gt;%\n    select(sheet, numeric) %&gt;%\n    rename(commune = sheet,\n           electors = numeric)\n\nI can now add this to the data:\n\nfinal_data &lt;- final_data %&gt;% \n    full_join(electors_commune) %&gt;%\n    mutate(log_electors = log(electors) * 200)\n## Joining, by = \"commune\"\n\nIn the last line, I create a new column called log_electors that I then multiply by 200. This will be useful later.\n\n\nNow I can add the r argument inside the aes() function on the third line, to make the pie chart size proportional to the number of electors in that commune:\n\nggplot() +\n  geom_polygon(data = communes_df, aes(x = long, y = lat, group = commune), colour = \"grey\", fill = NA) +\n    geom_scatterpie(data = final_data, aes(x=long, y=lat, group = commune, r = electors), \n                    cols = c(\"ADR\", \"CSV\", \"déi gréng\", \"déi Lénk\", \"DP\", \"KPL\", \"LSAP\", \"PIRATEN\")) +\n    labs(title = \"Share of total vote in each commune, 2018 elections\") +\n    theme_void() +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank(),\n          legend.text = element_text(colour = \"white\"),\n          plot.background = element_rect(\"#272b30\"),\n          plot.title = element_text(colour = \"white\")) +\n    scale_fill_manual(values = c(\"ADR\" = \"#009dd1\",\n                                 \"CSV\" = \"#ee7d00\",\n                                 \"déi gréng\" = \"#45902c\",\n                                 \"déi Lénk\" = \"#182024\",\n                                 \"DP\" = \"#002a54\",\n                                 \"KPL\" = \"#ff0000\",\n                                 \"LSAP\" = \"#ad3648\",\n                                 \"PIRATEN\" = \"#ad5ea9\"))\n## Warning: Removed 32 rows containing non-finite values (stat_pie).\n\n\n\n\nOk, that was not a good idea! Perhaps the best option would be to have one map per circonscription. For this, I need the list of communes by circonscription. This is available on Wikipedia. Here are the lists:\n\ncentre &lt;- c(\"Bissen\", \"Colmar-Berg\", \"Fischbach\", \"Heffingen\", \"Larochette\",\n            \"Lintgen\", \"Lorentzweiler\", \"Mersch\", \"Nommern\", \"Helperknapp\", \"Bertrange\", \"Contern\", \n            \"Hesperange\", \"Luxembourg\", \"Niederanven\", \"Sandweiler\", \"Schuttrange\", \"Steinsel\", \n            \"Strassen\", \"Walferdange\", \"Weiler-la-Tour\")\n\nest &lt;- c(\"Beaufort\", \"Bech\", \"Berdorf\", \"Consdorf\", \"Echternach\", \"Rosport-Mompach\", \"Waldbillig\",\n         \"Betzdorf\", \"Biwer\", \"Flaxweiler\", \"Grevenmacher\", \"Junglinster\", \"Manternach\", \"Mertert\",\n         \"Wormeldange\",\"Bous\", \"Dalheim\", \"Lenningen\", \"Mondorf-les-Bains\", \"Remich\", \"Schengen\",\n         \"Stadtbredimus\", \"Waldbredimus\")\n\nnord &lt;- c(\"Clervaux\", \"Parc Hosingen\", \"Troisvierges\", \"Weiswampach\", \"Wincrange\", \"Bettendorf\", \n          \"Bourscheid\", \"Diekirch\", \"Erpeldange-sur-Sûre\", \"Ettelbruck\", \"Feulen\", \"Mertzig\", \"Reisdorf\", \n          \"Schieren\", \"Vallée de l'Ernz\", \"Beckerich\", \"Ell\", \"Grosbous\", \"Préizerdaul\", \n          \"Rambrouch\", \"Redange\", \"Saeul\", \"Useldange\", \"Vichten\", \"Wahl\", \"Putscheid\", \"Tandel\",\n          \"Vianden\", \"Boulaide\", \"Esch-sur-Sûre\", \"Goesdorf\", \"Kiischpelt\", \"Lac de la Haute Sûre\",\n          \"Wiltz\", \"Winseler\")\n\nsud &lt;- c(\"Dippach\", \"Garnich\", \"Käerjeng\", \"Kehlen\", \"Koerich\", \"Kopstal\", \"Mamer\", \n         \"Habscht\", \"Steinfort\", \"Bettembourg\", \"Differdange\", \"Dudelange\", \"Esch-sur-Alzette\", \n         \"Frisange\", \"Kayl\", \"Leudelange\", \"Mondercange\", \"Pétange\", \"Reckange-sur-Mess\", \"Roeser\",\n         \"Rumelange\", \"Sanem\", \"Schifflange\")\n\ncirconscriptions &lt;- list(\"centre\" = centre, \"est\" = est,\n                         \"nord\" = nord, \"sud\" = sud)\n\nNow, I can make one map per circonscription. First, let’s split the data sets by circonscription:\n\ncommunes_df_by_circonscription &lt;- circonscriptions %&gt;%\n    map(~filter(communes_df, commune %in% .))\n\nfinal_data_by_circonscription &lt;- circonscriptions %&gt;%\n    map(~filter(final_data, commune %in% .))\n\nBy using pmap(), I can reuse the code to generate the plot to each element of the two lists. This is nice because I do not need to copy and paste the code 4 times:\n\npmap(list(x = communes_df_by_circonscription,\n          y = final_data_by_circonscription,\n          z = names(communes_df_by_circonscription)),\n     function(x, y, z){\n         ggplot() +\n        geom_polygon(data = x, aes(x = long, y = lat, group = commune), \n                     colour = \"grey\", fill = NA) +\n        geom_scatterpie(data = y, aes(x=long, y=lat, group = commune), \n                        cols = c(\"ADR\", \"CSV\", \"déi gréng\", \"déi Lénk\", \"DP\", \"KPL\", \"LSAP\", \"PIRATEN\")) +\n        labs(title = paste0(\"Share of total vote in each commune, 2018 elections for circonscription \", z)) +\n        theme_void() +\n        theme(legend.position = \"bottom\",\n              legend.title = element_blank(),\n              legend.text = element_text(colour = \"white\"),\n              plot.background = element_rect(\"#272b30\"),\n              plot.title = element_text(colour = \"white\")) + \n        scale_fill_manual(values = c(\"ADR\" = \"#009dd1\",\n                                     \"CSV\" = \"#ee7d00\",\n                                     \"déi gréng\" = \"#45902c\",\n                                     \"déi Lénk\" = \"#182024\",\n                                     \"DP\" = \"#002a54\",\n                                     \"KPL\" = \"#ff0000\",\n                                     \"LSAP\" = \"#ad3648\",\n                                     \"PIRATEN\" = \"#ad5ea9\"))\n     }\n)\n## $centre\n\n\n\n## \n## $est\n\n\n\n## \n## $nord\n\n\n\n## \n## $sud\n\n\n\n\nI created an anonymous function of three argument, x, y and z. If you are unfamiliar with pmap(), study the above code closely. If you have questions, do not hesitate to reach out!\n\n\nThe pie charts are still quite small, but if I try to change the size of the pie charts, I’ll have the same problem as before: inside the same circonscription, some communes have really a lot of electors, and some a very small number. Perhaps I can try with the log of the electors?\n\npmap(list(x = communes_df_by_circonscription,\n          y = final_data_by_circonscription,\n          z = names(communes_df_by_circonscription)),\n     function(x, y, z){\n         ggplot() +\n        geom_polygon(data = x, aes(x = long, y = lat, group = commune), \n                     colour = \"grey\", fill = NA) +\n        geom_scatterpie(data = y, aes(x=long, y=lat, group = commune, r = log_electors), \n                        cols = c(\"ADR\", \"CSV\", \"déi gréng\", \"déi Lénk\", \"DP\", \"KPL\", \"LSAP\", \"PIRATEN\")) +\n        labs(title = paste0(\"Share of total vote in each commune, 2018 elections for circonscription \", z)) +\n        theme_void() +\n        theme(legend.position = \"bottom\",\n              legend.title = element_blank(),\n              legend.text = element_text(colour = \"white\"),\n              plot.background = element_rect(\"#272b30\"),\n              plot.title = element_text(colour = \"white\")) + \n        scale_fill_manual(values = c(\"ADR\" = \"#009dd1\",\n                                     \"CSV\" = \"#ee7d00\",\n                                     \"déi gréng\" = \"#45902c\",\n                                     \"déi Lénk\" = \"#182024\",\n                                     \"DP\" = \"#002a54\",\n                                     \"KPL\" = \"#ff0000\",\n                                     \"LSAP\" = \"#ad3648\",\n                                     \"PIRATEN\" = \"#ad5ea9\"))\n     }\n)\n## $centre\n\n\n\n## \n## $est\n\n\n\n## \n## $nord\n## Warning: Removed 16 rows containing non-finite values (stat_pie).\n\n\n\n## \n## $sud\n\n\n\n\nThis looks better now!"
  },
  {
    "objectID": "posts/2018-10-27-lux_elections_analysis.html#conclusion",
    "href": "posts/2018-10-27-lux_elections_analysis.html#conclusion",
    "title": "Maps with pie charts on top of each administrative division: an example with Luxembourg’s elections data",
    "section": "\nConclusion\n",
    "text": "Conclusion\n\n\nHaving data in a machine readable format is really important. The amount of code I had to write to go from the Excel Workbooks that contained the data to this plots is quite large, but if the data was in a machine readable format to start with, I could have focused on the plots immediately.\n\n\nThe good thing is that I got to practice my skills and discovered {scatterpie}!\n\n\n\nAppendix\n\n\nThe following lines of code extract the data (from the 2013 elections) from the Excel Workbooks that can be found in Luxembourguish Open Data Portal.\n\n\nI will not comment them, as they work in a similar way than in the previous blog post where I extracted the data from the 2018 elections. The only difference, is that the sheet with the national level data was totally different, so I did not extract it. The first reason is because I don’t need it for this blog post, the second is because I was lazy. For me, that’s two pretty good reasons not to do something. If you have a question concerning the code below, don’t hesitate to reach out though!\n\nlibrary(\"tidyverse\")\nlibrary(\"tidyxl\")\nlibrary(\"brotools\")\n\npath &lt;- Sys.glob(\"content/blog/2013*xlsx\")[-5]\n\nelections_raw_2013 &lt;- map(path, xlsx_cells) %&gt;%\n    map(~filter(., sheet != \"Sommaire\"))\n\nelections_sheets_2013 &lt;- map(map(path, xlsx_sheet_names), ~`%-l%`(., \"Sommaire\"))\n\nlist_targets &lt;- list(\"Centre\" = seq(9, 32),\n                    \"Est\" = seq(9, 18),\n                    \"Nord\" = seq(9, 20),\n                    \"Sud\" = seq(9, 34))\n\nposition_parties_national &lt;- seq(1, 24, by = 3)\n\nextract_party &lt;- function(dataset, starting_col, target_rows){\n    \n    almost_clean &lt;- dataset %&gt;%\n        filter(row %in% target_rows) %&gt;%\n        filter(col %in% c(starting_col, starting_col + 1)) %&gt;%\n        select(character, numeric) %&gt;%\n        fill(numeric, .direction = \"up\") %&gt;%\n        filter(!is.na(character))\n    \n    party_name &lt;- almost_clean$character[1]\n    \n    almost_clean$character[1] &lt;- \"Pourcentage\"\n    \n    almost_clean$party &lt;- party_name\n    \n    colnames(almost_clean) &lt;- c(\"Variables\", \"Values\", \"Party\")\n    \n    almost_clean %&gt;%\n        mutate(Year = 2013) %&gt;%\n        select(Party, Year, Variables, Values)\n    \n}\n\n\n# Treat one district\n\nextract_district &lt;- function(dataset, sheets, target_rows, position_parties_national){\n\n    list_data_districts &lt;- map(sheets, ~filter(.data = dataset, sheet == .)) \n\n    elections_districts_2013 &lt;- map(.x = list_data_districts,\n                                    ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = target_rows))\n\n    map2(.y = elections_districts_2013, .x = sheets,\n         ~mutate(.y, locality = .x, division = \"Commune\", Year = \"2013\")) %&gt;%\n        bind_rows()\n}\n\nelections_2013 &lt;- pmap_dfr(list(x = elections_raw_2013, \n          y = elections_sheets_2013,\n          z = list_targets), \n     function(x, y, z){\n         map_dfr(position_parties_national, \n             ~extract_district(dataset = x, sheets = y, target_rows = z, position_parties_national = .))\n     })\n\n# Correct districts\nelections_2013 &lt;- elections_2013 %&gt;%\n    mutate(division = case_when(locality == \"CENTRE\" ~ \"Electoral district\",\n                                locality == \"EST\" ~ \"Electoral district\",\n                                locality == \"NORD\" ~ \"Electoral district\",\n                                locality == \"SUD\" ~ \"Electoral district\",\n                                TRUE ~ division))"
  },
  {
    "objectID": "posts/2018-10-27-lux_elections_analysis.html#appendix",
    "href": "posts/2018-10-27-lux_elections_analysis.html#appendix",
    "title": "Maps with pie charts on top of each administrative division: an example with Luxembourg’s elections data",
    "section": "\nAppendix\n",
    "text": "Appendix\n\n\nThe following lines of code extract the data (from the 2013 elections) from the Excel Workbooks that can be found in Luxembourguish Open Data Portal.\n\n\nI will not comment them, as they work in a similar way than in the previous blog post where I extracted the data from the 2018 elections. The only difference, is that the sheet with the national level data was totally different, so I did not extract it. The first reason is because I don’t need it for this blog post, the second is because I was lazy. For me, that’s two pretty good reasons not to do something. If you have a question concerning the code below, don’t hesitate to reach out though!\n\nlibrary(\"tidyverse\")\nlibrary(\"tidyxl\")\nlibrary(\"brotools\")\n\npath &lt;- Sys.glob(\"content/blog/2013*xlsx\")[-5]\n\nelections_raw_2013 &lt;- map(path, xlsx_cells) %&gt;%\n    map(~filter(., sheet != \"Sommaire\"))\n\nelections_sheets_2013 &lt;- map(map(path, xlsx_sheet_names), ~`%-l%`(., \"Sommaire\"))\n\nlist_targets &lt;- list(\"Centre\" = seq(9, 32),\n                    \"Est\" = seq(9, 18),\n                    \"Nord\" = seq(9, 20),\n                    \"Sud\" = seq(9, 34))\n\nposition_parties_national &lt;- seq(1, 24, by = 3)\n\nextract_party &lt;- function(dataset, starting_col, target_rows){\n    \n    almost_clean &lt;- dataset %&gt;%\n        filter(row %in% target_rows) %&gt;%\n        filter(col %in% c(starting_col, starting_col + 1)) %&gt;%\n        select(character, numeric) %&gt;%\n        fill(numeric, .direction = \"up\") %&gt;%\n        filter(!is.na(character))\n    \n    party_name &lt;- almost_clean$character[1]\n    \n    almost_clean$character[1] &lt;- \"Pourcentage\"\n    \n    almost_clean$party &lt;- party_name\n    \n    colnames(almost_clean) &lt;- c(\"Variables\", \"Values\", \"Party\")\n    \n    almost_clean %&gt;%\n        mutate(Year = 2013) %&gt;%\n        select(Party, Year, Variables, Values)\n    \n}\n\n\n# Treat one district\n\nextract_district &lt;- function(dataset, sheets, target_rows, position_parties_national){\n\n    list_data_districts &lt;- map(sheets, ~filter(.data = dataset, sheet == .)) \n\n    elections_districts_2013 &lt;- map(.x = list_data_districts,\n                                    ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = target_rows))\n\n    map2(.y = elections_districts_2013, .x = sheets,\n         ~mutate(.y, locality = .x, division = \"Commune\", Year = \"2013\")) %&gt;%\n        bind_rows()\n}\n\nelections_2013 &lt;- pmap_dfr(list(x = elections_raw_2013, \n          y = elections_sheets_2013,\n          z = list_targets), \n     function(x, y, z){\n         map_dfr(position_parties_national, \n             ~extract_district(dataset = x, sheets = y, target_rows = z, position_parties_national = .))\n     })\n\n# Correct districts\nelections_2013 &lt;- elections_2013 %&gt;%\n    mutate(division = case_when(locality == \"CENTRE\" ~ \"Electoral district\",\n                                locality == \"EST\" ~ \"Electoral district\",\n                                locality == \"NORD\" ~ \"Electoral district\",\n                                locality == \"SUD\" ~ \"Electoral district\",\n                                TRUE ~ division))"
  },
  {
    "objectID": "posts/2024-02-02-nix_for_r_part_9.html",
    "href": "posts/2024-02-02-nix_for_r_part_9.html",
    "title": "Reproducible data science with Nix, part 9 – rix is looking for testers!",
    "section": "",
    "text": "After 5 months of work, Philipp Baumann and myself are happy to announce that our package, {rix} is getting quite close to being in a state we consider “done” (well, at least, for a first release). We plan on submit it first to rOpenSci for review, and later to CRAN. But in the meantime, if you could test the package, we’d be grateful! We are especially interested to see if you find the documentation clear, and if you are able to run the features that require an installation of Nix, the nix_build() and with_nix() functions. And I would truly recommend you read this blog post to the end, because I guarantee you’ll have your mind blown! If that’s not the case, send an insult my way on social media."
  },
  {
    "objectID": "posts/2024-02-02-nix_for_r_part_9.html#what-is-rix",
    "href": "posts/2024-02-02-nix_for_r_part_9.html#what-is-rix",
    "title": "Reproducible data science with Nix, part 9 – rix is looking for testers!",
    "section": "\nWhat is rix?\n",
    "text": "What is rix?\n\n\n{rix} is an R package that leverages Nix, a powerful package manager focusing on reproducible builds. With Nix, it is possible to create project-specific environments that contain a project-specific version of R and R packages (as well as other tools or languages, if needed). You can use {rix} and Nix to replace renv and Docker with one single tool. Nix is an incredibly useful piece of software for ensuring reproducibility of projects, in research or otherwise, or for running web applications like Shiny apps or plumber APIs in a controlled environment. The advantage of using Nix over Docker is that the environments that you define using Nix are not isolated from the rest of your machine: you can still access files and other tools installed on your computer.\n\n\nFor example, here is how you could use {rix} to generate a file called default.nix, which can then be used by Nix to actually build that environment for you:\n\nlibrary(rix)\n\npath_default_nix &lt;- tempdir()\n\nrix(r_ver = \"latest\",\n    r_pkgs = c(\"dplyr\", \"ggplot2\"),\n    system_pkgs = NULL,\n    git_pkgs = NULL,\n    ide = \"code\",\n    shell_hook = NULL,\n    project_path = path_default_nix,\n    overwrite = TRUE,\n    print = TRUE)\n## # This file was generated by the {rix} R package v0.5.1.9000 on 2024-02-02\n## # with following call:\n## # &gt;rix(r_ver = \"5ad9903c16126a7d949101687af0aa589b1d7d3d\",\n## #  &gt; r_pkgs = c(\"dplyr\",\n## #  &gt; \"ggplot2\"),\n## #  &gt; system_pkgs = NULL,\n## #  &gt; git_pkgs = NULL,\n## #  &gt; ide = \"code\",\n## #  &gt; project_path = path_default_nix,\n## #  &gt; overwrite = TRUE,\n## #  &gt; print = TRUE,\n## #  &gt; shell_hook = NULL)\n## # It uses nixpkgs' revision 5ad9903c16126a7d949101687af0aa589b1d7d3d for reproducibility purposes\n## # which will install R version latest\n## # Report any issues to https://github.com/b-rodrigues/rix\n## let\n##  pkgs = import (fetchTarball \"https://github.com/NixOS/nixpkgs/archive/5ad9903c16126a7d949101687af0aa589b1d7d3d.tar.gz\") {};\n##  rpkgs = builtins.attrValues {\n##   inherit (pkgs.rPackages) dplyr ggplot2 languageserver;\n## };\n##    system_packages = builtins.attrValues {\n##   inherit (pkgs) R glibcLocales nix ;\n## };\n##   in\n##   pkgs.mkShell {\n##     LOCALE_ARCHIVE = if pkgs.system == \"x86_64-linux\" then  \"${pkgs.glibcLocales}/lib/locale/locale-archive\" else \"\";\n##     LANG = \"en_US.UTF-8\";\n##     LC_ALL = \"en_US.UTF-8\";\n##     LC_TIME = \"en_US.UTF-8\";\n##     LC_MONETARY = \"en_US.UTF-8\";\n##     LC_PAPER = \"en_US.UTF-8\";\n##     LC_MEASUREMENT = \"en_US.UTF-8\";\n## \n##     buildInputs = [  rpkgs  system_packages  ];\n##       \n##   }\n\nYou don’t need to have Nix installed to use {rix} and generate this expression! This is especially useful if you want to generate an expression that should then be used in a CI/CD environment for example.\n\n\nBut if you do have Nix installed, then you can use two great functions that Philipp implemented, which we are really excited to tell you about!"
  },
  {
    "objectID": "posts/2024-02-02-nix_for_r_part_9.html#nix_build-and-with_nix",
    "href": "posts/2024-02-02-nix_for_r_part_9.html#nix_build-and-with_nix",
    "title": "Reproducible data science with Nix, part 9 – rix is looking for testers!",
    "section": "\nnix_build() and with_nix()\n",
    "text": "nix_build() and with_nix()\n\n\nWhen you have a default.nix file that was generated by rix::rix(), and if you have Nix installed on your system, you can build the corresponding environment using the command line tool nix-build. But you can also build that environment straight from an R session, by using rix::nix_build()!\n\n\nBut the reason nix_build() is really useful, is because it gets called by with_nix(). with_nix() is a very interesting function, because it allows you to evaluate a single function within a so-called subshell. That subshell can have a whole other version of R and R packages than your main session, and you can use it to execute an arbitrary function (or a whole, complex expression), and then get the result back into your main session. You could use older versions of packages to get a result that might not be possible to get in a current version. Consider the following example: on a recent version of {stringr}, stringr::str_subset(c(““,”a”), ““) results in an error, but older versions would return ”a”. Returning an error is actually what this should do, but hey, if you have code that relies on that old behaviour you can now execute that old code within a subshell that contains that older version of {stringr}. Start by creating a folder to contain everything needed for your subshell:\n\npath_env_stringr &lt;- file.path(\".\", \"_env_stringr_1.4.1\")\n\nThen, it is advised to use rix::rix_init() to generate an .Rprofile for that subshell, which sets a number of environment variables. This way, when the R session in that subshell starts, we don’t have any interference between that subshell and the main R session, as the R packages that must be available to the subshell are only taken from the Nix store. The Nix store is where software installed by Nix is… stored, and we don’t want R to be confused and go look for R packages in the user’s library, which could happen without this specific .Rprofile file:\n\nrix_init(\n  project_path = path_env_stringr,\n  rprofile_action = \"overwrite\",\n  message_type = \"simple\"\n)\n## \n## ### Bootstrapping isolated, project-specific, and runtime-pure R setup via Nix ###\n## \n## ==&gt; Created isolated nix-R project folder:\n##  /home/cbrunos/six_to/dev_env/b-rodrigues.github.com/content/blog/_env_stringr_1.4.1 \n## ==&gt; R session running via Nix (nixpkgs)\n## * R session not running from RStudio\n## ==&gt; Added `.Rprofile` file and code lines for new R sessions launched from:\n## /home/cbrunos/six_to/dev_env/b-rodrigues.github.com/content/blog/_env_stringr_1.4.1\n## \n## * Added the location of the Nix store to `PATH` environmental variable for new R sessions on host/docker RStudio:\n## /nix/var/nix/profiles/default/bin\n\nWe now generate the default.nix file for that subshell:\n\nrix(\n  r_ver = \"latest\",\n  r_pkgs = \"stringr@1.4.1\",\n  overwrite = TRUE,\n  project_path = path_env_stringr\n)\n\nNotice how we use the latest version of R (we could have used any other), but {stringr} on version 1.4.1. Finally, we use with_nix() to evaluate stringr::str_subset(c(““,”a”), ““) inside that subshell:\n\nout_nix_stringr &lt;- with_nix(\n  expr = function() stringr::str_subset(c(\"\", \"a\"), \"\"),\n  program = \"R\",\n  exec_mode = \"non-blocking\",\n  project_path = path_env_stringr,\n  message_type = \"simple\"\n)\n## * R session not running from RStudio\n## ### Prepare to exchange arguments and globals for `expr` between the host and Nix R sessions ###\n## * checking code in `expr` for potential problems:\n##  `codetools::checkUsage(fun = expr)`\n## \n## * checking code in `expr` for potential problems:\n## \n## * checking code in `globals_exprs` for potential problems:\n## \n## ==&gt; Running deparsed expression via `nix-shell` in non-blocking mode:\n## \n## \n## ==&gt; Process ID (PID) is 19688.\n## ==&gt; Receiving stdout and stderr streams...\n## \n## ==&gt; `expr` succeeded!\n## ### Finished code evaluation in `nix-shell` ###\n## \n## * Evaluating `expr` in `nix-shell` returns:\n## [1] \"a\"\n\nFinally, we can check if the result is really “a” or not:\n\nidentical(\"a\", out_nix_stringr)\n## [1] TRUE\n\nwith_nix() should work whether you installed your main R session using Nix, or not, but we’re not sure this is true for Windows (or rather, WSL2): we don’t have a Windows license to test this on Windows, so if you’re on Windows and use WSL2 and want to test this, we would be very happy to hear from you!\n\n\nIf you’re interested into using project-specific, and reproducible development environments, give {rix} and Nix a try! Learn more about {rix} on its Github repository here or website. We wrote many vignettes that are conveniently numbered, so don’t hesitate to get started!"
  },
  {
    "objectID": "posts/2018-12-02-hyper-parameters.html",
    "href": "posts/2018-12-02-hyper-parameters.html",
    "title": "What hyper-parameters are, and what to do with them; an illustration with ridge regression",
    "section": "",
    "text": "This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free here. This is taken from Chapter 7, which deals with statistical models. In the text below, I explain what hyper-parameters are, and as an example I run a ridge regression using the {glmnet} package. The book is still being written, so comments are more than welcome!"
  },
  {
    "objectID": "posts/2018-12-02-hyper-parameters.html#hyper-parameters",
    "href": "posts/2018-12-02-hyper-parameters.html#hyper-parameters",
    "title": "What hyper-parameters are, and what to do with them; an illustration with ridge regression",
    "section": "\nHyper-parameters\n",
    "text": "Hyper-parameters\n\n\nHyper-parameters are parameters of the model that cannot be directly learned from the data. A linear regression does not have any hyper-parameters, but a random forest for instance has several. You might have heard of ridge regression, lasso and elasticnet. These are extensions to linear models that avoid over-fitting by penalizing large models. These extensions of the linear regression have hyper-parameters that the practitioner has to tune. There are several ways one can tune these parameters, for example, by doing a grid-search, or a random search over the grid or using more elaborate methods. To introduce hyper-parameters, let’s get to know ridge regression, also called Tikhonov regularization.\n\n\n\nRidge regression\n\n\nRidge regression is used when the data you are working with has a lot of explanatory variables, or when there is a risk that a simple linear regression might overfit to the training data, because, for example, your explanatory variables are collinear. If you are training a linear model and then you notice that it generalizes very badly to new, unseen data, it is very likely that the linear model you trained overfits the data. In this case, ridge regression might prove useful. The way ridge regression works might seem counter-intuititive; it boils down to fitting a worse model to the training data, but in return, this worse model will generalize better to new data.\n\n\nThe closed form solution of the ordinary least squares estimator is defined as:\n\n\\[\\widehat{\\beta} = (X'X)^{-1}X'Y\\]\n\nwhere \\(X\\) is the design matrix (the matrix made up of the explanatory variables) and \\(Y\\) is the dependent variable. For ridge regression, this closed form solution changes a little bit:\n\n\\[\\widehat{\\beta} = (X'X + \\lambda I_p)^{-1}X'Y\\]\n\nwhere \\(lambda \\in \\mathbb{R}\\) is an hyper-parameter and \\(I_p\\) is the identity matrix of dimension \\(p\\) (\\(p\\) is the number of explanatory variables). This formula above is the closed form solution to the following optimisation program:\n\n\\[ \\sum_{i=1}^n \\left(y_i - \\sum_{j=1}^px_{ij}\\beta_j\\right)^2 \\]\n\nsuch that:\n\n\\[ \\sum_{j=1}^p(\\beta_j)^2 &lt; c\\]\n\nfor any strictly positive (c).\n\n\nThe glmnet() function from the {glmnet} package can be used for ridge regression, by setting the alpha argument to 0 (setting it to 1 would do LASSO, and setting it to a number between 0 and 1 would do elasticnet). But in order to compare linear regression and ridge regression, let me first divide the data into a training set and a testing set. I will be using the Housing data from the {Ecdat} package:\n\nlibrary(tidyverse)\nlibrary(Ecdat)\nlibrary(glmnet)\nindex &lt;- 1:nrow(Housing)\n\nset.seed(12345)\ntrain_index &lt;- sample(index, round(0.90*nrow(Housing)), replace = FALSE)\n\ntest_index &lt;- setdiff(index, train_index)\n\ntrain_x &lt;- Housing[train_index, ] %&gt;% \n    select(-price)\n\ntrain_y &lt;- Housing[train_index, ] %&gt;% \n    pull(price)\n\ntest_x &lt;- Housing[test_index, ] %&gt;% \n    select(-price)\n\ntest_y &lt;- Housing[test_index, ] %&gt;% \n    pull(price)\n\nI do the train/test split this way, because glmnet() requires a design matrix as input, and not a formula. Design matrices can be created using the model.matrix() function:\n\ntrain_matrix &lt;- model.matrix(train_y ~ ., data = train_x)\n\ntest_matrix &lt;- model.matrix(test_y ~ ., data = test_x)\n\nTo run an unpenalized linear regression, we can set the penalty to 0:\n\nmodel_lm_ridge &lt;- glmnet(y = train_y, x = train_matrix, alpha = 0, lambda = 0)\n\nThe model above provides the same result as a linear regression. Let’s compare the coefficients between the two:\n\ncoef(model_lm_ridge)\n## 13 x 1 sparse Matrix of class \"dgCMatrix\"\n##                       s0\n## (Intercept) -3247.030393\n## (Intercept)     .       \n## lotsize         3.520283\n## bedrooms     1745.211187\n## bathrms     14337.551325\n## stories      6736.679470\n## drivewayyes  5687.132236\n## recroomyes   5701.831289\n## fullbaseyes  5708.978557\n## gashwyes    12508.524241\n## aircoyes    12592.435621\n## garagepl     4438.918373\n## prefareayes  9085.172469\n\nand now the coefficients of the linear regression (because I provide a design matrix, I have to use lm.fit() instead of lm() which requires a formula, not a matrix.)\n\ncoef(lm.fit(x = train_matrix, y = train_y))\n##  (Intercept)      lotsize     bedrooms      bathrms      stories \n## -3245.146665     3.520357  1744.983863 14336.336858  6737.000410 \n##  drivewayyes   recroomyes  fullbaseyes     gashwyes     aircoyes \n##  5686.394123  5700.210775  5709.493884 12509.005265 12592.367268 \n##     garagepl  prefareayes \n##  4439.029607  9085.409155\n\nas you can see, the coefficients are the same. Let’s compute the RMSE for the unpenalized linear regression:\n\npreds_lm &lt;- predict(model_lm_ridge, test_matrix)\n\nrmse_lm &lt;- sqrt(mean((preds_lm - test_y)^2))\n\nThe RMSE for the linear unpenalized regression is equal to 14463.08.\n\n\nLet’s now run a ridge regression, with lambda equal to 100, and see if the RMSE is smaller:\n\nmodel_ridge &lt;- glmnet(y = train_y, x = train_matrix, alpha = 0, lambda = 100)\n\nand let’s compute the RMSE again:\n\npreds &lt;- predict(model_ridge, test_matrix)\n\nrmse &lt;- sqrt(mean((preds - test_y)^2))\n\nThe RMSE for the linear penalized regression is equal to 14460.71, which is smaller than before. But which value of lambda gives smallest RMSE? To find out, one must run model over a grid of lambda values and pick the model with lowest RMSE. This procedure is available in the cv.glmnet() function, which picks the best value for lambda:\n\nbest_model &lt;- cv.glmnet(train_matrix, train_y)\n# lambda that minimises the MSE\nbest_model$lambda.min\n## [1] 66.07936\n\nAccording to cv.glmnet() the best value for lambda is 66.0793576. In the next section, we will implement cross validation ourselves, in order to find the hyper-parameters of a random forest."
  },
  {
    "objectID": "posts/2020-12-05-poorman_translate.html",
    "href": "posts/2020-12-05-poorman_translate.html",
    "title": "Poorman’s automated translation with R and Google Sheets using {googlesheets4}",
    "section": "",
    "text": "A little trick I thought about this week; using Google Sheets, which includes a “googletranslate()” function to translate a survey that we’re preparing at work, from French to English, and using R of course. You’ll need a Google account for this. Also, keep in mind that you’ll be sending the text you want to translate to Google, so don’t go sending out anything sensitive.\n\n\nFirst, let’s load the needed packages:\n\nlibrary(googlesheets4)\nlibrary(dplyr)\nlibrary(tibble)\n\nAs an example, I’ll be defining a tibble with one column, and two rows. Each cell contains a sentence in French from the best show in the entire French speaking world, Kaamelott:\n\nmy_french_tibble &lt;- tribble(~french,\n                  \"J'apprécie les fruits au sirop\",\n                  \"C'est pas faux\")\n\nTo this tibble, I’m now adding two more columns, that contain the following string: “=googletranslate(A:A,”fr“,”en“)”. This is exactly what you would write in the formula bar in Sheets. Then, we need to convert that to an actual Google Sheets formula using gs4_formula():\n\n(\nmy_french_tibble &lt;- my_french_tibble %&gt;%\n  mutate(english = '=googletranslate(A:A, \"fr\", \"en\")') %&gt;%\n  mutate(portuguese = '=googletranslate(A:A, \"fr\", \"pt\")') %&gt;%\n  mutate(english = gs4_formula(english),\n         portuguese = gs4_formula(portuguese))\n)\n## Warning: `...` is not empty.\n## \n## We detected these problematic arguments:\n## * `needs_dots`\n## \n## These dots only exist to allow future extensions and should be empty.\n## Did you misspecify an argument?\n## # A tibble: 2 x 3\n##   french     english                           portuguese                       \n##   &lt;chr&gt;      &lt;fmla&gt;                            &lt;fmla&gt;                           \n## 1 J'appréci… =googletranslate(A:A, \"fr\", \"en\") =googletranslate(A:A, \"fr\", \"pt\")\n## 2 C'est pas… =googletranslate(A:A, \"fr\", \"en\") =googletranslate(A:A, \"fr\", \"pt\")\n\nWe’re ready to send this to Google Sheets. As soon as the sheet gets uploaded, the formulas will be evaluated, yielding translations in both English and Portuguese.\n\n\nTo upload the tibble to sheets, run the following:\n\nfrench_sheet &lt;- gs4_create(\"repliques_kaamelott\",\n                           sheets = list(perceval = my_french_tibble))\n\nYou’ll be asked if you want to cache your credentials so that you don’t need to re-authenticate between R sessions:\n\n\n\n\n\nYour browser will the open a tab asking you to login to Google:\n\n\n\n\n\nAt this point, you might get a notification on your phone, alerting you that there was a login to your account:\n\n\n\n\n\nIf you go on your Google Sheets account, this is what you’ll see:\n\n\n\n\n\nAnd if you open the sheet:\n\n\n\n\n\nPretty nice, no? You can of course download the workbook, or better yet, never leave your R session at all and simply get back the workbook using either the {googledrive} package, which simply needs the name of the workbook ({googledrive} also needs authentication):\n\n(\ntranslations &lt;- googledrive::drive_get(\"repliques_kaamelott\") %&gt;%\n  read_sheet\n)\n\nYou’ll get a new data frame with the translation:\n\nReading from \"repliques_kaamelott\"\nRange \"perceval\"\n# A tibble: 2 x 3\n  french                    english                     portuguese              \n  &lt;chr&gt;                     &lt;chr&gt;                       &lt;chr&gt;                   \n1 J'apprécie les fruits au… I appreciate the fruits in… I apreciar os frutos em…\n2 C'est pas faux            It is not false             Não é falsa             \n\nOr you can use the link to the sheet (which does not require to re-authenticate at this point):\n\ntranslations &lt;- read_sheet(\"the_link_goes_here\", \"perceval\")\n\nYou could of course encapsulate all these steps into a function and have any text translated very easily! Just be careful not to send out any confidential information out…"
  },
  {
    "objectID": "posts/2016-03-31-unit-testing-with-r.html",
    "href": "posts/2016-03-31-unit-testing-with-r.html",
    "title": "Unit testing with R",
    "section": "",
    "text": "I've been introduced to unit testing while working with colleagues on quite a big project for which we use Python.\n\n\nAt first I was a bit skeptical about the need of writing unit tests, but now I must admit that I am seduced by the idea and by the huge time savings it allows. Naturally, I was wondering if the same could be achieved with R, and was quite happy to find out that it also possible to write unit tests in R using a package called testthat.\n\n\nUnit tests (Not to be confused with unit root tests for time series) are small functions that test your code and help you make sure everything is alright. I'm going to show how the testthat packages works with a very trivial example, that might not do justice to the idea of unit testing. But you'll hopefully see why writing unit tests is not a waste of your time, especially if your project gets very complex (if you're writing a package for example).\n\n\nFirst, you'll need to download and install testthat. Some dependencies will also be installed.\n\n\nNow, you'll need a function to test. Let's suppose you've written a function that returns the nth Fibonacci number:\n\nFibonacci &lt;- function(n){\n    a &lt;- 0\n    b &lt;- 1\n    for (i in 1:n){\n        temp &lt;- b\n        b &lt;- a\n        a &lt;- a + temp\n    }\n    return(a)\n}\n\n\nYou then save this function in a file, let's call it fibo.R. What you'll probably do once you've written this function, is to try it out:\n\nFibonacci(5)\n\n## [1] 5\n\n\nYou'll see that the function returns the right result and continue programming. The idea behind unit testing is write a bunch of functions that you can run after you make changes to your code, just to check that everything is still running as it should.\n\n\nLet's create a script called test_fibo.R and write the following code in it:\n\ntest_that(\"Test Fibo(15)\",{\n  phi &lt;- (1 + sqrt(5))/2\n  psi &lt;- (1 - sqrt(5))/2\n  expect_equal(Fibonacci(15), (phi**15 - psi**15)/sqrt(5))\n})\n\n\nThe code above uses Binet's formula, a closed form formula that gives the nth Fibonacci number and compares it our implementation of the algorithm. If you didn't know about Binet's formula, you could simply compute some numbers by hand and compare them to what your function returns, for example. The function expect_equal is a function from the package testthat and does exactly what it tells. We expect the result of our implementation to be equal to the result of Binet's Formula. The file test_fibo.R can contain as many tests as you need. Also, the file that contains the tests must start with the string test, so that testthat knows with files it has to run.\n\n\nNow, we're almost done, create yet another script, let's call it run_tests.R and write the following code in it:\n\nlibrary(testthat) \n\nsource(\"path/to/fibo.R\")\n\ntest_results &lt;- test_dir(\"path/to/tests\", reporter=\"summary\")\n\n\nAfter running these lines, and if everything goes well, you should see a message like this:\n\n&gt; library(testthat)\n&gt; source(\"path/to/fibo.R\")\n&gt; test_results &lt;- test_dir(\"path/to/tests\", reporter=\"summary\")\n\n.\nYour tests are dandy! \n\n\nNotice the small . over the message? This means that one test was run successfully. You'll get one dot per successful test. If you take a look at test_results you'll see this:\n\n&gt; test_results\n         file context          test nb failed skipped error  user system  real\n1 test_fibo.R         Test Fibo(15)  1      0   FALSE FALSE 0.004      0 0.006\n\n\nYou'll see each file and each function inside the files that were tested, and also whether the test was skipped, failed etc. This may seem overkill for such a simple function, but imagine that you write dozens of functions that get more and more complex over time. You might have to change a lot of lines because as time goes by you add new functionality, but don't want to break what was working. Running your unit tests each time you make changes can help you pinpoint regressions in your code. Unit tests can also help you start with your code. It can happen that sometimes you don't know exactly how to start; well you could start by writing a unit test that returns the result you want to have and then try to write the code to make that unit test pass. This is called test-driven development.\n\n\nI hope that this post motivated you to write unit tests and make you a better R programmer!"
  },
  {
    "objectID": "posts/2017-08-27-why_tidyeval.html",
    "href": "posts/2017-08-27-why_tidyeval.html",
    "title": "Why I find tidyeval useful",
    "section": "",
    "text": "First thing’s first: maybe you shouldn’t care about tidyeval. Maybe you don’t need it. If you exclusively work interactively, I don’t think that learning about tidyeval is important. I can only speak for me, and explain to you why I personally find tidyeval useful.\n\n\nI wanted to write this blog post after reading this twitter thread and specifically this question.\n\n\nMara Averick then wrote this blogpost linking to 6 other blog posts that give some tidyeval examples. Reading them, plus the Programming with dplyr vignette should help you get started with tidyeval.\n\n\nBut maybe now you know how to use it, but not why and when you should use it… Basically, whenever you want to write a function that looks something like this:\n\nmy_function(my_data, one_column_inside_data)\n\nis when you want to use the power of tidyeval.\n\n\nI work at STATEC, Luxembourg’s national institute of statistics. I work on all kinds of different projects, and when data gets updated (for example because a new round of data collection for some survey finished), I run my own scripts on the fresh data to make the data nice and tidy for analysis. Because surveys get updated, sometimes column names change a little bit, and this can cause some issues.\n\n\nVery recently, a dataset I work with got updated. Data collection was finished, so I just loaded my hombrewed package written for this project, changed the path from last year’s script to this year’s fresh data path, ran the code, and watched as the folders got populated with new ggplot2 graphs and LaTeX tables with descriptive statistics and regression results. This is then used to generate this year’s report. However, by looking at the graphs, I noticed something weird; some graphs were showing some very strange patterns. It turns out that one column got its name changed, and also one of its values got changed too.\n\n\nLast year, this column, let’s call it spam, had values 1 for good and 0 for bad. This year the column is called Spam and the values are 1 and 2. When I found out that this was the source of the problem, I just had to change the arguments of my functions from\n\ngenerate_spam_plot(dataset = data2016, column = spam, value = 1)\ngenerate_spam_plot(dataset = data2016, column = spam, value = 0)\n\nto\n\ngenerate_spam_plot(dataset = data2017, column = Spam, value = 1)\ngenerate_spam_plot(dataset = data2017, column = Spam, value = 2)\n\nwithout needing to change anything else. This is why I use tidyeval; without it, writing a function such as genereta_spam_plot would not be easy. It would be possible, but not easy.\n\n\nIf you want to know more about tidyeval and working programmatically with R, I shamelessly invite you to read a book I’ve been working on: https://b-rodrigues.github.io/fput/ It’s still a WIP, but maybe you’ll find it useful. I plan on finishing it by the end of the year, but there’s already some content to keep you busy!"
  },
  {
    "objectID": "posts/2021-03-05-covid_lu.html",
    "href": "posts/2021-03-05-covid_lu.html",
    "title": "Using explainability methods to understand (some part) of the spread of COVID-19 in a landlocked country",
    "section": "",
    "text": "This blog post is based on an article I’m currently working on which you can find here. Contributions more than welcome!"
  },
  {
    "objectID": "posts/2021-03-05-covid_lu.html#you-expect-me-to-read-all-this",
    "href": "posts/2021-03-05-covid_lu.html#you-expect-me-to-read-all-this",
    "title": "Using explainability methods to understand (some part) of the spread of COVID-19 in a landlocked country",
    "section": "\n“You expect me to read all this?”\n",
    "text": "“You expect me to read all this?”\n\n\nThe gist of this blog post can be summarised in the following sentence: lagged positive cases of the neighbouring regions of Luxembourg predict weekly positive cases in Luxembourg. But prediction is not the goal of all this, but rather, understanding. Go grab a hot beverage and read on."
  },
  {
    "objectID": "posts/2021-06-04-own_knit_server.html#rage-is-my-fuel",
    "href": "posts/2021-06-04-own_knit_server.html#rage-is-my-fuel",
    "title": "Building your own knitr compile farm on your Raspberry Pi with {plumber}",
    "section": "\nRage is my fuel\n",
    "text": "Rage is my fuel\n\n\nI’ve had the {plumber} package on my radar for quite some time, but never tried it. However, a couple of weeks ago, I finally had a reason to try it out and see how the package works.\n\n\nOne of my main problems in life is that my work laptop runs Windows, and my second problem is that I need to compile () documents (via Rmarkdown) on Windows, and it’s just a pain. Not because of Rmarkdown, nor (), but because of Windows. Windows and UTF-8 don’t mix well, and I’ve grown so frustrated that I thought about creating my own Rmarkdown knitr compile farm using my Raspberry Pi 4 to solve this issue. The idea would be to send in the encrypted .Rmd file and get back an encrypted .pdf file. Dear reader, you surely think that this is overkill; let me assure you, it is not. I have wasted so much time on Windows because Windows is a joke that cannot properly handle THE MOST COMMON TEXT ENCODING IN THE UNIVERSE that this the only way out. Even Yihui Xie, the creator of the {knitr} package (among many others), wrote a blog post titled My Biggest Regret in the knitr Package, in which he explains how Windows’ crappy handling of UTF-8 made him make a regrettable decision. The issue Yihui Xie discusses is now resolved since {rmarkdown} version 2, as stated in the release notes (ctrl-f “utf-8”), but, for some reason, I still have problems with UTF-8 on Windows. While it is a fact that characters like the french é, è, ô, ç etc are now properly shown in a compiled document, any such character in a plot will not show properly, as you can see in the screenshot below:\n\n\n\n\n\nI did not really ever notice this issue in the past because I wrote 100% of my documents in English, but now that I’m a public servant in a country where French is the administrative language, man, am I having a bad time.\n\n\nNow, I make sure my .Rmd files are encoded in UTF-8, but I still get issues with plots. I tried changing the graphics device to Cairo or {ragg}, but I still have these issues.\n\n\nWho knows, maybe this is also a case of PEBKAC, but in that case it’s still Windows’ fault for making me feel bad.\n\n\nAnyway, this was reason enough for me to start developing an API that would allow me to get a nice looking PDF compiled on a serious operating system."
  },
  {
    "objectID": "posts/2021-06-04-own_knit_server.html#getting-started-docker",
    "href": "posts/2021-06-04-own_knit_server.html#getting-started-docker",
    "title": "Building your own knitr compile farm on your Raspberry Pi with {plumber}",
    "section": "\nGetting started: Docker\n",
    "text": "Getting started: Docker\n\n\nI started by writing a prototype on my local machine that (sort of, but not really) worked, but to put it on my Raspberry Pi I wanted to create a new Docker image to make deployment easier. For this, just like I did for this other blog post, I wrote a ’Dockerfile’ and pushed an image to Docker Hub. The Dockerfile is heavily inspired by hvalev’s Dockerfile, and also by the official plumber one you can find here. I then built the image on my Raspberry Pi.\n\n\nYou can use the Dockerfile to build your own image, which you can find here, or you can pull the one I pushed on Docker Hub. Now, something important: this Docker image does not contain my plumber.R file. So the first time you’re going to run it, it’ll fail. You’ll need to make one further adaptation on your server first.\n\n\nPut your plumber.R where you want, and copy the path to the file. For instance, suppose that you put the file at: /path/to/your/apis/plumber.R. Then, you can finally run the image like so:\n\ndocker run -d -it -p 8000:8000 -v /path/to/your/apis:/srv/plumber/ --rm --name tex-plumber tex-plumber:latest\n\nDocker looks for a plumber file inside /srv/plumber/ but that’s inside the image; this path gets sort of linked to your /path/to/your/apis/ and thus the plumber.R file you put there will be run. You can also put this there beforehand, adapt the Dockerfile and then build the image. It’s not the most elegant way to do it, but hey, I’m a beginner.\n\n\nThese instructions are very general and independent from my API I’m discussing here. What follows will be specific to my API."
  },
  {
    "objectID": "posts/2021-06-04-own_knit_server.html#an-api-that-ingests-an-rmd-file-and-spits-out-a-compiled-document",
    "href": "posts/2021-06-04-own_knit_server.html#an-api-that-ingests-an-rmd-file-and-spits-out-a-compiled-document",
    "title": "Building your own knitr compile farm on your Raspberry Pi with {plumber}",
    "section": "\nAn API that ingests an Rmd file and spits out a compiled document\n",
    "text": "An API that ingests an Rmd file and spits out a compiled document\n\n\nFirst of all, none of this would have been possible without the following Stackoverflow threads and Github repos:\n\n\n\nhttps://stackoverflow.com/questions/63808430/r-plumber-getting-as-excel-xlsx/63809737#63809737\n\n\nhttps://github.com/ChrisBeeley/reports_with_plumber/blob/master/plumber.R\n\n\nhttps://stackoverflow.com/questions/64639748/how-to-upload-a-xlsx-file-in-plumber-api-as-a-input\n\n\n\nand Bruno Tremblay’s help on this thread I made calling for help. You’ll probably notice that the answers in the stackoverflow threads all come from Bruno Tremblay, so a big thank you to him!\n\n\nWith his help, I was able to clob together this API:\n\n#* Knit Rmarkdown document\n#* @param data:file The Rmd file\n#* @param string The output format\n#* @post /knit\n# We use serializer contentType, the pdf serializer is the plot output from grDevices\n# Since the content is already in the right format from render, we just need to set\n# the content-type\n#* @serializer contentType list(type = \"application/gzip\")\nfunction(data, output_format) { \n  # Save the RMD file to a temporary location\n  rmd_doc &lt;- file.path(tempdir(), names(data))\n  writeBin(data[[1]], rmd_doc)\n  # render document to the selected output format\n  # (file will be saved side by side with source and with the right extension)\n  output &lt;- rmarkdown::render(rmd_doc, output_format)\n  tar(\"output.tar.gz\", normalizePath(output), compression = \"gzip\", tar = \"tar\")\n  # remove files on exit\n  on.exit({file.remove(rmd_doc, output, \"output.tar.gz\")}, add = TRUE)\n  # Include file in response as attachment\n  value &lt;- readBin(\"output.tar.gz\", \"raw\", file.info(\"output.tar.gz\")$size)\n  plumber::as_attachment(value, basename(\"output.tar.gz\"))\n}\n\nThis will go inside the plumber.R script. When the Docker image is running, you can hit the endpoint /knit to knit a document. But before discussing how to hit the API, let’s go through the above code.\n\nfunction(data, output_format) { \n  # Save the RMD file to a temporary location\n  rmd_doc &lt;- file.path(tempdir(), names(data))\n  writeBin(data[[1]], rmd_doc)\n\nThis function takes two arguments: data and output_format. data is your Rmd file (I should have named this better… oh well) that you will send via a POST. The Rmd will get written to a temporary location. In a previous version of the function I’ve used writeLines instead of writeBin which works as well.\n\n\nThe next lines render the output as the provided output format (through the second argument, output_format) and the output file gets compressed to a tar.gz archive. Why? The first reason is, obviously, to save precious bandwidth. The second, most important reason, is for the API to be able to download it.\n\n  output &lt;- rmarkdown::render(rmd_doc, output_format)\n  tar(\"output.tar.gz\", normalizePath(output), compression = \"gzip\", tar = \"tar\")\n\nThe way I understand how this works, is that if you want your API to return an attachment, you need to set the right content type. This is done by decorating the function with the right serializer:\n\n#* @serializer contentType list(type = \"application/gzip\")\n\nAt first I only wanted PDF files, and thus set the pdf serializer. This was a mistake, as the pdf serializer is only used if the API is supposed to return a plot (in the pdf format). When this was pointed out to me (in the Rstudio forums), Bruno Tremblay showed me the right solution:\n\n#* @serializer contentType list(type = \"application/pdf\")\n\nwhich worked! However, I then thought about how I would make the API more flexible by allowing the user to compile any format, and this is when I thought about compressing the file and returning a tar.gz file instead.\n\n\nThe first line of the final lines:\n\n  on.exit({file.remove(rmd_doc, output, \"output.tar.gz\")}, add = TRUE)\n  # Include file in response as attachment\n  value &lt;- readBin(\"output.tar.gz\", \"raw\", file.info(\"output.tar.gz\")$size)\n  plumber::as_attachment(value, basename(\"output.tar.gz\"))\n\nsimply clean up after exiting. The final lines read in the compressed file in a variable called variable which then gets downloaded automatically as an attachment.\n\n\nOk, so now, how do I get a document compiled? With the following script:\n\nlibrary(httr)\nlibrary(magrittr)\n\nmy_file &lt;- \"testmark\"\n\nres &lt;- \n  POST(\n    \"http://url_to_compile_farm:8000/knit?output_format=html_document\",\n    body = list(\n      data = upload_file(paste0(my_file, \".Rmd\"), \"text/plain\")\n    )\n  ) %&gt;%\n  content()\n\nnames(res)\n\noutput_filename &lt;- file(paste0(my_file, \".tar.gz\"), \"wb\")\nwriteBin(object = res, con = output_filename)\nclose(output_filename)\n\nThis script is saved in a folder which also contains testmark.Rmd, which is the Rmarkdown file I want to compile (and which gets sent to the server as the data argument). You’ll notice in the url that the second argument from my API is defined there:\n\n\"http://url_to_compile_farm:8000/knit?output_format=html_document\"\n\nyou can change html_document to pdf_document or word_document to get a PDF or Word document respectively.\n\n\nI’m pretty happy with this solution, even though it’s quite rough, and still needs some adjustments. For instance, I want to make sure that I can leave this API running without worry; so I need to build in some authentication mechanism, which will probably be quite primitive, but perhaps good enough. I also need to send and receive encrypted documents, and not plain text."
  },
  {
    "objectID": "posts/2021-06-04-own_knit_server.html#further-reading",
    "href": "posts/2021-06-04-own_knit_server.html#further-reading",
    "title": "Building your own knitr compile farm on your Raspberry Pi with {plumber}",
    "section": "\nFurther reading\n",
    "text": "Further reading\n\n\nIf you’re into tinkering with Raspberry Pi’s, Rstudio Server an {plumber}, Tyler Littlefield has a pretty cool github repo with lots of interesting stuff. Definitely give it a look!"
  },
  {
    "objectID": "posts/2018-04-14-playing_with_furrr.html",
    "href": "posts/2018-04-14-playing_with_furrr.html",
    "title": "Imputing missing values in parallel using {furrr}",
    "section": "",
    "text": "Today I saw this tweet on my timeline:\n\n\n\nFor those of us that just can't wait until RStudio officially supports parallel purrr in #rstats, boy have I got something for you. Introducing furrr, parallel purrr through the use of futures. Go ahead, break things, you know you want to:https://t.co/l9z1UC2Tew\n\n— Davis Vaughan (@dvaughan32) April 13, 2018\n\n\n\nand as a heavy {purrr} user, as well as the happy owner of a 6-core AMD Ryzen 5 1600X cpu, I was very excited to try out {furrr}. For those unfamiliar with {purrr}, you can read some of my previous blog posts on it here, here or here.\n\n\nTo summarize very quickly: {purrr} contains so-called higher order functions, which are functions that take other functions as argument. One such function is map(). Consider the following simple example:\n\nnumbers &lt;- seq(1, 10)\n\nIf you want the square root of this numbers, you can of course simply use the sqrt() function, because it is vectorized:\n\nsqrt(numbers)\n##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751\n##  [8] 2.828427 3.000000 3.162278\n\nBut in a lot of situations, the solution is not so simple. Sometimes you have to loop over the values. This is what we would need to do if sqrt() was not vectorized:\n\nsqrt_numbers &lt;- rep(0, 10)\n\nfor(i in length(numbers)){\n  sqrt_numbers[i] &lt;- sqrt(numbers[i])\n}\n\nFirst, you need to initialize a container, and then you have to populate the sqrt_numbers list with the results. Using, {purrr} is way easier:\n\nlibrary(tidyverse)\nmap(numbers, sqrt)\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 1.414214\n## \n## [[3]]\n## [1] 1.732051\n## \n## [[4]]\n## [1] 2\n## \n## [[5]]\n## [1] 2.236068\n## \n## [[6]]\n## [1] 2.44949\n## \n## [[7]]\n## [1] 2.645751\n## \n## [[8]]\n## [1] 2.828427\n## \n## [[9]]\n## [1] 3\n## \n## [[10]]\n## [1] 3.162278\n\nmap() is only one of the nice functions that are bundled inside {purrr}. Mastering {purrr} can really make you a much more efficient R programmer. Anyways, recently, I have been playing around with imputation and the {mice} package. {mice} comes with an example dataset called boys, let’s take a look at it:\n\nlibrary(mice)\n\ndata(boys)\n\nbrotools::describe(boys) %&gt;%\n  select(variable, type, n_missing, everything())\n## # A tibble: 9 x 13\n##   variable type    n_missing  nobs   mean    sd mode     min   max   q25\n##   &lt;chr&gt;    &lt;chr&gt;       &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numeric         0   748   9.16  6.89 0.035  0.035  21.2  1.58\n## 2 bmi      Numeric        21   748  18.1   3.05 14.54 11.8    31.7 15.9 \n## 3 hc       Numeric        46   748  51.5   5.91 33.7  33.7    65   48.1 \n## 4 hgt      Numeric        20   748 132.   46.5  50.1  50     198   84.9 \n## 5 tv       Numeric       522   748  11.9   7.99 &lt;NA&gt;   1      25    4   \n## 6 wgt      Numeric         4   748  37.2  26.0  3.65   3.14  117.  11.7 \n## 7 gen      Factor        503   748  NA    NA    &lt;NA&gt;  NA      NA   NA   \n## 8 phb      Factor        503   748  NA    NA    &lt;NA&gt;  NA      NA   NA   \n## 9 reg      Factor          3   748  NA    NA    south NA      NA   NA   \n## # ... with 3 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, n_unique &lt;int&gt;\n\nIn the code above I use the describe() function from my personal package to get some summary statistics of the boys dataset (you can read more about this function here). I am especially interested in the number of missing values, which is why I re-order the columns. If I did not re-order the columns, it would not appear in the output on my blog.\n\n\nWe see that some columns have a lot of missing values. Using the mice function, it is very easy to impute them:\n\nstart &lt;- Sys.time()\nimp_boys &lt;- mice(boys, m = 10, maxit = 100, printFlag = FALSE)\nend &lt;- Sys.time() - start\n\nprint(end)\n## Time difference of 3.290611 mins\n\nImputation on a single core took around 3 minutes on my computer. This might seem ok, but if you have a larger data set with more variables, 3 minutes can become 3 hours. And if you increase maxit, which helps convergence, or the number of imputations, 3 hours can become 30 hours. With a 6-core CPU this could potentially be brought down to 5 hours (in theory). Let’s see if we can go faster, but first let’s take a look at the imputed data.\n\n\nThe mice() function returns a mids object. If you want to look at the data, you have to use the complete() function (careful, there is also a complete() function in the {tidyr} package, so to avoid problems, I suggest you explicitely call mice::complete()):\n\nimp_boys &lt;- mice::complete(imp_boys, \"long\")\n\nbrotools::describe(imp_boys) %&gt;%\n  select(variable, type, n_missing, everything())\n## # A tibble: 11 x 13\n##    variable type   n_missing  nobs   mean     sd mode     min   max    q25\n##    &lt;chr&gt;    &lt;chr&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n##  1 .id      Numer…         0  7480 374.   216.   1      1     748   188.  \n##  2 .imp     Numer…         0  7480   5.5    2.87 1      1      10     3   \n##  3 age      Numer…         0  7480   9.16   6.89 0.035  0.035  21.2   1.58\n##  4 bmi      Numer…         0  7480  18.0    3.03 14.54 11.8    31.7  15.9 \n##  5 hc       Numer…         0  7480  51.6    5.89 33.7  33.7    65    48.3 \n##  6 hgt      Numer…         0  7480 131.    46.5  50.1  50     198    83   \n##  7 tv       Numer…         0  7480   8.39   8.09 2      1      25     2   \n##  8 wgt      Numer…         0  7480  37.1   26.0  3.65   3.14  117.   11.7 \n##  9 gen      Factor         0  7480  NA     NA    G1    NA      NA    NA   \n## 10 phb      Factor         0  7480  NA     NA    P1    NA      NA    NA   \n## 11 reg      Factor         0  7480  NA     NA    south NA      NA    NA   \n## # ... with 3 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, n_unique &lt;int&gt;\n\nAs expected, no more missing values. The “long” argument inside mice::complete() is needed if you want the complete() function to return a long dataset. Doing the above “manually” using {purrr} is possible with the following code:\n\nstart &lt;- Sys.time()\nimp_boys_purrr &lt;- map(rep(1, 10), ~mice(data = boys, m = ., maxit = 100, printFlag = FALSE))\nend &lt;- Sys.time() - start\n\nprint(end)\n## Time difference of 3.393966 mins\n\nWhat this does is map the function ~mice(data = boys, m = ., maxit = 100, printFlag = FALSE) to a list of 1s, and creates 10 imputed data sets. m = . means that m will be equal to whatever is inside the list we are mapping our function over, so 1, then 1 then another 1 etc…. It took around the same amount of time as using mice() directly.\n\n\nimp_boys_purrr is now a list of 10 mids objects. We thus need to map mice::complete() to imp_boys_purrr to get the data:\n\nimp_boys_purrr_complete &lt;- map(imp_boys_purrr, mice::complete)\n\nNow, imp_boys_purrr_complete is a list of 10 datasets. Let’s map brotools::describe() to it:\n\nmap(imp_boys_purrr_complete, brotools::describe)\n## [[1]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.03 14.54 11.8    31.7 15.9    17.4  19.5\n## 3 hc       Numer…   748  51.7   5.90 33.7  33.7    65   48.3    53.1  56  \n## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   84     146.  175. \n## 5 tv       Numer…   748   8.35  8.00 3      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.2  26.0  3.65   3.14  117.  11.7    34.7  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[2]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.03 14.54 11.8    31.7 15.9    17.5  19.5\n## 3 hc       Numer…   748  51.6   5.88 33.7  33.7    65   48.3    53.2  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.5   145.  175  \n## 5 tv       Numer…   748   8.37  8.02 1      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.1  26.0  3.65   3.14  117.  11.9    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P2    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[3]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.1   3.04 14.54 11.8    31.7 15.9    17.5  19.5\n## 3 hc       Numer…   748  51.6   5.87 33.7  33.7    65   48.5    53.3  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.0   145.  175  \n## 5 tv       Numer…   748   8.46  8.14 2      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.2  26.1  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[4]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.1   3.02 14.54 11.8    31.7 15.9    17.5  19.4\n## 3 hc       Numer…   748  51.7   5.93 33.7  33.7    65   48.5    53.4  56  \n## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   82.9   145.  175  \n## 5 tv       Numer…   748   8.45  8.11 2      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.2  26.0  3.65   3.14  117.  11.7    34.7  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[5]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.03 14.54 11.8    31.7 15.9    17.5  19.5\n## 3 hc       Numer…   748  51.6   5.91 33.7  33.7    65   48.3    53.2  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.0   146.  175. \n## 5 tv       Numer…   748   8.21  8.02 3      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.1  26.0  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[6]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.05 14.54 11.8    31.7 15.9    17.4  19.5\n## 3 hc       Numer…   748  51.7   5.89 33.7  33.7    65   48.3    53.2  56  \n## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   83.0   146.  175  \n## 5 tv       Numer…   748   8.44  8.24 3      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.1  26.0  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[7]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.1   3.04 14.54 11.8    31.7 15.9    17.4  19.5\n## 3 hc       Numer…   748  51.6   5.88 33.7  33.7    65   48.2    53.2  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.5   146.  175  \n## 5 tv       Numer…   748   8.47  8.15 2      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.2  26.1  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[8]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.04 14.54 11.8    31.7 15.9    17.4  19.4\n## 3 hc       Numer…   748  51.6   5.85 33.7  33.7    65   48.2    53.3  56  \n## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   83.0   146.  175  \n## 5 tv       Numer…   748   8.36  8.06 2      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.2  26.1  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[9]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.05 14.54 11.8    31.7 15.9    17.4  19.5\n## 3 hc       Numer…   748  51.6   5.90 33.7  33.7    65   48.3    53.2  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.9   146.  175  \n## 5 tv       Numer…   748   8.57  8.25 1      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.1  26.1  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[10]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.04 14.54 11.8    31.7 15.9    17.4  19.5\n## 3 hc       Numer…   748  51.6   5.89 33.7  33.7    65   48.3    53.1  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.0   146.  175  \n## 5 tv       Numer…   748   8.49  8.18 2      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.1  26.1  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n\nBefore merging this 10 datasets together into one, it would be nice to have a column with the id of the datasets. This can easily be done with a variant of purrr::map(), called map2():\n\nimp_boys_purrr &lt;- map2(.x = seq(1,10), .y = imp_boys_purrr_complete, ~mutate(.y, imp_id = as.character(.x)))\n\nmap2() applies a function, say f(), to 2 lists sequentially: f(x_1, y_1), then f(x_2, y_2), etc… So here I map mutate() to create a new column, imp_id in each dataset. Now let’s bind the rows and take a look at the data:\n\nimp_boys_purrr &lt;- bind_rows(imp_boys_purrr)\n\nimp_boys_purrr %&gt;%\n  brotools::describe() %&gt;%\n  select(variable, type, n_missing, everything())\n## # A tibble: 10 x 13\n##    variable type     n_missing  nobs   mean    sd mode     min   max   q25\n##    &lt;chr&gt;    &lt;chr&gt;        &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 age      Numeric          0  7480   9.16  6.89 0.035  0.035  21.2  1.58\n##  2 bmi      Numeric          0  7480  18.0   3.04 14.54 11.8    31.7 15.9 \n##  3 hc       Numeric          0  7480  51.6   5.89 33.7  33.7    65   48.3 \n##  4 hgt      Numeric          0  7480 131.   46.5  50.1  50     198   83   \n##  5 tv       Numeric          0  7480   8.42  8.11 3      1      25    2   \n##  6 wgt      Numeric          0  7480  37.1  26.0  3.65   3.14  117.  11.7 \n##  7 imp_id   Charact…         0  7480  NA    NA    1     NA      NA   NA   \n##  8 gen      Factor           0  7480  NA    NA    G1    NA      NA   NA   \n##  9 phb      Factor           0  7480  NA    NA    P1    NA      NA   NA   \n## 10 reg      Factor           0  7480  NA    NA    south NA      NA   NA   \n## # ... with 3 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, n_unique &lt;int&gt;\n\nYou may ask yourself why I am bothering with all this. This will become apparent now. We can now use the code we wrote to get our 10 imputed datasets using purrr::map() and simply use furrr::future_map() to parallelize the imputation process:\n\nlibrary(furrr)\n## Loading required package: future\nplan(multiprocess)\n\nstart &lt;- Sys.time()\nimp_boys_future &lt;- future_map(rep(1, 10), ~mice(data = boys, m = ., maxit = 100, printFlag = FALSE))\nend &lt;- Sys.time() - start\n\nprint(end)\n## Time difference of 33.73772 secs\n\nBoooom! Much faster! And simply by loading {furrr}, then using plan(multiprocess) to run the code in parallel (if you forget that, the code will run on a single core) and using future_map() instead of map().\n\n\nLet’s take a look at the data:\n\nimp_boys_future_complete &lt;- map(imp_boys_future, mice::complete)\n\nimp_boys_future &lt;- map2(.x = seq(1,10), .y = imp_boys_future_complete, ~mutate(.y, imp_id = as.character(.x)))\n\nimp_boys_future &lt;- bind_rows(imp_boys_future)\n\nimp_boys_future %&gt;%\n  brotools::describe() %&gt;%\n  select(variable, type, n_missing, everything())\n## # A tibble: 10 x 13\n##    variable type     n_missing  nobs   mean    sd mode     min   max   q25\n##    &lt;chr&gt;    &lt;chr&gt;        &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 age      Numeric          0  7480   9.16  6.89 0.035  0.035  21.2  1.58\n##  2 bmi      Numeric          0  7480  18.0   3.04 14.54 11.8    31.7 15.9 \n##  3 hc       Numeric          0  7480  51.6   5.89 33.7  33.7    65   48.4 \n##  4 hgt      Numeric          0  7480 131.   46.5  50.1  50     198   83   \n##  5 tv       Numeric          0  7480   8.35  8.09 3      1      25    2   \n##  6 wgt      Numeric          0  7480  37.1  26.0  3.65   3.14  117.  11.7 \n##  7 imp_id   Charact…         0  7480  NA    NA    1     NA      NA   NA   \n##  8 gen      Factor           0  7480  NA    NA    G1    NA      NA   NA   \n##  9 phb      Factor           0  7480  NA    NA    P1    NA      NA   NA   \n## 10 reg      Factor           0  7480  NA    NA    south NA      NA   NA   \n## # ... with 3 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, n_unique &lt;int&gt;\n\nSo imputation went from 3.4 minutes (around 200 seconds) to 30 seconds. How cool is that? If you want to play around with {furrr} you must install it from Github, as it is not yet available on CRAN:\n\ndevtools::install_github(\"DavisVaughan/furrr\")\n\nIf you are not comfortable with map() (and thus future_map()) but still want to impute in parallel, there is this very nice script here to do just that. I created a package around this script, called parlMICE (the same name as the script), to make installation and usage easier. You can install it like so:\n\ndevtools::install_github(\"b-rodrigues/parlMICE\")"
  },
  {
    "objectID": "posts/2019-11-02-mice_exp.html#introduction",
    "href": "posts/2019-11-02-mice_exp.html#introduction",
    "title": "Multiple data imputation and explainability",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nImputing missing values is quite an important task, but in my experience, very often, it is performed using very simplistic approaches. The basic approach is to impute missing values for numerical features using the average of each feature, or using the mode for categorical features. There are better ways of imputing missing values, for instance by predicting the values using a regression model, or KNN. However, imputing only once is not enough, because each imputed value carries with it a certain level of uncertainty. To account for this, it is better to perform multiple imputation. This means that if you impute your dataset 10 times, you’ll end up with 10 different datasets. Then, you should perform your analysis 10 times, for instance, if training a machine learning model, you should train it on the 10 datasets (and do a train/test split for each, even potentially tune a model for each). Finally, you should pool the results of these 10 analyses.\n\n\nI have met this approach in the social sciences and statistical literature in general, but very rarely in machine learning. Usually, in the social sciences, explainability is the goal of fitting statistical models to data, and the approach I described above is very well suited for this. Fit 10 (linear) regressions to each imputed dataset, and then pool the estimated coefficients/weights together. Rubin’s rule is used to pool these estimates. You can read more about this rule here. In machine learning, the task is very often prediction; in this case, you should pool the predictions. Computing the average and other statistics of the predictions seem to work just fine in practice.\n\n\nHowever, if you are mainly interested in explainability, how should you proceed? I’ve thought a bit about it, and the answer, is “exactly the same way”… I think. What I’m sure about, is you should impute m times, run the analysis m times (which in this case will include getting explanations) and then pool. So the idea is to be able to pool explanations."
  },
  {
    "objectID": "posts/2019-11-02-mice_exp.html#explainability-in-the-standard-case-no-missing-values",
    "href": "posts/2019-11-02-mice_exp.html#explainability-in-the-standard-case-no-missing-values",
    "title": "Multiple data imputation and explainability",
    "section": "\nExplainability in the “standard” case (no missing values)\n",
    "text": "Explainability in the “standard” case (no missing values)\n\n\nTo illustrate this idea, I’ll be using the {mice} package for multiple imputation, {h2o} for the machine learning bit and{iml} for explainability. Note that I could have used any other machine learning package instead of {h2o} as {iml} is totally package-agnostic. However, I have been experimenting with {h2o}’s automl implementation lately, so I happened to have code on hand. Let’s start with the “standard” case where the data does not have any missing values.\n\n\nFirst let’s load the needed packages and initialize h2o functions with h2o.init():\n\nlibrary(tidyverse)\nlibrary(Ecdat)\nlibrary(mice)\nlibrary(h2o)\nlibrary(iml)\nh2o.init()\n\nI’ll be using the DoctorContacts data. Here’s a description:\n\n\n\n\nClick to view the description of the data\n\n\nDoctorContacts              package:Ecdat              R Documentation\n\nContacts With Medical Doctor\n\nDescription:\n\n     a cross-section from 1977-1978\n\n     _number of observations_ : 20186\n\nUsage:\n\n     data(DoctorContacts)\n     \nFormat:\n\n     A time serie containing :\n\n     mdu number of outpatient visits to a medical doctor\n\n     lc log(coinsrate+1) where coinsurance rate is 0 to 100\n\n     idp individual deductible plan ?\n\n     lpi log(annual participation incentive payment) or 0 if no payment\n\n     fmde log(max(medical deductible expenditure)) if IDP=1 and MDE&gt;1\n          or 0 otherw\n\n     physlim physical limitation ?\n\n     ndisease number of chronic diseases\n\n     health self-rate health (excellent,good,fair,poor)\n\n     linc log of annual family income (in \\$)\n\n     lfam log of family size\n\n     educdec years of schooling of household head\n\n     age exact age\n\n     sex sex (male,female)\n\n     child age less than 18 ?\n\n     black is household head black ?\n\nSource:\n\n     Deb, P.  and P.K.  Trivedi (2002) “The Structure of Demand for\n     Medical Care: Latent Class versus Two-Part Models”, _Journal of\n     Health Economics_, *21*, 601-625.\n\nReferences:\n\n     Cameron, A.C.  and P.K.  Trivedi (2005) _Microeconometrics :\n     methods and applications_, Cambridge, pp. 553-556 and 565.\n\n\nThe task is to predict \"mdu\", the number of outpatient visits to an MD. Let’s prepare the data and split it into 3; a training, validation and holdout set.\n\ndata(\"DoctorContacts\")\n\ncontacts &lt;- as.h2o(DoctorContacts)\nsplits &lt;- h2o.splitFrame(data=contacts, ratios = c(0.7, 0.2))\n\noriginal_train &lt;- splits[[1]]\n\nvalidation &lt;- splits[[2]]\n\nholdout &lt;- splits[[3]]\n\nfeatures_names &lt;- setdiff(colnames(original_train), \"mdu\")\n\nAs you see, the ratios argument c(0.7, 0.2) does not add up to 1. This means that the first of the splits will have 70% of the data, the second split 20% and the final 10% will be the holdout set.\n\n\nLet’s first go with a poisson regression. To obtain the same results as with R’s built-in glm() function, I use the options below, as per H2o’s glm faq.\n\n\nIf you read Cameron and Trivedi’s Microeconometrics, where this data is presented in the context of count models, you’ll see that they also fit a negative binomial model 2 to this data, as it allows for overdispersion. Here, I’ll stick to a simple poisson regression, simply because the goal of this blog post is not to get the best model; as explained in the beginning, this is an attempt at pooling explanations when doing multiple imputation (and it’s also because GBMs, which I use below, do not handle the negative binomial model).\n\nglm_model &lt;- h2o.glm(y = \"mdu\", x = features_names,\n                     training_frame = original_train,\n                     validation_frame = validation,\n                     compute_p_values = TRUE,\n                     solver = \"IRLSM\",\n                     lambda = 0,\n                     remove_collinear_columns = TRUE,\n                     score_each_iteration = TRUE,\n                     family = \"poisson\", \n                     link = \"log\")\n\nNow that I have this simple model, which returns the (almost) same results as R’s glm() function, I can take a look at coefficients and see which are important, because GLMs are easily interpretable:\n\n\n\n\nClick to view h2o.glm()’s output\n\n\nsummary(glm_model)\n## Model Details:\n## ==============\n## \n## H2ORegressionModel: glm\n## Model Key:  GLM_model_R_1572735931328_5 \n## GLM Model: summary\n##    family link regularization number_of_predictors_total\n## 1 poisson  log           None                         16\n##   number_of_active_predictors number_of_iterations  training_frame\n## 1                          16                    5 RTMP_sid_8588_3\n## \n## H2ORegressionMetrics: glm\n## ** Reported on training data. **\n## \n## MSE:  17.6446\n## RMSE:  4.200547\n## MAE:  2.504063\n## RMSLE:  0.8359751\n## Mean Residual Deviance :  3.88367\n## R^2 :  0.1006768\n## Null Deviance :64161.44\n## Null D.o.F. :14131\n## Residual Deviance :54884.02\n## Residual D.o.F. :14115\n## AIC :83474.52\n## \n## \n## H2ORegressionMetrics: glm\n## ** Reported on validation data. **\n## \n## MSE:  20.85941\n## RMSE:  4.56721\n## MAE:  2.574582\n## RMSLE:  0.8403465\n## Mean Residual Deviance :  4.153042\n## R^2 :  0.09933874\n## Null Deviance :19667.55\n## Null D.o.F. :4078\n## Residual Deviance :16940.26\n## Residual D.o.F. :4062\n## AIC :25273.25\n## \n## \n## \n## \n## Scoring History: \n##             timestamp   duration iterations negative_log_likelihood\n## 1 2019-11-03 00:33:46  0.000 sec          0             64161.43611\n## 2 2019-11-03 00:33:46  0.004 sec          1             56464.99004\n## 3 2019-11-03 00:33:46  0.020 sec          2             54935.05581\n## 4 2019-11-03 00:33:47  0.032 sec          3             54884.19756\n## 5 2019-11-03 00:33:47  0.047 sec          4             54884.02255\n## 6 2019-11-03 00:33:47  0.063 sec          5             54884.02255\n##   objective\n## 1   4.54015\n## 2   3.99554\n## 3   3.88728\n## 4   3.88368\n## 5   3.88367\n## 6   3.88367\n## \n## Variable Importances: (Extract with `h2o.varimp`) \n## =================================================\n## \n##        variable relative_importance scaled_importance  percentage\n## 1    black.TRUE          0.67756097        1.00000000 0.236627982\n## 2   health.poor          0.48287163        0.71266152 0.168635657\n## 3  physlim.TRUE          0.33962316        0.50124369 0.118608283\n## 4   health.fair          0.25602066        0.37785627 0.089411366\n## 5      sex.male          0.19542639        0.28842628 0.068249730\n## 6      ndisease          0.16661902        0.24591001 0.058189190\n## 7      idp.TRUE          0.15703578        0.23176627 0.054842384\n## 8    child.TRUE          0.09988003        0.14741114 0.034881600\n## 9          linc          0.09830075        0.14508030 0.034330059\n## 10           lc          0.08126160        0.11993253 0.028379394\n## 11         lfam          0.07234463        0.10677213 0.025265273\n## 12         fmde          0.06622332        0.09773781 0.023127501\n## 13      educdec          0.06416087        0.09469387 0.022407220\n## 14  health.good          0.05501613        0.08119732 0.019213558\n## 15          age          0.03167598        0.04675000 0.011062359\n## 16          lpi          0.01938077        0.02860373 0.006768444\n\n\nAs a bonus, let’s see the output of the glm() function:\n\n\n\n\nClick to view glm()’s output\n\n\ntrain_tibble &lt;- as_tibble(original_train)\n\nr_glm &lt;- glm(mdu ~ ., data = train_tibble,\n            family = poisson(link = \"log\"))\n\nsummary(r_glm)\n## \n## Call:\n## glm(formula = mdu ~ ., family = poisson(link = \"log\"), data = train_tibble)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -5.7039  -1.7890  -0.8433   0.4816  18.4703  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  0.0005100  0.0585681   0.009   0.9931    \n## lc          -0.0475077  0.0072280  -6.573 4.94e-11 ***\n## idpTRUE     -0.1794563  0.0139749 -12.841  &lt; 2e-16 ***\n## lpi          0.0129742  0.0022141   5.860 4.63e-09 ***\n## fmde        -0.0166968  0.0042265  -3.951 7.80e-05 ***\n## physlimTRUE  0.3182780  0.0126868  25.087  &lt; 2e-16 ***\n## ndisease     0.0222300  0.0007215  30.811  &lt; 2e-16 ***\n## healthfair   0.2434235  0.0192873  12.621  &lt; 2e-16 ***\n## healthgood   0.0231824  0.0115398   2.009   0.0445 *  \n## healthpoor   0.4608598  0.0329124  14.003  &lt; 2e-16 ***\n## linc         0.0826053  0.0062208  13.279  &lt; 2e-16 ***\n## lfam        -0.1194981  0.0106904 -11.178  &lt; 2e-16 ***\n## educdec      0.0205582  0.0019404  10.595  &lt; 2e-16 ***\n## age          0.0041397  0.0005152   8.035 9.39e-16 ***\n## sexmale     -0.2096761  0.0104668 -20.032  &lt; 2e-16 ***\n## childTRUE    0.1529588  0.0179179   8.537  &lt; 2e-16 ***\n## blackTRUE   -0.6231230  0.0176758 -35.253  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 64043  on 14096  degrees of freedom\n## Residual deviance: 55529  on 14080  degrees of freedom\n## AIC: 84052\n## \n## Number of Fisher Scoring iterations: 6\n\n\nI could also use the excellent {ggeffects} package to see the marginal effects of different variables, for instance \"linc\":\n\nlibrary(ggeffects)\n\nggeffect(r_glm, \"linc\") %&gt;% \n    ggplot(aes(x, predicted)) +\n    geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \"#0f4150\") +\n    geom_line(colour = \"#82518c\") +\n    brotools::theme_blog()\n\n\n\n\nWe can see that as “linc” (and other covariates are held constant), the target variable increases.\n\n\nLet’s also take a look at the marginal effect of a categorical variable, namely \"sex\":\n\n\n\n\nClick to view another example of marginal effects\n\n\nlibrary(ggeffects)\n\nggeffect(r_glm, \"sex\") %&gt;% \n    ggplot(aes(x, predicted)) +\n    geom_point(colour = \"#82518c\") +\n    geom_errorbar(aes(x, ymin = conf.low, ymax = conf.high), colour = \"#82518c\") +\n    brotools::theme_blog()\n\n\n\nIn the case of the \"sex\" variable, men have significantly less doctor contacts than women.\n\n\nNow, let’s suppose that I want to train a model with a more complicated name, in order to justify my salary. Suppose I go with one of those nifty black-box models, for instance a GBM, which very likely will perform better than the GLM from before. GBMs are available in {h2o} via the h2o.gbm() function:\n\ngbm_model &lt;- h2o.gbm(y = \"mdu\", x = features_names,\n            training_frame = original_train,\n            validation_frame = validation,\n            distribution = \"poisson\",\n            score_each_iteration = TRUE,\n            ntrees = 110,\n            max_depth = 20,\n            sample_rate = 0.6,\n            col_sample_rate = 0.8,\n            col_sample_rate_per_tree = 0.9,\n            learn_rate = 0.05)\n\nTo find a set of good hyper-parameter values, I actually used h2o.automl() and then used the returned parameter values from the leader model. Maybe I’ll write another blog post about h2o.automl() one day, it’s quite cool. Anyways, now, how do I get me some explainability out of this? The model does perform better than the GLM as indicated by all the different metrics, but now I cannot compute any marginal effects, or anything like that. I do get feature importance by default with:\n\nh2o.varimp(gbm_model)\n## Variable Importances: \n##    variable relative_importance scaled_importance percentage\n## 1       age       380350.093750          1.000000   0.214908\n## 2      linc       282274.343750          0.742143   0.159492\n## 3  ndisease       245862.718750          0.646412   0.138919\n## 4       lpi       173552.734375          0.456297   0.098062\n## 5   educdec       148186.265625          0.389605   0.083729\n## 6      lfam       139174.312500          0.365911   0.078637\n## 7      fmde        94193.585938          0.247650   0.053222\n## 8    health        86160.679688          0.226530   0.048683\n## 9       sex        63502.667969          0.166958   0.035881\n## 10       lc        50674.968750          0.133232   0.028633\n## 11  physlim        45328.382812          0.119175   0.025612\n## 12    black        26376.841797          0.069349   0.014904\n## 13      idp        24809.185547          0.065227   0.014018\n## 14    child         9382.916992          0.024669   0.005302\n\nbut that’s it. And had I chosen a different “black-box” model, not based on trees, then I would not even have that. Thankfully, there’s the amazing {iml} package that contains a lot of functions for model-agnostic explanations. If you are not familiar with this package and the methods it implements, I highly encourage you to read the free online ebook written by the packages author, Christoph Molnar (who you can follow on Twitter).\n\n\nOut of the box, {iml} works with several machine learning frameworks, such as {caret} or {mlr} but not with {h2o}. However, this is not an issue; you only need to create a predict function which returns a data frame (h2o.predict() used for prediction with h2o models returns an h2o frame). I have found this interesting blog post from business-science.io which explains how to do this. I highly recommend you read this blog post, as it goes much deeper into the capabilities of {iml}.\n\n\nSo let’s write a predict function that {iml} can use:\n\n#source: https://www.business-science.io/business/2018/08/13/iml-model-interpretability.html\npredict_for_iml &lt;- function(model, newdata){\n  as_tibble(h2o.predict(model, as.h2o(newdata)))\n}\n\nAnd let’s now create a Predictor object. These objects are used by {iml} to create explanations:\n\njust_features &lt;- as_tibble(holdout[, 2:15])\nactual_target &lt;- as_tibble(holdout[, 1])\n\npredictor_original &lt;- Predictor$new(\n  model = gbm_model, \n  data = just_features, \n  y = actual_target, \n  predict.fun = predict_for_iml\n  )\n\npredictor_original can now be used to compute all kinds of explanations. I won’t go into much detail here, as this blog post is already quite long (and I haven’t even reached what I actually want to write about yet) and you can read more on the before-mentioned blog post or directly from Christoph Molnar’s ebook linked above.\n\n\nFirst, let’s compute a partial dependence plot, which shows the marginal effect of a variable on the outcome. This is to compare it to the one from the GLM model:\n\nfeature_effect_original &lt;- FeatureEffect$new(predictor_original, \"linc\", method = \"pdp\")\n\nplot(feature_effect_original) +\n    brotools::theme_blog()\n\n\n\nfeature_effect_original &lt;- FeatureEffect$new(predictor_original, \"linc\", method = \"pdp\")\n\nplot(feature_effect_original) +\n    brotools::theme_blog()\n\nQuite similar to the marginal effects from the GLM! Let’s now compute model-agnostic feature importances:\n\nfeature_importance_original &lt;- FeatureImp$new(predictor_original, loss = \"mse\")\n\nplot(feature_importance_original)\n\n\n\n\nAnd finally, the interaction effect of the sex variable interacted with all the others:\n\ninteraction_sex_original &lt;- Interaction$new(predictor_original, feature = \"sex\")\n\nplot(interaction_sex_original)\n\n\n\n\nOk so let’s assume that I’m happy with these explanations, and do need or want to go further. This would be the end of it in an ideal world, but this is not an ideal world unfortunately, but it’s the best we’ve got. In the real world, it often happens that data comes with missing values."
  },
  {
    "objectID": "posts/2019-11-02-mice_exp.html#missing-data-and-explainability",
    "href": "posts/2019-11-02-mice_exp.html#missing-data-and-explainability",
    "title": "Multiple data imputation and explainability",
    "section": "\nMissing data and explainability\n",
    "text": "Missing data and explainability\n\n\nAs explained in the beginning, I’ve been wondering how to deal with missing values when the goal of the analysis is explainability. How can the explanations be pooled? Let’s start with creating a data set with missing values, then perform multiple imputation, then perform the analysis.\n\n\nFirst, let me create a patterns matrix, that I will pass to the ampute() function from the {mice} package. This function creates a dataset with missing values, and by using its patterns argument, I can decide which columns should have missing values:\n\npatterns &lt;- -1*(diag(1, nrow = 15, ncol = 15) - 1)\n\npatterns[ ,c(seq(1, 6), c(9, 13))] &lt;- 0\n\namputed_train &lt;- ampute(as_tibble(original_train), prop = 0.1, patterns = patterns, mech = \"MNAR\")\n## Warning: Data is made numeric because the calculation of weights requires\n## numeric data\n\nLet’s take a look at the missingness pattern:\n\nnaniar::vis_miss(amputed_train$amp) + \n    brotools::theme_blog() + \n      theme(axis.text.x=element_text(angle=90, hjust=1))\n\n\n\n\nOk, so now let’s suppose that this was the dataset I was given. As a serious data scientist, I decide to perform multiple imputation first:\n\nimputed_train_data &lt;- mice(data = amputed_train$amp, m = 10)\n\nlong_train_data &lt;- complete(imputed_train_data, \"long\")\n\nSo because I performed multiple imputation 10 times, I now have 10 different datasets. I should now perform my analysis on these 10 datasets, which means I should run my GBM on each of them, and then get out the explanations for each of them. So let’s do just that. But first, let’s change the columns back to how they were; to perform amputation, the factor columns were converted to numbers:\n\nlong_train_data &lt;- long_train_data %&gt;% \n    mutate(idp = ifelse(idp == 1, FALSE, TRUE),\n           physlim = ifelse(physlim == 1, FALSE, TRUE),\n           health = as.factor(case_when(health == 1 ~ \"excellent\",\n                              health == 2 ~ \"fair\",\n                              health == 3 ~ \"good\", \n                              health == 4 ~  \"poor\")),\n           sex = as.factor(ifelse(sex == 1, \"female\", \"male\")),\n           child = ifelse(child == 1, FALSE, TRUE),\n           black = ifelse(black == 1, FALSE, TRUE))\n\nOk, so now we’re ready to go. I will use the h2o.gbm() function on each imputed data set. For this, I’ll use the group_by()-nest() trick which consists in grouping the dataset by the .imp column, then nesting it, then mapping the h2o.gbm() function to each imputed dataset. If you are not familiar with this, you can read this other blog post, which explains the approach. I define a custom function, train_on_imputed_data() to run h2o.gbm() on each imputed data set:\n\ntrain_on_imputed_data &lt;- function(long_data){\n    long_data %&gt;% \n        group_by(.imp) %&gt;% \n        nest() %&gt;% \n        mutate(model = map(data, ~h2o.gbm(y = \"mdu\", x = features_names,\n            training_frame = as.h2o(.),\n            validation_frame = validation,\n            distribution = \"poisson\",\n            score_each_iteration = TRUE,\n            ntrees = 110,\n            max_depth = 20,\n            sample_rate = 0.6,\n            col_sample_rate = 0.8,\n            col_sample_rate_per_tree = 0.9,\n            learn_rate = 0.05)))\n}\n\nNow the training takes place:\n\nimp_trained &lt;- train_on_imputed_data(long_train_data)\n\nLet’s take a look at imp_trained:\n\nimp_trained\n## # A tibble: 10 x 3\n## # Groups:   .imp [10]\n##     .imp            data model     \n##    &lt;int&gt; &lt;list&lt;df[,16]&gt;&gt; &lt;list&gt;    \n##  1     1   [14,042 × 16] &lt;H2ORgrsM&gt;\n##  2     2   [14,042 × 16] &lt;H2ORgrsM&gt;\n##  3     3   [14,042 × 16] &lt;H2ORgrsM&gt;\n##  4     4   [14,042 × 16] &lt;H2ORgrsM&gt;\n##  5     5   [14,042 × 16] &lt;H2ORgrsM&gt;\n##  6     6   [14,042 × 16] &lt;H2ORgrsM&gt;\n##  7     7   [14,042 × 16] &lt;H2ORgrsM&gt;\n##  8     8   [14,042 × 16] &lt;H2ORgrsM&gt;\n##  9     9   [14,042 × 16] &lt;H2ORgrsM&gt;\n## 10    10   [14,042 × 16] &lt;H2ORgrsM&gt;\n\nWe see that the column model contains one model for each imputed dataset. Now comes the part I wanted to write about (finally): getting explanations out of this. Getting the explanations from each model is not the hard part, that’s easily done using some {tidyverse} magic (if you’re following along, run this bit of code below, and go make dinner, have dinner, and wash the dishes, because it takes time to run):\n\nmake_predictors &lt;- function(model){\n    Predictor$new(\n        model = model, \n        data = just_features, \n        y = actual_target, \n        predict.fun = predict_for_iml\n        )\n}\n\nmake_effect &lt;- function(predictor_object, feature = \"linc\", method = \"pdp\"){\n    FeatureEffect$new(predictor_object, feature, method)\n}\n\nmake_feat_imp &lt;- function(predictor_object, loss = \"mse\"){\n    FeatureImp$new(predictor_object, loss)\n}\n\nmake_interactions &lt;- function(predictor_object, feature = \"sex\"){\n    Interaction$new(predictor_object, feature = feature)\n}\n\nimp_trained &lt;- imp_trained %&gt;%\n    mutate(predictors = map(model, make_predictors)) %&gt;% \n    mutate(effect_linc = map(predictors, make_effect)) %&gt;% \n    mutate(feat_imp = map(predictors, make_feat_imp)) %&gt;% \n    mutate(interactions_sex = map(predictors, make_interactions))\n\nOk so now that I’ve got these explanations, I am done with my analysis. This is the time to pool the results together. Remember, in the case of regression models as used in the social sciences, this means averaging the estimated model parameters and using Rubin’s rule to compute their standard errors. But in this case, this is not so obvious. Should the explanations be averaged? Should I instead analyse them one by one, and see if they differ? My gut feeling is that they shouldn’t differ much, but who knows? Perhaps the answer is doing a bit of both. I have checked online for a paper that would shed some light into this, but have not found any. So let’s take a closer look to the explanations. Let’s look at feature importance:\n\n\n\n\nClick to view the 10 feature importances\n\n\nimp_trained %&gt;% \n    pull(feat_imp)\n## [[1]]\n## Interpretation method:  FeatureImp \n## error function: mse\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##    feature importance.05 importance importance.95 permutation.error\n## 1 ndisease     1.0421605   1.362672      1.467244          22.03037\n## 2     fmde     0.8611917   1.142809      1.258692          18.47583\n## 3      lpi     0.8706659   1.103367      1.196081          17.83817\n## 4   health     0.8941010   1.098014      1.480508          17.75164\n## 5       lc     0.8745229   1.024288      1.296668          16.55970\n## 6    black     0.7537278   1.006294      1.095054          16.26879\n## \n## [[2]]\n## Interpretation method:  FeatureImp \n## error function: mse\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##    feature importance.05 importance importance.95 permutation.error\n## 1      age      0.984304   1.365702      1.473146          22.52529\n## 2     linc      1.102023   1.179169      1.457907          19.44869\n## 3 ndisease      1.075821   1.173938      1.642938          19.36241\n## 4     fmde      1.059303   1.150112      1.281291          18.96944\n## 5       lc      0.837573   1.132719      1.200556          18.68257\n## 6  physlim      0.763757   1.117635      1.644434          18.43379\n## \n## [[3]]\n## Interpretation method:  FeatureImp \n## error function: mse\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##    feature importance.05 importance importance.95 permutation.error\n## 1      age     0.8641304   1.334382      1.821797          21.62554\n## 2    black     1.0553001   1.301338      1.429119          21.09001\n## 3     fmde     0.8965085   1.208761      1.360217          19.58967\n## 4 ndisease     1.0577766   1.203418      1.651611          19.50309\n## 5     linc     0.9299725   1.114041      1.298379          18.05460\n## 6      sex     0.9854144   1.091391      1.361406          17.68754\n## \n## [[4]]\n## Interpretation method:  FeatureImp \n## error function: mse\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##   feature importance.05 importance importance.95 permutation.error\n## 1 educdec     0.9469049   1.263961      1.358115          20.52909\n## 2     age     1.0980269   1.197441      1.763202          19.44868\n## 3  health     0.8539843   1.133338      1.343389          18.40753\n## 4    linc     0.7608811   1.123423      1.328756          18.24649\n## 5     lpi     0.8203850   1.103394      1.251688          17.92118\n## 6   black     0.9476909   1.089861      1.328960          17.70139\n## \n## [[5]]\n## Interpretation method:  FeatureImp \n## error function: mse\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##   feature importance.05 importance importance.95 permutation.error\n## 1     lpi     0.9897789   1.336405      1.601778          22.03791\n## 2 educdec     0.8701162   1.236741      1.424602          20.39440\n## 3     age     0.8537786   1.181242      1.261411          19.47920\n## 4    lfam     1.0185313   1.133158      1.400151          18.68627\n## 5     idp     0.9502284   1.069772      1.203147          17.64101\n## 6    linc     0.8600586   1.042453      1.395231          17.19052\n## \n## [[6]]\n## Interpretation method:  FeatureImp \n## error function: mse\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##   feature importance.05 importance importance.95 permutation.error\n## 1      lc     0.7707383   1.208190      1.379422          19.65436\n## 2     sex     0.9309901   1.202629      1.479511          19.56391\n## 3    linc     1.0549563   1.138404      1.624217          18.51912\n## 4     lpi     0.9360817   1.135198      1.302084          18.46696\n## 5 physlim     0.7357272   1.132525      1.312584          18.42349\n## 6   child     1.0199964   1.109120      1.316306          18.04274\n## \n## [[7]]\n## Interpretation method:  FeatureImp \n## error function: mse\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##    feature importance.05 importance importance.95 permutation.error\n## 1     linc     0.9403425   1.262994      1.511122          20.65942\n## 2       lc     1.0481333   1.233136      1.602796          20.17103\n## 3 ndisease     1.1612194   1.212454      1.320208          19.83272\n## 4  educdec     0.7924637   1.197343      1.388218          19.58554\n## 5     lfam     0.8423790   1.178545      1.349884          19.27805\n## 6      age     0.9125829   1.168297      1.409525          19.11043\n## \n## [[8]]\n## Interpretation method:  FeatureImp \n## error function: mse\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##    feature importance.05 importance importance.95 permutation.error\n## 1      age     1.1281736   1.261273      1.609524          20.55410\n## 2   health     0.9134557   1.240597      1.432366          20.21716\n## 3     lfam     0.7469043   1.182294      1.345910          19.26704\n## 4      lpi     0.8088552   1.160863      1.491139          18.91779\n## 5 ndisease     1.0756671   1.104357      1.517278          17.99695\n## 6     fmde     0.6929092   1.093465      1.333544          17.81946\n## \n## [[9]]\n## Interpretation method:  FeatureImp \n## error function: mse\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##    feature importance.05 importance importance.95 permutation.error\n## 1  educdec     1.0188109   1.287697      1.381982          20.92713\n## 2      lpi     0.9853336   1.213095      1.479002          19.71473\n## 3     linc     0.8354715   1.195344      1.254350          19.42625\n## 4      age     0.9980451   1.179371      1.383545          19.16666\n## 5 ndisease     1.0492685   1.176804      1.397398          19.12495\n## 6     lfam     1.0814043   1.166626      1.264592          18.95953\n## \n## [[10]]\n## Interpretation method:  FeatureImp \n## error function: mse\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##    feature importance.05 importance importance.95 permutation.error\n## 1      age     0.9538824   1.211869      1.621151          19.53671\n## 2      sex     0.9148921   1.211253      1.298311          19.52678\n## 3     lfam     0.8227355   1.093094      1.393815          17.62192\n## 4 ndisease     0.8282127   1.090779      1.205994          17.58459\n## 5       lc     0.7004401   1.060870      1.541697          17.10244\n## 6   health     0.8137149   1.058324      1.183639          17.06138\n\n\nAs you can see, the feature importances are quite different from each other, but I don’t think this comes from the imputations, but rather from the fact that feature importance depends on shuffling the feature, which adds randomness to the measurement (source: https://christophm.github.io/interpretable-ml-book/feature-importance.html#disadvantages-9). To mitigate this, Christoph Molnar suggests repeating the the permutation and averaging the importance measures; I think that this would be my approach for pooling as well.\n\n\nLet’s now take a look at interactions:\n\n\n\n\nClick to view the 10 interactions\n\n\nimp_trained %&gt;% \n    pull(interactions_sex)\n## [[1]]\n## Interpretation method:  Interaction \n## \n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##       .feature .interaction\n## 1       lc:sex   0.07635197\n## 2      idp:sex   0.08172754\n## 3      lpi:sex   0.10704357\n## 4     fmde:sex   0.11267146\n## 5  physlim:sex   0.04099073\n## 6 ndisease:sex   0.16314524\n## \n## [[2]]\n## Interpretation method:  Interaction \n## \n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##       .feature .interaction\n## 1       lc:sex   0.10349820\n## 2      idp:sex   0.07432519\n## 3      lpi:sex   0.11651413\n## 4     fmde:sex   0.18123926\n## 5  physlim:sex   0.12952808\n## 6 ndisease:sex   0.14528876\n## \n## [[3]]\n## Interpretation method:  Interaction \n## \n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##       .feature .interaction\n## 1       lc:sex   0.05919320\n## 2      idp:sex   0.05586197\n## 3      lpi:sex   0.24253335\n## 4     fmde:sex   0.05240474\n## 5  physlim:sex   0.06404969\n## 6 ndisease:sex   0.14508072\n## \n## [[4]]\n## Interpretation method:  Interaction \n## \n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##       .feature .interaction\n## 1       lc:sex   0.02775529\n## 2      idp:sex   0.02050390\n## 3      lpi:sex   0.11781130\n## 4     fmde:sex   0.11084240\n## 5  physlim:sex   0.17932694\n## 6 ndisease:sex   0.07181589\n## \n## [[5]]\n## Interpretation method:  Interaction \n## \n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##       .feature .interaction\n## 1       lc:sex   0.12873151\n## 2      idp:sex   0.03681428\n## 3      lpi:sex   0.15879389\n## 4     fmde:sex   0.16952900\n## 5  physlim:sex   0.07031520\n## 6 ndisease:sex   0.10567463\n## \n## [[6]]\n## Interpretation method:  Interaction \n## \n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##       .feature .interaction\n## 1       lc:sex   0.15320481\n## 2      idp:sex   0.08645037\n## 3      lpi:sex   0.16674641\n## 4     fmde:sex   0.14671054\n## 5  physlim:sex   0.09236257\n## 6 ndisease:sex   0.14605618\n## \n## [[7]]\n## Interpretation method:  Interaction \n## \n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##       .feature .interaction\n## 1       lc:sex   0.04072960\n## 2      idp:sex   0.05641868\n## 3      lpi:sex   0.19491959\n## 4     fmde:sex   0.07119644\n## 5  physlim:sex   0.05777469\n## 6 ndisease:sex   0.16555363\n## \n## [[8]]\n## Interpretation method:  Interaction \n## \n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##       .feature .interaction\n## 1       lc:sex   0.04979709\n## 2      idp:sex   0.06036898\n## 3      lpi:sex   0.14009307\n## 4     fmde:sex   0.10927688\n## 5  physlim:sex   0.08761533\n## 6 ndisease:sex   0.20544585\n## \n## [[9]]\n## Interpretation method:  Interaction \n## \n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##       .feature .interaction\n## 1       lc:sex   0.08572075\n## 2      idp:sex   0.12254979\n## 3      lpi:sex   0.17532347\n## 4     fmde:sex   0.12557420\n## 5  physlim:sex   0.05084209\n## 6 ndisease:sex   0.13977328\n## \n## [[10]]\n## Interpretation method:  Interaction \n## \n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##       .feature .interaction\n## 1       lc:sex   0.08636490\n## 2      idp:sex   0.04807331\n## 3      lpi:sex   0.17922280\n## 4     fmde:sex   0.05728403\n## 5  physlim:sex   0.09392774\n## 6 ndisease:sex   0.13408956\n\n\nIt would seem that interactions are a bit more stable. Let’s average the values; for this I need to access the results element of the interactions object and the result out:\n\ninteractions_sex_result &lt;- imp_trained %&gt;% \n    mutate(interactions_results = map(interactions_sex, function(x)(x$results))) %&gt;% \n    pull()\n\ninteractions_sex_result is a list of dataframes, which means I can bind the rows together and compute whatever I need:\n\ninteractions_sex_result %&gt;% \n    bind_rows() %&gt;% \n    group_by(.feature) %&gt;% \n    summarise_at(.vars = vars(.interaction), \n                 .funs = funs(mean, sd, low_ci = quantile(., 0.05), high_ci = quantile(., 0.95)))\n## # A tibble: 13 x 5\n##    .feature       mean     sd low_ci high_ci\n##    &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1 age:sex      0.294  0.0668 0.181    0.369\n##  2 black:sex    0.117  0.0286 0.0763   0.148\n##  3 child:sex    0.0817 0.0308 0.0408   0.125\n##  4 educdec:sex  0.148  0.0411 0.104    0.220\n##  5 fmde:sex     0.114  0.0443 0.0546   0.176\n##  6 health:sex   0.130  0.0190 0.104    0.151\n##  7 idp:sex      0.0643 0.0286 0.0278   0.106\n##  8 lc:sex       0.0811 0.0394 0.0336   0.142\n##  9 lfam:sex     0.149  0.0278 0.125    0.198\n## 10 linc:sex     0.142  0.0277 0.104    0.179\n## 11 lpi:sex      0.160  0.0416 0.111    0.221\n## 12 ndisease:sex 0.142  0.0356 0.0871   0.187\n## 13 physlim:sex  0.0867 0.0415 0.0454   0.157\n\nThat seems pretty good. Now, what about the partial dependence? Let’s take a closer look:\n\n\n\n\nClick to view the 10 pdps\n\n\nimp_trained %&gt;% \n    pull(effect_linc)\n## [[1]]\n## Interpretation method:  FeatureEffect \n## features: linc[numerical]\n## grid size: 20\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##        linc   .y.hat .type\n## 1 0.0000000 1.652445   pdp\n## 2 0.5312226 1.687522   pdp\n## 3 1.0624453 1.687522   pdp\n## 4 1.5936679 1.687522   pdp\n## 5 2.1248905 1.685088   pdp\n## 6 2.6561132 1.694112   pdp\n## \n## [[2]]\n## Interpretation method:  FeatureEffect \n## features: linc[numerical]\n## grid size: 20\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##        linc   .y.hat .type\n## 1 0.0000000 1.813449   pdp\n## 2 0.5312226 1.816195   pdp\n## 3 1.0624453 1.816195   pdp\n## 4 1.5936679 1.816195   pdp\n## 5 2.1248905 1.804457   pdp\n## 6 2.6561132 1.797238   pdp\n## \n## [[3]]\n## Interpretation method:  FeatureEffect \n## features: linc[numerical]\n## grid size: 20\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##        linc   .y.hat .type\n## 1 0.0000000 1.906515   pdp\n## 2 0.5312226 2.039318   pdp\n## 3 1.0624453 2.039318   pdp\n## 4 1.5936679 2.039318   pdp\n## 5 2.1248905 2.002970   pdp\n## 6 2.6561132 2.000922   pdp\n## \n## [[4]]\n## Interpretation method:  FeatureEffect \n## features: linc[numerical]\n## grid size: 20\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##        linc   .y.hat .type\n## 1 0.0000000 1.799552   pdp\n## 2 0.5312226 2.012634   pdp\n## 3 1.0624453 2.012634   pdp\n## 4 1.5936679 2.012634   pdp\n## 5 2.1248905 1.982425   pdp\n## 6 2.6561132 1.966392   pdp\n## \n## [[5]]\n## Interpretation method:  FeatureEffect \n## features: linc[numerical]\n## grid size: 20\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##        linc   .y.hat .type\n## 1 0.0000000 1.929158   pdp\n## 2 0.5312226 1.905171   pdp\n## 3 1.0624453 1.905171   pdp\n## 4 1.5936679 1.905171   pdp\n## 5 2.1248905 1.879721   pdp\n## 6 2.6561132 1.869113   pdp\n## \n## [[6]]\n## Interpretation method:  FeatureEffect \n## features: linc[numerical]\n## grid size: 20\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##        linc   .y.hat .type\n## 1 0.0000000 2.147697   pdp\n## 2 0.5312226 2.162393   pdp\n## 3 1.0624453 2.162393   pdp\n## 4 1.5936679 2.162393   pdp\n## 5 2.1248905 2.119923   pdp\n## 6 2.6561132 2.115131   pdp\n## \n## [[7]]\n## Interpretation method:  FeatureEffect \n## features: linc[numerical]\n## grid size: 20\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##        linc   .y.hat .type\n## 1 0.0000000 1.776742   pdp\n## 2 0.5312226 1.957938   pdp\n## 3 1.0624453 1.957938   pdp\n## 4 1.5936679 1.957938   pdp\n## 5 2.1248905 1.933847   pdp\n## 6 2.6561132 1.885287   pdp\n## \n## [[8]]\n## Interpretation method:  FeatureEffect \n## features: linc[numerical]\n## grid size: 20\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##        linc   .y.hat .type\n## 1 0.0000000 2.020647   pdp\n## 2 0.5312226 2.017981   pdp\n## 3 1.0624453 2.017981   pdp\n## 4 1.5936679 2.017981   pdp\n## 5 2.1248905 1.981122   pdp\n## 6 2.6561132 2.017604   pdp\n## \n## [[9]]\n## Interpretation method:  FeatureEffect \n## features: linc[numerical]\n## grid size: 20\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##        linc   .y.hat .type\n## 1 0.0000000 1.811189   pdp\n## 2 0.5312226 2.003053   pdp\n## 3 1.0624453 2.003053   pdp\n## 4 1.5936679 2.003053   pdp\n## 5 2.1248905 1.938150   pdp\n## 6 2.6561132 1.918518   pdp\n## \n## [[10]]\n## Interpretation method:  FeatureEffect \n## features: linc[numerical]\n## grid size: 20\n## \n## Analysed predictor: \n## Prediction task: unknown \n## \n## \n## Analysed data:\n## Sampling from data.frame with 2013 rows and 14 columns.\n## \n## Head of results:\n##        linc   .y.hat .type\n## 1 0.0000000 1.780325   pdp\n## 2 0.5312226 1.850203   pdp\n## 3 1.0624453 1.850203   pdp\n## 4 1.5936679 1.850203   pdp\n## 5 2.1248905 1.880805   pdp\n## 6 2.6561132 1.881305   pdp\n\n\nAs you can see, the values are quite similar. I think that in the case of plots, the best way to visualize the impact of the imputation is to simply plot all the lines in a single plot:\n\neffect_linc_results &lt;- imp_trained %&gt;% \n    mutate(effect_linc_results = map(effect_linc, function(x)(x$results))) %&gt;% \n    select(.imp, effect_linc_results) %&gt;% \n    unnest(effect_linc_results)\n\neffect_linc_results %&gt;% \n    bind_rows() %&gt;% \n    ggplot() + \n    geom_line(aes(y = .y.hat, x = linc, group = .imp), colour = \"#82518c\") + \n    brotools::theme_blog()\n\n\n\n\nOverall, the partial dependence plot seems to behave in a very similar way across the different imputed datasets!\n\n\nTo conclude, I think that the approach I suggest here is nothing revolutionary; it is consistent with the way one should conduct an analysis with multiple imputed datasets. However, the pooling step is non-trivial and there is no magic recipe; it really depends on the goal of the analysis and what you want or need to show."
  },
  {
    "objectID": "posts/2025-01-31-new_blog.html",
    "href": "posts/2025-01-31-new_blog.html",
    "title": "New year, new blog",
    "section": "",
    "text": "Happy new year! The blog has a new look! Well it’s not that different on the surface. But under the hood, it is quite different indeed!\nMy previous setup was: GitHub to host the code, on each push the build process would get started on Netlify and then it would be hosted there. The engine was Hugo.\nThis blog now still uses GitHub to host the code, but now also uses GitHub pages for hosting and the engine is Quarto. The blog also gets built on GitHub Actions inside of a Nix environment: so I just need to push and everything gets built! Here’s the workflow that achieves this.\nWhat’s really amazing with Nix, is that I can preview my blog locally using exactly the same environment as the one that will be used for building it on GitHub actions. So if it works on my machine it’s going to work anywhere.\nYou’ll notice that the last step uses the rstats-on-nix/quarto-nix-actions/publish@main action that is a fork of the quarto-dev/quarto-actions actions that just makes them work inside of a Nix shell! This fork is hosted on the rstats-on-nix organization: I have a lot to say about this organization, but that’s for a future blog post!\nMigrating the pages was a rather long process, as I needed to make sure everything was rendering correctly: because the folder structure of Quarto blogs is different than the structure of Hugo blogs, I had to update many paths. This was quite tedious and I didn’t want to use a script for this as I also wanted to take this opportunity to make some adjustments, such as centering images properly and correcting some typos if I saw some. It was also quite interesting to re-read some of my old blog posts.\nOne neat thing about Quarto is the possibility to use pre- and post-render scripts that can be written in R. I’m using one to correctly sort the blog posts in the main page, as for some reason they weren’t being sorted properly. Here’s the post-render script in question.\nNow I can go back to working on rix."
  },
  {
    "objectID": "posts/2023-12-19-nix_for_r_part_8.html",
    "href": "posts/2023-12-19-nix_for_r_part_8.html",
    "title": "Reproducible data science with Nix, part 8 – nixpkgs, a tale of the magic of free and open source software and a call for charity",
    "section": "",
    "text": "This is part 8 of a series of blog posts about Nix. Check out the other parts here. TLDR: free and open source software is one of the most important common goods with enormous positive externalities: if you want to help funding it, keep reading!\n\n\nI wanted to quickly discuss about nixpkgs, which is the collection of packages that can be installed using Nix. Why is a project like Nix and nixpkgs important, even if you don’t use Nix? In actuality, you may not realise it, but you very much benefit from projects like Nix even if you don’t use it. Let me explain.\n\n\nnixpkgs is “just” a Github repository containing thousands upon thousands of Nix expressions. When installing a package, these expressions get evaluated, and the package in question gets installed. What installed means can vary: sometimes the package gets built from source, sometimes a pre-compiled binary package for your operating system gets downloaded and installed.\n\n\nFor example, here is the Nix expression that downloads and installs Quarto. This is an example of an expression that downloads the pre-compiled Quarto package from Quarto’s own Github repository, and then installs it. The installation process in this case is essentially making sure that Quarto is able to find its dependencies, which also get installed from Nix, and some R and Python packages to make Quarto work well with both languages also get installed.\n\n\nBecause Nix packages are “nothing but” Nix expressions hosted on Github, contributing to Nix is as simple as opening a PR. For example, here is a draft PR I opened to prepare for the imminent release of Quarto 1.4. My goal when I opened this draft PR was to get used to contributing to nixpkgs (this was my second or third PR to nixpkgs, and I did some rookie mistakes when opening my first ones) and also to make the latest version of Quarto available on Nix as quickly as possible. But this PR had an unexpected consequence: through it, we found a bug in Quarto, which was then fixed before the actual release of the next version!\n\n\nYou see, how these things work is that when software gets released, operating system specific packages get built downstream. In the case of Quarto, this is not entirely true though: the developers of Quarto release many pre-compiled packages for Windows, macOS and several Linux distribution themselves. But they don’t do so for many other operating systems (which is entirely normal: there’s just too many! So releasing pre-built binaries for the main operating systems is more than enough), so the maintainers of these other operating systems (or package managers) have to package the software themselves. In the case of scientific software like Quarto, this usually means that it must get packaged for the Conda package manager (popular among Python users) and Nix (and there’s certainly other package managers out there that provide Quarto for other exotic systems) (Note: in the case of Quarto, I think the Quarto devs themselves also package it for Conda, though).\n\n\nTurns out that when trying to package the pre-releases of Quarto for Nix, we discovered a regression in the upstream code that would not only affect packaging for Nix, but also for other package managers. We opened an issue on Quarto’s issue tracker and after some discussion, the bug was identified and adressed in a matter of hours. And now everyone gets to enjoy a better version of Quarto!\n\n\nThis type of thing happens quite a lot in the background of open source development. My mind always gets blown when I think about the enormous amount of hours that get put by hobbyists and paid developers into open source and how well everything works. Truly a Christmas miracle (but one that happens all around the year)!\n\n\nBut it’s not all good and perfect. Some software is more complex to package, and requires much more work. For example the RStudio IDE is one of these. It’s a complex piece of software with many dependencies, and while it is available on Nix, it can only be installed on Windows and Linux. If you’re a Nix user on macOS, you won’t be able to install RStudio, unfortunately. And, unfortunately also, if you install RStudio using the usual macOS installer, it won’t be able to find any version of R and R packages installed with Nix. This is because RStudio needs to be patched to make it work nicely with Nix (just like we have to patch and prepare Quarto to play well with Nix). And packaging Rstudio for Nix on macOS requires some expertise and hardware that we R users/contributers to Nix don’t have all have access to.\n\n\nThis is where I appeal to your generosity: I have contacted a company called Numtide which offers a packaging service. You tell them which software you want on Nix, they write the expression and open a PR to nixpkgs. But this costs money: so I started a Gofundme which you can find here to fund this. The goal is 4500€, which would cover the work, plus Gofundme fees and interest rate risk. I stated in the Gofundme that if the goal was not reached until the end of the year, I would donate all the money to the R foundation, but I might extend it to end of January 2024 instead.\n\n\nSo here is my ask: if you want to help make free and open source software better, consider donating to this Gofundme! As explained above, even if you don’t use Nix, everyone can benefit from work that is done by everyone, be it upstream or downstream. And if the goal is not met, your donation will go to the R foundation anyways!\n\n\nThe link to the Gofundme is here.\n\n\nI hope you can help out with this and make free and open source available and better for everyone.\n\n\nMany thanks, merry Christmas and happy new year!"
  },
  {
    "objectID": "posts/2020-02-23-synthpop.html#intro-the-need-for-microdata-and-the-risk-of-disclosure",
    "href": "posts/2020-02-23-synthpop.html#intro-the-need-for-microdata-and-the-risk-of-disclosure",
    "title": "Synthetic micro-datasets: a promising middle ground between data privacy and data analysis",
    "section": "\nIntro: the need for microdata, and the risk of disclosure\n",
    "text": "Intro: the need for microdata, and the risk of disclosure\n\n\nSurvey and administrative data are essential for scientific research, however accessing such datasets can be very tricky, or even impossible. In my previous job I was responsible for getting access to such “scientific micro-datasets” from institutions like Eurostat. In general, getting access to these micro datasets was only a question of filling out some forms and signing NDAs. But this was true only because my previous employer was an accredited research entity. Companies from the private sector or unaffiliated, individual, researchers cannot get access to the microdata sets. This is because institutions that produce such datasets absolutely do not want any type of personal information to be disclosed.\n\n\nFor instance, with the labour force survey, a National Statistical Institute (NSI) collects information about wages, family structure, educational attainment and much more. If, say, a politician would answer to the survey and his answers would leak to the public that would be disastrous for NSIs. So this is why access is restricted to accredited research institutions. You may be asking yourself, “how could the politicians answers leak? The data is anonymized!” Indeed it is, but in some cases that may not be enough to ensure that information does not get disclosed. Suppose that the dataset contains enough information to allow you to know for certain that you found said politician, assume that this politician is a 43 year old man, has two children, a PhD in theology and lives in Strassen, one of Luxembourg-City very nice neighborhoods. It would be quite easy to find him in the dataset and then find out his wage.\n\n\nTo avoid this, researchers are required to perform output checking, which means going through the set of outputs (summary tables, graphs, tables with regression coefficients…) and making sure that it is not possible to find out individuals. For instance, in Luxembourg there are two companies in the tobacco industry. Luxembourg’s NSI cannot release the total turnover of the industry, because then company A would subtract its turnover from the total and find out its competitor’s turnover. Now these are all hypothetical examples, and we might argue that the risk of leakage is quite low, especially if NSIs make sure to lower the precision of the variables, by providing age categories instead of the exact age for example. Or capping wages that exceed a certain fixed amount. In any case for now most NSIs don’t release micro data to the public, and this poses some challenges for research. First of all, even for researchers, it would be great if the data was freely accessible. It would allow research to go straight to data analysis and look at the structure of the data before applying for access, with the risk of getting access to useless data. And of course it would be great for the public at large to be able to freely access such data, for educational purposes at the very least. It would also increase competition between research institutions and the private sector when it comes to conducting studies using such data. Free access to the microdata would level the playing field. Now, some NSIs do release micro data to the public, see Eustat, the NSI from the Basque country, an autonomous region of Spain. It is not clear to me if they also have more detailed data that is only accessible to researchers, but the data they offer is already quite interesting.\n\n\nA middle ground between only releasing data to researchers and making it completely freely accessible is to create a synthetic dataset, which does not contain any of the original records, but which still allows to perform meaningful analyses.\n\n\nI’m not yet very familiar with the details of the procedure, but in this blog post I’ll use Eustat’s microdata to generate a synthetic dataset. I’ll then perform the same analysis on both the original dataset and the synthetic dataset. The dataset I’ll be using can be found here, and is called Population with relation to activity (PRA):\n\n\nThe Survey on the Population in Relation to Activity operation is a continuous source of information on the characteristics and dynamics of the labour force of the Basque Country. It records the relation to productive activity of the population resident in family households, as well as the changes produced in labour situations; it produces indicators of conjunctural variations in the evolution of the active population; it also estimates the degree of participation of the population in economically non-productive activities. It offers information on the province and capital level.\n\n\nI’ll then compare the results of the analyses performed on the two datasets which will hopefully be very similar. To create the synthetic dataset, I’ll be using the {synthpop} package. You can read the detailed vignette here - pdf warning -. First, let me perform some cleaning steps. There are four datasets included in the archive. Let’s load them:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(readxl)\nlibrary(synthpop)\n\nlist_data &lt;- Sys.glob(\"MICRO*.csv\")\n\ndataset &lt;- map(list_data, read_csv2) %&gt;%\n  bind_rows()\n\nhead(dataset)\n\nThe columns are labeled in Spanish so I’m copy pasting the labels into Google translate and paste them back into my script. I saved the English names into the english.rds object for posterity. These steps are detailed in the next lines:\n\ndictionary &lt;- read_xlsx(\"Microdatos_PRA_2019/diseño_registro_microdatos_pra.xlsx\", sheet=\"Valores\",\n                        col_names = FALSE)\n## New names:\n## * `` -&gt; ...1\n## * `` -&gt; ...2\n## * `` -&gt; ...3\ncol_names &lt;- dictionary %&gt;%\n  filter(!is.na(...1)) %&gt;%\n  dplyr::select(1:2)\n\n# copy to clipboard, paste to google translate\n# couldn't be bothered to use an api and google cloud or whatever\n#clipr::write_clip(col_names$`...2`)\n\n#english &lt;- clipr::read_clip()\n\nenglish &lt;- readRDS(\"english_col_names.rds\")\n\ncol_names$english &lt;- english\n\ncolnames(dataset) &lt;- col_names$english\n\ndataset &lt;- janitor::clean_names(dataset)\n\nI now create a function that will perform the cleaning steps:\n\nbasic_cleaning &lt;- function(dataset){\n  dataset %&gt;%\n  dplyr::filter(age %in% c(\"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\")) %&gt;%\n  dplyr::filter(!is.na(job_search)) %&gt;%  \n  dplyr::select(territory, capital, sex, place_of_birth, age, nationality, level_of_studies_completed,\n                job_search, main_occupation, type_of_contract, hours) %&gt;%\n  dplyr::mutate_at(.vars = vars(-hours), .funs=as.factor)\n}"
  },
  {
    "objectID": "posts/2020-02-23-synthpop.html#putting-on-my-econometricians-hat",
    "href": "posts/2020-02-23-synthpop.html#putting-on-my-econometricians-hat",
    "title": "Synthetic micro-datasets: a promising middle ground between data privacy and data analysis",
    "section": "\nPutting on my econometricians hat\n",
    "text": "Putting on my econometricians hat\n\n\nLet’s now suppose that I’m only interested in running a logistic regression:\n\npra &lt;- basic_cleaning(dataset)\n\nhead(pra)\n## # A tibble: 6 x 11\n##   territory capital sex   place_of_birth age   nationality level_of_studie…\n##   &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt; &lt;fct&gt;       &lt;fct&gt;           \n## 1 48        9       6     1              09    1           1               \n## 2 48        9       1     1              09    1           2               \n## 3 48        1       1     1              11    1           3               \n## 4 48        1       6     1              10    1           3               \n## 5 48        9       6     1              07    1           3               \n## 6 48        9       1     1              09    1           1               \n## # … with 4 more variables: job_search &lt;fct&gt;, main_occupation &lt;fct&gt;,\n## #   type_of_contract &lt;fct&gt;, hours &lt;dbl&gt;\nlogit_model &lt;- glm(job_search ~ ., data = pra, family = binomial())\n\n# Create a tidy dataset with the results of the regression\ntidy_logit_model &lt;- tidy(logit_model, conf.int = TRUE) %&gt;%\n  mutate(dataset = \"true\")\n\nLet’s now take a look at the coefficients, by plotting their value along with their confidence intervals:\n\nggplot(tidy_logit_model, aes(x = term, y = estimate)) +\n  geom_point(colour = \"#82518c\") +\n  geom_hline(yintercept = 0, colour = \"red\") +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), colour = \"#657b83\") +\n  brotools::theme_blog() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\nOk, so now, how would the results change if I run the same analysis on the synthetic dataset? First, I need to generate this synthetic dataset:\n\nmy_seed &lt;- 1234\n\nsynthetic_data &lt;- syn(pra, seed = my_seed)\n## Synthesis\n## -----------\n##  territory capital sex place_of_birth age nationality level_of_studies_completed job_search main_occupation type_of_contract\n##  hours\n\nThe synthetic data is generated by a single call to the syn() function included in the {synthpop} package. Let’s take a look at the generated object:\n\nsynthetic_data\n## Call:\n## ($call) syn(data = pra, seed = my_seed)\n## \n## Number of synthesised data sets: \n## ($m)  1 \n## \n## First rows of synthesised data set: \n## ($syn)\n##   territory capital sex place_of_birth age nationality\n## 1        48       9   1              1  06           1\n## 2        01       9   6              3  09           1\n## 3        48       3   1              1  08           1\n## 4        48       9   6              1  11           1\n## 5        20       2   6              1  09           1\n## 6        48       1   6              1  11           1\n##   level_of_studies_completed job_search main_occupation type_of_contract hours\n## 1                          3          N               2                1    40\n## 2                          1          S               9                6    10\n## 3                          1          N               6             &lt;NA&gt;    32\n## 4                          2          N               4                1    32\n## 5                          3          N               5             &lt;NA&gt;    40\n## 6                          1          S               7             &lt;NA&gt;    NA\n## ...\n## \n## Synthesising methods: \n## ($method)\n##                  territory                    capital \n##                   \"sample\"                     \"cart\" \n##                        sex             place_of_birth \n##                     \"cart\"                     \"cart\" \n##                        age                nationality \n##                     \"cart\"                     \"cart\" \n## level_of_studies_completed                 job_search \n##                     \"cart\"                     \"cart\" \n##            main_occupation           type_of_contract \n##                     \"cart\"                     \"cart\" \n##                      hours \n##                     \"cart\" \n## \n## Order of synthesis: \n## ($visit.sequence)\n##                  territory                    capital \n##                          1                          2 \n##                        sex             place_of_birth \n##                          3                          4 \n##                        age                nationality \n##                          5                          6 \n## level_of_studies_completed                 job_search \n##                          7                          8 \n##            main_occupation           type_of_contract \n##                          9                         10 \n##                      hours \n##                         11 \n## \n## Matrix of predictors: \n## ($predictor.matrix)\n##                            territory capital sex place_of_birth age nationality\n## territory                          0       0   0              0   0           0\n## capital                            1       0   0              0   0           0\n## sex                                1       1   0              0   0           0\n## place_of_birth                     1       1   1              0   0           0\n## age                                1       1   1              1   0           0\n## nationality                        1       1   1              1   1           0\n## level_of_studies_completed         1       1   1              1   1           1\n## job_search                         1       1   1              1   1           1\n## main_occupation                    1       1   1              1   1           1\n## type_of_contract                   1       1   1              1   1           1\n## hours                              1       1   1              1   1           1\n##                            level_of_studies_completed job_search\n## territory                                           0          0\n## capital                                             0          0\n## sex                                                 0          0\n## place_of_birth                                      0          0\n## age                                                 0          0\n## nationality                                         0          0\n## level_of_studies_completed                          0          0\n## job_search                                          1          0\n## main_occupation                                     1          1\n## type_of_contract                                    1          1\n## hours                                               1          1\n##                            main_occupation type_of_contract hours\n## territory                                0                0     0\n## capital                                  0                0     0\n## sex                                      0                0     0\n## place_of_birth                           0                0     0\n## age                                      0                0     0\n## nationality                              0                0     0\n## level_of_studies_completed               0                0     0\n## job_search                               0                0     0\n## main_occupation                          0                0     0\n## type_of_contract                         1                0     0\n## hours                                    1                1     0\n\nAs you can see, synthetic_data is a list with several elements. The data is inside the syn element. Let’s extract it, and perform the same analysis from before:\n\nsyn_pra &lt;- synthetic_data$syn\n\nhead(syn_pra)\n##   territory capital sex place_of_birth age nationality\n## 1        48       9   1              1  06           1\n## 2        01       9   6              3  09           1\n## 3        48       3   1              1  08           1\n## 4        48       9   6              1  11           1\n## 5        20       2   6              1  09           1\n## 6        48       1   6              1  11           1\n##   level_of_studies_completed job_search main_occupation type_of_contract hours\n## 1                          3          N               2                1    40\n## 2                          1          S               9                6    10\n## 3                          1          N               6             &lt;NA&gt;    32\n## 4                          2          N               4                1    32\n## 5                          3          N               5             &lt;NA&gt;    40\n## 6                          1          S               7             &lt;NA&gt;    NA\nsyn_pra &lt;- basic_cleaning(syn_pra)\n\nlogit_model_syn &lt;- glm(job_search ~ ., data = syn_pra, family = binomial())\n\ntidy_logit_syn &lt;- tidy(logit_model_syn, conf.int = TRUE) %&gt;%\n  mutate(dataset = \"syn\")\n\nggplot(tidy_logit_syn, aes(x = term, y = estimate)) +\n  geom_point(colour = \"#82518c\") +\n  geom_hline(yintercept = 0, colour = \"red\") +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), colour = \"#657b83\") +\n  brotools::theme_blog() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\nTo ease the comparison between the coefficients of the model, let’s create a single graph:\n\ncoeff_models &lt;- bind_rows(list(tidy_logit_model, tidy_logit_syn))\n\nggplot(coeff_models, aes(x = term, y = estimate, colour = dataset)) +\n  geom_point() +\n  geom_hline(yintercept = 0) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +\n  brotools::theme_blog() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\nThis is quite interesting; generally, there is quite some overlap between the synthetic data and the real data! There are some differences though, for instance, main_occupation6 is significant with the synthetic data, but is not with the real data. There’s the possibility to generate more than one synthetic dataset, which would very likely reduce the noise."
  },
  {
    "objectID": "posts/2020-02-23-synthpop.html#putting-on-my-data-scientist-hat",
    "href": "posts/2020-02-23-synthpop.html#putting-on-my-data-scientist-hat",
    "title": "Synthetic micro-datasets: a promising middle ground between data privacy and data analysis",
    "section": "\nPutting on my data scientist hat\n",
    "text": "Putting on my data scientist hat\n\n\nNow let’s suppose that I am only interested into prediction. For this, I am going to split my dataset into a training and testing set, then run a logistic regression and a random forest, assess the models’ performance with 10-fold cross validation. I’ll do this on both the real and the synthetic data. To perform the analysis, I’ll be using the {tidymodels} framework; I’m going to explain the code that follows line by line, because I’ll very likely write a blog post focusing on {tidymodels} soon.\n\n\nSo, let’s write a function that does exactly what I explained above:\n\ntraining_and_evaluating &lt;- function(dataset){\n\n  pra_split &lt;- initial_split(dataset, prop = 0.8)\n  \n  pra_train &lt;- training(pra_split)\n  pra_test &lt;- testing(pra_split)\n  \n  pra_cv_splits &lt;- vfold_cv(pra_train, v = 10)\n  \n  preprocess &lt;- recipe(job_search ~ ., data = pra) %&gt;%\n    step_knnimpute(all_predictors())\n  \n  logit_pra &lt;- logistic_reg() %&gt;%\n    set_engine(\"glm\")\n  \n  fitted_logit &lt;- fit_resamples(preprocess,\n                                model = logit_pra,\n                                resamples = pra_cv_splits,\n                                control = control_resamples(save_pred = TRUE))\n  \n  metric_logit &lt;- fitted_logit$.metrics %&gt;%\n    bind_rows() %&gt;%\n    group_by(.metric) %&gt;%\n    summarise_at(.vars = vars(.estimate), .funs = lst(mean, sd)) %&gt;%\n    mutate(model = \"logit\")\n  \n  rf_pra &lt;- rand_forest(mode = \"classification\") %&gt;%\n    set_engine(engine = \"ranger\")\n  \n  fitted_forest &lt;- fit_resamples(preprocess,\n                                model = rf_pra,\n                                resamples = pra_cv_splits,\n                                control = control_resamples(save_pred = TRUE))\n  \n  metric_forest &lt;- fitted_forest$.metrics %&gt;%\n    bind_rows() %&gt;%\n    group_by(.metric) %&gt;%\n    summarise_at(.vars = vars(.estimate), .funs = lst(mean, sd)) %&gt;%\n    mutate(model = \"forest\")\n\n\n  bind_rows(list(metric_logit, metric_forest))\n}\n\nNow I can run this function on both the real and the synthetic data, and look at the performance of the logistic regression and of the random forest:\n\ntrue_data_performance &lt;- training_and_evaluating(pra)\n\nsyn_data_performance &lt;- training_and_evaluating(syn_pra)\ntrue_data_performance\n## # A tibble: 4 x 4\n##   .metric   mean      sd model \n##   &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; \n## 1 accuracy 0.882 0.00816 logit \n## 2 roc_auc  0.708 0.0172  logit \n## 3 accuracy 0.907 0.00619 forest\n## 4 roc_auc  0.879 0.0123  forest\nsyn_data_performance\n## # A tibble: 4 x 4\n##   .metric   mean      sd model \n##   &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; \n## 1 accuracy 0.882 0.00758 logit \n## 2 roc_auc  0.691 0.0182  logit \n## 3 accuracy 0.899 0.00615 forest\n## 4 roc_auc  0.857 0.0124  forest\n\nThe performance is pretty much the same!\n\n\nGenerating synthetic data is a very promising approach, that I certainly will be using more; I think that such approaches can also be very interesting in the private sector (not only to ease access to microdata for researchers) especially within large companies. For instance, it can happen that the data owners from say, an insurance company, are not very keen on sharing sensitive client information with their data scientists. However, by generating a synthetic dataset and sharing the synthetic data with their data science team, the data owners avoid any chance of disclosure of sensitive information, while at the same time allowing their data scientists to develop interesting analyses or applications on the data!"
  },
  {
    "objectID": "posts/2016-07-30-merge-a-list-of-datasets-together.html",
    "href": "posts/2016-07-30-merge-a-list-of-datasets-together.html",
    "title": "Merge a list of datasets together",
    "section": "",
    "text": "Last week I showed how to read a lot of datasets at once with R, and this week I’ll continue from there and show a very simple function that uses this list of read datasets and merges them all together.\n\n\nFirst we’ll use read_list() to read all the datasets at once (for more details read last week’s post):\n\nlibrary(\"readr\")\nlibrary(\"tibble\")\n\ndata_files &lt;- list.files(pattern = \".csv\")\n\nprint(data_files)\n## [1] \"data_1.csv\" \"data_2.csv\" \"data_3.csv\"\nlist_of_data_sets &lt;- read_list(data_files, read_csv)\n\nglimpse(list_of_data_sets)\n## List of 3\n##  $ data_1:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,018930679\" \"0,8748013128\" \"0,1025635934\" \"0,6246140983\" ...\n##   ..$ col2: chr [1:19] \"0,0377725807\" \"0,5959457638\" \"0,4429121533\" \"0,558387159\" ...\n##   ..$ col3: chr [1:19] \"0,6241767189\" \"0,031324594\" \"0,2238059868\" \"0,2773350732\" ...\n##  $ data_2:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9098418493\" \"0,1127788509\" \"0,5818891392\" \"0,1011773532\" ...\n##   ..$ col2: chr [1:19] \"0,7455905887\" \"0,4015039612\" \"0,6625796605\" \"0,029955339\" ...\n##   ..$ col3: chr [1:19] \"0,327232932\" \"0,2784035673\" \"0,8092386735\" \"0,1216045306\" ...\n##  $ data_3:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9236124896\" \"0,6303271761\" \"0,6413583054\" \"0,5573887416\" ...\n##   ..$ col2: chr [1:19] \"0,2114708388\" \"0,6984538266\" \"0,0469865249\" \"0,9271510226\" ...\n##   ..$ col3: chr [1:19] \"0,4941919971\" \"0,7391538511\" \"0,3876723797\" \"0,2815014394\" ...\n\nYou see that all these datasets have the same column names. We can now merge them using this simple function:\n\nmulti_join &lt;- function(list_of_loaded_data, join_func, ...){\n\n    require(\"dplyr\")\n\n    output &lt;- Reduce(function(x, y) {join_func(x, y, ...)}, list_of_loaded_data)\n\n    return(output)\n}\n\nThis function uses Reduce(). Reduce() is a very important function that can be found in all functional programming languages. What does Reduce() do? Let’s take a look at the following example:\n\nReduce(`+`, c(1, 2, 3, 4, 5))\n## [1] 15\n\nReduce() has several arguments, but you need to specify at least two: a function, here + and a list, here c(1, 2, 3, 4, 5). The next code block shows what Reduce() basically does:\n\n0 + c(1, 2, 3, 4, 5)\n0 + 1 + c(2, 3, 4, 5)\n0 + 1 + 2 + c(3, 4, 5)\n0 + 1 + 2 + 3 + c(4, 5)\n0 + 1 + 2 + 3 + 4 + c(5)\n0 + 1 + 2 + 3 + 4 + 5\n\n0 had to be added as in “init”. You can also specify this “init” to Reduce():\n\nReduce(`+`, c(1, 2, 3, 4, 5), init = 20)\n## [1] 35\n\nSo what multi_join() does, is the same operation as in the example above, but where the function is a user supplied join or merge function, and the list of datasets is the one read with read_list().\n\n\nLet’s see what happens when we use multi_join() on our list:\n\nmerged_data &lt;- multi_join(list_of_data_sets, full_join)\nclass(merged_data)\n## [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\nglimpse(merged_data)\n## Observations: 57\n## Variables: 3\n## $ col1 &lt;chr&gt; \"0,018930679\", \"0,8748013128\", \"0,1025635934\", \"0,6246140...\n## $ col2 &lt;chr&gt; \"0,0377725807\", \"0,5959457638\", \"0,4429121533\", \"0,558387...\n## $ col3 &lt;chr&gt; \"0,6241767189\", \"0,031324594\", \"0,2238059868\", \"0,2773350...\n\nYou should make sure that all the data frames have the same column names but you can also join data frames with different column names if you give the argument by to the join function. This is possible thanks to … that allows you to pass further argument to join_func().\n\n\nThis function was inspired by the one found on the blog Coffee and Econometrics in the Morning."
  },
  {
    "objectID": "posts/2019-01-04-newspapers.html",
    "href": "posts/2019-01-04-newspapers.html",
    "title": "Looking into 19th century ads from a Luxembourguish newspaper with R",
    "section": "",
    "text": "The national library of Luxembourg published some very interesting data sets; scans of historical newspapers! There are several data sets that you can download, from 250mb up to 257gb. I decided to take a look at the 32gb “ML Starter Pack”. It contains high quality scans of one year of the L’indépendence Luxembourgeoise (Luxembourguish independence) from the year 1877. To make life easier to data scientists, the national library also included ALTO and METS files (which is a XML schema that is used to describe the layout and contents of physical text sources, such as pages of a book or newspaper) which can be easily parsed by R.\n\n\nL’indépendence Luxembourgeoise is quite interesting in that it is a Luxembourguish newspaper written in French. Luxembourg always had 3 languages that were used in different situations, French, German and Luxembourguish. Luxembourguish is the language people used (and still use) for day to day life and to speak to their baker. Historically however, it was not used for the press or in politics. Instead it was German that was used for the press (or so I thought) and French in politics (only in 1984 was Luxembourguish made an official Language of Luxembourg). It turns out however that L’indépendence Luxembourgeoise, a daily newspaper that does not exist anymore, was in French. This piqued my interest, and it also made analysis easier, for 2 reasons: I first started with the Luxemburger Wort (Luxembourg’s Word I guess would be a translation), which still exists today, but which is in German. And at that time, German was written using the Fraktur font, which makes it barely readable. Look at the alphabet in Fraktur:\n\n𝕬 𝕭 𝕮 𝕯 𝕰 𝕱 𝕲 𝕳 𝕴 𝕵 𝕶 𝕷 𝕸 𝕹 𝕺 𝕻 𝕼 𝕽 𝕾 𝕿 𝖀 𝖁 𝖂 𝖃 𝖄 𝖅\n𝖆 𝖇 𝖈 𝖉 𝖊 𝖋 𝖌 𝖍 𝖎 𝖏 𝖐 𝖑 𝖒 𝖓 𝖔 𝖕 𝖖 𝖗 𝖘 𝖙 𝖚 𝖛 𝖜 𝖝 𝖞 𝖟\n\nIt’s not like German is already hard enough, they had to invent the least readable font ever to write German in, to make extra sure it would be hell to decipher.\n\n\nSo basically I couldn’t be bothered to try to read a German newspaper in Fraktur. That’s when I noticed the L’indépendence Luxembourgeoise… A Luxembourguish newspaper? Written in French? Sounds interesting.\n\n\nAnd oh boy. Interesting it was.\n\n\n19th century newspapers articles were something else. There’s this article for instance:\n\n\n\n\n\nFor those of you that do not read French, this article relates that in France, the ministry of justice required priests to include prayers on the Sunday that follows the start of the new season of parliamentary discussions, in order for God to provide senators his help.\n\n\nThere this gem too:\n\n\n\n\n\nThis article presents the tallest soldier of the German army, called Emhke, and nominated by the German Emperor himself to accompany him during his visit to Palestine. Emhke was 2.08 meters tall and weighted 236 pounds (apparently at the time Luxembourg was not fully sold on the metric system).\n\n\nAnyway, I decided to take a look at ads. The last paper of this 4 page newspaper always contained ads and other announcements. For example, there’s this ad for a pharmacy:\n\n\n\n\n\nthat sells tea, and mineral water. Yes, tea and mineral water. In a pharmacy. Or this one:\n\n\n\n\n\nwhich is literally upside down in the newspaper (the one from the 10th of April 1877). I don’t know if it’s a mistake or if it’s a marketing ploy, but it did catch my attention, 140 years later, so bravo. This is an announcement made by a shop owner that wants to sell all his merchandise for cheap, perhaps to make space for new stuff coming in?\n\n\nSo I decided brush up on my natural language processing skills with R and do topic modeling on these ads. The challenge here is that a single document, the 4th page of the newspaper, contains a lot of ads. So it will probably be difficult to clearly isolate topics. But let’s try nonetheless. First of all, let’s load all the .xml files that contain the data. These files look like this:\n\n&lt;TextLine ID=\"LINE6\" STYLEREFS=\"TS11\" HEIGHT=\"42\" WIDTH=\"449\" HPOS=\"165\" VPOS=\"493\"&gt;\n                                    &lt;String ID=\"S16\" CONTENT=\"l’après-midi,\" WC=\"0.638\" CC=\"0803367024653\" HEIGHT=\"42\" WIDTH=\"208\" HPOS=\"165\" VPOS=\"493\"/&gt;\n                                    &lt;SP ID=\"SP11\" WIDTH=\"24\" HPOS=\"373\" VPOS=\"493\"/&gt;\n                                    &lt;String ID=\"S17\" CONTENT=\"le\" WC=\"0.8\" CC=\"40\" HEIGHT=\"30\" WIDTH=\"29\" HPOS=\"397\" VPOS=\"497\"/&gt;\n                                    &lt;SP ID=\"SP12\" WIDTH=\"14\" HPOS=\"426\" VPOS=\"497\"/&gt;\n                                    &lt;String ID=\"S18\" CONTENT=\"Gouverne\" WC=\"0.638\" CC=\"72370460\" HEIGHT=\"31\" WIDTH=\"161\" HPOS=\"440\" VPOS=\"496\" SUBS_TYPE=\"HypPart1\" SUBS_CONTENT=\"Gouvernement\"/&gt;\n                                    &lt;HYP CONTENT=\"-\" WIDTH=\"11\" HPOS=\"603\" VPOS=\"514\"/&gt;\n                                  &lt;/TextLine&gt;\n                        &lt;TextLine ID=\"LINE7\" STYLEREFS=\"TS11\" HEIGHT=\"41\" WIDTH=\"449\" HPOS=\"166\" VPOS=\"541\"&gt;\n                                    &lt;String ID=\"S19\" CONTENT=\"ment\" WC=\"0.725\" CC=\"0074\" HEIGHT=\"26\" WIDTH=\"81\" HPOS=\"166\" VPOS=\"545\" SUBS_TYPE=\"HypPart2\" SUBS_CONTENT=\"Gouvernement\"/&gt;\n                                    &lt;SP ID=\"SP13\" WIDTH=\"24\" HPOS=\"247\" VPOS=\"545\"/&gt;\n                                    &lt;String ID=\"S20\" CONTENT=\"Royal\" WC=\"0.62\" CC=\"74503\" HEIGHT=\"41\" WIDTH=\"100\" HPOS=\"271\" VPOS=\"541\"/&gt;\n                                    &lt;SP ID=\"SP14\" WIDTH=\"26\" HPOS=\"371\" VPOS=\"541\"/&gt;\n                                    &lt;String ID=\"S21\" CONTENT=\"Grand-Ducal\" WC=\"0.682\" CC=\"75260334005\" HEIGHT=\"32\" WIDTH=\"218\" HPOS=\"397\" VPOS=\"541\"/&gt;\n                                  &lt;/TextLine&gt;\n\nI’m interested in the “CONTENT” tag, which contains the words. Let’s first get that into R.\n\n\nLoad the packages, and the files:\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(topicmodels)\nlibrary(brotools)\n\nad_pages &lt;- str_match(list.files(path = \"./\", all.files = TRUE, recursive = TRUE), \".*4-alto.xml\") %&gt;%\n    discard(is.na)\n\nI save the path of all the pages at once into the ad_pages variables. To understand how and why this works, you must take a look at the hierarchy of the folder:\n\n\n\n\n\nInside each of these folder, there is a text folder, and inside this folder there are the .xml files. Because this structure is bit complex, I use the list.files() function with the all.files and recursive argument set to TRUE which allow me to dig deep into the folder structure and list every single file. I am only interested into the 4th page though, so that’s why I use str_match() to only keep the 4th page using the “.*4-alto.xml” regular expression. This is the right regular expression, because the files are named like so:\n\n1877-12-29_01-00004-alto.xml\n\nSo in the end, ad_pages is a list of all the paths to these files. I then write a function to extract the contents of the “CONTENT” tag. Here is the function.\n\nget_words &lt;- function(page_path){\n    \n    page &lt;- read_file(page_path)\n    \n    page_name &lt;- str_extract(page_path, \"1.*(?=-0000)\") \n    \n    page %&gt;%  \n        str_split(\"\\n\", simplify = TRUE) %&gt;% \n        keep(str_detect(., \"CONTENT\")) %&gt;% \n        str_extract(\"(?&lt;=CONTENT)(.*?)(?=WC)\") %&gt;% \n        discard(is.na) %&gt;% \n        str_extract(\"[:alpha:]+\") %&gt;% \n        tolower %&gt;% \n        as_tibble %&gt;% \n        rename(tokens = value) %&gt;% \n        mutate(page = page_name)\n}\n\nThis function takes the path to a page as argument, and returns a tibble with the two columns: one containing the words, which I called tokens and the second the name of the document this word was found. I uploaded on .xml file here so that you can try the function yourself. The difficult part is str_extract(“(?&lt;=CONTENT)(.*?)(?=WC)“) which is were the words inside the “CONTENT” tag get extracted.\n\n\nI then map this function to all the pages, and get a nice tibble with all the words:\n\nad_words &lt;- map_dfr(ad_pages, get_words)\nad_words\n## # A tibble: 1,114,662 x 2\n##    tokens     page                            \n##    &lt;chr&gt;      &lt;chr&gt;                           \n##  1 afin       1877-01-05_01/text/1877-01-05_01\n##  2 de         1877-01-05_01/text/1877-01-05_01\n##  3 mettre     1877-01-05_01/text/1877-01-05_01\n##  4 mes        1877-01-05_01/text/1877-01-05_01\n##  5 honorables 1877-01-05_01/text/1877-01-05_01\n##  6 clients    1877-01-05_01/text/1877-01-05_01\n##  7 à          1877-01-05_01/text/1877-01-05_01\n##  8 même       1877-01-05_01/text/1877-01-05_01\n##  9 d          1877-01-05_01/text/1877-01-05_01\n## 10 avantages  1877-01-05_01/text/1877-01-05_01\n## # … with 1,114,652 more rows\n\nI then do some further cleaning, removing stop words (French and German, because there are some ads in German) and a bunch of garbage characters and words, which are probably when the OCR failed. I also remove some German words from the few German ads that are in the paper, because they have a very high tf-idf (I’ll explain below what that is). I also remove very common words in ads that were just like stopwords. Every ad of a shop mentioned their clients with honorable clientèle, or used the word vente, and so on. This is what you see below in the very long calls to str_remove_all. I also compute the tf_idf and I am grateful to ThinkR blog post on that, which you can read here. It’s in French though, but the idea of the blog post is to present topic modeling with Wikipedia articles. You can also read the section on tf-idf from the Text Mining with R ebook, here. tf-idf gives a measure of how common words are. Very common words, like stopwords, have a tf-idf of 0. So I use this to further remove very common words, by only keeping words with a tf-idf greater than 0.01. This is why I manually remove garbage words and German words below, because they are so uncommon that they have a very high tf-idf and mess up the rest of the analysis. To find these words I had to go back and forth between the tibble of cleaned words and my code, and manually add all these exceptions. It took some time, but definitely made the results of the next steps better. I then use cast_dtm to cast the tibble into a DocumentTermMatrix object, which is needed for the LDA() function that does the topic modeling:\n\nstopwords_fr &lt;- read_csv(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-fr/master/stopwords-fr.txt\",\n                         col_names = FALSE)\n## Parsed with column specification:\n## cols(\n##   X1 = col_character()\n## )\nstopwords_de &lt;- read_csv(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/stopwords-de.txt\",\n                         col_names = FALSE)\n## Parsed with column specification:\n## cols(\n##   X1 = col_character()\n## )\n## Warning: 1 parsing failure.\n## row col  expected    actual                                                                                   file\n## 157  -- 1 columns 2 columns 'https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/stopwords-de.txt'\nad_words2 &lt;- ad_words %&gt;% \n    filter(!is.na(tokens)) %&gt;% \n    mutate(tokens = str_remove_all(tokens, \n                                   '[|\\\\|!|\"|#|$|%|&|\\\\*|+|,|-|.|/|:|;|&lt;|=|&gt;|?|@|^|_|`|’|\\'|‘|(|)|\\\\||~|=|]|°|&lt;|&gt;|«|»|\\\\d{1,100}|©|®|•|—|„|“|-|¦\\\\\\\\|”')) %&gt;%\n    mutate(tokens = str_remove_all(tokens,\n                                   \"j'|j’|m’|m'|n’|n'|c’|c'|qu’|qu'|s’|s'|t’|t'|l’|l'|d’|d'|luxembourg|honneur|rue|prix|maison|frs|ber|adresser|unb|mois|vente|informer|sann|neben|rbudj|artringen|salz|eingetragen|ort|ftofjenb|groifdjen|ort|boch|chem|jahrgang|uoa|genannt|neuwahl|wechsel|sittroe|yerlorenkost|beichsmark|tttr|slpril|ofto|rbudj|felben|acferftücf|etr|eft|sbege|incl|estce|bes|franzosengrund|qne|nne|mme|qni|faire|id|kil\")) %&gt;%\n    anti_join(stopwords_de, by = c(\"tokens\" = \"X1\")) %&gt;% \n    filter(!str_detect(tokens, \"§\")) %&gt;% \n    mutate(tokens = ifelse(tokens == \"inédite\", \"inédit\", tokens)) %&gt;% \n    filter(tokens != \"\") %&gt;% \n    anti_join(stopwords_fr, by = c(\"tokens\" = \"X1\")) %&gt;% \n    count(page, tokens) %&gt;% \n    bind_tf_idf(tokens, page, n) %&gt;% \n    arrange(desc(tf_idf))\n\ndtm_long &lt;- ad_words2 %&gt;% \n    filter(tf_idf &gt; 0.01) %&gt;% \n    cast_dtm(page, tokens, n)\n\nTo read more details on this, I suggest you take a look at the following section of the Text Mining with R ebook: Latent Dirichlet Allocation.\n\n\nI choose to model 10 topics (k = 10), and set the alpha parameter to 5. This hyperparamater controls how many topics are present in one document. Since my ads are all in one page (one document), I increased it. Let’s fit the model, and plot the results:\n\nlda_model_long &lt;- LDA(dtm_long, k = 10, control = list(alpha = 5))\n\nI plot the per-topic-per-word probabilities, the “beta” from the model and plot the 5 words that contribute the most to each topic:\n\nresult &lt;- tidy(lda_model_long, \"beta\")\n\nresult %&gt;%\n    group_by(topic) %&gt;%\n    top_n(5, beta) %&gt;%\n    ungroup() %&gt;%\n    arrange(topic, -beta) %&gt;% \n    mutate(term = reorder(term, beta)) %&gt;%\n    ggplot(aes(term, beta, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\") +\n    coord_flip() +\n    theme_blog()\n\n\n\n\nSo some topics seem clear to me, other not at all. For example topic 4 seems to be about shoes made out of leather. The word semelle, sole, also appears. Then there’s a lot of topics that reference either music, bals, or instruments. I guess these are ads for local music festivals, or similar events. There’s also an ad for what seems to be bundles of sticks, topic 3: chêne is oak, copeaux is shavings and you know what fagots is. The first word stère which I did not know is a unit of volume equal to one cubic meter (see Wikipedia). So they were likely selling bundle of oak sticks by the cubic meter. For the other topics, I either lack context or perhaps I just need to adjust k, the number of topics to model, and alpha to get better results. In the meantime, topic 1 is about shoes (chaussures), theatre, fuel (combustible) and farts (pet). Really wonder what they were selling in that shop.\n\n\nIn any case, this was quite an interesting project. I learned a lot about topic modeling and historical newspapers of my country! I do not know if I will continue exploring it myself, but I am really curious to see what others will do with it!"
  },
  {
    "objectID": "posts/2024-02-29-nix_for_r_part_10.html",
    "href": "posts/2024-02-29-nix_for_r_part_10.html",
    "title": "Reproducible data science with Nix, part 10 – contributing to nixpkgs",
    "section": "",
    "text": "I’ve very recently started contributing to the nixpkgs repository of packages, which contains all the packages you can install from the Nix package manager. My contributions are fairly modest: I help fix R packages that need some tweaking to make them successfully build for Nix. Most of these fixes are very simple one-liners.\nMost users of any free and open source tool rarely contribute to the development of this tool: I don’t think it is due to lack of skills and/or time or interest, but mostly because starting to contribute to a tool requires some knowledge that is rarely written down (even more so for an entire ecosystem). These tools and ecosystems grow organically, and if you’re not in the right spot at the right time or are not lucky enough to have kind people taking time to explain things to you, contributing might feel completely overwhelming.\nThankfully, I was very lucky to have found the small but very active community of R contributors to nixpkgs on Matrix which very kindly took the time to bring me up to speed!\nI wanted to share my experiences in this blog post: but this blog post is not just going to be about me contributing to nixpkgs from the perspective of an R user (and giving you some pointers on how to start yourself), but also about how I built a report (let’s call it like that) to keep track of which R packages got fixed. This report is built using R, Nix, Github Actions and lists all the failed R package builds from Hydra (more on this later). The report gets updated every day automatically at midnight, and is accessible here. I also used a very minimalistic approach to build this: no {tidyverse} packages, and no Quarto. Why? Mostly just to keep dependencies at a minimum to accelerate CI/CD, but also for fun. And honestly, I must admit that base R is more than capable on its own and had forgotten that."
  },
  {
    "objectID": "posts/2024-02-29-nix_for_r_part_10.html#contributing-to-nixpkgs",
    "href": "posts/2024-02-29-nix_for_r_part_10.html#contributing-to-nixpkgs",
    "title": "Reproducible data science with Nix, part 10 – contributing to nixpkgs",
    "section": "\nContributing to nixpkgs\n",
    "text": "Contributing to nixpkgs\n\n\nAs explained in part 8, nixpkgs is “nothing but” a huge GitHub repository containing thousands of Nix expressions. These expressions are then used to actually build the software that then gets installed by Nix. For example, this is the expression for Quarto. As you can see, it starts by downloading the pre-compiled binary, and then applying “patches”. Essentially making sure that Quarto installed by Nix is able to find the other pieces installed by Nix that Quarto needs (Deno, Pandoc, Typst and so on). It then continues by installing Quarto itself (because we’re downloading a pre-compiled binary, installation consists in moving files in the right spot), finally some tests are executed (quarto check) and then some metadata is defined. Not every package is defined like this, with a single Nix expression, though. For example, individual R packages are not defined like this. Instead, every package from CRAN and Bioconductor gets built using only a handful of files that can be found here.\n\n\n(By the way, you can look for packages and find their associated Nix expressions on the NixOS package search).\n\n\nThe way this works, is that periodically the generate-r-packages.R script is run and generates the cran-packages.nix file (and the equivalent Bioconductor files). For each package on CRAN, a line gets written in the script with the package’s name, its current version on CRAN, and very importantly its dependencies. For example, here is the line for {dplyr}:\n\ndplyr = derive2 { name=\"dplyr\"; version=\"1.1.4\";\n   sha256=\"1jsq8pj12bngy66xms486j8a65wxvyqs944q9rxkiaylsla08wyg\";\n   depends=[cli generics glue lifecycle magrittr pillar R6 rlang tibble tidyselect vctrs]; };\n\nThese dependencies are actually the packages that can be found in the DESCRIPTION file under Imports. cran-packages.nix (and the same goes for the Bioconductor equivalents, bioc-packages.nix, bioc-annotation-packages.nix and bioc-experiment-packages.nix) get imported in the default.nix file. In it, another file, generic-builder.nix gets also imported, which contains a function that will attempt building the package. Most of the time this succeeds, but some packages require further tweaks. Packages that have a field NeedsCompilation in their DESCRIPTION files are usually candidates for further tweaking: these packages require system-level dependencies, which are often listed under SystemRequirements (but not always, which complicates matters). For example, the {terra} package has these system requirements listed in itself DESCRIPTION file:\n\nSystemRequirements:  C++17, GDAL (&gt;= 2.2.3), GEOS (&gt;= 3.4.0), PROJ (&gt;= 4.9.3), sqlite3\n\nso these also need to be added if we want to build them on Nix. But if we look at the line for {terra} in cran-packages.nix, this is what we see:\n\nterra = derive2 { name=\"terra\"; version=\"1.7-65\"; \n  sha256=\"0m9s5am8l6il1q0skab614cx0qjsb1i9xcv6nm0sdzj7p9lrzkfl\"; \n  depends=[Rcpp]; };\n\nOnly {Rcpp} is listed, which is a dependency, yes, but an R package dependency, not a system-level requirement. System-level requirements need to be added in the default.nix file manually. In the default.nix, you’ll find a long list of packages called packagesWithNativeBuildInputs and packagesWithBuildInputs. NativeBuildInputs and BuildInputs are Nix jargon for dependencies the package needs, at compile-time and then at run-time specifically. For example, {Rcpp} is a BuildInput of {terra}, while the system-level requirements are NativeBuildInputs (in the context of R packages though, this rarely matters. If you want more details, refer to this Gist I’ve forked).\n\n\nFor {terra}, this means that we need to add this line to the list {packagesWithNativeBuildInputs} (I simplified the syntax here a bit):\n\nterra = [ gdal proj geos ];\n\ngdal, proj and geos are the system requirements that need to be added for {terra} to build successfully on Hydra."
  },
  {
    "objectID": "posts/2024-02-29-nix_for_r_part_10.html#hydra",
    "href": "posts/2024-02-29-nix_for_r_part_10.html#hydra",
    "title": "Reproducible data science with Nix, part 10 – contributing to nixpkgs",
    "section": "\nHydra\n",
    "text": "Hydra\n\n\nHydra is a tool for continuous integration testing and software release that uses a purely functional language to describe build jobs and their dependencies (source: the Hydra Manual)\n\n\nIf you’re coming from R, think of Hydra as R-hub, which will check and build your R package before submitting to CRAN. Hydra periodically tries to rebuild packages. If that package fails, then the log gets hosted. When it comes to R packages, we can check which packages built successfully or not on here.\n\n\nAs of writing, the latest evaluation was in mid-January. A new release of R is going to get released on the 29th of February (or maybe was already released, I’m not sure when this blog post is going to get posted), and this is when new evaluations will likely be executed. Evaluations are the processes by which Nix expressions get… evaluated and used to actually build packages. So if we look into the results of the evaluation of the 17th of January, we see that 757 jobs failed:\n\n\n\n\n\n\n\nOne job doesn’t strictly correspond to one package though: packages get built for different architectures, and each architecture gets its build process. If we log into the details of the first package whose build failed {AIUQ}, we see this:\n\n\n\n\n\n\n\nFrom the log we see that actually what failed one of its dependencies, {SuperGauss}, so fixing {SuperGauss} will likely fix {AIUQ} (I say likely because maybe another needed dependency also fails). So we could try to fix {SuperGauss} first. Let’s see why {SuperGauss}, by clicking on raw:\n\n\n\n\n\n\n\nHere is what we see:\n\nRunning phase: unpackPhase\nunpacking source archive /nix/store/615bdvjchxrd7wp5m7dhg4g04yv7ncza-SuperGauss_2.0.3.tar.gz\nsource root is SuperGauss\nsetting SOURCE_DATE_EPOCH to timestamp 1645735202 of file SuperGauss/MD5\nRunning phase: patchPhase\nRunning phase: updateAutotoolsGnuConfigScriptsPhase\nRunning phase: configurePhase\nRunning phase: buildPhase\nRunning phase: checkPhase\nRunning phase: installPhase\n* installing *source* package 'SuperGauss' ...\n** package 'SuperGauss' successfully unpacked and MD5 sums checked\n** using staged installation\nchecking for gcc... /nix/store/xq8920m5mbd83vdlydwli7qsh67gfm5v-gcc-wrapper-13.2.0/bin/cc\nchecking whether the C compiler works... yes\nchecking for C compiler default output file name... a.out\nchecking for suffix of executables... \nchecking whether we are cross compiling... no\nchecking for suffix of object files... o\nchecking whether we are using the GNU C compiler... yes\nchecking whether /nix/store/xq8920m5mbd83vdlydwli7qsh67gfm5v-gcc-wrapper-13.2.0/bin/cc accepts -g... yes\nchecking for /nix/store/xq8920m5mbd83vdlydwli7qsh67gfm5v-gcc-wrapper-13.2.0/bin/cc option to accept ISO C89... none needed\nchecking for pkg-config... no\nchecking for FFTW... configure: error: in `/build/SuperGauss':\nconfigure: error: The pkg-config script could not be found or is too old.  Make sure it\nis in your PATH or set the PKG_CONFIG environment variable to the full\npath to pkg-config.\n\nAlternatively, you may set the environment variables FFTW_CFLAGS\nand FFTW_LIBS to avoid the need to call pkg-config.\nSee the pkg-config man page for more details.\n\nTo get pkg-config, see &lt;http://pkg-config.freedesktop.org/&gt;.\nSee `config.log' for more details\nERROR: configuration failed for package 'SuperGauss'\n* removing '/nix/store/jxv5p85x24xmfcnifw2ibvx9jhk9f2w4-r-SuperGauss-2.0.3/library/SuperGauss'\n\nThis is essentially what we would see if we tried to install {SuperGauss} on Linux. The error message is quite clear here: a system-level dependency, pkg-config is missing. Looks like we found our first package to fix!"
  },
  {
    "objectID": "posts/2024-02-29-nix_for_r_part_10.html#fixing-a-package",
    "href": "posts/2024-02-29-nix_for_r_part_10.html#fixing-a-package",
    "title": "Reproducible data science with Nix, part 10 – contributing to nixpkgs",
    "section": "\nFixing a package\n",
    "text": "Fixing a package\n\n\nThe first step is to fork and clone the nixpkgs GitHub repository to your computer (be patient, the repository is huge so the download will take some time):\n\ngit clone git@github.com:b-rodrigues/nixpkgs.git\n\nIt’s also a good idea to add the original nixpkgs as an upstream:\n\ngit remote add upstream https://github.com/NixOS/nixpkgs\n\nThis way, you can pull changes from the original nixpkgs repository into your fork easily with:\n\ngit fetch upstream master\ngit merge upstream/master\n\nThese two commands synchronize your local copy of the repository with upstream. So now we can create a new branch to try to fix {SuperGauss}:\n\ngit branch -b fix_supergauss\n\nand then we should try to build {SuperGauss} locally. This is because it might have been fixed in the meantime by someone else, so let’s try to build it with (run the following command in a terminal at the root of your local copy of the nixpkgs repository):\n\nnix-build -A rPackages.SuperGauss\n\nbut I often prefer to use this instead, because this will build the package and drop me into a shell where I can start R, load the package, and try it by running some of its examples:\n\nnix-shell -I nixpkgs=/path/to/my/nixpkgs -p rPackages.SuperGauss R\n\nIf any of the commands above fail with the same error message as on Hydra, we know that it hasn’t been fixed yet. So the fix consists in opening the pkgs/development/r-modules/default.nix and add the following line:\n\nSuperGauss = [ pkg-config ];\n\nin either the lists packagesWithBuildInputs or packagesWithNativeBuildInputs (as explained above, it doesn’t really matter). Trying to rebuild SuperGauss again will result in a new error message. Another dependecy needs to be added:\n\nSuperGauss = [ pkg-config fftw.dev ];\n\nThen, building succeeds! We can now commit, push, and open a pull request. Commit messages need to be formatted in a certain way, as per nixpkgs contributing guide, so:\n\ngit add .\ngit commit -m \"rPackages.SuperGauss: add dependencies\"\n\nalso, there should only be one commit per fix. So if in the process of fixing a package you commited several times, you will need to use git rebase to squash all the commits into one. Once you open the pull request, a maintainer will get pinged, and merge the PR if everything is alright (which is usually the case for these one-liners). You can see the PR for {SuperGauss} here.\n\n\nThe process is relatively simple once you did it once or twice, but there are some issues: there is no easy way to find out on which packages we should focus on. For example, is {SuperGauss} really that important? The fix was very simple, so it’s ok, but if it took more effort, should we spend the limited time we have on it, or should we focus on another package? Also, if someone has already opened a PR to fix a package, but that PR hasn’t been merged yet, if I try to also fix the same package and try to build the package, it would still fail. So I might think that no one is taking care of it, and waste time duplicating efforts instead of either focusing on another package, or reviewing the open PR to accelerate the process of merging.\n\n\nDiscussing this with other contributors, László Kupcsik suggested we could use {packageRank} to find out which packages are getting a lot of downloads from CRAN, and so we could focus on fixing these packages first. This is a great idea and it gave me the idea to build some kind of report that would do this automatically for us, and also list opened and merged PRs so we wouldn’t risk duplicating efforts.\n\n\nThis report can be found here and now I’ll explain how I built it."
  },
  {
    "objectID": "posts/2024-02-29-nix_for_r_part_10.html#which-packages-to-fix-and-keeping-track-of-prs",
    "href": "posts/2024-02-29-nix_for_r_part_10.html#which-packages-to-fix-and-keeping-track-of-prs",
    "title": "Reproducible data science with Nix, part 10 – contributing to nixpkgs",
    "section": "\nWhich packages to fix and keeping track of PRs\n",
    "text": "Which packages to fix and keeping track of PRs\n\n\nSo the main idea was to know on which packages to focus on. So essentially, we wanted this table:\n\n\n\n\n\n\n\nbut with {packageRank} added to it. So the first step was to scrape this table, using {rvest}. This is what you can find on lines 11 to 63 of this {targets} workflow (alongside some basic cleaning). I won’t go too much into detail, but if something’s not clear, ping me on twitter or Mastodon or even open an issue on the report’s repository.\n\n\nNext I also get the reason the package failed building. So in the example from before, {AIUQ} failed because {SuperGauss} failed. On Hydra, you should be clicking to see this, but here I scrape it as well automatically, and add this information in a column called fails_because_of. This is what you can read on lines 65 to 77. I use a function called safe_get_failed_deps(), which you can find in the functions.R script on here. safe_get_failed_deps() wraps the main function, get_failed_deps(), with tryCatch(). This is because if anything goes wrong, I want my function to return NULL instead of an error, which would crash the whole pipeline.\n\n\nNext, I add the packages’ rank using a function that wraps packageRank::packageRank() called safe_packageRank() on line 97.\n\n\nsafe_packageRank() uses tryCatch() to return NULL in case there’s an error. This is needed because packageRank() will only work on CRAN packages, but Hydra also tries to build Bioconductor packages: when these packages’ names get passed to packageRank(), an error gets returned because these are not CRAN packages:\n\npackageRank(\"haha\")\nError: haha: misspelled or not on CRAN/Archive.\n\nbut instead of an error that would stop the pipeline, I prefer it simply returns NULL, hence tryCatch(). Also, I compute the rank of the package listed under the fails_because_of column and not the package column. If we go back to our example from before, {AIUQ} failed because {SuperGauss} failed, I’m actually interested in the rank of {SuperGauss}, and not {AIUQ} (which I way I went to all the trouble to scrape the failing dependency).\n\n\nSo, for now, when comparing to the table on Hydra, we have two further columns with the dependency that actually fails (or not, if the package fails on its own and not because of a dependency), and the rank of either the dependency that fails or the package itself.\n\n\nNext, I’d like to see if PRs have already been opened and merged. For this, I use the gh tool, which is a command line tool to interact with GitHub repositories. I wrote the get_prs() wrapper around gh to list the opened or the merged PRs of the nixpkgs repository. This is what it looks like (and is defined here):\n\nget_prs &lt;- function(state){\n\n  output_path &lt;- paste0(state, \"_prs.json\")\n\n  # Run the command\n  system(paste0(\n    \"gh pr list --state=\", state,\n    \" --search=rPackages -R NixOS/nixpkgs --json title,updatedAt,url &gt; \",\n    output_path\n  ))\n\n  # Return path for targets\n  output_path\n}\n\nBecause the PRs follow the contributing guidelines, I can easily process the PRs titles to get the name of the package (I essentially need to go from the string “rPackages.SuperGauss: fixing build” to “SuperGauss”) using regular expressions. This is what happens in the clean_prs() function here.\n\n\nMost of what follows is merging the right data frames and ensuring that I have something clean to show. Finally, an .Rmd document gets compiled, which you can find here. This will get compiled to an .html file which is what you see when you click here.\n\n\nThis runs every day at midnight using GitHub actions (the workflow is here) and then I use the raw.githack.com here to serve the rendered HTML file. So every time I push, or at midnight, the action runs, computes the package rank, checks if new PRs are available or have been merged, and the rendered file is immediately available. How’s that for serverless CI/CD?\n\n\nIf you are interested in using Nix to make your analyses reproducible, check out the other blog posts in this series and join our small but motivated community of R contributors to nixpkgs on Matrix. If you are interested in the history of Nix, checkout this super interesting blog post by Blair Fix.\n\n\nIf you’re interested into using project-specific, and reproducible development environments, give {rix} and Nix a try! Learn more about {rix} on its Github repository here or website. We wrote many vignettes that are conveniently numbered, so don’t hesitate to get started!\n\n\nThanks to the colleagues of the Matrix nixpkgs R channel for the fruitful discussions that helped shape this blog post and for proof-reading."
  },
  {
    "objectID": "posts/2022-12-21-longevity.html",
    "href": "posts/2022-12-21-longevity.html",
    "title": "Code longevity of the R programming language",
    "section": "",
    "text": "I’ve been working on a way to evaluate how old R code runs on the current version of R, and am now ready to share some results. It all started with this tweet:\nThe problem is that you have to find old code laying around. Some people have found old code they wrote a decade or more ago and tried to rerun it; there’s this blog post by Thomas Lumley and this other one by Colin Gillespie that I find fascinating, but ideally we’d have more than a handful of old scripts laying around. This is when Dirk Eddelbuettel suggested this:\nAnd this is what I did. I wrote a lot of code to achieve this graph here:\nThis graph shows the following: for each version of R, starting with R version 0.6.0 (released in 1997), how well the examples that came with a standard installation of R run on the current version of R (version 4.2.2 as of writing). These are the examples from the default packages like {base}, {stats}, {stats4}, and so on. Turns out that more than 75% of the example code from version 0.6.0 still works on the current version of R. A small fraction output a message (which doesn’t mean the code doesn’t work), some 5% raise a warning, which again doesn’t necessarily mean that the code doesn’t work, and finally around 20% or so errors. As you can see, the closer we get to the current release, the less errors get raised.\n(But something important should be noted: just because some old piece of code runs without error, doesn’t mean that the result is exactly the same. There might be cases where the same function returns different results on different versions of R.)\nThen, once I had this graph, I had to continue with packages. How well do old examples from any given package run on the current version of the same package?\nWhat I came up with is a Docker image that runs this for you, and even starts a Shiny app to let you explore the results. All you have to do is edit one line in the Dockerfile. This Docker image uses a lot of code from other projects, and I even had to write a package for this, called {wontrun}."
  },
  {
    "objectID": "posts/2022-12-21-longevity.html#the-wontrun-package",
    "href": "posts/2022-12-21-longevity.html#the-wontrun-package",
    "title": "Code longevity of the R programming language",
    "section": "\nThe {wontrun} package\n",
    "text": "The {wontrun} package\n\n\nThe problem I needed to solve was how to easily run examples from archived packages. So I needed to first have an easy way to download them, then extract the examples, and then run them. So to help me with this I wrote the {wontrun} package (thanks again to Deemah for suggesting the name and making the hex logo!). To be honest, the quality of this package could be improved. Documentation is still lacking, and the package only seems to work on Linux (but that’s not an issue, since it really only makes sense to use it within Docker). In any case, this package has a function to download the archived source code for a given package, using the get_archived_sources() function. This function takes the name of a package as an input and returns a data frame with the archived sources and the download links to them. To actually download the source packages, the get_examples() function is used. This function extracts the examples from the man/ folder included in source packages, and converts the examples into scripts. Remember that example files are in the .Rd format, which is some kind of markup language format. Thankfully, there’s a function called Rd2ex() from the {tools} package which I use to convert .Rd files into .R scripts.\n\n\nThen, all that there is to do is to run these scripts. But that’s not as easy as one might think. This is becuse I first need to make sure that the latest version of the package is installed, and ideally, I don’t want to pollute my library with packages that I never use but only wanted to assess for their code longevity. I also need to make sure that I’m running all these scripts all else being equal: so same version of R, same version of the current packages and same operating system. That why I needed to use Docker for this. Also, all the required dependencies to run the examples should get installed as well. Sometimes, some examples load data from another package. So for this, I’m using the renv::dependencies() function which scans a file for calls to library() or package::function() to list the dependencies and then install them. This all happens automatically.\n\n\nTo conclude this section: I cannot stress how much I’m relying on work by other people for this. This is the NAMESPACE file of the {wontrun} package (I’m only showing the import statements):\n\nimportFrom(callr,r_vanilla)\nimportFrom(ctv,ctv)\nimportFrom(dplyr,filter)\nimportFrom(dplyr,group_by)\nimportFrom(dplyr,mutate)\nimportFrom(dplyr,rename)\nimportFrom(dplyr,select)\nimportFrom(dplyr,ungroup)\nimportFrom(furrr,future_map2)\nimportFrom(future,multisession)\nimportFrom(future,plan)\nimportFrom(janitor,clean_names)\nimportFrom(lubridate,year)\nimportFrom(lubridate,ymd)\nimportFrom(lubridate,ymd_hm)\nimportFrom(magrittr,\"%&gt;%\")\nimportFrom(pacman,p_load)\nimportFrom(pkgsearch,cran_package)\nimportFrom(purrr,keep)\nimportFrom(purrr,map)\nimportFrom(purrr,map_chr)\nimportFrom(purrr,map_lgl)\nimportFrom(purrr,pluck)\nimportFrom(purrr,pmap_chr)\nimportFrom(purrr,possibly)\nimportFrom(renv,dependencies)\nimportFrom(rlang,`!!`)\nimportFrom(rlang,cnd_message)\nimportFrom(rlang,quo)\nimportFrom(rlang,try_fetch)\nimportFrom(rvest,html_nodes)\nimportFrom(rvest,html_table)\nimportFrom(rvest,read_html)\nimportFrom(stringr,str_extract)\nimportFrom(stringr,str_remove_all)\nimportFrom(stringr,str_replace)\nimportFrom(stringr,str_trim)\nimportFrom(tibble,as_tibble)\nimportFrom(tidyr,unnest)\nimportFrom(tools,Rd2ex)\nimportFrom(withr,with_package)\n\nThat’s a lot of packages, most of them from Posit. What can I say, these packages are great! Even if I could reduce the number of dependencies from {wontrun}, I honestly cannot be bothered, I’ve been spoilt by the quality of Posit packages."
  },
  {
    "objectID": "posts/2022-12-21-longevity.html#docker-for-reproducibility",
    "href": "posts/2022-12-21-longevity.html#docker-for-reproducibility",
    "title": "Code longevity of the R programming language",
    "section": "\nDocker for reproducibility\n",
    "text": "Docker for reproducibility\n\n\nThe Dockerfile I wrote is based on Ubuntu 22.04, compiles R 4.2.2 from source, and sets the repositories to https://packagemanager.rstudio.com/cran/linux/jammy/2022-11-21 . This way, the packages get downloaded exactly as they were on November 21st 2022. This ensures that if readers of this blog post want to run this to assess the code longevity of some R packages, we can compare results and be certain that any conditions raised are not specific to any difference in R or package version. It should be noted that this Dockerfile is based on the work of the Rocker project, and more specifically their versioned images which are recommended when reproducibility is needed. Becuse the code runs inside Docker, it doesn’t matter if the {wontrun} package only runs on Linux (I think that this is the case because of the untar() function which I use to decompress the downloaded compressed archives from CRAN, and which seems to have a different behaviour on Linux vs Windows. No idea how this function behaves on macOS).\n\n\nThe image defined by this Dockerfile is quite heavy, because I also installed all possible dependencies required to run R packages smoothly. This is because even though the Posit repositories install compiled packages on Linux, shared libraries are still needed for the packages to run.\n\n\nHere is what the Dockerfile looks like:\n\nFROM brodriguesco/wontrun:r4.2.2\n\n# This gets the shiny app\nRUN git clone https://github.com/b-rodrigues/longevity_app.git\n\n# These are needed for the Shiny app\nRUN R -e \"install.packages(c('dplyr', 'forcats', 'ggplot2', 'shiny', 'shinyWidgets', 'DT'))\"\n\nRUN mkdir /home/intermediary_output/\nRUN mkdir /home/output/\n\nCOPY wontrun.R /home/wontrun.R\n\n# Add one line per package you want to asses\nRUN Rscript '/home/wontrun.R' dplyr 6\nRUN Rscript '/home/wontrun.R' haven 6\n\nCMD mv /home/intermediary_output/* /home/output/ && R -e 'shiny::runApp(\"/longevity_app\", port = 1506, host = \"0.0.0.0\")'\n\nAs you can see it starts by pulling an image from Docker Hub called wontrun:r4.2.2. This is the image based on Ubuntu 22.04 with R compiled from source and all dependencies pre-installed. (This Dockerfile is available here.)\n\n\nThen my Shiny app gets cloned, the required packages for the app to run get installed, and some needed directories get made. Now comes the interesting part; a script called wontrun.R gets copied. This is what the script looks like:\n\n#!/usr/bin/env Rscript\nargs &lt;- commandArgs(trailingOnly = TRUE)\n\nlibrary(wontrun)\n\npackages_sources &lt;- get_archived_sources(args[1])\n\nout &lt;- wontrun(packages_sources ,\n               ncpus = args[2],\n               setup = TRUE,\n               wontrun_lib = \"/usr/local/lib/R/site-library/\")\n\nsaveRDS(object = out,\n        file = paste0(\"/home/intermediary_output/\", args[1], \".rds\"))\n\nThis script uses the {wontrun} package to get the archived sources of a package of interest, and the examples get executed and results tallied using the wontrun() function. The results then get saved into an .rds file.\n\n\nCalling this script is done with this line in the Dockerfile:\n\nRUN Rscript '/home/wontrun.R' dplyr 6\n\nThe dplyr and 6 get passed down to the wontrun.R script as a list called args. So args[1] is the “dplyr” string, and args[2] is 6. This means that the examples from archived versions of the {dplyr} package will get assessed on the current version of {dplyr} using 6 cores. You can add as many lines as you want and thus assess as many packages as you want. Once you’re done with editing the Dockerfile, you can build the image; this will actually run the code, so depending on how many packages you want to assess and the complexity of the examples, this may take some hours. To build the image run this in a console:\n\ndocker build -t code_longevity_packages .\n\nNow, you still need to actually run a container based on this image. Running the container will move the .rds files from the container to your machine so you can actually get to the results, and it will also start a Shiny app in which you will be able to upload the .rds file and explore the results. Run the container with (and don’t forget to change path/to/repository/ with the correct path on your machine):\n\ndocker run --rm --name code_longevity_packages_container -v /path/to/repository/code_longevity_packages/output:/home/output:rw -p 1506:1506 code_longevity_packages\n\nGo over to http://localhost:1506/ to start the Shiny app and explore the results:"
  },
  {
    "objectID": "posts/2022-12-21-longevity.html#a-collaborative-project",
    "href": "posts/2022-12-21-longevity.html#a-collaborative-project",
    "title": "Code longevity of the R programming language",
    "section": "\nA collaborative project\n",
    "text": "A collaborative project\n\n\nNow it’s your turn: are you curious about the code longevity of one particular package? Then fork the repository, edit the Dockerfile, build, run and do a pull request! It’d be great to have an overview of the code longevity of as many packages as possible. I thought about looking at the longevity of several packages that form a group, like the tidyverse packages or packages from CRAN Task views. Please also edit the package_list.txt file which lists packages for which we already have results. Find the repository here:\n\n\nhttps://github.com/b-rodrigues/code_longevity_packages\n\n\nBy the way, if you want the results of the language itself (so the results of running the examples of {base}, {stats}, etc), go here and click “download”.\n\n\nLooking forward to your pull requests!"
  },
  {
    "objectID": "posts/2019-06-20-tidy_eval_saga.html",
    "href": "posts/2019-06-20-tidy_eval_saga.html",
    "title": "Curly-Curly, the successor of Bang-Bang",
    "section": "",
    "text": "Writing functions that take data frame columns as arguments is a problem that most R users have been confronted with at some point. There are different ways to tackle this issue, and this blog post will focus on the solution provided by the latest release of the {rlang} package. You can read the announcement here, which explains really well what was wrong with the old syntax, and how the new syntax works now.\n\n\nI have written about the problem of writing functions that use data frame columns as arguments three years ago and two year ago too. Last year, I wrote a blog post that showed how to map a list of functions to a list of datasets with a list of columns as arguments that used the !!quo(column_name) syntax (the !! is pronounced bang-bang). Now, there is a new sheriff in town, {{}}, introduced in {rlang} version 0.4.0 that makes things even easier. The suggested pronunciation of {{}} is curly-curly, but there is no consensus yet.\n\n\nFirst, let’s load the {tidyverse}:\n\nlibrary(tidyverse)\n\nLet’s suppose that I need to write a function that takes a data frame, as well as a column from this data frame as arguments:\n\nhow_many_na &lt;- function(dataframe, column_name){\n  dataframe %&gt;%\n    filter(is.na(column_name)) %&gt;%\n    count()\n}\n\nLet’s try this function out on the starwars data:\n\ndata(starwars)\n\nhead(starwars)\n## # A tibble: 6 x 13\n##   name  height  mass hair_color skin_color eye_color birth_year gender\n##   &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; \n## 1 Luke…    172    77 blond      fair       blue            19   male  \n## 2 C-3PO    167    75 &lt;NA&gt;       gold       yellow         112   &lt;NA&gt;  \n## 3 R2-D2     96    32 &lt;NA&gt;       white, bl… red             33   &lt;NA&gt;  \n## 4 Dart…    202   136 none       white      yellow          41.9 male  \n## 5 Leia…    150    49 brown      light      brown           19   female\n## 6 Owen…    178   120 brown, gr… light      blue            52   male  \n## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n## #   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nAs you can see, there are missing values in the hair_color column. Let’s try to count how many missing values are in this column:\n\nhow_many_na(starwars, hair_color)\nError: object 'hair_color' not found\n\nR cannot find the hair_color column, and yet it is in the data! Well, this is actually exactly the issue. The issue is that the column is inside the dataframe, but when calling the function with hair_color as the second argument, R is looking for a variable called hair_color that does not exist. What about trying with \"hair_color\"?\n\nhow_many_na(starwars, \"hair_color\")\n## # A tibble: 1 x 1\n##       n\n##   &lt;int&gt;\n## 1     0\n\nNow we get something, but something wrong!\n\n\nOne way to solve this issue, is to not use the filter() function, and instead rely on base R:\n\nhow_many_na_base &lt;- function(dataframe, column_name){\n  na_index &lt;- is.na(dataframe[, column_name])\n  nrow(dataframe[na_index, column_name])\n}\n\nhow_many_na_base(starwars, \"hair_color\")\n## [1] 5\n\nThis works, but not using the {tidyverse} at all is not an option, at least for me. For instance, the next function, which uses a grouping variable, would be difficult to implement without the {tidyverse}:\n\nsummarise_groups &lt;- function(dataframe, grouping_var, column_name){\n  dataframe %&gt;%\n    group_by(grouping_var) %&gt;%  \n    summarise(mean(column_name, na.rm = TRUE))\n}\n\nCalling this function results in the following error message:\n\nError: Column `grouping_var` is unknown\n\nBefore the release of {rlang} 0.4.0 this is was the solution:\n\nsummarise_groups &lt;- function(dataframe, grouping_var, column_name){\n\n  grouping_var &lt;- enquo(grouping_var)\n  column_name &lt;- enquo(column_name)\n  mean_name &lt;- paste0(\"mean_\", quo_name(column_name))\n\n  dataframe %&gt;%\n    group_by(!!grouping_var) %&gt;%  \n    summarise(!!(mean_name) := mean(!!column_name, na.rm = TRUE))\n}\n\nThe core of the function remained very similar to the version from before, but now one has to use the enquo()-!! syntax. While not overly difficult to use, it is cumbersome.\n\n\nNow this can be simplified using the new {{}} syntax:\n\nsummarise_groups &lt;- function(dataframe, grouping_var, column_name){\n\n  dataframe %&gt;%\n    group_by({{grouping_var}}) %&gt;%  \n    summarise({{column_name}} := mean({{column_name}}, na.rm = TRUE))\n}\n\nMuch easier and cleaner! You still have to use the := operator instead of = for the column name however. Also, from my understanding, if you want to modify the column names, for instance in this case return \"mean_height\" instead of height you have to keep using the enquo()-!! syntax."
  },
  {
    "objectID": "posts/2018-09-15-time_use.html",
    "href": "posts/2018-09-15-time_use.html",
    "title": "How Luxembourguish residents spend their time: a small {flexdashboard} demo using the Time use survey data",
    "section": "",
    "text": "In a previous blog post I have showed how you could use the {tidyxl} package to go from a human readable Excel Workbook to a tidy data set (or flat file, as they are also called). Some people then contributed their solutions, which is always something I really enjoy when it happens. This way, I also get to learn things!\n\n\n@expersso proposed a solution without {tidyxl}:\n\n\n\nInteresting data wrangling exercise in #rstats. My solution (without using {tidyxl}): https://t.co/VjuOoM82yX https://t.co/VsXFyowigu\n\n— Eric (@expersso) September 12, 2018\n\n\n\nBen Stenhaug also proposed a solution on his github which is simpler than my code in a lot of ways!\n\n\nUpdate: @nacnudus also contributed his own version using {unpivotr}:\n\n\n\nHere’s a version using unpivotr https://t.co/l2hy6zCuKj\n\n— Duncan Garmonsway (@nacnudus) September 15, 2018\n\n\n\nNow, it would be too bad not to further analyze this data. I’ve been wanting to play around with the {flexdashboard} package for some time now, but never really got the opportunity to do so. The opportunity has now arrived. Using the cleaned data from the last post, I will further tweak it a little bit, and then produce a very simple dashboard using {flexdashboard}.\n\n\nIf you want to skip the rest of the blog post and go directly to the dashboard, just click here.\n\n\nTo make the data useful, I need to convert the strings that represent the amount of time spent doing a task (for example “1:23”) to minutes. For this I use the {chron} package:\n\nclean_data &lt;- clean_data %&gt;%\n    mutate(time_in_minutes = paste0(time, \":00\")) %&gt;% # I need to add \":00\" for the seconds else it won't work\n    mutate(time_in_minutes = \n               chron::hours(chron::times(time_in_minutes)) * 60 + \n               chron::minutes(chron::times(time_in_minutes)))\n\nrio::export(clean_data, \"clean_data.csv\")\n\nNow we’re ready to go! Below is the code to build the dashboard; if you want to try, you should copy and paste the code inside a Rmd document:\n\n---\ntitle: \"Time Use Survey of Luxembourguish residents\"\noutput: \n  flexdashboard::flex_dashboard:\n    orientation: columns\n    vertical_layout: fill\nruntime: shiny\n\n---\n\n`` `{r setup, include=FALSE}\nlibrary(flexdashboard)\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(ggthemes)\n\nmain_categories &lt;- c(\"Personal care\",\n                     \"Employment\",\n                     \"Study\",\n                     \"Household and family care\",\n                     \"Voluntary work and meetings\",\n                     \"Social life and entertainment\",\n                     \"Sports and outdoor activities\",\n                     \"Hobbies and games\",\n                     \"Media\",\n                     \"Travel\")\n\ndf &lt;- read.csv(\"clean_data.csv\") %&gt;%\n    rename(Population = population) %&gt;%\n    rename(Activities = activities)\n`` `\n\nInputs {.sidebar}\n-----------------------------------------------------------------------\n\n`` `{r}\n\nselectInput(inputId = \"activitiesName\", \n            label = \"Choose an activity\", \n            choices = unique(df$Activities))\n\nselectInput(inputId = \"dayName\", \n            label = \"Choose a day\", \n            choices = unique(df$day), \n            selected = \"Year 2014_Monday til Friday\")\n\nselectInput(inputId = \"populationName\", \n            label = \"Choose a population\", \n            choices = unique(df$Population), \n            multiple = TRUE, selected = c(\"Male\", \"Female\"))\n\n`` `\n\nThe Time Use Survey (TUS) aims to measure accurately how people allocate their time across different day-to-day activities. To this end, people are asked to keep records of all their activities in a time diary. For each activity, additional information is collected about whether or not the person was alone doing it or together with other persons, where did the activity take place, etc. The main studies on time use have been conducted to calculate indicators making possible comparative analysis of quality of life within the same population or between countries. International studies care more about specific activities such as work (unpaid or not), free time, leisure, personal care (including sleep), etc.\nSource: http://statistiques.public.lu/en/surveys/espace-households/time-use/index.html\n\nLayout based on https://jjallaire.shinyapps.io/shiny-biclust/\n\nRow\n-----------------------------------------------------------------------\n\n### Minutes spent per day on certain activities\n    \n`` `{r}\ndfInput &lt;- reactive({\n        df %&gt;% filter(Activities == input$activitiesName,\n                      Population %in% input$populationName,\n                      day %in% input$dayName)\n    })\n\n    dfInput2 &lt;- reactive({\n        df %&gt;% filter(Activities %in% main_categories,\n                      Population %in% input$populationName,\n                      day %in% input$dayName)\n    })\n    \n  renderPlotly({\n\n        df1 &lt;- dfInput()\n\n        p1 &lt;- ggplot(df1, \n                     aes(x = Activities, y = time_in_minutes, fill = Population)) +\n            geom_col(position = \"dodge\") + \n            theme_minimal() + \n            xlab(\"Activities\") + \n            ylab(\"Time in minutes\") +\n            scale_fill_gdocs()\n\n        ggplotly(p1)})\n`` `\n\nRow \n-----------------------------------------------------------------------\n\n### Proportion of the day spent on main activities\n    \n`` `{r}\nrenderPlotly({\n    \n       df2 &lt;- dfInput2()\n       \n       p2 &lt;- ggplot(df2, \n                   aes(x = Population, y = time_in_minutes, fill = Activities)) +\n           geom_bar(stat=\"identity\", position=\"fill\") + \n            xlab(\"Proportion\") + \n            ylab(\"Proportion\") +\n           theme_minimal() +\n           scale_fill_gdocs()\n       \n       ggplotly(p2)\n   })\n`` `\n\nYou will see that I have defined the following atomic vector:\n\nmain_categories &lt;- c(\"Personal care\",\n                     \"Employment\",\n                     \"Study\",\n                     \"Household and family care\",\n                     \"Voluntary work and meetings\",\n                     \"Social life and entertainment\",\n                     \"Sports and outdoor activities\",\n                     \"Hobbies and games\",\n                     \"Media\",\n                     \"Travel\")\n\nIf you go back to the raw Excel file, you will see that these main categories are then split into secondary activities. The first bar plot of the dashboard does not distinguish between the main and secondary activities, whereas the second barplot only considers the main activities. I could have added another column to the data that helped distinguish whether an activity was a main or secondary one, but I was lazy. The source code of the dashboard is very simple as it uses R Markdown. To have interactivity, I’ve used Shiny to dynamically filter the data, and built the plots with {ggplot2}. Finally, I’ve passed the plots to the ggplotly() function from the {plotly} package for some quick and easy javascript goodness!"
  },
  {
    "objectID": "posts/2016-12-21-work-on-lists-of-datasets-instead-of-individual-datasets-by-using-functional-programming.html",
    "href": "posts/2016-12-21-work-on-lists-of-datasets-instead-of-individual-datasets-by-using-functional-programming.html",
    "title": "Work on lists of datasets instead of individual datasets by using functional programming",
    "section": "",
    "text": "Analyzing a lot of datasets can be tedious. In my work, I often have to compute descriptive statistics, or plot some graphs for some variables for a lot of datasets. The variables in question have the same name accross the datasets but are measured for different years. As an example, imagine you have this situation:\n\n\ndata2000 &lt;- mtcars\ndata2001 &lt;- mtcars\n\n\nFor the sake of argument, imagine that data2000 is data from a survey conducted in the year 2000 and data2001 is the same survey but conducted in the year 2001. For illustration purposes, I use the mtcars dataset, but I could have used any other example. In these sort of situations, the variables are named the same in both datasets. Now if I want to check the summary statistics of a variable, I might do it by running:\n\n\nsummary(data2000$cyl)\nsummary(data2001$cyl)\n\n\nbut this can get quite tedious, especially if instead of only having two years of data, you have 20 years. Another possibility is to merge both datasets and then check the summary statistics of the variable of interest. But this might require a lot of preprocessing, and sometimes you really just want to do a quick check, or some dirty graphs. So you might be tempted to write a loop, which would require to put these two datasets in some kind of structure, such as a list:\n\n\nlist_data &lt;- list(\"data2000\" = data2000, \"data2001\" = data2001)\n\nfor (i in 1:2){\n    print(summary(list_data[[i]]$cyl))\n}\n\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000 \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000\n\nBut this also might get tedious, especially if you want to do this for a lot of different variables, and want to use different functions than summary().\n\n\nAnother, simpler way of doing this, is to use purrr::map() or lapply(). But there is a catch though: how do we specify the column we want to work on? Let’s try some things out:\n\n\nlibrary(purrr)\n\nmap(list_data, summary(cyl))\n\nError in summary(cyl) : object 'cyl' not found\n\nMaybe this will work:\n\n\nmap(list_data, summary, cyl)\n\n## $data2000\n     mpg             cyl             disp             hp       \nMin.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \nMedian :19.20   Median :6.000   Median :196.3   Median :123.0  \nMean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \nMax.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n     drat             wt             qsec             vs        \nMin.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \nMedian :3.695   Median :3.325   Median :17.71   Median :0.0000  \nMean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \nMax.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n      am              gear            carb      \nMin.   :0.0000   Min.   :3.000   Min.   :1.000  \n1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \nMedian :0.0000   Median :4.000   Median :2.000  \nMean   :0.4062   Mean   :3.688   Mean   :2.812  \n3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \nMax.   :1.0000   Max.   :5.000   Max.   :8.000  \n\ndata2001\n     mpg             cyl             disp             hp       \nMin.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \nMedian :19.20   Median :6.000   Median :196.3   Median :123.0  \nMean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \nMax.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n     drat             wt             qsec             vs        \nMin.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \nMedian :3.695   Median :3.325   Median :17.71   Median :0.0000  \nMean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \nMax.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n      am              gear            carb      \nMin.   :0.0000   Min.   :3.000   Min.   :1.000  \n1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \nMedian :0.0000   Median :4.000   Median :2.000  \nMean   :0.4062   Mean   :3.688   Mean   :2.812  \n3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \nMax.   :1.0000   Max.   :5.000   Max.   :8.000\n\nNot quite! You get the summary statistics of every variable, cyl simply gets ignored. This might be ok in our small toy example, but if you have dozens of datasets with hundreds of variables, the output becomes unreadable. The solution is to use an anonymous functions:\n\n\nmap(list_data, (function(x) summary(x$cyl)))\n\n## $data2000\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000 \n\n$data2001\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000\n\nThis is, in my opinion, much more readable than a loop, and the output of this is another list, so it’s easy to save it:\n\n\nsummary_cyl &lt;- map(list_data, (function(x) summary(x$cyl)))\nstr(summary_cyl)\n\n## List of 2\n$ data2000:Classes 'summaryDefault', 'table'  Named num [1:6] 4 4 6 6.19 8 ...\n .. ..- attr(*, \"names\")= chr [1:6] \"Min.\" \"1st Qu.\" \"Median\" \"Mean\" ...\n$ data2001:Classes 'summaryDefault', 'table'  Named num [1:6] 4 4 6 6.19 8 ...\n .. ..- attr(*, \"names\")= chr [1:6] \"Min.\" \"1st Qu.\" \"Median\" \"Mean\" ...\n\nWith the loop, you would need to “allocate” an empty list that you would fill at each iteration.\n\n\nSo this is already nice, but wouldn’t it be nicer to simply have to type:\n\n\nsummary(list_data$cyl)\n\n\nand have the summary of variable cyl for each dataset in the list? Well it is possible with the following function I wrote to make my life easier:\n\n\nto_map &lt;- function(func){\n  function(list, column, ...){\n    if(missing(column)){\n        res &lt;- purrr::map(list, (function(x) func(x, ...)))\n      } else {\n        res &lt;- purrr::map(list, (function(x) func(x[column], ...)))\n             }\n    res\n  }\n}\n\n\nBy following this chapter of Hadley Wickham’s book, Advanced R, I was able to write this function. What does it do? It basically generalizes a function to work on a list of datasets instead of just on a dataset. So for example, in the case of summary():\n\n\nsummarymap &lt;- to_map(summary)\n\nsummarymap(list_data, \"cyl\")\n\n$data2000\n     cyl       \nMin.   :4.000  \n1st Qu.:4.000  \nMedian :6.000  \nMean   :6.188  \n3rd Qu.:8.000  \nMax.   :8.000  \n\n$data2001\n     cyl       \nMin.   :4.000  \n1st Qu.:4.000  \nMedian :6.000  \nMean   :6.188  \n3rd Qu.:8.000  \nMax.   :8.000\n\nSo now everytime I want to have summary statistics for a variable, I just need to use summarymap():\n\n\nsummarymap(list_data, \"mpg\")\n\n## $data2000\n      mpg       \n Min.   :10.40  \n 1st Qu.:15.43  \n Median :19.20  \n Mean   :20.09  \n 3rd Qu.:22.80  \n Max.   :33.90  \n\n$data2001\n      mpg       \n Min.   :10.40  \n 1st Qu.:15.43  \n Median :19.20  \n Mean   :20.09  \n 3rd Qu.:22.80  \n Max.   :33.90\n\nIf I want the summary statistics for every variable, I simply omit the column name:\n\n\nsummarymap(list_data)\n\n$data2000\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n$data2001\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000\n\nI can use any function:\n\n\ntablemap &lt;- to_map(table)\n\ntablemap(list_data, \"cyl\")\n\n## $data2000\n\n 4  6  8 \n11  7 14 \n\n$data2001\n\n 4  6  8 \n11  7 14\n\ntablemap(list_data, \"mpg\")\n\n## $data2000\n\n10.4 13.3 14.3 14.7   15 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7 19.2 19.7 \n   2    1    1    1    1    2    1    1    1    1    1    1    1    2    1 \n  21 21.4 21.5 22.8 24.4   26 27.3 30.4 32.4 33.9 \n   2    2    1    2    1    1    1    2    1    1 \n\n$data2001\n\n10.4 13.3 14.3 14.7   15 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7 19.2 19.7 \n   2    1    1    1    1    2    1    1    1    1    1    1    1    2    1 \n  21 21.4 21.5 22.8 24.4   26 27.3 30.4 32.4 33.9 \n   2    2    1    2    1    1    1    2    1    1\n\nI hope you will find this little function useful, and as usual, for any comments just drop me an email by clicking the red enveloppe in the top right corner or tweet me."
  },
  {
    "objectID": "posts/2019-08-14-lpm.html",
    "href": "posts/2019-08-14-lpm.html",
    "title": "Using linear models with binary dependent variables, a simulation study",
    "section": "",
    "text": "This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free here. This is taken from Chapter 8, in which I discuss advanced functional programming methods for modeling.\n\n\nAs written just above (note: as written above in the book), map() simply applies a function to a list of inputs, and in the previous section we mapped ggplot() to generate many plots at once. This approach can also be used to map any modeling functions, for instance lm() to a list of datasets.\n\n\nFor instance, suppose that you wish to perform a Monte Carlo simulation. Suppose that you are dealing with a binary choice problem; usually, you would use a logistic regression for this.\n\n\nHowever, in certain disciplines, especially in the social sciences, the so-called Linear Probability Model is often used as well. The LPM is a simple linear regression, but unlike the standard setting of a linear regression, the dependent variable, or target, is a binary variable, and not a continuous variable. Before you yell “Wait, that’s illegal”, you should know that in practice LPMs do a good job of estimating marginal effects, which is what social scientists and econometricians are often interested in. Marginal effects are another way of interpreting models, giving how the outcome (or the target) changes given a change in a independent variable (or a feature). For instance, a marginal effect of 0.10 for age would mean that probability of success would increase by 10% for each added year of age.\n\n\nThere has been a lot of discussion on logistic regression vs LPMs, and there are pros and cons of using LPMs. Micro-econometricians are still fond of LPMs, even though the pros of LPMs are not really convincing. However, quoting Angrist and Pischke:\n\n\n“While a nonlinear model may fit the CEF (population conditional expectation function) for LDVs (limited dependent variables) more closely than a linear model, when it comes to marginal effects, this probably matters little” (source: Mostly Harmless Econometrics)\n\n\nso LPMs are still used for estimating marginal effects.\n\n\nLet us check this assessment with one example. First, we simulate some data, then run a logistic regression and compute the marginal effects, and then compare with a LPM:\n\nset.seed(1234)\nx1 &lt;- rnorm(100)\nx2 &lt;- rnorm(100)\n  \nz &lt;- .5 + 2*x1 + 4*x2\n\np &lt;- 1/(1 + exp(-z))\n\ny &lt;- rbinom(100, 1, p)\n\ndf &lt;- tibble(y = y, x1 = x1, x2 = x2)\n\nThis data generating process generates data from a binary choice model. Fitting the model using a logistic regression allows us to recover the structural parameters:\n\nlogistic_regression &lt;- glm(y ~ ., data = df, family = binomial(link = \"logit\"))\n\nLet’s see a summary of the model fit:\n\nsummary(logistic_regression)\n## \n## Call:\n## glm(formula = y ~ ., family = binomial(link = \"logit\"), data = df)\n## \n## Deviance Residuals: \n##      Min        1Q    Median        3Q       Max  \n## -2.91941  -0.44872   0.00038   0.42843   2.55426  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)   0.0960     0.3293   0.292 0.770630    \n## x1            1.6625     0.4628   3.592 0.000328 ***\n## x2            3.6582     0.8059   4.539 5.64e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 138.629  on 99  degrees of freedom\n## Residual deviance:  60.576  on 97  degrees of freedom\n## AIC: 66.576\n## \n## Number of Fisher Scoring iterations: 7\n\nWe do recover the parameters that generated the data, but what about the marginal effects? We can get the marginal effects easily using the {margins} package:\n\nlibrary(margins)\n\nmargins(logistic_regression)\n## Average marginal effects\n## glm(formula = y ~ ., family = binomial(link = \"logit\"), data = df)\n##      x1     x2\n##  0.1598 0.3516\n\nOr, even better, we can compute the true marginal effects, since we know the data generating process:\n\nmeffects &lt;- function(dataset, coefs){\n  X &lt;- dataset %&gt;% \n  select(-y) %&gt;% \n  as.matrix()\n  \n  dydx_x1 &lt;- mean(dlogis(X%*%c(coefs[2], coefs[3]))*coefs[2])\n  dydx_x2 &lt;- mean(dlogis(X%*%c(coefs[2], coefs[3]))*coefs[3])\n  \n  tribble(~term, ~true_effect,\n          \"x1\", dydx_x1,\n          \"x2\", dydx_x2)\n}\n\n(true_meffects &lt;- meffects(df, c(0.5, 2, 4)))\n## # A tibble: 2 x 2\n##   term  true_effect\n##   &lt;chr&gt;       &lt;dbl&gt;\n## 1 x1          0.175\n## 2 x2          0.350\n\nOk, so now what about using this infamous Linear Probability Model to estimate the marginal effects?\n\nlpm &lt;- lm(y ~ ., data = df)\n\nsummary(lpm)\n## \n## Call:\n## lm(formula = y ~ ., data = df)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.83953 -0.31588 -0.02885  0.28774  0.77407 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  0.51340    0.03587  14.314  &lt; 2e-16 ***\n## x1           0.16771    0.03545   4.732 7.58e-06 ***\n## x2           0.31250    0.03449   9.060 1.43e-14 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.3541 on 97 degrees of freedom\n## Multiple R-squared:  0.5135, Adjusted R-squared:  0.5034 \n## F-statistic: 51.18 on 2 and 97 DF,  p-value: 6.693e-16\n\nIt’s not too bad, but maybe it could have been better in other circumstances. Perhaps if we had more observations, or perhaps for a different set of structural parameters the results of the LPM would have been closer. The LPM estimates the marginal effect of x1 to be 0.1677134 vs 0.1597956 for the logistic regression and for x2, the LPM estimation is 0.3124966 vs 0.351607. The true marginal effects are 0.1750963 and 0.3501926 for x1 and x2 respectively.\n\n\nJust as to assess the accuracy of a model data scientists perform cross-validation, a Monte Carlo study can be performed to asses how close the estimation of the marginal effects using a LPM is to the marginal effects derived from a logistic regression. It will allow us to test with datasets of different sizes, and generated using different structural parameters.\n\n\nFirst, let’s write a function that generates data. The function below generates 10 datasets of size 100 (the code is inspired by this StackExchange answer):\n\ngenerate_datasets &lt;- function(coefs = c(.5, 2, 4), sample_size = 100, repeats = 10){\n\n  generate_one_dataset &lt;- function(coefs, sample_size){\n  x1 &lt;- rnorm(sample_size)\n  x2 &lt;- rnorm(sample_size)\n  \n  z &lt;- coefs[1] + coefs[2]*x1 + coefs[3]*x2\n\n  p &lt;- 1/(1 + exp(-z))\n\n  y &lt;- rbinom(sample_size, 1, p)\n\n  df &lt;- tibble(y = y, x1 = x1, x2 = x2)\n  }\n\n  simulations &lt;- rerun(.n = repeats, generate_one_dataset(coefs, sample_size))\n \n  tibble(\"coefs\" = list(coefs), \"sample_size\" = sample_size, \"repeats\" = repeats, \"simulations\" = list(simulations))\n}\n\nLet’s first generate one dataset:\n\none_dataset &lt;- generate_datasets(repeats = 1)\n\nLet’s take a look at one_dataset:\n\none_dataset\n## # A tibble: 1 x 4\n##   coefs     sample_size repeats simulations\n##   &lt;list&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;     \n## 1 &lt;dbl [3]&gt;         100       1 &lt;list [1]&gt;\n\nAs you can see, the tibble with the simulated data is inside a list-column called simulations. Let’s take a closer look:\n\nstr(one_dataset$simulations)\n## List of 1\n##  $ :List of 1\n##   ..$ :Classes 'tbl_df', 'tbl' and 'data.frame': 100 obs. of  3 variables:\n##   .. ..$ y : int [1:100] 0 1 1 1 0 1 1 0 0 1 ...\n##   .. ..$ x1: num [1:100] 0.437 1.06 0.452 0.663 -1.136 ...\n##   .. ..$ x2: num [1:100] -2.316 0.562 -0.784 -0.226 -1.587 ...\n\nThe structure is quite complex, and it’s important to understand this, because it will have an impact on the next lines of code; it is a list, containing a list, containing a dataset! No worries though, we can still map over the datasets directly, by using modify_depth() instead of map().\n\n\nNow, let’s fit a LPM and compare the estimation of the marginal effects with the true marginal effects. In order to have some confidence in our results, we will not simply run a linear regression on that single dataset, but will instead simulate hundreds, then thousands and ten of thousands of data sets, get the marginal effects and compare them to the true ones (but here I won’t simulate more than 500 datasets).\n\n\nLet’s first generate 10 datasets:\n\nmany_datasets &lt;- generate_datasets()\n\nNow comes the tricky part. I have this object, many_datasets looking like this:\n\nmany_datasets\n## # A tibble: 1 x 4\n##   coefs     sample_size repeats simulations\n##   &lt;list&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;     \n## 1 &lt;dbl [3]&gt;         100      10 &lt;list [10]&gt;\n\nI would like to fit LPMs to the 10 datasets. For this, I will need to use all the power of functional programming and the {tidyverse}. I will be adding columns to this data frame using mutate() and mapping over the simulations list-column using modify_depth(). The list of data frames is at the second level (remember, it’s a list containing a list containing data frames).\n\n\nI’ll start by fitting the LPMs, then using broom::tidy() I will get a nice data frame of the estimated parameters. I will then only select what I need, and then bind the rows of all the data frames. I will do the same for the true marginal effects.\n\n\nI highly suggest that you run the following lines, one after another. It is complicated to understand what’s going on if you are not used to such workflows. However, I hope to convince you that once it will click, it’ll be much more intuitive than doing all this inside a loop. Here’s the code:\n\nresults &lt;- many_datasets %&gt;% \n  mutate(lpm = modify_depth(simulations, 2, ~lm(y ~ ., data = .x))) %&gt;% \n  mutate(lpm = modify_depth(lpm, 2, broom::tidy)) %&gt;% \n  mutate(lpm = modify_depth(lpm, 2, ~select(., term, estimate))) %&gt;% \n  mutate(lpm = modify_depth(lpm, 2, ~filter(., term != \"(Intercept)\"))) %&gt;% \n  mutate(lpm = map(lpm, bind_rows)) %&gt;% \n  mutate(true_effect = modify_depth(simulations, 2, ~meffects(., coefs = coefs[[1]]))) %&gt;% \n  mutate(true_effect = map(true_effect, bind_rows))\n\nThis is how results looks like:\n\nresults\n## # A tibble: 1 x 6\n##   coefs     sample_size repeats simulations lpm             true_effect    \n##   &lt;list&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;      &lt;list&gt;          &lt;list&gt;         \n## 1 &lt;dbl [3]&gt;         100      10 &lt;list [10]&gt; &lt;tibble [20 × … &lt;tibble [20 × …\n\nLet’s take a closer look to the lpm and true_effect columns:\n\nresults$lpm\n## [[1]]\n## # A tibble: 20 x 2\n##    term  estimate\n##    &lt;chr&gt;    &lt;dbl&gt;\n##  1 x1       0.228\n##  2 x2       0.353\n##  3 x1       0.180\n##  4 x2       0.361\n##  5 x1       0.165\n##  6 x2       0.374\n##  7 x1       0.182\n##  8 x2       0.358\n##  9 x1       0.125\n## 10 x2       0.345\n## 11 x1       0.171\n## 12 x2       0.331\n## 13 x1       0.122\n## 14 x2       0.309\n## 15 x1       0.129\n## 16 x2       0.332\n## 17 x1       0.102\n## 18 x2       0.374\n## 19 x1       0.176\n## 20 x2       0.410\nresults$true_effect\n## [[1]]\n## # A tibble: 20 x 2\n##    term  true_effect\n##    &lt;chr&gt;       &lt;dbl&gt;\n##  1 x1          0.183\n##  2 x2          0.366\n##  3 x1          0.166\n##  4 x2          0.331\n##  5 x1          0.174\n##  6 x2          0.348\n##  7 x1          0.169\n##  8 x2          0.339\n##  9 x1          0.167\n## 10 x2          0.335\n## 11 x1          0.173\n## 12 x2          0.345\n## 13 x1          0.157\n## 14 x2          0.314\n## 15 x1          0.170\n## 16 x2          0.340\n## 17 x1          0.182\n## 18 x2          0.365\n## 19 x1          0.161\n## 20 x2          0.321\n\nLet’s bind the columns, and compute the difference between the true and estimated marginal effects:\n\nsimulation_results &lt;- results %&gt;% \n  mutate(difference = map2(.x = lpm, .y = true_effect, bind_cols)) %&gt;% \n  mutate(difference = map(difference, ~mutate(., difference = true_effect - estimate))) %&gt;% \n  mutate(difference = map(difference, ~select(., term, difference))) %&gt;% \n  pull(difference) %&gt;% \n  .[[1]]\n\nLet’s take a look at the simulation results:\n\nsimulation_results %&gt;% \n  group_by(term) %&gt;% \n  summarise(mean = mean(difference), \n            sd = sd(difference))\n## # A tibble: 2 x 3\n##   term     mean     sd\n##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n## 1 x1     0.0122 0.0370\n## 2 x2    -0.0141 0.0306\n\nAlready with only 10 simulated datasets, the difference in means is not significant. Let’s rerun the analysis, but for difference sizes. In order to make things easier, we can put all the code into a nifty function:\n\nmonte_carlo &lt;- function(coefs, sample_size, repeats){\n  many_datasets &lt;- generate_datasets(coefs, sample_size, repeats)\n  \n  results &lt;- many_datasets %&gt;% \n    mutate(lpm = modify_depth(simulations, 2, ~lm(y ~ ., data = .x))) %&gt;% \n    mutate(lpm = modify_depth(lpm, 2, broom::tidy)) %&gt;% \n    mutate(lpm = modify_depth(lpm, 2, ~select(., term, estimate))) %&gt;% \n    mutate(lpm = modify_depth(lpm, 2, ~filter(., term != \"(Intercept)\"))) %&gt;% \n    mutate(lpm = map(lpm, bind_rows)) %&gt;% \n    mutate(true_effect = modify_depth(simulations, 2, ~meffects(., coefs = coefs[[1]]))) %&gt;% \n    mutate(true_effect = map(true_effect, bind_rows))\n\n  simulation_results &lt;- results %&gt;% \n    mutate(difference = map2(.x = lpm, .y = true_effect, bind_cols)) %&gt;% \n    mutate(difference = map(difference, ~mutate(., difference = true_effect - estimate))) %&gt;% \n    mutate(difference = map(difference, ~select(., term, difference))) %&gt;% \n    pull(difference) %&gt;% \n    .[[1]]\n\n  simulation_results %&gt;% \n    group_by(term) %&gt;% \n    summarise(mean = mean(difference), \n              sd = sd(difference))\n}\n\nAnd now, let’s run the simulation for different parameters and sizes:\n\nmonte_carlo(c(.5, 2, 4), 100, 10)\n## # A tibble: 2 x 3\n##   term      mean     sd\n##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n## 1 x1    -0.00826 0.0291\n## 2 x2    -0.00732 0.0412\nmonte_carlo(c(.5, 2, 4), 100, 100)\n## # A tibble: 2 x 3\n##   term     mean     sd\n##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n## 1 x1    0.00360 0.0392\n## 2 x2    0.00517 0.0446\nmonte_carlo(c(.5, 2, 4), 100, 500)\n## # A tibble: 2 x 3\n##   term       mean     sd\n##   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n## 1 x1    -0.00152  0.0371\n## 2 x2    -0.000701 0.0423\nmonte_carlo(c(pi, 6, 9), 100, 10)\n## # A tibble: 2 x 3\n##   term      mean     sd\n##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n## 1 x1    -0.00829 0.0546\n## 2 x2     0.00178 0.0370\nmonte_carlo(c(pi, 6, 9), 100, 100)\n## # A tibble: 2 x 3\n##   term     mean     sd\n##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n## 1 x1    0.0107  0.0608\n## 2 x2    0.00831 0.0804\nmonte_carlo(c(pi, 6, 9), 100, 500)\n## # A tibble: 2 x 3\n##   term     mean     sd\n##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n## 1 x1    0.00879 0.0522\n## 2 x2    0.0113  0.0668\n\nWe see that, at least for this set of parameters, the LPM does a good job of estimating marginal effects.\n\n\nNow, this study might in itself not be very interesting to you, but I believe the general approach is quite useful and flexible enough to be adapted to all kinds of use-cases."
  },
  {
    "objectID": "blog/2018-10-27-lux_elections_analysis/index.html",
    "href": "blog/2018-10-27-lux_elections_analysis/index.html",
    "title": "Maps with pie charts on top of each administrative division: an example with Luxembourg’s elections data",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2023-10-20-nix_for_r_part7/index.html",
    "href": "blog/2023-10-20-nix_for_r_part7/index.html",
    "title": "Reproducible data science with Nix, part 7 – Building a Quarto book using Nix on Github Actions",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2016-07-26-read-a-lot-of-datasets-at-once-with-r/index.html",
    "href": "blog/2016-07-26-read-a-lot-of-datasets-at-once-with-r/index.html",
    "title": "Read a lot of datasets at once with R",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-04-10-brotools_describe/index.html",
    "href": "blog/2018-04-10-brotools_describe/index.html",
    "title": "Get basic summary statistics for all the variables in a data frame",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2023-09-20-nix_for_r_part6/index.html",
    "href": "blog/2023-09-20-nix_for_r_part6/index.html",
    "title": "Reproducible data science with Nix, part 6 – CI/CD has never been easier",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-05-21-heavy_syntax/index.html",
    "href": "blog/2022-05-21-heavy_syntax/index.html",
    "title": "Get packages that introduce unique syntax adopted less?",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2020-03-10-exp_tidymodels/index.html",
    "href": "blog/2020-03-10-exp_tidymodels/index.html",
    "title": "Explainbility of {tidymodels} models with {iml}",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-05-04-diffindiff_part2/index.html",
    "href": "blog/2019-05-04-diffindiff_part2/index.html",
    "title": "Fast food, causality and R packages, part 2",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2024-08-28-nix_for_r_part_12/index.html",
    "href": "blog/2024-08-28-nix_for_r_part_12/index.html",
    "title": "Reproducible data science with Nix, part 12 – Nix as a polyglot build automation tool for data science",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2013-11-07-gmm-with-rmd/index.html",
    "href": "blog/2013-11-07-gmm-with-rmd/index.html",
    "title": "Nonlinear Gmm with R - Example with a logistic regression",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-09-08-steam_linux/index.html",
    "href": "blog/2018-09-08-steam_linux/index.html",
    "title": "The year of the GNU+Linux desktop is upon us: using user ratings of Steam Play compatibility to play around with regex and the tidyverse",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-12-27-fun_gganimate/index.html",
    "href": "blog/2018-12-27-fun_gganimate/index.html",
    "title": "Some fun with {gganimate}",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-10-12-cluster_ts/index.html",
    "href": "blog/2019-10-12-cluster_ts/index.html",
    "title": "Cluster multiple time series using K-means",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2016-12-24-functional-programming-and-unit-testing-for-data-munging-with-r-available-on-leanpub/index.html",
    "href": "blog/2016-12-24-functional-programming-and-unit-testing-for-data-munging-with-r-available-on-leanpub/index.html",
    "title": "Functional programming and unit testing for data munging with R available on Leanpub",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2016-07-30-merge-a-list-of-datasets-together/index.html",
    "href": "blog/2016-07-30-merge-a-list-of-datasets-together/index.html",
    "title": "Merge a list of datasets together",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-10-23-licenses/index.html",
    "href": "blog/2022-10-23-licenses/index.html",
    "title": "R, its license and my take on it",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-09-11-human_to_machine/index.html",
    "href": "blog/2018-09-11-human_to_machine/index.html",
    "title": "Going from a human readable Excel file to a machine-readable csv with {tidyxl}",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-01-19-mapping_functions_with_any_cols/index.html",
    "href": "blog/2018-01-19-mapping_functions_with_any_cols/index.html",
    "title": "Mapping a list of functions to a list of datasets with a list of columns as arguments",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2021-12-17-expand_knitr/index.html",
    "href": "blog/2021-12-17-expand_knitr/index.html",
    "title": "How to write code that returns (Rmarkdown) code",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-12-21-longevity/index.html",
    "href": "blog/2022-12-21-longevity/index.html",
    "title": "Code longevity of the R programming language",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2020-01-26-harold/index.html",
    "href": "blog/2020-01-26-harold/index.html",
    "title": "Dynamic discrete choice models, reinforcement learning and Harold, part 1",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-12-02-hyper-parameters/index.html",
    "href": "blog/2018-12-02-hyper-parameters/index.html",
    "title": "What hyper-parameters are, and what to do with them; an illustration with ridge regression",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2014-04-23-r-s4-rootfinding/index.html",
    "href": "blog/2014-04-23-r-s4-rootfinding/index.html",
    "title": "Object Oriented Programming with R: An example with a Cournot duopoly",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2016-12-21-work-on-lists-of-datasets-instead-of-individual-datasets-by-using-functional-programming/index.html",
    "href": "blog/2016-12-21-work-on-lists-of-datasets-instead-of-individual-datasets-by-using-functional-programming/index.html",
    "title": "Work on lists of datasets instead of individual datasets by using functional programming",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-04-28-diffindiff_part1/index.html",
    "href": "blog/2019-04-28-diffindiff_part1/index.html",
    "title": "Fast food, causality and R packages, part 1",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2015-05-03-update-introduction-r-programming/index.html",
    "href": "blog/2015-05-03-update-introduction-r-programming/index.html",
    "title": "Update to Introduction to programming econometrics with R",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2023-04-25-10_years/index.html",
    "href": "blog/2023-04-25-10_years/index.html",
    "title": "I’ve been blogging for 10 years",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2021-03-28-survey/index.html",
    "href": "blog/2021-03-28-survey/index.html",
    "title": "The link between keyboard layouts and typing speed - Data collection phase",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-10-21-lux_elections/index.html",
    "href": "blog/2018-10-21-lux_elections/index.html",
    "title": "Getting the data from the Luxembourguish elections out of Excel",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-06-20-tidy_eval_saga/index.html",
    "href": "blog/2019-06-20-tidy_eval_saga/index.html",
    "title": "Curly-Curly, the successor of Bang-Bang",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2024-09-27-nix_part_13/index.html",
    "href": "blog/2024-09-27-nix_part_13/index.html",
    "title": "Reproducible data science with Nix, part 13 – {rix} is on CRAN!",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2020-03-26-bepo_lu/index.html",
    "href": "blog/2020-03-26-bepo_lu/index.html",
    "title": "What would a keyboard optimised for Luxembourguish look like?",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2017-07-02-dplyr-0-70-tutorial/index.html",
    "href": "blog/2017-07-02-dplyr-0-70-tutorial/index.html",
    "title": "Lesser known dplyr 0.7* tricks",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2017-03-27-introducing_brotools/index.html",
    "href": "blog/2017-03-27-introducing_brotools/index.html",
    "title": "Introducing brotools",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-05-26-safer_programs/index.html",
    "href": "blog/2022-05-26-safer_programs/index.html",
    "title": "Some learnings from functional programming you can use to write safer programs",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-03-31-tesseract/index.html",
    "href": "blog/2019-03-31-tesseract/index.html",
    "title": "Get text from pdfs or images using OCR: a tutorial with {tesseract} and {magick}",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-11-25-tidy_cv/index.html",
    "href": "blog/2018-11-25-tidy_cv/index.html",
    "title": "A tutorial on tidy cross-validation with R",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-02-04-newspapers_shiny_app_tutorial/index.html",
    "href": "blog/2019-02-04-newspapers_shiny_app_tutorial/index.html",
    "title": "Building a shiny app to explore historical newspapers: a step-by-step guide",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-12-24-modern_objects/index.html",
    "href": "blog/2018-12-24-modern_objects/index.html",
    "title": "Objects types and some useful R functions for beginners",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2020-04-20-no_excuse/index.html",
    "href": "blog/2020-04-20-no_excuse/index.html",
    "title": "No excuse not to be a Bayesian anymore",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2017-12-27-build_formulae/index.html",
    "href": "blog/2017-12-27-build_formulae/index.html",
    "title": "Building formulae",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-06-02-arcane/index.html",
    "href": "blog/2022-06-02-arcane/index.html",
    "title": "R will always be arcane to those who do not make a serious effort to learn it…",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-01-13-newspapers_mets_alto/index.html",
    "href": "blog/2019-01-13-newspapers_mets_alto/index.html",
    "title": "Making sense of the METS and ALTO XML standards",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2017-02-17-how_to_use_jailbreakr/index.html",
    "href": "blog/2017-02-17-how_to_use_jailbreakr/index.html",
    "title": "How to use jailbreakr",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2021-04-17-post_strat/index.html",
    "href": "blog/2021-04-17-post_strat/index.html",
    "title": "Dealing with non-representative samples with post-stratification",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2013-01-29-method-of-simulated-moments-with-r/index.html",
    "href": "blog/2013-01-29-method-of-simulated-moments-with-r/index.html",
    "title": "Method of Simulated Moments with R",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-02-16-importing_30gb_of_data/index.html",
    "href": "blog/2018-02-16-importing_30gb_of_data/index.html",
    "title": "Importing 30GB of data into R with sparklyr",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2020-11-21-guis_mistake/index.html",
    "href": "blog/2020-11-21-guis_mistake/index.html",
    "title": "Graphical User Interfaces were a mistake but you can still make things right",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2023-05-08-dock_dev_env/index.html",
    "href": "blog/2023-05-08-dock_dev_env/index.html",
    "title": "Why you should consider working on a dockerized development environment",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-01-03-lists_all_the_way/index.html",
    "href": "blog/2018-01-03-lists_all_the_way/index.html",
    "title": "It’s lists all the way down",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2023-03-03-quarto_books/index.html",
    "href": "blog/2023-03-03-quarto_books/index.html",
    "title": "What I’ve learned making an .epub Ebook with Quarto",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2021-03-02-no_shiny_dashboard/index.html",
    "href": "blog/2021-03-02-no_shiny_dashboard/index.html",
    "title": "Server(shiny)-less dashboards with R, {htmlwidgets} and {crosstalk}",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-03-20-pivot/index.html",
    "href": "blog/2019-03-20-pivot/index.html",
    "title": "Pivoting data frames just got easier thanks to pivot_wide() and pivot_long()",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-11-03-nethack_analysis/index.html",
    "href": "blog/2018-11-03-nethack_analysis/index.html",
    "title": "Analyzing NetHack data, part 1: What kills the players",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-11-15-tidy_gridsearch/index.html",
    "href": "blog/2018-11-15-tidy_gridsearch/index.html",
    "title": "Searching for the optimal hyper-parameters of an ARIMA model in parallel: the tidy gridsearch approach",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-06-24-fun_ts/index.html",
    "href": "blog/2018-06-24-fun_ts/index.html",
    "title": "Forecasting my weight with R",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-03-03-historical_vowpal/index.html",
    "href": "blog/2019-03-03-historical_vowpal/index.html",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 1",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2021-03-05-covid_lu/index.html",
    "href": "blog/2021-03-05-covid_lu/index.html",
    "title": "Using explainability methods to understand (some part) of the spread of COVID-19 in a landlocked country",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2017-08-27-why_tidyeval/index.html",
    "href": "blog/2017-08-27-why_tidyeval/index.html",
    "title": "Why I find tidyeval useful",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-03-05-historical_vowpal_part2/index.html",
    "href": "blog/2019-03-05-historical_vowpal_part2/index.html",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-02-11-census-random_forest/index.html",
    "href": "blog/2018-02-11-census-random_forest/index.html",
    "title": "Predicting job search by training a random forest on an unbalanced dataset",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2023-08-12-nix_for_r_part4/index.html",
    "href": "blog/2023-08-12-nix_for_r_part4/index.html",
    "title": "Reproducible data science with Nix, part 4 – So long, {renv} and Docker, and thanks for all the fish",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-12-21-tidyverse_pi/index.html",
    "href": "blog/2018-12-21-tidyverse_pi/index.html",
    "title": "Using the tidyverse for more than data manipulation: estimating pi with Monte Carlo methods",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-10-01-why_js_shiny/index.html",
    "href": "blog/2022-10-01-why_js_shiny/index.html",
    "title": "Why and how to use JS in your Shiny app",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2021-09-05-speedrunning_rows/index.html",
    "href": "blog/2021-09-05-speedrunning_rows/index.html",
    "title": "Speedrunning row-oriented workflows",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-01-05-lists_all_the_way2/index.html",
    "href": "blog/2018-01-05-lists_all_the_way2/index.html",
    "title": "It’s lists all the way down, part 2: We need to go deeper",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2015-11-11-bootstrapping-did-with-r/index.html",
    "href": "blog/2015-11-11-bootstrapping-did-with-r/index.html",
    "title": "Bootstrapping standard errors for difference-in-differences estimation with R",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2016-11-04-ive-started-writing-a-book-functional-programming-and-unit-testing-for-data-munging-with-r/index.html",
    "href": "blog/2016-11-04-ive-started-writing-a-book-functional-programming-and-unit-testing-for-data-munging-with-r/index.html",
    "title": "I’ve started writing a ‘book’: Functional programming and unit testing for data munging with R",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-05-19-spacemacs/index.html",
    "href": "blog/2019-05-19-spacemacs/index.html",
    "title": "The never-ending editor war (?)",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2021-03-19-no_loops_tidyeval/index.html",
    "href": "blog/2021-03-19-no_loops_tidyeval/index.html",
    "title": "How to treat as many files as fit on your hard disk without loops (sorta) nor running out of memory all the while being as lazy as possible",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2018-11-21-lux_castle/index.html",
    "href": "blog/2018-11-21-lux_castle/index.html",
    "title": "The best way to visit Luxembourguish castles is doing data science + combinatorial optimization",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2021-02-06-echarts_map/index.html",
    "href": "blog/2021-02-06-echarts_map/index.html",
    "title": "How to draw a map of arbitrary contiguous regions, or visualizing the spread of COVID-19 in the Greater Region",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2017-03-29-make-ggplot2-purrr/index.html",
    "href": "blog/2017-03-29-make-ggplot2-purrr/index.html",
    "title": "Make ggplot2 purrr",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2023-03-07-dry_wit/index.html",
    "href": "blog/2023-03-07-dry_wit/index.html",
    "title": "Software engineering techniques that non-programmers who write a lot of code can benefit from — the DRY WIT approach",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-06-04-cosine_sim/index.html",
    "href": "blog/2019-06-04-cosine_sim/index.html",
    "title": "Using cosine similarity to find matching documents: a tutorial using Seneca’s letters to his friend Lucilius",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2023-09-29-voyager/index.html",
    "href": "blog/2023-09-29-voyager/index.html",
    "title": "ZSA Voyager review",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-01-04-newspapers/index.html",
    "href": "blog/2019-01-04-newspapers/index.html",
    "title": "Looking into 19th century ads from a Luxembourguish newspaper with R",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2020-11-05-retire_data_science/index.html",
    "href": "blog/2020-11-05-retire_data_science/index.html",
    "title": "It’s time to retire the “data scientist” label",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2019-06-12-intermittent/index.html",
    "href": "blog/2019-06-12-intermittent/index.html",
    "title": "Intermittent demand, Croston and Die Hard",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "blog/2022-04-11-monads/index.html",
    "href": "blog/2022-04-11-monads/index.html",
    "title": "Why you should(n’t) care about Monads if you’re an R programmer",
    "section": "",
    "text": "Permanently moved."
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "Packages",
    "section": "",
    "text": "Simplifies the creation of reproducible data science environments using the ‘Nix’ package manager, as described in Dolstra (2006) &lt;ISBN 90-393-4130-3&gt;. The included ‘rix()’ function generates a complete description of the environment as a ‘default.nix’ file, which can then be built using ’Nix’. This results in project specific software environments with pinned versions of R, packages, linked system dependencies, and other tools. Additional helpers make it easy to run R code in ‘Nix’ software environments for testing and production."
  },
  {
    "objectID": "packages.html#rix-reproducible-data-science-environments-with-nix",
    "href": "packages.html#rix-reproducible-data-science-environments-with-nix",
    "title": "Packages",
    "section": "",
    "text": "Simplifies the creation of reproducible data science environments using the ‘Nix’ package manager, as described in Dolstra (2006) &lt;ISBN 90-393-4130-3&gt;. The included ‘rix()’ function generates a complete description of the environment as a ‘default.nix’ file, which can then be built using ’Nix’. This results in project specific software environments with pinned versions of R, packages, linked system dependencies, and other tools. Additional helpers make it easy to run R code in ‘Nix’ software environments for testing and production."
  },
  {
    "objectID": "packages.html#chronicler-add-logging-to-functions",
    "href": "packages.html#chronicler-add-logging-to-functions",
    "title": "Packages",
    "section": "chronicler: Add Logging to Functions",
    "text": "chronicler: Add Logging to Functions\n\n\nDecorate functions to make them return enhanced output. The enhanced output consists in an object of type ‘chronicle’ containing the result of the function applied to its arguments, as well as a log detailing when the function was run, what were its inputs, what were the errors (if the function failed to run) and other useful information. Tools to handle decorated functions are included, such as a forward pipe operator that makes chaining decorated functions possible."
  }
]