[
  {
    "objectID": "posts/2018-12-21-tidyverse_pi.html",
    "href": "posts/2018-12-21-tidyverse_pi.html",
    "title": "Using the tidyverse for more than data manipulation: estimating pi with Monte Carlo methods",
    "section": "",
    "text": "This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free here. This is taken from Chapter 5, which presents the {tidyverse} packages and how to use them to compute descriptive statistics and manipulate data. In the text below, I show how you can use the {tidyverse} functions and principles for the estimation of () using Monte Carlo simulation."
  },
  {
    "objectID": "posts/2018-12-21-tidyverse_pi.html#going-beyond-descriptive-statistics-and-data-manipulation",
    "href": "posts/2018-12-21-tidyverse_pi.html#going-beyond-descriptive-statistics-and-data-manipulation",
    "title": "Using the tidyverse for more than data manipulation: estimating pi with Monte Carlo methods",
    "section": "\nGoing beyond descriptive statistics and data manipulation\n",
    "text": "Going beyond descriptive statistics and data manipulation\n\n\nThe {tidyverse} collection of packages can do much more than simply data manipulation and descriptive statisics. You can use the principles we have covered and the functions you now know to do much more. For instance, you can use a few {tidyverse} functions to do Monte Carlo simulations, for example to estimate ().\n\n\nDraw the unit circle inside the unit square, the ratio of the area of the circle to the area of the square will be (/4). Then shot K arrows at the square; roughly (K*/4) should have fallen inside the circle. So if now you shoot N arrows at the square, and M fall inside the circle, you have the following relationship (M = N/4). You can thus compute () like so: (= 4M/N).\n\n\nThe more arrows N you throw at the square, the better approximation of () you’ll have. Let’s try to do this with a tidy Monte Carlo simulation. First, let’s randomly pick some points inside the unit square:\n\nlibrary(tidyverse)\nlibrary(brotools)\nn &lt;- 5000\n\nset.seed(2019)\npoints &lt;- tibble(\"x\" = runif(n), \"y\" = runif(n))\n\nNow, to know if a point is inside the unit circle, we need to check wether (x^2 + y^2 &lt; 1). Let’s add a new column to the points tibble, called inside equal to 1 if the point is inside the unit circle and 0 if not:\n\npoints &lt;- points %&gt;% \n    mutate(inside = map2_dbl(.x = x, .y = y, ~ifelse(.x**2 + .y**2 &lt; 1, 1, 0))) %&gt;% \n    rowid_to_column(\"N\")\n\nLet’s take a look at points:\n\npoints\n## # A tibble: 5,000 x 4\n##        N       x      y inside\n##    &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n##  1     1 0.770   0.984       0\n##  2     2 0.713   0.0107      1\n##  3     3 0.303   0.133       1\n##  4     4 0.618   0.0378      1\n##  5     5 0.0505  0.677       1\n##  6     6 0.0432  0.0846      1\n##  7     7 0.820   0.727       0\n##  8     8 0.00961 0.0758      1\n##  9     9 0.102   0.373       1\n## 10    10 0.609   0.676       1\n## # … with 4,990 more rows\n\nThe rowid_to_column() function, from the {tibble} package, adds a new column to the data frame with an id, going from 1 to the number of rows in the data frame. Now, I can compute the estimation of () at each row, by computing the cumulative sum of the 1’s in the inside column and dividing that by the current value of N column:\n\npoints &lt;- points %&gt;% \n    mutate(estimate = 4*cumsum(inside)/N)\n\ncumsum(inside) is the M from the formula. Now, we can finish by plotting the result:\n\nggplot(points) + \n    geom_line(aes(y = estimate, x = N), colour = \"#82518c\") + \n    geom_hline(yintercept = pi) +\n    theme_blog()\n\n\n\n\nIn Chapter 6, we are going to learn all about {ggplot2}.\n\n\nAs the number of tries grows, the estimation of () gets better.\n\n\nUsing a data frame as a structure to hold our simulated points and the results makes it very easy to avoid loops, and thus write code that is more concise and easier to follow. If you studied a quantitative field in u8niversity, you might have done a similar exercise at the time, very likely by defining a matrix to hold your points, and an empty vector to hold whether a particular point was inside the unit circle. Then you wrote a loop to compute whether a point was inside the unit circle, save this result in the before-defined empty vector and then compute the estimation of (). Again, I take this opportunity here to stress that there is nothing wrong with this approach per se, but R, with the {tidyverse} is better suited for a workflow where lists or data frames are the central objects and where the analyst operates over them with functional programming techniques."
  },
  {
    "objectID": "posts/2018-11-14-luxairport.html",
    "href": "posts/2018-11-14-luxairport.html",
    "title": "Easy time-series prediction with R: a tutorial with air traffic data from Lux Airport",
    "section": "",
    "text": "In this blog post, I will show you how you can quickly and easily forecast a univariate time series. I am going to use data from the EU Open Data Portal on air passenger transport. You can find the data here. I downloaded the data in the TSV format for Luxembourg Airport, but you could repeat the analysis for any airport.\n\n\nOnce you have the data, load some of the package we are going to need:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(forecast)\nlibrary(tsibble)\nlibrary(brotools)\n\nand define the following function:\n\nihs &lt;- function(x){\n    log(x + sqrt(x**2 + 1))\n}\n\nThis function, the inverse hyperbolic sine, is useful to transform data in a manner that is very close to logging it, but that allows for 0’s. The data from Eurostat is not complete for some reason, so there are some 0 sometimes. To avoid having to log 0, which in R yields -Inf, I use this transformation.\n\n\nNow, let’s load the data:\n\navia &lt;- read_tsv(\"avia_par_lu.tsv\")\n## Parsed with column specification:\n## cols(\n##   .default = col_character()\n## )\n## See spec(...) for full column specifications.\n\nLet’s take a look at the data:\n\nhead(avia)\n## # A tibble: 6 x 238\n##   `unit,tra_meas,… `2018Q1` `2018M03` `2018M02` `2018M01` `2017Q4` `2017Q3`\n##   &lt;chr&gt;            &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;   \n## 1 FLIGHT,CAF_PAS,… 511      172       161       178       502      475     \n## 2 FLIGHT,CAF_PAS,… :        :         :         :         :        :       \n## 3 FLIGHT,CAF_PAS,… :        :         :         :         399      306     \n## 4 FLIGHT,CAF_PAS,… 485      167       151       167       493      497     \n## 5 FLIGHT,CAF_PAS,… 834      293       267       274       790      728     \n## 6 FLIGHT,CAF_PAS,… :        :         :         :         :        :       \n## # … with 231 more variables: `2017Q2` &lt;chr&gt;, `2017Q1` &lt;chr&gt;,\n## #   `2017M12` &lt;chr&gt;, `2017M11` &lt;chr&gt;, `2017M10` &lt;chr&gt;, `2017M09` &lt;chr&gt;,\n## #   `2017M08` &lt;chr&gt;, `2017M07` &lt;chr&gt;, `2017M06` &lt;chr&gt;, `2017M05` &lt;chr&gt;,\n## #   `2017M04` &lt;chr&gt;, `2017M03` &lt;chr&gt;, `2017M02` &lt;chr&gt;, `2017M01` &lt;chr&gt;,\n## #   `2017` &lt;chr&gt;, `2016Q4` &lt;chr&gt;, `2016Q3` &lt;chr&gt;, `2016Q2` &lt;chr&gt;,\n## #   `2016Q1` &lt;chr&gt;, `2016M12` &lt;chr&gt;, `2016M11` &lt;chr&gt;, `2016M10` &lt;chr&gt;,\n## #   `2016M09` &lt;chr&gt;, `2016M08` &lt;chr&gt;, `2016M07` &lt;chr&gt;, `2016M06` &lt;chr&gt;,\n## #   `2016M05` &lt;chr&gt;, `2016M04` &lt;chr&gt;, `2016M03` &lt;chr&gt;, `2016M02` &lt;chr&gt;,\n## #   `2016M01` &lt;chr&gt;, `2016` &lt;chr&gt;, `2015Q4` &lt;chr&gt;, `2015Q3` &lt;chr&gt;,\n## #   `2015Q2` &lt;chr&gt;, `2015Q1` &lt;chr&gt;, `2015M12` &lt;chr&gt;, `2015M11` &lt;chr&gt;,\n## #   `2015M10` &lt;chr&gt;, `2015M09` &lt;chr&gt;, `2015M08` &lt;chr&gt;, `2015M07` &lt;chr&gt;,\n## #   `2015M06` &lt;chr&gt;, `2015M05` &lt;chr&gt;, `2015M04` &lt;chr&gt;, `2015M03` &lt;chr&gt;,\n## #   `2015M02` &lt;chr&gt;, `2015M01` &lt;chr&gt;, `2015` &lt;chr&gt;, `2014Q4` &lt;chr&gt;,\n## #   `2014Q3` &lt;chr&gt;, `2014Q2` &lt;chr&gt;, `2014Q1` &lt;chr&gt;, `2014M12` &lt;chr&gt;,\n## #   `2014M11` &lt;chr&gt;, `2014M10` &lt;chr&gt;, `2014M09` &lt;chr&gt;, `2014M08` &lt;chr&gt;,\n## #   `2014M07` &lt;chr&gt;, `2014M06` &lt;chr&gt;, `2014M05` &lt;chr&gt;, `2014M04` &lt;chr&gt;,\n## #   `2014M03` &lt;chr&gt;, `2014M02` &lt;chr&gt;, `2014M01` &lt;chr&gt;, `2014` &lt;chr&gt;,\n## #   `2013Q4` &lt;chr&gt;, `2013Q3` &lt;chr&gt;, `2013Q2` &lt;chr&gt;, `2013Q1` &lt;chr&gt;,\n## #   `2013M12` &lt;chr&gt;, `2013M11` &lt;chr&gt;, `2013M10` &lt;chr&gt;, `2013M09` &lt;chr&gt;,\n## #   `2013M08` &lt;chr&gt;, `2013M07` &lt;chr&gt;, `2013M06` &lt;chr&gt;, `2013M05` &lt;chr&gt;,\n## #   `2013M04` &lt;chr&gt;, `2013M03` &lt;chr&gt;, `2013M02` &lt;chr&gt;, `2013M01` &lt;chr&gt;,\n## #   `2013` &lt;chr&gt;, `2012Q4` &lt;chr&gt;, `2012Q3` &lt;chr&gt;, `2012Q2` &lt;chr&gt;,\n## #   `2012Q1` &lt;chr&gt;, `2012M12` &lt;chr&gt;, `2012M11` &lt;chr&gt;, `2012M10` &lt;chr&gt;,\n## #   `2012M09` &lt;chr&gt;, `2012M08` &lt;chr&gt;, `2012M07` &lt;chr&gt;, `2012M06` &lt;chr&gt;,\n## #   `2012M05` &lt;chr&gt;, `2012M04` &lt;chr&gt;, `2012M03` &lt;chr&gt;, `2012M02` &lt;chr&gt;,\n## #   `2012M01` &lt;chr&gt;, `2012` &lt;chr&gt;, …\n\nSo yeah, useless in that state. The first column actually is composed of 3 columns, merged together, and instead of having one column with the date, and another with the value, we have one column per date. Some cleaning is necessary before using this data.\n\n\nLet’s start with going from a wide to a long data set:\n\navia %&gt;%\n    select(\"unit,tra_meas,airp_pr\\\\time\", contains(\"20\")) %&gt;%\n    gather(date, passengers, -`unit,tra_meas,airp_pr\\\\time`)\n\nThe first line makes it possible to only select the columns that contain the string “20”, so selecting columns from 2000 onward. Then, using gather, I go from long to wide. The data looks like this now:\n\n## # A tibble: 117,070 x 3\n##    `unit,tra_meas,airp_pr\\\\time`  date   passengers\n##    &lt;chr&gt;                          &lt;chr&gt;  &lt;chr&gt;     \n##  1 FLIGHT,CAF_PAS,LU_ELLX_AT_LOWW 2018Q1 511       \n##  2 FLIGHT,CAF_PAS,LU_ELLX_BE_EBBR 2018Q1 :         \n##  3 FLIGHT,CAF_PAS,LU_ELLX_CH_LSGG 2018Q1 :         \n##  4 FLIGHT,CAF_PAS,LU_ELLX_CH_LSZH 2018Q1 485       \n##  5 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDF 2018Q1 834       \n##  6 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDI 2018Q1 :         \n##  7 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDM 2018Q1 1095      \n##  8 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDR 2018Q1 :         \n##  9 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDT 2018Q1 :         \n## 10 FLIGHT,CAF_PAS,LU_ELLX_DK_EKCH 2018Q1 :         \n## # … with 117,060 more rows\n\nNow, let’s separate the first column into 3 columns:\n\navia %&gt;%\n    select(\"unit,tra_meas,airp_pr\\\\time\", contains(\"20\")) %&gt;%\n    gather(date, passengers, -`unit,tra_meas,airp_pr\\\\time`) %&gt;%\n     separate(col = `unit,tra_meas,airp_pr\\\\time`, into = c(\"unit\", \"tra_meas\", \"air_pr\\\\time\"), sep = \",\")\n\nThis separates the first column into 3 new columns, “unit”, “tra_meas” and “air_pr”. This step is not necessary for the rest of the analysis, but might as well do it. The data looks like this now:\n\n## # A tibble: 117,070 x 5\n##    unit   tra_meas `air_pr\\\\time`  date   passengers\n##    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt;     \n##  1 FLIGHT CAF_PAS  LU_ELLX_AT_LOWW 2018Q1 511       \n##  2 FLIGHT CAF_PAS  LU_ELLX_BE_EBBR 2018Q1 :         \n##  3 FLIGHT CAF_PAS  LU_ELLX_CH_LSGG 2018Q1 :         \n##  4 FLIGHT CAF_PAS  LU_ELLX_CH_LSZH 2018Q1 485       \n##  5 FLIGHT CAF_PAS  LU_ELLX_DE_EDDF 2018Q1 834       \n##  6 FLIGHT CAF_PAS  LU_ELLX_DE_EDDI 2018Q1 :         \n##  7 FLIGHT CAF_PAS  LU_ELLX_DE_EDDM 2018Q1 1095      \n##  8 FLIGHT CAF_PAS  LU_ELLX_DE_EDDR 2018Q1 :         \n##  9 FLIGHT CAF_PAS  LU_ELLX_DE_EDDT 2018Q1 :         \n## 10 FLIGHT CAF_PAS  LU_ELLX_DK_EKCH 2018Q1 :         \n## # … with 117,060 more rows\n\nThe next steps are simple renamings. I have copy-pasted the information from the Eurostat page where you can view the data online. If you click here:\n\n\n\n\n\nyou will be able to select the variables you want displayed in the table, as well as the dictionary of the variables. I simply copy pasted it and recoded the variables. You can take a look at the whole cleaning workflow by clicking “Click to expand” below:\n\n\n\n\nClick here to take a look at the whole cleaning workflow\n\n\navia_clean &lt;- avia %&gt;%\n    select(\"unit,tra_meas,airp_pr\\\\time\", contains(\"20\")) %&gt;%\n    gather(date, passengers, -`unit,tra_meas,airp_pr\\\\time`) %&gt;%\n    separate(col = `unit,tra_meas,airp_pr\\\\time`, into = c(\"unit\", \"tra_meas\", \"air_pr\\\\time\"), sep = \",\") %&gt;%\n    mutate(tra_meas = fct_recode(tra_meas,\n         `Passengers on board` = \"PAS_BRD\",\n         `Passengers on board (arrivals)` = \"PAS_BRD_ARR\",\n         `Passengers on board (departures)` = \"PAS_BRD_DEP\",\n         `Passengers carried` = \"PAS_CRD\",\n         `Passengers carried (arrival)` = \"PAS_CRD_ARR\",\n         `Passengers carried (departures)` = \"PAS_CRD_DEP\",\n         `Passengers seats available` = \"ST_PAS\",\n         `Passengers seats available (arrivals)` = \"ST_PAS_ARR\",\n         `Passengers seats available (departures)` = \"ST_PAS_DEP\",\n         `Commercial passenger air flights` = \"CAF_PAS\",\n         `Commercial passenger air flights (arrivals)` = \"CAF_PAS_ARR\",\n         `Commercial passenger air flights (departures)` = \"CAF_PAS_DEP\")) %&gt;%\n    mutate(unit = fct_recode(unit,\n                             Passenger = \"PAS\",\n                             Flight = \"FLIGHT\",\n                             `Seats and berths` = \"SEAT\")) %&gt;%\n    mutate(destination = fct_recode(`air_pr\\\\time`,\n                                     `WIEN-SCHWECHAT` = \"LU_ELLX_AT_LOWW\",\n                                     `BRUSSELS` = \"LU_ELLX_BE_EBBR\",\n                                     `GENEVA` = \"LU_ELLX_CH_LSGG\",\n                                     `ZURICH` = \"LU_ELLX_CH_LSZH\",\n                                     `FRANKFURT/MAIN` = \"LU_ELLX_DE_EDDF\",\n                                     `HAMBURG` = \"LU_ELLX_DE_EDDH\",\n                                     `BERLIN-TEMPELHOF` = \"LU_ELLX_DE_EDDI\",\n                                     `MUENCHEN` = \"LU_ELLX_DE_EDDM\",\n                                     `SAARBRUECKEN` = \"LU_ELLX_DE_EDDR\",\n                                     `BERLIN-TEGEL` = \"LU_ELLX_DE_EDDT\",\n                                     `KOBENHAVN/KASTRUP` = \"LU_ELLX_DK_EKCH\",\n                                     `HURGHADA / INTL` = \"LU_ELLX_EG_HEGN\",\n                                     `IRAKLION/NIKOS KAZANTZAKIS` = \"LU_ELLX_EL_LGIR\",\n                                     `FUERTEVENTURA` = \"LU_ELLX_ES_GCFV\",\n                                     `GRAN CANARIA` = \"LU_ELLX_ES_GCLP\",\n                                     `LANZAROTE` = \"LU_ELLX_ES_GCRR\",\n                                     `TENERIFE SUR/REINA SOFIA` = \"LU_ELLX_ES_GCTS\",\n                                     `BARCELONA/EL PRAT` = \"LU_ELLX_ES_LEBL\",\n                                     `ADOLFO SUAREZ MADRID-BARAJAS` = \"LU_ELLX_ES_LEMD\",\n                                     `MALAGA/COSTA DEL SOL` = \"LU_ELLX_ES_LEMG\",\n                                     `PALMA DE MALLORCA` = \"LU_ELLX_ES_LEPA\",\n                                     `SYSTEM - PARIS` = \"LU_ELLX_FR_LF90\",\n                                     `NICE-COTE D'AZUR` = \"LU_ELLX_FR_LFMN\",\n                                     `PARIS-CHARLES DE GAULLE` = \"LU_ELLX_FR_LFPG\",\n                                     `STRASBOURG-ENTZHEIM` = \"LU_ELLX_FR_LFST\",\n                                     `KEFLAVIK` = \"LU_ELLX_IS_BIKF\",\n                                     `MILANO/MALPENSA` = \"LU_ELLX_IT_LIMC\",\n                                     `BERGAMO/ORIO AL SERIO` = \"LU_ELLX_IT_LIME\",\n                                     `ROMA/FIUMICINO` = \"LU_ELLX_IT_LIRF\",\n                                     `AGADIR/AL MASSIRA` = \"LU_ELLX_MA_GMAD\",\n                                     `AMSTERDAM/SCHIPHOL` = \"LU_ELLX_NL_EHAM\",\n                                     `WARSZAWA/CHOPINA` = \"LU_ELLX_PL_EPWA\",\n                                     `PORTO` = \"LU_ELLX_PT_LPPR\",\n                                     `LISBOA` = \"LU_ELLX_PT_LPPT\",\n                                     `STOCKHOLM/ARLANDA` = \"LU_ELLX_SE_ESSA\",\n                                     `MONASTIR/HABIB BOURGUIBA` = \"LU_ELLX_TN_DTMB\",\n                                     `ENFIDHA-HAMMAMET INTERNATIONAL` = \"LU_ELLX_TN_DTNH\",\n                                     `ENFIDHA ZINE EL ABIDINE BEN ALI` = \"LU_ELLX_TN_DTNZ\",\n                                     `DJERBA/ZARZIS` = \"LU_ELLX_TN_DTTJ\",\n                                     `ANTALYA (MIL-CIV)` = \"LU_ELLX_TR_LTAI\",\n                                     `ISTANBUL/ATATURK` = \"LU_ELLX_TR_LTBA\",\n                                     `SYSTEM - LONDON` = \"LU_ELLX_UK_EG90\",\n                                     `MANCHESTER` = \"LU_ELLX_UK_EGCC\",\n                                     `LONDON GATWICK` = \"LU_ELLX_UK_EGKK\",\n                                     `LONDON/CITY` = \"LU_ELLX_UK_EGLC\",\n                                     `LONDON HEATHROW` = \"LU_ELLX_UK_EGLL\",\n                                     `LONDON STANSTED` = \"LU_ELLX_UK_EGSS\",\n                                     `NEWARK LIBERTY INTERNATIONAL, NJ.` = \"LU_ELLX_US_KEWR\",\n                                     `O.R TAMBO INTERNATIONAL` = \"LU_ELLX_ZA_FAJS\")) %&gt;%\n    mutate(passengers = as.numeric(passengers)) %&gt;%\n    select(unit, tra_meas, destination, date, passengers)\n## Warning: NAs introduced by coercion\n\n\nThere is quarterly data and monthly data. Let’s separate the two:\n\navia_clean_quarterly &lt;- avia_clean %&gt;%\n    filter(tra_meas == \"Passengers on board (arrivals)\",\n           !is.na(passengers)) %&gt;%\n    filter(str_detect(date, \"Q\")) %&gt;%\n    mutate(date = yq(date))\n\nIn the “date” column, I detect the observations with “Q” in their name, indicating that it is quarterly data. I do the same for monthly data, but I have to add the string “01” to the dates. This transforms a date that looks like this “2018M1” to this “2018M101”. “2018M101” can then be converted into a date by using the ymd() function from lubridate. yq() was used for the quarterly data.\n\navia_clean_monthly &lt;- avia_clean %&gt;%\n    filter(tra_meas == \"Passengers on board (arrivals)\",\n           !is.na(passengers)) %&gt;%\n    filter(str_detect(date, \"M\")) %&gt;%\n    mutate(date = paste0(date, \"01\")) %&gt;%\n    mutate(date = ymd(date)) %&gt;%\n    select(destination, date, passengers)\n\nTime for some plots. Let’s start with the raw data:\n\navia_clean_monthly %&gt;%\n    group_by(date) %&gt;%\n    summarise(total = sum(passengers)) %&gt;%\n    ggplot() +\n    ggtitle(\"Raw data\") +\n    geom_line(aes(y = total, x = date), colour = \"#82518c\") +\n    scale_x_date(date_breaks = \"1 year\", date_labels = \"%m-%Y\") + \n    theme_blog()\n\n\n\n\nAnd now with the logged data (or rather, the data transformed using the inverted hyperbolic sine transformation):\n\navia_clean_monthly %&gt;%\n    group_by(date) %&gt;%\n    summarise(total = sum(passengers)) %&gt;%\n    mutate(total_ihs = ihs(total)) %&gt;%\n    ggplot() +\n    ggtitle(\"Logged data\") +\n    geom_line(aes(y = total_ihs, x = date), colour = \"#82518c\") +\n    scale_x_date(date_breaks = \"1 year\", date_labels = \"%m-%Y\") + \n    theme_blog()\n\n\n\n\nWe clearly see a seasonal pattern in the data. There is also an upward trend. We will have to deal with these two problems if we want to do some forecasting. For this, let’s limit ourselves to data from before 2015, and convert the “passengers” column from the data to a time series object, using the ts() function:\n\navia_clean_train &lt;- avia_clean_monthly %&gt;%\n    select(date, passengers) %&gt;%\n    filter(year(date) &lt; 2015) %&gt;%\n    group_by(date) %&gt;%\n    summarise(total_passengers = sum(passengers)) %&gt;%\n    pull(total_passengers) %&gt;%\n    ts(., frequency = 12, start = c(2005, 1))\n\nWe will try to pseudo-forecast the data from 2015 to the last point available, March 2018. First, let’s tranform the data:\n\nlogged_data &lt;- ihs(avia_clean_train)\n\nTaking the log, or ihs of the data deals with stabilizing the variance of the time series.\n\n\nThere might also be a need to difference the data. Computing the differences between consecutive observations makes the time-series stationary. This will be taken care of by the auto.arima() function, if needed. The auto.arima() function returns the best ARIMA model according to different statistical criterions, such as the AIC, AICc or BIC.\n\n(model_fit &lt;- auto.arima(logged_data))\n## Series: logged_data \n## ARIMA(2,1,1)(2,1,0)[12] \n## \n## Coefficients:\n##           ar1      ar2      ma1     sar1     sar2\n##       -0.4061  -0.2431  -0.3562  -0.5590  -0.3282\n## s.e.   0.2003   0.1432   0.1994   0.0911   0.0871\n## \n## sigma^2 estimated as 0.004503:  log likelihood=137.11\n## AIC=-262.21   AICc=-261.37   BIC=-246.17\n\nauto.arima() found that the best model would be an (ARIMA(2, 1, 1)(2, 1, 0)_{12}). This is an seasonal autoregressive model, with p = 2, d = 1, q = 1, P = 2 and D = 1.\n\nmodel_forecast &lt;- forecast(model_fit, h = 39)\n\nI can now forecast the model for the next 39 months (which correspond to the data available).\n\n\nTo plot the forecast, one could do a simple call to the plot function. But the resulting plot is not very aesthetic. To plot my own, I have to grab the data that was forecast, and do some munging again:\n\npoint_estimate &lt;- model_forecast$mean %&gt;%\n    as_tsibble() %&gt;%\n    rename(point_estimate = value,\n           date = index)\n\nupper &lt;- model_forecast$upper %&gt;%\n    as_tsibble() %&gt;%\n    spread(key, value) %&gt;%\n    rename(date = index,\n           upper80 = `80%`,\n           upper95 = `95%`)\n\nlower &lt;- model_forecast$lower %&gt;%\n    as_tsibble() %&gt;%\n    spread(key, value) %&gt;%\n    rename(date = index,\n           lower80 = `80%`,\n           lower95 = `95%`)\n\nestimated_data &lt;- reduce(list(point_estimate, upper, lower), full_join, by = \"date\")\n\nas_tsibble() is a function from the {tsibble} package that converts objects that are time-series aware to time-aware tibbles. If you are not familiar with ts_tibble(), I urge you to run the above lines one by one, and especially to compare as_tsibble() with the standard as_tibble() from the {tibble} package.\n\n\nThis is how estimated_data looks:\n\nhead(estimated_data)\n## # A tsibble: 6 x 6 [1M]\n##       date point_estimate upper80 upper95 lower80 lower95\n##      &lt;mth&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n## 1 2015 Jan           11.9    12.0    12.1    11.8    11.8\n## 2 2015 Feb           11.9    12.0    12.0    11.8    11.7\n## 3 2015 Mar           12.1    12.2    12.3    12.0    12.0\n## 4 2015 Apr           12.2    12.3    12.4    12.1    12.1\n## 5 2015 May           12.3    12.4    12.4    12.2    12.1\n## 6 2015 Jun           12.3    12.4    12.5    12.2    12.1\n\nWe can now plot the data, with the forecast, and with the 95% confidence interval:\n\navia_clean_monthly %&gt;%\n    group_by(date) %&gt;%\n    summarise(total = sum(passengers)) %&gt;%\n    mutate(total_ihs = ihs(total)) %&gt;%\n    ggplot() +\n    ggtitle(\"Logged data\") +\n    geom_line(aes(y = total_ihs, x = date), colour = \"#82518c\") +\n    scale_x_date(date_breaks = \"1 year\", date_labels = \"%m-%Y\") +\n    geom_ribbon(data = estimated_data, aes(x = date, ymin = lower95, ymax = upper95), fill = \"#666018\", alpha = 0.2) +\n    geom_line(data = estimated_data, aes(x = date, y = point_estimate), linetype = 2, colour = \"#8e9d98\") +\n    theme_blog()\n\n\n\n\nThe pseudo-forecast (the dashed line) is not very far from the truth, only overestimating the seasonal peaks, but the true line is within the 95% confidence interval, which is good!"
  },
  {
    "objectID": "posts/2017-11-14-peace_r.html",
    "href": "posts/2017-11-14-peace_r.html",
    "title": "Peace of mind with purrr",
    "section": "",
    "text": "I think what I enjoy the most about functional programming is the peace of mind that comes with it. With functional programming, there’s a lot of stuff you don’t need to think about. You can write functions that are general enough so that they solve a variety of problems. For example, imagine for a second that R does not have the sum() function anymore. If you want to compute the sum of, say, the first 100 integers, you could write a loop that would do that for you:\n\nnumbers = 0\n\nfor (i in 1:100){\n  numbers = numbers + i\n}\n\nprint(numbers)\n\n[1] 5050\n\n\nThe problem with this approach, is that you cannot reuse any of the code there, even if you put it inside a function. For instance, what if you want to merge 4 datasets together? You would need something like this:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata(mtcars)\n\nmtcars1 = mtcars %&gt;%\n  mutate(id = \"1\")\n\nmtcars2 = mtcars %&gt;%\n  mutate(id = \"2\")\n\nmtcars3 = mtcars %&gt;%\n  mutate(id = \"3\")\n\nmtcars4 = mtcars %&gt;%\n  mutate(id = \"4\")\n\ndatasets = list(mtcars1, mtcars2, mtcars3, mtcars4)\n\ntemp = datasets[[1]]\n\nfor(i in 1:3){\n  temp = full_join(temp, datasets[[i+1]])\n}\n\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\n\n\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\n\nglimpse(temp)\n\nRows: 128\nColumns: 12\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n$ id   &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n\n\nOf course, the logic is very similar as before, but you need to think carefully about the structure holding your elements (which can be numbers, datasets, characters, etc…) as well as be careful about indexing correctly… and depending on the type of objects you are working on, you might need to tweak the code further.\nHow would a functional programming approach make this easier? Of course, you could use purrr::reduce() to solve these problems. However, since I assumed that sum() does not exist, I will also assume that purrr::reduce() does not exist either and write my own, clumsy implementation. Here’s the code:\n\nmy_reduce = function(a_list, a_func, init = NULL, ...){\n\n  if(is.null(init)){\n    init = `[[`(a_list, 1)\n    a_list = tail(a_list, -1)\n  }\n\n  car = `[[`(a_list, 1)\n  cdr = tail(a_list, -1)\n  init = a_func(init, car, ...)\n\n  if(length(cdr) != 0){\n    my_reduce(cdr, a_func, init, ...)\n  }\n  else {\n    init\n  }\n}\n\nThis can look much more complicated than before, but the idea is quite simple; if you know about recursive functions (recursive functions are functions that call themselves). I won’t explain how the function works, because it is not the main point of the article (but if you’re curious, I encourage you to play around with it). The point is that now, I can do the following:\n\nmy_reduce(list(1,2,3,4,5), `+`)\n\n[1] 15\n\nmy_reduce(datasets, full_join) %&gt;% glimpse\n\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\n\n\nRows: 128\nColumns: 12\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n$ id   &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n\n\nBut since my_reduce() is very general, I can even do this:\n\nmy_reduce(list(1, 2, 3, 4, \"5\"), paste)\n\n[1] \"1 2 3 4 5\"\n\n\nOf course, paste() is vectorized, so you could just as well do paste(1, 2, 3, 4, 5), but again, I want to insist on the fact that writing functions, even if they look a bit complicated, can save you a huge amount of time in the long run.\nBecause I know that my function is quite general, I can be confident that it will work in a lot of different situations; as long as the a_func argument is a binary operator that combines the elements inside a_list, it’s going to work. And I don’t need to think about indexing, about having temporary variables or thinking about the structure that will hold my results."
  },
  {
    "objectID": "posts/2018-11-21-lux_castle.html",
    "href": "posts/2018-11-21-lux_castle.html",
    "title": "The best way to visit Luxembourguish castles is doing data science + combinatorial optimization",
    "section": "",
    "text": "Inspired by David Schoch’s blog post, Traveling Beerdrinker Problem. Check out his blog, he has some amazing posts!"
  },
  {
    "objectID": "posts/2018-11-21-lux_castle.html#introduction",
    "href": "posts/2018-11-21-lux_castle.html#introduction",
    "title": "The best way to visit Luxembourguish castles is doing data science + combinatorial optimization",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nLuxembourg, as any proper European country, is full of castles. According to Wikipedia,\n\n\n“By some optimistic estimates, there are as many as 130 castles in Luxembourg but more realistically there are probably just over a hundred, although many of these could be considered large residences or manor houses rather than castles”.\n\n\nI see the editors are probably German or French, calling our castles manor houses! They only say that because Luxembourg is small, so our castles must be small too, right?\n\n\nBanter aside, with that many castles, what is the best way to visit them all? And by best way I mean shortest way. This is a classical Travelling salesman problem. To solve this, I need the following elements:\n\n\n\nA list of castles to visit, with their coordinates\n\n\nThe distances between these castles to each other\n\n\nA program to solve the TSP\n\n\n\nLet’s start by loading some packages:\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(rvest)\nlibrary(curl)\nlibrary(brotools)\nlibrary(RJSONIO)\nlibrary(TSP)\nlibrary(ggimage)\n\nFirst step; scrape the data."
  },
  {
    "objectID": "posts/2018-11-21-lux_castle.html#scraping-the-data-thats-the-data-science-part",
    "href": "posts/2018-11-21-lux_castle.html#scraping-the-data-thats-the-data-science-part",
    "title": "The best way to visit Luxembourguish castles is doing data science + combinatorial optimization",
    "section": "\nScraping the data (that’s the data science part)\n",
    "text": "Scraping the data (that’s the data science part)\n\n\nLet’s start by having a list of castles. For this, I go to the French Wikipedia page of Luxembourguish castles.\n\n\nThe Luxembourguish page is more exhaustive, but the names are in Luxembourguish, and I doubt that OpenStreetMap, which I’ll use to get the coordinates, understands Luxembourguish.\n\n\nThis list has around 50 castles, a reasonable amount of castles. Scraping the table is quite easy:\n\npage &lt;- read_html(\"https://fr.wikipedia.org/wiki/Liste_de_ch%C3%A2teaux_luxembourgeois\")\n\ncastles &lt;- page %&gt;%\n    html_node(\".wikitable\") %&gt;%\n    html_table(fill = TRUE) %&gt;%\n    select(Nom, Localité) %&gt;%\n    mutate(query = paste0(Nom, \", \", Localité))\n\nI also add a query column which concatenates the name of the castle (“Nom”) to where it is found (“Localité”). The query should be a better choice that simply the castle name to get the coordinates.\n\n\nNow, I need to add the coordinates to this data frame. For this, I use a function I found online that gets the coordinates from OpenStreetMap:\n\n## geocoding function using OSM Nominatim API\n## details: http://wiki.openstreetmap.org/wiki/Nominatim\n## made by: D.Kisler\n\n#https://datascienceplus.com/osm-nominatim-with-r-getting-locations-geo-coordinates-by-its-address/\n\nnominatim_osm &lt;- function(address = NULL){\n    if(suppressWarnings(is.null(address)))\n        return(data.frame())\n    tryCatch(\n        d &lt;- jsonlite::fromJSON(\n            gsub('\\\\@addr\\\\@', gsub('\\\\s+', '\\\\%20', address),\n                 'http://nominatim.openstreetmap.org/search/@addr@?format=json&addressdetails=0&limit=1')\n        ), error = function(c) return(data.frame())\n    )\n    if(length(d) == 0) return(data.frame())\n    return(data.frame(lon = as.numeric(d$lon), lat = as.numeric(d$lat)))\n}\n\nI can now easily add the coordinates by mapping the nominatim_osm() function to the query column I built before:\n\ncastles_osm &lt;- castles %&gt;%\n    mutate(geolocation = map(query, nominatim_osm))\n\nLet’s take a look at castles_osm:\n\nhead(castles_osm)\n##                            Nom    Localité\n## 1         Château d'Ansembourg  Ansembourg\n## 2 Nouveau Château d'Ansembourg  Ansembourg\n## 3             Château d'Aspelt      Aspelt\n## 4          Château de Beaufort    Beaufort\n## 5            Château de Beggen Dommeldange\n## 6       Château de Colmar-Berg Colmar-Berg\n##                                      query         geolocation\n## 1         Château d'Ansembourg, Ansembourg 6.046748, 49.700693\n## 2 Nouveau Château d'Ansembourg, Ansembourg   6.04760, 49.70085\n## 3                 Château d'Aspelt, Aspelt 6.222653, 49.524822\n## 4            Château de Beaufort, Beaufort 2.757293, 43.297466\n## 5           Château de Beggen, Dommeldange 6.137765, 49.643383\n## 6      Château de Colmar-Berg, Colmar-Berg 6.087944, 49.814687\n\nI now clean the data. There were several mistakes or castles that were not found, which I added manually. I did not notice these mistakes immediately, but when I computed the distances matrix I notices several inconsistencies; 0’s in positions other than the diagonal, as well as NAs. So I went back to the raw data and corrected what was wrong, this time by looking at Google Maps. Thankfully there were not that many mistakes. Below the whole workflow:\n\n# Little helper function to clean the lon and lat columns\nextract_numbers &lt;- function(string){\n    str_extract_all(string, \"\\\\d+\", simplify = TRUE) %&gt;%\n        paste0(collapse = \".\")\n}\n\ncastles &lt;- castles_osm %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Château de Wintrange\", \"6.3517223, 49.5021975\", geolocation)) %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Septfontaines, Rollingergrund\", \"6.1028634, 49.6257147\", geolocation)) %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Château de Septfontaines\", \"5.9617443, 49.7006292\", geolocation)) %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Château de Senningen\", \"6.2342581, 49.6464632\", geolocation)) %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Château de Schauwenburg\", \"6.0478341, 49.6110245\", geolocation)) %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Château de Schuttbourg\", \"5.8980951, 49.7878706\", geolocation)) %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Château de Meysembourg\", \"6.1864882, 49.7704348\", geolocation)) %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Château de Mamer\", \"6.0232432, 49.6262397\", geolocation)) %&gt;%\n    mutate(geolocation = \n               ifelse(Nom == \"Château de Born\", \"6.5125214, 49.7611168\", geolocation)) %&gt;%\n    # Found chateau de Betzdorf in Germany, not Luxembourg:\n    mutate(geolocation = ifelse(Nom == \"Château Betzdorf\", \"6.330278, 49.694167\", geolocation)) %&gt;%\n    # Found château de Clemency in France, not Luxembourg:\n    mutate(geolocation = ifelse(Nom == \"Château de Clemency\", \"5.874167, 49.598056\", geolocation)) %&gt;%\n    separate(geolocation, into = c(\"lon\", \"lat\"), sep = \",\") %&gt;%\n    filter(!is.na(lat)) %&gt;%\n    mutate(lon = map(lon, extract_numbers)) %&gt;%\n    mutate(lat = map(lat, extract_numbers)) %&gt;%\n    # Château de Beaufort found is in southern France, not the one in lux\n    # Château de Dudelange is wrong (same as Bettembourg)\n    # Château de Pétange is wrong (same as Differdange)\n    # Château d'Urspelt is wrong (same as Clervaux)\n    # Château d'Hesperange is wrong (same as Palais Grand-Ducal)\n    mutate(lon = ifelse(Nom == \"Château de Beaufort\", \"6.2865176\", lon),\n           lat = ifelse(Nom == \"Château de Beaufort\", \"49.8335306\", lat)) %&gt;%\n    mutate(lon = ifelse(Nom == \"Château Dudelange\", \"6.0578438\", lon),\n           lat = ifelse(Nom == \"Château Dudelange\", \"49.4905049\", lat)) %&gt;%\n    mutate(lon = ifelse(Nom == \"Château de Pétange\", \"6.105703\", lon),\n           lat = ifelse(Nom == \"Château de Pétange\", \"49.7704746\", lat)) %&gt;%\n    mutate(lon = ifelse(Nom == \"Château d' Urspelt\", \"6.043375\", lon),\n           lat = ifelse(Nom == \"Château d' Urspelt\", \"50.075342\", lat)) %&gt;%\n    mutate(lon = ifelse(Nom == \"Château d'Hesperange\", \"6.1524302\", lon),\n           lat = ifelse(Nom == \"Château d'Hesperange\", \"49.573071\", lat)) %&gt;%\n    mutate(latlon = paste0(lat, \",\", lon)) %&gt;%\n    mutate(lon = as.numeric(lon), lat = as.numeric(lat))\n\nIn the end, I have 48 castles, 2 of them were not found neither by OpenStreetMap nor Google Maps.\n\n\nNow I can get the distances matrix. For this, I opened an account at Graphhopper and used their Matrix API. When you open a free account, you get a standard account for free for two weeks, which was perfect for this little exercise.\n\n\nTo use the Matrix API you can make a call with curl from your terminal, like this:\n\ncurl \"https://graphhopper.com/api/1/matrix?point=49.932707,11.588051&point=50.241935,10.747375&point=50.118817,11.983337&type=json&vehicle=car&debug=true&out_array=weights&out_array=times&out_array=distances&key=[YOUR_KEY]\"\n\nTo use this from R, I use the {curl} package and the curl_download() function to download and write the output to disk.\n\n\nI built the url like this. First, the “points” part:\n\npoints &lt;- paste(castles$latlon, collapse = \"&point=\")\n\n\n\nClick if you want to see the “points” string\n\n\npoints\n## [1] \"49.70069265,6.04674779400653&point=49.7008533,6.04759957386294&point=49.5248216,6.2226525964455&point=49.8335306,6.2865176&point=49.64338295,6.1377647435619&point=49.8146867,6.08794389490417&point=49.5749356,5.9841033&point=49.5173197,6.09641390513718&point=49.8760687,6.22027097982788&point=49.694167,6.330278&point=49.7611168,6.5125214&point=49.70256665,6.21740997690437&point=49.905581,6.07950107769784&point=49.9127745,6.13764166375989&point=49.598056,5.874167&point=50.0544533,6.03028463135369&point=49.75943095,5.82586812555896&point=49.52132545,5.88917535225117&point=49.6345518,6.1386377&point=49.4905049,6.0578438&point=49.8600716,6.11163732377525&point=49.9110418,5.93440053120085&point=49.7475976,6.18681116161273&point=49.61092115,6.13288873913352&point=49.573071,6.1524302&point=49.71207855,6.05156617599082&point=49.6694157,5.9496767&point=49.7704143,6.18888954785334&point=49.6262397,6.0232432&point=49.7478579,6.10315847283333&point=49.7704348,6.1864882&point=49.6328906,6.25941956000154&point=49.7704746,6.105703&point=49.54325715,5.9262570638974&point=49.470114,6.3658507&point=49.719675,6.09334070925783&point=49.7878706,5.8980951&point=49.6110245,6.0478341&point=49.6464632,6.2342581&point=49.7006292,5.9617443&point=49.6257147,6.1028634&point=49.556964,6.380786&point=50.075342,6.043375&point=49.7682266,5.9803414&point=49.9348908,6.20279648757301&point=49.6604088,6.1337864&point=49.9664662,5.93854270968922&point=49.5021975,6.3517223\"\n\n\nThen, I added my key, and pasted these elements together to form the correct url:\n\nmy_key &lt;- \"my_key_was_here\"\n\nurl &lt;- paste0(\"https://graphhopper.com/api/1/matrix?point=\", points, \"&type=json&vehicle=car&debug=true&out_array=weights&out_array=times&out_array=distances&key=\", my_key)\n\nThen, I get the matrix like this:\n\ncastles_dist &lt;- \"distances_graphhopper.json\"\ncurl_download(url, castles_dist)\n\nLet’s take a look at the object:\n\ndistances &lt;- castles_dist$distances\n\n\n\nClick if you want to see the distance object\n\n\ndistances\n## [[1]]\n##  [1]     0    48 46364 38416 16619 20387 19617 31990 31423 46587 60894\n## [12] 19171 36961 30701 25734 52929 22843 42618 18138 40015 24860 39395\n## [23] 17163 18938 28107  2570 10882 16888 12302  9350 16599 32025 14369\n## [34] 40780 56004  6069 17602 16112 31552  8180 14523 49431 53199 13354\n## [45] 43769 15868 46237 53617\n## \n## [[2]]\n##  [1]    48     0 46412 38464 16667 20435 19665 32038 31471 46635 60942\n## [12] 19219 37009 30749 25781 52977 22890 42665 18186 40063 24908 39443\n## [23] 17211 18986 28155  2618 10930 16936 12350  9398 16647 32073 14417\n## [34] 40828 56052  6116 17650 16160 31599  8228 14571 49478 53247 13402\n## [45] 43817 15916 46285 53665\n## \n## [[3]]\n##  [1] 46900 46947     0 48698 30281 45548 30424 17056 56584 31187 52215\n## [12] 28799 62122 55862 39130 78090 66375 33585 23961 21009 50021 64556\n## [23] 35740 25853 10852 49283 43218 43052 37317 36283 42763 17250 39530\n## [34] 31748 14513 33919 60798 31885 22872 44629 37023 14605 78360 44602\n## [45] 68930 26320 71398 12126\n## \n## [[4]]\n##  [1] 38214 38261 48754     0 34949 23582 55661 48848 11274 29579 26880\n## [12] 25631 32853 20633 67818 46577 47882 65319 33355 58499 26540 40460\n## [23] 24359 38061 43577 37674 61661 21704 55760 29822 25030 36698 27956\n## [34] 63482 72862 32945 42596 50328 34302 42495 38740 46782 46847 35141\n## [45] 20752 33520 47302 56789\n## \n## [[5]]\n##  [1] 16494 16541 25311 35192     0 25375 19477 16687 36411 20939 42124\n## [12] 13107 41949 35689 27116 57917 44116 30670  1432 26337 29848 44383\n## [23] 18673  5767 11224 18877 20958 23922 15058 16110 23633 13255 19357\n## [34] 28833 40700 15310 30392 12024 12782 20050  7178 30661 58187 24429\n## [45] 48757  2511 51225 33346\n## \n## [[6]]\n##  [1] 18468 18516 43459 23632 29633     0 33496 43553 15352 45965 60272\n## [12] 23583 20890 14630 39612 36858 27623 60024 28268 53203  8789 21614\n## [23] 17217 32765 38282 17929 29164 13853 27119  8892 15798 31403  7026\n## [34] 58187 67567 13199 22337 27919 30929 22750 26330 48809 37128 14882\n## [45] 27698 21128 28456 51494\n## \n## [[7]]\n##  [1] 19645 19693 30022 55860 21434 35353     0 17740 46389 45070 59377\n## [12] 35962 51927 45668 10941 67895 36650 11975 16038 16675 39826 49570\n## [23] 42902 13747 19018 22029 13093 32857  7837 24321 32568 30508 29335\n## [34]  8659 39662 20998 33544  9053 30034 17958 19337 40306 68165 29296\n## [45] 58736 20683 59099 37275\n## \n## [[8]]\n##  [1] 33194 33242 17113 48244 16576 45094 16196     0 56130 37454 51761\n## [12] 28345 61668 55408 26667 77635 52670 21122 15199  8546 49567 64102\n## [23] 35286 12148 11167 35578 29512 42597 23612 35829 42308 22891 39076\n## [34] 19285 26753 30159 47093 12671 22418 30923 23317 27397 77906 44148\n## [45] 68476 25866 70944 24366\n## \n## [[9]]\n##  [1] 34049 34097 59040 11039 45215 18025 49077 59134     0 35131 34289\n## [12] 25588 18148  8956 55193 34944 47717 75605 43849 68785 11835 33390\n## [23] 19644 48347 53863 33510 44745 17010 42701 26274 22029 46984 22791\n## [34] 73768 83148 28780 42477 43501 46511 38331 41912 64390 35215 29033\n## [45] 11504 36710 37265 67075\n## \n## [[10]]\n##  [1] 40768 40815 31200 29561 26887 47108 44877 38064 35204     0 24550\n## [12] 11501 46805 40546 57034 62773 59493 54535 25521 47714 51581 66116\n## [23] 18215 27276 32792 40228 50876 23464 44975 37844 23175 16279 41090\n## [34] 52698 33741 35479 54253 39544 10472 52287 30906 22876 63043 46162\n## [45] 44908 19378 72959 30101\n## \n## [[11]]\n##  [1] 54812 54860 52014 26837 40931 61152 58921 52108 34149 24114     0\n## [12] 30135 56417 42752 71078 69452 73537 68579 39565 61758 65626 80160\n## [23] 33264 41320 46836 54272 64920 34961 59020 51888 38287 30323 55134\n## [34] 66742 46672 49523 68297 53588 33981 66331 44951 37998 69722 60206\n## [45] 39488 41924 87003 44227\n## \n## [[12]]\n##  [1] 19189 19237 28715 23758 13122 25545 35622 28809 25568 11495 27547\n## [12]     0 42119 35859 47779 58087 37930 45280 12217 38459 30018 44553\n## [23]  8427 18021 23537 18649 41621 13676 35721 16280 13387 16659 19527\n## [34] 43443 52822 13901 32690 30289 10679 28544 17602 34064 58357 24599\n## [45] 34791 11692 51395 36749\n## \n## [[13]]\n##  [1] 36813 36860 61804 32740 47978 20788 51841 61898 18594 46628 54032\n## [12] 41927     0 11355 57957 25913 31393 78369 46612 71548 10254 18035\n## [23] 33039 51110 56626 36273 47508 29674 45464 29037 31619 49747 26873\n## [34] 76531 85911 31544 28499 46264 49274 41094 44675 67153 26183 28043\n## [45] 22973 39473 21910 69838\n## \n## [[14]]\n##  [1] 30553 30601 55544 20685 41718 14528 45581 55638  9008 40368 42945\n## [12] 35668 11355     0 51697 26249 44221 72109 40353 65288  8339 26597\n## [23] 26779 44850 50367 30014 41249 23415 39204 22778 25360 43488 20613\n## [34] 70272 79652 25284 38981 40004 43014 34835 38415 60894 26519 25537\n## [45] 13328 33213 30473 63579\n## \n## [[15]]\n##  [1] 25606 25654 38728 67712 27136 41314 10938 26445 52350 56922 71229\n## [12] 37346 57888 51628     0 73856 27918 10658 28655 25381 45787 55480\n## [23] 42912 31851 30870 27990 14521 38818 16023 30282 38529 42360 35296\n## [34]  9692 48367 26996 35203 18968 41886 19386 25040 49012 74126 25555\n## [45] 64696 26385 63610 45980\n## \n## [[16]]\n##  [1]  52597  52644  77588  46234  63762  36572  67625  77681  34889  62412\n## [11]  69484  57711  25729  26142  73741      0  51707  94153  62396  87332\n## [21]  30382  29845  48823  66894  72410  52057  63292  45458  61248  44821\n## [31]  47403  65531  42657  92315 101695  47328  48813  62048  65058  56878\n## [41]  60459  82937   3927  48357  26809  55257  16900  85622\n## \n## [[17]]\n##  [1] 22742 22790 66383 47823 44393 27506 36726 52009 45012 59978 74285\n## [12] 37595 31384 44290 27751 49817     0 38346 45912 60034 38449 23624\n## [23] 32592 49107 48126 21474 20735 31486 33280 24718 31198 45415 24781\n## [34] 37380 76023 27212  9559 36224 44942 16608 42297 71343 51639 13101\n## [45] 57358 43642 33153 73636\n## \n## [[18]]\n##  [1] 43466 43513 32742 64592 35685 61442 12026 20459 72478 53802 68109\n## [12] 44693 78016 71756 10711 93983 38600     0 31781 19395 65915 80450\n## [23] 51634 28730 27749 45849 20641 58945 19674 52177 58656 39239 55424\n## [34]  6452 42381 40430 45885 20422 38766 25506 33588 43026 94253 53116\n## [45] 84824 34934 87292 39994\n## \n## [[19]]\n##  [1] 17092 17140 23308 33262  1432 30429 15982 15299 41465 25814 40121\n## [12] 12039 47003 40743 27715 62970 44714 29283     0 24950 34902 49437\n## [23] 17605  4379  9837 19476 21557 22854 15656 21164 22565 11252 24411\n## [34] 27446 39313 18800 37574 12622 10778 20648  5791 28657 63241 29483\n## [45] 53811  3497 56279 31342\n## \n## [[20]]\n##  [1] 40369 40417 21028 58519 26851 55368 16853  5415 66404 47729 62035\n## [12] 38620 71942 65683 25560 87910 59845 20015 25473     0 59842 74376\n## [23] 45561 22423 21442 42753 36687 52872 30786 46104 52583 33166 49351\n## [34] 18178 30668 37333 54268 25355 32693 38098 30492 31312 88180 50019\n## [45] 78751 31837 81219 28281\n## \n## [[21]]\n##  [1] 25435 25483 50426 23657 36601  9410 40463 50520  9511 52933 67240\n## [12] 30550 10254  8470 46579 30697 31384 66991 35235 60171     0 22368\n## [23] 19572 39732 45249 24896 36131 16208 34087 17660 18153 38370 15495\n## [34] 65154 74534 20166 26099 34887 37897 29717 33298 55776 30967 18643\n## [45] 21538 28096 29210 58461\n## \n## [[22]]\n##  [1] 39601 39649 64592 40786 50767 21730 50009 64686 34013 67099 81406\n## [12] 44716 18356 26775 55533 27246 27163 81157 49401 74337 22735     0\n## [23] 35431 53898 59415 39062 38903 32067 42694 31826 34012 52536 26191\n## [34] 79320 88700 34332 24269 49053 52063 31939 47464 69942 29069 20615\n## [45] 35072 42262 10583 72627\n## \n## [[23]]\n##  [1] 17968 18016 35386 24320 18687 19877 42293 35480 19624 18209 33230\n## [12]  8427 33133 26873 42921 49101 32654 51951 17782 45130 21032 35141\n## [23]     0 21756 30208 17139 28664  5714 26043  8833  5426 23330 10461\n## [34] 50114 59493 12123 27413 26843 20737 23268 23168 40735 49371 19323\n## [45] 29874 17258 41983 43420\n## \n## [[24]]\n##  [1] 17512 17560 15422 36059  4428 33226 13943 13260 44262 27280 41587\n## [12] 14836 49800 43540 24703 65768 41703 27243  3050 22910 37699 52234\n## [23] 20402     0  7797 19895 18546 25651 12645 23961 25362 12718 27208\n## [34] 25406 37273 16329 31410  8506 12244 19956  6213 26147 66038 32280\n## [45] 56608  6317 59076 34886\n## \n## [[25]]\n##  [1] 27872 27920 10877 44424 10352 41273 18191 11042 52309 33634 47940\n## [12] 24525 57847 51588 30348 73815 47348 27849  8975 20692 45747 60281\n## [23] 31466  5924     0 30256 24190 38777 18290 32009 38488 19071 35255\n## [34] 26012 35056 24837 41771 12858 18598 25601 17995 21917 74085 40327\n## [45] 64656 19340 67124 21432\n## \n## [[26]]\n##  [1]  2570  2618 48748 37876 19003 19847 22001 34374 30884 46048 60355\n## [12] 18632 36421 30162 28117 52389 21574 45001 28350 42399 24321 32360\n## [23] 17165 21322 30491     0 13266 16059 14686  8521 15770 31485 13830\n## [34] 43164 58387  5529 16334 18496 31012 10564 16906 48891 52659 12086\n## [45] 43230 18252 45698 56000\n## \n## [[27]]\n##  [1] 10882 10930 43104 61689 21113 31083 13102 28730 42119 50899 65206\n## [12] 31324 47657 41397 14188 63624 20735 20742 22632 36755 35556 38436\n## [23] 27859 25828 24847 13266     0 27584 10000 20046 27295 36337 25065\n## [34] 19286 52743 16764 22410 12945 35863  6736 19017 48063 63894 18161\n## [45] 54465 20362 47965 50356\n## \n## [[28]]\n##  [1] 16863 16910 42857 21661 23936 13872 32894 42951 16990 23458 34927\n## [12] 13676 29769 23509 39010 45736 31548 59422 23031 52601 17668 31777\n## [23]  5714 27005 37680 16033 27558     0 26517  7728   640 30801  9331\n## [34] 57585 66965 12597 26308 27318 25985 22162 25728 48207 46006 18217\n## [45] 26509 22507 38619 50892\n## \n## [[29]]\n##  [1] 12254 12302 36928 55514 14937 28982  7761 22554 40018 44724 59031\n## [12] 25148 45556 39296 16297 61524 33297 19546 16457 30579 33455 42179\n## [23] 25758 19652 18671 14638 10140 26486     0 17950 26197 30161 22964\n## [34] 16230 46568 14086 26153  3631 29688 11548 12841 41888 61794 21904\n## [45] 52364 14186 51708 44180\n## \n## [[30]]\n##  [1]  9350  9398 36159 28322 22333 12073 24364 36253 23110 38665 52972\n## [12] 16282 28647 22388 30480 44615 24850 52724 20968 45903 16547 31081\n## [23]  8859 25465 30981  8521 20046  7753 17987     0  7465 24103  5802\n## [34] 50887 60266  4067 19610 18787 23629 17344 17198 41508 44885 11519\n## [45] 35456 13828 37924 44193\n## \n## [[31]]\n##  [1] 16574 16622 42568 24986 23647 18483 32605 42662 21985 23169 38253\n## [12] 13387 31714 25454 38721 47681 31259 59133 22742 52312 19613 33722\n## [23]  5426 26716 37391 15744 27269   640 26229  7439     0 30512  9042\n## [34] 57296 66676 12308 26019 27029 25696 21873 25440 47918 47951 17928\n## [45] 28454 22218 40564 50603\n## \n## [[32]]\n##  [1] 26503 26551 16931 35994 12622 32844 30612 23800 43880 15938 30245\n## [12] 16095 49418 43158 42770 65386 45229 40271 11257 33450 37317 51852\n## [23] 23036 13012 18528 25964 36612 30348 30711 23579 30059     0 26826\n## [34] 38433 28491 21215 39989 25280  5038 38023 16642 16486 65656 31898\n## [45] 56226 13616 58694 22401\n## \n## [[33]]\n##  [1] 14182 14230 39173 27991 25347  7318 29210 39267 22746 41679 55986\n## [12] 19297 27681 21422 35326 43649 24862 55738 23982 48917 15581 26003\n## [23] 10461 28479 33996 13643 24878  9331 22833  5790  9042 27117     0\n## [34] 53901 63281  8913 19576 23633 26643 18464 22044 44523 43919 12121\n## [45] 29215 16842 35531 47208\n## \n## [[34]]\n##  [1] 28166 28214 31292 63142 34235 59992  8710 19009 71028 52352 66659\n## [12] 43243 76566 70306  9382 92533 36885  6378 30331 17945 64465 64447\n## [23] 50184 27280 26299 30550 18965 57495 16357 50727 57206 37789 53974\n## [34]     0 40931 38980 44170 17105 37316 23829 32138 41576 92803 34921\n## [45] 83374 33484 85842 38544\n## \n## [[35]]\n##  [1]  56656  56703  14836  71705  40037  68555  40180  26812  79591  33969\n## [11]  46710  51806  85129  78869  48886 101097  76131  43341  38660  30765\n## [21]  73028  87563  58747  35609  34628  59039  52974  66059  47073  59291\n## [31]  65770  28534  62537  41504      0  53620  70554  41641  45879  54384\n## [41]  46778  10804 101367  67609  91937  49327  94405   4470\n## \n## [[36]]\n##  [1]  6069  6116 38793 33128 15430 15099 20926 29765 26135 41299 55606\n## [12] 13883 31673 25413 27213 47641 27484 40392 23602 37789 19572 34107\n## [23] 11875 28099 33615  5529 16764 12602 14123  4067 12314 26737  9081\n## [34] 38555 53778     0 22244 14923 26263 14062 13334 44142 47911 14153\n## [45] 38481 11296 40949 46827\n## \n## [[37]]\n##  [1] 17599 17647 60259 42618 30514 22302 33512 45885 39869 54835 69142\n## [12] 32452 28491 39147 35036 46923  9559 56513 37137 53910 33306 20731\n## [23] 27449 41635 42002 16331 22406 26343 26197 19575 26054 40272 19576\n## [34] 54675 69899 22068     0 30007 39799 15442 28418 57678 48746  7897\n## [45] 52215 29998 30260 67512\n## \n## [[38]]\n##  [1] 16015 16063 31108 49694 12129 29728  8933 12719 40764 38904 53211\n## [12] 29795 46302 40042 19294 62270 36294 27361 13648 24759 34201 48736\n## [23] 26504  8314 12851 18398 13137 27231  3723 18696 26943 24341 23710\n## [34] 17000 40748 14832 29913     0 23868 14545 10032 36068 62540 25665\n## [45] 53110 11378 55578 38360\n## \n## [[39]]\n##  [1] 25665 25713 22868 35156 11784 32006 29774 22961 43042 10466 34023\n## [12] 15257 48580 42320 41932 64548 44391 39433 10419 32612 36479 51014\n## [23] 22198 12174 17690 25126 35774 29510 29873 22741 29221  5081 25988\n## [34] 37595 46975 20377 39151 24442     0 37185 15804 21298 64818 31060\n## [45] 55388 12778 57856 30902\n## \n## [[40]]\n##  [1]  8180  8228 44691 42719 20175 24690 17967 30317 35726 52486 66793\n## [12] 28309 41264 35005 19052 57232 16608 25606 21695 38341 29164 31472\n## [23] 23306 27415 26433 10564  6736 22201 11607 17344 21912 37924 18672\n## [34] 24151 54330 14062 15446 14551 37450     0 18079 49650 57502 11198\n## [45] 48073 19424 41001 51943\n## \n## [[41]]\n##  [1] 14500 14548 37450 38853  8571 28213 17483 23076 39249 31405 45712\n## [12] 17630 44787 38527 25122 60755 42122 33703  5844 31101 32686 47221\n## [23] 23196  6938 19193 16883 18965 25716 13064 17181 25428 16842 22195\n## [34] 31866 47090 13317 28398 10030 16369 18056     0 34248 61025 24150\n## [45] 51595  7820 54063 44703\n## \n## [[42]]\n##  [1] 44607 44655 14655 46763 30726 50948 40494 27126 61984 22848 38359\n## [12] 34199 67522 61262 49200 83489 76445 43655 29360 31079 55421 69956\n## [23] 41140 24844 22014 44067 53288 48451 47387 41683 48162 16486 44930\n## [34] 41818 10865 39319 58092 41955 21254 54699 47093     0 83759 50002\n## [45] 74330 31720 76798  7225\n## \n## [[43]]\n##  [1]  53049  53096  78040  46686  64214  37024  68076  78133  35341  62864\n## [11]  69936  58163  26181  26593  74193   3927  51887  94604  62848  87784\n## [21]  30834  37251  49275  67346  72862  52509  63744  45910  61700  45273\n## [31]  47855  65983  43109  92767 102147  47780  48993  62500  65510  57330\n## [41]  60911  83389      0  48032  27261  55709  18722  86074\n## \n## [[44]]\n##  [1] 13481 13529 44241 35163 30415 14847 29395 44335 31781 46747 61054\n## [12] 24364 28035 31060 30271 46467 13182 52395 29050 49792 25219 20275\n## [23] 19361 33547 39063 12213 18288 18256 22079 11488 17967 32185 12121\n## [34] 50557 68348 13981  7897 25889 31711 11325 24300 49590 53557     0\n## [45] 44128 21910 29803 52275\n## \n## [[45]]\n##  [1] 40446 40494 65437 20517 51611 24421 55474 65531 11556 44883 39488\n## [12] 34762 24423 13328 61590 26917 54113 82002 50246 75181 18231 34751\n## [23] 29845 54743 60259 39906 51142 26480 49097 32670 28426 53381 29188\n## [34] 80164 89544 35177 48873 49897 52907 44727 48308 70786 27187 35430\n## [45]     0 43106 33848 73471\n## \n## [[46]]\n##  [1] 15720 15767 25996 33807  2532 23092 18703 24296 34128 19554 42809\n## [12] 11722 39666 33406 26342 55633 43342 34923  4080 32321 27565 42100\n## [23] 17288  7632 20818 18103 20185 22537 14284 13827 22248 13940 17074\n## [34] 33086 48309 11317 30237 11250 11347 19276  7865 31345 55903 22146\n## [45] 46474     0 48942 34030\n## \n## [[47]]\n##  [1] 48326 48373 73317 53314 59491 30454 61401 73410 39547 75823 76564\n## [12] 53440 23890 33221 63609 16972 35356 74204 58125 83061 30123 12418\n## [23] 44155 62623 68139 47786 50295 40791 54086 40550 42736 61260 37582\n## [34] 73238 97424 43057 32463 57777 60787 43331 56188 78666 18794 32007\n## [45] 33889 50986     0 81351\n## \n## [[48]]\n##  [1] 53987 54035 12168 56733 37369 53583 37512 24144 64619 31546 44287\n## [12] 36834 70157 63897 46218 86125 73463 40673 35992 28097 58056 72591\n## [23] 43775 32941 31960 56371 50305 51087 44405 44319 50798 22413 47565\n## [34] 38836  4459 50952 67886 38973 30907 51716 44110  7225 86395 52637\n## [45] 76965 34355 79433     0\n\n\ndistances is a list where the first element is the distances from the first castle to all the others. Let’s make it a matrix:\n\ndistances_matrix &lt;- distances %&gt;%\n    reduce(rbind)\n\n\n\nClick if you want to see the distance matrix\n\n\ndistances_matrix\n##      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11]\n## out     0    48 46364 38416 16619 20387 19617 31990 31423 46587 60894\n##        48     0 46412 38464 16667 20435 19665 32038 31471 46635 60942\n##     46900 46947     0 48698 30281 45548 30424 17056 56584 31187 52215\n##     38214 38261 48754     0 34949 23582 55661 48848 11274 29579 26880\n##     16494 16541 25311 35192     0 25375 19477 16687 36411 20939 42124\n##     18468 18516 43459 23632 29633     0 33496 43553 15352 45965 60272\n##     19645 19693 30022 55860 21434 35353     0 17740 46389 45070 59377\n##     33194 33242 17113 48244 16576 45094 16196     0 56130 37454 51761\n##     34049 34097 59040 11039 45215 18025 49077 59134     0 35131 34289\n##     40768 40815 31200 29561 26887 47108 44877 38064 35204     0 24550\n##     54812 54860 52014 26837 40931 61152 58921 52108 34149 24114     0\n##     19189 19237 28715 23758 13122 25545 35622 28809 25568 11495 27547\n##     36813 36860 61804 32740 47978 20788 51841 61898 18594 46628 54032\n##     30553 30601 55544 20685 41718 14528 45581 55638  9008 40368 42945\n##     25606 25654 38728 67712 27136 41314 10938 26445 52350 56922 71229\n##     52597 52644 77588 46234 63762 36572 67625 77681 34889 62412 69484\n##     22742 22790 66383 47823 44393 27506 36726 52009 45012 59978 74285\n##     43466 43513 32742 64592 35685 61442 12026 20459 72478 53802 68109\n##     17092 17140 23308 33262  1432 30429 15982 15299 41465 25814 40121\n##     40369 40417 21028 58519 26851 55368 16853  5415 66404 47729 62035\n##     25435 25483 50426 23657 36601  9410 40463 50520  9511 52933 67240\n##     39601 39649 64592 40786 50767 21730 50009 64686 34013 67099 81406\n##     17968 18016 35386 24320 18687 19877 42293 35480 19624 18209 33230\n##     17512 17560 15422 36059  4428 33226 13943 13260 44262 27280 41587\n##     27872 27920 10877 44424 10352 41273 18191 11042 52309 33634 47940\n##      2570  2618 48748 37876 19003 19847 22001 34374 30884 46048 60355\n##     10882 10930 43104 61689 21113 31083 13102 28730 42119 50899 65206\n##     16863 16910 42857 21661 23936 13872 32894 42951 16990 23458 34927\n##     12254 12302 36928 55514 14937 28982  7761 22554 40018 44724 59031\n##      9350  9398 36159 28322 22333 12073 24364 36253 23110 38665 52972\n##     16574 16622 42568 24986 23647 18483 32605 42662 21985 23169 38253\n##     26503 26551 16931 35994 12622 32844 30612 23800 43880 15938 30245\n##     14182 14230 39173 27991 25347  7318 29210 39267 22746 41679 55986\n##     28166 28214 31292 63142 34235 59992  8710 19009 71028 52352 66659\n##     56656 56703 14836 71705 40037 68555 40180 26812 79591 33969 46710\n##      6069  6116 38793 33128 15430 15099 20926 29765 26135 41299 55606\n##     17599 17647 60259 42618 30514 22302 33512 45885 39869 54835 69142\n##     16015 16063 31108 49694 12129 29728  8933 12719 40764 38904 53211\n##     25665 25713 22868 35156 11784 32006 29774 22961 43042 10466 34023\n##      8180  8228 44691 42719 20175 24690 17967 30317 35726 52486 66793\n##     14500 14548 37450 38853  8571 28213 17483 23076 39249 31405 45712\n##     44607 44655 14655 46763 30726 50948 40494 27126 61984 22848 38359\n##     53049 53096 78040 46686 64214 37024 68076 78133 35341 62864 69936\n##     13481 13529 44241 35163 30415 14847 29395 44335 31781 46747 61054\n##     40446 40494 65437 20517 51611 24421 55474 65531 11556 44883 39488\n##     15720 15767 25996 33807  2532 23092 18703 24296 34128 19554 42809\n##     48326 48373 73317 53314 59491 30454 61401 73410 39547 75823 76564\n##     53987 54035 12168 56733 37369 53583 37512 24144 64619 31546 44287\n##     [,12] [,13] [,14] [,15]  [,16] [,17] [,18] [,19] [,20] [,21] [,22]\n## out 19171 36961 30701 25734  52929 22843 42618 18138 40015 24860 39395\n##     19219 37009 30749 25781  52977 22890 42665 18186 40063 24908 39443\n##     28799 62122 55862 39130  78090 66375 33585 23961 21009 50021 64556\n##     25631 32853 20633 67818  46577 47882 65319 33355 58499 26540 40460\n##     13107 41949 35689 27116  57917 44116 30670  1432 26337 29848 44383\n##     23583 20890 14630 39612  36858 27623 60024 28268 53203  8789 21614\n##     35962 51927 45668 10941  67895 36650 11975 16038 16675 39826 49570\n##     28345 61668 55408 26667  77635 52670 21122 15199  8546 49567 64102\n##     25588 18148  8956 55193  34944 47717 75605 43849 68785 11835 33390\n##     11501 46805 40546 57034  62773 59493 54535 25521 47714 51581 66116\n##     30135 56417 42752 71078  69452 73537 68579 39565 61758 65626 80160\n##         0 42119 35859 47779  58087 37930 45280 12217 38459 30018 44553\n##     41927     0 11355 57957  25913 31393 78369 46612 71548 10254 18035\n##     35668 11355     0 51697  26249 44221 72109 40353 65288  8339 26597\n##     37346 57888 51628     0  73856 27918 10658 28655 25381 45787 55480\n##     57711 25729 26142 73741      0 51707 94153 62396 87332 30382 29845\n##     37595 31384 44290 27751  49817     0 38346 45912 60034 38449 23624\n##     44693 78016 71756 10711  93983 38600     0 31781 19395 65915 80450\n##     12039 47003 40743 27715  62970 44714 29283     0 24950 34902 49437\n##     38620 71942 65683 25560  87910 59845 20015 25473     0 59842 74376\n##     30550 10254  8470 46579  30697 31384 66991 35235 60171     0 22368\n##     44716 18356 26775 55533  27246 27163 81157 49401 74337 22735     0\n##      8427 33133 26873 42921  49101 32654 51951 17782 45130 21032 35141\n##     14836 49800 43540 24703  65768 41703 27243  3050 22910 37699 52234\n##     24525 57847 51588 30348  73815 47348 27849  8975 20692 45747 60281\n##     18632 36421 30162 28117  52389 21574 45001 28350 42399 24321 32360\n##     31324 47657 41397 14188  63624 20735 20742 22632 36755 35556 38436\n##     13676 29769 23509 39010  45736 31548 59422 23031 52601 17668 31777\n##     25148 45556 39296 16297  61524 33297 19546 16457 30579 33455 42179\n##     16282 28647 22388 30480  44615 24850 52724 20968 45903 16547 31081\n##     13387 31714 25454 38721  47681 31259 59133 22742 52312 19613 33722\n##     16095 49418 43158 42770  65386 45229 40271 11257 33450 37317 51852\n##     19297 27681 21422 35326  43649 24862 55738 23982 48917 15581 26003\n##     43243 76566 70306  9382  92533 36885  6378 30331 17945 64465 64447\n##     51806 85129 78869 48886 101097 76131 43341 38660 30765 73028 87563\n##     13883 31673 25413 27213  47641 27484 40392 23602 37789 19572 34107\n##     32452 28491 39147 35036  46923  9559 56513 37137 53910 33306 20731\n##     29795 46302 40042 19294  62270 36294 27361 13648 24759 34201 48736\n##     15257 48580 42320 41932  64548 44391 39433 10419 32612 36479 51014\n##     28309 41264 35005 19052  57232 16608 25606 21695 38341 29164 31472\n##     17630 44787 38527 25122  60755 42122 33703  5844 31101 32686 47221\n##     34199 67522 61262 49200  83489 76445 43655 29360 31079 55421 69956\n##     58163 26181 26593 74193   3927 51887 94604 62848 87784 30834 37251\n##     24364 28035 31060 30271  46467 13182 52395 29050 49792 25219 20275\n##     34762 24423 13328 61590  26917 54113 82002 50246 75181 18231 34751\n##     11722 39666 33406 26342  55633 43342 34923  4080 32321 27565 42100\n##     53440 23890 33221 63609  16972 35356 74204 58125 83061 30123 12418\n##     36834 70157 63897 46218  86125 73463 40673 35992 28097 58056 72591\n##     [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33]\n## out 17163 18938 28107  2570 10882 16888 12302  9350 16599 32025 14369\n##     17211 18986 28155  2618 10930 16936 12350  9398 16647 32073 14417\n##     35740 25853 10852 49283 43218 43052 37317 36283 42763 17250 39530\n##     24359 38061 43577 37674 61661 21704 55760 29822 25030 36698 27956\n##     18673  5767 11224 18877 20958 23922 15058 16110 23633 13255 19357\n##     17217 32765 38282 17929 29164 13853 27119  8892 15798 31403  7026\n##     42902 13747 19018 22029 13093 32857  7837 24321 32568 30508 29335\n##     35286 12148 11167 35578 29512 42597 23612 35829 42308 22891 39076\n##     19644 48347 53863 33510 44745 17010 42701 26274 22029 46984 22791\n##     18215 27276 32792 40228 50876 23464 44975 37844 23175 16279 41090\n##     33264 41320 46836 54272 64920 34961 59020 51888 38287 30323 55134\n##      8427 18021 23537 18649 41621 13676 35721 16280 13387 16659 19527\n##     33039 51110 56626 36273 47508 29674 45464 29037 31619 49747 26873\n##     26779 44850 50367 30014 41249 23415 39204 22778 25360 43488 20613\n##     42912 31851 30870 27990 14521 38818 16023 30282 38529 42360 35296\n##     48823 66894 72410 52057 63292 45458 61248 44821 47403 65531 42657\n##     32592 49107 48126 21474 20735 31486 33280 24718 31198 45415 24781\n##     51634 28730 27749 45849 20641 58945 19674 52177 58656 39239 55424\n##     17605  4379  9837 19476 21557 22854 15656 21164 22565 11252 24411\n##     45561 22423 21442 42753 36687 52872 30786 46104 52583 33166 49351\n##     19572 39732 45249 24896 36131 16208 34087 17660 18153 38370 15495\n##     35431 53898 59415 39062 38903 32067 42694 31826 34012 52536 26191\n##         0 21756 30208 17139 28664  5714 26043  8833  5426 23330 10461\n##     20402     0  7797 19895 18546 25651 12645 23961 25362 12718 27208\n##     31466  5924     0 30256 24190 38777 18290 32009 38488 19071 35255\n##     17165 21322 30491     0 13266 16059 14686  8521 15770 31485 13830\n##     27859 25828 24847 13266     0 27584 10000 20046 27295 36337 25065\n##      5714 27005 37680 16033 27558     0 26517  7728   640 30801  9331\n##     25758 19652 18671 14638 10140 26486     0 17950 26197 30161 22964\n##      8859 25465 30981  8521 20046  7753 17987     0  7465 24103  5802\n##      5426 26716 37391 15744 27269   640 26229  7439     0 30512  9042\n##     23036 13012 18528 25964 36612 30348 30711 23579 30059     0 26826\n##     10461 28479 33996 13643 24878  9331 22833  5790  9042 27117     0\n##     50184 27280 26299 30550 18965 57495 16357 50727 57206 37789 53974\n##     58747 35609 34628 59039 52974 66059 47073 59291 65770 28534 62537\n##     11875 28099 33615  5529 16764 12602 14123  4067 12314 26737  9081\n##     27449 41635 42002 16331 22406 26343 26197 19575 26054 40272 19576\n##     26504  8314 12851 18398 13137 27231  3723 18696 26943 24341 23710\n##     22198 12174 17690 25126 35774 29510 29873 22741 29221  5081 25988\n##     23306 27415 26433 10564  6736 22201 11607 17344 21912 37924 18672\n##     23196  6938 19193 16883 18965 25716 13064 17181 25428 16842 22195\n##     41140 24844 22014 44067 53288 48451 47387 41683 48162 16486 44930\n##     49275 67346 72862 52509 63744 45910 61700 45273 47855 65983 43109\n##     19361 33547 39063 12213 18288 18256 22079 11488 17967 32185 12121\n##     29845 54743 60259 39906 51142 26480 49097 32670 28426 53381 29188\n##     17288  7632 20818 18103 20185 22537 14284 13827 22248 13940 17074\n##     44155 62623 68139 47786 50295 40791 54086 40550 42736 61260 37582\n##     43775 32941 31960 56371 50305 51087 44405 44319 50798 22413 47565\n##     [,34]  [,35] [,36] [,37] [,38] [,39] [,40] [,41] [,42]  [,43] [,44]\n## out 40780  56004  6069 17602 16112 31552  8180 14523 49431  53199 13354\n##     40828  56052  6116 17650 16160 31599  8228 14571 49478  53247 13402\n##     31748  14513 33919 60798 31885 22872 44629 37023 14605  78360 44602\n##     63482  72862 32945 42596 50328 34302 42495 38740 46782  46847 35141\n##     28833  40700 15310 30392 12024 12782 20050  7178 30661  58187 24429\n##     58187  67567 13199 22337 27919 30929 22750 26330 48809  37128 14882\n##      8659  39662 20998 33544  9053 30034 17958 19337 40306  68165 29296\n##     19285  26753 30159 47093 12671 22418 30923 23317 27397  77906 44148\n##     73768  83148 28780 42477 43501 46511 38331 41912 64390  35215 29033\n##     52698  33741 35479 54253 39544 10472 52287 30906 22876  63043 46162\n##     66742  46672 49523 68297 53588 33981 66331 44951 37998  69722 60206\n##     43443  52822 13901 32690 30289 10679 28544 17602 34064  58357 24599\n##     76531  85911 31544 28499 46264 49274 41094 44675 67153  26183 28043\n##     70272  79652 25284 38981 40004 43014 34835 38415 60894  26519 25537\n##      9692  48367 26996 35203 18968 41886 19386 25040 49012  74126 25555\n##     92315 101695 47328 48813 62048 65058 56878 60459 82937   3927 48357\n##     37380  76023 27212  9559 36224 44942 16608 42297 71343  51639 13101\n##      6452  42381 40430 45885 20422 38766 25506 33588 43026  94253 53116\n##     27446  39313 18800 37574 12622 10778 20648  5791 28657  63241 29483\n##     18178  30668 37333 54268 25355 32693 38098 30492 31312  88180 50019\n##     65154  74534 20166 26099 34887 37897 29717 33298 55776  30967 18643\n##     79320  88700 34332 24269 49053 52063 31939 47464 69942  29069 20615\n##     50114  59493 12123 27413 26843 20737 23268 23168 40735  49371 19323\n##     25406  37273 16329 31410  8506 12244 19956  6213 26147  66038 32280\n##     26012  35056 24837 41771 12858 18598 25601 17995 21917  74085 40327\n##     43164  58387  5529 16334 18496 31012 10564 16906 48891  52659 12086\n##     19286  52743 16764 22410 12945 35863  6736 19017 48063  63894 18161\n##     57585  66965 12597 26308 27318 25985 22162 25728 48207  46006 18217\n##     16230  46568 14086 26153  3631 29688 11548 12841 41888  61794 21904\n##     50887  60266  4067 19610 18787 23629 17344 17198 41508  44885 11519\n##     57296  66676 12308 26019 27029 25696 21873 25440 47918  47951 17928\n##     38433  28491 21215 39989 25280  5038 38023 16642 16486  65656 31898\n##     53901  63281  8913 19576 23633 26643 18464 22044 44523  43919 12121\n##         0  40931 38980 44170 17105 37316 23829 32138 41576  92803 34921\n##     41504      0 53620 70554 41641 45879 54384 46778 10804 101367 67609\n##     38555  53778     0 22244 14923 26263 14062 13334 44142  47911 14153\n##     54675  69899 22068     0 30007 39799 15442 28418 57678  48746  7897\n##     17000  40748 14832 29913     0 23868 14545 10032 36068  62540 25665\n##     37595  46975 20377 39151 24442     0 37185 15804 21298  64818 31060\n##     24151  54330 14062 15446 14551 37450     0 18079 49650  57502 11198\n##     31866  47090 13317 28398 10030 16369 18056     0 34248  61025 24150\n##     41818  10865 39319 58092 41955 21254 54699 47093     0  83759 50002\n##     92767 102147 47780 48993 62500 65510 57330 60911 83389      0 48032\n##     50557  68348 13981  7897 25889 31711 11325 24300 49590  53557     0\n##     80164  89544 35177 48873 49897 52907 44727 48308 70786  27187 35430\n##     33086  48309 11317 30237 11250 11347 19276  7865 31345  55903 22146\n##     73238  97424 43057 32463 57777 60787 43331 56188 78666  18794 32007\n##     38836   4459 50952 67886 38973 30907 51716 44110  7225  86395 52637\n##     [,45] [,46] [,47] [,48]\n## out 43769 15868 46237 53617\n##     43817 15916 46285 53665\n##     68930 26320 71398 12126\n##     20752 33520 47302 56789\n##     48757  2511 51225 33346\n##     27698 21128 28456 51494\n##     58736 20683 59099 37275\n##     68476 25866 70944 24366\n##     11504 36710 37265 67075\n##     44908 19378 72959 30101\n##     39488 41924 87003 44227\n##     34791 11692 51395 36749\n##     22973 39473 21910 69838\n##     13328 33213 30473 63579\n##     64696 26385 63610 45980\n##     26809 55257 16900 85622\n##     57358 43642 33153 73636\n##     84824 34934 87292 39994\n##     53811  3497 56279 31342\n##     78751 31837 81219 28281\n##     21538 28096 29210 58461\n##     35072 42262 10583 72627\n##     29874 17258 41983 43420\n##     56608  6317 59076 34886\n##     64656 19340 67124 21432\n##     43230 18252 45698 56000\n##     54465 20362 47965 50356\n##     26509 22507 38619 50892\n##     52364 14186 51708 44180\n##     35456 13828 37924 44193\n##     28454 22218 40564 50603\n##     56226 13616 58694 22401\n##     29215 16842 35531 47208\n##     83374 33484 85842 38544\n##     91937 49327 94405  4470\n##     38481 11296 40949 46827\n##     52215 29998 30260 67512\n##     53110 11378 55578 38360\n##     55388 12778 57856 30902\n##     48073 19424 41001 51943\n##     51595  7820 54063 44703\n##     74330 31720 76798  7225\n##     27261 55709 18722 86074\n##     44128 21910 29803 52275\n##         0 43106 33848 73471\n##     46474     0 48942 34030\n##     33889 50986     0 81351\n##     76965 34355 79433     0\n\n\nLet’s baptize the rows and columns:\n\ncolnames(distances_matrix) &lt;- castles$Nom\n\nrownames(distances_matrix) &lt;- castles$Nom\n\nNow that we have the data, we can solve the TSP."
  },
  {
    "objectID": "posts/2018-11-21-lux_castle.html#solving-the-travelling-salesman-problem-thats-the-combinatorial-optimization-part",
    "href": "posts/2018-11-21-lux_castle.html#solving-the-travelling-salesman-problem-thats-the-combinatorial-optimization-part",
    "title": "The best way to visit Luxembourguish castles is doing data science + combinatorial optimization",
    "section": "\nSolving the Travelling salesman problem (that’s the combinatorial optimization part)\n",
    "text": "Solving the Travelling salesman problem (that’s the combinatorial optimization part)\n\n\nLet’s first coerce the distances_matrix to an ATSP object, which is needed for the solver. ATSP stands for asymmetrical TSP. Asymmetrical because the distances_matrix is not symmetric, meaning that going from Castle A to Castle B is longer than going from Castle B to Castle A (for example).\n\natsp_castles &lt;- ATSP(distances_matrix)\n\nI then define a list of all the available methods:\n\nmethods &lt;- c(\"identity\", \"random\", \"nearest_insertion\",\n             \"cheapest_insertion\", \"farthest_insertion\", \"arbitrary_insertion\",\n             \"nn\", \"repetitive_nn\", \"two_opt\")\n\nAnd solve the problem with all the methods:\n\nsolutions &lt;- map(methods, ~solve_TSP(x = atsp_castles, method = ., two_opt = TRUE, rep = 10,  two_opt_repetitions = 10)) %&gt;%\n    set_names(methods)\n## Warning: executing %dopar% sequentially: no parallel backend registered\n\nI do this because the results vary depending on the methods, and I want to be exhaustive (solving this problem is quite fast, so there’s no reason not to do it):\n\nsolutions_df &lt;- solutions %&gt;%\n    map_df(as.numeric)\n\nsolutions_df is a data frame with the order of the castles to visit in rows and the method used in columns.\n\n\n\n\nClick if you want to see the solutions\n\n\nsolutions_df\n## # A tibble: 48 x 9\n##    identity random nearest_inserti… cheapest_insert… farthest_insert…\n##       &lt;dbl&gt;  &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n##  1        1     10               37               44               15\n##  2        2     11               17               37               27\n##  3       36      4               22               17               29\n##  4       33      9               47               40               38\n##  5        6     45               16               27               41\n##  6       44     43               43               15               19\n##  7       37     16               13               18                5\n##  8       17     47               21               34               46\n##  9       22     22               14                7               12\n## 10       47     13               45               20               23\n## # … with 38 more rows, and 4 more variables: arbitrary_insertion &lt;dbl&gt;,\n## #   nn &lt;dbl&gt;, repetitive_nn &lt;dbl&gt;, two_opt &lt;dbl&gt;\n\n\nNow, let’s extract the tour lengths, see which one is the minimum, then plot it.\n\ntour_lengths &lt;- solutions %&gt;%\n    map_dbl(tour_length)\n\nwhich.min(tour_lengths)\n## arbitrary_insertion \n##                   6\n\nThe total length of the tour is 474 kilometers (that’s 295 miles). Before plotting the data, let’s re-order it according to the solution:\n\ncastles_to_visit &lt;- castles[pull(solutions_df, names(which.min(tour_lengths))), ]"
  },
  {
    "objectID": "posts/2018-11-21-lux_castle.html#plot-the-solution",
    "href": "posts/2018-11-21-lux_castle.html#plot-the-solution",
    "title": "The best way to visit Luxembourguish castles is doing data science + combinatorial optimization",
    "section": "\nPlot the solution\n",
    "text": "Plot the solution\n\n\nTo plot the solution, I first use a data frame I created with the longitude and latitude of Luxembourguish communes, from the geojson file available on the OpenData Portal. I converted it to a data frame because it is easier to manipulate this way. The code to do that is in the appendix of this blog post:\n\ncommunes_df &lt;- read_csv(\"communes_df.csv\")\n## Parsed with column specification:\n## cols(\n##   lon = col_double(),\n##   lat = col_double(),\n##   commune = col_character()\n## )\n\nNow I can use {ggplot2} to create the map with the tour. I use geom_polygon() to build the map, geom_point() to add the castles, geom_path() to connect the points according to the solution I found and geom_point() again to highlight the starting castle:\n\nggplot() +\n    geom_polygon(data = communes_df, aes(x = lon, y = lat, group = commune), colour = \"grey\", fill = NA) +\n    geom_point(data = castles, aes(x = lon, y = lat), colour = \"#82518c\", size = 3) +\n    geom_path(data = castles_to_visit, aes(x = lon, y = lat), colour = \"#647e0e\") +\n    geom_point(data = (slice(castles_to_visit, 1)), aes(x = lon, y = lat), colour = \"white\", size = 5) +\n    theme_void() +\n    ggtitle(\"The shortest tour to visit 48 Luxembourguish castles\") +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank(),\n          legend.text = element_text(colour = \"white\"),\n          plot.background = element_rect(\"#272b30\"),\n          plot.title = element_text(colour = \"white\")) \n\n\n\n\nThe white point is the starting point of the tour. As a bonus, let’s do the same plot without points, but castles emojis instead (using the {ggimage} package):\n\nggplot() +\n    geom_polygon(data = communes_df, aes(x = lon, y = lat, group = commune), colour = \"grey\", fill = NA) +\n    geom_emoji(data = castles, aes(x = lon, y = lat, image = \"1f3f0\")) + # &lt;- this is the hex code for the \"european castle\" emoji\n    geom_path(data = castles_to_visit, aes(x = lon, y = lat), colour = \"#647e0e\") +\n    theme_void() +\n    ggtitle(\"The shortest tour to visit 48 Luxembourguish castles\") +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank(),\n          legend.text = element_text(colour = \"white\"),\n          plot.background = element_rect(\"#272b30\"),\n          plot.title = element_text(colour = \"white\"))\n## Warning: Ignoring unknown parameters: image_colour\n\n\n\n\nIt’s horrible."
  },
  {
    "objectID": "posts/2018-11-21-lux_castle.html#appendix",
    "href": "posts/2018-11-21-lux_castle.html#appendix",
    "title": "The best way to visit Luxembourguish castles is doing data science + combinatorial optimization",
    "section": "\nAppendix\n",
    "text": "Appendix\n\n\nThe code below converts the geojson that can be downloaded from the OpenData Portal to csv. A csv file is easier to handle. I only focus on the communes.\n\nlimadmin &lt;- RJSONIO::fromJSON(\"limadmin.geojson\")\n\ncommunes &lt;- limadmin$communes\n\nextract_communes &lt;- function(features){\n\n    res &lt;- features$geometry$coordinates %&gt;%\n        map(lift(rbind)) %&gt;%\n        as.data.frame() %&gt;%\n        rename(lon = X1,\n               lat = X2)\n\n    res %&gt;%\n        mutate(commune = features$properties[1])\n}\n\ncommunes_df &lt;- map(limadmin$communes$features, extract_communes)\n\n## Steinfort and Waldbredimus special treatment:\n\nsteinfort &lt;- limadmin$communes$features[[5]]$geometry$coordinates[[1]] %&gt;%\n    map(lift(rbind)) %&gt;%\n    as.data.frame() %&gt;%\n    rename(lon = X1,\n           lat = X2) %&gt;%\n    mutate(commune = \"Steinfort\")\n\nwaldbredimus &lt;- limadmin$communes$features[[44]]$geometry$coordinates[[1]] %&gt;%\n    map(lift(rbind)) %&gt;%\n    as.data.frame() %&gt;%\n    rename(lon = X1,\n           lat = X2) %&gt;%\n    mutate(commune = \"Waldbredimus\")\n\ncommunes_df[[5]] &lt;- NULL\ncommunes_df[[43]] &lt;- NULL\n\n\ncommunes_df &lt;- bind_rows(communes_df, list(steinfort, waldbredimus))"
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html",
    "title": "How to use jailbreakr",
    "section": "",
    "text": "The jailbreakr package is probably one of the most interesting packages I came across recently. This package makes it possible to extract messy data from spreadsheets. What is meant by messy? I am sure you already had to deal with spreadsheets that contained little tables inside a single sheet for example. As far as I know, there is no simple way of extracting these tables without having to fiddle around a lot. This is now over with jailbreakr. Well not entirely, because jailbreakr is still in development, but it works well already. If you want to know more about the planned features, you can watch the following video by Jenny Bryan, one of the package’s authors."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html#what-is-jailbreakr",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html#what-is-jailbreakr",
    "title": "How to use jailbreakr",
    "section": "",
    "text": "The jailbreakr package is probably one of the most interesting packages I came across recently. This package makes it possible to extract messy data from spreadsheets. What is meant by messy? I am sure you already had to deal with spreadsheets that contained little tables inside a single sheet for example. As far as I know, there is no simple way of extracting these tables without having to fiddle around a lot. This is now over with jailbreakr. Well not entirely, because jailbreakr is still in development, but it works well already. If you want to know more about the planned features, you can watch the following video by Jenny Bryan, one of the package’s authors."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html#installation-and-data",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html#installation-and-data",
    "title": "How to use jailbreakr",
    "section": "\nInstallation and data\n",
    "text": "Installation and data\n\n\nYou will have to install the package from Github, as it is not on CRAN yet. Here is the Github link. To install the package, just run the following commands in an R console:\n\ndevtools::install_github(c(\"hadley/xml2\",\n                           \"rsheets/linen\",\n                           \"rsheets/cellranger\",\n                           \"rsheets/rexcel\",\n                           \"rsheets/jailbreakr\"))\n\nIf you get the following error:\n\ndevtools::install_github(\"hadley/xml2\")\nDownloading GitHub repo hadley/xml2@master\nfrom URL https://api.github.com/repos/hadley/xml2/zipball/master\nError in system(full, intern = quiet, ignore.stderr = quiet, ...) :\n    error in running command\n\nand if you’re on a GNU+Linux distribution try to run the following command:\n\noptions(unzip = \"internal\")\n\nand then run github_install() again.\n\n\nAs you can see, you need some other packages to make it work. Now we are going to get some data. We are going to download some time series from the European Commission, data I had to deal with recently. Download the data by clicking here and look for the spreadsheet titled Investment_total_factors_nace2.xlsx. The data we are interested in is on the second sheet, named TOT. You cannot import this sheet easily into R because there are four tables on the same sheet. Let us use jailbreakr to get these tables out of the sheet and into nice, tidy, data frames."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html#jailbreakr-to-the-rescue",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html#jailbreakr-to-the-rescue",
    "title": "How to use jailbreakr",
    "section": "\njailbreakr to the rescue\n",
    "text": "jailbreakr to the rescue\n\n\nThe first step is to read the data in. For this, we are going to use the rexcel package, which is also part of the rsheets organization on Github that was set up by Jenny Brian and Rich Fitzjohn, the authors of these packages. rexcel imports the sheet you want but not in a way that is immediately useful to you. It just gets the sheet into R, which makes it then possible to use jailbreakr’s magic on it. First, let’s import the packages we need:\n\nlibrary(\"rexcel\")\nlibrary(\"jailbreakr\")\n\nWe need to check which sheet to import. There are two sheets, and we want to import the one called TOT, the second one. But is it really the second one? I have noticed that sometimes, there are hidden sheets which makes importing the one you want impossible. So first, let use use another package, readxl and its function excel_sheets() to make sure we are extracting the sheet we really need:\n\nsheets &lt;- readxl::excel_sheets(path_to_data)\n\ntot_sheet &lt;- which(sheets == \"TOT\")\n\nprint(tot_sheet)\n## [1] 3\n\nAs you can see, the sheet we want is not the second, but the third! Let us import this sheet into R now (this might take more time than you think; on my computer it takes around 10 seconds):\n\nmy_sheet &lt;- rexcel_read(path_to_data, sheet = tot_sheet)\n\nNow we can start using jailbreakr. The function split_sheet() is the one that splits the sheet into little tables:\n\ntables &lt;- split_sheet(my_sheet)\nstr(tables)\n## List of 4\n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 34 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list \n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 33 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list \n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 32 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list \n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 33 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list\n\ntables is actually a list containing worksheet_view objects. Take a look at the dim attribute: you see the dimensions of the tables there. When I started using jailbreakr I was stuck here. I was looking for the function that would extract the data frames and could not find it. Then I watched the video and I understood what I had to do: a worksheet_view object has a values() method that does the extraction for you. This is a bit unusual in R (it made me feel like I was using Python); maybe in future versions this values() method will become a separate function of its own in the package. What happens when we use values()?\n\nlibrary(\"purrr\")\nlist_of_data &lt;-  map(tables, (function(x)(x$values())))\nmap(list_of_data, head)\n## [[1]]\n##      [,1]     [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]\n## [1,] \"TOT\"    NA      NA    NA    NA    NA    NA    NA    NA    NA   \n## [2,] \"DEMAND\" 33603   33969 34334 34699 35064 35430 35795 36160 36525\n## [3,] \"FDEMT\"  \"FDEMN\" NA    NA    NA    NA    NA    NA    NA    NA   \n## [4,] \"EU\"     \":\"     16.9  -1.4  20.2  34.5  31.4  37.5  39    37.3 \n## [5,] \"EA\"     \":\"     15.5  -13.1 14.8  30.9  25.1  35.2  39.2  37.1 \n## [6,] \"BE\"     \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   42.3  43.1 \n##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n## [1,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [2,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543\n## [3,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [4,] 39.2  27.5  20.6  21.4  29.8  26.4  32.5  47.1  19    -1.3  23.5 \n## [5,] 39.5  25.3  18.2  18.9  27.4  23    28.2  46.1  12.3  -9.3  19.3 \n## [6,] 45.8  42.2  42.9  43.8  45.8  47.4  49.1  50.9  48.2  46.9  46.3 \n##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] \n## [1,] NA    NA    NA    NA    NA    NA    NA    \n## [2,] 40908 41274 41639 42004 42369 42735 43100 \n## [3,] NA    NA    NA    NA    NA    NA    NA    \n## [4,] 29    22    21.1  25.6  31.8  22.9  \"30.7\"\n## [5,] 26.2  18.6  15.7  21.7  28.8  17.3  26.6  \n## [6,] 46.8  47.1  48.2  50.1  49.2  34.5  34.4  \n## \n## [[2]]\n##      [,1]        [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]\n## [1,] \"FINANCIAL\" 33603   33969 34334 34699 35064 35430 35795 36160 36525\n## [2,] \"FFINT\"     \"FFINN\" NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] \"EU\"        \":\"     -5.1  -6.2  2.7   6.7   9     14.4  13.9  14   \n## [4,] \"EA\"        \":\"     -8.8  -13.5 -3.4  2.6   5.7   12.5  13.2  13.1 \n## [5,] \"BE\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   21.5  22.4 \n## [6,] \"BG\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"  \n##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n## [1,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543\n## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] 16.4  9.4   7.4   8.1   12.4  8.4   13.6  23.4  4.1   -4    10.9 \n## [4,] 16.5  8     6.8   5.1   9.9   4.8   8.4   24.3  -2.8  -10.5 9.3  \n## [5,] 20.9  22.3  32.2  33.5  33.8  34.8  35    34.5  37.2  33.5  32.7 \n## [6,] \":\"   \":\"   20.8  24    27.1  28.3  33.4  37.5  37.7  26.6  30.4 \n##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] \n## [1,] 40908 41274 41639 42004 42369 42735 43100 \n## [2,] NA    NA    NA    NA    NA    NA    NA    \n## [3,] 12.4  10.2  8.8   13.4  17.4  6.2   \"12.3\"\n## [4,] 9     7.2   5     11    13.1  -1    6.5   \n## [5,] 31.5  32.3  33    31.7  32.2  19.9  20.5  \n## [6,] 33.8  35.6  36    41.5  41.6  44.2  43.8  \n## \n## [[3]]\n##      [,1]        [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]\n## [1,] \"TECHNICAL\" 33603   33969 34334 34699 35064 35430 35795 36160 36525\n## [2,] \"FTECT\"     \"FTECN\" NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] \"EU\"        \":\"     39.2  37.6  38.3  40    40.7  42.8  43.5  43.8 \n## [4,] \"EA\"        \":\"     39.7  36.2  37.5  41.2  40    44    44.8  44.9 \n## [5,] \"BE\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   58.8  58.5 \n## [6,] \"BG\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"  \n##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n## [1,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543\n## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] 37    31.1  27.2  30.9  30.4  30.3  27.4  40.5  25.8  23.1  27.4 \n## [4,] 37    30.3  27.4  31    29.9  29.7  24.8  41    23.4  19.5  26.4 \n## [5,] 58.3  58.4  57.7  59.2  59.6  59.4  60.2  59.5  60.5  57.9  56.3 \n## [6,] \":\"   \":\"   17.3  17.5  21.1  21.5  25.3  28.2  26.1  21    25.3 \n##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] \n## [1,] 40908 41274 41639 42004 42369 42735 43100 \n## [2,] NA    NA    NA    NA    NA    NA    NA    \n## [3,] 28.9  26.3  31.3  32.1  32.1  30.2  \"34.6\"\n## [4,] 28.5  25.9  32.1  32.4  33.1  30.2  36    \n## [5,] 56.7  57.7  57.9  58.6  59.1  13.1  13.1  \n## [6,] 24.6  26.8  30.4  31.9  34.1  34.8  33.7  \n## \n## [[4]]\n##      [,1]    [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10] [,11]\n## [1,] \"OTHER\" 33603   33969 34334 34699 35064 35430 35795 36160 36525 36891\n## [2,] \"FOTHT\" \"FOTHN\" NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] \"EU\"    \":\"     2.9   -0.5  3.9   3.9   1     4.1   4.7   7     7.2  \n## [4,] \"EA\"    \":\"     2.3   -4.9  1.4   1.3   -2.4  1.1   3.2   5.8   7    \n## [5,] \"BE\"    \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   14    14.9  15.9 \n## [6,] \"BG\"    \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"  \n##      [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]\n## [1,] 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543 40908\n## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] -1.5  6.2   8.1   7.6   1.4   2.4   13.7  -1.9  -3.2  1.1   1.1  \n## [4,] -3.7  5.5   7.1   7.2   -2.2  0.4   15.5  -4.6  -8.4  0.3   -3.3 \n## [5,] 16.3  22.8  23.1  22.4  24.5  25.3  25.5  26.6  26.6  24.7  24.6 \n## [6,] \":\"   -2.3  -0.8  2.4   2.9   3.5   4.8   5.5   2.2   3.3   3.2  \n##      [,23] [,24] [,25] [,26] [,27] [,28]\n## [1,] 41274 41639 42004 42369 42735 43100\n## [2,] NA    NA    NA    NA    NA    NA   \n## [3,] -1.6  0.9   2.7   1.9   -3.3  \"2.1\"\n## [4,] -2.3  0.6   2.5   2.1   -5.4  1.7  \n## [5,] 26.4  25.9  25    25.3  4.7   5.2  \n## [6,] 5.9   7     8.2   9.6   9.4   9.1\n\nWe are getting really close to something useful! Now we can get the first table and do some basic cleaning to have a tidy dataset:\n\ndataset1 &lt;- list_of_data[[1]]\n\ndataset1 &lt;- dataset1[-c(1:3), ]\ndataset1[dataset1 == \":\"] &lt;- NA\ncolnames(dataset1) &lt;- c(\"country\", seq(from = 1991, to = 2017))\n\nhead(dataset1)\n##      country 1991 1992 1993  1994 1995 1996 1997 1998 1999 2000 2001 2002\n## [1,] \"EU\"    NA   16.9 -1.4  20.2 34.5 31.4 37.5 39   37.3 39.2 27.5 20.6\n## [2,] \"EA\"    NA   15.5 -13.1 14.8 30.9 25.1 35.2 39.2 37.1 39.5 25.3 18.2\n## [3,] \"BE\"    NA   NA   NA    NA   NA   NA   NA   42.3 43.1 45.8 42.2 42.9\n## [4,] \"BG\"    NA   NA   NA    NA   NA   NA   NA   NA   NA   NA   NA   39.6\n## [5,] \"CZ\"    NA   NA   NA    NA   NA   NA   NA   NA   NA   NA   NA   54.9\n## [6,] \"DK\"    49.5 45   50    59.5 62.5 55.5 60.5 57.5 56   61.5 57.5 59.5\n##      2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016\n## [1,] 21.4 29.8 26.4 32.5 47.1 19   -1.3 23.5 29   22   21.1 25.6 31.8 22.9\n## [2,] 18.9 27.4 23   28.2 46.1 12.3 -9.3 19.3 26.2 18.6 15.7 21.7 28.8 17.3\n## [3,] 43.8 45.8 47.4 49.1 50.9 48.2 46.9 46.3 46.8 47.1 48.2 50.1 49.2 34.5\n## [4,] 43   42.8 45.5 49.1 52.6 50.7 39.5 45.5 47.4 45.6 50.5 51.4 49.9 53.2\n## [5,] 37   48.5 67.9 66.4 66.8 69.3 64.7 61   56   47.5 53   53.5 67.5 58  \n## [6,] 53.5 50   59   64   63   56   33.5 57   47   48   52   45.5 40.5 36.5\n##      2017  \n## [1,] \"30.7\"\n## [2,] 26.6  \n## [3,] 34.4  \n## [4,] 52.8  \n## [5,] 59.5  \n## [6,] 37.5\n\nEt voilà! We went from a messy spreadsheet to a tidy dataset in a matter of minutes. Even though this package is still in early development and not all the features that are planned are available, the basics are there and can save you a lot of pain!"
  },
  {
    "objectID": "posts/2017-01-07-my-free-book-has-a-cover.html",
    "href": "posts/2017-01-07-my-free-book-has-a-cover.html",
    "title": "My free book has a cover!",
    "section": "",
    "text": "I’m currently writing a book as a hobby. It’s titled Functional programming and unit testing for data munging with R and you can get it for free here. You can also read it online for free on my webpage What’s the book about?\nHere’s the teaser text:\n\nLearn the basics of functional programming, unit testing and package development for the R programming language in order to make your data tidy!\n\nThe book now has a beautiful cover thanks to @putosaure. Putosaure is a Paris based graphic designer who also reviews video games. He is also a very good friend of mine and I am very happy he made this beautiful cover for my book:\n\n\n\nIn it, we see a guy holding a shield with the Greek letter lambda, which also happens to be the letter to designate functional programming. I’ve added the title with the Komika Title font.\nConsider this cover in beta, it’ll probably evolve some more. But I couldn’t wait to use it!\nI love it. Hope you’ll love it too!"
  },
  {
    "objectID": "posts/2018-02-16-importing_30gb_of_data.html",
    "href": "posts/2018-02-16-importing_30gb_of_data.html",
    "title": "Importing 30GB of data into R with sparklyr",
    "section": "",
    "text": "Disclaimer: the first part of this blog post draws heavily from Working with CSVs on the Command Line, which is a beautiful resource that lists very nice tips and tricks to work with CSV files before having to load them into R, or any other statistical software. I highly recommend it! Also, if you find this interesting, read also Data Science at the Command Line another great resource!\n\n\nIn this blog post I am going to show you how to analyze 30GB of data. 30GB of data does not qualify as big data, but it’s large enough that you cannot simply import it into R and start working on it, unless you have a machine with a lot of RAM.\n\n\nLet’s start by downloading some data. I am going to import and analyze (very briefly) the airline dataset that you can download from Microsoft here. I downloaded the file AirOnTimeCSV.zip from AirOnTime87to12. Once you decompress it, you’ll end up with 303 csv files, each around 80MB. Before importing them into R, I will use command line tools to bind the rows together. But first, let’s make sure that the datasets all have the same columns. I am using Linux, and if you are too, or if you are using macOS, you can follow along. Windows users that installed the Linux Subsystem can also use the commands I am going to show! First, I’ll use the head command in bash. If you’re familiar with head() from R, the head command in bash works exactly the same:\n\n[18-02-15 21:12] brodriguesco in /Documents/AirOnTimeCSV ➤ head -5 airOT198710.csv\n\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\",\"FL_DATE\",\"UNIQUE_CARRIER\",\"TAIL_NUM\",\"FL_NUM\",\n1987,10,1,4,1987-10-01,\"AA\",\"\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0901\",1.00,\n1987,10,2,5,1987-10-02,\"AA\",\"\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0901\",1.00\n1987,10,3,6,1987-10-03,\"AA\",\"\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0859\",-1.00\n1987,10,4,7,1987-10-04,\"AA\",\"\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0900\",0.00,\n\nlet’s also check the 5 first lines of the last file:\n\n[18-02-15 21:13] cbrunos in brodriguesco in /Documents/AirOnTimeCSV ➤ head -5 airOT201212.csv\n\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\",\"FL_DATE\",\"UNIQUE_CARRIER\",\"TAIL_NUM\",\"FL_NUM\",\n2012,12,1,6,2012-12-01,\"AA\",\"N322AA\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0852\",\n2012,12,2,7,2012-12-02,\"AA\",\"N327AA\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0853\",\n2012,12,3,1,2012-12-03,\"AA\",\"N319AA\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0856\"\n2012,12,4,2,2012-12-04,\"AA\",\"N329AA\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"1006\"\n\nWhy do that in bash instead of R? This way, I don’t need to import the data into R before checking its contents!\n\n\nIt does look like the structure did not change. Before importing the data into R, I am going to bind the rows of the datasets using other command line tools. Again, the reason I don’t import all the files into R is because I would need around 30GB of RAM to do so. So it’s easier to do it with bash:\n\nhead -1 airOT198710.csv &gt; combined.csv\nfor file in $(ls airOT*); do cat $file | sed \"1 d\" &gt;&gt; combined.csv; done\n\nOn the first line I use head again to only copy the column names (the first line of the first file) into a new file called combined.csv.\n\n\nThis &gt; operator looks like the now well known pipe operator in R, %&gt;%, but in bash, %&gt;% is actually |, not &gt;. &gt; redirects the output of the left hand side to a file on the right hand side, not to another command. On the second line, I loop over the files. I list the files with ls, and because I want only to loop over those that are named airOTxxxxx I use a regular expression, airOT* to only list those. The second part is do cat $file. do is self-explanatory, and cat stands for catenate. Think of it as head, but on all rows instead of just 5; it prints $file to the terminal. $file one element of the list of files I am looping over. But because I don’t want to see the contents of $file on my terminal, I redirect the output with the pipe, | to another command, sed. sed has an option, \"1 d\", and what this does is filtering out the first line, containing the header, from $file before appending it with &gt;&gt; to combined.csv. If you found this interesting, read more about it here.\n\n\nThis creates a 30GB CSV file that you can then import. But how? There seems to be different ways to import and work with larger than memory data in R using your personal computer. I chose to use {sparklyr}, an R package that allows you to work with Apache Spark from R. Apache Spark is a fast and general engine for large-scale data processing, and {sparklyr} not only offers bindings to it, but also provides a complete {dplyr} backend. Let’s start:\n\nlibrary(sparklyr)\nlibrary(tidyverse)\n\nspark_dir = \"/my_2_to_disk/spark/\"\n\nI first load {sparklyr} and the {tidyverse} and also define a spark_dir. This is because Spark creates a lot of temporary files that I want to save there instead of my root partition, which is on my SSD. My root partition only has around 20GO of space left, so whenever I tried to import the data I would get the following error:\n\njava.io.IOException: No space left on device\n\nIn order to avoid this error, I define this directory on my 2TO hard disk. I then define the temporary directory using the two lines below:\n\nconfig = spark_config()\n\nconfig$`sparklyr.shell.driver-java-options` &lt;-  paste0(\"-Djava.io.tmpdir=\", spark_dir)\n\nThis is not sufficient however; when I tried to read in the data, I got another error:\n\njava.lang.OutOfMemoryError: Java heap space\n\nThe solution for this one is to add the following lines to your config():\n\nconfig$`sparklyr.shell.driver-memory` &lt;- \"4G\"\nconfig$`sparklyr.shell.executor-memory` &lt;- \"4G\"\nconfig$`spark.yarn.executor.memoryOverhead` &lt;- \"512\"\n\nFinally, I can load the data. Because I am working on my machine, I connect to a \"local\" Spark instance. Then, using spark_read_csv(), I specify the Spark connection, sc, I give a name to the data that will be inside the database and the path to it:\n\nsc = spark_connect(master = \"local\", config = config)\n\nair = spark_read_csv(sc, name = \"air\", path = \"combined.csv\")\n\nOn my machine, this took around 25 minutes, and RAM usage was around 6GO.\n\n\nIt is possible to use standard {dplyr} verbs with {sparklyr} objects, so if I want the mean delay at departure per day, I can simply write:\n\ntic = Sys.time()\nmean_dep_delay = air %&gt;%\n  group_by(YEAR, MONTH, DAY_OF_MONTH) %&gt;%\n  summarise(mean_delay = mean(DEP_DELAY))\n(toc = Sys.time() - tic)\nTime difference of 0.05634999 secs\n\nThat’s amazing, only 0.06 seconds to compute these means! Wait a minute, that’s weird… I mean my computer is brand new and quite powerful but still… Let’s take a look at mean_dep_delay:\n\nhead(mean_dep_delay)\n# Source:   lazy query [?? x 4]\n# Database: spark_connection\n# Groups:   YEAR, MONTH\n   YEAR MONTH DAY_OF_MONTH mean_delay\n  &lt;int&gt; &lt;int&gt;        &lt;int&gt;      &lt;dbl&gt;\n1  1987    10            9       6.71\n2  1987    10           10       3.72\n3  1987    10           12       4.95\n4  1987    10           14       4.53\n5  1987    10           23       6.48\n6  1987    10           29       5.77\nWarning messages:\n1: Missing values are always removed in SQL.\nUse `AVG(x, na.rm = TRUE)` to silence this warning\n2: Missing values are always removed in SQL.\nUse `AVG(x, na.rm = TRUE)` to silence this warning\n\nSurprisingly, this takes around 5 minutes to print? Why? Look at the class of mean_dep_delay: it’s a lazy query that only gets evaluated once I need it. Look at the first line; lazy query [?? x 4]. This means that I don’t even know how many rows are in mean_dep_delay! The contents of mean_dep_delay only get computed once I explicitly ask for them. I do so with the collect() function, which transfers the Spark object into R’s memory:\n\ntic = Sys.time()\nr_mean_dep_delay = collect(mean_dep_delay)\n(toc = Sys.time() - tic)\nTime difference of 5.2399 mins\n\nAlso, because it took such a long time to compute: I save it to disk:\n\nsaveRDS(r_mean_dep_delay, \"mean_dep_delay.rds\")\n\nSo now that I transferred this sparklyr table to a standard tibble in R, I can create a nice plot of departure delays:\n\nlibrary(lubridate)\n\ndep_delay =  r_mean_dep_delay %&gt;%\n  arrange(YEAR, MONTH, DAY_OF_MONTH) %&gt;%\n  mutate(date = ymd(paste(YEAR, MONTH, DAY_OF_MONTH, sep = \"-\")))\n\nggplot(dep_delay, aes(date, mean_delay)) + geom_smooth()\n## `geom_smooth()` using method = 'gam'\n\n\n\n\nThat’s it for now, but in a future blog post I will continue to explore this data!\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2017-03-29-make-ggplot2-purrr.html",
    "href": "posts/2017-03-29-make-ggplot2-purrr.html",
    "title": "Make ggplot2 purrr",
    "section": "",
    "text": "Update: I’ve included another way of saving a separate plot by group in this article, as pointed out by @monitus. Actually, this is the preferred solution; using dplyr::do() is deprecated, according to Hadley Wickham himself.\n\n\nI’ll be honest: the title is a bit misleading. I will not use purrr that much in this blog post. Actually, I will use one single purrr function, at the very end. I use dplyr much more. However Make ggplot2 purrr sounds better than Make ggplot dplyr or whatever the verb for dplyr would be.\n\n\nAlso, this blog post was inspired by a stackoverflow question and in particular one of the answers. So I don’t bring anything new to the table, but I found this stackoverflow answer so useful and so underrated (only 16 upvotes as I’m writing this!) that I wanted to write something about it.\n\n\nBasically the idea of this blog post is to show how to create graphs using ggplot2, but by grouping by a factor variable beforehand. To illustrate this idea, let’s use the data from the Penn World Tables 9.0. The easiest way to get this data is to install the package called pwt9 with:\n\ninstall.packages(\"pwt9\")\n\nand then load the data with:\n\ndata(\"pwt9.0\")\n\nNow, let’s load the needed packages. I am also using ggthemes which makes themeing your ggplots very easy. I’ll be making Tufte-style plots.\n\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(pwt9)\n\nFirst let’s select a list of countries:\n\ncountry_list &lt;- c(\"France\", \"Germany\", \"United States of America\", \"Luxembourg\", \"Switzerland\", \"Greece\")\n\nsmall_pwt &lt;- pwt9.0 %&gt;%\n  filter(country %in% country_list)\n\nLet’s us also order the countries in the data frame as I have written them in country_list:\n\nsmall_pwt &lt;- small_pwt %&gt;%\n  mutate(country = factor(country, levels = country_list, ordered = TRUE))\n\nYou might be wondering why this is important. At the end of the article, we are going to save the plots to disk. If we do not re-order the countries inside the data frame as in country_list, the name of the files will not correspond to the correct plots!\n\n\nUpdate: While this can still be interesting to know, especially if you want to order the bars of a barplot made with ggplot2, I included a suggestion by @expersso that does not require your data to be ordered!\n\n\nNow when you want to plot the same variable by countries, say avh (Average annual hours worked by persons engaged), the usual way to do this is with one of facet_wrap() or facet_grid():\n\nggplot(data = small_pwt) + theme_tufte() +\n  geom_line(aes(y = avh, x = year)) +\n  facet_wrap(~country)\n\n\n\nggplot(data = small_pwt) + theme_tufte() +\n  geom_line(aes(y = avh, x = year)) +\n  facet_grid(country~.)\n\n\n\n\nAs you can see, for this particular example, facet_grid() is not very useful, but do notice its argument, country~., which is different from facet_wrap()’s argument. This way, I get the graphs stacked horizontally. If I had used facet_grid(~country) the graphs would be side by side and completely unreadable.\n\n\nNow, let’s go to the meat of this post: what if you would like to have one single graph for each country? You’d probably think of using dplyr::group_by() to form the groups and then the graphs. This is the way to go, but you also have to use dplyr::do(). This is because as far as I understand, ggplot2 is not dplyr-aware, and using an arbitrary function with groups is only possible with dplyr::do().\n\n\nUpdate: As explained in the intro above, I also added the solution that uses tidyr::nest():\n\n# Ancient, deprecated way of doing this\nplots &lt;- small_pwt %&gt;%\n  group_by(country) %&gt;%\n  do(plot = ggplot(data = .) + theme_tufte() +\n       geom_line(aes(y = avh, x = year)) +\n       ggtitle(unique(.$country)) +\n       ylab(\"Year\") +\n       xlab(\"Average annual hours worked by persons engaged\"))\n\nAnd this is the approach that uses tidyr::nest():\n\n# Preferred approach\nplots &lt;- small_pwt %&gt;%\n  group_by(country) %&gt;%\n  nest() %&gt;%\n  mutate(plot = map2(data, country, ~ggplot(data = .x) + theme_tufte() +\n       geom_line(aes(y = avh, x = year)) +\n       ggtitle(.y) +\n       ylab(\"Year\") +\n       xlab(\"Average annual hours worked by persons engaged\")))\n\nIf you know dplyr at least a little bit, the above lines should be easy for you to understand. But notice how we get the title of the graphs, with ggtitle(unique(.$country)), which was actually the point of the stackoverflow question.\n\n\nUpdate: The modern version uses tidyr::nest(). Its documentation tells us:\n\n\nThere are many possible ways one could choose to nest columns inside a data frame. nest() creates a list of data frames containing all the nested variables: this seems to be the most useful form in practice. Let’s take a closer look at what it does exactly:\n\nsmall_pwt %&gt;%\n  group_by(country) %&gt;%\n  nest() %&gt;%\n  head()\n## # A tibble: 6 x 2\n##   country                  data              \n##   &lt;ord&gt;                    &lt;list&gt;            \n## 1 Switzerland              &lt;tibble [65 × 46]&gt;\n## 2 Germany                  &lt;tibble [65 × 46]&gt;\n## 3 France                   &lt;tibble [65 × 46]&gt;\n## 4 Greece                   &lt;tibble [65 × 46]&gt;\n## 5 Luxembourg               &lt;tibble [65 × 46]&gt;\n## 6 United States of America &lt;tibble [65 × 46]&gt;\n\nThis is why I love lists in R; we get a tibble where each element of the column data is itself a tibble. We can now apply any function that we know works on lists.\n\n\nWhat might be surprising though, is the object that is created by this code. Let’s take a look at plots:\n\nprint(plots)\n## # A tibble: 6 x 3\n##   country                  data               plot    \n##   &lt;ord&gt;                    &lt;list&gt;             &lt;list&gt;  \n## 1 Switzerland              &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 2 Germany                  &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 3 France                   &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 4 Greece                   &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 5 Luxembourg               &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 6 United States of America &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n\nAs dplyr::do()’s documentation tells us, the return values get stored inside a list. And this is exactly what we get back; a list of plots! Lists are a very flexible and useful class, and you cannot spell list without purrr (at least not when you’re a neRd).\n\n\nHere are the final lines that use purrr::map2() to save all these plots at once inside your working directory:\n\n\nUpdate: I have changed the code below which does not require your data frame to be ordered according to the variable country_list.\n\n# file_names &lt;- paste0(country_list, \".pdf\")\n\nmap2(paste0(plots$country, \".pdf\"), plots$plot, ggsave)\n\nAs I said before, if you do not re-order the countries inside the data frame, the names of the files and the plots will not match. Try running all the code without re-ordering, you’ll see!\n\n\nI hope you found this post useful. You can follow me on twitter for blog updates.\n\n\nUpdate: Many thanks to the readers of this article and for their useful suggestions. I love the R community; everyday I learn something new and useful!"
  },
  {
    "objectID": "posts/2018-11-01-nethack.html",
    "href": "posts/2018-11-01-nethack.html",
    "title": "From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack",
    "section": "",
    "text": "If someone told me a decade ago (back before I'd ever heard the term \"roguelike\") what I'd be doing today, I would have trouble believing this…Yet here we are. pic.twitter.com/N6Hh6A4tWl\n\n— Josh Ge (@GridSageGames) June 21, 2018"
  },
  {
    "objectID": "posts/2018-11-01-nethack.html#update-07-11-2018",
    "href": "posts/2018-11-01-nethack.html#update-07-11-2018",
    "title": "From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack",
    "section": "\nUpdate 07-11-2018\n",
    "text": "Update 07-11-2018\n\n\nThe {nethack} package currently on Github contains a sample of 6000 NetHack games played on the alt.org/nethack public server between April and November 2018. This data was kindly provided by @paxed. The tutorial in this blog post is still useful if you want to learn more about scraping with R and building a data package."
  },
  {
    "objectID": "posts/2018-11-01-nethack.html#abstract",
    "href": "posts/2018-11-01-nethack.html#abstract",
    "title": "From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack",
    "section": "\nAbstract\n",
    "text": "Abstract\n\n\nIn this post, I am going to show you how you can scrape tables from a website, and then create a package with the tidied data to share with the world. The data I am going to scrape comes from a NetHack public server (link). The data I discuss in this blog post is available in the {nethack} package I created and I will walk you through the process of releasing your package on CRAN. However, {nethack} is too large to be on CRAN (75 mb, while the maximum allowed is 5mb), so you can install it to play around with the data from github:\n\ndevtools::install_github(\"b-rodrigues/nethack\")\n\nAnd to use it:\n\nlibrary(nethack)\ndata(\"nethack\")\n\nThe data contains information on games played from 2001 to 2018; 322485 rows and 14 columns. I will analyze the data in a future blog post. This post focuses on getting and then sharing the data. By the way, all the content from the public server I scrape is under the CC BY 4.0 license.\n\n\nI built the package by using the very useful {devtools} package."
  },
  {
    "objectID": "posts/2018-11-01-nethack.html#introduction",
    "href": "posts/2018-11-01-nethack.html#introduction",
    "title": "From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nNetHack is a game released in 1987 that is still being played and developed today. NetHack is a roguelike game, meaning that is has procedurally generated dungeons and permadeath. If you die, you have to start over, and because the dungeons are procedurally generated, this means that you cannot learn the layout of the dungeons you explore or know when ennemies are going to attack or even what ennemies are going to attack. Ennemies are not the only thing that you have to be careful about; you can die from a lot of different events, as you will see in this post. Objects that you find, such as a silver ring, might be helpful in a run, but be cursed in the next run.\n\n\nThe latest version of the game, 3.6.1, was released on April 27th 2018, and this is how it looks like:\n\n\n\n\n\nThe graphics are… bare-bones to say the least. The game runs inside a terminal emulator and is available for any platform. The goal of NetHack is to explore a dungeon and go down every level until you find the Amulet of Yendor. Once you find this Amulet, you have to go all the way back upstairs, enter and fight your way through the Elemental Planes, enter the final Astral Plane, and then finally offer the Amulet of Yendor to your god to finish the game. Needless to say, NetHack is very difficult and players can go years without ever finishing the game.\n\n\nWhen you start an new game, you have to create a character, which can have several attributes. You have to choose a race (human, elf, orc, etc), a role (tourist, samurai, mage, etc) and an alignment (neutral, law, chaos) and these choices impact your base stats.\n\n\nIf you can’t get past the ASCII graphics, you can play NetHack with tileset:\n\n\n\n\n\nYou can install NetHack on your computer or you can play online on a public server, such as this one. There are several advantages when playing on a pubic server; the player does not have to install anyhing, and we data enthusiasts have access to a mine of information! For example, you can view the following table which contains data on all the games played on October 25th 2018. These tables start in the year 2001, and I am going to scrape the info from these tables, which will allow me to answer several questions. For instance, what is the floor most players die on? What kills most players? What role do players choose more often? I will explore this questions in a future blog post, but for now I will focus on scraping the data and realeasing it as a package to CRAN."
  },
  {
    "objectID": "posts/2018-11-01-nethack.html#scraping-the-data",
    "href": "posts/2018-11-01-nethack.html#scraping-the-data",
    "title": "From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack",
    "section": "\nScraping the data\n",
    "text": "Scraping the data\n\n\nTo scrape the data I wrote a big function that does several things:\n\nlibrary(\"tidyverse\")\nlibrary(\"rvest\")\n\n\nscrape_one_day &lt;- function(link){\n\n    convert_to_seconds &lt;- function(time_string){\n        time_numeric &lt;- time_string %&gt;%\n            str_split(\":\", simplify = TRUE) %&gt;%\n            as.numeric\n     \n     time_in_seconds &lt;- time_numeric * c(3600, 60, 1)\n     \n     if(is.na(time_in_seconds)){\n         time_in_seconds &lt;- 61\n     } else {\n         time_in_seconds &lt;- sum(time_in_seconds)\n     }\n     return(time_in_seconds)\n    }\n\n    Sys.sleep(1)\n\n    date &lt;- str_extract(link, \"\\\\d{8}\")\n\n    read_lines_slow &lt;- function(...){\n        Sys.sleep(1)\n        read_lines(...)\n    }\n    \n    page &lt;- read_html(link)\n\n        # Get links\n    dumplogs &lt;- page %&gt;% \n        html_nodes(xpath = '//*[(@id = \"perday\")]//td') %&gt;%\n        html_children() %&gt;%\n        html_attr(\"href\") %&gt;%\n        keep(str_detect(., \"dumplog\"))\n\n    # Get table\n    table &lt;- page %&gt;%\n        html_node(xpath = '//*[(@id = \"perday\")]') %&gt;%\n        html_table(fill = TRUE)\n\n    if(is_empty(dumplogs)){\n        print(\"dumplogs empty\")\n        dumplogs &lt;- rep(NA, nrow(table))\n    } else {\n        dumplogs &lt;- dumplogs\n    }\n    \n    final &lt;- table %&gt;%\n        janitor::clean_names() %&gt;%\n        mutate(dumplog_links = dumplogs)\n\n    print(paste0(\"cleaning data of date \", date))\n    \n    clean_final &lt;- final %&gt;%\n        select(-x) %&gt;%\n        rename(role = x_2,\n               race = x_3,\n               gender = x_4,\n               alignment = x_5) %&gt;%\n        mutate(time_in_seconds = map(time, convert_to_seconds)) %&gt;%\n        filter(!(death %in% c(\"quit\", \"escaped\")), time_in_seconds &gt; 60) %&gt;%\n        mutate(dumplog = map(dumplog_links, ~possibly(read_lines_slow, otherwise = NA)(.))) %&gt;%\n        mutate(time_in_seconds = ifelse(time_in_seconds == 61, NA, time_in_seconds))\n\n    saveRDS(clean_final, paste0(\"datasets/data_\", date, \".rds\"))\n\n}\n\nLet’s go through each part. The first part is a function that converts strings like “02:21:76” to seconds:\n\nconvert_to_seconds &lt;- function(time_string){\n    time_numeric &lt;- time_string %&gt;%\n        str_split(\":\", simplify = TRUE) %&gt;%\n        as.numeric\n \ntime_in_seconds &lt;- time_numeric * c(3600, 60, 1)\n \nif(is.na(time_in_seconds)){\n  time_in_seconds &lt;- 61\n  } else {\n    time_in_seconds &lt;- sum(time_in_seconds)\n    }\nreturn(time_in_seconds)\n}\n\nI will use this function on the column that gives the length of the run. However, before March 2008 this column is always empty, this is why I have the if()…else() statement at the end; if the time in seconds is NA, then I make it 61 seconds. I do this because I want to keep runs longer than 60 seconds, something I use filter() for later. But when filtering, if the condition returns NA (which happens when you do NA &gt; 60) then you get an error, and the function fails.\n\n\nThe website links I am going to scrape all have the date of the day the runs took place. I am going to keep this date because I will need to name the datasets I am going to write to disk:\n\ndate &lt;- str_extract(link, \"\\\\d{8}\")\n\nNext, I define this function:\n\nread_lines_slow &lt;- function(...){\n    Sys.sleep(1)\n    read_lines(...)\n}\n\nIt is a wrapper around the readr::read_lines() with a call to Sys.sleep(1). I will be scraping a lot of pages, so letting one second pass between each page will not overload the servers so much.\n\n\nI then read the link with read_html() and start by getting the links of the dumplogs:\n\npage &lt;- read_html(link)\n\n# Get links\ndumplogs &lt;- page %&gt;% \n    html_nodes(xpath = '//*[(@id = \"perday\")]//td') %&gt;%\n    html_children() %&gt;%\n    html_attr(\"href\") %&gt;%\n    keep(str_detect(., \"dumplog\"))\n\nYou might be wondering what are dumplogs. Take a look at this screenshot:\n\n\n\n\n\nWhen you click on those d’s, you land on a page like this one (I archived it to be sure that this link will not die). These logs contain a lot of info that I want to keep. To find the right xpath to scrape the links, //*[(@id = \"perday\")]//td, I used the SelectorGadget* extension for Chrome. First I chose the table:\n\n\n\n\n\nand then the links I am interested in:\n\n\n\n\n\nPutting them together, I get the right “xpath”. But just as with the time of the run, dumplogs are only available after a certain date. So in case the dumplogs column is empty, I relpace it with NA.\n\nif(is_empty(dumplogs)){\n    print(\"dumplogs empty\")\n    dumplogs &lt;- rep(NA, nrow(table))\n} else {\n    dumplogs &lt;- dumplogs\n}\n\nThe rest is quite simple:\n\n# Get table\ntable &lt;- page %&gt;%\n    html_node(xpath = '//*[(@id = \"perday\")]') %&gt;%\n    html_table(fill = TRUE)\n               \nfinal &lt;- table %&gt;%\n    janitor::clean_names() %&gt;%\n    mutate(dumplog_links = dumplogs)\n           \nprint(paste0(\"cleaning data of date \", date))\n\nI scrape the table, and then join the dumplog links to the table inside a new column called “dumplog_links”.\n\n\nBecause what follows is a long process, I print a message to let me know the progress of the scraping.\n\n\nNow the last part:\n\nclean_final &lt;- final %&gt;%\n    select(-x) %&gt;%\n    rename(role = x_2,\n           race = x_3,\n           gender = x_4,\n           alignment = x_5) %&gt;%\n    mutate(time_in_seconds = map(time, convert_to_seconds)) %&gt;%\n    filter(!(death %in% c(\"quit\", \"escaped\")), time_in_seconds &gt; 60) %&gt;%\n    mutate(dumplog = map(dumplog_links, ~possibly(read_lines_slow, otherwise = NA)(.))) %&gt;%\n    mutate(time_in_seconds = ifelse(time_in_seconds == 61, NA, time_in_seconds))\n\nI first remove and remane columns. Then I convert the “time” column into seconds and also remove runs that lasted less than 60 seconds or that ended either in “quit” (the player left the game) or “escaped” (the player left the dungeon and the game ended immediately). There are a lot of runs like that and they’re not interesting. Finally, and this is what takes long, I create a new list-column where each element is the contents of the dumplog for that run. I wrap read_lines_slow() around purrr::possibly() because dumplogs are missing for certains runs and when I try to read them I get an 404 error back. Getting such an error stops the whole process, so with purrr::possibly() I can specify that in that case I want NA back. Basically, a function wrapped inside purrr::possibly() never fails! Finally, if a game lasts for 61 seconds, I convert it back to NA (remember this was used to avoid having problems with the filter() function).\n\n\nFinally, I export what I scraped to disk:\n\nsaveRDS(clean_final, paste0(\"datasets/data_\", date, \".rds\"))\n\nThis is where I use the date; to name the data. This is really important because scraping takes a very long time, so if I don’t write the progress to disk as it goes, I might lose hours of work if my internet goes down, or if computer freezes or whatever.\n\n\nIn the lines below I build the links that I am going to scrape. They’re all of the form: https://alt.org/nethack/gamesday.php?date=YYYYMMDD so it’s quite easy to create a list of dates to scrape, for example, for the year 2017:\n\nlink &lt;- \"https://alt.org/nethack/gamesday.php?date=\"\n\ndates &lt;- seq(as.Date(\"2017/01/01\"), as.Date(\"2017/12/31\"), by = \"day\") %&gt;%\n    str_remove_all(\"-\")\n\nlinks &lt;- paste0(link, dates)\n\nNow I can easily scrape the data. To make extra sure that I will not have problems during the scraping process, for example if on a given day no games were played (and thus there is no table to scrape, which would result in an error) , I use the same trick as above by using purrr::possibly():\n\nmap(links, ~possibly(scrape_one_day, otherwise = NULL)(.))\n\nThe scraping process took a very long time. I scraped all the data by letting my computer run for three days!\n\n\nAfter this long process, I import all the .rds files into R:\n\npath_to_data &lt;- Sys.glob(\"datasets/*.rds\")\nnethack_data &lt;- map(path_to_data, readRDS)\n\nand take a look at one of them:\n\nnethack_data[[5812]] %&gt;% \n  View()\n\nLet’s convert the “score” column to integer. For this, I will need to convert strings that look like “12,232” to integers. I’ll write a short function to do this:\n\nto_numeric &lt;- function(string){\n  str_remove_all(string, \",\") %&gt;%\n    as.numeric\n}\nnethack_data &lt;- nethack_data %&gt;%\n  map(~mutate(., score = to_numeric(score)))\n\nLet’s merge the data into a single data frame:\n\nnethack_data &lt;- bind_rows(nethack_data)\n\nNow that I have a nice data frame, I will remove some columns and start the process of making a packages. I remove the columns that I created and that are now useless (such as the dumplog_links column).\n\nnethack_data &lt;- nethack_data %&gt;%\n  select(rank, score, name, time, turns, lev_max, hp_max, role, race, gender, alignment, death,\n         date, dumplog)\n\nExport this to .rds format, as it will be needed later:\n\nsaveRDS(nethack_data, \"nethack_data.rds\")"
  },
  {
    "objectID": "posts/2018-11-01-nethack.html#making-a-package-to-share-your-data-with-the-world",
    "href": "posts/2018-11-01-nethack.html#making-a-package-to-share-your-data-with-the-world",
    "title": "From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack",
    "section": "\nMaking a package to share your data with the world\n",
    "text": "Making a package to share your data with the world\n\n\nAs stated in the beginning of this post, I will walk you through the process of creating and releasing your package on CRAN. However, the data I scraped was too large to be made available as a CRAN package. But you can still get the data from Github (link is in the abstract at the beginning of the post).\n\n\nMaking a data package is a great way to learn how to make packages, because it is relatively easy to do (for example, you do not need to write unit tests). First, let’s start a new project in RStudio:\n\n\n\n\n\nThen select “R package”:\n\n\n\n\n\nThen name your package, create a git repository and then click on “Create Project”:\n\n\n\n\n\nRStudio wil open the hello.R script which you can now modify. You got to learn from the best, so I suggest that you modify hello.R by taking inspiration from the babynames package made by Hadley Wickham which you can find here. You do not need the first two lines, and can focus on lines 4 to 13. Then, rename the script to data.R. This is how {nethack}’s looks like:\n\n#' NetHack runs data.\n#'\n#' Data on NetHack runs scraped from https://alt.org/nethack/gamesday.php\n#'\n#' @format A data frame with 14 variables: \\code{rank}, \\code{score},\n#'   \\code{name}, \\code{time}, \\code{turns}, \\code{lev_max}, \\code{hp_max}, \\code{role}, \\code{race},\n#'   \\code{gender}, \\code{alignment}, \\code{death}, \\code{date} and \\code{dumplog}\n#' \\describe{\n#' \\item{rank}{The rank of the player on that day}\n#' \\item{score}{The score the player achieved on that run}\n#' \\item{name}{The name of the player}\n#' \\item{time}{The time the player took to finish the game}\n#' \\item{turns}{The number of turns the player played before finishing the game}\n#' \\item{lev_max}{First digit: the level the player died on; second digit: the deepest explored level}\n#' \\item{hp_max}{The maximum character health points the player achieved}\n#' \\item{role}{The role the player chose to play as}\n#' \\item{race}{The race the player chose to play as}\n#' \\item{gender}{The gender the playr chose to play as}\n#' \\item{alignement}{The alignement the playr chose to play as}\n#' \\item{death}{The reason of death of the character}\n#' \\item{date}{The date the game took place}\n#' \\item{dumplog}{The log of the end game; this is a list column}\n#' }\n\"nethack\"\n\nThe comments are special, the “#” is followed by a ’; these are special comments that will be parsed by roxygen2::roxygenise() and converted to documentation files.\n\n\nNext is the DESCRIPTION file. Here is how {nethack}’s looks like:\n\nPackage: nethack\nType: Package\nTitle: Data from the Video Game NetHack\nVersion: 0.1.0\nAuthors@R: person(\"Bruno André\", \"Rodrigues Coelho\", email = \"bruno@brodrigues.co\",\n                  role = c(\"aut\", \"cre\"))\nDescription: Data from NetHack runs played between 2001 to 2018 on \n    &lt;https://alt.org/nethack/&gt;, a NetHack public server.\nDepends: R (&gt;= 2.10)\nLicense: CC BY 4.0\nEncoding: UTF-8\nLazyData: true\nRoxygenNote: 6.1.0\n\nAdapt yours accordingly. I chose the license CC BY 4.0 because this was the licence under which the original data was published. It is also a good idea to add a Vignette:\n\ndevtools::use_vignette(\"the_nethack_package\")\n\nVignettes are very useful documentation with more details and examples.\n\n\nIt is also good practice to add the script that was used to scrape the data. Such scripts go into data-raw/. Create this folder with:\n\ndevtools::use_data_raw()\n\nThis creates the data-raw/ folder where I save the script that scrapes the data. Now is time to put the data in the package. Start by importing the data:\n\nnethack &lt;- readRDS(\"nethack_data.rds\")\n\nTo add the data to your package, you can use the following command:\n\ndevtools::use_data(nethack, compress = \"xz\")\n\nThis will create the data/ folder and put the data in there in the .rda format. I use the “compress” option to make the data smaller. You can now create the documentation by running:\n\nroxygen2::roxygenise()\n\nPay attention to the log messages: you might need to remove files (for example the documentation hello.R, under the folder man/).\n\n\nNow you can finaly run R CMD Check by clicking the Check button on the “Build” pane:\n\n\n\n\n\nThis will extensively check the package for ERRORS, WARNINGS and NOTES. You need to make sure that the check passes without any ERRORS or WARNINGS and try as much as possible to remove all NOTES too. If you cannot remove a NOTE, for example in my case the following:\n\nchecking installed package size ... NOTE\n  installed size is 169.7Mb\n  sub-directories of 1Mb or more:\n    data  169.6Mb\nR CMD check results\n0 errors | 0 warnings  | 1 note \n\nYou should document it in a new file called cran-comments.md:\n\n## Test environments\n* local openSUSE Tumbleweed install, R 3.5.1\n* win-builder (devel and release)\n\n## R CMD check results\nThere were no ERRORs or WARNINGs.\n\nThere was 1 NOTE:\n\n    *   installed size is 169.7Mb\nsub-directories of 1Mb or more:\n    data  169.6Mb\n\nThe dataset contains 17 years of NetHack games played, hence the size. This package will not be updated often (max once a year).\n\nOnce you have eliminated all errors and warnings, you are almost ready to go.\n\n\nYou need now to test the package on different platforms. This depends a bit on the system you run, for me, because I run openSUSE (a GNU+Linux distribution) I have to test on Windows. This can be done with:\n\n devtools::build_win(version = \"R-release\")\n\nand:\n\n devtools::build_win(version = \"R-devel\")\n\nExplain that you have tested your package on several platforms in the cran-comments.md file.\n\n\nFinally you can add a README.md and a NEWS.md file and start the process of publishing the package on CRAN:\n\ndevtools:release()\n\nIf you want many more details than what you can find in this blog post, I urge you to read “R Packages” by Hadley Wickham, which you can read for free here."
  },
  {
    "objectID": "posts/2018-11-25-tidy_cv.html#introduction",
    "href": "posts/2018-11-25-tidy_cv.html#introduction",
    "title": "A tutorial on tidy cross-validation with R",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nThis blog posts will use several packages from the {tidymodels} collection of packages, namely {recipes}, {rsample} and {parsnip} to train a random forest the tidy way. I will also use {mlrMBO} to tune the hyper-parameters of the random forest."
  },
  {
    "objectID": "posts/2018-11-25-tidy_cv.html#set-up",
    "href": "posts/2018-11-25-tidy_cv.html#set-up",
    "title": "A tutorial on tidy cross-validation with R",
    "section": "\nSet up\n",
    "text": "Set up\n\n\nLet’s load the needed packages:\n\nlibrary(\"tidyverse\")\nlibrary(\"tidymodels\")\nlibrary(\"parsnip\")\nlibrary(\"brotools\")\nlibrary(\"mlbench\")\n\nLoad the data, included in the {mlrbench} package:\n\ndata(\"BostonHousing2\")\n\nI will train a random forest to predict the housing price, which is the cmedv column:\n\nhead(BostonHousing2)\n##         town tract      lon     lat medv cmedv    crim zn indus chas   nox\n## 1     Nahant  2011 -70.9550 42.2550 24.0  24.0 0.00632 18  2.31    0 0.538\n## 2 Swampscott  2021 -70.9500 42.2875 21.6  21.6 0.02731  0  7.07    0 0.469\n## 3 Swampscott  2022 -70.9360 42.2830 34.7  34.7 0.02729  0  7.07    0 0.469\n## 4 Marblehead  2031 -70.9280 42.2930 33.4  33.4 0.03237  0  2.18    0 0.458\n## 5 Marblehead  2032 -70.9220 42.2980 36.2  36.2 0.06905  0  2.18    0 0.458\n## 6 Marblehead  2033 -70.9165 42.3040 28.7  28.7 0.02985  0  2.18    0 0.458\n##      rm  age    dis rad tax ptratio      b lstat\n## 1 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n## 2 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n## 3 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n## 4 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n## 5 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n## 6 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n\nOnly keep relevant columns:\n\nboston &lt;- BostonHousing2 %&gt;% \n    select(-medv, -town, -lon, -lat) %&gt;% \n    rename(price = cmedv)\n\nI remove town, lat and lon because the information contained in the column tract is enough.\n\n\nTo train and evaluate the model’s performance, I split the data in two. One data set, which I call the training set, will be further split into two down below. I won’t touch the second data set, the test set, until the very end.\n\ntrain_test_split &lt;- initial_split(boston, prop = 0.9)\n\nhousing_train &lt;- training(train_test_split)\n\nhousing_test &lt;- testing(train_test_split)\n\nI want to train a random forest to predict price of houses, but random forests have so-called hyperparameters, which are parameters that cannot be estimated, or learned, from the data. Instead, these parameters have to be chosen by the analyst. In order to choose them, you can use values from the literature that seemed to have worked well (like is done in Macro-econometrics) or you can further split the train set into two, create a grid of hyperparameter, train the model on one part of the data for all values of the grid, and compare the predictions of the models on the second part of the data. You then stick with the model that performed the best, for example, the model with lowest RMSE. The thing is, you can’t estimate the true value of the RMSE with only one value. It’s like if you wanted to estimate the height of the population by drawing one single observation from the population. You need a bit more observations. To approach the true value of the RMSE for a give set of hyperparameters, instead of doing one split, I’ll do 30. I then compute the average RMSE, which implies training 30 models for each combination of the values of the hyperparameters I am interested in.\n\n\nFirst, let’s split the training data again, using the mc_cv() function from {rsample} package. This function implements Monte Carlo cross-validation:\n\nvalidation_data &lt;- mc_cv(housing_train, prop = 0.9, times = 30)\n\nWhat does validation_data look like?\n\nvalidation_data\n## # # Monte Carlo cross-validation (0.9/0.1) with 30 resamples  \n## # A tibble: 30 x 2\n##    splits           id        \n##    &lt;list&gt;           &lt;chr&gt;     \n##  1 &lt;split [411/45]&gt; Resample01\n##  2 &lt;split [411/45]&gt; Resample02\n##  3 &lt;split [411/45]&gt; Resample03\n##  4 &lt;split [411/45]&gt; Resample04\n##  5 &lt;split [411/45]&gt; Resample05\n##  6 &lt;split [411/45]&gt; Resample06\n##  7 &lt;split [411/45]&gt; Resample07\n##  8 &lt;split [411/45]&gt; Resample08\n##  9 &lt;split [411/45]&gt; Resample09\n## 10 &lt;split [411/45]&gt; Resample10\n## # … with 20 more rows\n\nLet’s look further down:\n\nvalidation_data$splits[[1]]\n## &lt;411/45/456&gt;\n\nThe first value is the number of rows of the first set, the second value of the second, and the third was the original amount of values in the training data, before splitting again.\n\n\nHow should we call these two new data sets? The author of {rsample}, Max Kuhn, talks about the analysis and the assessment sets:\n\n\n\nrsample convention for now but I intend on using it everywhere. Reusing training and testing is insane.\n\n— Max Kuhn (@topepos) November 24, 2018\n\n\n\nNow, in order to continue I need pre-process the data. I will do this in three steps. The first and the second step are used to center and scale the numeric variables and the third step converts character and factor variables to dummy variables. This is needed because I will train a random forest, which cannot handle factor variables directly. Let’s define a recipe to do that, and start by pre-processing the testing set. I write a wrapper function around the recipe, because I will need to apply this recipe to various data sets:\n\nsimple_recipe &lt;- function(dataset){\n    recipe(price ~ ., data = dataset) %&gt;%\n        step_center(all_numeric()) %&gt;%\n        step_scale(all_numeric()) %&gt;%\n        step_dummy(all_nominal())\n}\n\nOnce the recipe is defined, I can use the prep() function, which estimates the parameters from the data which are needed to process the data. For example, for centering, prep() estimates the mean which will then be subtracted from the variables. With bake() the estimates are then applied on the data:\n\ntesting_rec &lt;- prep(simple_recipe(housing_test), testing = housing_test)\n\ntest_data &lt;- bake(testing_rec, newdata = housing_test)\n## Warning: Please use `new_data` instead of `newdata` with `bake`. \n## In recipes versions &gt;= 0.1.4, this will cause an error.\n\nIt is important to split the data before using prep() and bake(), because if not, you will use observations from the test set in the prep() step, and thus introduce knowledge from the test set into the training data. This is called data leakage, and must be avoided. This is why it is necessary to first split the training data into an analysis and an assessment set, and then also pre-process these sets separately. However, the validation_data object cannot now be used with recipe(), because it is not a dataframe. No worries, I simply need to write a function that extracts the analysis and assessment sets from the validation_data object, applies the pre-processing, trains the model, and returns the RMSE. This will be a big function, at the center of the analysis.\n\n\nBut before that, let’s run a simple linear regression, as a benchmark. For the linear regression, I will not use any CV, so let’s pre-process the training set:\n\ntrainlm_rec &lt;- prep(simple_recipe(housing_train), testing = housing_train)\n\ntrainlm_data &lt;- bake(trainlm_rec, newdata = housing_train)\n## Warning: Please use `new_data` instead of `newdata` with `bake`. \n## In recipes versions &gt;= 0.1.4, this will cause an error.\nlinreg_model &lt;- lm(price ~ ., data = trainlm_data)\n\nbroom::augment(linreg_model, newdata = test_data) %&gt;% \n    rmse(price, .fitted)\n## # A tibble: 1 x 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rmse    standard       0.438\n\nbroom::augment() adds the predictions to the test_data in a new column, .fitted. I won’t use this trick with the random forest, because there is no augment() method for random forests from the {ranger} which I’ll use. I’ll add the predictions to the data myself.\n\n\nOk, now let’s go back to the random forest and write the big function:\n\nmy_rf &lt;- function(mtry, trees, split, id){\n    \n    analysis_set &lt;- analysis(split)\n    \n    analysis_prep &lt;- prep(simple_recipe(analysis_set), training = analysis_set)\n    \n    analysis_processed &lt;- bake(analysis_prep, newdata = analysis_set)\n    \n    model &lt;- rand_forest(mtry = mtry, trees = trees) %&gt;%\n        set_engine(\"ranger\", importance = 'impurity') %&gt;%\n        fit(price ~ ., data = analysis_processed)\n\n    assessment_set &lt;- assessment(split)\n    \n    assessment_prep &lt;- prep(simple_recipe(assessment_set), testing = assessment_set)\n    \n    assessment_processed &lt;- bake(assessment_prep, newdata = assessment_set)\n\n    tibble::tibble(\"id\" = id,\n        \"truth\" = assessment_processed$price,\n        \"prediction\" = unlist(predict(model, new_data = assessment_processed)))\n}\n\nThe rand_forest() function is available from the {parsnip} package. This package provides an unified interface to a lot of other machine learning packages. This means that instead of having to learn the syntax of range() and randomForest() and, and… you can simply use the rand_forest() function and change the engine argument to the one you want (ranger, randomForest, etc).\n\n\nLet’s try this function:\n\nresults_example &lt;- map2_df(.x = validation_data$splits,\n                           .y = validation_data$id,\n                           ~my_rf(mtry = 3, trees = 200, split = .x, id = .y))\nhead(results_example)\n## # A tibble: 6 x 3\n##   id           truth prediction\n##   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Resample01  0.0235    -0.104 \n## 2 Resample01 -0.135     -0.0906\n## 3 Resample01 -0.378     -0.158 \n## 4 Resample01 -0.232      0.0623\n## 5 Resample01 -0.0859     0.0173\n## 6 Resample01  0.169      0.303\n\nI can now compute the RMSE when mtry = 3 and trees = 200:\n\nresults_example %&gt;%\n    group_by(id) %&gt;%\n    rmse(truth, prediction) %&gt;%\n    summarise(mean_rmse = mean(.estimate)) %&gt;%\n    pull\n## [1] 0.4319164\n\nThe random forest has already lower RMSE than the linear regression. The goal now is to lower this RMSE by tuning the mtry and trees hyperparameters. For this, I will use Bayesian Optimization methods implemented in the {mlrMBO} package."
  },
  {
    "objectID": "posts/2018-11-25-tidy_cv.html#bayesian-hyperparameter-optimization",
    "href": "posts/2018-11-25-tidy_cv.html#bayesian-hyperparameter-optimization",
    "title": "A tutorial on tidy cross-validation with R",
    "section": "\nBayesian hyperparameter optimization\n",
    "text": "Bayesian hyperparameter optimization\n\n\nI will re-use the code from above, and define a function that does everything from pre-processing to returning the metric I want to minimize by tuning the hyperparameters, the RMSE:\n\ntuning &lt;- function(param, validation_data){\n\n    mtry &lt;- param[1]\n    trees &lt;- param[2]\n\n    results &lt;- purrr::map2_df(.x = validation_data$splits,\n                       .y = validation_data$id,\n                       ~my_rf(mtry = mtry, trees = trees, split = .x, id = .y))\n\n    results %&gt;%\n        group_by(id) %&gt;%\n        rmse(truth, prediction) %&gt;%\n        summarise(mean_rmse = mean(.estimate)) %&gt;%\n        pull\n}\n\nThis is exactly the code from before, but it now returns the RMSE. Let’s try the function with the values from before:\n\ntuning(c(3, 200), validation_data)\n## [1] 0.4330951\n\nLet’s also plot the value of RMSE for mtry = 3 and trees from 200 to 300. This takes some time, because I need to evaluate this costly function 100 times. If evaluating the function was cheap, I could have made a 3D plot by varying values of mtry too, but then again if evaluating the function was cheap, I would run an exhaustive grid search to find the hyperparameters instead of using Bayesian optimization.\n\nplot_points &lt;- crossing(\"mtry\" = 3, \"trees\" = seq(200, 300))\n\nplot_data &lt;- plot_points %&gt;% \n    mutate(value = map_dbl(seq(200, 300), ~tuning(c(3, .), validation_data)))\nplot_data %&gt;% \n    ggplot(aes(y = value, x = trees)) + \n    geom_line(colour = \"#82518c\") + \n    theme_blog() +\n    ggtitle(\"RMSE for mtry = 3\")\n\n\n\n\nFor mtry = 3 the minimum seems to lie around 255. The function to minimize is not smooth at all.\n\n\nI now follow the code that can be found in the arxiv paper to run the optimization. I think I got the gist of the paper, but I did not understand everything yet. For now, I am still experimenting with the library at the moment, but from what I understand, a simpler model, called the surrogate model, is used to look for promising points and to evaluate the value of the function at these points. This seems somewhat similar (in spirit) to the Indirect Inference method as described in Gourieroux, Monfort, Renault.\n\n\nLet’s first load the package and create the function to optimize:\n\nlibrary(\"mlrMBO\")\nfn &lt;- makeSingleObjectiveFunction(name = \"tuning\",\n                                 fn = tuning,\n                                 par.set = makeParamSet(makeIntegerParam(\"x1\", lower = 3, upper = 8),\n                                                        makeIntegerParam(\"x2\", lower = 50, upper = 500)))\n\nThis function is based on the function I defined before. The parameters to optimize are also defined as are their bounds. I will look for mtry between the values of 3 and 8, and trees between 50 and 500.\n\n\nNow comes the part I didn’t quite get.\n\n# Create initial random Latin Hypercube Design of 10 points\nlibrary(lhs)# for randomLHS\ndes &lt;- generateDesign(n = 5L * 2L, getParamSet(fn), fun = randomLHS)\n\nI think this means that these 10 points are the points used to start the whole process. I did not understand why they have to be sampled from a hypercube, but ok. Then I choose the surrogate model, a random forest too, and predict the standard error. Here also, I did not quite get why the standard error can be an option.\n\n# Specify kriging model with standard error estimation\nsurrogate &lt;- makeLearner(\"regr.ranger\", predict.type = \"se\", keep.inbag = TRUE)\n\nHere I define some options:\n\n# Set general controls\nctrl &lt;- makeMBOControl()\nctrl &lt;- setMBOControlTermination(ctrl, iters = 10L)\nctrl &lt;- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())\n\nAnd this is the optimization part:\n\n# Start optimization\nresult &lt;- mbo(fn, des, surrogate, ctrl, more.args = list(\"validation_data\" = validation_data))\nresult\n## Recommended parameters:\n## x1=6; x2=381\n## Objective: y = 0.393\n## \n## Optimization path\n## 10 + 10 entries in total, displaying last 10 (or less):\n##    x1  x2         y dob eol error.message exec.time            ei\n## 11  6 370 0.3943479   1  NA          &lt;NA&gt;     8.913 -3.134568e-05\n## 12  6 362 0.3950402   2  NA          &lt;NA&gt;     8.844 -2.987934e-05\n## 13  6 373 0.3939587   3  NA          &lt;NA&gt;     8.939 -2.259674e-05\n## 14  6 394 0.3962875   4  NA          &lt;NA&gt;     9.342 -7.427682e-06\n## 15  6 368 0.3944954   5  NA          &lt;NA&gt;     8.760 -4.121337e-06\n## 16  6 378 0.3938796   6  NA          &lt;NA&gt;     8.949 -4.503591e-07\n## 17  6 381 0.3934176   7  NA          &lt;NA&gt;     9.109 -1.141853e-06\n## 18  6 380 0.3948077   8  NA          &lt;NA&gt;     9.026 -4.718394e-08\n## 19  6 381 0.3932636   9  NA          &lt;NA&gt;     9.022 -9.801395e-08\n## 20  6 383 0.3953004  10  NA          &lt;NA&gt;     9.184 -1.579619e-09\n##    error.model train.time prop.type propose.time           se      mean\n## 11        &lt;NA&gt;      0.014 infill_ei        0.449 0.0010924600 0.3955131\n## 12        &lt;NA&gt;      0.012 infill_ei        0.458 0.0007415920 0.3948705\n## 13        &lt;NA&gt;      0.012 infill_ei        0.460 0.0006116756 0.3947185\n## 14        &lt;NA&gt;      0.012 infill_ei        0.729 0.0003104694 0.3943572\n## 15        &lt;NA&gt;      0.023 infill_ei        0.444 0.0003446061 0.3945085\n## 16        &lt;NA&gt;      0.013 infill_ei        0.458 0.0002381887 0.3944642\n## 17        &lt;NA&gt;      0.013 infill_ei        0.492 0.0002106454 0.3943200\n## 18        &lt;NA&gt;      0.013 infill_ei        0.516 0.0002093524 0.3940764\n## 19        &lt;NA&gt;      0.014 infill_ei        0.756 0.0002481260 0.3941597\n## 20        &lt;NA&gt;      0.013 infill_ei        0.483 0.0001687982 0.3939285\n\nSo the recommended parameters are 6 for mtry and 381 for trees. The value of the RMSE is lower than before, and equals 0.393. Let’s now train the random forest on the training data with this values. First, I pre-process the training data:\n\ntraining_rec &lt;- prep(simple_recipe(housing_train), testing = housing_train)\n\ntrain_data &lt;- bake(training_rec, newdata = housing_train)\n## Warning: Please use `new_data` instead of `newdata` with `bake`. \n## In recipes versions &gt;= 0.1.4, this will cause an error.\n\nLet’s now train our final model and predict the prices:\n\nfinal_model &lt;- rand_forest(mtry = 6, trees = 381) %&gt;%\n        set_engine(\"ranger\", importance = 'impurity') %&gt;%\n        fit(price ~ ., data = train_data)\n\nprice_predict &lt;- predict(final_model, new_data = select(test_data, -price))\n\nLet’s transform the data back and compare the predicted prices to the true ones visually:\n\ncbind(price_predict * sd(housing_train$price) + mean(housing_train$price), \n      housing_test$price)\n##        .pred housing_test$price\n## 1  34.811111               34.7\n## 2  20.591304               22.9\n## 3  19.463920               18.9\n## 4  20.321990               21.7\n## 5  19.063132               17.5\n## 6  15.969125               14.5\n## 7  18.203023               15.6\n## 8  17.139943               13.9\n## 9  21.393329               24.2\n## 10 27.508482               25.0\n## 11 24.030162               24.1\n## 12 21.222857               21.2\n## 13 23.052677               22.2\n## 14 20.303233               19.3\n## 15 21.134554               21.7\n## 16 22.913097               18.5\n## 17 20.029506               18.8\n## 18 18.045923               16.2\n## 19 17.321006               13.3\n## 20 18.201785               13.4\n## 21 29.928316               32.5\n## 22 24.339983               26.4\n## 23 45.518316               42.3\n## 24 29.551251               26.7\n## 25 26.513473               30.1\n## 26 42.984738               46.7\n## 27 43.513001               48.3\n## 28 25.436146               23.3\n## 29 21.766247               24.3\n## 30 36.328740               36.0\n## 31 32.830061               31.0\n## 32 38.736098               35.2\n## 33 31.573311               32.0\n## 34 19.847848               19.4\n## 35 23.401032               23.1\n## 36 22.000914               19.4\n## 37 20.155696               18.7\n## 38 21.342003               22.6\n## 39 20.846330               19.9\n## 40 13.752108               13.8\n## 41 12.499064               13.1\n## 42 15.019987               16.3\n## 43  8.489851                7.2\n## 44  7.803981               10.4\n## 45 18.629488               20.8\n## 46 14.645669               14.3\n## 47 15.094423               15.2\n## 48 20.470057               17.7\n## 49 15.147170               13.3\n## 50 15.880035               13.6\n\nLet’s now compute the RMSE:\n\ntibble::tibble(\"truth\" = test_data$price,\n        \"prediction\" = unlist(price_predict)) %&gt;% \n    rmse(truth, prediction)\n## # A tibble: 1 x 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rmse    standard       0.327\n\nVery nice."
  },
  {
    "objectID": "posts/2016-06-21-careful-with-trycatch.html",
    "href": "posts/2016-06-21-careful-with-trycatch.html",
    "title": "Careful with tryCatch",
    "section": "",
    "text": "tryCatch is one of the functions that allows the users to handle errors in a simple way. With it, you can do things like: if(error), then(do this).\n\n\nTake the following example:\n\nsqrt(\"a\")\nError in sqrt(\"a\") : non-numeric argument to mathematical function\n\nNow maybe you’d want something to happen when such an error happens. You can achieve that with tryCatch:\n\ntryCatch(sqrt(\"a\"), error=function(e) print(\"You can't take the square root of a character, silly!\"))\n## [1] \"You can't take the square root of a character, silly!\"\n\nWhy am I interested in tryCatch?\n\n\nI am currently working with dates, specifically birthdays of people in my data sets. For a given mother, the birthday of her child is given in three distinct columns: a column for the child’s birth year, birth month and birth day respectively. I’ve wanted to put everything in a single column and convert the birthday to unix time (I have a very good reason to do that, but I won’t bore you with the details).\n\n\nLet’s create some data:\n\nmother &lt;- as.data.frame(list(month=12, day=1, year=1988))\n\nIn my data, there’s a lot more columns of course, such as the mother’s wage, education level, etc, but for illustration purposes, this is all that’s needed.\n\n\nNow, to create this birthday column:\n\nmother$birth1 &lt;- as.POSIXct(paste0(as.character(mother$year), \n                                   \"-\", as.character(mother$month), \n                                   \"-\", as.character(mother$day)), \n                            origin=\"1970-01-01\")\n\nand to convert it to unix time:\n\nmother$birth1 &lt;- as.numeric(as.POSIXct(paste0(as.character(mother$year), \n                                              \"-\", as.character(mother$month), \n                                              \"-\", as.character(mother$day)),\n                                       origin=\"1970-01-01\"))\n\nprint(mother)\n##   month day year    birth1\n## 1    12   1 1988 596934000\n\nNow let’s see what happens in this other example here:\n\nmother2 &lt;- as.data.frame(list(month=2, day=30, year=1988))\n\nmother2$birth1 &lt;- as.POSIXct(paste0(as.character(mother2$year), \n                                    \"-\", as.character(mother2$month), \n                                    \"-\", as.character(mother2$day)), \n                             origin=\"1970-01-01\")\n\nThis is what happens:\n\nError in as.POSIXlt.character(x, tz, ...) : \n  character string is not in a standard unambiguous format\n\nThis error is to be expected; there is no 30th of February! It turns out that in some rare cases, weird dates like this exist in my data. Probably some encoding errors. Not a problem I thought, I could use tryCatch and return NA in the case of an error.\n\nmother2 &lt;- as.data.frame(list(month=2, day=30, year=1988))\n\nmother2$birth1 &lt;- tryCatch(as.POSIXct(paste0(as.character(mother2$year), \n                                    \"-\", as.character(mother2$month), \n                                    \"-\", as.character(mother2$day)), \n                             origin=\"1970-01-01\"), error=function(e) NA)\n\nprint(mother2)\n##   month day year birth1\n## 1     2  30 1988     NA\n\nPretty great, right? Well, no. Take a look at what happens in this case:\n\nmother &lt;- as.data.frame(list(month=c(12, 2), day=c(1, 30), year=c(1988, 1987)))\nprint(mother)\n##   month day year\n## 1    12   1 1988\n## 2     2  30 1987\n\nWe’d expect to have a correct date for the first mother and an NA for the second. However, this is what happens\n\nmother$birth1 &lt;- tryCatch(as.POSIXct(paste0(as.character(mother$year), \n                                    \"-\", as.character(mother$month), \n                                    \"-\", as.character(mother$day)), \n                             origin=\"1970-01-01\"), error=function(e) NA)\n\nprint(mother)\n##   month day year birth1\n## 1    12   1 1988     NA\n## 2     2  30 1987     NA\n\nAs you can see, we now have an NA for both mothers! That’s actually to be expected. Indeed, this little example illustrates it well:\n\nsqrt(c(4, 9, \"haha\"))\nError in sqrt(c(4, 9, \"haha\")) : \n  non-numeric argument to mathematical function\n\nBut you’d like to have this:\n\n[1]  2  3 NA\n\nSo you could make the same mistake as myself and use tryCatch:\n\ntryCatch(sqrt(c(4, 9, \"haha\")), error=function(e) NA)\n## [1] NA\n\nBut you only get NA in return. That’s actually completely normal, but it took me off-guard and I spent quite some time to figure out what was happening. Especially because I had written unit tests to test my function create_birthdays() that was doing the above computations and all tests were passing! The problem was that in my tests, I only had a single individual, so for a wrong date, having NA for this individual was expected behaviour. But in a panel, only some individuals have a weird date like the 30th of February, but because of those, the whole column was filled with NA’s! What I’m doing now is trying to either remove these weird birthdays (there are mothers whose children were born on the 99-99-9999. Documentation is lacking, but this probably means missing value), or tyring to figure out how to only get NA’s for the “weird” dates. I guess that the answer lies with dplyr’s group_by() and mutate() to compute this birthdays for each individual separately."
  },
  {
    "objectID": "posts/2018-06-10-scraping_pdfs.html",
    "href": "posts/2018-06-10-scraping_pdfs.html",
    "title": "Getting data from pdfs using the pdftools package",
    "section": "",
    "text": "It is often the case that data is trapped inside pdfs, but thankfully there are ways to extract it from the pdfs. A very nice package for this task is pdftools (Github link) and this blog post will describe some basic functionality from that package.\n\n\nFirst, let’s find some pdfs that contain interesting data. For this post, I’m using the diabetes country profiles from the World Health Organization. You can find them here. If you open one of these pdfs, you are going to see this:\n\n\n\n\n\nhttp://www.who.int/diabetes/country-profiles/lux_en.pdf?ua=1\n\n\n\n\nI’m interested in this table here in the middle:\n\n\n\n\n\nhttp://www.who.int/diabetes/country-profiles/lux_en.pdf?ua=1\n\n\n\n\nI want to get the data from different countries, put it all into a nice data frame and make a simple plot.\n\n\nLet’s first start by loading the needed packages:\n\nlibrary(\"pdftools\")\nlibrary(\"glue\")\nlibrary(\"tidyverse\")\n## ── Attaching packages ────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──\n## ✔ ggplot2 2.2.1     ✔ purrr   0.2.5\n## ✔ tibble  1.4.2     ✔ dplyr   0.7.5\n## ✔ tidyr   0.8.1     ✔ stringr 1.3.1\n## ✔ readr   1.1.1     ✔ forcats 0.3.0\n## ── Conflicts ───────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::collapse() masks glue::collapse()\n## ✖ dplyr::filter()   masks stats::filter()\n## ✖ dplyr::lag()      masks stats::lag()\nlibrary(\"ggthemes\")\n\ncountry &lt;- c(\"lux\", \"fra\", \"deu\", \"usa\", \"prt\", \"gbr\")\n\nurl &lt;- \"http://www.who.int/diabetes/country-profiles/{country}_en.pdf?ua=1\"\n\nThe first 4 lines load the needed packages for this exercise: pdftools is the package that I described in the beginning of the post, glue is optional but offers a nice alternative to the paste() and paste0() functions. Take a closer look at the url: you’ll see that I wrote {country}. This is not in the original links; the original links look like this (for example for the USA):\n\n\"http://www.who.int/diabetes/country-profiles/usa_en.pdf?ua=1\"\n\nSo because I’m interested in several countries, I created a vector with the country codes of the countries I’m interested in. Now, using the glue() function, something magical happens:\n\n(urls &lt;- glue(url))\n## http://www.who.int/diabetes/country-profiles/lux_en.pdf?ua=1\n## http://www.who.int/diabetes/country-profiles/fra_en.pdf?ua=1\n## http://www.who.int/diabetes/country-profiles/deu_en.pdf?ua=1\n## http://www.who.int/diabetes/country-profiles/usa_en.pdf?ua=1\n## http://www.who.int/diabetes/country-profiles/prt_en.pdf?ua=1\n## http://www.who.int/diabetes/country-profiles/gbr_en.pdf?ua=1\n\nThis created a vector with all the links where {country} is replaced by each of the codes contained in the variable country.\n\n\nI use the same trick to create the names of the pdfs that I will download:\n\npdf_names &lt;- glue(\"report_{country}.pdf\")\n\nAnd now I can download them:\n\nwalk2(urls, pdf_names, download.file, mode = \"wb\")\n\nwalk2() is a function from the purrr package that is similar to map2(). You could use map2() for this, but walk2() is cleaner here, because dowload.file() is a function with a so-called side effect; it downloads files. map2() is used for functions without side effects.\n\n\nNow, I can finally use the pdf_text() function from the pdftools function to get the text from the pdfs:\n\nraw_text &lt;- map(pdf_names, pdf_text)\n\nraw_text is a list of where each element is the text from one of the pdfs. Let’s take a look:\n\nstr(raw_text)\n## List of 6\n##  $ : chr \"Luxembourg                                                                                                     \"| __truncated__\n##  $ : chr \"France                                                                                                         \"| __truncated__\n##  $ : chr \"Germany                                                                                                        \"| __truncated__\n##  $ : chr \"United States Of America                                                                                       \"| __truncated__\n##  $ : chr \"Portugal                                                                                                       \"| __truncated__\n##  $ : chr \"United Kingdom                                                                                                 \"| __truncated__\n\nLet’s take a look at one of these elements, which is nothing but a very long character:\n\nraw_text[[1]]\n## [1] \"Luxembourg                                                                                                                                          Total population: 567 000\\n                                                                                                                                                         Income group: High\\nMortality\\nNumber of diabetes deaths                                                                     Number of deaths attributable to high blood glucose\\n                                                                     males         females                                                            males       females\\nages 30–69                                                           &lt;100            &lt;100     ages 30–69                                              &lt;100          &lt;100\\nages 70+                                                             &lt;100            &lt;100     ages 70+                                                &lt;100          &lt;100\\nProportional mortality (% of total deaths, all ages)                                          Trends in age-standardized prevalence of diabetes\\n                    Communicable,\\n                   maternal, perinatal              Injuries                                                    35%\\n                    and nutritional                   6%                     Cardiovascular\\n                      conditions                                               diseases\\n                          6%                                                      33%\\n                                                                                                                30%\\n                                                                                                                25%\\n                                                                                              % of population\\n               Other NCDs\\n                  16%                                                                                           20%\\n                                     No data available                                                          15%           No data available\\n              Diabetes                                                                                          10%\\n                 2%\\n                                                                                                                5%\\n                   Respiratory\\n                    diseases\\n                       6%                                                                                       0%\\n                                                           Cancers\\n                                                            31%\\n                                                                                                                                  males     females\\nPrevalence of diabetes and related risk factors\\n                                                                                                                      males               females               total\\nDiabetes                                                                                                              8.3%                 5.3%                 6.8%\\nOverweight                                                                                                            70.7%               51.5%                61.0%\\nObesity                                                                                                               28.3%               21.3%                24.8%\\nPhysical inactivity                                                                                                   28.2%               31.7%                30.0%\\nNational response to diabetes\\nPolicies, guidelines and monitoring\\nOperational policy/strategy/action plan for diabetes                                                                                                ND\\nOperational policy/strategy/action plan to reduce overweight and obesity                                                                            ND\\nOperational policy/strategy/action plan to reduce physical inactivity                                                                               ND\\nEvidence-based national diabetes guidelines/protocols/standards                                                                                     ND\\nStandard criteria for referral of patients from primary care to higher level of care                                                                ND\\nDiabetes registry                                                                                                                                   ND\\nRecent national risk factor survey in which blood glucose was measured                                                                              ND\\nAvailability of medicines, basic technologies and procedures in the public health sector\\nMedicines in primary care facilities                                                          Basic technologies in primary care facilities\\nInsulin                                                                               ND      Blood glucose measurement                                             ND\\nMetformin                                                                             ND      Oral glucose tolerance test                                           ND\\nSulphonylurea                                                                         ND      HbA1c test                                                            ND\\nProcedures                                                                                    Dilated fundus examination                                            ND\\nRetinal photocoagulation                                                              ND      Foot vibration perception by tuning fork                              ND\\nRenal replacement therapy by dialysis                                                 ND      Foot vascular status by Doppler                                       ND\\nRenal replacement therapy by transplantation                                          ND      Urine strips for glucose and ketone measurement                       ND\\nND = country did not respond to country capacity survey\\n〇 = not generally available   ● = generally available\\nWorld Health Organization – Diabetes country profiles, 2016.\\n\"\n\nAs you can see, this is a very long character string with some line breaks (the \"\" character). So first, we need to split this string into a character vector by the \"\" character. Also, it might be difficult to see, but the table starts at the line with the following string: \"Prevalence of diabetes\" and ends with \"National response to diabetes\". Also, we need to get the name of the country from the text and add it as a column. As you can see, a whole lot of operations are needed, so what I do is put all these operations into a function that I will apply to each element of raw_text:\n\nclean_table &lt;- function(table){\n    table &lt;- str_split(table, \"\\n\", simplify = TRUE)\n    country_name &lt;- table[1, 1] %&gt;% \n        stringr::str_squish() %&gt;% \n        stringr::str_extract(\".+?(?=\\\\sTotal)\")\n    table_start &lt;- stringr::str_which(table, \"Prevalence of diabetes\")\n    table_end &lt;- stringr::str_which(table, \"National response to diabetes\")\n    table &lt;- table[1, (table_start +1 ):(table_end - 1)]\n    table &lt;- str_replace_all(table, \"\\\\s{2,}\", \"|\")\n    text_con &lt;- textConnection(table)\n    data_table &lt;- read.csv(text_con, sep = \"|\")\n    colnames(data_table) &lt;- c(\"Condition\", \"Males\", \"Females\", \"Total\")\n    dplyr::mutate(data_table, Country = country_name)\n}\n\nI advise you to go through all these operations and understand what each does. However, I will describe some of the lines, such as this one:\n\nstringr::str_extract(\".+?(?=\\\\sTotal)\")\n\nThis uses a very bizarre looking regular expression: \".+?(?=\\sTotal)\". This extracts everything before a space, followed by the string \"Total\". This is because the first line, the one that contains the name of the country looks like this: \"Luxembourg Total population: 567 000\". So everything before a space followed by the word \"Total\" is the country name. Then there’s these lines:\n\ntable &lt;- str_replace_all(table, \"\\\\s{2,}\", \"|\")\ntext_con &lt;- textConnection(table)\ndata_table &lt;- read.csv(text_con, sep = \"|\")\n\nThe first lines replaces 2 spaces or more (“\\s{2,}”) with \"|\". The reason I do this is because then I can read the table back into R as a data frame by specifying the separator as the “|” character. On the second line, I define table as a text connection, that I can then read back into R using read.csv(). On the second to the last line I change the column names and then I add a column called \"Country\" to the data frame.\n\n\nNow, I can map this useful function to the list of raw text extracted from the pdfs:\n\ndiabetes &lt;- map_df(raw_text, clean_table) %&gt;% \n    gather(Sex, Share, Males, Females, Total) %&gt;% \n    mutate(Share = as.numeric(str_extract(Share, \"\\\\d{1,}\\\\.\\\\d{1,}\")))\n\nI reshape the data with the gather() function (see what the data looks like before and after reshaping). I then convert the \"Share\" column into a numeric (it goes from something that looks like \"12.3 %\" into 12.3) and then I can create a nice plot. But first let’s take a look at the data:\n\ndiabetes\n##              Condition                  Country     Sex Share\n## 1             Diabetes               Luxembourg   Males   8.3\n## 2           Overweight               Luxembourg   Males  70.7\n## 3              Obesity               Luxembourg   Males  28.3\n## 4  Physical inactivity               Luxembourg   Males  28.2\n## 5             Diabetes                   France   Males   9.5\n## 6           Overweight                   France   Males  69.9\n## 7              Obesity                   France   Males  25.3\n## 8  Physical inactivity                   France   Males  21.2\n## 9             Diabetes                  Germany   Males   8.4\n## 10          Overweight                  Germany   Males  67.0\n## 11             Obesity                  Germany   Males  24.1\n## 12 Physical inactivity                  Germany   Males  20.1\n## 13            Diabetes United States Of America   Males   9.8\n## 14          Overweight United States Of America   Males  74.1\n## 15             Obesity United States Of America   Males  33.7\n## 16 Physical inactivity United States Of America   Males  27.6\n## 17            Diabetes                 Portugal   Males  10.7\n## 18          Overweight                 Portugal   Males  65.0\n## 19             Obesity                 Portugal   Males  21.4\n## 20 Physical inactivity                 Portugal   Males  33.5\n## 21            Diabetes           United Kingdom   Males   8.4\n## 22          Overweight           United Kingdom   Males  71.1\n## 23             Obesity           United Kingdom   Males  28.5\n## 24 Physical inactivity           United Kingdom   Males  35.4\n## 25            Diabetes               Luxembourg Females   5.3\n## 26          Overweight               Luxembourg Females  51.5\n## 27             Obesity               Luxembourg Females  21.3\n## 28 Physical inactivity               Luxembourg Females  31.7\n## 29            Diabetes                   France Females   6.6\n## 30          Overweight                   France Females  58.6\n## 31             Obesity                   France Females  26.1\n## 32 Physical inactivity                   France Females  31.2\n## 33            Diabetes                  Germany Females   6.4\n## 34          Overweight                  Germany Females  52.7\n## 35             Obesity                  Germany Females  21.4\n## 36 Physical inactivity                  Germany Females  26.5\n## 37            Diabetes United States Of America Females   8.3\n## 38          Overweight United States Of America Females  65.3\n## 39             Obesity United States Of America Females  36.3\n## 40 Physical inactivity United States Of America Females  42.1\n## 41            Diabetes                 Portugal Females   7.8\n## 42          Overweight                 Portugal Females  55.0\n## 43             Obesity                 Portugal Females  22.8\n## 44 Physical inactivity                 Portugal Females  40.8\n## 45            Diabetes           United Kingdom Females   6.9\n## 46          Overweight           United Kingdom Females  62.4\n## 47             Obesity           United Kingdom Females  31.1\n## 48 Physical inactivity           United Kingdom Females  44.3\n## 49            Diabetes               Luxembourg   Total   6.8\n## 50          Overweight               Luxembourg   Total  61.0\n## 51             Obesity               Luxembourg   Total  24.8\n## 52 Physical inactivity               Luxembourg   Total  30.0\n## 53            Diabetes                   France   Total   8.0\n## 54          Overweight                   France   Total  64.1\n## 55             Obesity                   France   Total  25.7\n## 56 Physical inactivity                   France   Total  26.4\n## 57            Diabetes                  Germany   Total   7.4\n## 58          Overweight                  Germany   Total  59.7\n## 59             Obesity                  Germany   Total  22.7\n## 60 Physical inactivity                  Germany   Total  23.4\n## 61            Diabetes United States Of America   Total   9.1\n## 62          Overweight United States Of America   Total  69.6\n## 63             Obesity United States Of America   Total  35.0\n## 64 Physical inactivity United States Of America   Total  35.0\n## 65            Diabetes                 Portugal   Total   9.2\n## 66          Overweight                 Portugal   Total  59.8\n## 67             Obesity                 Portugal   Total  22.1\n## 68 Physical inactivity                 Portugal   Total  37.3\n## 69            Diabetes           United Kingdom   Total   7.7\n## 70          Overweight           United Kingdom   Total  66.7\n## 71             Obesity           United Kingdom   Total  29.8\n## 72 Physical inactivity           United Kingdom   Total  40.0\n\nNow let’s go for the plot:\n\nggplot(diabetes) + theme_fivethirtyeight() + scale_fill_hc() +\n    geom_bar(aes(y = Share, x = Sex, fill = Country), \n             stat = \"identity\", position = \"dodge\") +\n    facet_wrap(~Condition)\n\n\n\n\nThat was a whole lot of work for such a simple plot!\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2018-11-15-tidy_gridsearch.html#introduction",
    "href": "posts/2018-11-15-tidy_gridsearch.html#introduction",
    "title": "Searching for the optimal hyper-parameters of an ARIMA model in parallel: the tidy gridsearch approach",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nIn this blog post, I’ll use the data that I cleaned in a previous blog post, which you can download here. If you want to follow along, download the monthly data.\n\n\nIn the previous blog post, I used the auto.arima() function to very quickly get a “good-enough” model to predict future monthly total passengers flying from LuxAirport. “Good-enough” models can be all you need in a lot of situations, but perhaps you’d like to have a better model. I will show here how you can get a better model by searching through a grid of hyper-parameters.\n\n\nThis blog post was partially inspired by: https://drsimonj.svbtle.com/grid-search-in-the-tidyverse"
  },
  {
    "objectID": "posts/2018-11-15-tidy_gridsearch.html#the-problem",
    "href": "posts/2018-11-15-tidy_gridsearch.html#the-problem",
    "title": "Searching for the optimal hyper-parameters of an ARIMA model in parallel: the tidy gridsearch approach",
    "section": "\nThe problem\n",
    "text": "The problem\n\n\nSARIMA models have a lot of hyper-parameters, 7 in total! Three trend hyper-parameters, p, d, q, same as for an ARIMA model, and four seasonal hyper-parameters, P, D, Q, S. The traditional way t o search for these hyper-parameters is the so-called Box-Jenkins method. You can read about it here. This method was described in a 1970 book, Time series analysis: Forecasting and control by Box and Jenkins. The method requires that you first prepare the data by logging it and differencing it, in order to make the time series stationary. You then need to analyze ACF and PACF plots, in order to determine the right amount of lags… It take some time, but this method made sense in a time were computing power was very expensive. Today, we can simply let our computer search through thousands of models, check memes on the internet, and come back to the best fit. This blog post is for you, the busy data scientist meme connoisseurs who cannot waste time with theory and other such useless time drains, when there are literally thousands of new memes being created and shared every day. Every second counts. To determine what model is best, I will do pseudo out-of-sample forecasting and compute the RMSE for each model. I will then choose the model that has the lowest RMSE."
  },
  {
    "objectID": "posts/2018-11-15-tidy_gridsearch.html#setup",
    "href": "posts/2018-11-15-tidy_gridsearch.html#setup",
    "title": "Searching for the optimal hyper-parameters of an ARIMA model in parallel: the tidy gridsearch approach",
    "section": "\nSetup\n",
    "text": "Setup\n\n\nLet’s first load some libraries:\n\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(lubridate)\nlibrary(furrr)\nlibrary(tsibble)\nlibrary(brotools)\n\nihs &lt;- function(x){\n    log(x + sqrt(x**2 + 1))\n}\n\nNow, let’s load the data:\n\navia_clean_monthly &lt;- read_csv(\"https://raw.githubusercontent.com/b-rodrigues/avia_par_lu/master/avia_clean_monthy.csv\")\n## Parsed with column specification:\n## cols(\n##   destination = col_character(),\n##   date = col_date(format = \"\"),\n##   passengers = col_double()\n## )\n\nLet’s split the data into a training set and into a testing set:\n\navia_clean_train &lt;- avia_clean_monthly %&gt;%\n    select(date, passengers) %&gt;%\n    filter(year(date) &lt; 2015) %&gt;%\n    group_by(date) %&gt;%\n    summarise(total_passengers = sum(passengers)) %&gt;%\n    pull(total_passengers) %&gt;%\n    ts(., frequency = 12, start = c(2005, 1))\n\navia_clean_test &lt;- avia_clean_monthly %&gt;%\n    select(date, passengers) %&gt;%\n    filter(year(date) &gt;= 2015) %&gt;%\n    group_by(date) %&gt;%\n    summarise(total_passengers = sum(passengers)) %&gt;%\n    pull(total_passengers) %&gt;%\n    ts(., frequency = 12, start = c(2015, 1))\n\nlogged_train_data &lt;- ihs(avia_clean_train)\n\nlogged_test_data &lt;- ihs(avia_clean_test)\n\nI also define a helper function:\n\nto_tibble &lt;- function(forecast_object){\n    point_estimate &lt;- forecast_object$mean %&gt;%\n        as_tsibble() %&gt;%\n        rename(point_estimate = value,\n               date = index)\n\n    upper &lt;- forecast_object$upper %&gt;%\n        as_tsibble() %&gt;%\n        spread(key, value) %&gt;%\n        rename(date = index,\n               upper80 = `80%`,\n               upper95 = `95%`)\n\n    lower &lt;- forecast_object$lower %&gt;%\n        as_tsibble() %&gt;%\n        spread(key, value) %&gt;%\n        rename(date = index,\n               lower80 = `80%`,\n               lower95 = `95%`)\n\n    reduce(list(point_estimate, upper, lower), full_join)\n}\n\nThis function takes a forecast object as argument, and returns a nice tibble. This will be useful later, and is based on the code I already used in my previous blog post.\n\n\nNow, let’s take a closer look at the arima() function:\n\nARIMA Modelling of Time Series\n\nDescription\n\nFit an ARIMA model to a univariate time series.\n\nUsage\n\narima(x, order = c(0L, 0L, 0L),\n      seasonal = list(order = c(0L, 0L, 0L), period = NA),\n      xreg = NULL, include.mean = TRUE,\n      transform.pars = TRUE,\n      fixed = NULL, init = NULL,\n      method = c(\"CSS-ML\", \"ML\", \"CSS\"), n.cond,\n      SSinit = c(\"Gardner1980\", \"Rossignol2011\"),\n      optim.method = \"BFGS\",\n      optim.control = list(), kappa = 1e6)\n\nThe user is supposed to enter the hyper-parameters as two lists, one called order for p, d, q and one called seasonal for P, D, Q, S. So what we need is to define these lists:\n\norder_list &lt;- list(\"p\" = seq(0, 3),\n                   \"d\" = seq(0, 2),\n                   \"q\" = seq(0, 3)) %&gt;%\n    cross() %&gt;%\n    map(lift(c))\n\nI first start with order_list. This list has 3 elements, “p”, “d” and “q”. Each element is a sequence from 0 to 3 (2 in the case of “d”). When I pass this list to purrr::cross() I get the product set of the starting list, so in this case a list of 434 = 48 elements. However, this list looks pretty bad:\n\nlist(\"p\" = seq(0, 3),\n     \"d\" = seq(0, 2),\n     \"q\" = seq(0, 3)) %&gt;%\n    cross() %&gt;%\n    head(3)\n## [[1]]\n## [[1]]$p\n## [1] 0\n## \n## [[1]]$d\n## [1] 0\n## \n## [[1]]$q\n## [1] 0\n## \n## \n## [[2]]\n## [[2]]$p\n## [1] 1\n## \n## [[2]]$d\n## [1] 0\n## \n## [[2]]$q\n## [1] 0\n## \n## \n## [[3]]\n## [[3]]$p\n## [1] 2\n## \n## [[3]]$d\n## [1] 0\n## \n## [[3]]$q\n## [1] 0\n\nI would like to have something like this instead:\n\n[[1]]\np d q \n0 0 0 \n\n[[2]]\np d q \n1 0 0 \n\n[[3]]\np d q \n2 0 0 \n\nThis is possible with the last line, map(lift(c)). There’s a lot going on in this very small line of code. First of all, there’s map(). map() iterates over lists, and applies a function, in this case lift(c). purrr::lift() is a very interesting function that lifts the domain of definition of a function from one type of input to another. The function whose input I am lifting is c(). So now, c() can take a list instead of a vector. Compare the following:\n\n# The usual\n\nc(\"a\", \"b\")\n## [1] \"a\" \"b\"\n# Nothing happens\nc(list(\"a\", \"b\"))\n## [[1]]\n## [1] \"a\"\n## \n## [[2]]\n## [1] \"b\"\n# Magic happens\nlift(c)(list(\"a\", \"b\"))\n## [1] \"a\" \"b\"\n\nSo order_list is exactly what I wanted:\n\nhead(order_list)\n## [[1]]\n## p d q \n## 0 0 0 \n## \n## [[2]]\n## p d q \n## 1 0 0 \n## \n## [[3]]\n## p d q \n## 2 0 0 \n## \n## [[4]]\n## p d q \n## 3 0 0 \n## \n## [[5]]\n## p d q \n## 0 1 0 \n## \n## [[6]]\n## p d q \n## 1 1 0\n\nI do the same for season_list:\n\nseason_list &lt;- list(\"P\" = seq(0, 3),\n                    \"D\" = seq(0, 2),\n                    \"Q\" = seq(0, 3),\n                    \"period\" = 12)  %&gt;%\n    cross() %&gt;%\n    map(lift(c))\n\nI now coerce these two lists of vectors to tibbles:\n\norderdf &lt;- tibble(\"order\" = order_list)\n\nseasondf &lt;- tibble(\"season\" = season_list)\n\nAnd I can now finally create the grid of hyper-parameters:\n\nhyper_parameters_df &lt;- crossing(orderdf, seasondf)\n\nnrows &lt;- nrow(hyper_parameters_df)\n\nhead(hyper_parameters_df)\n## # A tibble: 6 x 2\n##   order     season   \n##   &lt;list&gt;    &lt;list&gt;   \n## 1 &lt;int [3]&gt; &lt;dbl [4]&gt;\n## 2 &lt;int [3]&gt; &lt;dbl [4]&gt;\n## 3 &lt;int [3]&gt; &lt;dbl [4]&gt;\n## 4 &lt;int [3]&gt; &lt;dbl [4]&gt;\n## 5 &lt;int [3]&gt; &lt;dbl [4]&gt;\n## 6 &lt;int [3]&gt; &lt;dbl [4]&gt;\n\nThe hyper_parameters_df data frame has 2304 rows, meaning, I will now estimate 2304 models, and will do so in parallel. Let’s just take a quick look at the internals of hyper_parameters_df:\n\nglimpse(hyper_parameters_df)\n## Observations: 2,304\n## Variables: 2\n## $ order  &lt;list&gt; [&lt;0, 0, 0&gt;, &lt;0, 0, 0&gt;, &lt;0, 0, 0&gt;, &lt;0, 0, 0&gt;, &lt;0, 0, 0&gt;, …\n## $ season &lt;list&gt; [&lt;0, 0, 0, 12&gt;, &lt;1, 0, 0, 12&gt;, &lt;2, 0, 0, 12&gt;, &lt;3, 0, 0, …\n\nSo in the order column, the vector 0, 0, 0 is repeated as many times as there are combinations of P, D, Q, S for season. Same for all the other vectors of the order column."
  },
  {
    "objectID": "posts/2018-11-15-tidy_gridsearch.html#training-the-models",
    "href": "posts/2018-11-15-tidy_gridsearch.html#training-the-models",
    "title": "Searching for the optimal hyper-parameters of an ARIMA model in parallel: the tidy gridsearch approach",
    "section": "\nTraining the models\n",
    "text": "Training the models\n\n\nBecause training these models might take some time, I will use the fantastic {furrr} package by Davis Vaughan to train the arima() function in parallel. For this, I first define 8 workers:\n\nplan(multiprocess, workers = 8)\n\nAnd then I run the code:\n\ntic &lt;- Sys.time()\nmodels_df &lt;- hyper_parameters_df %&gt;%\n    mutate(models = future_map2(.x = order,\n                         .y = season,\n                         ~possibly(arima, otherwise = NULL)(x = logged_train_data,\n                                                                           order = .x, seasonal = .y)))\nrunning_time &lt;- Sys.time() - tic\n\nI use future_map2(), which is just like map2() but running in parallel. I add a new column to the data called models, which will contain the models trained over all the different combinations of order and season. The models are trained on the logged_train_data.\n\n\nTraining the 2304 models took 18 minutes, which is plenty of time to browse the latest memes, but still quick enough that it justifies the whole approach. Let’s take a look at the models_df object:\n\nhead(models_df)\n## # A tibble: 6 x 3\n##   order     season    models \n##   &lt;list&gt;    &lt;list&gt;    &lt;list&gt; \n## 1 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;\n## 2 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;\n## 3 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;\n## 4 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;\n## 5 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;\n## 6 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;\n\nAs you can see, the models column contains all the trained models. The model on the first row, was trained with the hyperparameters of row 1, and so on. But, our work is not over! We now need to find the best model. First, I add a new column to the tibble, which contains the forecast. From the forecast, I extract the point estimate:\n\nmodels_df %&gt;%\n    mutate(forecast = map(models, ~possibly(forecast, otherwise = NULL)(., h = 39))) %&gt;%\n    mutate(point_forecast = map(forecast, ~.$`mean`)) %&gt;%\n    ....\n\nYou have to be familiar with a forecast object to understand the last line: a forecast object is a list with certain elements, the point estimates, the confidence intervals, and so on. To get the point estimates, I have to extract the “mean” element from the list. Hence the weird ~.$mean. Then I need to add a new list-column, where each element is the vector of true values, meaning the data from 2015 to 2018. Because I have to add it as a list of size 2304, I do that with purrr::rerun():\n\nrerun(5, c(\"a\", \"b\", \"c\"))\n## [[1]]\n## [1] \"a\" \"b\" \"c\"\n## \n## [[2]]\n## [1] \"a\" \"b\" \"c\"\n## \n## [[3]]\n## [1] \"a\" \"b\" \"c\"\n## \n## [[4]]\n## [1] \"a\" \"b\" \"c\"\n## \n## [[5]]\n## [1] \"a\" \"b\" \"c\"\n\nIt is then easy to compute the RMSE, which I add as a column to the original data:\n\n... %&gt;%\n    mutate(true_value = rerun(nrows, logged_test_data)) %&gt;%\n    mutate(rmse = map2_dbl(point_forecast, true_value,\n                           ~sqrt(mean((.x - .y) ** 2))))\n\nThe whole workflow is here:\n\nmodels_df &lt;- models_df %&gt;%\n    mutate(forecast = map(models, ~possibly(forecast, otherwise = NULL)(., h = 39))) %&gt;%\n    mutate(point_forecast = map(forecast, ~.$`mean`)) %&gt;%\n    mutate(true_value = rerun(nrows, logged_test_data)) %&gt;%\n    mutate(rmse = map2_dbl(point_forecast, true_value,\n                           ~sqrt(mean((.x - .y) ** 2))))\n\nThis is how models_df looks now:\n\nhead(models_df)\n## # A tibble: 6 x 7\n##   order     season    models  forecast   point_forecast true_value  rmse\n##   &lt;list&gt;    &lt;list&gt;    &lt;list&gt;  &lt;list&gt;     &lt;list&gt;         &lt;list&gt;     &lt;dbl&gt;\n## 1 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.525\n## 2 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.236\n## 3 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.235\n## 4 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.217\n## 5 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.190\n## 6 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.174\n\nNow, I can finally select the best performing model. I select the model with minimum RMSE:\n\nbest_model &lt;- models_df %&gt;%\n    filter(rmse == min(rmse, na.rm = TRUE))\n\nAnd save the forecast into a new variable, as a tibble, using my to_tibble() function:\n\n(best_model_forecast &lt;- to_tibble(best_model$forecast[[1]]))\n## Joining, by = \"date\"\n## Joining, by = \"date\"\n## # A tsibble: 39 x 6 [1M]\n##        date point_estimate upper80 upper95 lower80 lower95\n##       &lt;mth&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1 2015 Jan           11.9    12.1    12.1    11.8    11.7\n##  2 2015 Feb           11.9    12.0    12.1    11.7    11.6\n##  3 2015 Mar           12.1    12.3    12.3    11.9    11.9\n##  4 2015 Apr           12.2    12.3    12.4    12.0    11.9\n##  5 2015 May           12.2    12.4    12.5    12.1    12.0\n##  6 2015 Jun           12.3    12.4    12.5    12.1    12.0\n##  7 2015 Jul           12.2    12.3    12.4    12.0    11.9\n##  8 2015 Aug           12.3    12.5    12.6    12.2    12.1\n##  9 2015 Sep           12.3    12.5    12.6    12.2    12.1\n## 10 2015 Oct           12.2    12.4    12.5    12.1    12.0\n## # … with 29 more rows\n\nAnd now, I can plot it:\n\navia_clean_monthly %&gt;%\n    group_by(date) %&gt;%\n    summarise(total = sum(passengers)) %&gt;%\n    mutate(total_ihs = ihs(total)) %&gt;%\n    ggplot() +\n    ggtitle(\"Logged data\") +\n    geom_line(aes(y = total_ihs, x = date), colour = \"#82518c\") +\n    scale_x_date(date_breaks = \"1 year\", date_labels = \"%m-%Y\") +\n    geom_ribbon(data = best_model_forecast, aes(x = date, ymin = lower95, ymax = upper95), \n                fill = \"#666018\", alpha = 0.2) +\n    geom_line(data = best_model_forecast, aes(x = date, y = point_estimate), linetype = 2, colour = \"#8e9d98\") +\n    theme_blog()\n\n\n\n\nCompared to the previous blog post, the dotted line now seems to follow the true line even better! However, this is not suprising, as I am using the test set as a validation set, which might lead to overfitting the hyperparameters to the test set. Also, I am not saying that you should always do a gridsearch whenever you have a problem like this one. In the case of univariate time series, I am still doubtful that a gridsearch like this is really necessary. The goal of this blog post was not to teach you how to look for hyperparameters per se, but more to show you how to do a grid search the tidy way. I’ll be writing about proper hyperparameter optimization in a future blog post. Also, the other thing I wanted to show was the power of {furrr}."
  },
  {
    "objectID": "posts/2017-03-27-introducing_brotools.html",
    "href": "posts/2017-03-27-introducing_brotools.html",
    "title": "Introducing brotools",
    "section": "",
    "text": "I’m happy to announce my first R package, called brotools. This is a package that contains functions that are specific to my needs but that you might find also useful. I blogged about some of these functions, so if you follow my blog you might already be familiar with some of them. It is not on CRAN and might very well never be. The code is hosted on bitbucket and you can install the package with\n\ndevtools::install_bitbucket(\"b-rodrigues/brotools\")\n\nHope you’ll find the brotools useful!"
  },
  {
    "objectID": "posts/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions.html",
    "href": "posts/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions.html",
    "title": "Data frame columns as arguments to dplyr functions",
    "section": "",
    "text": "Suppose that you would like to create a function which does a series of computations on a data frame. You would like to pass a column as this function’s argument. Something like:\n\ndata(cars)\nconvertToKmh &lt;- function(dataset, col_name){\n  dataset$col_name &lt;- dataset$speed * 1.609344\n  return(dataset)\n}\n\nThis example is obviously not very interesting (you don’t need a function for this), but it will illustrate the point. You would like to append a column called speed_in_kmh with the speed in kilometers per hour to this dataset, but this is what happens:\n\nhead(convertToKmh(cars, \"speed_in_kmh\"))\n##   speed dist  col_name\n1     4    2  6.437376\n2     4   10  6.437376\n3     7    4 11.265408\n4     7   22 11.265408\n5     8   16 12.874752\n6     9   10 14.484096\n\nYour column is not called speed_in_kmh but col_name! It turns out that there is a very simple solution:\n\nconvertToKmh &lt;- function(dataset, col\\_name){\n  dataset[col_name] &lt;- dataset$speed * 1.609344\n  return(dataset)\n}\n\nhead(convertToKmh(cars, \"speed\\_in\\_kmh\"))\n##   speed dist speed\\_in\\_kmh\n1     4    2     6.437376\n2     4   10     6.437376\n3     7    4    11.265408\n4     7   22    11.265408\n5     8   16    12.874752\n6     9   10    14.484096\n\nYou can access columns with [] instead of $.\n\n\nBut sometimes you want to do more complex things and for example have a function that groups by a variable and then computes new variables, filters by another and so on. You would like to avoid having to hard code these variables in your function, because then why write a function and of course you would like to use dplyr to do it.\n\n\nI often use dplyr functions in my functions. For illustration purposes, consider this very simple function:\n\nsimpleFunction &lt;- function(dataset, col_name){\n  require(\"dplyr\")\n  dataset %&gt;% \n    group\\_by(col\\_name) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\nsimpleFunction(cars, \"dist\")\n\nThis function takes a dataset as an argument, as well as a column name. However, this does not work. You get this error:\n\nError: unknown variable to group by : col_name \n\nThe variable col_name is passed to simpleFunction() as a string, but group_by() requires a variable name. So why not try to convert col_name to a name?\n\nsimpleFunction &lt;- function(dataset, col_name){\n  require(\"dplyr\")\n  col\\_name &lt;- as.name(col_name)\n  dataset %&gt;% \n    group\\_by(col\\_name) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\nsimpleFunction(cars, \"dist\")\n\nYou get the same error as before:\n\nError: unknown variable to group by : col_name \n\nSo how can you pass a column name to group_by()? Well, there is another version of group_by() called group_by_() that uses standard evaluation. You can learn more about it here. Let’s take a look at what happens when we use group_by_():\n\nsimpleFunction &lt;- function(dataset, col_name){\n  require(\"dplyr\")\n  dataset %&gt;% \n    group\\_by\\_(col_name) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\nsimpleFunction(cars, \"dist\")\nA tibble: 35 x 2\n dist mean\\_speed\n&lt;dbl&gt;      &lt;dbl&gt;\n1      2        4.0\n2      4        7.0\n3     10        6.5\n4     14       12.0\n5     16        8.0\n6     17       11.0\n7     18       10.0\n8     20       13.5\n9     22        7.0\n10    24       12.0\n ... with 25 more rows\n\nWe can even use a formula instead of a string:\n\nsimpleFunction(cars, ~dist)\n A tibble: 35 x 2\n    dist mean_speed\n   &lt;dbl&gt;      &lt;dbl&gt;\n1      2        4.0\n2      4        7.0\n3     10        6.5\n4     14       12.0\n5     16        8.0\n6     17       11.0\n7     18       10.0\n8     20       13.5\n9     22        7.0\n10    24       12.0\n... with 25 more rows\n\nWhat if you want to pass column names and constants, for example to filter without hardcoding anything?\n\n\nTrying to do it naively will only yield pain and despair:\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  require(\"dplyr\")\n  dataset %&gt;% \n    filter\\_(col\\_name == value) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n&gt; simpleFunction(cars, \"dist\", 10)\n\n  mean_speed\n1        NaN\n\n&gt; simpleFunction(cars, dist, 10)\n\n Error in col_name == value : \n  comparison (1) is possible only for atomic and list types \n  \n&gt; simpleFunction(cars, ~dist, 10)\n\n  mean_speed\n1        NaN\n\n\nTo solve this issue, we need to know a little bit about two concepts, lazy evaluation and non-standard evaluation. I recommend you read the following document from Hadley Wickham’s book Advanced R as well as the part on lazy evaluation here.\n\n\nA nice package called lazyeval can help us out. We would like to make R understand that the column name is not col_name but the string inside it \"dist\", and now we would like to use filter() for dist equal to 10.\n\n\nIn the lazyeval package, you’ll find the function interp(). interp() allows you to\n\n\n\nbuild an expression up from a mixture of constants and variables.\n\n\n\nTake a look at this example:\n\nlibrary(lazyeval)\ninterp(~x+y, x = 2)\n## ~2 + y\n\nWhat you get back is this nice formula that you can then use within functions. To see why this is useful, let’s look at the above example again, and make it work using interp():\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  require(\"dplyr\")\n  require(\"lazyeval\")\n  filter\\_criteria &lt;- interp(~y == x, .values=list(y = as.name(col_name), x = value))\n  dataset %&gt;% \n    filter\\_(filter_criteria) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(cars, \"dist\", 10)\n  mean\\_speed\n1        6.5\n\nAnd now it works! For some reason, you have to pass the column name as a string though.\n\n\nSources: apart from the documents above, the following stackoverflow threads helped me out quite a lot: In R: pass column name as argument and use it in function with dplyr::mutate() and lazyeval::interp() and Non-standard evaluation (NSE) in dplyr’s filter_ & pulling data from MySQL."
  },
  {
    "objectID": "posts/2018-11-16-rgenoud_arima.html#introduction",
    "href": "posts/2018-11-16-rgenoud_arima.html#introduction",
    "title": "Using a genetic algorithm for the hyperparameter optimization of a SARIMA model",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nIn this blog post, I’ll use the data that I cleaned in a previous blog post, which you can download here. If you want to follow along, download the monthly data. In my last blog post I showed how to perform a grid search the “tidy” way. As an example, I looked for the right hyperparameters of a SARIMA model. However, the goal of the post was not hyperparameter optimization per se, so I did not bother with tuning the hyperparameters on a validation set, and used the test set for both validation of the hyperparameters and testing the forecast. Of course, this is not great because doing this might lead to overfitting the hyperparameters to the test set. So in this blog post I split my data into trainig, validation and testing sets and use a genetic algorithm to look for the hyperparameters. Again, this is not the most optimal way to go about this problem, since the {forecast} package contains the very useful auto.arima() function. I just wanted to see what kind of solution a genetic algorithm would return, and also try different cost functions. If you’re interested, read on!"
  },
  {
    "objectID": "posts/2018-11-16-rgenoud_arima.html#setup",
    "href": "posts/2018-11-16-rgenoud_arima.html#setup",
    "title": "Using a genetic algorithm for the hyperparameter optimization of a SARIMA model",
    "section": "\nSetup\n",
    "text": "Setup\n\n\nLet’s first load some libraries and define some helper functions (the helper functions were explained in the previous blog posts):\n\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(rgenoud)\nlibrary(parallel)\nlibrary(lubridate)\nlibrary(furrr)\nlibrary(tsibble)\nlibrary(brotools)\n\nihs &lt;- function(x){\n    log(x + sqrt(x**2 + 1))\n}\n\nto_tibble &lt;- function(forecast_object){\n    point_estimate &lt;- forecast_object$mean %&gt;%\n        as_tsibble() %&gt;%\n        rename(point_estimate = value,\n               date = index)\n\n    upper &lt;- forecast_object$upper %&gt;%\n        as_tsibble() %&gt;%\n        spread(key, value) %&gt;%\n        rename(date = index,\n               upper80 = `80%`,\n               upper95 = `95%`)\n\n    lower &lt;- forecast_object$lower %&gt;%\n        as_tsibble() %&gt;%\n        spread(key, value) %&gt;%\n        rename(date = index,\n               lower80 = `80%`,\n               lower95 = `95%`)\n\n    reduce(list(point_estimate, upper, lower), full_join)\n}\n\nNow, let’s load the data:\n\navia_clean_monthly &lt;- read_csv(\"https://raw.githubusercontent.com/b-rodrigues/avia_par_lu/master/avia_clean_monthy.csv\")\n## Parsed with column specification:\n## cols(\n##   destination = col_character(),\n##   date = col_date(format = \"\"),\n##   passengers = col_double()\n## )\n\nLet’s split the data into a train set, a validation set and a test set:\n\navia_clean_train &lt;- avia_clean_monthly %&gt;%\n    select(date, passengers) %&gt;%\n    filter(year(date) &lt; 2013) %&gt;%\n    group_by(date) %&gt;%\n    summarise(total_passengers = sum(passengers)) %&gt;%\n    pull(total_passengers) %&gt;%\n    ts(., frequency = 12, start = c(2005, 1))\n\navia_clean_validation &lt;- avia_clean_monthly %&gt;%\n    select(date, passengers) %&gt;%\n    filter(between(year(date), 2013, 2016)) %&gt;%\n    group_by(date) %&gt;%\n    summarise(total_passengers = sum(passengers)) %&gt;%\n    pull(total_passengers) %&gt;%\n    ts(., frequency = 12, start = c(2013, 1))\n\navia_clean_test &lt;- avia_clean_monthly %&gt;%\n    select(date, passengers) %&gt;%\n    filter(year(date) &gt;= 2016) %&gt;%\n    group_by(date) %&gt;%\n    summarise(total_passengers = sum(passengers)) %&gt;%\n    pull(total_passengers) %&gt;%\n    ts(., frequency = 12, start = c(2016, 1))\n\nlogged_test_data &lt;- ihs(avia_clean_test)\n\nlogged_validation_data &lt;- ihs(avia_clean_validation)\n\nlogged_train_data &lt;- ihs(avia_clean_train)\n\nI will train the models on data from 2005 to 2012, look for the hyperparameters on data from 2013 to 2016 and test the accuracy on data from 2016 to March 2018. For this kind of exercise, the ideal situation would be to perform cross-validation. Doing this with time-series data is not obvious because of the autocorrelation between observations, which would be broken by sampling independently which is required by CV. Also, if for example you do leave-one-out CV, you would end up trying to predict a point in, say, 2017, with data from 2018, which does not make sense. So you should be careful about that. {forecast} is able to perform CV for time series and scikit-learn, the Python package, is able to perform cross-validation of time series data too. I will not do it in this blog post and simply focus on the genetic algorithm part.\n\n\nLet’s start by defining the cost function to minimize. I’ll try several, in the first one I will minimize the RMSE:\n\ncost_function_rmse &lt;- function(param, train_data, validation_data, forecast_periods){\n    order &lt;- param[1:3]\n    season &lt;- c(param[4:6], 12)\n    model &lt;- purrr::possibly(arima, otherwise = NULL)(x = train_data, order = order, \n                                                      seasonal = season,\n                                                      method = \"ML\")\n    if(is.null(model)){\n        return(9999999)\n    } else {\n      forecast_model &lt;- forecast::forecast(model, h = forecast_periods)\n      point_forecast &lt;- forecast_model$mean\n      sqrt(mean(point_forecast - validation_data) ** 2)\n    }\n}\n\nIf arima() is not able to estimate a model for the given parameters, I force it to return NULL, and in that case force the cost function to return a very high cost. If a model was successfully estimated, then I compute the RMSE.\n\n\nLet’s also take a look at what auto.arima() says:\n\nstarting_model &lt;- auto.arima(logged_train_data)\nsummary(starting_model)\n## Series: logged_train_data \n## ARIMA(3,0,0)(0,1,1)[12] with drift \n## \n## Coefficients:\n##          ar1     ar2     ar3     sma1   drift\n##       0.2318  0.2292  0.3661  -0.8498  0.0029\n## s.e.  0.1016  0.1026  0.1031   0.2101  0.0010\n## \n## sigma^2 estimated as 0.004009:  log likelihood=107.98\n## AIC=-203.97   AICc=-202.88   BIC=-189.38\n## \n## Training set error measures:\n##                        ME       RMSE        MAE         MPE      MAPE\n## Training set 0.0009924108 0.05743719 0.03577996 0.006323241 0.3080978\n##                   MASE        ACF1\n## Training set 0.4078581 -0.02707016\n\nLet’s compute the cost at this vector of parameters:\n\ncost_function_rmse(c(1, 0, 2, 2, 1, 0),\n              train_data = logged_train_data,\n              validation_data = logged_validation_data,\n              forecast_periods = 65)\n## [1] 0.1731473\n\nOk, now let’s start with optimizing the hyperparameters. Let’s help the genetic algorithm a little bit by defining where it should perform the search:\n\ndomains &lt;- matrix(c(0, 3, 0, 2, 0, 3, 0, 3, 0, 2, 0, 3), byrow = TRUE, ncol = 2)\n\nThis matrix constraints the first parameter to lie between 0 and 3, the second one between 0 and 2, and so on.\n\n\nLet’s call the genoud() function from the {rgenoud} package, and use 8 cores:\n\ncl &lt;- makePSOCKcluster(8)\nclusterExport(cl, c('logged_train_data', 'logged_validation_data'))\n\ntic &lt;- Sys.time()\n\nauto_arima_rmse &lt;- genoud(cost_function_rmse,\n                     nvars = 6,\n                     data.type.int = TRUE,\n                     starting.values = c(1, 0, 2, 2, 1, 0), # &lt;- from auto.arima\n                     Domains = domains,\n                     cluster = cl,\n                     train_data = logged_train_data,\n                     validation_data = logged_validation_data,\n                     forecast_periods = length(logged_validation_data),\n                     hard.generation.limit = TRUE)\ntoc_rmse &lt;- Sys.time() - tic\n\nmakePSOCKcluster() is a function from the {parallel} package. I must also export the global variables logged_train_data or logged_validation_data. If I don’t do that, the workers called by genoud() will not know about these variables and an error will be returned. The option data.type.int = TRUE force the algorithm to look only for integers, and hard.generation.limit = TRUE forces the algorithm to stop after 100 generations.\n\n\nThe process took 7 minutes, which is faster than doing the grid search. What was the solution found?\n\nauto_arima_rmse\n## $value\n## [1] 0.0001863039\n## \n## $par\n## [1] 3 2 1 1 2 1\n## \n## $gradients\n## [1] NA NA NA NA NA NA\n## \n## $generations\n## [1] 11\n## \n## $peakgeneration\n## [1] 1\n## \n## $popsize\n## [1] 1000\n## \n## $operators\n## [1] 122 125 125 125 125 126 125 126   0\n\nLet’s train the model using the arima() function at these parameters:\n\nbest_model_rmse &lt;- arima(logged_train_data, order = auto_arima_rmse$par[1:3], \n                         season = list(order = auto_arima_rmse$par[4:6], period = 12),\n                         method = \"ML\")\n\nsummary(best_model_rmse)\n## \n## Call:\n## arima(x = logged_train_data, order = auto_arima_rmse$par[1:3], seasonal = list(order = auto_arima_rmse$par[4:6], \n##     period = 12), method = \"ML\")\n## \n## Coefficients:\n##           ar1      ar2      ar3      ma1     sar1     sma1\n##       -0.6999  -0.4541  -0.0476  -0.9454  -0.4996  -0.9846\n## s.e.   0.1421   0.1612   0.1405   0.1554   0.1140   0.2193\n## \n## sigma^2 estimated as 0.006247:  log likelihood = 57.34,  aic = -100.67\n## \n## Training set error measures:\n##                         ME       RMSE        MAE          MPE      MAPE\n## Training set -0.0006142355 0.06759545 0.04198561 -0.005408262 0.3600483\n##                   MASE         ACF1\n## Training set 0.4386693 -0.008298546\n\nLet’s extract the forecasts:\n\nbest_model_rmse_forecast &lt;- forecast::forecast(best_model_rmse, h = 65)\n\nbest_model_rmse_forecast &lt;- to_tibble(best_model_rmse_forecast)\n## Joining, by = \"date\"\n## Joining, by = \"date\"\nstarting_model_forecast &lt;- forecast(starting_model, h = 65)\n\nstarting_model_forecast &lt;- to_tibble(starting_model_forecast)\n## Joining, by = \"date\"\n## Joining, by = \"date\"\n\nand plot the forecast to see how it looks:\n\navia_clean_monthly %&gt;%\n    group_by(date) %&gt;%\n    summarise(total = sum(passengers)) %&gt;%\n    mutate(total_ihs = ihs(total)) %&gt;%\n    ggplot() +\n    ggtitle(\"Minimization of RMSE\") +\n    geom_line(aes(y = total_ihs, x = date), colour = \"#82518c\") +\n    scale_x_date(date_breaks = \"1 year\", date_labels = \"%m-%Y\") +\n    geom_ribbon(data = best_model_rmse_forecast, aes(x = date, ymin = lower95, ymax = upper95),\n                fill = \"#666018\", alpha = 0.2) +\n    geom_line(data = best_model_rmse_forecast, aes(x = date, y = point_estimate), \n              linetype = 2, colour = \"#8e9d98\") +\n    geom_ribbon(data = starting_model_forecast, aes(x = date, ymin = lower95, ymax = upper95),\n                fill = \"#98431e\", alpha = 0.2) +\n    geom_line(data = starting_model_forecast, aes(x = date, y = point_estimate), \n              linetype = 2, colour = \"#a53031\") +\n    theme_blog()\n\n\n\n\nThe yellowish line and confidence intervals come from minimizing the genetic algorithm, and the redish from auto.arima(). Interesting; the point estimate is very precise, but the confidence intervals are very wide. Low bias, high variance.\n\n\nNow, let’s try with another cost function, where I minimize the BIC, similar to the auto.arima() function:\n\ncost_function_bic &lt;- function(param, train_data, validation_data, forecast_periods){\n    order &lt;- param[1:3]\n    season &lt;- c(param[4:6], 12)\n    model &lt;- purrr::possibly(arima, otherwise = NULL)(x = train_data, order = order, \n                                                      seasonal = season,\n                                                      method = \"ML\")\n    if(is.null(model)){\n        return(9999999)\n    } else {\n        BIC(model)\n    }\n}\n\nLet’s take a look at the cost at the parameter values returned by auto.arima():\n\ncost_function_bic(c(1, 0, 2, 2, 1, 0),\n              train_data = logged_train_data,\n              validation_data = logged_validation_data,\n              forecast_periods = 65)\n## [1] -184.6397\n\nLet the genetic algorithm run again:\n\ncl &lt;- makePSOCKcluster(8)\nclusterExport(cl, c('logged_train_data', 'logged_validation_data'))\n\ntic &lt;- Sys.time()\n\nauto_arima_bic &lt;- genoud(cost_function_bic,\n                     nvars = 6,\n                     data.type.int = TRUE,\n                     starting.values = c(1, 0, 2, 2, 1, 0), # &lt;- from auto.arima\n                     Domains = domains,\n                     cluster = cl,\n                     train_data = logged_train_data,\n                     validation_data = logged_validation_data,\n                     forecast_periods = length(logged_validation_data),\n                     hard.generation.limit = TRUE)\ntoc_bic &lt;- Sys.time() - tic\n\nThis time, it took 6 minutes, a bit slower than before. Let’s take a look at the solution:\n\nauto_arima_bic\n## $value\n## [1] -201.0656\n## \n## $par\n## [1] 0 1 1 1 0 1\n## \n## $gradients\n## [1] NA NA NA NA NA NA\n## \n## $generations\n## [1] 12\n## \n## $peakgeneration\n## [1] 1\n## \n## $popsize\n## [1] 1000\n## \n## $operators\n## [1] 122 125 125 125 125 126 125 126   0\n\nLet’s train the model at these parameters:\n\nbest_model_bic &lt;- arima(logged_train_data, order = auto_arima_bic$par[1:3], \n                        season = list(order = auto_arima_bic$par[4:6], period = 12),\n                        method = \"ML\")\n\nsummary(best_model_bic)\n## \n## Call:\n## arima(x = logged_train_data, order = auto_arima_bic$par[1:3], seasonal = list(order = auto_arima_bic$par[4:6], \n##     period = 12), method = \"ML\")\n## \n## Coefficients:\n##           ma1    sar1    sma1\n##       -0.6225  0.9968  -0.832\n## s.e.   0.0835  0.0075   0.187\n## \n## sigma^2 estimated as 0.004145:  log likelihood = 109.64,  aic = -211.28\n## \n## Training set error measures:\n##                       ME       RMSE        MAE        MPE      MAPE\n## Training set 0.003710982 0.06405303 0.04358164 0.02873561 0.3753513\n##                   MASE        ACF1\n## Training set 0.4553447 -0.03450603\n\nAnd let’s plot the results:\n\nbest_model_bic_forecast &lt;- forecast::forecast(best_model_bic, h = 65)\n\nbest_model_bic_forecast &lt;- to_tibble(best_model_bic_forecast)\n## Joining, by = \"date\"\n## Joining, by = \"date\"\navia_clean_monthly %&gt;%\n    group_by(date) %&gt;%\n    summarise(total = sum(passengers)) %&gt;%\n    mutate(total_ihs = ihs(total)) %&gt;%\n    ggplot() +\n    ggtitle(\"Minimization of BIC\") +\n    geom_line(aes(y = total_ihs, x = date), colour = \"#82518c\") +\n    scale_x_date(date_breaks = \"1 year\", date_labels = \"%m-%Y\") +\n    geom_ribbon(data = best_model_bic_forecast, aes(x = date, ymin = lower95, ymax = upper95),\n                fill = \"#5160a0\", alpha = 0.2) +\n    geom_line(data = best_model_bic_forecast, aes(x = date, y = point_estimate), \n              linetype = 2, colour = \"#208480\") +\n    geom_ribbon(data = starting_model_forecast, aes(x = date, ymin = lower95, ymax = upper95),\n                fill = \"#98431e\", alpha = 0.2) +\n    geom_line(data = starting_model_forecast, aes(x = date, y = point_estimate), \n              linetype = 2, colour = \"#a53031\") +\n    theme_blog()\n\n\n\n\nThe solutions are very close, both in terms of point estimates and confidence intervals. Bias increased, but variance lowered… This gives me an idea! What if I minimize the RMSE, while keeping the number of parameters low, as a kind of regularization? This is somewhat what minimising BIC does, but let’s try to do it a more “naive” approach:\n\ncost_function_rmse_low_k &lt;- function(param, train_data, validation_data, forecast_periods, max.order){\n    order &lt;- param[1:3]\n    season &lt;- c(param[4:6], 12)\n    if(param[1] + param[3] + param[4] + param[6] &gt; max.order){\n        return(9999999)\n    } else {\n        model &lt;- purrr::possibly(arima, otherwise = NULL)(x = train_data, \n                                                          order = order, \n                                                          seasonal = season,\n                                                          method = \"ML\")\n    }\n    if(is.null(model)){\n        return(9999999)\n    } else {\n        forecast_model &lt;- forecast::forecast(model, h = forecast_periods)\n        point_forecast &lt;- forecast_model$mean\n        sqrt(mean(point_forecast - validation_data) ** 2)\n    }\n}\n\nThis is also similar to what auto.arima() does; by default, the max.order argument in auto.arima() is set to 5, and is the sum of p + q + P + Q. So I’ll try something similar.\n\n\nLet’s take a look at the cost at the parameter values returned by auto.arima():\n\ncost_function_rmse_low_k(c(1, 0, 2, 2, 1, 0),\n              train_data = logged_train_data,\n              validation_data = logged_validation_data,\n              forecast_periods = 65,\n              max.order = 5)\n## [1] 0.1731473\n\nLet’s see what will happen:\n\ncl &lt;- makePSOCKcluster(8)\nclusterExport(cl, c('logged_train_data', 'logged_validation_data'))\n\ntic &lt;- Sys.time()\n\nauto_arima_rmse_low_k &lt;- genoud(cost_function_rmse_low_k,\n                         nvars = 6,\n                         data.type.int = TRUE,\n                         starting.values = c(1, 0, 2, 2, 1, 0), # &lt;- from auto.arima\n                         max.order = 5,\n                         Domains = domains,\n                         cluster = cl,\n                         train_data = logged_train_data,\n                         validation_data = logged_validation_data,\n                         forecast_periods = length(logged_validation_data),\n                         hard.generation.limit = TRUE)\ntoc_rmse_low_k &lt;- Sys.time() - tic\n\nIt took 1 minute to train this one, quite fast! Let’s take a look:\n\nauto_arima_rmse_low_k\n## $value\n## [1] 0.002503478\n## \n## $par\n## [1] 1 2 0 3 1 0\n## \n## $gradients\n## [1] NA NA NA NA NA NA\n## \n## $generations\n## [1] 11\n## \n## $peakgeneration\n## [1] 1\n## \n## $popsize\n## [1] 1000\n## \n## $operators\n## [1] 122 125 125 125 125 126 125 126   0\n\nAnd let’s plot it:\n\nbest_model_rmse_low_k &lt;- arima(logged_train_data, order = auto_arima_rmse_low_k$par[1:3], \n                               season = list(order = auto_arima_rmse_low_k$par[4:6], period = 12),\n                               method = \"ML\")\n\nsummary(best_model_rmse_low_k)\n## \n## Call:\n## arima(x = logged_train_data, order = auto_arima_rmse_low_k$par[1:3], seasonal = list(order = auto_arima_rmse_low_k$par[4:6], \n##     period = 12), method = \"ML\")\n## \n## Coefficients:\n##           ar1     sar1     sar2     sar3\n##       -0.6468  -0.7478  -0.5263  -0.1143\n## s.e.   0.0846   0.1171   0.1473   0.1446\n## \n## sigma^2 estimated as 0.01186:  log likelihood = 57.88,  aic = -105.76\n## \n## Training set error measures:\n##                        ME      RMSE        MAE         MPE      MAPE\n## Training set 0.0005953302 0.1006917 0.06165919 0.003720452 0.5291736\n##                   MASE       ACF1\n## Training set 0.6442205 -0.3706693\nbest_model_rmse_low_k_forecast &lt;- forecast::forecast(best_model_rmse_low_k, h = 65)\n\nbest_model_rmse_low_k_forecast &lt;- to_tibble(best_model_rmse_low_k_forecast)\n## Joining, by = \"date\"\n## Joining, by = \"date\"\navia_clean_monthly %&gt;%\n    group_by(date) %&gt;%\n    summarise(total = sum(passengers)) %&gt;%\n    mutate(total_ihs = ihs(total)) %&gt;%\n    ggplot() +\n    ggtitle(\"Minimization of RMSE + low k\") +\n    geom_line(aes(y = total_ihs, x = date), colour = \"#82518c\") +\n    scale_x_date(date_breaks = \"1 year\", date_labels = \"%m-%Y\") +\n    geom_ribbon(data = best_model_rmse_low_k_forecast, aes(x = date, ymin = lower95, ymax = upper95),\n                fill = \"#5160a0\", alpha = 0.2) +\n    geom_line(data = best_model_rmse_low_k_forecast, aes(x = date, y = point_estimate), \n              linetype = 2, colour = \"#208480\") +\n    geom_ribbon(data = starting_model_forecast, aes(x = date, ymin = lower95, ymax = upper95),\n                fill = \"#98431e\", alpha = 0.2) +\n    geom_line(data = starting_model_forecast, aes(x = date, y = point_estimate), \n              linetype = 2, colour = \"#a53031\") +\n    theme_blog()\n\n\n\n\nLooks like this was not the right strategy. There might be a better cost function than what I have tried, but looks like minimizing the BIC is the way to go."
  },
  {
    "objectID": "posts/2016-07-30-merge-a-list-of-datasets-together.html",
    "href": "posts/2016-07-30-merge-a-list-of-datasets-together.html",
    "title": "Merge a list of datasets together",
    "section": "",
    "text": "Last week I showed how to read a lot of datasets at once with R, and this week I’ll continue from there and show a very simple function that uses this list of read datasets and merges them all together.\n\n\nFirst we’ll use read_list() to read all the datasets at once (for more details read last week’s post):\n\nlibrary(\"readr\")\nlibrary(\"tibble\")\n\ndata_files &lt;- list.files(pattern = \".csv\")\n\nprint(data_files)\n## [1] \"data_1.csv\" \"data_2.csv\" \"data_3.csv\"\nlist_of_data_sets &lt;- read_list(data_files, read_csv)\n\nglimpse(list_of_data_sets)\n## List of 3\n##  $ data_1:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,018930679\" \"0,8748013128\" \"0,1025635934\" \"0,6246140983\" ...\n##   ..$ col2: chr [1:19] \"0,0377725807\" \"0,5959457638\" \"0,4429121533\" \"0,558387159\" ...\n##   ..$ col3: chr [1:19] \"0,6241767189\" \"0,031324594\" \"0,2238059868\" \"0,2773350732\" ...\n##  $ data_2:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9098418493\" \"0,1127788509\" \"0,5818891392\" \"0,1011773532\" ...\n##   ..$ col2: chr [1:19] \"0,7455905887\" \"0,4015039612\" \"0,6625796605\" \"0,029955339\" ...\n##   ..$ col3: chr [1:19] \"0,327232932\" \"0,2784035673\" \"0,8092386735\" \"0,1216045306\" ...\n##  $ data_3:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9236124896\" \"0,6303271761\" \"0,6413583054\" \"0,5573887416\" ...\n##   ..$ col2: chr [1:19] \"0,2114708388\" \"0,6984538266\" \"0,0469865249\" \"0,9271510226\" ...\n##   ..$ col3: chr [1:19] \"0,4941919971\" \"0,7391538511\" \"0,3876723797\" \"0,2815014394\" ...\n\nYou see that all these datasets have the same column names. We can now merge them using this simple function:\n\nmulti_join &lt;- function(list_of_loaded_data, join_func, ...){\n\n    require(\"dplyr\")\n\n    output &lt;- Reduce(function(x, y) {join_func(x, y, ...)}, list_of_loaded_data)\n\n    return(output)\n}\n\nThis function uses Reduce(). Reduce() is a very important function that can be found in all functional programming languages. What does Reduce() do? Let’s take a look at the following example:\n\nReduce(`+`, c(1, 2, 3, 4, 5))\n## [1] 15\n\nReduce() has several arguments, but you need to specify at least two: a function, here + and a list, here c(1, 2, 3, 4, 5). The next code block shows what Reduce() basically does:\n\n0 + c(1, 2, 3, 4, 5)\n0 + 1 + c(2, 3, 4, 5)\n0 + 1 + 2 + c(3, 4, 5)\n0 + 1 + 2 + 3 + c(4, 5)\n0 + 1 + 2 + 3 + 4 + c(5)\n0 + 1 + 2 + 3 + 4 + 5\n\n0 had to be added as in “init”. You can also specify this “init” to Reduce():\n\nReduce(`+`, c(1, 2, 3, 4, 5), init = 20)\n## [1] 35\n\nSo what multi_join() does, is the same operation as in the example above, but where the function is a user supplied join or merge function, and the list of datasets is the one read with read_list().\n\n\nLet’s see what happens when we use multi_join() on our list:\n\nmerged_data &lt;- multi_join(list_of_data_sets, full_join)\nclass(merged_data)\n## [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\nglimpse(merged_data)\n## Observations: 57\n## Variables: 3\n## $ col1 &lt;chr&gt; \"0,018930679\", \"0,8748013128\", \"0,1025635934\", \"0,6246140...\n## $ col2 &lt;chr&gt; \"0,0377725807\", \"0,5959457638\", \"0,4429121533\", \"0,558387...\n## $ col3 &lt;chr&gt; \"0,6241767189\", \"0,031324594\", \"0,2238059868\", \"0,2773350...\n\nYou should make sure that all the data frames have the same column names but you can also join data frames with different column names if you give the argument by to the join function. This is possible thanks to … that allows you to pass further argument to join_func().\n\n\nThis function was inspired by the one found on the blog Coffee and Econometrics in the Morning."
  },
  {
    "objectID": "posts/2016-03-31-unit-testing-with-r.html",
    "href": "posts/2016-03-31-unit-testing-with-r.html",
    "title": "Unit testing with R",
    "section": "",
    "text": "I've been introduced to unit testing while working with colleagues on quite a big project for which we use Python.\n\n\nAt first I was a bit skeptical about the need of writing unit tests, but now I must admit that I am seduced by the idea and by the huge time savings it allows. Naturally, I was wondering if the same could be achieved with R, and was quite happy to find out that it also possible to write unit tests in R using a package called testthat.\n\n\nUnit tests (Not to be confused with unit root tests for time series) are small functions that test your code and help you make sure everything is alright. I'm going to show how the testthat packages works with a very trivial example, that might not do justice to the idea of unit testing. But you'll hopefully see why writing unit tests is not a waste of your time, especially if your project gets very complex (if you're writing a package for example).\n\n\nFirst, you'll need to download and install testthat. Some dependencies will also be installed.\n\n\nNow, you'll need a function to test. Let's suppose you've written a function that returns the nth Fibonacci number:\n\nFibonacci &lt;- function(n){\n    a &lt;- 0\n    b &lt;- 1\n    for (i in 1:n){\n        temp &lt;- b\n        b &lt;- a\n        a &lt;- a + temp\n    }\n    return(a)\n}\n\n\nYou then save this function in a file, let's call it fibo.R. What you'll probably do once you've written this function, is to try it out:\n\nFibonacci(5)\n\n## [1] 5\n\n\nYou'll see that the function returns the right result and continue programming. The idea behind unit testing is write a bunch of functions that you can run after you make changes to your code, just to check that everything is still running as it should.\n\n\nLet's create a script called test_fibo.R and write the following code in it:\n\ntest_that(\"Test Fibo(15)\",{\n  phi &lt;- (1 + sqrt(5))/2\n  psi &lt;- (1 - sqrt(5))/2\n  expect_equal(Fibonacci(15), (phi**15 - psi**15)/sqrt(5))\n})\n\n\nThe code above uses Binet's formula, a closed form formula that gives the nth Fibonacci number and compares it our implementation of the algorithm. If you didn't know about Binet's formula, you could simply compute some numbers by hand and compare them to what your function returns, for example. The function expect_equal is a function from the package testthat and does exactly what it tells. We expect the result of our implementation to be equal to the result of Binet's Formula. The file test_fibo.R can contain as many tests as you need. Also, the file that contains the tests must start with the string test, so that testthat knows with files it has to run.\n\n\nNow, we're almost done, create yet another script, let's call it run_tests.R and write the following code in it:\n\nlibrary(testthat) \n\nsource(\"path/to/fibo.R\")\n\ntest_results &lt;- test_dir(\"path/to/tests\", reporter=\"summary\")\n\n\nAfter running these lines, and if everything goes well, you should see a message like this:\n\n&gt; library(testthat)\n&gt; source(\"path/to/fibo.R\")\n&gt; test_results &lt;- test_dir(\"path/to/tests\", reporter=\"summary\")\n\n.\nYour tests are dandy! \n\n\nNotice the small . over the message? This means that one test was run successfully. You'll get one dot per successful test. If you take a look at test_results you'll see this:\n\n&gt; test_results\n         file context          test nb failed skipped error  user system  real\n1 test_fibo.R         Test Fibo(15)  1      0   FALSE FALSE 0.004      0 0.006\n\n\nYou'll see each file and each function inside the files that were tested, and also whether the test was skipped, failed etc. This may seem overkill for such a simple function, but imagine that you write dozens of functions that get more and more complex over time. You might have to change a lot of lines because as time goes by you add new functionality, but don't want to break what was working. Running your unit tests each time you make changes can help you pinpoint regressions in your code. Unit tests can also help you start with your code. It can happen that sometimes you don't know exactly how to start; well you could start by writing a unit test that returns the result you want to have and then try to write the code to make that unit test pass. This is called test-driven development.\n\n\nI hope that this post motivated you to write unit tests and make you a better R programmer!"
  },
  {
    "objectID": "posts/2019-07-19-statmatch.html",
    "href": "posts/2019-07-19-statmatch.html",
    "title": "Statistical matching, or when one single data source is not enough",
    "section": "",
    "text": "I was recently asked how to go about matching several datasets where different samples of individuals were interviewed. This sounds like a big problem; say that you have dataset A and B, and that A contain one sample of individuals, and B another sample of individuals, then how could you possibly match the datasets? Matching datasets requires a common identifier, for instance, suppose that A contains socio-demographic information on a sample of individuals I, while B, contains information on wages and hours worked on the same sample of individuals I, then yes, it will be possible to match/merge/join both datasets.\n\n\nBut that was not what I was asked about; I was asked about a situation where the same population gets sampled twice, and each sample answers to a different survey. For example the first survey is about labour market information and survey B is about family structure. Would it be possible to combine the information from both datasets?\n\n\nTo me, this sounded a bit like missing data imputation problem, but where all the information about the variables of interest was missing! I started digging a bit, and found that not only there was already quite some literature on it, there is even a package for this, called {StatMatch} with a very detailed vignette. The vignette is so detailed, that I will not write any code, I just wanted to share this package!"
  },
  {
    "objectID": "posts/2018-01-05-lists_all_the_way2.html",
    "href": "posts/2018-01-05-lists_all_the_way2.html",
    "title": "It’s lists all the way down, part 2: We need to go deeper",
    "section": "",
    "text": "Shortly after my previous blog post, I saw this tweet on my timeline:\n\n\n\nThe purrr resolution for 2018 - learn at least one purrr function per week - is officially launched with encouragement and inspiration from @statwonk and @hadleywickham. We start with modify_depth: https://t.co/dCMnSHP7Pl. Please join to learn and share. #rstats\n\n— Isabella R. Ghement (@IsabellaGhement) January 3, 2018\n\n\n\nThis is a great initiative, and a big coincidence, as I just had blogged about nested lists and how to map over them. I also said this in my previous blog post:\n\n\n\nThere is also another function that you might want to study, modify_depth() which solves related issues but I will end the blog post here. I might talk about it in a future blog post.\n\n\n\nAnd so after I got this reply from @IsabellaGhement:\n\n\n\nBruno, I would love it if you would chime in with an explicit contrast between nested map calls (which I personally find a bit clunky) and alternatives. In other words, present solutions side-by-side and highlight pros and cons. That would be very useful! 🤗\n\n— Isabella R. Ghement (@IsabellaGhement) January 4, 2018\n\n\n\nWhat else was I supposed to do than blog about purrr::modify_depth()?\n\n\nBear in mind that I was not really familiar with this function before writing my last blog post; and even then, I decided to keep it for another blog post, which is this one. Which came much faster than what I had originally planned. So I might have missed some functionality; if that’s the case don’t hesitate to tweet me an example or send me an email! (bruno at brodrigues dot co)\n\n\nSo what is this blog post about? It’s about lists, nested lists, and some things that you can do with them. Let’s use the same example as in my last post:\n\nlibrary(tidyverse)\ndata(mtcars)\n\nnice_function = function(df, param1, param2){\n  df = df %&gt;%\n    filter(cyl == param1, am == param2) %&gt;%\n    mutate(result = mpg * param1 * (2 - param2))\n\n  return(df)\n}\n\nnice_function(mtcars, 4, 0)\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\nvalues_cyl = c(4, 6, 8)\n\nvalues_am = c(0, 1)\n\nNow that we’re here, we would like to apply nice_function() to each element of values_cyl and values_am. In essence, loop over these values. But because loops are not really easy to manipulate, (as explained, in part, here) I use the map* family of functions included in purrr (When I teach R, I only show loops in the advanced topics chapter of my notes). So let’s “loop” over values_cyl and values_am with map() (and not map_df(); there is a reason for this, bear with me):\n\n(result = map(values_am, ~map(values_cyl, nice_function, df = mtcars, param2 = .)))\n## [[1]]\n## [[1]][[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\n## \n## [[1]][[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## \n## [[1]][[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## \n## \n## [[2]]\n## [[2]][[1]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## \n## [[2]][[2]]\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2\n## \n## [[2]][[3]]\n##    mpg cyl disp  hp drat   wt qsec vs am gear carb result\n## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4\n## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0\n\nUntil now, nothing new compared to my previous post (so if you have a hard time to follow what I’m doing here, go read it here).\n\n\nAs far as I know, there is no way, in this example, to avoid this nested map call. However, suppose now that you want to apply a function to each single data frame contained in the list result. Of course, here, you could simply use bind_rows() to have a single data frame and then apply your function to it. But suppose that you want to keep this list structure; at the end, I will give an example of why you might want that, using another purrr function, walk() and Thomas’ J. Leeper brilliant rio package.\n\n\nSo suppose you want to use this function here:\n\ndouble_col = function(dataset, col){\n  col = enquo(col)\n  col_name = paste0(\"double_\", quo_name(col))\n  dataset %&gt;%\n    mutate(!!col_name := 2*(!!col))\n}\n\nto double the values of a column of a dataset. It uses tidyeval’s enquo(), quo_name() and !!() functions to make it work with tidyverse functions such as mutate(). You can use it like this:\n\ndouble_col(mtcars, hp)\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb double_hp\n## 1  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4       220\n## 2  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4       220\n## 3  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1       186\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1       220\n## 5  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2       350\n## 6  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1       210\n## 7  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4       490\n## 8  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2       124\n## 9  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2       190\n## 10 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4       246\n## 11 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4       246\n## 12 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3       360\n## 13 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3       360\n## 14 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3       360\n## 15 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4       410\n## 16 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4       430\n## 17 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4       460\n## 18 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1       132\n## 19 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2       104\n## 20 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1       130\n## 21 21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1       194\n## 22 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2       300\n## 23 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2       300\n## 24 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4       490\n## 25 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2       350\n## 26 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1       132\n## 27 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2       182\n## 28 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2       226\n## 29 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4       528\n## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6       350\n## 31 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8       670\n## 32 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2       218\n\nNice, but you want to use this function on all of the data frames contained in your result list. You can use a nested map() as before:\n\nmap(result, ~map(., .f = double_col, col = disp))\n## [[1]]\n## [[1]][[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2\n## \n## [[1]][[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8       516.0\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2       450.0\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4       335.2\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6       335.2\n## \n## [[1]][[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2       720.0\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8       720.0\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4       551.6\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8       551.6\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2       551.6\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4       944.0\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4       920.0\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2       880.0\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0       636.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2       608.0\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8       700.0\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2       800.0\n## \n## \n## [[2]]\n## [[2]][[1]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2       216.0\n## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6       157.4\n## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6       151.4\n## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6       142.2\n## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2       158.0\n## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0       240.6\n## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6       190.2\n## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6       242.0\n## \n## [[2]][[2]]\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0         320\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0         320\n## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2         290\n## \n## [[2]][[3]]\n##    mpg cyl disp  hp drat   wt qsec vs am gear carb result double_disp\n## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4         702\n## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0         602\n\nbut there’s an easier solution, which is using modify_depth():\n\n(result = modify_depth(result, .depth = 2, double_col, col = disp))\n## [[1]]\n## [[1]][[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2\n## \n## [[1]][[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8       516.0\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2       450.0\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4       335.2\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6       335.2\n## \n## [[1]][[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2       720.0\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8       720.0\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4       551.6\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8       551.6\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2       551.6\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4       944.0\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4       920.0\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2       880.0\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0       636.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2       608.0\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8       700.0\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2       800.0\n## \n## \n## [[2]]\n## [[2]][[1]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2       216.0\n## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6       157.4\n## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6       151.4\n## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6       142.2\n## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2       158.0\n## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0       240.6\n## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6       190.2\n## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6       242.0\n## \n## [[2]][[2]]\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0         320\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0         320\n## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2         290\n## \n## [[2]][[3]]\n##    mpg cyl disp  hp drat   wt qsec vs am gear carb result double_disp\n## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4         702\n## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0         602\n\nSo how does it work? modify_depth() needs a list and a .depth argument, which corresponds to where you you want to apply your function. The following lines of code might help you understand:\n\n# Depth of 1:\n\nresult[[1]]\n## [[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2\n## \n## [[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8       516.0\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2       450.0\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4       335.2\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6       335.2\n## \n## [[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2       720.0\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8       720.0\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4       551.6\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8       551.6\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2       551.6\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4       944.0\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4       920.0\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2       880.0\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0       636.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2       608.0\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8       700.0\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2       800.0\n\nIn this example, a depth of 1 corresponds to a list of three data frame. Can you use your function double_col() on a list of three data frames? No, because the domain of double_col() is the set of data frames, not the set of lists of data frames. So you need to go deeper:\n\n# Depth of 2:\n\nresult[[1]][[1]] # or try result[[1]][[2]] or result[[1]][[3]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2\n\nAt the depth of 2, you’re dealing with data frames! So you can use your function double_col(). With a depth of 2, one might not see the added value of modify_depth() over nested map calls, but if you have to go even deeper, nested map calls are very confusing and verbose.\n\n\nNow for the last part; why doing all this, and not simply bind all the rows, apply double_col() and call it a day? Well, suppose that there is a reason you have these data frames inside lists; for example, the first element, i.e., result[[1]] might be data for, say, Portugal, for 3 different years. result[[2]] however, is data for France, for the same years. Suppose also that you have to give this data, after having worked on it, to a colleague (or to another institution) in the Excel format; one Excel workbook per country, one sheet per year. This example might seem contrived, but I have been confronted to this exact situation very often. Well, if you bind all the rows together, how are you going to save the data in the workbooks like you are required to?\n\n\nWell, thanks to rio, one line of code is enough:\n\nlibrary(rio)\n\nwalk2(result, list(\"portugal.xlsx\", \"france.xlsx\"), export)\n\nI know what you’re thinking; Bruno, that’s two lines of code!. Yes, but I had to load rio. Also, walk() (and walk2()) are basically the same as map(), but you use walk() over map() when you are only interested in the side effect of the function you are applying over your list; here, export() which is rio’s function to write data to disk. The side effect of this function is… writing data to disk! You could have used map2() just the same, but I wanted to show you walk2() (however, you cannot replace map() by walk() in most cases; try it and see what happens).\n\n\nHere’s what it looks like:\n\n\n\n\n\nI have two Excel workbooks, (one per list), where each sheet is a data frame!\n\n\nIf you enjoy these blog posts, you can follow me on twitter."
  },
  {
    "objectID": "posts/2019-03-03-historical_vowpal.html",
    "href": "posts/2019-03-03-historical_vowpal.html",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 1",
    "section": "",
    "text": "Can I get enough of historical newspapers data? Seems like I don’t. I already wrote four (1, 2, 3 and 4) blog posts, but there’s still a lot to explore. This blog post uses a new batch of data announced on twitter:\nand this data could not have arrived at a better moment, since something else got announced via Twitter recently:\nI wanted to try using Vowpal Wabbit for a couple of weeks now because it seems to be the perfect tool for when you’re dealing with what I call big-ish data: data that is not big data, and might fit in your RAM, but is still a PITA to deal with. It can be data that is large enough to take 30 seconds to be imported into R, and then every operation on it lasts for minutes, and estimating/training a model on it might eat up all your RAM. Vowpal Wabbit avoids all this because it’s an online-learning system. Vowpal Wabbit is capable of training a model with data that it sees on the fly, which means VW can be used for real-time machine learning, but also for when the training data is very large. Each row of the data gets streamed into VW which updates the estimated parameters of the model (or weights) in real time. So no need to first import all the data into R!\nThe goal of this blog post is to get started with VW, and build a very simple logistic model to classify documents using the historical newspapers data from the National Library of Luxembourg, which you can download here (scroll down and download the Text Analysis Pack). The goal is not to build the best model, but a model. Several steps are needed for this: prepare the data, install VW and train a model using {RVowpalWabbit}."
  },
  {
    "objectID": "posts/2019-03-03-historical_vowpal.html#step-1-preparing-the-data",
    "href": "posts/2019-03-03-historical_vowpal.html#step-1-preparing-the-data",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 1",
    "section": "\nStep 1: Preparing the data\n",
    "text": "Step 1: Preparing the data\n\n\nThe data is in a neat .xml format, and extracting what I need will be easy. However, the input format for VW is a bit unusual; it resembles .psv files (Pipe Separated Values) but allows for more flexibility. I will not dwell much into it, but for our purposes, the file must look like this:\n\n1 | this is the first observation, which in our case will be free text\n2 | this is another observation, its label, or class, equals 2\n4 | this is another observation, of class 4\n\nThe first column, before the “|” is the target class we want to predict, and the second column contains free text.\n\n\nThe raw data looks like this:\n\n\n\n\nClick if you want to see the raw data\n\n\n&lt;OAI-PMH xmlns=\"http://www.openarchives.org/OAI/2.0/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd\"&gt;\n&lt;responseDate&gt;2019-02-28T11:13:01&lt;/responseDate&gt;\n&lt;request&gt;http://www.eluxemburgensia.lu/OAI&lt;/request&gt;\n&lt;ListRecords&gt;\n&lt;record&gt;\n&lt;header&gt;\n&lt;identifier&gt;digitool-publish:3026998-DTL45&lt;/identifier&gt;\n&lt;datestamp&gt;2019-02-28T11:13:01Z&lt;/datestamp&gt;\n&lt;/header&gt;\n&lt;metadata&gt;\n&lt;oai_dc:dc xmlns:oai_dc=\"http://www.openarchives.org/OAI/2.0/oai_dc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:dcterms=\"http://purl.org/dc/terms/\" xsi:schemaLocation=\"http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd\"&gt;\n&lt;dc:identifier&gt;\nhttps://persist.lu/ark:/70795/6gq1q1/articles/DTL45\n&lt;/dc:identifier&gt;\n&lt;dc:source&gt;newspaper/indeplux/1871-12-29_01&lt;/dc:source&gt;\n&lt;dcterms:isPartOf&gt;L'indépendance luxembourgeoise&lt;/dcterms:isPartOf&gt;\n&lt;dcterms:isReferencedBy&gt;\nissue:newspaper/indeplux/1871-12-29_01/article:DTL45\n&lt;/dcterms:isReferencedBy&gt;\n&lt;dc:date&gt;1871-12-29&lt;/dc:date&gt;\n&lt;dc:publisher&gt;Jean Joris&lt;/dc:publisher&gt;\n&lt;dc:relation&gt;3026998&lt;/dc:relation&gt;\n&lt;dcterms:hasVersion&gt;\nhttp://www.eluxemburgensia.lu/webclient/DeliveryManager?pid=3026998#panel:pp|issue:3026998|article:DTL45\n&lt;/dcterms:hasVersion&gt;\n&lt;dc:description&gt;\nCONSEIL COMMUNAL de la ville de Luxembourg. Séance du 23 décembre 1871. (Suite.) Art. 6. Glacière communale. M. le Bourgmcstr ¦ . Le collège échevinal propose un autro mode de se procurer de la glace. Nous avons dépensé 250 fr. cha- que année pour distribuer 30 kilos do glace; c’est une trop forte somme pour un résultat si minime. Nous aurions voulu nous aboucher avec des fabricants de bière ou autres industriels qui nous auraient fourni de la glace en cas de besoin. L’architecte qui été chargé de passer un contrat, a été trouver des négociants, mais ses démarches n’ont pas abouti. \n&lt;/dc:description&gt;\n&lt;dc:title&gt;\nCONSEIL COMMUNAL de la ville de Luxembourg. Séance du 23 décembre 1871. (Suite.)\n&lt;/dc:title&gt;\n&lt;dc:type&gt;ARTICLE&lt;/dc:type&gt;\n&lt;dc:language&gt;fr&lt;/dc:language&gt;\n&lt;dcterms:extent&gt;863&lt;/dcterms:extent&gt;\n&lt;/oai_dc:dc&gt;\n&lt;/metadata&gt;\n&lt;/record&gt;\n&lt;/ListRecords&gt;\n&lt;/OAI-PMH&gt;\n\n\nI need several things from this file:\n\n\n\nThe title of the newspaper: &lt;dcterms:isPartOf&gt;L’indépendance luxembourgeoise&lt;/dcterms:isPartOf&gt;\n\n\nThe type of the article: &lt;dc:type&gt;ARTICLE&lt;/dc:type&gt;. Can be Article, Advertisement, Issue, Section or Other.\n\n\nThe contents: &lt;dc:description&gt;CONSEIL COMMUNAL de la ville de Luxembourg. Séance du ….&lt;/dc:description&gt;\n\n\n\nI will only focus on newspapers in French, even though newspapers in German also had articles in French. This is because the tag &lt;dc:language&gt;fr&lt;/dc:language&gt; is not always available. If it were, I could simply look for it and extract all the content in French easily, but unfortunately this is not the case.\n\n\nFirst of all, let’s get the data into R:\n\nlibrary(\"tidyverse\")\nlibrary(\"xml2\")\nlibrary(\"furrr\")\n\nfiles &lt;- list.files(path = \"export01-newspapers1841-1878/\", all.files = TRUE, recursive = TRUE)\n\nThis results in a character vector with the path to all the files:\n\nhead(files)\n[1] \"000/1400000/1400000-ADVERTISEMENT-DTL78.xml\"   \"000/1400000/1400000-ADVERTISEMENT-DTL79.xml\"  \n[3] \"000/1400000/1400000-ADVERTISEMENT-DTL80.xml\"   \"000/1400000/1400000-ADVERTISEMENT-DTL81.xml\"  \n[5] \"000/1400000/1400000-MODSMD_ARTICLE1-DTL34.xml\" \"000/1400000/1400000-MODSMD_ARTICLE2-DTL35.xml\"\n\nNow I write a function that does the needed data preparation steps. I describe what the function does in the comments inside:\n\nto_vw &lt;- function(xml_file){\n\n    # read in the xml file\n    file &lt;- read_xml(paste0(\"export01-newspapers1841-1878/\", xml_file))\n\n    # Get the newspaper\n    newspaper &lt;- xml_find_all(file, \".//dcterms:isPartOf\") %&gt;% xml_text()\n\n    # Only keep the newspapers written in French\n    if(!(newspaper %in% c(\"L'UNION.\",\n                          \"L'indépendance luxembourgeoise\",\n                          \"COURRIER DU GRAND-DUCHÉ DE LUXEMBOURG.\",\n                          \"JOURNAL DE LUXEMBOURG.\",\n                          \"L'AVENIR\",\n                          \"L’Arlequin\",\n                          \"La Gazette du Grand-Duché de Luxembourg\",\n                          \"L'AVENIR DE LUXEMBOURG\",\n                          \"L'AVENIR DU GRAND-DUCHE DE LUXEMBOURG.\",\n                          \"L'AVENIR DU GRAND-DUCHÉ DE LUXEMBOURG.\",\n                          \"Le gratis luxembourgeois\",\n                          \"Luxemburger Zeitung – Journal de Luxembourg\",\n                          \"Recueil des mémoires et des travaux publiés par la Société de Botanique du Grand-Duché de Luxembourg\"))){\n        return(NULL)\n    } else {\n        # Get the type of the content. Can be article, advert, issue, section or other\n        type &lt;- xml_find_all(file, \".//dc:type\") %&gt;% xml_text()\n\n        type &lt;- case_when(type == \"ARTICLE\" ~ \"1\",\n                          type == \"ADVERTISEMENT\" ~ \"2\",\n                          type == \"ISSUE\" ~ \"3\",\n                          type == \"SECTION\" ~ \"4\",\n                          TRUE ~ \"5\"\n        )\n\n        # Get the content itself. Only keep alphanumeric characters, and remove any line returns or \n        # carriage returns\n        description &lt;- xml_find_all(file, \".//dc:description\") %&gt;%\n            xml_text() %&gt;%\n            str_replace_all(pattern = \"[^[:alnum:][:space:]]\", \"\") %&gt;%\n            str_to_lower() %&gt;%\n            str_replace_all(\"\\r?\\n|\\r|\\n\", \" \")\n\n        # Return the final object: one line that looks like this\n        # 1 | bla bla\n        paste(type, \"|\", description)\n    }\n\n}\n\nI can now run this code to parse all the files, and I do so in parallel, thanks to the {furrr} package:\n\nplan(multiprocess, workers = 12)\n\ntext_fr &lt;- files %&gt;%\n    future_map(to_vw)\n\ntext_fr &lt;- text_fr %&gt;%\n    discard(is.null)\n\nwrite_lines(text_fr, \"text_fr.txt\")"
  },
  {
    "objectID": "posts/2019-03-03-historical_vowpal.html#step-2-install-vowpal-wabbit",
    "href": "posts/2019-03-03-historical_vowpal.html#step-2-install-vowpal-wabbit",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 1",
    "section": "\nStep 2: Install Vowpal Wabbit\n",
    "text": "Step 2: Install Vowpal Wabbit\n\n\nTo easiest way to install VW must be using Anaconda, and more specifically the conda package manager. Anaconda is a Python (and R) distribution for scientific computing and it comes with a package manager called conda which makes installing Python (or R) packages very easy. While VW is a standalone piece of software, it can also be installed by conda or pip. Instead of installing the full Anaconda distribution, you can install Miniconda, which only comes with the bare minimum: a Python executable and the conda package manager. You can find Miniconda here and once it’s installed, you can install VW with:\n\nconda install -c gwerbin vowpal-wabbit \n\nIt is also possible to install VW with pip, as detailed here, but in my experience, managing Python packages with pip is not super. It is better to manage your Python distribution through conda, because it creates environments in your home folder which are independent of the system’s Python installation, which is often out-of-date."
  },
  {
    "objectID": "posts/2019-03-03-historical_vowpal.html#step-3-building-a-model",
    "href": "posts/2019-03-03-historical_vowpal.html#step-3-building-a-model",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 1",
    "section": "\nStep 3: Building a model\n",
    "text": "Step 3: Building a model\n\n\nVowpal Wabbit can be used from the command line, but there are interfaces for Python and since a few weeks, for R. The R interface is quite crude for now, as it’s still in very early stages. I’m sure it will evolve, and perhaps a Vowpal Wabbit engine will be added to {parsnip}, which would make modeling with VW really easy.\n\n\nFor now, let’s only use 10000 lines for prototyping purposes before running the model on the whole file. Because the data is quite large, I do not want to import it into R. So I use command line tools to manipulate this data directly from my hard drive:\n\n# Prepare data\nsystem2(\"shuf\", args = \"-n 10000 text_fr.txt &gt; small.txt\")\n\nshuf is a Unix command, and as such the above code should work on GNU/Linux systems, and most likely macOS too. shuf generates random permutations of a given file to standard output. I use &gt; to direct this output to another file, which I called small.txt. The -n 10000 options simply means that I want 10000 lines.\n\n\nI then split this small file into a training and a testing set:\n\n# Adapted from http://bitsearch.blogspot.com/2009/03/bash-script-to-split-train-and-test.html\n\n# The command below counts the lines in small.txt. This is not really needed, since I know that the \n# file only has 10000 lines, but I kept it here for future reference\n# notice the stdout = TRUE option. This is needed because the output simply gets shown in R's\n# command line and does get saved into a variable.\nnb_lines &lt;- system2(\"cat\", args = \"small.txt | wc -l\", stdout = TRUE)\n\nsystem2(\"split\", args = paste0(\"-l\", as.numeric(nb_lines)*0.99, \" small.txt data_split/\"))\n\nsplit is the Unix command that does the splitting. I keep 99% of the lines in the training set and 1% in the test set. This creates two files, aa and ab. I rename them using the mv Unix command:\n\nsystem2(\"mv\", args = \"data_split/aa data_split/small_train.txt\")\nsystem2(\"mv\", args = \"data_split/ab data_split/small_test.txt\")\n\nOk, now let’s run a model using the VW command line utility from R, using system2():\n\noaa_fit &lt;- system2(\"~/miniconda3/bin/vw\", args = \"--oaa 5 -d data_split/small_train.txt -f small_oaa.model\", stderr = TRUE)\n\nI need to point system2() to the vw executable, and then add some options. –oaa stands for one against all and is a way of doing multiclass classification; first, one class gets classified by a logistic classifier against all the others, then the other class against all the others, then the other…. The 5 in the option means that there are 5 classes.\n\n\n-d data_split/train.txt specifies the path to the training data. -f means “final regressor” and specifies where you want to save the trained model.\n\n\nThis is the output that get’s captured and saved into oaa_fit:\n\n [1] \"final_regressor = oaa.model\"                                             \n [2] \"Num weight bits = 18\"                                                    \n [3] \"learning rate = 0.5\"                                                     \n [4] \"initial_t = 0\"                                                           \n [5] \"power_t = 0.5\"                                                           \n [6] \"using no cache\"                                                          \n [7] \"Reading datafile = data_split/train.txt\"                                 \n [8] \"num sources = 1\"                                                         \n [9] \"average  since         example        example  current  current  current\"\n[10] \"loss     last          counter         weight    label  predict features\"\n[11] \"1.000000 1.000000            1            1.0        3        1       87\"\n[12] \"1.000000 1.000000            2            2.0        1        3     2951\"\n[13] \"1.000000 1.000000            4            4.0        1        3      506\"\n[14] \"0.625000 0.250000            8            8.0        1        1      262\"\n[15] \"0.625000 0.625000           16           16.0        1        2      926\"\n[16] \"0.500000 0.375000           32           32.0        4        1        3\"\n[17] \"0.375000 0.250000           64           64.0        1        1      436\"\n[18] \"0.296875 0.218750          128          128.0        2        2      277\"\n[19] \"0.238281 0.179688          256          256.0        2        2      118\"\n[20] \"0.158203 0.078125          512          512.0        2        2       61\"\n[21] \"0.125000 0.091797         1024         1024.0        2        2      258\"\n[22] \"0.096191 0.067383         2048         2048.0        1        1       45\"\n[23] \"0.085205 0.074219         4096         4096.0        1        1      318\"\n[24] \"0.076172 0.067139         8192         8192.0        2        1      523\"\n[25] \"\"                                                                        \n[26] \"finished run\"                                                            \n[27] \"number of examples = 9900\"                                               \n[28] \"weighted example sum = 9900.000000\"                                      \n[29] \"weighted label sum = 0.000000\"                                           \n[30] \"average loss = 0.073434\"                                                 \n[31] \"total feature number = 4456798\"  \n\nNow, when I try to run the same model using RVowpalWabbit::vw() I get the following error:\n\noaa_class &lt;- c(\"--oaa\", \"5\",\n               \"-d\", \"data_split/small_train.txt\",\n               \"-f\", \"vw_models/small_oaa.model\")\n\nresult &lt;- vw(oaa_class)\nError in Rvw(args) : unrecognised option '--oaa'\n\nI think the problem might be because I installed Vowpal Wabbit using conda, and the package cannot find the executable. I’ll open an issue with reproducible code and we’ll see.\n\n\nIn any case, that’s it for now! In the next blog post, we’ll see how to get the accuracy of this very simple model, and see how to improve it!"
  },
  {
    "objectID": "posts/2013-01-29-method-of-simulated-moments-with-r.html",
    "href": "posts/2013-01-29-method-of-simulated-moments-with-r.html",
    "title": "Method of Simulated Moments with R",
    "section": "",
    "text": "This document details section 12.5.6. Unobserved Heterogeneity Example. The original source code giving the results from table 12.3 are available from the authors' site here and written for Stata. This is an attempt to translate the code to R.\n\n\nConsult the original source code if you want to read the authors' comments. If you want the R source code without all the commentaries, grab it here. This is not guaranteed to work, nor to be correct. It could set your pet on fire and/or eat your first born. Use at your own risk. I may, or may not, expand this example. Corrections, constructive criticism are welcome.\n\n\nThe model is the same as the one described here, so I won't go into details. The moment condition used is \\(E[(y_i-\\theta-u_i)]=0\\), so we can replace the expectation operator by the empirical mean:\n\n\n\\[\\dfrac{1}{N} \\sum_{i=1}^N(y_i - \\theta - E[u_i])=0\\]\n\n\nSupposing that \\(E[\\overline{u}]\\) is unknown, we can instead use the method of simulated moments for \\(\\theta\\) defined by:\n\n\n\\[\\dfrac{1}{N} \\sum_{i=1}^N(y_i - \\theta - \\dfrac{1}{S} \\sum_{s=1}^S u_i^s)=0\\]\n\n\nImport the data\n\n\nYou can consult the original source code to see how the authors simulated the data. To get the same results, and verify that I didn't make mistakes I prefer importing their data directly from their website.\n\n\ndata &lt;- read.table(\"http://cameron.econ.ucdavis.edu/mmabook/mma12p2mslmsm.asc\")\nu &lt;- data[, 1]\ne &lt;- data[, 2]\ny &lt;- data[, 3]\nnumobs &lt;- length(u)\nsimreps &lt;- 10000\n\n\nSimulation\n\n\nIn the code below, we simulate the equation defined above:\n\n\nusim &lt;- -log(-log(runif(simreps)))\nesim &lt;- rnorm(simreps, 0, 1)\n\nisim &lt;- 0\nwhile (isim &lt; simreps) {\n\n    usim = usim - log(-log(runif(simreps)))\n    esim = esim + rnorm(simreps, 0, 1)\n\n    isim = isim + 1\n\n}\n\nusimbar = usim/simreps\nesimbar = esim/simreps\n\ntheta = y - usimbar - esimbar\n\ntheta_msm &lt;- mean(theta)\napprox_sterror &lt;- sd(theta)/sqrt(simreps)\n\n\nThese steps yield the following results:\n\n\ntheta_msm\n\n[1] 1.187978\n\n\nand\n\napprox_sterror\n\n[1] 0.01676286"
  },
  {
    "objectID": "posts/2016-07-26-read-a-lot-of-datasets-at-once-with-r.html",
    "href": "posts/2016-07-26-read-a-lot-of-datasets-at-once-with-r.html",
    "title": "Read a lot of datasets at once with R",
    "section": "",
    "text": "I often have to read a lot of datasets at once using R. So I’ve wrote the following function to solve this issue:\n\nread_list &lt;- function(list_of_datasets, read_func){\n\n        read_and_assign &lt;- function(dataset, read_func){\n                dataset_name &lt;- as.name(dataset)\n                dataset_name &lt;- read_func(dataset)\n        }\n\n        # invisible is used to suppress the unneeded output\n        output &lt;- invisible(\n                sapply(list_of_datasets,\n                           read_and_assign, read_func = read_func, simplify = FALSE, USE.NAMES = TRUE))\n\n        # Remove the extension at the end of the data set names\n        names_of_datasets &lt;- c(unlist(strsplit(list_of_datasets, \"[.]\"))[c(T, F)])\n        names(output) &lt;- names_of_datasets\n        return(output)\n}\n\nYou need to supply a list of datasets as well as the function to read the datasets to read_list. So for example to read in .csv files, you could use read.csv() (or read_csv() from the readr package, which I prefer to use), or read_dta() from the package haven for STATA files, and so on.\n\n\nNow imagine you have some data in your working directory. First start by saving the name of the datasets in a variable:\n\ndata_files &lt;- list.files(pattern = \".csv\")\n\nprint(data_files)\n## [1] \"data_1.csv\" \"data_2.csv\" \"data_3.csv\"\n\nNow you can read all the data sets and save them in a list with read_list():\n\nlibrary(\"readr\")\nlibrary(\"tibble\")\n\nlist_of_data_sets &lt;- read_list(data_files, read_csv)\n\n\nglimpse(list_of_data_sets)\n## List of 3\n##  $ data_1:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,018930679\" \"0,8748013128\" \"0,1025635934\" \"0,6246140983\" ...\n##   ..$ col2: chr [1:19] \"0,0377725807\" \"0,5959457638\" \"0,4429121533\" \"0,558387159\" ...\n##   ..$ col3: chr [1:19] \"0,6241767189\" \"0,031324594\" \"0,2238059868\" \"0,2773350732\" ...\n##  $ data_2:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9098418493\" \"0,1127788509\" \"0,5818891392\" \"0,1011773532\" ...\n##   ..$ col2: chr [1:19] \"0,7455905887\" \"0,4015039612\" \"0,6625796605\" \"0,029955339\" ...\n##   ..$ col3: chr [1:19] \"0,327232932\" \"0,2784035673\" \"0,8092386735\" \"0,1216045306\" ...\n##  $ data_3:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9236124896\" \"0,6303271761\" \"0,6413583054\" \"0,5573887416\" ...\n##   ..$ col2: chr [1:19] \"0,2114708388\" \"0,6984538266\" \"0,0469865249\" \"0,9271510226\" ...\n##   ..$ col3: chr [1:19] \"0,4941919971\" \"0,7391538511\" \"0,3876723797\" \"0,2815014394\" ...\n\nIf you prefer not to have the datasets in a list, but rather import them into the global environment, you can change the above function like so:\n\nread_list &lt;- function(list_of_datasets, read_func){\n\n        read_and_assign &lt;- function(dataset, read_func){\n                assign(dataset, read_func(dataset), envir = .GlobalEnv)\n        }\n\n        # invisible is used to suppress the unneeded output\n        output &lt;- invisible(\n                sapply(list_of_datasets,\n                           read_and_assign, read_func = read_func, simplify = FALSE, USE.NAMES = TRUE))\n\n}\n\nBut I personnally don’t like this second option, but I put it here for completeness."
  },
  {
    "objectID": "posts/2014-04-23-r-s4-rootfinding.html",
    "href": "posts/2014-04-23-r-s4-rootfinding.html",
    "title": "Object Oriented Programming with R: An example with a Cournot duopoly",
    "section": "",
    "text": "I started reading Applied Computational Economics & Finance by Mario J. Miranda and Paul L. Fackler. It is a very interesting book that I recommend to every one of my colleagues. The only issue I have with this book, is that the programming language they use is Matlab, which is proprietary. While there is a free as in freedom implementation of the Matlab language, namely Octave, I still prefer using R. In this post, I will illustrate one example the authors present in the book with R, using the package rootSolve. rootSolve implements Newtonian algorithms to find roots of functions; to specify the functions for which I want the roots, I use R's Object-Oriented Programming (OOP) capabilities to build a model that returns two functions. This is optional, but I found that it was a good example to illustrate OOP, even though simpler solutions exist, one of which was proposed by reddit user TheDrownedKraken (whom I thank) and will be presented at the end of the article.\n\n\nTheoretical background\n\n\nThe example is taken from Miranda's and Fackler's book, on page 35. The authors present a Cournot duopoly model. In a Cournot duopoly model, two firms compete against each other by quantities. Both produce a certain quantity of an homogenous good, and take the quantity produce by their rival as given.\n\n\nThe inverse demand of the good is :\n\n\\[P(q) = q^{-\\dfrac{1}{\\eta}}\\]\n\nthe cost function for firm i is:\n\n\\[C_i(q_i) = P(q_1+q_2)*q_i - C_i(q_i)\\]\n\nand the profit for firm i:\n\n\\[\\pi_i(q1,q2) = P(q_1+q_2)q_i - C_i(q_i)\\]\n\nThe optimality condition for firm i is thus:\n\n\\[\\dfrac{\\partial \\pi_i}{\\partial q_i} = (q1+q2)^{-\\dfrac{1}{\\eta}} - \\dfrac{1}{\\eta} (q_1+q_2)^{\\dfrac{-1}{\\eta-1}}q_i - c_iq_i=0.\\]\n\nImplementation in R\n\n\nIf we want to find the optimal quantities (q_1) and (q_2) we need to program the optimality condition and we could also use the jacobian of the optimality condition. The jacobian is generally useful to speed up the root finding routines. This is were OOP is useful. First let's create a new class, called Model:\n\n\nsetClass(Class = \"Model\", slots = list(OptimCond = \"function\", JacobiOptimCond = \"function\"))\n\n\nThis new class has two slots, which here are functions (in general slots are properties of your class); we need the model to return the optimality condition and the jacobian of the optimality condition.\n\n\nNow we can create a function which will return these two functions for certain values of the parameters, c and  of the model:\n\n\nmy_mod &lt;- function(eta, c) {\n    e = -1/eta\n\n    OptimCond &lt;- function(q) {\n        return(sum(q)^e + e * sum(q)^(e - 1) * q - diag(c) %*% q)\n    }\n\n    JacobiOptimCond &lt;- function(q) {\n      return(\n        (e * sum(q)^e) * array(1, c(2, 2)) +\n        (e * sum(q)^(e - 1)) * diag(1, 2) +\n        (e - 1) * e * sum(q)^(e - 2) * q * c(1, 1) - diag(c)\n      )\n    }\n\n    return(new(\"Model\", OptimCond = OptimCond, JacobiOptimCond = JacobiOptimCond))\n\n}\n\n\nThe function my_mod takes two parameters, eta and c and returns two functions, the optimality condition and the jacobian of the optimality condition. Both are now accessible via my_mod(eta=1.6,c = c(0.6,0.8))@OptimCond and my_mod(eta=1.6,c = c(0.6,0.8))@JacobiOptimCond respectively (and by specifying values for eta and c).\n\n\nNow, we can use the rootSolve package to get the optimal values (q_1) and (q_2)\n\n\nlibrary(\"rootSolve\")\n\nmultiroot(f = my_mod(eta = 1.6, c = c(0.6, 0.8))@OptimCond,\n          start = c(1, 1),\n          maxiter = 100,\n          jacfunc = my_mod(eta = 1.6, c = c(0.6, 0.8))@JacobiOptimCond)\n\n## $root\n## [1] 0.8396 0.6888\n## \n## $f.root\n##            [,1]\n## [1,] -2.220e-09\n## [2,]  9.928e-09\n## \n## $iter\n## [1] 4\n## \n## $estim.precis\n## [1] 6.074e-09\n\n\nAfter 4 iterations, we get that  and  are equal to 0.84 and 0.69 respectively, which are the same values as in the book!\n\n\nSuggestion by Reddit user, TheDrownedKraken\n\n\nI posted this blog post on the rstats subbreddit on www.reddit.com. I got a very useful comment by reddit member TheDrownedKraken which suggested the following approach, which doesn't need a new class to be build. I thank him for this. Here is his suggestion:\n\n\ngenerator &lt;- function(eta, a) {\n    e = -1/eta\n\n    OptimCond &lt;- function(q) {\n        return(sum(q)^e + e * sum(q)^(e - 1) * q - diag(a) %*% q)\n    }\n\n    JacobiOptimCond &lt;- function(q) {\n      return(\n        (e * sum(q)^e) * array(1, c(2, 2)) +\n        (e * sum(q)^(e - 1)) * diag(1, 2) +\n        (e - 1) * e * sum(q)^(e - 2) * q * c(1, 1) - diag(a)\n      )\n    }\n\n    return(list(OptimCond = OptimCond, JacobiOptimCond = JacobiOptimCond))\n\n}\n\nf.s &lt;- generator(eta = 1.6, a = c(0.6, 0.8))\n\nmultiroot(f = f.s$OptimCond, start = c(1, 1), maxiter = 100, jacfunc = f.s$JacobiOptimCond)\n\n## $root\n## [1] 0.8396 0.6888\n## \n## $f.root\n##            [,1]\n## [1,] -2.220e-09\n## [2,]  9.928e-09\n## \n## $iter\n## [1] 4\n## \n## $estim.precis\n## [1] 6.074e-09"
  },
  {
    "objectID": "posts/2018-03-12-keep_trying.html",
    "href": "posts/2018-03-12-keep_trying.html",
    "title": "Keep trying that api call with purrr::possibly()",
    "section": "",
    "text": "Sometimes you need to call an api to get some result from a web service, but sometimes this call might fail. You might get an error 500 for example, or maybe you’re making too many calls too fast. Regarding this last point, I really encourage you to read Ethics in Web Scraping.\n\n\nIn this blog post I will show you how you can keep trying to make this api call using purrr::possibly().\n\n\nFor this, let’s use this function that will simulate an api call:\n\nget_data = function(){\n  number = rbinom(1, 1, 0.9)\n  ifelse(number == 0, \"OK\", stop(\"Error: too many calls!\"))\n}\n\nThis function simply returns a random draw from a binomial distribution. If this number equals 0 with probability 0.1, the function returns “OK”, if not, it throws an error. Because the probability of success is only 10%, your api call might be unsuccessful:\n\nget_data()\nError in ifelse(number == 0, \"OK\", stop(\"Error: too many calls!\")) :\n  Error: too many calls!\n\nHow to keep trying until it works? For this, we’re going to use purrr::possibly(); this function takes another function as argument and either returns the result, or another output in case of error, that the user can define:\n\npossibly_get_data = purrr::possibly(get_data, otherwise = NULL)\n\nLet’s try it:\n\nset.seed(12)\npossibly_get_data()\n## NULL\n\nWith set.seed(12), the function returns a number different from 0, and thus throws an error: but because we’re wrapping the function around purrr::possibly(), the function now returns NULL. The first step is done; now we can use this to our advantage:\n\ndefinitely_get_data = function(func, n_tries, sleep, ...){\n\n  possibly_func = purrr::possibly(func, otherwise = NULL)\n\n  result = NULL\n  try_number = 1\n\n  while(is.null(result) && try_number &lt;= n_tries){\n    print(paste(\"Try number: \", try_number))\n    try_number = try_number + 1\n    result = possibly_func(...)\n    Sys.sleep(sleep)\n  }\n\n  return(result)\n}\n\ndefinitely_get_data() is a function that takes any function as argument, as well as a user provided number of tries (as well as … to pass further arguments to func()). Remember, if func() fails, it will return NULL; the while loop ensures that while the result is NULL, and the number of tries is below what you provided, the function will keep getting called. I didn’t talk about sleep; this argument is provided to Sys.sleep() which introduces a break between calls that is equal to sleep seconds. This ensures you don’t make too many calls too fast. Let’s try it out:\n\nset.seed(123)\ndefinitely_get_data(get_data, 10, 1)\n## [1] \"Try number:  1\"\n## [1] \"Try number:  2\"\n## [1] \"Try number:  3\"\n## [1] \"Try number:  4\"\n## [1] \"Try number:  5\"\n## [1] \"OK\"\n\nIt took 5 tries to get the result! However, if after 10 tries get_data() fails to return what you need it will stop (but you can increase the number of tries…).\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2019-03-20-pivot.html",
    "href": "posts/2019-03-20-pivot.html",
    "title": "Pivoting data frames just got easier thanks to pivot_wide() and pivot_long()",
    "section": "",
    "text": "Update: pivot_wide() and pivot_long() are now called pivot_wider() and pivot_longer(), so the code below needs to be updated accondingly.\n\n\nThere’s a lot going on in the development version of {tidyr}. New functions for pivoting data frames, pivot_wide() and pivot_long() are coming, and will replace the current functions, spread() and gather(). spread() and gather() will remain in the package though:\n\n{{% tweet “1108107722128613377” %}}\n\nIf you want to try out these new functions, you need to install the development version of {tidyr}:\n\ndevtools::install_github(\"tidyverse/tidyr\")\n\nand you can read the vignette here. Because these functions are still being developed, some more changes might be introduced, but I guess that the main functionality will not change much.\n\n\nLet’s play around with these functions and the mtcars data set. First let’s load the packages and the data:\n\nlibrary(tidyverse)\ndata(mtcars)\n\nFirst, let’s create a wide dataset, by spreading the levels of the “am” column to two new columns:\n\nmtcars_wide1 &lt;- mtcars %&gt;% \n    pivot_wide(names_from = \"am\", values_from = \"mpg\") \n\nmtcars_wide1 %&gt;% \n    select(`0`, `1`, everything())\n## # A tibble: 32 x 11\n##      `0`   `1`   cyl  disp    hp  drat    wt  qsec    vs  gear  carb\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1  NA    21       6  160    110  3.9   2.62  16.5     0     4     4\n##  2  NA    21       6  160    110  3.9   2.88  17.0     0     4     4\n##  3  NA    22.8     4  108     93  3.85  2.32  18.6     1     4     1\n##  4  21.4  NA       6  258    110  3.08  3.22  19.4     1     3     1\n##  5  18.7  NA       8  360    175  3.15  3.44  17.0     0     3     2\n##  6  18.1  NA       6  225    105  2.76  3.46  20.2     1     3     1\n##  7  14.3  NA       8  360    245  3.21  3.57  15.8     0     3     4\n##  8  24.4  NA       4  147.    62  3.69  3.19  20       1     4     2\n##  9  22.8  NA       4  141.    95  3.92  3.15  22.9     1     4     2\n## 10  19.2  NA       6  168.   123  3.92  3.44  18.3     1     4     4\n## # … with 22 more rows\n\npivot_wide()’s arguments are quite explicit: names_from = is where you specify the column that will be spread across the data frame, meaning, the levels of this column will become new columns. values_from = is where you specify the column that will fill in the values of the new columns.\n\n\n“0” and “1” are the new columns (“am” had two levels, 0 and 1), which contain the miles per gallon for manual and automatic cars respectively. Let’s also take a look at the data frame itself:\n\nmtcars_wide1 %&gt;% \n    select(`0`, `1`, everything())\n## # A tibble: 32 x 11\n##      `0`   `1`   cyl  disp    hp  drat    wt  qsec    vs  gear  carb\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1  NA    21       6  160    110  3.9   2.62  16.5     0     4     4\n##  2  NA    21       6  160    110  3.9   2.88  17.0     0     4     4\n##  3  NA    22.8     4  108     93  3.85  2.32  18.6     1     4     1\n##  4  21.4  NA       6  258    110  3.08  3.22  19.4     1     3     1\n##  5  18.7  NA       8  360    175  3.15  3.44  17.0     0     3     2\n##  6  18.1  NA       6  225    105  2.76  3.46  20.2     1     3     1\n##  7  14.3  NA       8  360    245  3.21  3.57  15.8     0     3     4\n##  8  24.4  NA       4  147.    62  3.69  3.19  20       1     4     2\n##  9  22.8  NA       4  141.    95  3.92  3.15  22.9     1     4     2\n## 10  19.2  NA       6  168.   123  3.92  3.44  18.3     1     4     4\n## # … with 22 more rows\n\nNow suppose that we want to spread the values of “am” times “cyl”, and filling the data with the values of “mpg”:\n\nmtcars_wide2 &lt;- mtcars %&gt;% \n    pivot_wide(names_from = c(\"am\", \"cyl\"), values_from = \"mpg\") \n\nmtcars_wide2 %&gt;% \n    select(matches(\"^0|1\"), everything())\n## # A tibble: 32 x 14\n##    `1_6` `1_4` `0_6` `0_8` `0_4` `1_8`  disp    hp  drat    wt  qsec    vs\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1    21  NA    NA    NA    NA      NA  160    110  3.9   2.62  16.5     0\n##  2    21  NA    NA    NA    NA      NA  160    110  3.9   2.88  17.0     0\n##  3    NA  22.8  NA    NA    NA      NA  108     93  3.85  2.32  18.6     1\n##  4    NA  NA    21.4  NA    NA      NA  258    110  3.08  3.22  19.4     1\n##  5    NA  NA    NA    18.7  NA      NA  360    175  3.15  3.44  17.0     0\n##  6    NA  NA    18.1  NA    NA      NA  225    105  2.76  3.46  20.2     1\n##  7    NA  NA    NA    14.3  NA      NA  360    245  3.21  3.57  15.8     0\n##  8    NA  NA    NA    NA    24.4    NA  147.    62  3.69  3.19  20       1\n##  9    NA  NA    NA    NA    22.8    NA  141.    95  3.92  3.15  22.9     1\n## 10    NA  NA    19.2  NA    NA      NA  168.   123  3.92  3.44  18.3     1\n## # … with 22 more rows, and 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;\n\nAs you can see, this is easily achieved by simply providing more columns to names_from =.\n\n\nFinally, it is also possible to use an optional data set which contains the specifications of the new columns:\n\nmtcars_spec &lt;- mtcars %&gt;% \n    expand(am, cyl, .value = \"mpg\") %&gt;%\n    unite(\".name\", am, cyl, remove = FALSE)\n\nmtcars_spec\n## # A tibble: 6 x 4\n##   .name    am   cyl .value\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n## 1 0_4       0     4 mpg   \n## 2 0_6       0     6 mpg   \n## 3 0_8       0     8 mpg   \n## 4 1_4       1     4 mpg   \n## 5 1_6       1     6 mpg   \n## 6 1_8       1     8 mpg\n\nThis optional data set defines how the columns “0_4”, “0_6” etc are constructed, and also the value that shall be used to fill in the values. “am” and “cyl” will be used to create the “.name” and the “mpg” column will be used for the “.value”:\n\nmtcars %&gt;% \n    pivot_wide(spec = mtcars_spec) %&gt;% \n    select(matches(\"^0|1\"), everything())\n## # A tibble: 32 x 14\n##    `0_4` `0_6` `0_8` `1_4` `1_6` `1_8`  disp    hp  drat    wt  qsec    vs\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1  NA    NA    NA    NA      21    NA  160    110  3.9   2.62  16.5     0\n##  2  NA    NA    NA    NA      21    NA  160    110  3.9   2.88  17.0     0\n##  3  NA    NA    NA    22.8    NA    NA  108     93  3.85  2.32  18.6     1\n##  4  NA    21.4  NA    NA      NA    NA  258    110  3.08  3.22  19.4     1\n##  5  NA    NA    18.7  NA      NA    NA  360    175  3.15  3.44  17.0     0\n##  6  NA    18.1  NA    NA      NA    NA  225    105  2.76  3.46  20.2     1\n##  7  NA    NA    14.3  NA      NA    NA  360    245  3.21  3.57  15.8     0\n##  8  24.4  NA    NA    NA      NA    NA  147.    62  3.69  3.19  20       1\n##  9  22.8  NA    NA    NA      NA    NA  141.    95  3.92  3.15  22.9     1\n## 10  NA    19.2  NA    NA      NA    NA  168.   123  3.92  3.44  18.3     1\n## # … with 22 more rows, and 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;\n\nUsing a spec is especially useful if you need to make new levels that are not in the data. For instance, suppose that there are actually 10-cylinder cars too, but they do not appear in our sample. We would like to make the fact that they’re missing explicit:\n\nmtcars_spec2 &lt;- mtcars %&gt;% \n    expand(am, \"cyl\" = c(cyl, 10), .value = \"mpg\") %&gt;%\n    unite(\".name\", am, cyl, remove = FALSE)\n\nmtcars_spec2\n## # A tibble: 8 x 4\n##   .name    am   cyl .value\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n## 1 0_4       0     4 mpg   \n## 2 0_6       0     6 mpg   \n## 3 0_8       0     8 mpg   \n## 4 0_10      0    10 mpg   \n## 5 1_4       1     4 mpg   \n## 6 1_6       1     6 mpg   \n## 7 1_8       1     8 mpg   \n## 8 1_10      1    10 mpg\nmtcars %&gt;% \n    pivot_wide(spec = mtcars_spec2) %&gt;% \n    select(matches(\"^0|1\"), everything())\n## # A tibble: 32 x 16\n##    `0_4` `0_6` `0_8` `0_10` `1_4` `1_6` `1_8` `1_10`  disp    hp  drat\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1  NA    NA    NA       NA  NA      21    NA     NA  160    110  3.9 \n##  2  NA    NA    NA       NA  NA      21    NA     NA  160    110  3.9 \n##  3  NA    NA    NA       NA  22.8    NA    NA     NA  108     93  3.85\n##  4  NA    21.4  NA       NA  NA      NA    NA     NA  258    110  3.08\n##  5  NA    NA    18.7     NA  NA      NA    NA     NA  360    175  3.15\n##  6  NA    18.1  NA       NA  NA      NA    NA     NA  225    105  2.76\n##  7  NA    NA    14.3     NA  NA      NA    NA     NA  360    245  3.21\n##  8  24.4  NA    NA       NA  NA      NA    NA     NA  147.    62  3.69\n##  9  22.8  NA    NA       NA  NA      NA    NA     NA  141.    95  3.92\n## 10  NA    19.2  NA       NA  NA      NA    NA     NA  168.   123  3.92\n## # … with 22 more rows, and 5 more variables: wt &lt;dbl&gt;, qsec &lt;dbl&gt;,\n## #   vs &lt;dbl&gt;, gear &lt;dbl&gt;, carb &lt;dbl&gt;\n\nAs you can see, we now have two more columns have been added, and they are full of NA’s.\n\n\nNow, let’s try to go from wide to long data sets, using pivot_long():\n\nmtcars_wide1 %&gt;% \n  pivot_long(cols = c(`1`, `0`), names_to = \"am\", values_to = \"mpg\") %&gt;% \n  select(am, mpg, everything())\n## # A tibble: 64 x 11\n##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb\n##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 1      21       6   160   110  3.9   2.62  16.5     0     4     4\n##  2 0      NA       6   160   110  3.9   2.62  16.5     0     4     4\n##  3 1      21       6   160   110  3.9   2.88  17.0     0     4     4\n##  4 0      NA       6   160   110  3.9   2.88  17.0     0     4     4\n##  5 1      22.8     4   108    93  3.85  2.32  18.6     1     4     1\n##  6 0      NA       4   108    93  3.85  2.32  18.6     1     4     1\n##  7 1      NA       6   258   110  3.08  3.22  19.4     1     3     1\n##  8 0      21.4     6   258   110  3.08  3.22  19.4     1     3     1\n##  9 1      NA       8   360   175  3.15  3.44  17.0     0     3     2\n## 10 0      18.7     8   360   175  3.15  3.44  17.0     0     3     2\n## # … with 54 more rows\n\nThe arguments of pivot_long() are quite explicit too, and similar to the ones in pivot_wide(). cols = is where the user specifies the columns that need to be pivoted. names_to = is where the user can specify the name of the new columns, whose levels will be exactly the ones specified to cols =. values_to = is where the user specifies the column name of the new column that will contain the values.\n\n\nIt is also possible to specify the columns that should not be transformed, by using -:\n\nmtcars_wide1 %&gt;% \n  pivot_long(cols = -matches(\"^[[:alpha:]]\"), names_to = \"am\", values_to = \"mpg\") %&gt;% \n  select(am, mpg, everything())\n## # A tibble: 64 x 11\n##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb\n##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 1      21       6   160   110  3.9   2.62  16.5     0     4     4\n##  2 0      NA       6   160   110  3.9   2.62  16.5     0     4     4\n##  3 1      21       6   160   110  3.9   2.88  17.0     0     4     4\n##  4 0      NA       6   160   110  3.9   2.88  17.0     0     4     4\n##  5 1      22.8     4   108    93  3.85  2.32  18.6     1     4     1\n##  6 0      NA       4   108    93  3.85  2.32  18.6     1     4     1\n##  7 1      NA       6   258   110  3.08  3.22  19.4     1     3     1\n##  8 0      21.4     6   258   110  3.08  3.22  19.4     1     3     1\n##  9 1      NA       8   360   175  3.15  3.44  17.0     0     3     2\n## 10 0      18.7     8   360   175  3.15  3.44  17.0     0     3     2\n## # … with 54 more rows\n\nHere the columns that should not be modified are all those that start with a letter, hence the “1” regular expression. It is also possible to remove all the NA’s from the data frame, with na.rm =.\n\nmtcars_wide1 %&gt;% \n  pivot_long(cols = c(`1`, `0`), names_to = \"am\", values_to = \"mpg\", na.rm = TRUE) %&gt;% \n  select(am, mpg, everything())\n## # A tibble: 32 x 11\n##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb\n##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 1      21       6  160    110  3.9   2.62  16.5     0     4     4\n##  2 1      21       6  160    110  3.9   2.88  17.0     0     4     4\n##  3 1      22.8     4  108     93  3.85  2.32  18.6     1     4     1\n##  4 0      21.4     6  258    110  3.08  3.22  19.4     1     3     1\n##  5 0      18.7     8  360    175  3.15  3.44  17.0     0     3     2\n##  6 0      18.1     6  225    105  2.76  3.46  20.2     1     3     1\n##  7 0      14.3     8  360    245  3.21  3.57  15.8     0     3     4\n##  8 0      24.4     4  147.    62  3.69  3.19  20       1     4     2\n##  9 0      22.8     4  141.    95  3.92  3.15  22.9     1     4     2\n## 10 0      19.2     6  168.   123  3.92  3.44  18.3     1     4     4\n## # … with 22 more rows\n\nWe can also pivot data frames where the names of the columns are made of two or more variables, for example in our mtcars_wide2 data frame:\n\nmtcars_wide2 %&gt;% \n    select(matches(\"^0|1\"), everything())\n## # A tibble: 32 x 14\n##    `1_6` `1_4` `0_6` `0_8` `0_4` `1_8`  disp    hp  drat    wt  qsec    vs\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1    21  NA    NA    NA    NA      NA  160    110  3.9   2.62  16.5     0\n##  2    21  NA    NA    NA    NA      NA  160    110  3.9   2.88  17.0     0\n##  3    NA  22.8  NA    NA    NA      NA  108     93  3.85  2.32  18.6     1\n##  4    NA  NA    21.4  NA    NA      NA  258    110  3.08  3.22  19.4     1\n##  5    NA  NA    NA    18.7  NA      NA  360    175  3.15  3.44  17.0     0\n##  6    NA  NA    18.1  NA    NA      NA  225    105  2.76  3.46  20.2     1\n##  7    NA  NA    NA    14.3  NA      NA  360    245  3.21  3.57  15.8     0\n##  8    NA  NA    NA    NA    24.4    NA  147.    62  3.69  3.19  20       1\n##  9    NA  NA    NA    NA    22.8    NA  141.    95  3.92  3.15  22.9     1\n## 10    NA  NA    19.2  NA    NA      NA  168.   123  3.92  3.44  18.3     1\n## # … with 22 more rows, and 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;\n\nAll the columns that start with either “0” or “1” must be pivoted:\n\nmtcars_wide2 %&gt;% \n  pivot_long(cols = matches(\"0|1\"), names_to = \"am_cyl\", values_to = \"mpg\", na.rm = TRUE) %&gt;% \n  select(am_cyl, everything())\n## # A tibble: 32 x 10\n##    am_cyl  disp    hp  drat    wt  qsec    vs  gear  carb   mpg\n##    &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 1_6     160    110  3.9   2.62  16.5     0     4     4  21  \n##  2 1_6     160    110  3.9   2.88  17.0     0     4     4  21  \n##  3 1_4     108     93  3.85  2.32  18.6     1     4     1  22.8\n##  4 0_6     258    110  3.08  3.22  19.4     1     3     1  21.4\n##  5 0_8     360    175  3.15  3.44  17.0     0     3     2  18.7\n##  6 0_6     225    105  2.76  3.46  20.2     1     3     1  18.1\n##  7 0_8     360    245  3.21  3.57  15.8     0     3     4  14.3\n##  8 0_4     147.    62  3.69  3.19  20       1     4     2  24.4\n##  9 0_4     141.    95  3.92  3.15  22.9     1     4     2  22.8\n## 10 0_6     168.   123  3.92  3.44  18.3     1     4     4  19.2\n## # … with 22 more rows\n\nNow, there is one new column, “am_cyl” which must still be transformed by separating “am_cyl” into two new columns:\n\nmtcars_wide2 %&gt;% \n  pivot_long(cols = matches(\"0|1\"), names_to = \"am_cyl\", values_to = \"mpg\", na.rm = TRUE) %&gt;% \n  separate(am_cyl, into = c(\"am\", \"cyl\"), sep = \"_\") %&gt;% \n  select(am, cyl, everything())\n## # A tibble: 32 x 11\n##    am    cyl    disp    hp  drat    wt  qsec    vs  gear  carb   mpg\n##    &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 1     6      160    110  3.9   2.62  16.5     0     4     4  21  \n##  2 1     6      160    110  3.9   2.88  17.0     0     4     4  21  \n##  3 1     4      108     93  3.85  2.32  18.6     1     4     1  22.8\n##  4 0     6      258    110  3.08  3.22  19.4     1     3     1  21.4\n##  5 0     8      360    175  3.15  3.44  17.0     0     3     2  18.7\n##  6 0     6      225    105  2.76  3.46  20.2     1     3     1  18.1\n##  7 0     8      360    245  3.21  3.57  15.8     0     3     4  14.3\n##  8 0     4      147.    62  3.69  3.19  20       1     4     2  24.4\n##  9 0     4      141.    95  3.92  3.15  22.9     1     4     2  22.8\n## 10 0     6      168.   123  3.92  3.44  18.3     1     4     4  19.2\n## # … with 22 more rows\n\nIt is also possible to achieve this using a data frame with the specification of what you need:\n\nmtcars_spec_long &lt;- mtcars_wide2 %&gt;% \n  pivot_long_spec(matches(\"0|1\"), values_to = \"mpg\") %&gt;% \n  separate(name, c(\"am\", \"cyl\"), sep = \"_\")\n\nmtcars_spec_long\n## # A tibble: 6 x 4\n##   .name .value am    cyl  \n##   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;\n## 1 1_6   mpg    1     6    \n## 2 1_4   mpg    1     4    \n## 3 0_6   mpg    0     6    \n## 4 0_8   mpg    0     8    \n## 5 0_4   mpg    0     4    \n## 6 1_8   mpg    1     8\n\nProviding this spec to pivot_long() solves the issue:\n\nmtcars_wide2 %&gt;% \n  pivot_long(spec = mtcars_spec_long, na.rm = TRUE) %&gt;% \n  select(am, cyl, everything())\n## # A tibble: 32 x 11\n##    am    cyl    disp    hp  drat    wt  qsec    vs  gear  carb   mpg\n##    &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 1     6      160    110  3.9   2.62  16.5     0     4     4  21  \n##  2 1     6      160    110  3.9   2.88  17.0     0     4     4  21  \n##  3 1     4      108     93  3.85  2.32  18.6     1     4     1  22.8\n##  4 0     6      258    110  3.08  3.22  19.4     1     3     1  21.4\n##  5 0     8      360    175  3.15  3.44  17.0     0     3     2  18.7\n##  6 0     6      225    105  2.76  3.46  20.2     1     3     1  18.1\n##  7 0     8      360    245  3.21  3.57  15.8     0     3     4  14.3\n##  8 0     4      147.    62  3.69  3.19  20       1     4     2  24.4\n##  9 0     4      141.    95  3.92  3.15  22.9     1     4     2  22.8\n## 10 0     6      168.   123  3.92  3.44  18.3     1     4     4  19.2\n## # … with 22 more rows\n\nStay tuned to Hadley Wickham’s twitter as there will definitely be announcements soon!"
  },
  {
    "objectID": "posts/2018-04-10-brotools_describe.html",
    "href": "posts/2018-04-10-brotools_describe.html",
    "title": "Get basic summary statistics for all the variables in a data frame",
    "section": "",
    "text": "I have added a new function to my {brotools} package, called describe(), which takes a data frame as an argument, and returns another data frame with descriptive statistics. It is very much inspired by the {skmir} package but also by assist::describe() (click on the packages to be redirected to the respective Github repos) but I wanted to write my own for two reasons: first, as an exercice, and second I really only needed the function skim_to_wide() from {skimr}. So instead of installing a whole package for a single function, I decided to write my own (since I use {brotools} daily).\n\n\nBelow you can see it in action:\n\nlibrary(dplyr)\ndata(starwars)\nbrotools::describe(starwars)\n## # A tibble: 10 x 13\n##    variable  type   nobs  mean    sd mode     min   max   q25 median   q75\n##    &lt;chr&gt;     &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n##  1 birth_ye… Nume…    87  87.6 155.  19         8   896  35       52  72  \n##  2 height    Nume…    87 174.   34.8 172       66   264 167      180 191  \n##  3 mass      Nume…    87  97.3 169.  77        15  1358  55.6     79  84.5\n##  4 eye_color Char…    87  NA    NA   blue      NA    NA  NA       NA  NA  \n##  5 gender    Char…    87  NA    NA   male      NA    NA  NA       NA  NA  \n##  6 hair_col… Char…    87  NA    NA   blond     NA    NA  NA       NA  NA  \n##  7 homeworld Char…    87  NA    NA   Tatoo…    NA    NA  NA       NA  NA  \n##  8 name      Char…    87  NA    NA   Luke …    NA    NA  NA       NA  NA  \n##  9 skin_col… Char…    87  NA    NA   fair      NA    NA  NA       NA  NA  \n## 10 species   Char…    87  NA    NA   Human     NA    NA  NA       NA  NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n\nAs you can see, the object that is returned by describe() is a tibble.\n\n\nFor now, this function does not handle dates, but it’s in the pipeline.\n\n\nYou can also only describe certain columns:\n\nbrotools::describe(starwars, height, mass, name)\n## # A tibble: 3 x 13\n##   variable type    nobs  mean    sd mode      min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 height   Numer…    87 174.   34.8 172        66   264 167      180 191  \n## 2 mass     Numer…    87  97.3 169.  77         15  1358  55.6     79  84.5\n## 3 name     Chara…    87  NA    NA   Luke S…    NA    NA  NA       NA  NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n\nIf you want to try it out, you can install {brotools} from Github:\n\ndevtools::install_github(\"b-rodrigues/brotools\")\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2018-12-24-modern_objects.html",
    "href": "posts/2018-12-24-modern_objects.html",
    "title": "Objects types and some useful R functions for beginners",
    "section": "",
    "text": "This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free here. This is taken from Chapter 2, which explains the different R objects you can manipulate as well as some functions to get you started."
  },
  {
    "objectID": "posts/2018-12-24-modern_objects.html#objects-types-and-useful-r-functions-to-get-started",
    "href": "posts/2018-12-24-modern_objects.html#objects-types-and-useful-r-functions-to-get-started",
    "title": "Objects types and some useful R functions for beginners",
    "section": "\nObjects, types and useful R functions to get started\n",
    "text": "Objects, types and useful R functions to get started\n\n\nAll objects in R have a given type. You already know most of them, as these types are also used in mathematics. Integers, floating point numbers, or floats, matrices, etc, are all objects you are already familiar with. But R has other, maybe lesser known data types (that you can find in a lot of other programming languages) that you need to become familiar with. But first, we need to learn how to assign a value to a variable. This can be done in two ways:\n\na &lt;- 3\n\nor\n\na = 3\n\nin very practical terms, there is no difference between the two. I prefer using &lt;- for assigning values to variables and reserve = for passing arguments to functions, for example:\n\nspam &lt;- mean(x = c(1,2,3))\n\nI think this is less confusing than:\n\nspam = mean(x = c(1,2,3))\n\nbut as I explained above you can use whatever you feel most comfortable with.\n\n\n\nThe numeric class\n\n\nTo define single numbers, you can do the following:\n\na &lt;- 3\n\nThe class() function allows you to check the class of an object:\n\nclass(a)\n## [1] \"numeric\"\n\nDecimals are defined with the character .:\n\na &lt;- 3.14\n\nR also supports integers. If you find yourself in a situation where you explicitly need an integer and not a floating point number, you can use the following:\n\na  &lt;- as.integer(3)\nclass(a)\n## [1] \"integer\"\n\nThe as.integer() function is very useful, because it converts its argument into an integer. There is a whole family of as.*() functions. To convert a into a floating point number again:\n\nclass(as.numeric(a))\n## [1] \"numeric\"\n\nThere is also is.numeric() which tests whether a number is of the numeric class:\n\nis.numeric(a)\n## [1] TRUE\n\nThese functions are very useful, there is one for any of the supported types in R. Later, we are going to learn about the {purrr} package, which is a very powerful package for functional programming. This package includes further such functions.\n\n\n\n\nThe character class\n\n\nUse ” “ to define characters (called strings in other programming languages):\n\na &lt;- \"this is a string\"\nclass(a)\n## [1] \"character\"\n\nTo convert something to a character you can use the as.character() function:\n\na &lt;- 4.392\n\nclass(a)\n## [1] \"numeric\"\nclass(as.character(a))\n## [1] \"character\"\n\nIt is also possible to convert a character to a numeric:\n\na &lt;- \"4.392\"\n\nclass(a)\n## [1] \"character\"\nclass(as.numeric(a))\n## [1] \"numeric\"\n\nBut this only works if it makes sense:\n\na &lt;- \"this won't work, chief\"\n\nclass(a)\n## [1] \"character\"\nas.numeric(a)\n## Warning: NAs introduced by coercion\n## [1] NA\n\nA very nice package to work with characters is {stringr}, which is also part of the {tidyverse}.\n\n\n\n\nThe factor class\n\n\nFactors look like characters, but are very different. They are the representation of categorical variables. A {tidyverse} package to work with factors is {forcats}. You would rarely use factor variables outside of datasets, so for now, it is enough to know that this class exists. We are going to learn more about factor variables in Chapter 4, by using the {forcats} package.\n\n\n\n\nThe Date class\n\n\nDates also look like characters, but are very different too:\n\nas.Date(\"2019/03/19\")\n## [1] \"2019-03-19\"\nclass(as.Date(\"2019/03/19\"))\n## [1] \"Date\"\n\nManipulating dates and time can be tricky, but thankfully there’s a {tidyverse} package for that, called {lubridate}. We are going to go over this package in Chapter 4.\n\n\n\n\nThe logical class\n\n\nThis class is the result of logical comparisons, for example, if you type:\n\n4 &gt; 3\n## [1] TRUE\n\nR returns TRUE, which is an object of class logical:\n\nk &lt;- 4 &gt; 3\nclass(k)\n## [1] \"logical\"\n\nIn other programming languages, logicals are often called bools. A logical variable can only have two values, either TRUE or FALSE. You can test the truthiness of a variable with isTRUE():\n\nk &lt;- 4 &gt; 3\nisTRUE(k)\n## [1] TRUE\n\nHow can you test if a variable is false? There is not a isFALSE() function (at least not without having to load a package containing this function), but there is way to do it:\n\nk &lt;- 4 &gt; 3\n!isTRUE(k)\n## [1] FALSE\n\nThe ! operator indicates negation, so the above expression could be translated as is k not TRUE?. There are other such operators, namely &, &&, |, ||. & means and and | stands for or. You might be wondering what the difference between & and && is? Or between | and ||? & and | work on vectors, doing pairwise comparisons:\n\none &lt;- c(TRUE, FALSE, TRUE, FALSE)\ntwo &lt;- c(FALSE, TRUE, TRUE, TRUE)\none & two\n## [1] FALSE FALSE  TRUE FALSE\n\nCompare this to the && operator:\n\none &lt;- c(TRUE, FALSE, TRUE, FALSE)\ntwo &lt;- c(FALSE, TRUE, TRUE, TRUE)\none && two\n## [1] FALSE\n\nThe && and || operators only compare the first element of the vectors and stop as soon as a the return value can be safely determined. This is called short-circuiting. Consider the following:\n\none &lt;- c(TRUE, FALSE, TRUE, FALSE)\ntwo &lt;- c(FALSE, TRUE, TRUE, TRUE)\nthree &lt;- c(TRUE, TRUE, FALSE, FALSE)\none && two && three\n## [1] FALSE\none || two || three\n## [1] TRUE\n\nThe || operator stops as soon it evaluates to TRUE whereas the && stops as soon as it evaluates to FALSE. Personally, I rarely use || or && because I get confused. I find using | or & in combination with the all() or any() functions much more useful:\n\none &lt;- c(TRUE, FALSE, TRUE, FALSE)\ntwo &lt;- c(FALSE, TRUE, TRUE, TRUE)\nany(one & two)\n## [1] TRUE\nall(one & two)\n## [1] FALSE\n\nany() checks whether any of the vector’s elements are TRUE and all() checks if all elements of the vector are TRUE.\n\n\nAs a final note, you should know that is possible to use T for TRUE and F for FALSE but I would advise against doing this, because it is not very explicit.\n\n\n\n\nVectors and matrices\n\n\nYou can create a vector in different ways. But first of all, it is important to understand that a vector in most programming languages is nothing more than a list of things. These things can be numbers (either integers or floats), strings, or even other vectors. A vector in R can only contain elements of one single type. This is not the case for a list, which is much more flexible. We will talk about lists shortly, but let’s first focus on vectors and matrices.\n\n\n\nThe c() function\n\n\nA very important function that allows you to build a vector is c():\n\na &lt;- c(1,2,3,4,5)\n\nThis creates a vector with elements 1, 2, 3, 4, 5. If you check its class:\n\nclass(a)\n## [1] \"numeric\"\n\nThis can be confusing: you where probably expecting a to be of class vector or something similar. This is not the case if you use c() to create the vector, because c() doesn’t build a vector in the mathematical sense, but a so-called atomic vector. Checking its dimension:\n\ndim(a)\n## NULL\n\nreturns NULL because an atomic vector doesn’t have a dimension. If you want to create a true vector, you need to use cbind() or rbind().\n\n\nBut before continuing, be aware that atomic vectors can only contain elements of the same type:\n\nc(1, 2, \"3\")\n## [1] \"1\" \"2\" \"3\"\n\nbecause “3” is a character, all the other values get implicitly converted to characters. You have to be very careful about this, and if you use atomic vectors in your programming, you have to make absolutely sure that no characters or logicals or whatever else are going to convert your atomic vector to something you were not expecting.\n\n\n\n\ncbind() and rbind()\n\n\nYou can create a true vector with cbind():\n\na &lt;- cbind(1, 2, 3, 4, 5)\n\nCheck its class now:\n\nclass(a)\n## [1] \"matrix\"\n\nThis is exactly what we expected. Let’s check its dimension:\n\ndim(a)\n## [1] 1 5\n\nThis returns the dimension of a using the LICO notation (number of LInes first, the number of COlumns).\n\n\nIt is also possible to bind vectors together to create a matrix.\n\nb &lt;- cbind(6,7,8,9,10)\n\nNow let’s put vector a and b into a matrix called matrix_c using rbind(). rbind() functions the same way as cbind() but glues the vectors together by rows and not by columns.\n\nmatrix_c &lt;- rbind(a,b)\nprint(matrix_c)\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    2    3    4    5\n## [2,]    6    7    8    9   10\n\n\n\nThe matrix class\n\n\nR also has support for matrices. For example, you can create a matrix of dimension (5,5) filled with 0’s with the matrix() function:\n\nmatrix_a &lt;- matrix(0, nrow = 5, ncol = 5)\n\nIf you want to create the following matrix:\n\n\n[ B = (\n\\[\\begin{array}{ccc}\n2 &amp; 4 &amp; 3 \\\\\n1 &amp; 5 &amp; 7\n\\end{array}\\]\n) ]\n\n\nyou would do it like this:\n\nB &lt;- matrix(c(2, 4, 3, 1, 5, 7), nrow = 2, byrow = TRUE)\n\nThe option byrow = TRUE means that the rows of the matrix will be filled first.\n\n\nYou can access individual elements of matrix_a like so:\n\nmatrix_a[2, 3]\n## [1] 0\n\nand R returns its value, 0. We can assign a new value to this element if we want. Try:\n\nmatrix_a[2, 3] &lt;- 7\n\nand now take a look at matrix_a again.\n\nprint(matrix_a)\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    0    0    0    0    0\n## [2,]    0    0    7    0    0\n## [3,]    0    0    0    0    0\n## [4,]    0    0    0    0    0\n## [5,]    0    0    0    0    0\n\nRecall our vector b:\n\nb &lt;- cbind(6,7,8,9,10)\n\nTo access its third element, you can simply write:\n\nb[3]\n## [1] 8\n\nI have heard many people praising R for being a matrix based language. Matrices are indeed useful, and statisticians are used to working with them. However, I very rarely use matrices in my day to day work, and prefer an approach based on data frames (which will be discussed below). This is because working with data frames makes it easier to use R’s advanced functional programming language capabilities, and this is where R really shines in my opinion. Working with matrices almost automatically implies using loops and all the iterative programming techniques, à la Fortran, which I personally believe are ill-suited for interactive statistical programming (as discussed in the introduction).\n\n\n\n\n\nThe list class\n\n\nThe list class is a very flexible class, and thus, very useful. You can put anything inside a list, such as numbers:\n\nlist1 &lt;- list(3, 2)\n\nor other lists constructed with c():\n\nlist2 &lt;- list(c(1, 2), c(3, 4))\n\nyou can also put objects of different classes in the same list:\n\nlist3 &lt;- list(3, c(1, 2), \"lists are amazing!\")\n\nand of course create list of lists:\n\nmy_lists &lt;- list(list1, list2, list3)\n\nTo check the contents of a list, you can use the structure function str():\n\nstr(my_lists)\n## List of 3\n##  $ :List of 2\n##   ..$ : num 3\n##   ..$ : num 2\n##  $ :List of 2\n##   ..$ : num [1:2] 1 2\n##   ..$ : num [1:2] 3 4\n##  $ :List of 3\n##   ..$ : num 3\n##   ..$ : num [1:2] 1 2\n##   ..$ : chr \"lists are amazing!\"\n\nor you can use RStudio’s Environment pane:\n\n\n\n\n\nYou can also create named lists:\n\nlist4 &lt;- list(\"a\" = 2, \"b\" = 8, \"c\" = \"this is a named list\")\n\nand you can access the elements in two ways:\n\nlist4[[1]]\n## [1] 2\n\nor, for named lists:\n\nlist4$c\n## [1] \"this is a named list\"\n\nLists are used extensively because they are so flexible. You can build lists of datasets and apply functions to all the datasets at once, build lists of models, lists of plots, etc… In the later chapters we are going to learn all about them. Lists are central objects in a functional programming workflow for interactive statistical analysis.\n\n\n\n\nThe data.frame and tibble classes\n\n\nIn the next chapter we are going to learn how to import datasets into R. Once you import data, the resulting object is either a data.frame or a tibble depending on which package you used to import the data. tibbles extend data.frames so if you know about data.frame objects already, working with tibbles will be very easy. tibbles have a better print() method, and some other niceties.\n\n\nHowever, I want to stress that these objects are central to R and are thus very important; they are actually special cases of lists, discussed above. There are different ways to print a data.frame or a tibble if you wish to inspect it. You can use View(my_data) to show the my_data data.frame in the View pane of RStudio:\n\n\n\n\n\nYou can also use the str() function:\n\nstr(my_data)\n\nAnd if you need to access an individual column, you can use the $ sign, same as for a list:\n\nmy_data$col1\n\n\n\nFormulas\n\n\nWe will learn more about formulas later, but because it is an important object, it is useful if you already know about them early on. A formula is defined in the following way:\n\nmy_formula &lt;- ~x\n\nclass(my_formula)\n## [1] \"formula\"\n\nFormula objects are defined using the ~ symbol. Formulas are useful to define statistical models, for example for a linear regression:\n\nlm(y ~ x)\n\nor also to define anonymous functions, but more on this later."
  },
  {
    "objectID": "posts/2018-12-24-modern_objects.html#models",
    "href": "posts/2018-12-24-modern_objects.html#models",
    "title": "Objects types and some useful R functions for beginners",
    "section": "\nModels\n",
    "text": "Models\n\n\nA statistical model is an object like any other in R:\n\ndata(mtcars)\n\nmy_model &lt;- lm(mpg ~ hp, mtcars)\n\nclass(my_model)\n## [1] \"lm\"\n\nmy_model is an object of class lm. You can apply different functions to a model object:\n\nsummary(my_model)\n## \n## Call:\n## lm(formula = mpg ~ hp, data = mtcars)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.7121 -2.1122 -0.8854  1.5819  8.2360 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***\n## hp          -0.06823    0.01012  -6.742 1.79e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.863 on 30 degrees of freedom\n## Multiple R-squared:  0.6024, Adjusted R-squared:  0.5892 \n## F-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\nThis class will be explored in later chapters.\n\n\n\nNULL, NA and NaN\n\n\nThe NULL, NA and NaN classes are pretty special. NULL is returned when the result of function is undetermined. For example, consider list4:\n\nlist4\n## $a\n## [1] 2\n## \n## $b\n## [1] 8\n## \n## $c\n## [1] \"this is a named list\"\n\nif you try to access an element that does not exist, such as d, you will get NULL back:\n\nlist4$d\n## NULL\n\nNaN means “Not a Number” and is returned when a function return something that is not a number:\n\nsqrt(-1)\n## Warning in sqrt(-1): NaNs produced\n## [1] NaN\n\nor:\n\n0/0\n## [1] NaN\n\nBasically, numbers that cannot be represented as floating point numbers are NaN.\n\n\nFinally, there’s NA which is closely related to NaN but is used for missing values. NA stands for Not Available. There are several types of NAs:\n\n\n\nNA_integer_\n\n\nNA_real_\n\n\nNA_complex_\n\n\nNA_character_\n\n\n\nbut these are in principle only used when you need to program your own functions and need to explicitly test for the missingness of, say, a character value.\n\n\nTo test whether a value is NA, use the is.na() function.\n\n\n\n\nUseful functions to get you started\n\n\nThis section will list several basic R functions that are very useful and should be part of your toolbox.\n\n\n\nSequences\n\n\nThere are several functions that create sequences, seq(), seq_along() and rep(). rep() is easy enough:\n\nrep(1, 10)\n##  [1] 1 1 1 1 1 1 1 1 1 1\n\nThis simply repeats 1 10 times. You can repeat other objects too:\n\nrep(\"HAHA\", 10)\n##  [1] \"HAHA\" \"HAHA\" \"HAHA\" \"HAHA\" \"HAHA\" \"HAHA\" \"HAHA\" \"HAHA\" \"HAHA\" \"HAHA\"\n\nTo create a sequence, things are not as straightforward. There is seq():\n\nseq(1, 10)\n##  [1]  1  2  3  4  5  6  7  8  9 10\nseq(70, 80)\n##  [1] 70 71 72 73 74 75 76 77 78 79 80\n\nIt is also possible to provide a by argument:\n\nseq(1, 10, by = 2)\n## [1] 1 3 5 7 9\n\nseq_along() behaves similarly, but returns the length of the object passed to it. So if you pass list4 to seq_along(), it will return a sequence from 1 to 3:\n\nseq_along(list4)\n## [1] 1 2 3\n\nwhich is also true for seq() actually:\n\nseq(list4)\n## [1] 1 2 3\n\nbut these two functions behave differently for arguments of length equal to 1:\n\nseq(10)\n##  [1]  1  2  3  4  5  6  7  8  9 10\nseq_along(10)\n## [1] 1\n\nSo be quite careful about that. I would advise you do not use seq(), but only seq_along() and seq_len(). seq_len() only takes arguments of length 1:\n\nseq_len(10)\n##  [1]  1  2  3  4  5  6  7  8  9 10\nseq_along(10)\n## [1] 1\n\nThe problem with seq() is that it is unpredictable; depending on its input, the output will either be an integer or a sequence. When programming, it is better to have function that are stricter and fail when confronted to special cases, instead of returning some result. This is a bit of a recurrent issue with R, and the functions from the {tidyverse} mitigate this issue by being stricter than their base R counterparts. For example, consider the ifelse() function from base R:\n\nifelse(3 &gt; 5, 1, \"this is false\")\n## [1] \"this is false\"\n\nand compare it to {dplyr}’s implementation, if_else():\n\nif_else(3 &gt; 5, 1, \"this is false\")\nError: `false` must be type double, not character\nCall `rlang::last_error()` to see a backtrace\n\nif_else() fails because the return value when FALSE is not a double (a real number) but a character. This might seem unnecessarily strict, but at least it is predictable. This makes debugging easier when used inside functions. In Chapter 8 we are going to learn how to write our own functions, and being strict makes programming easier.\n\n\n\n\nBasic string manipulation\n\n\nFor now, we have not closely studied character objects, we only learned how to define them. Later, in Chapter 5 we will learn about the {stringr} package which provides useful function to work with strings. However, there are several base R functions that are very useful that you might want to know nonetheless, such as paste() and paste0():\n\npaste(\"Hello\", \"amigo\")\n## [1] \"Hello amigo\"\n\nbut you can also change the separator if needed:\n\npaste(\"Hello\", \"amigo\", sep = \"--\")\n## [1] \"Hello--amigo\"\n\npaste0() is the same as paste() but does not have any sep argument:\n\npaste0(\"Hello\", \"amigo\")\n## [1] \"Helloamigo\"\n\nIf you provide a vector of characters, you can also use the collapse argument, which places whatever you provide for collapse between the characters of the vector:\n\npaste0(c(\"Joseph\", \"Mary\", \"Jesus\"), collapse = \", and \")\n## [1] \"Joseph, and Mary, and Jesus\"\n\nTo change the case of characters, you can use toupper() and tolower():\n\ntolower(\"HAHAHAHAH\")\n## [1] \"hahahahah\"\ntoupper(\"hueuehuehuheuhe\")\n## [1] \"HUEUEHUEHUHEUHE\"\n\n\n\nMathematical functions\n\n\nFinally, there are the classical mathematical functions that you know and love:\n\n\n\nsqrt()\n\n\nexp()\n\n\nlog()\n\n\nabs()\n\n\nsin(), cos(), tan(), and others\n\n\nsum(), cumsum(), prod(), cumprod()\n\n\nmax(), min()\n\n\n\nand many others…"
  },
  {
    "objectID": "posts/2019-01-31-newspapers_shiny_app.html",
    "href": "posts/2019-01-31-newspapers_shiny_app.html",
    "title": "Using Data Science to read 10 years of Luxembourguish newspapers from the 19th century",
    "section": "",
    "text": "I have been playing around with historical newspaper data (see here and here). I have extracted the data from the largest archive available, as described in the previous blog post, and now created a shiny dashboard where it is possible to visualize the most common words per article, as well as read a summary of each article. The summary was made using a method called textrank, using the {textrank} package, which extracts relevant sentences using the Pagerank (developed by Google) algorithm. You can read the scientific paper here for more info.\n\n\nYou can play around with the app by clicking here. In the next blog post, I will explain how I created the app, step by step. It’s going to be a long blog post!\n\n\nUsing the app, I noticed that some war happened around November 1860 in China, which turned out to be the Second Opium War. The war actually ended in October 1860, but apparently the news took several months to travel to Europe.\n\n\nI also learned that already in the 1861, there was public transportation between some Luxembourguish villages, and French villages that were by the border (see the publication from the 17th of December 1861).\n\n\nLet me know if you find about historical events using my app!"
  },
  {
    "objectID": "posts/2019-05-04-diffindiff_part2.html",
    "href": "posts/2019-05-04-diffindiff_part2.html",
    "title": "Fast food, causality and R packages, part 2",
    "section": "",
    "text": "I am currently working on a package for the R programming language; its initial goal was to simply distribute the data used in the Card and Krueger 1994 paper that you can read here (PDF warning). However, I decided that I would add code to perform diff-in-diff.\n\n\nIn my previous blog post I showed how to set up the structure of your new package. In this blog post, I will only focus on getting Card and Krueger’s data and prepare it for distribution. The next blog posts will focus on writing a function to perform difference-in-differences.\n\n\nIf you want to distribute data through a package, you first need to use the usethis::use_data_raw() function (as shown in part 1).\n\n\nThis creates a data-raw folder, and inside you will find the DATASET.R script. You can edit this script to prepare the data.\n\n\nFirst, let’s download the data from Card’s website, unzip it and load the data into R. All these operations will be performed from R:\n\nlibrary(tidyverse)\n\ntempfile_path &lt;- tempfile()\n\ndownload.file(\"http://davidcard.berkeley.edu/data_sets/njmin.zip\", destfile = tempfile_path)\n\ntempdir_path &lt;- tempdir()\n\nunzip(tempfile_path, exdir = tempdir_path)\n\nTo download and unzip a file from R, first, you need to define where you want to save the file. Because I am not interested in keeping the downloaded file, I use the tempfile() function to get a temporary file in my /tmp/ folder (which is the folder that contains temporary files and folders in a GNU+Linux system). Then, using download.file() I download the file, and save it in my temporary file. I then create a temporary directory using tempdir() (the idea is the same as with tempfile()), and use this folder to save the files that I will unzip, using the unzip() function. This folder now contains several files:\n\ncheck.sas\ncodebook\npublic.csv\nread.me\nsurvey1.nj\nsurvey2.nj\n\ncheck.sas is the SAS script Card and Krueger used. It’s interesting, because it is quite simple, quite short (170 lines long) and yet the impact of Card and Krueger’s research was and has been very important for the field of econometrics. This script will help me define my own functions. codebook, you guessed it, contains the variables’ descriptions. I will use this to name the columns of the data and to write the dataset’s documentation.\n\n\npublic.csv is the data. It does not contain any column names:\n\n 46 1 0 0 0 0 0 1 0 0  0 30.00 15.00  3.00   .    19.0   .   1    .  2  6.50 16.50  1.03  1.03  0.52  3  3 1 1 111792  1  3.50 35.00  3.00  4.30  26.0  0.08 1 2  6.50 16.50  1.03   .    0.94  4  4    \n 49 2 0 0 0 0 0 1 0 0  0  6.50  6.50  4.00   .    26.0   .   0    .  2 10.00 13.00  1.01  0.90  2.35  4  3 1 1 111292  .  0.00 15.00  4.00  4.45  13.0  0.05 0 2 10.00 13.00  1.01  0.89  2.35  4  4    \n506 2 1 0 0 0 0 1 0 0  0  3.00  7.00  2.00   .    13.0  0.37 0  30.0 2 11.00 10.00  0.95  0.74  2.33  3  3 1 1 111292  .  3.00  7.00  4.00  5.00  19.0  0.25 . 1 11.00 11.00  0.95  0.74  2.33  4  3    \n 56 4 1 0 0 0 0 1 0 0  0 20.00 20.00  4.00  5.00  26.0  0.10 1   0.0 2 10.00 12.00  0.87  0.82  1.79  2  2 1 1 111492  .  0.00 36.00  2.00  5.25  26.0  0.15 0 2 10.00 12.00  0.92  0.79  0.87  2  2    \n 61 4 1 0 0 0 0 1 0 0  0  6.00 26.00  5.00  5.50  52.0  0.15 1   0.0 3 10.00 12.00  0.87  0.77  1.65  2  2 1 1 111492  . 28.00  3.00  6.00  4.75  13.0  0.15 0 2 10.00 12.00  1.01  0.84  0.95  2  2    \n 62 4 1 0 0 0 0 1 0 0  2  0.00 31.00  5.00  5.00  26.0  0.07 0  45.0 2 10.00 12.00  0.87  0.77  0.95  2  2 1 1 111492  .   .     .     .     .    26.0   .   0 2 10.00 12.00   .    0.84  1.79  3  3    \n\nMissing data is defined by . and the delimiter is the space character. read.me is a README file. Finally, survey1.nj and survey2.nj are the surveys that were administered to the fast food restaurants’ managers; one in February (before the raise) and the second one in November (after the minimum wage raise).\n\n\nThe next lines import the codebook:\n\ncodebook &lt;- read_lines(file = paste0(tempdir_path, \"/codebook\"))\n\nvariable_names &lt;- codebook %&gt;%\n    `[`(8:59) %&gt;%\n    `[`(-c(5, 6, 13, 14, 32, 33)) %&gt;%\n    str_sub(1, 13) %&gt;%\n    str_squish() %&gt;%\n    str_to_lower()\n\nOnce I import the codebook, I select lines 8 to 59 using the [() function. If you’re not familiar with this notation, try the following in a console:\n\nseq(1, 100)[1:10]\n##  [1]  1  2  3  4  5  6  7  8  9 10\n\nand compare:\n\nseq(1, 100) %&gt;% \n  `[`(., 1:10)\n##  [1]  1  2  3  4  5  6  7  8  9 10\n\nboth are equivalent, as you can see. You can also try the following:\n\n1 + 10\n## [1] 11\n1 %&gt;% \n  `+`(., 10)\n## [1] 11\n\nUsing the same trick, I remove lines that I do not need, and then using stringr::str_sub(1, 13) I only keep the first 13 characters (which are the variable names, plus some white space characters) and then, to remove all the unneeded white space characters I use stringr::squish(), and then change the column names to lowercase.\n\n\nI then load the data, and add the column names that I extracted before:\n\ndataset &lt;- read_table2(paste0(tempdir_path, \"/public.dat\"),\n                      col_names = FALSE)\n\ndataset &lt;- dataset %&gt;%\n    select(-X47) %&gt;%\n    `colnames&lt;-`(., variable_names) %&gt;%\n    mutate_all(as.numeric) %&gt;%\n    mutate(sheet = as.character(sheet))\n\nI use the same trick as before. I rename the 47th column, which is empty, I name the columns with colnames&lt;-().\n\n\nAfter this, I perform some data cleaning. It’s mostly renaming categories of categorical variables, and creating a “true” panel format. Several variables were measured at several points in time. Variables that were measured a second time have a “2” at the end of their name. I remove these variables, and add an observation data variable. So my data as twice as many rows as the original data, but that format makes it way easier to work with. Below you can read the full code:\n\n\n\n\nClick if you want to see the code\n\n\ndataset &lt;- dataset %&gt;%\n    mutate(chain = case_when(chain == 1 ~ \"bk\",\n                             chain == 2 ~ \"kfc\",\n                             chain == 3 ~ \"roys\",\n                             chain == 4 ~ \"wendys\")) %&gt;%\n    mutate(state = case_when(state == 1 ~ \"New Jersey\",\n                             state == 0 ~ \"Pennsylvania\")) %&gt;%\n    mutate(region = case_when(southj == 1 ~ \"southj\",\n              centralj == 1 ~ \"centralj\",\n              northj == 1 ~ \"northj\",\n              shore == 1 ~ \"shorej\",\n              pa1 == 1 ~ \"pa1\",\n              pa2 == 1 ~ \"pa2\")) %&gt;%\n    mutate(meals = case_when(meals == 0 ~ \"None\",\n                             meals == 1 ~ \"Free meals\",\n                             meals == 2 ~ \"Reduced price meals\",\n                             meals == 3 ~ \"Both free and reduced price meals\")) %&gt;%\n    mutate(meals2 = case_when(meals2 == 0 ~ \"None\",\n                             meals2 == 1 ~ \"Free meals\",\n                             meals2 == 2 ~ \"Reduced price meals\",\n                             meals2 == 3 ~ \"Both free and reduced price meals\")) %&gt;%\n    mutate(status2 = case_when(status2 == 0 ~ \"Refused 2nd interview\",\n                               status2 == 1 ~ \"Answered 2nd interview\",\n                               status2 == 2 ~ \"Closed for renovations\",\n                               status2 == 3 ~ \"Closed permanently\",\n                               status2 == 4 ~ \"Closed for highway construction\",\n                               status2 == 5 ~ \"Closed due to Mall fire\")) %&gt;%\n    mutate(co_owned = if_else(co_owned == 1, \"Yes\", \"No\")) %&gt;%\n    mutate(bonus = if_else(bonus == 1, \"Yes\", \"No\")) %&gt;%\n    mutate(special2 = if_else(special2 == 1, \"Yes\", \"No\")) %&gt;%\n    mutate(type2 = if_else(type2 == 1, \"Phone\", \"Personal\")) %&gt;%\n    select(sheet, chain, co_owned, state, region, everything()) %&gt;%\n    select(-southj, -centralj, -northj, -shore, -pa1, -pa2) %&gt;%\n    mutate(date2 = lubridate::mdy(date2)) %&gt;%\n    rename(open2 = open2r) %&gt;%\n    rename(firstinc2 = firstin2)\n\ndataset1 &lt;- dataset %&gt;%\n    select(-ends_with(\"2\"), -sheet, -chain, -co_owned, -state, -region, -bonus) %&gt;%\n    mutate(type = NA_character_,\n           status = NA_character_,\n           date = NA)\n\ndataset2 &lt;- dataset %&gt;%\n    select(ends_with(\"2\")) %&gt;%\n    #mutate(bonus = NA_character_) %&gt;%\n    rename_all(~str_remove(., \"2\"))\n\nother_cols &lt;- dataset %&gt;%\n    select(sheet, chain, co_owned, state, region, bonus)\n\nother_cols_1 &lt;- other_cols %&gt;%\n    mutate(observation = \"February 1992\")\n\nother_cols_2 &lt;- other_cols %&gt;%\n    mutate(observation = \"November 1992\")\n\ndataset1 &lt;- bind_cols(other_cols_1, dataset1)\ndataset2 &lt;- bind_cols(other_cols_2, dataset2)\n\nnjmin &lt;- bind_rows(dataset1, dataset2) %&gt;%\n    select(sheet, chain, state, region, observation, everything())\n\n\nThe line I would like to comment is the following:\n\ndataset %&gt;%\n    select(-ends_with(\"2\"), -sheet, -chain, -co_owned, -state, -region, -bonus)\n\nThis select removes every column that ends with the character “2” (among others). I split the data in two, to then bind the rows together and thus create my long dataset. I then save the data into the data/ folder:\n\nusethis::use_data(njmin, overwrite = TRUE)\n\nThis saves the data as an .rda file. To enable users to read the data by typing data(“njmin”), you need to create a data.R script in the R/ folder. You can read my data.R script below:\n\n\n\n\nClick if you want to see the code\n\n\n#' Data from the Card and Krueger 1994 paper *Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania*\n#'\n#' This dataset was downloaded and distributed with the permission of David Card. The original\n#' data contains 410 observations and 46 variables. The data distributed in this package is\n#' exactly the same, but was changed from a wide to a long dataset, which is better suited for\n#' manipulation with *tidyverse* functions.\n#'\n#' @format A data frame with 820 rows and 28 variables:\n#' \\describe{\n#'   \\item{\\code{sheet}}{Sheet number (unique store id).}\n#'   \\item{\\code{chain}}{The fastfood chain: bk is Burger King, kfc is Kentucky Fried Chicken, wendys is Wendy's, roys is Roy Rogers.}\n#'   \\item{\\code{state}}{State where the restaurant is located.}\n#'   \\item{\\code{region}}{pa1 is northeast suburbs of Phila, pa2 is Easton etc, centralj is central NJ, northj is northern NJ, southj is south NJ.}\n#'   \\item{\\code{observation}}{Date of first (February 1992) and second (November 1992) observation.}\n#'   \\item{\\code{co_owned}}{\"Yes\" if company owned.}\n#'   \\item{\\code{ncalls}}{Number of call-backs. Is 0 if contacted on first call.}\n#'   \\item{\\code{empft}}{Number full-time employees.}\n#'   \\item{\\code{emppt}}{Number part-time employees.}\n#'   \\item{\\code{nmgrs}}{Number of managers/assistant managers.}\n#'   \\item{\\code{wage_st}}{Starting wage ($/hr).}\n#'   \\item{\\code{inctime}}{Months to usual first raise.}\n#'   \\item{\\code{firstinc}}{Usual amount of first raise (\\$/hr).}\n#'   \\item{\\code{bonus}}{\"Yes\" if cash bounty for new workers.}\n#'   \\item{\\code{pctaff}}{\\% of employees affected by new minimum.}\n#'   \\item{\\code{meals}}{Free/reduced priced code.}\n#'   \\item{\\code{open}}{Hour of opening.}\n#'   \\item{\\code{hrsopen}}{Number of hours open per day.}\n#'   \\item{\\code{psode}}{Price of medium soda, including tax.}\n#'   \\item{\\code{pfry}}{Price of small fries, including tax.}\n#'   \\item{\\code{pentree}}{Price of entree, including tax.}\n#'   \\item{\\code{nregs}}{Number of cash registers in store.}\n#'   \\item{\\code{nregs11}}{Number of registers open at 11:00 pm.}\n#'   \\item{\\code{type}}{Type of 2nd interview.}\n#'   \\item{\\code{status}}{Status of 2nd interview.}\n#'   \\item{\\code{date}}{Date of 2nd interview.}\n#'   \\item{\\code{nregs11}}{\"Yes\" if special program for new workers.}\n#' }\n#' @source \\url{http://davidcard.berkeley.edu/data_sets.html}\n\"njmin\"\n\n\nI have documented the data, and using roxygen2::royxgenise() to create the dataset’s documentation.\n\n\nThe data can now be used to create some nifty plots:\n\nggplot(njmin, aes(wage_st)) + geom_density(aes(fill = state), alpha = 0.3) +\n    facet_wrap(vars(observation)) + theme_blog() +\n    theme(legend.title = element_blank(), plot.caption = element_text(colour = \"white\")) +\n    labs(title = \"Distribution of starting wage rates in fast food restaurants\",\n         caption = \"On April 1st, 1992, New Jersey's minimum wage rose from $4.25 to $5.05. Source: Card and Krueger (1994)\")\n## Warning: Removed 41 rows containing non-finite values (stat_density).\n\n\n\n\nIn the next blog post, I am going to write a first function to perform diff and diff, and we will learn how to make it available to users, document and test it!"
  },
  {
    "objectID": "posts/2017-08-27-why_tidyeval.html",
    "href": "posts/2017-08-27-why_tidyeval.html",
    "title": "Why I find tidyeval useful",
    "section": "",
    "text": "First thing’s first: maybe you shouldn’t care about tidyeval. Maybe you don’t need it. If you exclusively work interactively, I don’t think that learning about tidyeval is important. I can only speak for me, and explain to you why I personally find tidyeval useful.\n\n\nI wanted to write this blog post after reading this twitter thread and specifically this question.\n\n\nMara Averick then wrote this blogpost linking to 6 other blog posts that give some tidyeval examples. Reading them, plus the Programming with dplyr vignette should help you get started with tidyeval.\n\n\nBut maybe now you know how to use it, but not why and when you should use it… Basically, whenever you want to write a function that looks something like this:\n\nmy_function(my_data, one_column_inside_data)\n\nis when you want to use the power of tidyeval.\n\n\nI work at STATEC, Luxembourg’s national institute of statistics. I work on all kinds of different projects, and when data gets updated (for example because a new round of data collection for some survey finished), I run my own scripts on the fresh data to make the data nice and tidy for analysis. Because surveys get updated, sometimes column names change a little bit, and this can cause some issues.\n\n\nVery recently, a dataset I work with got updated. Data collection was finished, so I just loaded my hombrewed package written for this project, changed the path from last year’s script to this year’s fresh data path, ran the code, and watched as the folders got populated with new ggplot2 graphs and LaTeX tables with descriptive statistics and regression results. This is then used to generate this year’s report. However, by looking at the graphs, I noticed something weird; some graphs were showing some very strange patterns. It turns out that one column got its name changed, and also one of its values got changed too.\n\n\nLast year, this column, let’s call it spam, had values 1 for good and 0 for bad. This year the column is called Spam and the values are 1 and 2. When I found out that this was the source of the problem, I just had to change the arguments of my functions from\n\ngenerate_spam_plot(dataset = data2016, column = spam, value = 1)\ngenerate_spam_plot(dataset = data2016, column = spam, value = 0)\n\nto\n\ngenerate_spam_plot(dataset = data2017, column = Spam, value = 1)\ngenerate_spam_plot(dataset = data2017, column = Spam, value = 2)\n\nwithout needing to change anything else. This is why I use tidyeval; without it, writing a function such as genereta_spam_plot would not be easy. It would be possible, but not easy.\n\n\nIf you want to know more about tidyeval and working programmatically with R, I shamelessly invite you to read a book I’ve been working on: https://b-rodrigues.github.io/fput/ It’s still a WIP, but maybe you’ll find it useful. I plan on finishing it by the end of the year, but there’s already some content to keep you busy!"
  },
  {
    "objectID": "posts/2018-07-08-rob_stderr.html",
    "href": "posts/2018-07-08-rob_stderr.html",
    "title": "Dealing with heteroskedasticity; regression with robust standard errors using R",
    "section": "",
    "text": "First of all, is it heteroskedasticity or heteroscedasticity? According to McCulloch (1985), heteroskedasticity is the proper spelling, because when transliterating Greek words, scientists use the Latin letter k in place of the Greek letter κ (kappa). κ sometimes is transliterated as the Latin letter c, but only when these words entered the English language through French, such as scepter.\n\n\nNow that this is out of the way, we can get to the meat of this blogpost (foreshadowing pun). A random variable is said to be heteroskedastic, if its variance is not constant. For example, the variability of expenditures may increase with income. Richer families may spend a similar amount on groceries as poorer people, but some rich families will sometimes buy expensive items such as lobster. The variability of expenditures for rich families is thus quite large. However, the expenditures on food of poorer families, who cannot afford lobster, will not vary much. Heteroskedasticity can also appear when data is clustered; for example, variability of expenditures on food may vary from city to city, but is quite constant within a city.\n\n\nTo illustrate this, let’s first load all the packages needed for this blog post:\n\nlibrary(robustbase)\nlibrary(tidyverse)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(modelr)\nlibrary(broom)\n\nFirst, let’s load and prepare the data:\n\ndata(\"education\")\n\neducation &lt;- education %&gt;% \n    rename(residents = X1,\n           per_capita_income = X2,\n           young_residents = X3,\n           per_capita_exp = Y,\n           state = State) %&gt;% \n    mutate(region = case_when(\n        Region == 1 ~ \"northeast\",\n        Region == 2 ~ \"northcenter\",\n        Region == 3 ~ \"south\",\n        Region == 4 ~ \"west\"\n    )) %&gt;% \n    select(-Region)\n\nI will be using the education data set from the {robustbase} package. I renamed some columns and changed the values of the Region column. Now, let’s do a scatterplot of per capita expenditures on per capita income:\n\nggplot(education, aes(per_capita_income, per_capita_exp)) + \n    geom_point() +\n    theme_dark()\n\n\n\n\nIt would seem that, as income increases, variability of expenditures increases too. Let’s look at the same plot by region:\n\nggplot(education, aes(per_capita_income, per_capita_exp)) + \n    geom_point() + \n    facet_wrap(~region) + \n    theme_dark()\n\n\n\n\nI don’t think this shows much; it would seem that observations might be clustered, but there are not enough observations to draw any conclusion from this plot (in any case, drawing conclusions from only plots is dangerous).\n\n\nLet’s first run a good ol’ linear regression:\n\nlmfit &lt;- lm(per_capita_exp ~ region + residents + young_residents + per_capita_income, data = education)\n\nsummary(lmfit)\n## \n## Call:\n## lm(formula = per_capita_exp ~ region + residents + young_residents + \n##     per_capita_income, data = education)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -77.963 -25.499  -2.214  17.618  89.106 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)       -467.40283  142.57669  -3.278 0.002073 ** \n## regionnortheast     15.72741   18.16260   0.866 0.391338    \n## regionsouth          7.08742   17.29950   0.410 0.684068    \n## regionwest          34.32416   17.49460   1.962 0.056258 .  \n## residents           -0.03456    0.05319  -0.650 0.519325    \n## young_residents      1.30146    0.35717   3.644 0.000719 ***\n## per_capita_income    0.07204    0.01305   5.520 1.82e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 39.88 on 43 degrees of freedom\n## Multiple R-squared:  0.6292, Adjusted R-squared:  0.5774 \n## F-statistic: 12.16 on 6 and 43 DF,  p-value: 6.025e-08\n\nLet’s test for heteroskedasticity using the Breusch-Pagan test that you can find in the {lmtest} package:\n\nbptest(lmfit)\n## \n##  studentized Breusch-Pagan test\n## \n## data:  lmfit\n## BP = 17.921, df = 6, p-value = 0.006432\n\nThis test shows that we can reject the null that the variance of the residuals is constant, thus heteroskedacity is present. To get the correct standard errors, we can use the vcovHC() function from the {sandwich} package (hence the choice for the header picture of this post):\n\nlmfit %&gt;% \n    vcovHC() %&gt;% \n    diag() %&gt;% \n    sqrt()\n##       (Intercept)   regionnortheast       regionsouth        regionwest \n##      311.31088691       25.30778221       23.56106307       24.12258706 \n##         residents   young_residents per_capita_income \n##        0.09184368        0.68829667        0.02999882\n\nBy default vcovHC() estimates a heteroskedasticity consistent (HC) variance covariance matrix for the parameters. There are several ways to estimate such a HC matrix, and by default vcovHC() estimates the “HC3” one. You can refer to Zeileis (2004) for more details.\n\n\nWe see that the standard errors are much larger than before! The intercept and regionwest variables are not statistically significant anymore.\n\n\nYou can achieve the same in one single step:\n\ncoeftest(lmfit, vcov = vcovHC(lmfit))\n## \n## t test of coefficients:\n## \n##                      Estimate  Std. Error t value Pr(&gt;|t|)  \n## (Intercept)       -467.402827  311.310887 -1.5014  0.14056  \n## regionnortheast     15.727405   25.307782  0.6214  0.53759  \n## regionsouth          7.087424   23.561063  0.3008  0.76501  \n## regionwest          34.324157   24.122587  1.4229  0.16198  \n## residents           -0.034558    0.091844 -0.3763  0.70857  \n## young_residents      1.301458    0.688297  1.8908  0.06540 .\n## per_capita_income    0.072036    0.029999  2.4013  0.02073 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIt’s is also easy to change the estimation method for the variance-covariance matrix:\n\ncoeftest(lmfit, vcov = vcovHC(lmfit, type = \"HC0\"))\n## \n## t test of coefficients:\n## \n##                      Estimate  Std. Error t value  Pr(&gt;|t|)    \n## (Intercept)       -467.402827  172.577569 -2.7084  0.009666 ** \n## regionnortheast     15.727405   20.488148  0.7676  0.446899    \n## regionsouth          7.087424   17.755889  0.3992  0.691752    \n## regionwest          34.324157   19.308578  1.7777  0.082532 .  \n## residents           -0.034558    0.054145 -0.6382  0.526703    \n## young_residents      1.301458    0.387743  3.3565  0.001659 ** \n## per_capita_income    0.072036    0.016638  4.3296 8.773e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAs I wrote above, by default, the type argument is equal to “HC3”.\n\n\nAnother way of dealing with heteroskedasticity is to use the lmrob() function from the {robustbase} package. This package is quite interesting, and offers quite a lot of functions for robust linear, and nonlinear, regression models. Running a robust linear regression is just the same as with lm():\n\nlmrobfit &lt;- lmrob(per_capita_exp ~ region + residents + young_residents + per_capita_income, \n                  data = education)\n\nsummary(lmrobfit)\n## \n## Call:\n## lmrob(formula = per_capita_exp ~ region + residents + young_residents + per_capita_income, \n##     data = education)\n##  \\--&gt; method = \"MM\"\n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -57.074 -14.803  -0.853  24.154 174.279 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)       -156.37169  132.73828  -1.178  0.24526   \n## regionnortheast     20.64576   26.45378   0.780  0.43940   \n## regionsouth         10.79695   29.42746   0.367  0.71549   \n## regionwest          45.22589   33.07950   1.367  0.17867   \n## residents            0.03406    0.04412   0.772  0.44435   \n## young_residents      0.57896    0.25512   2.269  0.02832 * \n## per_capita_income    0.04328    0.01442   3.000  0.00447 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Robust residual standard error: 26.4 \n## Multiple R-squared:  0.6235, Adjusted R-squared:  0.571 \n## Convergence in 24 IRWLS iterations\n## \n## Robustness weights: \n##  observation 50 is an outlier with |weight| = 0 ( &lt; 0.002); \n##  7 weights are ~= 1. The remaining 42 ones are summarized as\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n## 0.05827 0.85200 0.93870 0.85250 0.98700 0.99790 \n## Algorithmic parameters: \n##        tuning.chi                bb        tuning.psi        refine.tol \n##         1.548e+00         5.000e-01         4.685e+00         1.000e-07 \n##           rel.tol         scale.tol         solve.tol       eps.outlier \n##         1.000e-07         1.000e-10         1.000e-07         2.000e-03 \n##             eps.x warn.limit.reject warn.limit.meanrw \n##         1.071e-08         5.000e-01         5.000e-01 \n##      nResample         max.it       best.r.s       k.fast.s          k.max \n##            500             50              2              1            200 \n##    maxit.scale      trace.lev            mts     compute.rd fast.s.large.n \n##            200              0           1000              0           2000 \n##                   psi           subsampling                   cov \n##            \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \n## compute.outlier.stats \n##                  \"SM\" \n## seed : int(0)\n\nThis however, gives you different estimates than when fitting a linear regression model. The estimates should be the same, only the standard errors should be different. This is because the estimation method is different, and is also robust to outliers (at least that’s my understanding, I haven’t read the theoretical papers behind the package yet).\n\n\nFinally, it is also possible to bootstrap the standard errors. For this I will use the bootstrap() function from the {modelr} package:\n\nresamples &lt;- 100\n\nboot_education &lt;- education %&gt;% \n modelr::bootstrap(resamples)\n\nLet’s take a look at the boot_education object:\n\nboot_education\n## # A tibble: 100 x 2\n##    strap      .id  \n##    &lt;list&gt;     &lt;chr&gt;\n##  1 &lt;resample&gt; 001  \n##  2 &lt;resample&gt; 002  \n##  3 &lt;resample&gt; 003  \n##  4 &lt;resample&gt; 004  \n##  5 &lt;resample&gt; 005  \n##  6 &lt;resample&gt; 006  \n##  7 &lt;resample&gt; 007  \n##  8 &lt;resample&gt; 008  \n##  9 &lt;resample&gt; 009  \n## 10 &lt;resample&gt; 010  \n## # … with 90 more rows\n\nThe column strap contains resamples of the original data. I will run my linear regression from before on each of the resamples:\n\n(\n    boot_lin_reg &lt;- boot_education %&gt;% \n        mutate(regressions = \n                   map(strap, \n                       ~lm(per_capita_exp ~ region + residents + \n                               young_residents + per_capita_income, \n                           data = .))) \n)\n## # A tibble: 100 x 3\n##    strap      .id   regressions\n##    &lt;list&gt;     &lt;chr&gt; &lt;list&gt;     \n##  1 &lt;resample&gt; 001   &lt;lm&gt;       \n##  2 &lt;resample&gt; 002   &lt;lm&gt;       \n##  3 &lt;resample&gt; 003   &lt;lm&gt;       \n##  4 &lt;resample&gt; 004   &lt;lm&gt;       \n##  5 &lt;resample&gt; 005   &lt;lm&gt;       \n##  6 &lt;resample&gt; 006   &lt;lm&gt;       \n##  7 &lt;resample&gt; 007   &lt;lm&gt;       \n##  8 &lt;resample&gt; 008   &lt;lm&gt;       \n##  9 &lt;resample&gt; 009   &lt;lm&gt;       \n## 10 &lt;resample&gt; 010   &lt;lm&gt;       \n## # … with 90 more rows\n\nI have added a new column called regressions which contains the linear regressions on each bootstrapped sample. Now, I will create a list of tidied regression results:\n\n(\n    tidied &lt;- boot_lin_reg %&gt;% \n        mutate(tidy_lm = \n                   map(regressions, broom::tidy))\n)\n## # A tibble: 100 x 4\n##    strap      .id   regressions tidy_lm         \n##    &lt;list&gt;     &lt;chr&gt; &lt;list&gt;      &lt;list&gt;          \n##  1 &lt;resample&gt; 001   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  2 &lt;resample&gt; 002   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  3 &lt;resample&gt; 003   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  4 &lt;resample&gt; 004   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  5 &lt;resample&gt; 005   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  6 &lt;resample&gt; 006   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  7 &lt;resample&gt; 007   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  8 &lt;resample&gt; 008   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  9 &lt;resample&gt; 009   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n## 10 &lt;resample&gt; 010   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n## # … with 90 more rows\n\nbroom::tidy() creates a data frame of the regression results. Let’s look at one of these:\n\ntidied$tidy_lm[[1]]\n## # A tibble: 7 x 5\n##   term              estimate std.error statistic  p.value\n##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)       -571.     109.        -5.22  4.92e- 6\n## 2 regionnortheast    -48.0     17.2       -2.80  7.71e- 3\n## 3 regionsouth        -21.3     15.1       -1.41  1.66e- 1\n## 4 regionwest           1.88    13.9        0.135 8.93e- 1\n## 5 residents           -0.134    0.0608    -2.21  3.28e- 2\n## 6 young_residents      1.50     0.308      4.89  1.47e- 5\n## 7 per_capita_income    0.100    0.0125     8.06  3.85e-10\n\nThis format is easier to handle than the standard lm() output:\n\ntidied$regressions[[1]]\n## \n## Call:\n## lm(formula = per_capita_exp ~ region + residents + young_residents + \n##     per_capita_income, data = .)\n## \n## Coefficients:\n##       (Intercept)    regionnortheast        regionsouth  \n##         -571.0568           -48.0018           -21.3019  \n##        regionwest          residents    young_residents  \n##            1.8808            -0.1341             1.5042  \n## per_capita_income  \n##            0.1005\n\nNow that I have all these regression results, I can compute any statistic I need. But first, let’s transform the data even further:\n\nlist_mods &lt;- tidied %&gt;% \n    pull(tidy_lm)\n\nlist_mods is a list of the tidy_lm data frames. I now add an index and bind the rows together (by using map2_df() instead of map2()):\n\nmods_df &lt;- map2_df(list_mods, \n                   seq(1, resamples), \n                   ~mutate(.x, resample = .y))\n\nLet’s take a look at the final object:\n\nhead(mods_df, 25)\n## # A tibble: 25 x 6\n##    term              estimate std.error statistic  p.value resample\n##    &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;\n##  1 (Intercept)       -571.     109.        -5.22  4.92e- 6        1\n##  2 regionnortheast    -48.0     17.2       -2.80  7.71e- 3        1\n##  3 regionsouth        -21.3     15.1       -1.41  1.66e- 1        1\n##  4 regionwest           1.88    13.9        0.135 8.93e- 1        1\n##  5 residents           -0.134    0.0608    -2.21  3.28e- 2        1\n##  6 young_residents      1.50     0.308      4.89  1.47e- 5        1\n##  7 per_capita_income    0.100    0.0125     8.06  3.85e-10        1\n##  8 (Intercept)        -97.2    145.        -0.672 5.05e- 1        2\n##  9 regionnortheast     -1.48    10.8       -0.136 8.92e- 1        2\n## 10 regionsouth         12.5     11.4        1.09  2.82e- 1        2\n## # … with 15 more rows\n\nNow this is a very useful format, because I now can group by the term column and compute any statistics I need, in the present case the standard deviation:\n\n(\n    r.std.error &lt;- mods_df %&gt;% \n        group_by(term) %&gt;% \n        summarise(r.std.error = sd(estimate))\n)\n## # A tibble: 7 x 2\n##   term              r.std.error\n##   &lt;chr&gt;                   &lt;dbl&gt;\n## 1 (Intercept)          220.    \n## 2 per_capita_income      0.0197\n## 3 regionnortheast       24.5   \n## 4 regionsouth           21.1   \n## 5 regionwest            22.7   \n## 6 residents              0.0607\n## 7 young_residents        0.498\n\nWe can append this column to the linear regression model result:\n\nlmfit %&gt;% \n    broom::tidy() %&gt;% \n    full_join(r.std.error) %&gt;% \n    select(term, estimate, std.error, r.std.error)\n## Joining, by = \"term\"\n## # A tibble: 7 x 4\n##   term               estimate std.error r.std.error\n##   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n## 1 (Intercept)       -467.      143.        220.    \n## 2 regionnortheast     15.7      18.2        24.5   \n## 3 regionsouth          7.09     17.3        21.1   \n## 4 regionwest          34.3      17.5        22.7   \n## 5 residents           -0.0346    0.0532      0.0607\n## 6 young_residents      1.30      0.357       0.498 \n## 7 per_capita_income    0.0720    0.0131      0.0197\n\nAs you see, using the whole bootstrapping procedure is longer than simply using either one of the first two methods. However, this procedure is very flexible and can thus be adapted to a very large range of situations. Either way, in the case of heteroskedasticity, you can see that results vary a lot depending on the procedure you use, so I would advise to use them all as robustness tests and discuss the differences."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html",
    "href": "posts/2017-03-24-lesser_known_purrr.html",
    "title": "Lesser known purrr tricks",
    "section": "",
    "text": "purrr is a package that extends R’s functional programming capabilities. It brings a lot of new stuff to the table and in this post I show you some of the most useful (at least to me) functions included in purrr."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#getting-rid-of-loops-with-map",
    "href": "posts/2017-03-24-lesser_known_purrr.html#getting-rid-of-loops-with-map",
    "title": "Lesser known purrr tricks",
    "section": "\nGetting rid of loops with map()\n",
    "text": "Getting rid of loops with map()\n\nlibrary(purrr)\n\nnumbers &lt;- list(11, 12, 13, 14)\n\nmap_dbl(numbers, sqrt)\n## [1] 3.316625 3.464102 3.605551 3.741657\n\nYou might wonder why this might be preferred to a for loop? It’s a lot less verbose, and you do not need to initialise any kind of structure to hold the result. If you google “create empty list in R” you will see that this is very common. However, with the map() family of functions, there is no need for an initial structure. map_dbl() returns an atomic list of real numbers, but if you use map() you will get a list back. Try them all out!"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#map-conditionally",
    "href": "posts/2017-03-24-lesser_known_purrr.html#map-conditionally",
    "title": "Lesser known purrr tricks",
    "section": "\nMap conditionally\n",
    "text": "Map conditionally\n\n\n\nmap_if()\n\n# Create a helper function that returns TRUE if a number is even\nis_even &lt;- function(x){\n  !as.logical(x %% 2)\n}\n\nmap_if(numbers, is_even, sqrt)\n## [[1]]\n## [1] 11\n## \n## [[2]]\n## [1] 3.464102\n## \n## [[3]]\n## [1] 13\n## \n## [[4]]\n## [1] 3.741657\n\n\n\nmap_at()\n\nmap_at(numbers, c(1,3), sqrt)\n## [[1]]\n## [1] 3.316625\n## \n## [[2]]\n## [1] 12\n## \n## [[3]]\n## [1] 3.605551\n## \n## [[4]]\n## [1] 14\n\nmap_if() and map_at() have a further argument than map(); in the case of map_if(), a predicate function ( a function that returns TRUE or FALSE) and a vector of positions for map_at(). This allows you to map your function only when certain conditions are met, which is also something that a lot of people google for."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#map-a-function-with-multiple-arguments",
    "href": "posts/2017-03-24-lesser_known_purrr.html#map-a-function-with-multiple-arguments",
    "title": "Lesser known purrr tricks",
    "section": "\nMap a function with multiple arguments\n",
    "text": "Map a function with multiple arguments\n\nnumbers2 &lt;- list(1, 2, 3, 4)\n\nmap2(numbers, numbers2, `+`)\n## [[1]]\n## [1] 12\n## \n## [[2]]\n## [1] 14\n## \n## [[3]]\n## [1] 16\n## \n## [[4]]\n## [1] 18\n\nYou can map two lists to a function which takes two arguments using map_2(). You can even map an arbitrary number of lists to any function using pmap().\n\n\nBy the way, try this in: +(1,3) and see what happens."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#dont-stop-execution-of-your-function-if-something-goes-wrong",
    "href": "posts/2017-03-24-lesser_known_purrr.html#dont-stop-execution-of-your-function-if-something-goes-wrong",
    "title": "Lesser known purrr tricks",
    "section": "\nDon’t stop execution of your function if something goes wrong\n",
    "text": "Don’t stop execution of your function if something goes wrong\n\npossible_sqrt &lt;- possibly(sqrt, otherwise = NA_real_)\n\nnumbers_with_error &lt;- list(1, 2, 3, \"spam\", 4)\n\nmap(numbers_with_error, possible_sqrt)\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 1.414214\n## \n## [[3]]\n## [1] 1.732051\n## \n## [[4]]\n## [1] NA\n## \n## [[5]]\n## [1] 2\n\nAnother very common issue is to keep running your loop even when something goes wrong. In most cases the loop simply stops at the error, but you would like it to continue and see where it failed. Try to google “skip error in a loop” or some variation of it and you’ll see that a lot of people really just want that. This is possible by combining map() and possibly(). Most solutions involve the use of tryCatch() which I personally do not find very easy to use."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#dont-stop-execution-of-your-function-if-something-goes-wrong-and-capture-the-error",
    "href": "posts/2017-03-24-lesser_known_purrr.html#dont-stop-execution-of-your-function-if-something-goes-wrong-and-capture-the-error",
    "title": "Lesser known purrr tricks",
    "section": "\nDon’t stop execution of your function if something goes wrong and capture the error\n",
    "text": "Don’t stop execution of your function if something goes wrong and capture the error\n\nsafe_sqrt &lt;- safely(sqrt, otherwise = NA_real_)\n\nmap(numbers_with_error, safe_sqrt)\n## [[1]]\n## [[1]]$result\n## [1] 1\n## \n## [[1]]$error\n## NULL\n## \n## \n## [[2]]\n## [[2]]$result\n## [1] 1.414214\n## \n## [[2]]$error\n## NULL\n## \n## \n## [[3]]\n## [[3]]$result\n## [1] 1.732051\n## \n## [[3]]$error\n## NULL\n## \n## \n## [[4]]\n## [[4]]$result\n## [1] NA\n## \n## [[4]]$error\n## \n\nsafely() is very similar to possibly() but it returns a list of lists. An element is thus a list of the result and the accompagnying error message. If there is no error, the error component is NULL if there is an error, it returns the error message."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#transpose-a-list",
    "href": "posts/2017-03-24-lesser_known_purrr.html#transpose-a-list",
    "title": "Lesser known purrr tricks",
    "section": "\nTranspose a list\n",
    "text": "Transpose a list\n\nsafe_result_list &lt;- map(numbers_with_error, safe_sqrt)\n\ntranspose(safe_result_list)\n## $result\n## $result[[1]]\n## [1] 1\n## \n## $result[[2]]\n## [1] 1.414214\n## \n## $result[[3]]\n## [1] 1.732051\n## \n## $result[[4]]\n## [1] NA\n## \n## $result[[5]]\n## [1] 2\n## \n## \n## $error\n## $error[[1]]\n## NULL\n## \n## $error[[2]]\n## NULL\n## \n## $error[[3]]\n## NULL\n## \n## $error[[4]]\n## \n\nHere we transposed the above list. This means that we still have a list of lists, but where the first list holds all the results (which you can then access with safe_result_list$result) and the second list holds all the errors (which you can access with safe_result_list$error). This can be quite useful!"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#apply-a-function-to-a-lower-depth-of-a-list",
    "href": "posts/2017-03-24-lesser_known_purrr.html#apply-a-function-to-a-lower-depth-of-a-list",
    "title": "Lesser known purrr tricks",
    "section": "\nApply a function to a lower depth of a list\n",
    "text": "Apply a function to a lower depth of a list\n\ntransposed_list &lt;- transpose(safe_result_list)\n\ntransposed_list %&gt;%\n    at_depth(2, is_null)\n## Warning: at_depth() is deprecated, please use `modify_depth()` instead\n## $result\n## $result[[1]]\n## [1] FALSE\n## \n## $result[[2]]\n## [1] FALSE\n## \n## $result[[3]]\n## [1] FALSE\n## \n## $result[[4]]\n## [1] FALSE\n## \n## $result[[5]]\n## [1] FALSE\n## \n## \n## $error\n## $error[[1]]\n## [1] TRUE\n## \n## $error[[2]]\n## [1] TRUE\n## \n## $error[[3]]\n## [1] TRUE\n## \n## $error[[4]]\n## [1] FALSE\n## \n## $error[[5]]\n## [1] TRUE\n\nSometimes working with lists of lists can be tricky, especially when we want to apply a function to the sub-lists. This is easily done with at_depth()!"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#set-names-of-list-elements",
    "href": "posts/2017-03-24-lesser_known_purrr.html#set-names-of-list-elements",
    "title": "Lesser known purrr tricks",
    "section": "\nSet names of list elements\n",
    "text": "Set names of list elements\n\nname_element &lt;- c(\"sqrt()\", \"ok?\")\n\nset_names(transposed_list, name_element)\n## $`sqrt()`\n## $`sqrt()`[[1]]\n## [1] 1\n## \n## $`sqrt()`[[2]]\n## [1] 1.414214\n## \n## $`sqrt()`[[3]]\n## [1] 1.732051\n## \n## $`sqrt()`[[4]]\n## [1] NA\n## \n## $`sqrt()`[[5]]\n## [1] 2\n## \n## \n## $`ok?`\n## $`ok?`[[1]]\n## NULL\n## \n## $`ok?`[[2]]\n## NULL\n## \n## $`ok?`[[3]]\n## NULL\n## \n## $`ok?`[[4]]\n##"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#reduce-a-list-to-a-single-value",
    "href": "posts/2017-03-24-lesser_known_purrr.html#reduce-a-list-to-a-single-value",
    "title": "Lesser known purrr tricks",
    "section": "\nReduce a list to a single value\n",
    "text": "Reduce a list to a single value\n\nreduce(numbers, `*`)\n## [1] 24024\n\nreduce() applies the function * iteratively to the list of numbers. There’s also accumulate():\n\naccumulate(numbers, `*`)\n## [1]    11   132  1716 24024\n\nwhich keeps the intermediary results.\n\n\nThis function is very general, and you can reduce anything:\n\n\nMatrices:\n\nmat1 &lt;- matrix(rnorm(10), nrow = 2)\nmat2 &lt;- matrix(rnorm(10), nrow = 2)\nmat3 &lt;- matrix(rnorm(10), nrow = 2)\nlist_mat &lt;- list(mat1, mat2, mat3)\n\nreduce(list_mat, `+`)\n##             [,1]       [,2]       [,3]     [,4]      [,5]\n## [1,] -2.48530177  1.0110049  0.4450388 1.280802 1.3413979\n## [2,]  0.07596679 -0.6872268 -0.6579242 1.615237 0.8231933\n\neven data frames:\n\ndf1 &lt;- as.data.frame(mat1)\ndf2 &lt;- as.data.frame(mat2)\ndf3 &lt;- as.data.frame(mat3)\n\nlist_df &lt;- list(df1, df2, df3)\n\nreduce(list_df, dplyr::full_join)\n## Joining, by = c(\"V1\", \"V2\", \"V3\", \"V4\", \"V5\")\n## Joining, by = c(\"V1\", \"V2\", \"V3\", \"V4\", \"V5\")\n##           V1         V2          V3          V4         V5\n## 1 -0.6264538 -0.8356286  0.32950777  0.48742905  0.5757814\n## 2  0.1836433  1.5952808 -0.82046838  0.73832471 -0.3053884\n## 3 -0.8969145  1.5878453 -0.08025176  0.70795473  1.9844739\n## 4  0.1848492 -1.1303757  0.13242028 -0.23969802 -0.1387870\n## 5 -0.9619334  0.2587882  0.19578283  0.08541773 -1.2188574\n## 6 -0.2925257 -1.1521319  0.03012394  1.11661021  1.2673687\n\nHope you enjoyed this list of useful functions! If you enjoy the content of my blog, you can follow me on twitter."
  },
  {
    "objectID": "posts/2019-08-17-modern_R.html",
    "href": "posts/2019-08-17-modern_R.html",
    "title": "Modern R with the tidyverse is available on Leanpub",
    "section": "",
    "text": "Yesterday I released an ebook on Leanpub, called Modern R with the tidyverse, which you can also read for free here.\n\n\nIn this blog post, I want to give some context.\n\n\nModern R with the tidyverse is the second ebook I release on Leanpub. I released the first one, called Functional programming and unit testing for data munging with R around Christmas 2016 (I’ve retired it on Leanpub, but you can still read it for free here) . I just had moved back to my home country of Luxembourg and started a new job as a research assistant at the statistical national institute. Since then, lots of things happened; I’ve changed jobs and joined PwC Luxembourg as a data scientist, was promoted to manager, finished my PhD, and most importantly of all, I became a father.\n\n\nThrough all this, I continued blogging and working on a new ebook, called Modern R with the tidyverse. At first, this was supposed to be a separate book from the first one, but as I continued writing, I realized that updating and finishing the first one, would take a lot of effort, and also, that it wouldn’t make much sense in keeping both separated. So I decided to merge the content from the first ebook with the second, and update everything in one go.\n\n\nMy very first notes were around 50 pages if memory serves, and I used them to teach R at the University of Strasbourg while I employed there as a research and teaching assistant and working on my PhD. These notes were the basis of Functional programming and unit testing for data munging with R and now Modern R. Chapter 2 of Modern R is almost a simple copy and paste from these notes (with more sections added). These notes were first written around 2012-2013ish.\n\n\nModern R is the kind of text I would like to have had when I first started playing around with R, sometime around 2009-2010. It starts from the beginning, but also goes quite into details in the later chapters. For instance, the section on modeling with functional programming is quite advanced, but I believe that readers that read through all the book and reached that part would be armed with all the needed knowledge to follow. At least, this is my hope.\n\n\nNow, the book is still not finished. Two chapters are missing, but it should not take me long to finish them as I already have drafts lying around. However, exercises might still be in wrong places, and more are required. Also, generally, more polishing is needed.\n\n\nAs written in the first paragraph of this section, the book is available on Leanpub. Unlike my previous ebook, this one costs money; a minimum price of 4.99$ and a recommended price of 14.99$, but as mentioned you can read it for free online. I’ve hesitated to give it a minimum price of 0$, but I figured that since the book can be read for free online, and that Leanpub has a 45 days return policy where readers can get 100% reimbursed, no questions asked (and keep the downloaded ebook), readers were not taking a lot of risks by buying it for 5 bucks. I sure hope however that readers will find that this ebook is worth at least 5 bucks!\n\n\nNow why should you read it? There’s already a lot of books on learning how to use R. Well, I don’t really want to convince you to read it. But some people do seem to like my style of writing and my blog posts, so I guess these same people, or similar people, might like the ebook. Also, I think that this ebook covers a lot of different topics, enough of them to make you an efficient R user. But as I’ve written in the introduction of Modern R:\n\n\nSo what you can expect from this book is that this book is not the only one you should read.\n\n\nAnyways, hope you’ll enjoy Modern R, suggestions, criticisms and reviews welcome!\n\n\nBy the way, the cover of the book is a painting by John William Waterhouse, depicting Diogenes of Sinope, an ancient Greek philosopher, an absolute mad lad. Read his Wikipedia page, it’s worth it."
  },
  {
    "objectID": "posts/2015-05-03-update-introduction-r-programming.html",
    "href": "posts/2015-05-03-update-introduction-r-programming.html",
    "title": "Update to Introduction to programming econometrics with R",
    "section": "",
    "text": "This semester I taught a course on applied econometrics with the R programming language. For this, I created a document that I gave to my students and shared online. This is the kind of document I would have liked to read when I first started using R. I already had some programming experience in C and Pascal but this is not necessarily the case for everyone that is confronted to R when they start learning about econometrics.\nThis is why the beginning of the document focuses more on general programming knowledge and techniques, and then only on econometrics. People online seemed to like the document, as I’ve received some positive comments by David Smith from Revolution R (read his blog post about the document here) and Dave Giles which links to David’s blog post here. People on twitter have also retweeted David’s and Dave’s tweets to their blog posts and I’ve received some requests by people to send them the PDF by email (because they live in places where Dropbox is not accessible unfortunately).\nThe document is still a work in progress (and will probably remain so for a long time), but I’ve added some new sections about reproducible research and thought that this update could warrant a new blog post.\nFor now, only linear models are reviewed, but I think I’ll start adding some chapters about non-linear models soonish. The goal for these notes, however, is not to re-invent the wheel: there are lots of good books about econometrics with R out there and packages that estimate a very wide range of models. What I want for these notes, is to focus more on the programming knowledge an econometrician needs, in a very broad and general sense. I want my students to understand that R is a true programming language, and that they need to use every feature offered by such a language, and not think of R as a black box into which you only type pre-programmed commands, but also be able to program their own routines.\nAlso, I’ve made it possible to create the PDF using a Makefile. This may be useful for people that do not have access to Dropbox, but are familiar with git.\nYou can compile the book in two ways: first download the whole repository:\ngit clone git@bitbucket.org:b-rodrigues/programmingeconometrics.git\nand then, with Rstudio, open the file appliedEconometrics.Rnw and knit it. Another solution is to use the Makefile. Just type:\nmake\ninside a terminal (should work on GNU+Linux and OSX systems) and it should compile the document. You may get some message about “additional information” for some R packages. When these come up, just press Q on your keyboard to continue the compilation process.\nGet the notes here.\nAs always, if you have questions or suggestions, do not hesitate to send me an email and I sure hope you’ll find these notes useful!"
  },
  {
    "objectID": "posts/2018-12-27-fun_gganimate.html",
    "href": "posts/2018-12-27-fun_gganimate.html",
    "title": "Some fun with {gganimate}",
    "section": "",
    "text": "Your browser does not support the video tag.\nIn this short blog post I show you how you can use the {gganimate} package to create animations from {ggplot2} graphs with data from UNU-WIDER."
  },
  {
    "objectID": "posts/2018-12-27-fun_gganimate.html#wiid-data",
    "href": "posts/2018-12-27-fun_gganimate.html#wiid-data",
    "title": "Some fun with {gganimate}",
    "section": "\nWIID data\n",
    "text": "WIID data\n\n\nJust before Christmas, UNU-WIDER released a new edition of their World Income Inequality Database:\n\n\n\nNEW #DATAWe’ve just released a new version of the World Income Inequality Database.WIID4 includes #data from 7 new countries, now totalling 189, and reaches the year 2017. All data is freely available for download on our website: https://t.co/XFxuLvyKTC pic.twitter.com/rCf9eXN8D5\n\n— UNU-WIDER (@UNUWIDER) December 21, 2018\n\n\n\nThe data is available in Excel and STATA formats, and I thought it was a great opportunity to release it as an R package. You can install it with:\n\ndevtools::install_github(\"b-rodrigues/wiid4\")\n\nHere a short description of the data, taken from UNU-WIDER’s website:\n\n\n“The World Income Inequality Database (WIID) presents information on income inequality for developed, developing, and transition countries. It provides the most comprehensive set of income inequality statistics available and can be downloaded for free.\n\n\nWIID4, released in December 2018, covers 189 countries (including historical entities), with over 11,000 data points in total. With the current version, the latest observations now reach the year 2017.”\n\n\nIt was also a good opportunity to play around with the {gganimate} package. This package makes it possible to create animations and is an extension to {ggplot2}. Read more about it here."
  },
  {
    "objectID": "posts/2018-12-27-fun_gganimate.html#preparing-the-data",
    "href": "posts/2018-12-27-fun_gganimate.html#preparing-the-data",
    "title": "Some fun with {gganimate}",
    "section": "\nPreparing the data\n",
    "text": "Preparing the data\n\n\nTo create a smooth animation, I need to have a cylindrical panel data set; meaning that for each country in the data set, there are no missing years. I also chose to focus on certain variables only; net income, all the population of the country (instead of just focusing on the economically active for instance) as well as all the country itself (and not just the rural areas). On this link you can find a codebook (pdf warning), so you can understand the filters I defined below better.\n\n\nLet’s first load the packages, data and perform the necessary transformations:\n\nlibrary(wiid4)\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(gganimate)\nlibrary(brotools)\n\nsmall_wiid4 &lt;- wiid4 %&gt;%\n    mutate(eu = as.character(eu)) %&gt;%\n    mutate(eu = case_when(eu == \"1\" ~ \"EU member state\",\n                          eu == \"0\" ~ \"Non-EU member state\")) %&gt;%\n    filter(resource == 1, popcovr == 1, areacovr == 1, scale == 2) %&gt;%\n    group_by(country) %&gt;%\n    group_by(country, year) %&gt;%\n    filter(quality_score == max(quality_score)) %&gt;%\n    filter(source == min(source)) %&gt;%\n    filter(!is.na(bottom5)) %&gt;%\n    group_by(country) %&gt;%\n    mutate(flag = ifelse(all(seq(2004, 2016) %in% year), 1, 0)) %&gt;%\n    filter(flag == 1, year &gt; 2003) %&gt;%\n    mutate(year = lubridate::ymd(paste0(year, \"-01-01\")))\n\nFor some country and some years, there are several sources of data with varying quality. I only keep the highest quality sources with:\n\n    group_by(country, year) %&gt;%\n    filter(quality_score == max(quality_score)) %&gt;%\n\nIf there are different sources of equal quality, I give priority to the sources that are the most comparable across country (Luxembourg Income Study, LIS data) to less comparable sources with (at least that’s my understanding of the source variable):\n\n    filter(source == min(source)) %&gt;%\n\nI then remove missing data with:\n\n    filter(!is.na(bottom5)) %&gt;%\n\nbottom5 and top5 give the share of income that is controlled by the bottom 5% and top 5% respectively. These are the variables that I want to plot.\n\n\nFinally I keep the years 2004 to 2016, without any interruption with the following line:\n\n    mutate(flag = ifelse(all(seq(2004, 2016) %in% year), 1, 0)) %&gt;%\n    filter(flag == 1, year &gt; 2003) %&gt;%\n\nifelse(all(seq(2004, 2016) %in% year), 1, 0)) creates a flag that equals 1 only if the years 2004 to 2016 are present in the data without any interruption. Then I only keep the data from 2004 on and only where the flag variable equals 1.\n\n\nIn the end, I ended up only with European countries. It would have been interesting to have countries from other continents, but apparently only European countries provide data in an annual basis."
  },
  {
    "objectID": "posts/2018-12-27-fun_gganimate.html#creating-the-animation",
    "href": "posts/2018-12-27-fun_gganimate.html#creating-the-animation",
    "title": "Some fun with {gganimate}",
    "section": "\nCreating the animation\n",
    "text": "Creating the animation\n\n\nTo create the animation I first started by creating a static ggplot showing what I wanted; a scatter plot of the income by bottom and top 5%. The size of the bubbles should be proportional to the GDP of the country (another variable provided in the data). Once the plot looked how I wanted I added the lines that are specific to {gganimate}:\n\n    labs(title = 'Year: {frame_time}', x = 'Top 5', y = 'Bottom 5') +\n    transition_time(year) +\n    ease_aes('linear')\n\nI took this from {gganimate}’s README.\n\nanimation &lt;- ggplot(small_wiid4) +\n    geom_point(aes(y = bottom5, x = top5, colour = eu, size = log(gdp_ppp_pc_usd2011))) +\n    xlim(c(10, 20)) +\n    geom_label_repel(aes(y = bottom5, x = top5, label = country), hjust = 1, nudge_x = 20) +\n    theme(legend.position = \"bottom\") +\n    theme_blog() +\n    scale_color_blog() +\n    labs(title = 'Year: {frame_time}', x = 'Top 5', y = 'Bottom 5') +\n    transition_time(year) +\n    ease_aes('linear')\n\nI use geom_label_repel to place the countries’ labels on the right of the plot. If I don’t do this, the labels of the countries would be floating around and the animation would be unreadable.\n\n\nI then spent some time trying to render a nice webm instead of a gif. It took some trial and error and I am still not entirely satisfied with the result, but here is the code to render the animation:\n\nanimate(animation, renderer = ffmpeg_renderer(options = list(s = \"864x480\", \n                                                             vcodec = \"libvpx-vp9\",\n                                                             crf = \"15\",\n                                                             b = \"1600k\", \n                                                             vf = \"setpts=5*PTS\")))\n\nThe option vf = “setpts=5*PTS” is important because it slows the video down, so we can actually see something. crf = “15” is the quality of the video (lower is better), b = “1600k” is the bitrate, and vcodec = “libvpx-vp9” is the codec I use. The video you saw at the top of this post is the result. You can also find the video here, and here’s a gif if all else fails:\n\n\n\n \n\n\n\nI would have preferred if the video was smoother, which should be possible by creating more frames. I did not find such an option in {gganimate}, and perhaps there is none, at least for now.\n\n\nIn any case {gganimate} is pretty nice to play with, and I’ll definitely use it more!"
  },
  {
    "objectID": "posts/2018-10-05-ggplot2_purrr_officer.html",
    "href": "posts/2018-10-05-ggplot2_purrr_officer.html",
    "title": "Exporting editable plots from R to Powerpoint: making ggplot2 purrr with officer",
    "section": "",
    "text": "A kind reader let me know that the function create_pptx() is now outdated, and proposed an update which you can find here: here. Thank you @Jeremy!\n\n\nI was recently confronted to the following problem: creating hundreds of plots that could still be edited by our client. What this meant was that I needed to export the graphs in Excel or Powerpoint or some other such tool that was familiar to the client, and not export the plots directly to pdf or png as I would normally do. I still wanted to use R to do it though, because I could do what I always do to when I need to perform repetitive tasks such as producing hundreds of plots; map over a list of, say, countries, and make one plot per country. This is something I discussed in a previous blog post, Make ggplot2 purrr.\n\n\nSo, after some online seaching, I found the {officer} package. This package allows you to put objects into Microsoft documents. For example, editable plots in a Powerpoint document. This is what I will show in this blog post.\n\n\nLet’s start by loading the required packages:\n\nlibrary(\"tidyverse\")\nlibrary(\"officer\")\nlibrary(\"rvg\")\n\nThen, I will use the data from the time use survey, which I discussed in a previous blog post Going from a human readable Excel file to a machine-readable csv with {tidyxl}.\n\n\nYou can download the data here.\n\n\nLet’s import and prepare it:\n\ntime_use &lt;- rio::import(\"clean_data.csv\")\n\n\ntime_use &lt;- time_use %&gt;%\n    filter(population %in% c(\"Male\", \"Female\")) %&gt;%\n    filter(activities %in% c(\"Personal care\", \"Sleep\", \"Eating\", \n                             \"Employment\", \"Household and family care\")) %&gt;%\n    group_by(day) %&gt;%\n    nest()\n\nI only kept two categories, “Male” and “Female” and 5 activities. Then I grouped by day and nested the data. This is how it looks like:\n\ntime_use\n## # A tibble: 3 x 2\n##   day                         data             \n##   &lt;chr&gt;                       &lt;list&gt;           \n## 1 Year 2014_Monday til Friday &lt;tibble [10 × 4]&gt;\n## 2 Year 2014_Saturday          &lt;tibble [10 × 4]&gt;\n## 3 Year 2014_Sunday            &lt;tibble [10 × 4]&gt;\n\nAs shown, time_use is a tibble with 2 columns, the first day contains the days, and the second data, is of type list, and each element of these lists are tibbles themselves. Let’s take a look inside one:\n\ntime_use$data[1]\n## [[1]]\n## # A tibble: 10 x 4\n##    population activities                time  time_in_minutes\n##    &lt;chr&gt;      &lt;chr&gt;                     &lt;chr&gt;           &lt;int&gt;\n##  1 Male       Personal care             11:00             660\n##  2 Male       Sleep                     08:24             504\n##  3 Male       Eating                    01:46             106\n##  4 Male       Employment                08:11             491\n##  5 Male       Household and family care 01:59             119\n##  6 Female     Personal care             11:15             675\n##  7 Female     Sleep                     08:27             507\n##  8 Female     Eating                    01:48             108\n##  9 Female     Employment                06:54             414\n## 10 Female     Household and family care 03:49             229\n\nI can now create plots for each of the days with the following code:\n\nmy_plots &lt;- time_use %&gt;%\n    mutate(plots = map2(.y = day, .x = data, ~ggplot(data = .x) + theme_minimal() +\n                       geom_col(aes(y = time_in_minutes, x = activities, fill = population), \n                                position = \"dodge\") +\n                       ggtitle(.y) +\n                       ylab(\"Time in minutes\") +\n                       xlab(\"Activities\")))\n\nThese steps are all detailled in my blog post Make ggplot2 purrr. Let’s take a look at my_plots:\n\nmy_plots\n## # A tibble: 3 x 3\n##   day                         data              plots \n##   &lt;chr&gt;                       &lt;list&gt;            &lt;list&gt;\n## 1 Year 2014_Monday til Friday &lt;tibble [10 × 4]&gt; &lt;gg&gt;  \n## 2 Year 2014_Saturday          &lt;tibble [10 × 4]&gt; &lt;gg&gt;  \n## 3 Year 2014_Sunday            &lt;tibble [10 × 4]&gt; &lt;gg&gt;\n\nThe last column, called plots is a list where each element is a plot! We can take a look at one:\n\nmy_plots$plots[1]\n## [[1]]\n\n\n\n\nNow, this is where I could export these plots as pdfs or pngs. But this is not what I need. I need to export these plots as editable charts for Powerpoint. To do this for one image, I would do the following (as per {officer}’s documentation):\n\nread_pptx() %&gt;%\n    add_slide(layout = \"Title and Content\", master = \"Office Theme\") %&gt;%\n    ph_with_vg(code = print(one_plot), type = \"body\") %&gt;% \n    print(target = path)\n\nTo map this over a list of arguments, I wrote a wrapper:\n\ncreate_pptx &lt;- function(plot, path){\n    if(!file.exists(path)) {\n        out &lt;- read_pptx()\n    } else {\n        out &lt;- read_pptx(path)\n    }\n    \n    out %&gt;%\n        add_slide(layout = \"Title and Content\", master = \"Office Theme\") %&gt;%\n        ph_with_vg(code = print(plot), type = \"body\") %&gt;% \n        print(target = path)\n}\n\nThis function takes two arguments, plot and path. plot must be an plot object such as the ones contained inside the plots column of my_plots tibble. path is the path of where I want to save the pptx.\n\n\nThe first lines check if the file exists, if yes, the slides get added to the existing file, if not a new pptx gets created. The rest of the code is very similar to the one from the documentation. Now, to create my pptx I simple need to map over the plots column and provide a path:\n\nmap(my_plots$plots, create_pptx, path = \"test.pptx\")\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n## [[1]]\n## [1] \"/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx\"\n## \n## [[2]]\n## [1] \"/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx\"\n## \n## [[3]]\n## [1] \"/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx\"\n\nHere is the end result:\n\n\n\n\n\nInside Powerpoint (or in this case Libreoffice), the plots are geometric shapes that can now be edited!"
  },
  {
    "objectID": "posts/2019-08-14-lpm.html",
    "href": "posts/2019-08-14-lpm.html",
    "title": "Using linear models with binary dependent variables, a simulation study",
    "section": "",
    "text": "This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free here. This is taken from Chapter 8, in which I discuss advanced functional programming methods for modeling.\n\n\nAs written just above (note: as written above in the book), map() simply applies a function to a list of inputs, and in the previous section we mapped ggplot() to generate many plots at once. This approach can also be used to map any modeling functions, for instance lm() to a list of datasets.\n\n\nFor instance, suppose that you wish to perform a Monte Carlo simulation. Suppose that you are dealing with a binary choice problem; usually, you would use a logistic regression for this.\n\n\nHowever, in certain disciplines, especially in the social sciences, the so-called Linear Probability Model is often used as well. The LPM is a simple linear regression, but unlike the standard setting of a linear regression, the dependent variable, or target, is a binary variable, and not a continuous variable. Before you yell “Wait, that’s illegal”, you should know that in practice LPMs do a good job of estimating marginal effects, which is what social scientists and econometricians are often interested in. Marginal effects are another way of interpreting models, giving how the outcome (or the target) changes given a change in a independent variable (or a feature). For instance, a marginal effect of 0.10 for age would mean that probability of success would increase by 10% for each added year of age.\n\n\nThere has been a lot of discussion on logistic regression vs LPMs, and there are pros and cons of using LPMs. Micro-econometricians are still fond of LPMs, even though the pros of LPMs are not really convincing. However, quoting Angrist and Pischke:\n\n\n“While a nonlinear model may fit the CEF (population conditional expectation function) for LDVs (limited dependent variables) more closely than a linear model, when it comes to marginal effects, this probably matters little” (source: Mostly Harmless Econometrics)\n\n\nso LPMs are still used for estimating marginal effects.\n\n\nLet us check this assessment with one example. First, we simulate some data, then run a logistic regression and compute the marginal effects, and then compare with a LPM:\n\nset.seed(1234)\nx1 &lt;- rnorm(100)\nx2 &lt;- rnorm(100)\n  \nz &lt;- .5 + 2*x1 + 4*x2\n\np &lt;- 1/(1 + exp(-z))\n\ny &lt;- rbinom(100, 1, p)\n\ndf &lt;- tibble(y = y, x1 = x1, x2 = x2)\n\nThis data generating process generates data from a binary choice model. Fitting the model using a logistic regression allows us to recover the structural parameters:\n\nlogistic_regression &lt;- glm(y ~ ., data = df, family = binomial(link = \"logit\"))\n\nLet’s see a summary of the model fit:\n\nsummary(logistic_regression)\n## \n## Call:\n## glm(formula = y ~ ., family = binomial(link = \"logit\"), data = df)\n## \n## Deviance Residuals: \n##      Min        1Q    Median        3Q       Max  \n## -2.91941  -0.44872   0.00038   0.42843   2.55426  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)   0.0960     0.3293   0.292 0.770630    \n## x1            1.6625     0.4628   3.592 0.000328 ***\n## x2            3.6582     0.8059   4.539 5.64e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 138.629  on 99  degrees of freedom\n## Residual deviance:  60.576  on 97  degrees of freedom\n## AIC: 66.576\n## \n## Number of Fisher Scoring iterations: 7\n\nWe do recover the parameters that generated the data, but what about the marginal effects? We can get the marginal effects easily using the {margins} package:\n\nlibrary(margins)\n\nmargins(logistic_regression)\n## Average marginal effects\n## glm(formula = y ~ ., family = binomial(link = \"logit\"), data = df)\n##      x1     x2\n##  0.1598 0.3516\n\nOr, even better, we can compute the true marginal effects, since we know the data generating process:\n\nmeffects &lt;- function(dataset, coefs){\n  X &lt;- dataset %&gt;% \n  select(-y) %&gt;% \n  as.matrix()\n  \n  dydx_x1 &lt;- mean(dlogis(X%*%c(coefs[2], coefs[3]))*coefs[2])\n  dydx_x2 &lt;- mean(dlogis(X%*%c(coefs[2], coefs[3]))*coefs[3])\n  \n  tribble(~term, ~true_effect,\n          \"x1\", dydx_x1,\n          \"x2\", dydx_x2)\n}\n\n(true_meffects &lt;- meffects(df, c(0.5, 2, 4)))\n## # A tibble: 2 x 2\n##   term  true_effect\n##   &lt;chr&gt;       &lt;dbl&gt;\n## 1 x1          0.175\n## 2 x2          0.350\n\nOk, so now what about using this infamous Linear Probability Model to estimate the marginal effects?\n\nlpm &lt;- lm(y ~ ., data = df)\n\nsummary(lpm)\n## \n## Call:\n## lm(formula = y ~ ., data = df)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.83953 -0.31588 -0.02885  0.28774  0.77407 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  0.51340    0.03587  14.314  &lt; 2e-16 ***\n## x1           0.16771    0.03545   4.732 7.58e-06 ***\n## x2           0.31250    0.03449   9.060 1.43e-14 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.3541 on 97 degrees of freedom\n## Multiple R-squared:  0.5135, Adjusted R-squared:  0.5034 \n## F-statistic: 51.18 on 2 and 97 DF,  p-value: 6.693e-16\n\nIt’s not too bad, but maybe it could have been better in other circumstances. Perhaps if we had more observations, or perhaps for a different set of structural parameters the results of the LPM would have been closer. The LPM estimates the marginal effect of x1 to be 0.1677134 vs 0.1597956 for the logistic regression and for x2, the LPM estimation is 0.3124966 vs 0.351607. The true marginal effects are 0.1750963 and 0.3501926 for x1 and x2 respectively.\n\n\nJust as to assess the accuracy of a model data scientists perform cross-validation, a Monte Carlo study can be performed to asses how close the estimation of the marginal effects using a LPM is to the marginal effects derived from a logistic regression. It will allow us to test with datasets of different sizes, and generated using different structural parameters.\n\n\nFirst, let’s write a function that generates data. The function below generates 10 datasets of size 100 (the code is inspired by this StackExchange answer):\n\ngenerate_datasets &lt;- function(coefs = c(.5, 2, 4), sample_size = 100, repeats = 10){\n\n  generate_one_dataset &lt;- function(coefs, sample_size){\n  x1 &lt;- rnorm(sample_size)\n  x2 &lt;- rnorm(sample_size)\n  \n  z &lt;- coefs[1] + coefs[2]*x1 + coefs[3]*x2\n\n  p &lt;- 1/(1 + exp(-z))\n\n  y &lt;- rbinom(sample_size, 1, p)\n\n  df &lt;- tibble(y = y, x1 = x1, x2 = x2)\n  }\n\n  simulations &lt;- rerun(.n = repeats, generate_one_dataset(coefs, sample_size))\n \n  tibble(\"coefs\" = list(coefs), \"sample_size\" = sample_size, \"repeats\" = repeats, \"simulations\" = list(simulations))\n}\n\nLet’s first generate one dataset:\n\none_dataset &lt;- generate_datasets(repeats = 1)\n\nLet’s take a look at one_dataset:\n\none_dataset\n## # A tibble: 1 x 4\n##   coefs     sample_size repeats simulations\n##   &lt;list&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;     \n## 1 &lt;dbl [3]&gt;         100       1 &lt;list [1]&gt;\n\nAs you can see, the tibble with the simulated data is inside a list-column called simulations. Let’s take a closer look:\n\nstr(one_dataset$simulations)\n## List of 1\n##  $ :List of 1\n##   ..$ :Classes 'tbl_df', 'tbl' and 'data.frame': 100 obs. of  3 variables:\n##   .. ..$ y : int [1:100] 0 1 1 1 0 1 1 0 0 1 ...\n##   .. ..$ x1: num [1:100] 0.437 1.06 0.452 0.663 -1.136 ...\n##   .. ..$ x2: num [1:100] -2.316 0.562 -0.784 -0.226 -1.587 ...\n\nThe structure is quite complex, and it’s important to understand this, because it will have an impact on the next lines of code; it is a list, containing a list, containing a dataset! No worries though, we can still map over the datasets directly, by using modify_depth() instead of map().\n\n\nNow, let’s fit a LPM and compare the estimation of the marginal effects with the true marginal effects. In order to have some confidence in our results, we will not simply run a linear regression on that single dataset, but will instead simulate hundreds, then thousands and ten of thousands of data sets, get the marginal effects and compare them to the true ones (but here I won’t simulate more than 500 datasets).\n\n\nLet’s first generate 10 datasets:\n\nmany_datasets &lt;- generate_datasets()\n\nNow comes the tricky part. I have this object, many_datasets looking like this:\n\nmany_datasets\n## # A tibble: 1 x 4\n##   coefs     sample_size repeats simulations\n##   &lt;list&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;     \n## 1 &lt;dbl [3]&gt;         100      10 &lt;list [10]&gt;\n\nI would like to fit LPMs to the 10 datasets. For this, I will need to use all the power of functional programming and the {tidyverse}. I will be adding columns to this data frame using mutate() and mapping over the simulations list-column using modify_depth(). The list of data frames is at the second level (remember, it’s a list containing a list containing data frames).\n\n\nI’ll start by fitting the LPMs, then using broom::tidy() I will get a nice data frame of the estimated parameters. I will then only select what I need, and then bind the rows of all the data frames. I will do the same for the true marginal effects.\n\n\nI highly suggest that you run the following lines, one after another. It is complicated to understand what’s going on if you are not used to such workflows. However, I hope to convince you that once it will click, it’ll be much more intuitive than doing all this inside a loop. Here’s the code:\n\nresults &lt;- many_datasets %&gt;% \n  mutate(lpm = modify_depth(simulations, 2, ~lm(y ~ ., data = .x))) %&gt;% \n  mutate(lpm = modify_depth(lpm, 2, broom::tidy)) %&gt;% \n  mutate(lpm = modify_depth(lpm, 2, ~select(., term, estimate))) %&gt;% \n  mutate(lpm = modify_depth(lpm, 2, ~filter(., term != \"(Intercept)\"))) %&gt;% \n  mutate(lpm = map(lpm, bind_rows)) %&gt;% \n  mutate(true_effect = modify_depth(simulations, 2, ~meffects(., coefs = coefs[[1]]))) %&gt;% \n  mutate(true_effect = map(true_effect, bind_rows))\n\nThis is how results looks like:\n\nresults\n## # A tibble: 1 x 6\n##   coefs     sample_size repeats simulations lpm             true_effect    \n##   &lt;list&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;      &lt;list&gt;          &lt;list&gt;         \n## 1 &lt;dbl [3]&gt;         100      10 &lt;list [10]&gt; &lt;tibble [20 × … &lt;tibble [20 × …\n\nLet’s take a closer look to the lpm and true_effect columns:\n\nresults$lpm\n## [[1]]\n## # A tibble: 20 x 2\n##    term  estimate\n##    &lt;chr&gt;    &lt;dbl&gt;\n##  1 x1       0.228\n##  2 x2       0.353\n##  3 x1       0.180\n##  4 x2       0.361\n##  5 x1       0.165\n##  6 x2       0.374\n##  7 x1       0.182\n##  8 x2       0.358\n##  9 x1       0.125\n## 10 x2       0.345\n## 11 x1       0.171\n## 12 x2       0.331\n## 13 x1       0.122\n## 14 x2       0.309\n## 15 x1       0.129\n## 16 x2       0.332\n## 17 x1       0.102\n## 18 x2       0.374\n## 19 x1       0.176\n## 20 x2       0.410\nresults$true_effect\n## [[1]]\n## # A tibble: 20 x 2\n##    term  true_effect\n##    &lt;chr&gt;       &lt;dbl&gt;\n##  1 x1          0.183\n##  2 x2          0.366\n##  3 x1          0.166\n##  4 x2          0.331\n##  5 x1          0.174\n##  6 x2          0.348\n##  7 x1          0.169\n##  8 x2          0.339\n##  9 x1          0.167\n## 10 x2          0.335\n## 11 x1          0.173\n## 12 x2          0.345\n## 13 x1          0.157\n## 14 x2          0.314\n## 15 x1          0.170\n## 16 x2          0.340\n## 17 x1          0.182\n## 18 x2          0.365\n## 19 x1          0.161\n## 20 x2          0.321\n\nLet’s bind the columns, and compute the difference between the true and estimated marginal effects:\n\nsimulation_results &lt;- results %&gt;% \n  mutate(difference = map2(.x = lpm, .y = true_effect, bind_cols)) %&gt;% \n  mutate(difference = map(difference, ~mutate(., difference = true_effect - estimate))) %&gt;% \n  mutate(difference = map(difference, ~select(., term, difference))) %&gt;% \n  pull(difference) %&gt;% \n  .[[1]]\n\nLet’s take a look at the simulation results:\n\nsimulation_results %&gt;% \n  group_by(term) %&gt;% \n  summarise(mean = mean(difference), \n            sd = sd(difference))\n## # A tibble: 2 x 3\n##   term     mean     sd\n##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n## 1 x1     0.0122 0.0370\n## 2 x2    -0.0141 0.0306\n\nAlready with only 10 simulated datasets, the difference in means is not significant. Let’s rerun the analysis, but for difference sizes. In order to make things easier, we can put all the code into a nifty function:\n\nmonte_carlo &lt;- function(coefs, sample_size, repeats){\n  many_datasets &lt;- generate_datasets(coefs, sample_size, repeats)\n  \n  results &lt;- many_datasets %&gt;% \n    mutate(lpm = modify_depth(simulations, 2, ~lm(y ~ ., data = .x))) %&gt;% \n    mutate(lpm = modify_depth(lpm, 2, broom::tidy)) %&gt;% \n    mutate(lpm = modify_depth(lpm, 2, ~select(., term, estimate))) %&gt;% \n    mutate(lpm = modify_depth(lpm, 2, ~filter(., term != \"(Intercept)\"))) %&gt;% \n    mutate(lpm = map(lpm, bind_rows)) %&gt;% \n    mutate(true_effect = modify_depth(simulations, 2, ~meffects(., coefs = coefs[[1]]))) %&gt;% \n    mutate(true_effect = map(true_effect, bind_rows))\n\n  simulation_results &lt;- results %&gt;% \n    mutate(difference = map2(.x = lpm, .y = true_effect, bind_cols)) %&gt;% \n    mutate(difference = map(difference, ~mutate(., difference = true_effect - estimate))) %&gt;% \n    mutate(difference = map(difference, ~select(., term, difference))) %&gt;% \n    pull(difference) %&gt;% \n    .[[1]]\n\n  simulation_results %&gt;% \n    group_by(term) %&gt;% \n    summarise(mean = mean(difference), \n              sd = sd(difference))\n}\n\nAnd now, let’s run the simulation for different parameters and sizes:\n\nmonte_carlo(c(.5, 2, 4), 100, 10)\n## # A tibble: 2 x 3\n##   term      mean     sd\n##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n## 1 x1    -0.00826 0.0291\n## 2 x2    -0.00732 0.0412\nmonte_carlo(c(.5, 2, 4), 100, 100)\n## # A tibble: 2 x 3\n##   term     mean     sd\n##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n## 1 x1    0.00360 0.0392\n## 2 x2    0.00517 0.0446\nmonte_carlo(c(.5, 2, 4), 100, 500)\n## # A tibble: 2 x 3\n##   term       mean     sd\n##   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n## 1 x1    -0.00152  0.0371\n## 2 x2    -0.000701 0.0423\nmonte_carlo(c(pi, 6, 9), 100, 10)\n## # A tibble: 2 x 3\n##   term      mean     sd\n##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n## 1 x1    -0.00829 0.0546\n## 2 x2     0.00178 0.0370\nmonte_carlo(c(pi, 6, 9), 100, 100)\n## # A tibble: 2 x 3\n##   term     mean     sd\n##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n## 1 x1    0.0107  0.0608\n## 2 x2    0.00831 0.0804\nmonte_carlo(c(pi, 6, 9), 100, 500)\n## # A tibble: 2 x 3\n##   term     mean     sd\n##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n## 1 x1    0.00879 0.0522\n## 2 x2    0.0113  0.0668\n\nWe see that, at least for this set of parameters, the LPM does a good job of estimating marginal effects.\n\n\nNow, this study might in itself not be very interesting to you, but I believe the general approach is quite useful and flexible enough to be adapted to all kinds of use-cases."
  },
  {
    "objectID": "posts/2016-11-04-ive-started-writing-a-book-functional-programming-and-unit-testing-for-data-munging-with-r.html",
    "href": "posts/2016-11-04-ive-started-writing-a-book-functional-programming-and-unit-testing-for-data-munging-with-r.html",
    "title": "I’ve started writing a ‘book’: Functional programming and unit testing for data munging with R",
    "section": "",
    "text": "I have started writing a ‘book’ using the awesome bookdown package. In the book I explain and show why using functional programming and putting your functions in your own packages is the way to go when you want to clean, prepare and transform large data sets. It makes testing and documenting your code easier. You don’t need to think about managing paths either. The book is far from complete, but I plan on working on it steadily. For now, you can read an intro to functional programming, unit testing and creating your own packages that will hold your code. I also show you can write documentation for your functions. I am also looking for feedback; so if you have any suggestions, do not hesitate to shoot me an email or a tweet! You can read the book by clicking here."
  },
  {
    "objectID": "posts/2019-02-10-stringr_package.html",
    "href": "posts/2019-02-10-stringr_package.html",
    "title": "Manipulating strings with the {stringr} package",
    "section": "",
    "text": "This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free here. This is taken from Chapter 4, in which I introduce the {stringr} package."
  },
  {
    "objectID": "posts/2019-02-10-stringr_package.html#manipulate-strings-with-stringr",
    "href": "posts/2019-02-10-stringr_package.html#manipulate-strings-with-stringr",
    "title": "Manipulating strings with the {stringr} package",
    "section": "\nManipulate strings with {stringr}\n",
    "text": "Manipulate strings with {stringr}\n\n\n{stringr} contains functions to manipulate strings. In Chapter 10, I will teach you about regular expressions, but the functions contained in {stringr} allow you to already do a lot of work on strings, without needing to be a regular expression expert.\n\n\nI will discuss the most common string operations: detecting, locating, matching, searching and replacing, and exctracting/removing strings.\n\n\nTo introduce these operations, let us use an ALTO file of an issue of The Winchester News from October 31, 1910, which you can find on this link (to see how the newspaper looked like, click here). I re-hosted the file on a public gist for archiving purposes. While working on the book, the original site went down several times…\n\n\nALTO is an XML schema for the description of text OCR and layout information of pages for digitzed material, such as newspapers (source: ALTO Wikipedia page). For more details, you can read my blogpost on the matter, but for our current purposes, it is enough to know that the file contains the text of newspaper articles. The file looks like this:\n\n&lt;TextLine HEIGHT=\"138.0\" WIDTH=\"2434.0\" HPOS=\"4056.0\" VPOS=\"5814.0\"&gt;\n&lt;String STYLEREFS=\"ID7\" HEIGHT=\"108.0\" WIDTH=\"393.0\" HPOS=\"4056.0\" VPOS=\"5838.0\" CONTENT=\"timore\" WC=\"0.82539684\"&gt;\n&lt;ALTERNATIVE&gt;timole&lt;/ALTERNATIVE&gt;\n&lt;ALTERNATIVE&gt;tlnldre&lt;/ALTERNATIVE&gt;\n&lt;ALTERNATIVE&gt;timor&lt;/ALTERNATIVE&gt;\n&lt;ALTERNATIVE&gt;insole&lt;/ALTERNATIVE&gt;\n&lt;ALTERNATIVE&gt;landed&lt;/ALTERNATIVE&gt;\n&lt;/String&gt;\n&lt;SP WIDTH=\"74.0\" HPOS=\"4449.0\" VPOS=\"5838.0\"/&gt;\n&lt;String STYLEREFS=\"ID7\" HEIGHT=\"105.0\" WIDTH=\"432.0\" HPOS=\"4524.0\" VPOS=\"5847.0\" CONTENT=\"market\" WC=\"0.95238096\"/&gt;\n&lt;SP WIDTH=\"116.0\" HPOS=\"4956.0\" VPOS=\"5847.0\"/&gt;\n&lt;String STYLEREFS=\"ID7\" HEIGHT=\"69.0\" WIDTH=\"138.0\" HPOS=\"5073.0\" VPOS=\"5883.0\" CONTENT=\"as\" WC=\"0.96825397\"/&gt;\n&lt;SP WIDTH=\"74.0\" HPOS=\"5211.0\" VPOS=\"5883.0\"/&gt;\n&lt;String STYLEREFS=\"ID7\" HEIGHT=\"69.0\" WIDTH=\"285.0\" HPOS=\"5286.0\" VPOS=\"5877.0\" CONTENT=\"were\" WC=\"1.0\"&gt;\n&lt;ALTERNATIVE&gt;verc&lt;/ALTERNATIVE&gt;\n&lt;ALTERNATIVE&gt;veer&lt;/ALTERNATIVE&gt;\n&lt;/String&gt;\n&lt;SP WIDTH=\"68.0\" HPOS=\"5571.0\" VPOS=\"5877.0\"/&gt;\n&lt;String STYLEREFS=\"ID7\" HEIGHT=\"111.0\" WIDTH=\"147.0\" HPOS=\"5640.0\" VPOS=\"5838.0\" CONTENT=\"all\" WC=\"1.0\"/&gt;\n&lt;SP WIDTH=\"83.0\" HPOS=\"5787.0\" VPOS=\"5838.0\"/&gt;\n&lt;String STYLEREFS=\"ID7\" HEIGHT=\"111.0\" WIDTH=\"183.0\" HPOS=\"5871.0\" VPOS=\"5835.0\" CONTENT=\"the\" WC=\"0.95238096\"&gt;\n&lt;ALTERNATIVE&gt;tll&lt;/ALTERNATIVE&gt;\n&lt;ALTERNATIVE&gt;Cu&lt;/ALTERNATIVE&gt;\n&lt;ALTERNATIVE&gt;tall&lt;/ALTERNATIVE&gt;\n&lt;/String&gt;\n&lt;SP WIDTH=\"75.0\" HPOS=\"6054.0\" VPOS=\"5835.0\"/&gt;\n&lt;String STYLEREFS=\"ID3\" HEIGHT=\"132.0\" WIDTH=\"351.0\" HPOS=\"6129.0\" VPOS=\"5814.0\" CONTENT=\"cattle\" WC=\"0.95238096\"/&gt;\n&lt;/TextLine&gt;\n\nWe are interested in the strings after CONTENT=. We are going to use functions from the {stringr} package to get the strings after CONTENT=. In Chapter 10, we are going to explore this file again, but using complex regular expressions to get all the content in one go.\n\n\n\nGetting text data into Rstudio\n\n\nFirst of all, let us read in the file:\n\nwinchester &lt;- read_lines(\"https://gist.githubusercontent.com/b-rodrigues/5139560e7d0f2ecebe5da1df3629e015/raw/e3031d894ffb97217ddbad1ade1b307c9937d2c8/gistfile1.txt\")\n\nEven though the file is an XML file, I still read it in using read_lines() and not read_xml() from the {xml2} package. This is for the purposes of the current exercise, and also because I always have trouble with XML files, and prefer to treat them as simple text files, and use regular expressions to get what I need.\n\n\nNow that the ALTO file is read in and saved in the winchester variable, you might want to print the whole thing in the console. Before that, take a look at the structure:\n\nstr(winchester)\n##  chr [1:43] \"\" ...\n\nSo the winchester variable is a character atomic vector with 43 elements. So first, we need to understand what these elements are. Let’s start with the first one:\n\nwinchester[1]\n## [1] \"\"\n\nOk, so it seems like the first element is part of the header of the file. What about the second one?\n\nwinchester[2]\n## [1] \"&lt;meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=UTF-8\\\"&gt;&lt;base href=\\\"https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml\\\"&gt;&lt;style&gt;body{margin-left:0;margin-right:0;margin-top:0}#bN015htcoyT__google-cache-hdr{background:#f5f5f5;font:13px arial,sans-serif;text-align:left;color:#202020;border:0;margin:0;border-bottom:1px solid #cecece;line-height:16px;padding:16px 28px 24px 28px}#bN015htcoyT__google-cache-hdr *{display:inline;font:inherit;text-align:inherit;color:inherit;line-height:inherit;background:none;border:0;margin:0;padding:0;letter-spacing:0}#bN015htcoyT__google-cache-hdr a{text-decoration:none;color:#1a0dab}#bN015htcoyT__google-cache-hdr a:hover{text-decoration:underline}#bN015htcoyT__google-cache-hdr a:visited{color:#609}#bN015htcoyT__google-cache-hdr div{display:block;margin-top:4px}#bN015htcoyT__google-cache-hdr b{font-weight:bold;display:inline-block;direction:ltr}&lt;/style&gt;&lt;div id=\\\"bN015htcoyT__google-cache-hdr\\\"&gt;&lt;div&gt;&lt;span&gt;This is Google's cache of &lt;a href=\\\"https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml\\\"&gt;https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml&lt;/a&gt;.&lt;/span&gt;&nbsp;&lt;span&gt;It is a snapshot of the page as it appeared on 21 Jan 2019 05:18:18 GMT.&lt;/span&gt;&nbsp;&lt;span&gt;The &lt;a href=\\\"https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml\\\"&gt;current page&lt;/a&gt; could have changed in the meantime.&lt;/span&gt;&nbsp;&lt;a href=\\\"http://support.google.com/websearch/bin/answer.py?hl=en&amp;p=cached&amp;answer=1687222\\\"&gt;&lt;span&gt;Learn more&lt;/span&gt;.&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=\\\"display:inline-block;margin-top:8px;margin-right:104px;white-space:nowrap\\\"&gt;&lt;span style=\\\"margin-right:28px\\\"&gt;&lt;span style=\\\"font-weight:bold\\\"&gt;Full version&lt;/span&gt;&lt;/span&gt;&lt;span style=\\\"margin-right:28px\\\"&gt;&lt;a href=\\\"http://webcache.googleusercontent.com/search?q=cache:2BVPV8QGj3oJ:https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml&amp;hl=en&amp;gl=lu&amp;strip=1&amp;vwsrc=0\\\"&gt;&lt;span&gt;Text-only version&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;span style=\\\"margin-right:28px\\\"&gt;&lt;a href=\\\"http://webcache.googleusercontent.com/search?q=cache:2BVPV8QGj3oJ:https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml&amp;hl=en&amp;gl=lu&amp;strip=0&amp;vwsrc=1\\\"&gt;&lt;span&gt;View source&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;span style=\\\"display:inline-block;margin-top:8px;color:#717171\\\"&gt;&lt;span&gt;Tip: To quickly find your search term on this page, press &lt;b&gt;Ctrl+F&lt;/b&gt; or &lt;b&gt;⌘-F&lt;/b&gt; (Mac) and use the find bar.&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div style=\\\"position:relative;\\\"&gt;&lt;?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?&gt;\"\n\nSame. So where is the content? The file is very large, so if you print it in the console, it will take quite some time to print, and you will not really be able to make out anything. The best way would be to try to detect the string CONTENT and work from there.\n\n\n\n\nDetecting, getting the position and locating strings\n\n\nWhen confronted to an atomic vector of strings, you might want to know inside which elements you can find certain strings. For example, to know which elements of winchester contain the string CONTENT, use str_detect():\n\nwinchester %&gt;%\n  str_detect(\"CONTENT\")\n##  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [23] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [34] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n\nThis returns a boolean atomic vector of the same length as winchester. If the string CONTENT is nowhere to be found, the result will equal FALSE, if not it will equal TRUE. Here it is easy to see that the last element contains the string CONTENT. But what if instead of having 43 elements, the vector had 24192 elements? And hundreds would contain the string CONTENT? It would be easier to instead have the indices of the vector where one can find the word CONTENT. This is possible with str_which():\n\nwinchester %&gt;%\n  str_which(\"CONTENT\")\n## [1] 43\n\nHere, the result is 43, meaning that the 43rd element of winchester contains the string CONTENT somewhere. If we need more precision, we can use str_locate() and str_locate_all(). To explain how both these functions work, let’s create a very small example:\n\nancient_philosophers &lt;- c(\"aristotle\", \"plato\", \"epictetus\", \"seneca the younger\", \"epicurus\", \"marcus aurelius\")\n\nNow suppose I am interested in philosophers whose name ends in us. Let us use str_locate() first:\n\nancient_philosophers %&gt;%\n  str_locate(\"us\")\n##      start end\n## [1,]    NA  NA\n## [2,]    NA  NA\n## [3,]     8   9\n## [4,]    NA  NA\n## [5,]     7   8\n## [6,]     5   6\n\nYou can interpret the result as follows: in the rows, the index of the vector where the string us is found. So the 3rd, 5th and 6th philosopher have us somewhere in their name. The result also has two columns: start and end. These give the position of the string. So the string us can be found starting at position 8 of the 3rd element of the vector, and ends at position 9. Same goes for the other philisophers. However, consider Marcus Aurelius. He has two names, both ending with us. However, str_locate() only shows the position of the us in Marcus.\n\n\nTo get both us strings, you need to use str_locate_all():\n\nancient_philosophers %&gt;%\n  str_locate_all(\"us\")\n## [[1]]\n##      start end\n## \n## [[2]]\n##      start end\n## \n## [[3]]\n##      start end\n## [1,]     8   9\n## \n## [[4]]\n##      start end\n## \n## [[5]]\n##      start end\n## [1,]     7   8\n## \n## [[6]]\n##      start end\n## [1,]     5   6\n## [2,]    14  15\n\nNow we get the position of the two us in Marcus Aurelius. Doing this on the winchester vector will give use the position of the CONTENT string, but this is not really important right now. What matters is that you know how str_locate() and str_locate_all() work.\n\n\nSo now that we know what interests us in the 43nd element of winchester, let’s take a closer look at it:\n\nwinchester[43]\n\nAs you can see, it’s a mess:\n\n&lt;TextLine HEIGHT=\\\"126.0\\\" WIDTH=\\\"1731.0\\\" HPOS=\\\"17160.0\\\" VPOS=\\\"21252.0\\\"&gt;&lt;String HEIGHT=\\\"114.0\\\" WIDTH=\\\"354.0\\\" HPOS=\\\"17160.0\\\" VPOS=\\\"21264.0\\\" CONTENT=\\\"0tV\\\" WC=\\\"0.8095238\\\"/&gt;&lt;SP WIDTH=\\\"131.0\\\" HPOS=\\\"17514.0\\\" VPOS=\\\"21264.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"111.0\\\" WIDTH=\\\"474.0\\\" HPOS=\\\"17646.0\\\" VPOS=\\\"21258.0\\\" CONTENT=\\\"BATES\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"140.0\\\" HPOS=\\\"18120.0\\\" VPOS=\\\"21258.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"114.0\\\" WIDTH=\\\"630.0\\\" HPOS=\\\"18261.0\\\" VPOS=\\\"21252.0\\\" CONTENT=\\\"President\\\" WC=\\\"1.0\\\"&gt;&lt;ALTERNATIVE&gt;Prcideht&lt;/ALTERNATIVE&gt;&lt;ALTERNATIVE&gt;Pride&lt;/ALTERNATIVE&gt;&lt;/String&gt;&lt;/TextLine&gt;&lt;TextLine HEIGHT=\\\"153.0\\\" WIDTH=\\\"1689.0\\\" HPOS=\\\"17145.0\\\" VPOS=\\\"21417.0\\\"&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"105.0\\\" WIDTH=\\\"258.0\\\" HPOS=\\\"17145.0\\\" VPOS=\\\"21439.0\\\" CONTENT=\\\"WM\\\" WC=\\\"0.82539684\\\"&gt;&lt;TextLine HEIGHT=\\\"120.0\\\" WIDTH=\\\"2211.0\\\" HPOS=\\\"16788.0\\\" VPOS=\\\"21870.0\\\"&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"96.0\\\" WIDTH=\\\"102.0\\\" HPOS=\\\"16788.0\\\" VPOS=\\\"21894.0\\\" CONTENT=\\\"It\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"72.0\\\" HPOS=\\\"16890.0\\\" VPOS=\\\"21894.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"96.0\\\" WIDTH=\\\"93.0\\\" HPOS=\\\"16962.0\\\" VPOS=\\\"21885.0\\\" CONTENT=\\\"is\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"80.0\\\" HPOS=\\\"17055.0\\\" VPOS=\\\"21885.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"102.0\\\" WIDTH=\\\"417.0\\\" HPOS=\\\"17136.0\\\" VPOS=\\\"21879.0\\\" CONTENT=\\\"seldom\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"80.0\\\" HPOS=\\\"17553.0\\\" VPOS=\\\"21879.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"96.0\\\" WIDTH=\\\"267.0\\\" HPOS=\\\"17634.0\\\" VPOS=\\\"21873.0\\\" CONTENT=\\\"hard\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"81.0\\\" HPOS=\\\"17901.0\\\" VPOS=\\\"21873.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"87.0\\\" WIDTH=\\\"111.0\\\" HPOS=\\\"17982.0\\\" VPOS=\\\"21879.0\\\" CONTENT=\\\"to\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"81.0\\\" HPOS=\\\"18093.0\\\" VPOS=\\\"21879.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"96.0\\\" WIDTH=\\\"219.0\\\" HPOS=\\\"18174.0\\\" VPOS=\\\"21870.0\\\" CONTENT=\\\"find\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"77.0\\\" HPOS=\\\"18393.0\\\" VPOS=\\\"21870.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"69.0\\\" WIDTH=\\\"66.0\\\" HPOS=\\\"18471.0\\\" VPOS=\\\"21894.0\\\" CONTENT=\\\"a\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"77.0\\\" HPOS=\\\"18537.0\\\" VPOS=\\\"21894.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"78.0\\\" WIDTH=\\\"384.0\\\" HPOS=\\\"18615.0\\\" VPOS=\\\"21888.0\\\" CONTENT=\\\"succes\\\" WC=\\\"0.82539684\\\"&gt;&lt;ALTERNATIVE&gt;success&lt;/ALTERNATIVE&gt;&lt;/String&gt;&lt;/TextLine&gt;&lt;TextLine HEIGHT=\\\"126.0\\\" WIDTH=\\\"2316.0\\\" HPOS=\\\"16662.0\\\" VPOS=\\\"22008.0\\\"&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"75.0\\\" WIDTH=\\\"183.0\\\" HPOS=\\\"16662.0\\\" VPOS=\\\"22059.0\\\" CONTENT=\\\"sor\\\" WC=\\\"1.0\\\"&gt;&lt;ALTERNATIVE&gt;soar&lt;/ALTERNATIVE&gt;&lt;/String&gt;&lt;SP WIDTH=\\\"72.0\\\" HPOS=\\\"16845.0\\\" VPOS=\\\"22059.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"90.0\\\" WIDTH=\\\"168.0\\\" HPOS=\\\"16917.0\\\" VPOS=\\\"22035.0\\\" CONTENT=\\\"for\\\" WC=\\\"1.0\\\"/&gt;&lt;SP WIDTH=\\\"72.0\\\" HPOS=\\\"17085.0\\\" VPOS=\\\"22035.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"69.0\\\" WIDTH=\\\"267.0\\\" HPOS=\\\"17157.0\\\" VPOS=\\\"22050.0\\\" CONTENT=\\\"even\\\" WC=\\\"1.0\\\"&gt;&lt;ALTERNATIVE&gt;cen&lt;/ALTERNATIVE&gt;&lt;ALTERNATIVE&gt;cent&lt;/ALTERNATIVE&gt;&lt;/String&gt;&lt;SP WIDTH=\\\"77.0\\\" HPOS=\\\"17434.0\\\" VPOS=\\\"22050.0\\\"/&gt;&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"66.0\\\" WIDTH=\\\"63.0\\\" HPOS=\\\"17502.0\\\" VPOS=\\\"22044.0\\\"\n\nThe file was imported without any newlines. So we need to insert them ourselves, by splitting the string in a clever way.\n\n\n\n\nSplitting strings\n\n\nThere are two functions included in {stringr} to split strings, str_split() and str_split_fixed(). Let’s go back to our ancient philosophers. Two of them, Seneca the Younger and Marcus Aurelius have something else in common than both being Roman Stoic philosophers. Their names are composed of several words. If we want to split their names at the space character, we can use str_split() like this:\n\nancient_philosophers %&gt;%\n  str_split(\" \")\n## [[1]]\n## [1] \"aristotle\"\n## \n## [[2]]\n## [1] \"plato\"\n## \n## [[3]]\n## [1] \"epictetus\"\n## \n## [[4]]\n## [1] \"seneca\"  \"the\"     \"younger\"\n## \n## [[5]]\n## [1] \"epicurus\"\n## \n## [[6]]\n## [1] \"marcus\"   \"aurelius\"\n\nstr_split() also has a simplify = TRUE option:\n\nancient_philosophers %&gt;%\n  str_split(\" \", simplify = TRUE)\n##      [,1]        [,2]       [,3]     \n## [1,] \"aristotle\" \"\"         \"\"       \n## [2,] \"plato\"     \"\"         \"\"       \n## [3,] \"epictetus\" \"\"         \"\"       \n## [4,] \"seneca\"    \"the\"      \"younger\"\n## [5,] \"epicurus\"  \"\"         \"\"       \n## [6,] \"marcus\"    \"aurelius\" \"\"\n\nThis time, the returned object is a matrix.\n\n\nWhat about str_split_fixed()? The difference is that here you can specify the number of pieces to return. For example, you could consider the name “Aurelius” to be the middle name of Marcus Aurelius, and the “the younger” to be the middle name of Seneca the younger. This means that you would want to split the name only at the first space character, and not at all of them. This is easily achieved with str_split_fixed():\n\nancient_philosophers %&gt;%\n  str_split_fixed(\" \", 2)\n##      [,1]        [,2]         \n## [1,] \"aristotle\" \"\"           \n## [2,] \"plato\"     \"\"           \n## [3,] \"epictetus\" \"\"           \n## [4,] \"seneca\"    \"the younger\"\n## [5,] \"epicurus\"  \"\"           \n## [6,] \"marcus\"    \"aurelius\"\n\nThis gives the expected result.\n\n\nSo how does this help in our case? Well, if you look at how the ALTO file looks like, at the beginning of this section, you will notice that every line ends with the “&gt;” character. So let’s split at that character!\n\nwinchester_text &lt;- winchester[43] %&gt;%\n  str_split(\"&gt;\")\n\nLet’s take a closer look at winchester_text:\n\nstr(winchester_text)\n## List of 1\n##  $ : chr [1:19706] \"&lt;/processingStepSettings\" \"&lt;processingSoftware\" \"&lt;softwareCreator\" \"iArchives&lt;/softwareCreator\" ...\n\nSo this is a list of length one, and the first, and only, element of that list is an atomic vector with 19706 elements. Since this is a list of only one element, we can simplify it by saving the atomic vector in a variable:\n\nwinchester_text &lt;- winchester_text[[1]]\n\nLet’s now look at some lines:\n\nwinchester_text[1232:1245]\n##  [1] \"&lt;SP WIDTH=\\\"66.0\\\" HPOS=\\\"5763.0\\\" VPOS=\\\"9696.0\\\"/\"                                                                         \n##  [2] \"&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"108.0\\\" WIDTH=\\\"612.0\\\" HPOS=\\\"5829.0\\\" VPOS=\\\"9693.0\\\" CONTENT=\\\"Louisville\\\" WC=\\\"1.0\\\"\"\n##  [3] \"&lt;ALTERNATIVE\"                                                                                                                \n##  [4] \"Loniile&lt;/ALTERNATIVE\"                                                                                                        \n##  [5] \"&lt;ALTERNATIVE\"                                                                                                                \n##  [6] \"Lenities&lt;/ALTERNATIVE\"                                                                                                       \n##  [7] \"&lt;/String\"                                                                                                                    \n##  [8] \"&lt;/TextLine\"                                                                                                                  \n##  [9] \"&lt;TextLine HEIGHT=\\\"150.0\\\" WIDTH=\\\"2520.0\\\" HPOS=\\\"4032.0\\\" VPOS=\\\"9849.0\\\"\"                                                 \n## [10] \"&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"108.0\\\" WIDTH=\\\"510.0\\\" HPOS=\\\"4032.0\\\" VPOS=\\\"9861.0\\\" CONTENT=\\\"Tobacco\\\" WC=\\\"1.0\\\"/\"  \n## [11] \"&lt;SP WIDTH=\\\"113.0\\\" HPOS=\\\"4542.0\\\" VPOS=\\\"9861.0\\\"/\"                                                                        \n## [12] \"&lt;String STYLEREFS=\\\"ID7\\\" HEIGHT=\\\"105.0\\\" WIDTH=\\\"696.0\\\" HPOS=\\\"4656.0\\\" VPOS=\\\"9861.0\\\" CONTENT=\\\"Warehouse\\\" WC=\\\"1.0\\\"\" \n## [13] \"&lt;ALTERNATIVE\"                                                                                                                \n## [14] \"WHrchons&lt;/ALTERNATIVE\"\n\nThis now looks easier to handle. We can narrow it down to the lines that only contain the string we are interested in, “CONTENT”. First, let’s get the indices:\n\ncontent_winchester_index &lt;- winchester_text %&gt;%\n  str_which(\"CONTENT\")\n\nHow many lines contain the string “CONTENT”?\n\nlength(content_winchester_index)\n## [1] 4462\n\nAs you can see, this reduces the amount of data we have to work with. Let us save this is a new variable:\n\ncontent_winchester &lt;- winchester_text[content_winchester_index]\n\n\n\nMatching strings\n\n\nMatching strings is useful, but only in combination with regular expressions. As stated at the beginning of this section, we are going to learn about regular expressions in Chapter 10, but in order to make this section useful, we are going to learn the easiest, but perhaps the most useful regular expression: .*.\n\n\nLet’s go back to our ancient philosophers, and use str_match() and see what happens. Let’s match the “us” string:\n\nancient_philosophers %&gt;%\n  str_match(\"us\")\n##      [,1]\n## [1,] NA  \n## [2,] NA  \n## [3,] \"us\"\n## [4,] NA  \n## [5,] \"us\"\n## [6,] \"us\"\n\nNot very useful, but what about the regular expression .*? How could it help?\n\nancient_philosophers %&gt;%\n  str_match(\".*us\")\n##      [,1]             \n## [1,] NA               \n## [2,] NA               \n## [3,] \"epictetus\"      \n## [4,] NA               \n## [5,] \"epicurus\"       \n## [6,] \"marcus aurelius\"\n\nThat’s already very interesting! So how does .* work? To understand, let’s first start by using . alone:\n\nancient_philosophers %&gt;%\n  str_match(\".us\")\n##      [,1] \n## [1,] NA   \n## [2,] NA   \n## [3,] \"tus\"\n## [4,] NA   \n## [5,] \"rus\"\n## [6,] \"cus\"\n\nThis also matched whatever symbol comes just before the “u” from “us”. What if we use two . instead?\n\nancient_philosophers %&gt;%\n  str_match(\"..us\")\n##      [,1]  \n## [1,] NA    \n## [2,] NA    \n## [3,] \"etus\"\n## [4,] NA    \n## [5,] \"urus\"\n## [6,] \"rcus\"\n\nThis time, we get the two symbols that immediately precede “us”. Instead of continuing like this we now use the , which matches zero or more of .. So by combining  and ., we can match any symbol repeatedly, until there is nothing more to match. Note that there is also +, which works similarly to *, but it matches one or more symbols.\n\n\nThere is also a str_match_all():\n\nancient_philosophers %&gt;%\n  str_match_all(\".*us\")\n## [[1]]\n##      [,1]\n## \n## [[2]]\n##      [,1]\n## \n## [[3]]\n##      [,1]       \n## [1,] \"epictetus\"\n## \n## [[4]]\n##      [,1]\n## \n## [[5]]\n##      [,1]      \n## [1,] \"epicurus\"\n## \n## [[6]]\n##      [,1]             \n## [1,] \"marcus aurelius\"\n\nIn this particular case it does not change the end result, but keep it in mind for cases like this one:\n\nc(\"haha\", \"huhu\") %&gt;%\n  str_match(\"ha\")\n##      [,1]\n## [1,] \"ha\"\n## [2,] NA\n\nand:\n\nc(\"haha\", \"huhu\") %&gt;%\n  str_match_all(\"ha\")\n## [[1]]\n##      [,1]\n## [1,] \"ha\"\n## [2,] \"ha\"\n## \n## [[2]]\n##      [,1]\n\nWhat if we want to match names containing the letter “t”? Easy:\n\nancient_philosophers %&gt;%\n  str_match(\".*t.*\")\n##      [,1]                \n## [1,] \"aristotle\"         \n## [2,] \"plato\"             \n## [3,] \"epictetus\"         \n## [4,] \"seneca the younger\"\n## [5,] NA                  \n## [6,] NA\n\nSo how does this help us with our historical newspaper? Let’s try to get the strings that come after “CONTENT”:\n\nwinchester_content &lt;- winchester_text %&gt;%\n  str_match(\"CONTENT.*\")\n\nLet’s use our faithful str() function to take a look:\n\nwinchester_content %&gt;%\n  str\n##  chr [1:19706, 1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ...\n\nHum, there’s a lot of NA values! This is because a lot of the lines from the file did not have the string “CONTENT”, so there is no match possible. Let’s us remove all these NAs. Because the result is a matrix, we cannot use the filter() function from {dplyr}. So we need to convert it to a tibble first:\n\nwinchester_content &lt;- winchester_content %&gt;%\n  as.tibble() %&gt;%\n  filter(!is.na(V1))\n## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics).\n## This warning is displayed once per session.\n\nBecause matrix columns do not have names, when a matrix gets converted into a tibble, the firt column gets automatically called V1. This is why I filter on this column. Let’s take a look at the data:\n\nhead(winchester_content)\n## # A tibble: 6 x 1\n##   V1                                  \n##   &lt;chr&gt;                               \n## 1 \"CONTENT=\\\"J\\\" WC=\\\"0.8095238\\\"/\"   \n## 2 \"CONTENT=\\\"a\\\" WC=\\\"0.8095238\\\"/\"   \n## 3 \"CONTENT=\\\"Ira\\\" WC=\\\"0.95238096\\\"/\"\n## 4 \"CONTENT=\\\"mj\\\" WC=\\\"0.8095238\\\"/\"  \n## 5 \"CONTENT=\\\"iI\\\" WC=\\\"0.8095238\\\"/\"  \n## 6 \"CONTENT=\\\"tE1r\\\" WC=\\\"0.8095238\\\"/\"\n\n\n\nSearching and replacing strings\n\n\nWe are getting close to the final result. We still need to do some cleaning however. Since our data is inside a nice tibble, we might as well stick with it. So let’s first rename the column and change all the strings to lowercase:\n\nwinchester_content &lt;- winchester_content %&gt;% \n  mutate(content = tolower(V1)) %&gt;% \n  select(-V1)\n\nLet’s take a look at the result:\n\nhead(winchester_content)\n## # A tibble: 6 x 1\n##   content                             \n##   &lt;chr&gt;                               \n## 1 \"content=\\\"j\\\" wc=\\\"0.8095238\\\"/\"   \n## 2 \"content=\\\"a\\\" wc=\\\"0.8095238\\\"/\"   \n## 3 \"content=\\\"ira\\\" wc=\\\"0.95238096\\\"/\"\n## 4 \"content=\\\"mj\\\" wc=\\\"0.8095238\\\"/\"  \n## 5 \"content=\\\"ii\\\" wc=\\\"0.8095238\\\"/\"  \n## 6 \"content=\\\"te1r\\\" wc=\\\"0.8095238\\\"/\"\n\nThe second part of the string, “wc=….” is not really interesting. Let’s search and replace this with an empty string, using str_replace():\n\nwinchester_content &lt;- winchester_content %&gt;% \n  mutate(content = str_replace(content, \"wc.*\", \"\"))\n\nhead(winchester_content)\n## # A tibble: 6 x 1\n##   content            \n##   &lt;chr&gt;              \n## 1 \"content=\\\"j\\\" \"   \n## 2 \"content=\\\"a\\\" \"   \n## 3 \"content=\\\"ira\\\" \" \n## 4 \"content=\\\"mj\\\" \"  \n## 5 \"content=\\\"ii\\\" \"  \n## 6 \"content=\\\"te1r\\\" \"\n\nWe need to use the regular expression from before to replace “wc” and every character that follows. The same can be use to remove “content=”:\n\nwinchester_content &lt;- winchester_content %&gt;% \n  mutate(content = str_replace(content, \"content=\", \"\"))\n\nhead(winchester_content)\n## # A tibble: 6 x 1\n##   content    \n##   &lt;chr&gt;      \n## 1 \"\\\"j\\\" \"   \n## 2 \"\\\"a\\\" \"   \n## 3 \"\\\"ira\\\" \" \n## 4 \"\\\"mj\\\" \"  \n## 5 \"\\\"ii\\\" \"  \n## 6 \"\\\"te1r\\\" \"\n\nWe are almost done, but some cleaning is still necessary:\n\n\n\n\nExctracting or removing strings\n\n\nNow, because I now the ALTO spec, I know how to find words that are split between two sentences:\n\nwinchester_content %&gt;% \n  filter(str_detect(content, \"hyppart\"))\n## # A tibble: 64 x 1\n##    content                                                               \n##    &lt;chr&gt;                                                                 \n##  1 \"\\\"aver\\\" subs_type=\\\"hyppart1\\\" subs_content=\\\"average\\\" \"           \n##  2 \"\\\"age\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"average\\\" \"            \n##  3 \"\\\"considera\\\" subs_type=\\\"hyppart1\\\" subs_content=\\\"consideration\\\" \"\n##  4 \"\\\"tion\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"consideration\\\" \"     \n##  5 \"\\\"re\\\" subs_type=\\\"hyppart1\\\" subs_content=\\\"resigned\\\" \"            \n##  6 \"\\\"signed\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"resigned\\\" \"        \n##  7 \"\\\"install\\\" subs_type=\\\"hyppart1\\\" subs_content=\\\"installed\\\" \"      \n##  8 \"\\\"ed\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"installed\\\" \"           \n##  9 \"\\\"be\\\" subs_type=\\\"hyppart1\\\" subs_content=\\\"before\\\" \"              \n## 10 \"\\\"fore\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"before\\\" \"            \n## # … with 54 more rows\n\nFor instance, the word “average” was split over two lines, the first part of the word, “aver” on the first line, and the second part of the word, “age”, on the second line. We want to keep what comes after “subs_content”. Let’s extract the word “average” using str_extract(). However, because only some words were split between two lines, we first need to detect where the string “hyppart1” is located, and only then can we extract what comes after “subs_content”. Thus, we need to combine str_detect() to first detect the string, and then str_extract() to extract what comes after “subs_content”:\n\nwinchester_content &lt;- winchester_content %&gt;% \n  mutate(content = if_else(str_detect(content, \"hyppart1\"), \n                           str_extract_all(content, \"content=.*\", simplify = TRUE), \n                           content))\n\nLet’s take a look at the result:\n\nwinchester_content %&gt;% \n  filter(str_detect(content, \"content\"))\n## # A tibble: 64 x 1\n##    content                                                          \n##    &lt;chr&gt;                                                            \n##  1 \"content=\\\"average\\\" \"                                           \n##  2 \"\\\"age\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"average\\\" \"       \n##  3 \"content=\\\"consideration\\\" \"                                     \n##  4 \"\\\"tion\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"consideration\\\" \"\n##  5 \"content=\\\"resigned\\\" \"                                          \n##  6 \"\\\"signed\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"resigned\\\" \"   \n##  7 \"content=\\\"installed\\\" \"                                         \n##  8 \"\\\"ed\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"installed\\\" \"      \n##  9 \"content=\\\"before\\\" \"                                            \n## 10 \"\\\"fore\\\" subs_type=\\\"hyppart2\\\" subs_content=\\\"before\\\" \"       \n## # … with 54 more rows\n\nWe still need to get rid of the string “content=” and then of all the strings that contain “hyppart2”, which are not needed now:\n\nwinchester_content &lt;- winchester_content %&gt;% \n  mutate(content = str_replace(content, \"content=\", \"\")) %&gt;% \n  mutate(content = if_else(str_detect(content, \"hyppart2\"), NA_character_, content))\n\nhead(winchester_content)\n## # A tibble: 6 x 1\n##   content    \n##   &lt;chr&gt;      \n## 1 \"\\\"j\\\" \"   \n## 2 \"\\\"a\\\" \"   \n## 3 \"\\\"ira\\\" \" \n## 4 \"\\\"mj\\\" \"  \n## 5 \"\\\"ii\\\" \"  \n## 6 \"\\\"te1r\\\" \"\n\nAlmost done! We only need to remove the “ characters:\n\nwinchester_content &lt;- winchester_content %&gt;% \n  mutate(content = str_replace_all(content, \"\\\"\", \"\")) \n\nhead(winchester_content)\n## # A tibble: 6 x 1\n##   content\n##   &lt;chr&gt;  \n## 1 \"j \"   \n## 2 \"a \"   \n## 3 \"ira \" \n## 4 \"mj \"  \n## 5 \"ii \"  \n## 6 \"te1r \"\n\nLet’s remove space characters with str_trim():\n\nwinchester_content &lt;- winchester_content %&gt;% \n  mutate(content = str_trim(content)) \n\nhead(winchester_content)\n## # A tibble: 6 x 1\n##   content\n##   &lt;chr&gt;  \n## 1 j      \n## 2 a      \n## 3 ira    \n## 4 mj     \n## 5 ii     \n## 6 te1r\n\nTo finish off this section, let’s remove stop words (words that do not add any meaning to a sentence, such as “as”, “and”…) and words that are composed of less than 3 characters. You can find a dataset with stopwords inside the {stopwords} package:\n\nlibrary(stopwords)\n\ndata(data_stopwords_stopwordsiso)\n\neng_stopwords &lt;- tibble(\"content\" = data_stopwords_stopwordsiso$en)\n\nwinchester_content &lt;- winchester_content %&gt;% \n  anti_join(eng_stopwords) %&gt;% \n  filter(nchar(content) &gt; 3)\n## Joining, by = \"content\"\nhead(winchester_content)\n## # A tibble: 6 x 1\n##   content   \n##   &lt;chr&gt;     \n## 1 te1r      \n## 2 jilas     \n## 3 edition   \n## 4 winchester\n## 5 news      \n## 6 injuries\n\nThat’s it for this section! You now know how to work with strings, but in Chapter 10 we are going one step further by learning about regular expressions, which offer much more power."
  },
  {
    "objectID": "posts/2018-01-19-mapping_functions_with_any_cols.html",
    "href": "posts/2018-01-19-mapping_functions_with_any_cols.html",
    "title": "Mapping a list of functions to a list of datasets with a list of columns as arguments",
    "section": "",
    "text": "This week I had the opportunity to teach R at my workplace, again. This course was the “advanced R” course, and unlike the one I taught at the end of last year, I had one more day (so 3 days in total) where I could show my colleagues the joys of the tidyverse and R.\n\n\nTo finish the section on programming with R, which was the very last section of the whole 3 day course I wanted to blow their minds; I had already shown them packages from the tidyverse in the previous days, such as dplyr, purrr and stringr, among others. I taught them how to use ggplot2, broom and modelr. They also liked janitor and rio very much. I noticed that it took them a bit more time and effort for them to digest purrr::map() and purrr::reduce(), but they all seemed to see how powerful these functions were. To finish on a very high note, I showed them the ultimate purrr::map() use case.\n\n\nConsider the following; imagine you have a situation where you are working on a list of datasets. These datasets might be the same, but for different years, or for different countries, or they might be completely different datasets entirely. If you used rio::import_list() to read them into R, you will have them in a nice list. Let’s consider the following list as an example:\n\nlibrary(tidyverse)\ndata(mtcars)\ndata(iris)\n\ndata_list = list(mtcars, iris)\n\nI made the choice to have completely different datasets. Now, I would like to map some functions to the columns of these datasets. If I only worked on one, for example on mtcars, I would do something like:\n\nmy_summarise_f = function(dataset, cols, funcs){\n  dataset %&gt;%\n    summarise_at(vars(!!!cols), funs(!!!funcs))\n}\n\nAnd then I would use my function like so:\n\nmtcars %&gt;%\n  my_summarise_f(quos(mpg, drat, hp), quos(mean, sd, max))\n##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max\n## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93\n##   hp_max\n## 1    335\n\nmy_summarise_f() takes a dataset, a list of columns and a list of functions as arguments and uses tidy evaluation to apply mean(), sd(), and max() to the columns mpg, drat and hp of mtcars. That’s pretty useful, but not useful enough! Now I want to apply this to the list of datasets I defined above. For this, let’s define the list of columns I want to work on:\n\ncols_mtcars = quos(mpg, drat, hp)\ncols_iris = quos(Sepal.Length, Sepal.Width)\n\ncols_list = list(cols_mtcars, cols_iris)\n\nNow, let’s use some purrr magic to apply the functions I want to the columns I have defined in list_cols:\n\nmap2(data_list,\n     cols_list,\n     my_summarise_f, funcs = quos(mean, sd, max))\n## [[1]]\n##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max\n## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93\n##   hp_max\n## 1    335\n## \n## [[2]]\n##   Sepal.Length_mean Sepal.Width_mean Sepal.Length_sd Sepal.Width_sd\n## 1          5.843333         3.057333       0.8280661      0.4358663\n##   Sepal.Length_max Sepal.Width_max\n## 1              7.9             4.4\n\nThat’s pretty useful, but not useful enough! I want to also use different functions to different datasets!\n\n\nWell, let’s define a list of functions then:\n\nfuncs_mtcars = quos(mean, sd, max)\nfuncs_iris = quos(median, min)\n\nfuncs_list = list(funcs_mtcars, funcs_iris)\n\nBecause there is no map3(), we need to use pmap():\n\npmap(\n  list(\n    dataset = data_list,\n    cols = cols_list,\n    funcs = funcs_list\n  ),\n  my_summarise_f)\n## [[1]]\n##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max\n## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93\n##   hp_max\n## 1    335\n## \n## [[2]]\n##   Sepal.Length_median Sepal.Width_median Sepal.Length_min Sepal.Width_min\n## 1                 5.8                  3              4.3               2\n\nNow I’m satisfied! Let me tell you, this blew their minds 😄!\n\n\nTo be able to use things like that, I told them to always solve a problem for a single example, and from there, try to generalize their solution using functional programming tools found in purrr.\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2018-09-15-time_use.html",
    "href": "posts/2018-09-15-time_use.html",
    "title": "How Luxembourguish residents spend their time: a small {flexdashboard} demo using the Time use survey data",
    "section": "",
    "text": "In a previous blog post I have showed how you could use the {tidyxl} package to go from a human readable Excel Workbook to a tidy data set (or flat file, as they are also called). Some people then contributed their solutions, which is always something I really enjoy when it happens. This way, I also get to learn things!\n\n\n@expersso proposed a solution without {tidyxl}:\n\n\n\nInteresting data wrangling exercise in #rstats. My solution (without using {tidyxl}): https://t.co/VjuOoM82yX https://t.co/VsXFyowigu\n\n— Eric (@expersso) September 12, 2018\n\n\n\nBen Stenhaug also proposed a solution on his github which is simpler than my code in a lot of ways!\n\n\nUpdate: @nacnudus also contributed his own version using {unpivotr}:\n\n\n\nHere’s a version using unpivotr https://t.co/l2hy6zCuKj\n\n— Duncan Garmonsway (@nacnudus) September 15, 2018\n\n\n\nNow, it would be too bad not to further analyze this data. I’ve been wanting to play around with the {flexdashboard} package for some time now, but never really got the opportunity to do so. The opportunity has now arrived. Using the cleaned data from the last post, I will further tweak it a little bit, and then produce a very simple dashboard using {flexdashboard}.\n\n\nIf you want to skip the rest of the blog post and go directly to the dashboard, just click here.\n\n\nTo make the data useful, I need to convert the strings that represent the amount of time spent doing a task (for example “1:23”) to minutes. For this I use the {chron} package:\n\nclean_data &lt;- clean_data %&gt;%\n    mutate(time_in_minutes = paste0(time, \":00\")) %&gt;% # I need to add \":00\" for the seconds else it won't work\n    mutate(time_in_minutes = \n               chron::hours(chron::times(time_in_minutes)) * 60 + \n               chron::minutes(chron::times(time_in_minutes)))\n\nrio::export(clean_data, \"clean_data.csv\")\n\nNow we’re ready to go! Below is the code to build the dashboard; if you want to try, you should copy and paste the code inside a Rmd document:\n\n---\ntitle: \"Time Use Survey of Luxembourguish residents\"\noutput: \n  flexdashboard::flex_dashboard:\n    orientation: columns\n    vertical_layout: fill\nruntime: shiny\n\n---\n\n`` `{r setup, include=FALSE}\nlibrary(flexdashboard)\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(ggthemes)\n\nmain_categories &lt;- c(\"Personal care\",\n                     \"Employment\",\n                     \"Study\",\n                     \"Household and family care\",\n                     \"Voluntary work and meetings\",\n                     \"Social life and entertainment\",\n                     \"Sports and outdoor activities\",\n                     \"Hobbies and games\",\n                     \"Media\",\n                     \"Travel\")\n\ndf &lt;- read.csv(\"clean_data.csv\") %&gt;%\n    rename(Population = population) %&gt;%\n    rename(Activities = activities)\n`` `\n\nInputs {.sidebar}\n-----------------------------------------------------------------------\n\n`` `{r}\n\nselectInput(inputId = \"activitiesName\", \n            label = \"Choose an activity\", \n            choices = unique(df$Activities))\n\nselectInput(inputId = \"dayName\", \n            label = \"Choose a day\", \n            choices = unique(df$day), \n            selected = \"Year 2014_Monday til Friday\")\n\nselectInput(inputId = \"populationName\", \n            label = \"Choose a population\", \n            choices = unique(df$Population), \n            multiple = TRUE, selected = c(\"Male\", \"Female\"))\n\n`` `\n\nThe Time Use Survey (TUS) aims to measure accurately how people allocate their time across different day-to-day activities. To this end, people are asked to keep records of all their activities in a time diary. For each activity, additional information is collected about whether or not the person was alone doing it or together with other persons, where did the activity take place, etc. The main studies on time use have been conducted to calculate indicators making possible comparative analysis of quality of life within the same population or between countries. International studies care more about specific activities such as work (unpaid or not), free time, leisure, personal care (including sleep), etc.\nSource: http://statistiques.public.lu/en/surveys/espace-households/time-use/index.html\n\nLayout based on https://jjallaire.shinyapps.io/shiny-biclust/\n\nRow\n-----------------------------------------------------------------------\n\n### Minutes spent per day on certain activities\n    \n`` `{r}\ndfInput &lt;- reactive({\n        df %&gt;% filter(Activities == input$activitiesName,\n                      Population %in% input$populationName,\n                      day %in% input$dayName)\n    })\n\n    dfInput2 &lt;- reactive({\n        df %&gt;% filter(Activities %in% main_categories,\n                      Population %in% input$populationName,\n                      day %in% input$dayName)\n    })\n    \n  renderPlotly({\n\n        df1 &lt;- dfInput()\n\n        p1 &lt;- ggplot(df1, \n                     aes(x = Activities, y = time_in_minutes, fill = Population)) +\n            geom_col(position = \"dodge\") + \n            theme_minimal() + \n            xlab(\"Activities\") + \n            ylab(\"Time in minutes\") +\n            scale_fill_gdocs()\n\n        ggplotly(p1)})\n`` `\n\nRow \n-----------------------------------------------------------------------\n\n### Proportion of the day spent on main activities\n    \n`` `{r}\nrenderPlotly({\n    \n       df2 &lt;- dfInput2()\n       \n       p2 &lt;- ggplot(df2, \n                   aes(x = Population, y = time_in_minutes, fill = Activities)) +\n           geom_bar(stat=\"identity\", position=\"fill\") + \n            xlab(\"Proportion\") + \n            ylab(\"Proportion\") +\n           theme_minimal() +\n           scale_fill_gdocs()\n       \n       ggplotly(p2)\n   })\n`` `\n\nYou will see that I have defined the following atomic vector:\n\nmain_categories &lt;- c(\"Personal care\",\n                     \"Employment\",\n                     \"Study\",\n                     \"Household and family care\",\n                     \"Voluntary work and meetings\",\n                     \"Social life and entertainment\",\n                     \"Sports and outdoor activities\",\n                     \"Hobbies and games\",\n                     \"Media\",\n                     \"Travel\")\n\nIf you go back to the raw Excel file, you will see that these main categories are then split into secondary activities. The first bar plot of the dashboard does not distinguish between the main and secondary activities, whereas the second barplot only considers the main activities. I could have added another column to the data that helped distinguish whether an activity was a main or secondary one, but I was lazy. The source code of the dashboard is very simple as it uses R Markdown. To have interactivity, I’ve used Shiny to dynamically filter the data, and built the plots with {ggplot2}. Finally, I’ve passed the plots to the ggplotly() function from the {plotly} package for some quick and easy javascript goodness!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Econometrics and Free Software",
    "section": "",
    "text": "Welcome to my blog where I talk about R, Nix, Econometrics and Data Science. If you enjoy reading what I write, you might enjoy my books or want to follow me on Mastodon or Twitter or Bluesky. If you are 40+, click here instead. I also make videos on youtube.\n\n\n\n\n\n\n2019\n\n\n\nUsing linear models with binary dependent variables, a simulation study\n\n\nUsing cosine similarity to find matching documents: a tutorial using Seneca’s letters to his friend Lucilius\n\n\nUsing Data Science to read 10 years of Luxembourguish newspapers from the 19th century\n\n\nThe never-ending editor war (?)\n\n\nStatistical matching, or when one single data source is not enough\n\n\nPivoting data frames just got easier thanks to pivot_wide() and pivot_long()\n\n\nModern R with the tidyverse is available on Leanpub\n\n\nManipulating strings with the {stringr} package\n\n\nMaking sense of the METS and ALTO XML standards\n\n\nLooking into 19th century ads from a Luxembourguish newspaper with R\n\n\nIntermittent demand, Croston and Die Hard\n\n\nHistorical newspaper scraping with {tesseract} and R\n\n\nGet text from pdfs or images using OCR: a tutorial with {tesseract} and {magick}\n\n\nFor posterity: install {xml2} on GNU/Linux distros\n\n\nFast food, causality and R packages, part 2\n\n\nFast food, causality and R packages, part 1\n\n\nCurly-Curly, the successor of Bang-Bang\n\n\nClassification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2\n\n\nClassification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 1\n\n\nBuilding a shiny app to explore historical newspapers: a step-by-step guide\n\n\n\n2018\n\n\n\n{pmice}, an experimental package for missing data imputation in parallel using {mice} and {furrr}\n\n\nWhat hyper-parameters are, and what to do with them; an illustration with ridge regression\n\n\nUsing the tidyverse for more than data manipulation: estimating pi with Monte Carlo methods\n\n\nUsing a genetic algorithm for the hyperparameter optimization of a SARIMA model\n\n\nThe year of the GNU+Linux desktop is upon us: using user ratings of Steam Play compatibility to play around with regex and the tidyverse\n\n\nThe best way to visit Luxembourguish castles is doing data science + combinatorial optimization\n\n\nSome fun with {gganimate}\n\n\nSearching for the optimal hyper-parameters of an ARIMA model in parallel: the tidy gridsearch approach\n\n\nR or Python? Why not both? Using Anaconda Python within R with {reticulate}\n\n\nPredicting job search by training a random forest on an unbalanced dataset\n\n\nObjects types and some useful R functions for beginners\n\n\nMissing data imputation and instrumental variables regression: the tidy approach\n\n\nMaps with pie charts on top of each administrative division: an example with Luxembourg’s elections data\n\n\nMapping a list of functions to a list of datasets with a list of columns as arguments\n\n\nManipulate dates easily with {lubridate}\n\n\nKeep trying that api call with purrr::possibly()\n\n\nIt’s lists all the way down, part 2: We need to go deeper\n\n\nIt’s lists all the way down\n\n\nImputing missing values in parallel using {furrr}\n\n\nImporting 30GB of data into R with sparklyr\n\n\nHow Luxembourguish residents spend their time: a small {flexdashboard} demo using the Time use survey data\n\n\nGoing from a human readable Excel file to a machine-readable csv with {tidyxl}\n\n\nGetting {sparklyr}, {h2o}, {rsparkling} to work together and some fun with bash\n\n\nGetting the data from the Luxembourguish elections out of Excel\n\n\nGetting data from pdfs using the pdftools package\n\n\nGet basic summary statistics for all the variables in a data frame\n\n\nFrom webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack\n\n\nForecasting my weight with R\n\n\nExporting editable plots from R to Powerpoint: making ggplot2 purrr with officer\n\n\nEasy time-series prediction with R: a tutorial with air traffic data from Lux Airport\n\n\nDealing with heteroskedasticity; regression with robust standard errors using R\n\n\nAnalyzing NetHack data, part 2: What players kill the most\n\n\nAnalyzing NetHack data, part 1: What kills the players\n\n\nA tutorial on tidy cross-validation with R\n\n\n\n2017\n\n\n\ntidyr::spread() and dplyr::rename_at() in action\n\n\nWhy I find tidyeval useful\n\n\nTeaching the tidyverse to beginners\n\n\nPeace of mind with purrr\n\n\nMy free book has a cover!\n\n\nMake ggplot2 purrr\n\n\nLesser known purrr tricks\n\n\nLesser known dplyr tricks\n\n\nLesser known dplyr 0.7* tricks\n\n\nIntroducing brotools\n\n\nHow to use jailbreakr\n\n\nEasy peasy STATA-like marginal effects with R\n\n\nBuilding formulae\n\n\n\n2016\n\n\n\nWork on lists of datasets instead of individual datasets by using functional programming\n\n\nUnit testing with R\n\n\nRead a lot of datasets at once with R\n\n\nMerge a list of datasets together\n\n\nI’ve started writing a ‘book’: Functional programming and unit testing for data munging with R\n\n\nFunctional programming and unit testing for data munging with R available on Leanpub\n\n\nData frame columns as arguments to dplyr functions\n\n\nCareful with tryCatch\n\n\n\n2015\n\n\n\nUpdate to Introduction to programming econometrics with R\n\n\nIntroduction to programming econometrics with R\n\n\nExport R output to a file\n\n\nBootstrapping standard errors for difference-in-differences estimation with R\n\n\n\n2014\n\n\n\nR, R with Atlas, R with OpenBLAS and Revolution R Open: which is fastest?\n\n\nObject Oriented Programming with R: An example with a Cournot duopoly\n\n\n\n2013\n\n\n\nUsing R as a Computer Algebra System with Ryacas\n\n\nSimulated Maximum Likelihood with R\n\n\nNonlinear Gmm with R - Example with a logistic regression\n\n\nMethod of Simulated Moments with R\n\n\n\nNo matching items"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "Build reproducible analytical pipelines that will output consistent, high-quality analyses using the R programming language, Git, Github and Docker. Learn about functional and literate programming to keep your code concise, easier to test, easier to share and easily understandable by others.\n\n\n\n\nRead it for free: https://raps-with-r.dev/\n\n\nBuy the DRM free Epub or PDF: https://leanpub.com/raps-with-r/\n\n\nBuy a physical copy on Amazon: https://www.amazon.com/Building-reproducible-analytical-pipelines-R/dp/B0C87H6MGF\n\n\n\n\n\n\nThis book focuses on teaching you how to use R in a modern way. Why modern? Because you will learn about the tidyverse collection of packages, which enable you to write efficient, legible code. You will also learn about functional programming, some statistical methods, writing your own packages… By the end of the book, you will have all the knowledge and tools on hand to use R for your day-to-day data science tasks!\n\n\n\n\nRead it for free: https://modern-rstats.eu/\n\n\n\nBuy the DRM free Epub or PDF (very out of date, update coming Q4-2025): https://leanpub.com/modern_tidyverse/\n\n\nBuy a physical copy (coming one day)"
  },
  {
    "objectID": "books.html#building-reproducible-analytical-pipelines-with-r",
    "href": "books.html#building-reproducible-analytical-pipelines-with-r",
    "title": "Books",
    "section": "",
    "text": "Build reproducible analytical pipelines that will output consistent, high-quality analyses using the R programming language, Git, Github and Docker. Learn about functional and literate programming to keep your code concise, easier to test, easier to share and easily understandable by others.\n\n\n\n\nRead it for free: https://raps-with-r.dev/\n\n\nBuy the DRM free Epub or PDF: https://leanpub.com/raps-with-r/\n\n\nBuy a physical copy on Amazon: https://www.amazon.com/Building-reproducible-analytical-pipelines-R/dp/B0C87H6MGF\n\n\n\n\n\n\nThis book focuses on teaching you how to use R in a modern way. Why modern? Because you will learn about the tidyverse collection of packages, which enable you to write efficient, legible code. You will also learn about functional programming, some statistical methods, writing your own packages… By the end of the book, you will have all the knowledge and tools on hand to use R for your day-to-day data science tasks!\n\n\n\n\nRead it for free: https://modern-rstats.eu/\n\n\n\nBuy the DRM free Epub or PDF (very out of date, update coming Q4-2025): https://leanpub.com/modern_tidyverse/\n\n\nBuy a physical copy (coming one day)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Bruno Rodrigues and hold a PhD in Economics from the University of Strasbourg.\n\n\n\nI’m currently employed as a statistician for the Ministry of Research and Higher Education in Luxembourg. Before that I was senior data scientist and then manager in the data science team at PwC Luxembourg, and before that I was a research assistant at STATEC Research.\nMy hobbies are boxing, lifting weights, cycling, cooking and reading or listening to audiobooks, which is more compatible with the life of a young father. I started this blog to share my enthusiasm for statistics. My blog posts are reshared on R-bloggers and RWeekly. I also enjoy learning about the R programming language and sharing my knowledge. That’s why I made this blog and write ebooks. I also have a youtube channel, where I show some tips and tricks with R, or rant about stuff."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks, presentations, workshops…",
    "section": "",
    "text": "BUILDING REPRODUCIBLE ANALYTICAL PIPELINES WITH R, DOCKER AND NIX: online talk for RLadies Rome, on the 29th of June 2024. Repository, Slides and video.\nRix: reproducible environments with Nix: talk I gave for the online part of the 2024 edition of useR! in Salzburg. Video\nRIX: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk and tutorial I gave for the 2024 edition of the Rencontres R in Vannes, Brittany, France. Slides and tutorial. No video available.\nRix: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk I gave on the 7th of June, 2024 for the INED (Institute Nationale d’Études Démographiques, National institute for demographic studies). Repository, Slides and Video.\nReproducible research with Nix and rix, a talk I gave at the DIPF, Leibniz Institute for Research and Information in Education, in LOS! (Leibniz Open Science!), the event series of the Leibniz Strategy Forum on Open Science. The workshop was held online, on May 16, 2024. Repository, Slides and video of the talk\n“rix: An R package for reproducible dev environments with Nix (FOSDEM 2024)”, a talk I gave at FOSDEM 2024 on the 4th of February 2024 in the Nix and NixOS dev room. Video here.\n“Building reproducible analytical pipelines”, Workshop I gave for the “R User Group Tunis” on December 9th, 2023. Video here. Slides and code here.\n“Porquê usar monads (programação funcional)? Uma ilustração com o pacote {chronicler}” (Why use monads (functional programming)? An illustration with the {chronicler} package), Talk I gave (online) at the INE (Instituto Nacional de Estatística), on the 7th of June 2023. Slides in Portuguese.\n“La reproductibilité avec R, ou pourquoi celle-ci est située sur un continuum” (Reproducibility with R, or why it lies on a continuum). Talk I gave at the Rencontres R 2023 in Avignon, France, on the 26th of June 2023. Slides in French, Video with English subtitles.\n“Building reproducible analytical pipelines”, Workshop I gave for “Workshops for Ukraine” on June 29th, 2023. You can pull the Docker image with the slides and code here.\n“Building reproducible analytical pipelines”, Talk I gave for ReproducibiliTea UCL on the 19th of July, 2023. Slides and Video. You can pull the Docker image with the slides and code here."
  },
  {
    "objectID": "talks.html#talks",
    "href": "talks.html#talks",
    "title": "Talks, presentations, workshops…",
    "section": "",
    "text": "BUILDING REPRODUCIBLE ANALYTICAL PIPELINES WITH R, DOCKER AND NIX: online talk for RLadies Rome, on the 29th of June 2024. Repository, Slides and video.\nRix: reproducible environments with Nix: talk I gave for the online part of the 2024 edition of useR! in Salzburg. Video\nRIX: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk and tutorial I gave for the 2024 edition of the Rencontres R in Vannes, Brittany, France. Slides and tutorial. No video available.\nRix: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk I gave on the 7th of June, 2024 for the INED (Institute Nationale d’Études Démographiques, National institute for demographic studies). Repository, Slides and Video.\nReproducible research with Nix and rix, a talk I gave at the DIPF, Leibniz Institute for Research and Information in Education, in LOS! (Leibniz Open Science!), the event series of the Leibniz Strategy Forum on Open Science. The workshop was held online, on May 16, 2024. Repository, Slides and video of the talk\n“rix: An R package for reproducible dev environments with Nix (FOSDEM 2024)”, a talk I gave at FOSDEM 2024 on the 4th of February 2024 in the Nix and NixOS dev room. Video here.\n“Building reproducible analytical pipelines”, Workshop I gave for the “R User Group Tunis” on December 9th, 2023. Video here. Slides and code here.\n“Porquê usar monads (programação funcional)? Uma ilustração com o pacote {chronicler}” (Why use monads (functional programming)? An illustration with the {chronicler} package), Talk I gave (online) at the INE (Instituto Nacional de Estatística), on the 7th of June 2023. Slides in Portuguese.\n“La reproductibilité avec R, ou pourquoi celle-ci est située sur un continuum” (Reproducibility with R, or why it lies on a continuum). Talk I gave at the Rencontres R 2023 in Avignon, France, on the 26th of June 2023. Slides in French, Video with English subtitles.\n“Building reproducible analytical pipelines”, Workshop I gave for “Workshops for Ukraine” on June 29th, 2023. You can pull the Docker image with the slides and code here.\n“Building reproducible analytical pipelines”, Talk I gave for ReproducibiliTea UCL on the 19th of July, 2023. Slides and Video. You can pull the Docker image with the slides and code here."
  },
  {
    "objectID": "talks.html#interviews-podcasts",
    "href": "talks.html#interviews-podcasts",
    "title": "Talks, presentations, workshops…",
    "section": "Interviews, podcasts…",
    "text": "Interviews, podcasts…\n\nFrench Data Workers Podcast #1 Large Scale Testing : Contenir la covid-19 avec des dépistages ciblés\nFrench Bruno Rodrigues défend une approche basée sur la reproductibilité de la data science au Luxembourg\nEnglish Leanpub Frontmatter Podcast #263"
  },
  {
    "objectID": "posts/2018-11-10-nethack_analysis_part2.html",
    "href": "posts/2018-11-10-nethack_analysis_part2.html",
    "title": "Analyzing NetHack data, part 2: What players kill the most",
    "section": "",
    "text": "Link to webscraping the data\nLink to Analysis, part 1"
  },
  {
    "objectID": "posts/2018-11-10-nethack_analysis_part2.html#introduction",
    "href": "posts/2018-11-10-nethack_analysis_part2.html#introduction",
    "title": "Analyzing NetHack data, part 2: What players kill the most",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nThis is the third blog post that deals with data from the game NetHack, and oh boy, did a lot of things happen since the last blog post! Here’s a short timeline of the events:\n\n\n\nI scraped data from alt.org/nethack and made a package with the data available on Github (that package was too big for CRAN)\n\n\nThen, I analyzed the data, focusing on what monsters kill the players the most, and also where players die the most\n\n\n@GridSageGames, developer of the roguelike Cogmind and moderator of the roguelike subreddit, posted the blog post on reddit\n\n\nI noticed that actually, by scraping the data like I did, I only got a sample of 100 daily games\n\n\nThis point was also discussed on Reddit, and bhhak, an UnNetHack developer (UnNetHack is a fork of NetHack) suggested I used the xlogfiles instead\n\n\nxlogfiles are log files generated by NetHack, and are also available on alt.org/nethack\n\n\nI started scraping them, and getting a lot more data\n\n\nI got contacted on twitter by @paxed, an admin of alt.org/nethack:\n\n\n{{% tweet “1059333642592366593” %}}\n\n\nHe gave me access to ALL THE DATA on alt.org/nethack!\n\n\nThe admins of alt.org/nethack will release all the data to the public!\n\n\n\nSo, I will now continue with the blog post I wanted to do in the first place; focusing now on what roles players choose to play the most, and also which monsters they kill the most. BUT! Since all the data will be released to the public, my {nethack} package that contains data that I scraped is not that useful anymore. So I changed the nature of the package. Now the package contains some functions: a function to parse and prepare the xlogfiles from NetHack that you can download from alt.org/nethack (or from any other public server), a function to download dumplogs such as this one. These dumplogs contain a lot of info that I will extract in this blog post, using another function included in the {nethack} package. The package also contains a sample of 6000 runs from NetHack version 3.6.1.\n\n\nYou can install the package with the following command line:\n\ndevtools::install_github(\"b-rodrigues/nethack\")"
  },
  {
    "objectID": "posts/2018-11-10-nethack_analysis_part2.html#the-nethack-package",
    "href": "posts/2018-11-10-nethack_analysis_part2.html#the-nethack-package",
    "title": "Analyzing NetHack data, part 2: What players kill the most",
    "section": "\nThe {nethack} package\n",
    "text": "The {nethack} package\n\n\nIn part 1 I showed what killed players the most. Here, I will focus on what monsters players kill the most. Let’s start by loading some packages:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(magrittr)\nlibrary(ggridges)\nlibrary(brotools)\nlibrary(rvest)\nlibrary(nethack)\n\nLet’s first describe the data:\n\nbrotools::describe(nethack) %&gt;% \n  print(n = Inf)\n## # A tibble: 23 x 17\n##    variable type    nobs     mean       sd mode       min     max      q05\n##    &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n##  1 deathdn… Nume… 6.00e3  8.45e-1  1.30e+0 2       0.      7.00e0   0.    \n##  2 deathlev Nume… 6.00e3  4.32e+0  3.69e+0 10     -5.00e0  4.50e1   1.00e0\n##  3 deaths   Nume… 6.00e3  8.88e-1  3.54e-1 1       0.      5.00e0   0.    \n##  4 endtime  Nume… 6.00e3  1.53e+9  4.72e+6 1534…   1.52e9  1.54e9   1.53e9\n##  5 hp       Nume… 6.00e3  6.64e+0  4.96e+1 -1     -9.40e1  1.79e3  -8.00e0\n##  6 maxhp    Nume… 6.00e3  3.82e+1  5.29e+1 57      2.00e0  1.80e3   1.10e1\n##  7 maxlvl   Nume… 6.00e3  5.52e+0  6.36e+0 10      1.00e0  5.30e1   1.00e0\n##  8 points   Nume… 6.00e3  4.69e+4  4.18e+5 10523   0.      9.92e6   1.40e1\n##  9 realtime Nume… 6.00e3  4.42e+3  1.60e+4 4575    0.      3.23e5   6.90e1\n## 10 startti… Nume… 6.00e3  1.53e+9  4.72e+6 1534…   1.52e9  1.54e9   1.53e9\n## 11 turns    Nume… 6.00e3  3.60e+3  9.12e+3 6797    3.10e1  1.97e5   9.49e1\n## 12 align    Char… 6.00e3 NA       NA       Cha    NA      NA       NA     \n## 13 align0   Char… 6.00e3 NA       NA       Cha    NA      NA       NA     \n## 14 death    Char… 6.00e3 NA       NA       kill…  NA      NA       NA     \n## 15 gender   Char… 6.00e3 NA       NA       Fem    NA      NA       NA     \n## 16 gender0  Char… 6.00e3 NA       NA       Fem    NA      NA       NA     \n## 17 killed_… Char… 6.00e3 NA       NA       fain…  NA      NA       NA     \n## 18 name     Char… 6.00e3 NA       NA       drud…  NA      NA       NA     \n## 19 race     Char… 6.00e3 NA       NA       Elf    NA      NA       NA     \n## 20 role     Char… 6.00e3 NA       NA       Wiz    NA      NA       NA     \n## 21 dumplog  List  1.33e6 NA       NA       &lt;NA&gt;   NA      NA       NA     \n## 22 birthda… Date  6.00e3 NA       NA       &lt;NA&gt;   NA      NA       NA     \n## 23 deathda… Date  6.00e3 NA       NA       &lt;NA&gt;   NA      NA       NA     \n## # ... with 8 more variables: q25 &lt;dbl&gt;, median &lt;dbl&gt;, q75 &lt;dbl&gt;,\n## #   q95 &lt;dbl&gt;, n_missing &lt;int&gt;, n_unique &lt;int&gt;, starting_date &lt;date&gt;,\n## #   ending_date &lt;date&gt;\n\nAll these columns are included in xlogfiles. The data was prepared using two functions, included in {nethack}:\n\nxlog &lt;- read_delim(\"~/path/to/nethack361_xlog.csv\", \"\\t\", escape_double = FALSE, \n                   col_names = FALSE, trim_ws = TRUE)\n\nxlog_df &lt;- clean_xlog(xlog)\n\nnethack361_xlog.csv is the raw xlogfiles that you can get from NetHack public servers. clean_xlog() is a function that parses an xlogfile and returns a clean data frame. xlog_df will be a data frame that will look just as the one included in {nethack}. It is then possible to get the dumplog from each run included in xlog_df using get_dumplog():\n\nxlog_df &lt;- get_dumplog(xlog_df)\n\nThis function adds a column called dumplog with the dumplog of that run. I will now analyze the dumplog file, by focusing on monsters vanquished, genocided or extinct. In a future blogpost I will focus on other achievements."
  },
  {
    "objectID": "posts/2018-11-10-nethack_analysis_part2.html#roles-played-and-other-starting-stats",
    "href": "posts/2018-11-10-nethack_analysis_part2.html#roles-played-and-other-starting-stats",
    "title": "Analyzing NetHack data, part 2: What players kill the most",
    "section": "\nRoles played (and other starting stats)\n",
    "text": "Roles played (and other starting stats)\n\n\nI will take a look at the races, roles, gender and alignment players start with the most. I will do pie charts to visualize these variable, so first, let’s start by writing a general function that allows me to do just that:\n\ncreate_pie &lt;- function(dataset, variable, repel = FALSE){\n\n  if(repel){\n    geom_label &lt;- function(...){\n      ggrepel::geom_label_repel(...)\n    }\n  }\n\n  variable &lt;- enquo(variable)\n\n  dataset %&gt;%\n    count((!!variable)) %&gt;%\n    mutate(total = sum(n),\n           freq = n/total,\n           labels = scales::percent(freq)) %&gt;% \n    arrange(desc(freq)) %&gt;%\n    ggplot(aes(x = \"\", y = freq, fill = (!!variable))) + \n    geom_col() + \n    geom_label(aes(label = labels), position = position_stack(vjust = 0.25), show.legend = FALSE) + \n    coord_polar(\"y\") + \n    theme_blog() + \n    scale_fill_blog() + \n    theme(legend.title = element_blank(),\n          panel.grid = element_blank(),\n          axis.text = element_blank(),\n          axis.title = element_blank())\n}\n\nNow I can easily plot the share of races chosen:\n\ncreate_pie(nethack, race)\n\n\n\n\nor the share of alignment:\n\ncreate_pie(nethack, align0)\n\n\n\n\nSame for the share of gender:\n\ncreate_pie(nethack, gender0)\n\n\n\n\nand finally for the share of roles:\n\ncreate_pie(nethack, role, repel = TRUE) \n\n\n\n\ncreate_pie() is possible thanks to tidy evaluation in {ggplot2}, which makes it possible to write a function that passes data frame columns down to aes(). Before version 3.0 of {ggplot2} this was not possible, and writing such a function would have been a bit more complicated. Now, it’s as easy as pie, if I dare say.\n\n\nSomething else I want to look at, is the distribution of turns by role:\n\nnethack %&gt;%\n  filter(turns &lt; quantile(turns, 0.98)) %&gt;%\n  ggplot(aes(x = turns, y = role, group = role, fill = role)) +\n    geom_density_ridges(scale = 6, size = 0.25, rel_min_height = 0.01) + \n    theme_blog() + \n    scale_fill_blog() + \n    theme(axis.text.y = element_blank(),\n          axis.title.y = element_blank())\n## Picking joint bandwidth of 486\n\n\n\n\nI use the very cool {ggridges} package for that. The distribution seems to mostly be the same (of course, one should do a statistical test to be sure), but the one for the role “Valkyrie” seems to be quite different from the others. It is known that it is easier to win the game playing as a Valkyrie, but a question remains: is it really easier as a Valkyrie, or do good players tend to play as Valkyries more often?"
  },
  {
    "objectID": "posts/2018-11-10-nethack_analysis_part2.html#creatures-vanquished-genocided-or-extinct",
    "href": "posts/2018-11-10-nethack_analysis_part2.html#creatures-vanquished-genocided-or-extinct",
    "title": "Analyzing NetHack data, part 2: What players kill the most",
    "section": "\nCreatures vanquished, genocided or extinct\n",
    "text": "Creatures vanquished, genocided or extinct\n\n\nThe dumplog lists which, and how many of which, creatures were vanquished during the run, as well as creatures that were genocided and extinct. The player can genocide an entire species by reading a scroll of genocide (or by sitting on a throne). A species gets extinct if the player manages to kill every monster from that species (there’s other ways too, but for the sake of simplicity, let’s just say that when the players kills every monster from a species, the species is extinct). The following lines are an extract of a dumplog:\n\n\"Vanquished creatures:\"\n\"    Baalzebub\"\n\"    Orcus\"\n\"    Juiblex\"\n\"the Wizard of Yendor (4 times)\"\n\"    Pestilence (thrice)\"\n\"    Famine\"\n\"    Vlad the Impaler\"\n\"  4 arch-liches\"\n\"  an arch-lich\"\n\"  a high priest\"\n\"...\"\n\"...\"\n\"...\"\n\"2873 creatures vanquished.\" \n\nIf I want to analyze this, I have to first solve some problems:\n\n\n\nReplace “a” and “an” by “1”\n\n\nPut the digit in the string “(4 times)” in front of the name of the monster (going from “the Wizard of Yendor (4 times)” to “4 the Wizard of Yendor”)\n\n\nDo something similar for “twice” and “thrice”\n\n\nPut everything into singular (for example, arch-liches into arch-lich)\n\n\nTrim whitespace\n\n\nExtract the genocided or extinct status from the dumplog too\n\n\nFinally, return a data frame with all the needed info\n\n\n\nI wrote a function called extracted_defeated_monsters() and included it in the {nethack} package. I discuss this function in appendix, but what it does is extracting information from dumplog files about vanquished, genocided or extinct monsters and returns a tidy dataframe with that info. This function has a lot of things going on inside it, so if you’re interested in learning more about regular expression and other {tidyverse} tricks, I really encourage you to read its source code.\n\n\nI can now easily add this info to my data:\n\nnethack %&lt;&gt;%\n  mutate(monsters_destroyed = map(dumplog, ~possibly(extract_defeated_monsters, otherwise = NA)(.)))\n\nLet’s take a look at one of them:\n\nnethack$monsters_destroyed[[117]]\n## # A tibble: 285 x 3\n##    value monster              status\n##    &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; \n##  1     1 baalzebub            &lt;NA&gt;  \n##  2     1 orcu                 &lt;NA&gt;  \n##  3     1 juiblex              &lt;NA&gt;  \n##  4     4 the wizard of yendor &lt;NA&gt;  \n##  5     3 pestilence           &lt;NA&gt;  \n##  6     1 famine               &lt;NA&gt;  \n##  7     1 vlad the impaler     &lt;NA&gt;  \n##  8     4 arch-lich            &lt;NA&gt;  \n##  9     1 high priest          &lt;NA&gt;  \n## 10     1 medusa               &lt;NA&gt;  \n## # ... with 275 more rows\nnethack$monsters_destroyed[[117]] %&gt;% \n  count(status)\n## # A tibble: 3 x 2\n##   status        n\n##   &lt;chr&gt;     &lt;int&gt;\n## 1 extinct       2\n## 2 genocided     7\n## 3 &lt;NA&gt;        276\n\nThe status variable tells us if that monster was genocided or extinct during that run. status equal to “NA” means vanquished.\n\n\nIt is now possible to look at, say, the top 15 vanquished monsters (normalized):\n\nnethack %&gt;%\n  filter(!is.na(monsters_destroyed)) %&gt;%\n  pull(monsters_destroyed) %&gt;%\n  bind_rows %&gt;%\n  group_by(monster) %&gt;%\n  summarise(total = sum(value)) %&gt;%\n  top_n(15) %&gt;%\n  ungroup() %&gt;%\n  mutate(norm_total = (total - min(total))/(max(total) - min(total))) %&gt;%\n  mutate(monster = fct_reorder(monster, norm_total, .desc = FALSE)) %&gt;%\n  ggplot() + \n  geom_col(aes(y = norm_total, x = monster)) + \n  coord_flip() + \n  theme_blog() + \n  scale_fill_blog() + \n  ylab(\"Ranking\") +\n  xlab(\"Monster\")\n## Selecting by total\n\n\n\n\nIn this type of graph, the most vanquished monster, “gnome” has a value of 1, and the least vanquished one, 0. This normalization step is also used in the pre-processing step of machine learning algorithms. This helps convergence of the gradient descent algorithm for instance.\n\n\nMonsters can also get genocided or extinct. Let’s make a pie chart of the proportion of genocided and extinct monsters (I lump monsters that are genocided or extinct less than 5% of the times into a category called other). Because I want two pie charts, I nest the data after having grouped it by the status variable. This is a trick I discussed in this blog post and that I use very often:\n\nnethack %&gt;%\n  filter(!is.na(monsters_destroyed)) %&gt;%\n  pull(monsters_destroyed) %&gt;%\n  bind_rows %&gt;%\n  filter(!is.na(status)) %&gt;%\n  group_by(status) %&gt;% \n  count(monster) %&gt;% \n  mutate(monster = fct_lump(monster, prop = 0.05, w = n)) %&gt;% \n  group_by(status, monster) %&gt;% \n  summarise(total_count = sum(n)) %&gt;%\n  mutate(freq = total_count/sum(total_count),\n         labels = scales::percent(freq)) %&gt;%\n  arrange(desc(freq)) %&gt;%\n  group_by(status) %&gt;%\n  nest() %&gt;%\n  mutate(pie_chart = map2(.x = status,\n                          .y = data,\n                          ~ggplot(data = .y,\n                                  aes(x = \"\", y = freq, fill = (monster))) + \n    geom_col() + \n    ggrepel::geom_label_repel(aes(label = labels), position = position_stack(vjust = 0.25), show.legend = FALSE) + \n    coord_polar(\"y\") + \n    theme_blog() + \n    scale_fill_blog() + \n      ggtitle(.x) +\n    theme(legend.title = element_blank(),\n          panel.grid = element_blank(),\n          axis.text = element_blank(),\n          axis.title = element_blank())\n  )) %&gt;%\n  pull(pie_chart)\n## Warning in mutate_impl(.data, dots): Unequal factor levels: coercing to\n## character\n## Warning in mutate_impl(.data, dots): binding character and factor vector,\n## coercing into character vector\n\n## Warning in mutate_impl(.data, dots): binding character and factor vector,\n## coercing into character vector\n## [[1]]\n\n\n\n## \n## [[2]]\n\n\n\n\nThat was it for this one, the graphs are not that super sexy, but the amount of work that went into making them was quite consequent. The main reason was that parsing xlogfiles was a bit tricky, but the main challenge was extracting information from dumplog files. This proved to be a bit more complicated than expected (just take a look at the source code of extract_defeated_monsters() to get an idea…)."
  },
  {
    "objectID": "posts/2018-11-10-nethack_analysis_part2.html#bonus-plot",
    "href": "posts/2018-11-10-nethack_analysis_part2.html#bonus-plot",
    "title": "Analyzing NetHack data, part 2: What players kill the most",
    "section": "\nBonus plot\n",
    "text": "Bonus plot\n\n\n\nCorrect number of daily games\n\n\nThe daily number of games are available here. Let’s extract this info and remake the plot that shows the number of runs per day:\n\ngames &lt;- read_html(\"https://alt.org/nethack/dailygames_ct.html\") %&gt;%\n        html_nodes(xpath = '//table') %&gt;%\n        html_table(fill = TRUE) \n\nThis extracts all the tables and puts them into a list. Let’s take a look at one:\n\nhead(games[[1]])\n##   2018  2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018\n## 1         NA    1    2    3    4    5    6    7    8    9   10   11   12\n## 2  Jan 11639  275  370  394  363  392  276  288  324  297  411  413  430\n## 3  Feb 10819  375  384  359  376  440  345  498  457  416  376  421  416\n## 4  Mar 12148  411  403  421  392  447  391  451  298  350  309  309  369\n## 5  Apr 13957  456  513  482  516  475  490  397  431  436  438  541  493\n## 6  May 13361  595  509  576  620  420  443  407  539  440  446  404  282\n##   2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018\n## 1   13   14   15   16   17   18   19   20   21   22   23   24   25   26\n## 2  331  341  318  483  408  424  464  412  371  430  348  315  359  375\n## 3  385  367  443  324  283  341  385  398  361  379  399  276  455  460\n## 4  390  358  362  345  388  360  411  382  371  400  410  417  328  431\n## 5  593  537  396  578  403  435  526  448  339  377  476  492  528  393\n## 6  265  358  419  564  483  429  423  299  424  404  450  408  355  409\n##   2018 2018 2018 2018 2018\n## 1   27   28   29   30   31\n## 2  432  371  385  440  399\n## 3  353  347   NA   NA   NA\n## 4  386  484  493  486  395\n## 5  407  421  463  477   NA\n## 6  417  433  360  391  389\n\nLet’s clean this up.\n\nclean_table &lt;- function(df){\n  # Promotes first row to header\n  colnames(df) &lt;- df[1, ]\n  df &lt;- df[-1, ]\n  \n  # Remove column with total from the month\n  df &lt;- df[, -2]\n  \n  # Name the first column \"month\"\n  \n  colnames(df)[1] &lt;- \"month\"\n  \n  # Now put it in a tidy format\n  df %&gt;%\n    gather(day, games_played, -month)\n}\n\nNow I can clean up all the tables. I apply this function to each element of the list games. I also add a year column:\n\ngames &lt;- map(games, clean_table) %&gt;%\n  map2_dfr(.x = ., \n       .y = seq(2018, 2001),\n       ~mutate(.x, year = .y))\n\nNow I can easily create the plot I wanted\n\ngames %&lt;&gt;%\n  mutate(date = lubridate::ymd(paste(year, month, day, sep = \"-\")))\n## Warning: 122 failed to parse.\nggplot(games, aes(y = games_played, x = date)) + \n  geom_point(colour = \"#0f4150\") + \n  geom_smooth(colour = \"#82518c\") + \n  theme_blog() + \n  ylab(\"Total games played\")\n## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n## Warning: Removed 452 rows containing non-finite values (stat_smooth).\n## Warning: Removed 452 rows containing missing values (geom_point).\n\n\n\n\nThere’s actually a lot more games than 50 per day being played!"
  },
  {
    "objectID": "posts/2018-11-10-nethack_analysis_part2.html#appendix",
    "href": "posts/2018-11-10-nethack_analysis_part2.html#appendix",
    "title": "Analyzing NetHack data, part 2: What players kill the most",
    "section": "\nAppendix\n",
    "text": "Appendix\n\n\n\nFuzzy matching\n\n\nIf you take a look at the extract_defeated_monsters() source code, you’ll see that at some point I “singularize” monster names. I decided to deal with this singular/plural issue, “by hand”, but also explored other possibilities, such as matching the plural nouns with the singular nouns fuzzily. In the end it didn’t work out so well, but here’s the code for future reference.\n\nmonster_list &lt;- read_html(\"https://nethackwiki.com/wiki/Monsters_(by_difficulty)\") %&gt;%\n    html_nodes(\".prettytable\") %&gt;% \n    .[[1]] %&gt;%\n    html_table(fill = TRUE)\n\nmonster_list %&lt;&gt;%\n    select(monster = Name)\n\nhead(monster_list)\n##      monster\n## 1 Demogorgon\n## 2   Asmodeus\n## 3  Baalzebub\n## 4   Dispater\n## 5     Geryon\n## 6      Orcus\nlibrary(fuzzyjoin)\n\ntest_vanquished &lt;- extract_defeated_monsters(nethack$dumplog[[117]])\n\nhead(test_vanquished)\n## # A tibble: 6 x 3\n##   value monster              status\n##   &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; \n## 1     1 baalzebub            &lt;NA&gt;  \n## 2     1 orcu                 &lt;NA&gt;  \n## 3     1 juiblex              &lt;NA&gt;  \n## 4     4 the wizard of yendor &lt;NA&gt;  \n## 5     3 pestilence           &lt;NA&gt;  \n## 6     1 famine               &lt;NA&gt;\n\nYou can take a look at the result by expanding:\n\n\n\n\nClick to expand\n\n\nstringdist_left_join(test_vanquished, monster_list) %&gt;% \n  count(monster.y) %&gt;%\n  print(n = Inf)\n## Joining by: \"monster\"\n## # A tibble: 297 x 2\n##     monster.y                   n\n##     &lt;chr&gt;                   &lt;int&gt;\n##   1 acid blob                   1\n##   2 air elemental               2\n##   3 Aleax                       1\n##   4 aligned priest              1\n##   5 Angel                       1\n##   6 ape                         2\n##   7 arch-lich                   1\n##   8 Baalzebub                   1\n##   9 baby black dragon           1\n##  10 baby crocodile              1\n##  11 baby gray dragon            1\n##  12 baby green dragon           1\n##  13 baby long worm              1\n##  14 baby orange dragon          1\n##  15 baby white dragon           1\n##  16 baby yellow dragon          1\n##  17 balrog                      1\n##  18 baluchitherium              1\n##  19 barbed devil                1\n##  20 barrow wight                1\n##  21 bat                         1\n##  22 black dragon                1\n##  23 black light                 1\n##  24 black naga                  1\n##  25 black pudding               1\n##  26 black unicorn               1\n##  27 blue dragon                 1\n##  28 blue jelly                  1\n##  29 bone devil                  1\n##  30 brown mold                  1\n##  31 brown pudding               1\n##  32 bugbear                     1\n##  33 captain                     1\n##  34 carnivorous ape             1\n##  35 cave spider                 1\n##  36 centipede                   1\n##  37 chameleon                   1\n##  38 chickatrice                 2\n##  39 clay golem                  1\n##  40 cobra                       1\n##  41 cockatrice                  2\n##  42 couatl                      1\n##  43 coyote                      1\n##  44 crocodile                   1\n##  45 demilich                    1\n##  46 dingo                       1\n##  47 disenchanter                1\n##  48 dog                         1\n##  49 doppelganger                1\n##  50 dust vortex                 1\n##  51 dwarf                       2\n##  52 dwarf king                  1\n##  53 dwarf lord                  1\n##  54 dwarf mummy                 1\n##  55 dwarf zombie                1\n##  56 earth elemental             1\n##  57 electric eel                1\n##  58 elf                         1\n##  59 elf mummy                   1\n##  60 elf zombie                  1\n##  61 elf-lord                    1\n##  62 Elvenking                   1\n##  63 energy vortex               1\n##  64 erinys                      1\n##  65 ettin                       1\n##  66 ettin mummy                 1\n##  67 ettin zombie                1\n##  68 Famine                      1\n##  69 fire ant                    2\n##  70 fire elemental              2\n##  71 fire giant                  2\n##  72 fire vortex                 2\n##  73 flaming sphere              1\n##  74 flesh golem                 1\n##  75 floating eye                1\n##  76 fog cloud                   1\n##  77 forest centaur              1\n##  78 fox                         1\n##  79 freezing sphere             1\n##  80 frost giant                 1\n##  81 gargoyle                    1\n##  82 garter snake                1\n##  83 gas spore                   1\n##  84 gecko                       1\n##  85 gelatinous cube             1\n##  86 ghost                       2\n##  87 ghoul                       2\n##  88 giant ant                   3\n##  89 giant bat                   3\n##  90 giant beetle                1\n##  91 giant eel                   1\n##  92 giant mimic                 1\n##  93 giant mummy                 1\n##  94 giant rat                   3\n##  95 giant spider                1\n##  96 giant zombie                1\n##  97 glass piercer               1\n##  98 gnome                       1\n##  99 gnome king                  1\n## 100 gnome lord                  1\n## 101 gnome mummy                 1\n## 102 gnome zombie                1\n## 103 gnomish wizard              1\n## 104 goblin                      1\n## 105 gold golem                  2\n## 106 golden naga                 1\n## 107 golden naga hatchling       1\n## 108 gray ooze                   1\n## 109 gray unicorn                1\n## 110 Green-elf                   1\n## 111 gremlin                     1\n## 112 Grey-elf                    1\n## 113 grid bug                    1\n## 114 guardian naga               1\n## 115 guardian naga hatchling     1\n## 116 hell hound                  1\n## 117 hell hound pup              1\n## 118 hezrou                      1\n## 119 high priest                 1\n## 120 hill giant                  1\n## 121 hill orc                    1\n## 122 hobbit                      1\n## 123 hobgoblin                   1\n## 124 homunculus                  1\n## 125 horned devil                1\n## 126 horse                       2\n## 127 housecat                    1\n## 128 human                       1\n## 129 human mummy                 1\n## 130 human zombie                1\n## 131 ice devil                   1\n## 132 ice troll                   1\n## 133 ice vortex                  2\n## 134 iguana                      1\n## 135 imp                         1\n## 136 incubus                     1\n## 137 iron golem                  1\n## 138 iron piercer                1\n## 139 jabberwock                  1\n## 140 jackal                      1\n## 141 jaguar                      1\n## 142 jellyfish                   1\n## 143 Juiblex                     1\n## 144 Keystone Kop                1\n## 145 ki-rin                      1\n## 146 killer bee                  1\n## 147 kitten                      1\n## 148 kobold                      1\n## 149 kobold lord                 1\n## 150 kobold mummy                1\n## 151 kobold shaman               1\n## 152 kobold zombie               1\n## 153 Kop Lieutenant              1\n## 154 Kop Sergeant                1\n## 155 kraken                      2\n## 156 large cat                   1\n## 157 large dog                   1\n## 158 large kobold                1\n## 159 large mimic                 1\n## 160 leather golem               1\n## 161 leocrotta                   1\n## 162 leprechaun                  1\n## 163 lich                        2\n## 164 lichen                      2\n## 165 lieutenant                  1\n## 166 little dog                  1\n## 167 lizard                      1\n## 168 long worm                   1\n## 169 Lord Surtur                 1\n## 170 lurker above                1\n## 171 lynx                        1\n## 172 manes                       1\n## 173 marilith                    1\n## 174 master lich                 1\n## 175 master mind flayer          1\n## 176 Medusa                      1\n## 177 mind flayer                 1\n## 178 minotaur                    1\n## 179 monk                        2\n## 180 monkey                      1\n## 181 Mordor orc                  1\n## 182 mountain centaur            1\n## 183 mountain nymph              1\n## 184 mumak                       1\n## 185 nalfeshnee                  1\n## 186 Nazgul                      1\n## 187 newt                        1\n## 188 Norn                        1\n## 189 nurse                       2\n## 190 ochre jelly                 1\n## 191 ogre                        1\n## 192 ogre king                   1\n## 193 ogre lord                   1\n## 194 Olog-hai                    1\n## 195 orange dragon               1\n## 196 orc                         3\n## 197 orc mummy                   1\n## 198 orc shaman                  1\n## 199 orc zombie                  1\n## 200 orc-captain                 1\n## 201 Orcus                       1\n## 202 owlbear                     1\n## 203 page                        2\n## 204 panther                     1\n## 205 paper golem                 1\n## 206 Pestilence                  1\n## 207 piranha                     1\n## 208 pit fiend                   1\n## 209 pit viper                   1\n## 210 plains centaur              1\n## 211 pony                        1\n## 212 purple worm                 1\n## 213 pyrolisk                    1\n## 214 python                      1\n## 215 quantum mechanic            1\n## 216 quasit                      1\n## 217 queen bee                   1\n## 218 quivering blob              1\n## 219 rabid rat                   1\n## 220 ranger                      1\n## 221 raven                       2\n## 222 red dragon                  1\n## 223 red mold                    1\n## 224 red naga                    1\n## 225 rock mole                   1\n## 226 rock piercer                1\n## 227 rock troll                  1\n## 228 rogue                       2\n## 229 rope golem                  1\n## 230 roshi                       1\n## 231 rothe                       1\n## 232 rust monster                1\n## 233 salamander                  1\n## 234 sandestin                   1\n## 235 sasquatch                   1\n## 236 scorpion                    1\n## 237 sergeant                    1\n## 238 sewer rat                   1\n## 239 shade                       3\n## 240 shark                       2\n## 241 shocking sphere             1\n## 242 shrieker                    1\n## 243 silver dragon               1\n## 244 skeleton                    1\n## 245 small mimic                 1\n## 246 snake                       2\n## 247 soldier                     1\n## 248 soldier ant                 1\n## 249 spotted jelly               1\n## 250 stalker                     1\n## 251 steam vortex                1\n## 252 stone giant                 2\n## 253 stone golem                 1\n## 254 storm giant                 2\n## 255 straw golem                 1\n## 256 succubus                    1\n## 257 tengu                       1\n## 258 tiger                       1\n## 259 titanothere                 1\n## 260 trapper                     1\n## 261 troll                       1\n## 262 umber hulk                  1\n## 263 Uruk-hai                    1\n## 264 vampire                     1\n## 265 vampire bat                 1\n## 266 vampire lord                1\n## 267 violet fungus               1\n## 268 Vlad the Impaler            1\n## 269 vrock                       1\n## 270 warg                        2\n## 271 warhorse                    1\n## 272 water elemental             1\n## 273 water moccasin              1\n## 274 water nymph                 1\n## 275 werejackal                  2\n## 276 wererat                     2\n## 277 werewolf                    2\n## 278 white dragon                1\n## 279 white unicorn               1\n## 280 winged gargoyle             1\n## 281 winter wolf                 1\n## 282 winter wolf cub             1\n## 283 wizard                      1\n## 284 wolf                        1\n## 285 wood golem                  2\n## 286 wood nymph                  1\n## 287 Woodland-elf                1\n## 288 wraith                      1\n## 289 wumpus                      1\n## 290 xan                         3\n## 291 xorn                        2\n## 292 yellow dragon               1\n## 293 yellow light                1\n## 294 yellow mold                 1\n## 295 yeti                        1\n## 296 zruty                       1\n## 297 &lt;NA&gt;                        1\n\n\nAs you can see, some matches fail, especially for words that end in “y” in the singular, so “ies” in plural, or “fire vortices” that does not get matched to “fire vortex”. I tried all the methods but it’s either worse, or marginally better.\n\n\n\n\nExtracting info from dumplogfiles\n\n\n\n\nClick here to take a look at the source code from extract_defeated_monsters\n\n\n#' Extract information about defeated monsters from an xlogfile\n#' @param xlog A raw xlogfile\n#' @return A data frame with information on vanquished, genocided and extincted monsters\n#' @importFrom dplyr mutate select filter bind_rows full_join\n#' @importFrom tidyr separate\n#' @importFrom tibble as_tibble tibble\n#' @importFrom magrittr \"%&gt;%\"\n#' @importFrom purrr map2 possibly is_empty modify_if simplify discard\n#' @importFrom readr read_lines\n#' @importFrom stringr str_which str_replace_all str_replace str_trim str_detect str_to_lower str_extract_all str_extract\n#' @export\n#' @examples\n#' \\dontrun{\n#' get_dumplog(xlog)\n#' }\nextract_defeated_monsters &lt;- function(dumplog){\n\n    if(any(str_detect(dumplog, \"No creatures were vanquished.\"))){\n        return(NA)\n    } else {\n\n        start &lt;- dumplog %&gt;% # &lt;- dectect the start of the list\n            str_which(\"Vanquished creatures\")\n\n        end &lt;- dumplog %&gt;% # &lt;- detect the end of the list\n            str_which(\"\\\\d+ creatures vanquished.\")\n\n        if(is_empty(end)){ # This deals with the situation of only one vanquished creature\n            end &lt;- start + 2\n        }\n\n        list_creatures &lt;- dumplog[(start + 1):(end - 1)] %&gt;% # &lt;- extract the list\n            str_replace_all(\"\\\\s+an? \", \"1 \") %&gt;% # &lt;- replace a or an by 1\n            str_trim() # &lt;- trim white space\n\n        # The following function first extracts the digit in the string (123 times)\n        # and replaces the 1 with this digit\n        # This means that: \"1 the Wizard of Yendor (4 times)\" becomes \"4 the Wizard of Yendor (4 times)\"\n        str_extract_replace &lt;- function(string){\n            times &lt;- str_extract(string, \"\\\\d+(?=\\\\stimes)\")\n            str_replace(string, \"1\", times)\n        }\n\n        result &lt;- list_creatures %&gt;%\n            # If a string starts with a letter, add a 1\n            # This means that: \"Baalzebub\" becomes \"1 Baalzebub\"\n            modify_if(str_detect(., \"^[:alpha:]\"), ~paste(\"1\", .)) %&gt;%\n            # If the string \"(twice)\" is detected, replace \"1\" (that was added the line before) with \"2\"\n            modify_if(str_detect(., \"(twice)\"), ~str_replace(., \"1\", \"2\")) %&gt;%\n            # Same for \"(thrice)\"\n            modify_if(str_detect(., \"(thrice)\"), ~str_replace(., \"1\", \"3\")) %&gt;%\n            # Exctract the digit in \"digit times\" and replace the \"1\" with digit\n            modify_if(str_detect(., \"(\\\\d+ times)\"), str_extract_replace) %&gt;%\n            # Replace \"(times)\" or \"(twice)\" etc with \"\"\n            str_replace_all(\"\\\\(.*\\\\)\", \"\") %&gt;%\n            str_trim() %&gt;%\n            simplify() %&gt;%\n            # Convert the resulting list to a tibble. This tibble has one column:\n            # value\n            # 1 Baalzebub\n            # 2 dogs\n            #...\n            as_tibble() %&gt;%\n            # Use tidyr::separate to separate the \"value\" column into two columns. The extra pieces get merged\n            # So for example \"1 Vlad the Impaler\" becomes \"1\" \"Vlad the Impaler\" instead of \"1\" \"Vlad\" which\n            # would be the case without \"extra = \"merge\"\"\n            separate(value, into = c(\"value\", \"monster\"), extra = \"merge\") %&gt;%\n            mutate(value = as.numeric(value)) %&gt;%\n            mutate(monster = str_to_lower(monster))\n\n        # This function singularizes names:\n        singularize_monsters &lt;- function(nethack_data){\n            nethack_data %&gt;%\n                mutate(monster = str_replace_all(monster, \"mummies\", \"mummy\"),\n                       monster = str_replace_all(monster, \"jellies\", \"jelly\"),\n                       monster = str_replace_all(monster, \"vortices\", \"vortex\"),\n                       monster = str_replace_all(monster, \"elves\", \"elf\"),\n                       monster = str_replace_all(monster, \"wolves\", \"wolf\"),\n                       monster = str_replace_all(monster, \"dwarves\", \"dwarf\"),\n                       monster = str_replace_all(monster, \"liches\", \"lich\"),\n                       monster = str_replace_all(monster, \"baluchiteria\", \"baluchiterium\"),\n                       monster = str_replace_all(monster, \"homunculi\", \"homonculus\"),\n                       monster = str_replace_all(monster, \"mumakil\", \"mumak\"),\n                       monster = str_replace_all(monster, \"sasquatches\", \"sasquatch\"),\n                       monster = str_replace_all(monster, \"watchmen\", \"watchman\"),\n                       monster = str_replace_all(monster, \"zruties\", \"zruty\"),\n                       monster = str_replace_all(monster, \"xes$\", \"x\"),\n                       monster = str_replace_all(monster, \"s$\", \"\"))\n        }\n\n        result &lt;- singularize_monsters(result)\n    }\n    # If a player did not genocide or extinct any species, return the result:\n    if(any(str_detect(dumplog, \"No species were genocided or became extinct.\"))){\n        result &lt;- result %&gt;%\n            mutate(status = NA_character_)\n        return(result)\n    } else {\n\n        # If the player genocided or extincted species, add this info:\n        start &lt;- dumplog %&gt;% # &lt;- dectect the start of the list\n            str_which(\"Genocided or extinct species:\") # &lt;- sometimes this does not appear in the xlogfile\n\n        end &lt;- dumplog %&gt;% # &lt;- detect the end of the list\n            str_which(\"Voluntary challenges\")\n\n       if(is_empty(start)){# This deals with the situation start does not exist\n           start &lt;- end - 2\n       }\n\n        list_creatures &lt;- dumplog[(start + 1):(end - 1)] %&gt;% # &lt;- extract the list\n            str_trim() # &lt;- trim white space\n\n        extinct_species &lt;- list_creatures %&gt;%\n            str_extract_all(\"[:alpha:]+\\\\s(?=\\\\(extinct\\\\))\", simplify = T) %&gt;%\n            str_trim %&gt;%\n            discard(`==`(., \"\"))\n\n        extinct_species_df &lt;- tibble(monster = extinct_species, status = \"extinct\")\n\n        genocided_species_index &lt;- list_creatures %&gt;%\n            str_detect(pattern = \"extinct|species\") %&gt;%\n            `!`\n\n        genocided_species &lt;- list_creatures[genocided_species_index]\n\n        genocided_species_df &lt;- tibble(monster = genocided_species, status = \"genocided\")\n\n        genocided_or_extinct_df &lt;- singularize_monsters(bind_rows(extinct_species_df, genocided_species_df))\n\n        result &lt;- full_join(result, genocided_or_extinct_df, by = \"monster\") %&gt;%\n            filter(monster != \"\") # &lt;- this is to remove lines that were added by mistake, for example if start was empty\n\n        return(result)\n    }\n}"
  },
  {
    "objectID": "posts/2018-03-03-sparklyr_h2o_rsparkling.html",
    "href": "posts/2018-03-03-sparklyr_h2o_rsparkling.html",
    "title": "Getting {sparklyr}, {h2o}, {rsparkling} to work together and some fun with bash",
    "section": "",
    "text": "This is going to be the type of blog posts that would perhaps be better as a gist, but it is easier for me to use my blog as my own personal collection of gists. Plus, someone else might find this useful, so here it is! In this blog post I am going to show a little trick to randomly sample rows from a text file using bash, and then train a model using the {h2o} package. I will also use the {rsparkling} package. From {rsparkling}’s documentation: {rsparkling} is a package that provides an R interface to the H2O Sparkling Water machine learning library. and will be needed to transfer the data from Spark to H2O.\n\n\nIn a previous blog post I used the {sparklyr} package to load a 30GB csv file into R. I created the file by combining around 300 csv files, each around 80MB big. Here, I would like to use the machine learning functions included in the {h2o} packages to train a random forest on this data. However, I only want to have a simple prototype that simply runs, and check if all the packages work well together. If everything is ok, I’ll keep iterating to make the model better (in a possible subsequent post).\n\n\nFor fast prototyping, using 30GB of data is not a good idea, so I am going to sample 500000 from this file using the linux command line (works on macOS too and also on Windows if you installed the linux subsystem). Why not use R to sample 500000 rows? Because on my machine, loading the 30GB file takes 25 minutes. Sampling half a million lines from it would take quite long too. So here are some bash lines that do that directly on the file, without needing to load it into R beforehand:\n\n[18-03-03 21:50] brodriguesco in /Documents/AirOnTimeCSV ➤ get_seeded_random()\n{\n  seed=\"$1\"\n  openssl enc -aes-256-ctr -pass pass:\"$seed\" -nosalt \\\n  &lt;/dev/zero 2&gt;/dev/null\n}\n\n[18-03-03 21:50] brodriguesco in /Documents/AirOnTimeCSV ➤ sed \"1 d\" combined.csv | shuf --random-source=&lt;(get_seeded_random 42) -n 500000 &gt; small_combined_temp.csv\n\n[18-03-03 21:56] brodriguesco in /Documents/AirOnTimeCSV ➤ head -1 combined.csv &gt; colnames.csv\n\n[18-03-03 21:56] brodriguesco in /Documents/AirOnTimeCSV ➤ cat colnames.csv small_combined_temp.csv &gt; small_combined.csv\n\nThe first function I took from the gnu coreutils manual which allows me to fix the random seed to reproduce the same sampling of the file. Then I use \"sed 1 d\" cobmined.csv to remove the first line of combined.csv which is the header of the file. Then, I pipe the result of sed using | to shuf which does the shuffling. The option –random-source=&lt;(get_seeded_random 42) fixes the seed, and -n 500000 only shuffles 500000 and not the whole file. The final bit of the line, &gt; small_combined_temp.csv, saves the result to small_cobmined_temp.csv. Because I need to add back the header, I use head -1 to extract the first line of combined.csv and save it into colnames.csv. Finally, I bind the rows of both files using cat colnames.csv small_combined_temp.csv and save the result into small_combined.cvs. Taken together, all these steps took about 5 minutes (without counting the googling around for finding how to pass a fixed seed to shuf).\n\n\nNow that I have this small dataset, I can write a small prototype:\n\n\nFirst, you need to install {sparklyr}, {rsparkling} and {h2o}. Refer to this to know how to install the packages. I had a mismatch between the version of H2O that was automatically installed when I installed the {h2o} package, and the version of Spark that {sparklyr} installed but thankfully the {h2o} package returns a very helpful error message with the following lines:\n\ndetach(\"package:rsparkling\", unload = TRUE)\n                       if (\"package:h2o\" %in% search()) { detach(\"package:h2o\", unload = TRUE) }\n                       if (isNamespaceLoaded(\"h2o\")){ unloadNamespace(\"h2o\") }\n                       remove.packages(\"h2o\")\n                       install.packages(\"h2o\", type = \"source\", repos = \"https://h2o-release.s3.amazonaws.com/h2o/rel-weierstrass/2/R\")\n\nwhich tells you which version to install.\n\n\nSo now, let’s load everything:\n\nlibrary(sparklyr)\nlibrary(rsparkling)\nlibrary(h2o)\n## \n## ----------------------------------------------------------------------\n## \n## Your next step is to start H2O:\n##     &gt; h2o.init()\n## \n## For H2O package documentation, ask for help:\n##     &gt; ??h2o\n## \n## After starting H2O, you can use the Web UI at http://localhost:54321\n## For more information visit http://docs.h2o.ai\n## \n## ----------------------------------------------------------------------\n## \n## Attaching package: 'h2o'\n## The following objects are masked from 'package:stats':\n## \n##     cor, sd, var\n## The following objects are masked from 'package:base':\n## \n##     &&, %*%, %in%, ||, apply, as.factor, as.numeric, colnames,\n##     colnames&lt;-, ifelse, is.character, is.factor, is.numeric, log,\n##     log10, log1p, log2, round, signif, trunc\nh2o.init()\n## \n## H2O is not running yet, starting it now...\n## \n## Note:  In case of errors look at the following log files:\n##     /tmp/Rtmph48vf9/h2o_cbrunos_started_from_r.out\n##     /tmp/Rtmph48vf9/h2o_cbrunos_started_from_r.err\n## \n## \n## Starting H2O JVM and connecting: .. Connection successful!\n## \n## R is connected to the H2O cluster: \n##     H2O cluster uptime:         1 seconds 944 milliseconds \n##     H2O cluster version:        3.16.0.2 \n##     H2O cluster version age:    4 months and 15 days !!! \n##     H2O cluster name:           H2O_started_from_R_cbrunos_bpn152 \n##     H2O cluster total nodes:    1 \n##     H2O cluster total memory:   6.98 GB \n##     H2O cluster total cores:    12 \n##     H2O cluster allowed cores:  12 \n##     H2O cluster healthy:        TRUE \n##     H2O Connection ip:          localhost \n##     H2O Connection port:        54321 \n##     H2O Connection proxy:       NA \n##     H2O Internal Security:      FALSE \n##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 \n##     R Version:                  R version 3.4.4 (2018-03-15)\n## Warning in h2o.clusterInfo(): \n## Your H2O cluster version is too old (4 months and 15 days)!\n## Please download and install the latest version from http://h2o.ai/download/\n\nI left all the startup messages because they’re quite helpful. Especially that bit telling you to start H2O with h2o.init(). If something’s wrong, h2o.init() will give you helpful information.\n\n\nNow that all this is loaded, I can start working on the data (the steps below are explained in detail in my previous blog post):\n\nspark_dir = \"/my_2_to_disk/spark/\"\n\nconfig = spark_config()\n\nconfig$`sparklyr.shell.driver-memory` &lt;- \"4G\"\nconfig$`sparklyr.shell.executor-memory` &lt;- \"4G\"\nconfig$`spark.yarn.executor.memoryOverhead` &lt;- \"512\"\nconfig$`sparklyr.shell.driver-java-options` = paste0(\"-Djava.io.tmpdir=\", spark_dir)\n\nsc = spark_connect(master = \"local\", config = config)\n\nAnother useful function that allows you to check if everything is alright is h2o_context():\n\nh2o_context(sc)\n&lt;jobj[12]&gt;\n  org.apache.spark.h2o.H2OContext\n\nSparkling Water Context:\n * H2O name: sparkling-water-cbrunos_local-1520111879840\n * cluster size: 1\n * list of used nodes:\n  (executorId, host, port)\n  ------------------------\n  (driver,127.0.0.1,54323)\n  ------------------------\n\n  Open H2O Flow in browser: http://127.0.0.1:54323 (CMD + click in Mac OSX)\n\n\nNow, let’s load the data into R with {sparklyr}:\n\nair = spark_read_csv(sc, name = \"air\", path = \"small_combined.csv\")\n\nOf course, here, using Spark is overkill, because small_combined.csv is only around 100MB big, so no need for {sparklyr} but as stated in the beginning this is only to have a quick and dirty prototype. Once all the pieces are working together, I can iterate on the real data, for which {sparklyr} will be needed. Now, if I needed to use {dplyr} I could use it on air, but I don’t want to do anything on it, so I convert it to a h2o data frame. h2o data frames are needed as arguments for the machine learning algorithms included in the {h2o} package. as_h2o_frame() is a function included in {rsparkling}:\n\nair_hf = as_h2o_frame(sc, air)\n\nThen, I convert the columns I need to factors (I am only using factors here):\n\nair_hf$ORIGIN = as.factor(air_hf$ORIGIN)\nair_hf$UNIQUE_CARRIER = as.factor(air_hf$UNIQUE_CARRIER)\nair_hf$DEST = as.factor(air_hf$DEST)\n\n{h2o} functions need the names of the predictors and of the target columns, so let’s define that:\n\ntarget = \"ARR_DELAY\"\npredictors = c(\"UNIQUE_CARRIER\", \"ORIGIN\", \"DEST\")\n\nNow, let’s train a random Forest, without any hyper parameter tweaking:\n\nmodel = h2o.randomForest(predictors, target, training_frame = air_hf)\n\nNow that this runs, I will in the future split the data into training, validation and test set, and train a model with better hyper parameters. For now, let’s take a look at the summary of model:\n\nsummary(model)\nModel Details:\n==============\n\nH2ORegressionModel: drf\nModel Key:  DRF_model_R_1520111880605_1\nModel Summary:\n  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n1              50                       50            11055998        20\n  max_depth mean_depth min_leaves max_leaves mean_leaves\n1        20   20.00000       1856       6129  4763.42000\n\nH2ORegressionMetrics: drf\n** Reported on training data. **\n** Metrics reported on Out-Of-Bag training samples **\n\nMSE:  964.9246\nRMSE:  31.06324\nMAE:  17.65517\nRMSLE:  NaN\nMean Residual Deviance :  964.9246\n\n\n\n\n\nScoring History:\n             timestamp   duration number_of_trees training_rmse training_mae\n1  2018-03-03 22:52:24  0.035 sec               0\n2  2018-03-03 22:52:25  1.275 sec               1      30.93581     17.78216\n3  2018-03-03 22:52:25  1.927 sec               2      31.36998     17.78867\n4  2018-03-03 22:52:26  2.272 sec               3      31.36880     17.80359\n5  2018-03-03 22:52:26  2.564 sec               4      31.29683     17.79467\n6  2018-03-03 22:52:26  2.854 sec               5      31.31226     17.79467\n7  2018-03-03 22:52:27  3.121 sec               6      31.26214     17.78542\n8  2018-03-03 22:52:27  3.395 sec               7      31.20749     17.75703\n9  2018-03-03 22:52:27  3.666 sec               8      31.19706     17.74753\n10 2018-03-03 22:52:27  3.935 sec               9      31.16108     17.73547\n11 2018-03-03 22:52:28  4.198 sec              10      31.13725     17.72493\n12 2018-03-03 22:52:32  8.252 sec              27      31.07608     17.66648\n13 2018-03-03 22:52:36 12.462 sec              44      31.06325     17.65474\n14 2018-03-03 22:52:38 14.035 sec              50      31.06324     17.65517\n   training_deviance\n1\n2          957.02450\n3          984.07580\n4          984.00150\n5          979.49147\n6          980.45794\n7          977.32166\n8          973.90720\n9          973.25655\n10         971.01272\n11         969.52856\n12         965.72249\n13         964.92530\n14         964.92462\n\nVariable Importances: (Extract with `h2o.varimp`)\n=================================================\n\nVariable Importances:\n        variable relative_importance scaled_importance percentage\n1         ORIGIN    291883392.000000          1.000000   0.432470\n2           DEST    266749168.000000          0.913890   0.395230\n3 UNIQUE_CARRIER    116289536.000000          0.398411   0.172301\n&gt;\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2016-12-21-work-on-lists-of-datasets-instead-of-individual-datasets-by-using-functional-programming.html",
    "href": "posts/2016-12-21-work-on-lists-of-datasets-instead-of-individual-datasets-by-using-functional-programming.html",
    "title": "Work on lists of datasets instead of individual datasets by using functional programming",
    "section": "",
    "text": "Analyzing a lot of datasets can be tedious. In my work, I often have to compute descriptive statistics, or plot some graphs for some variables for a lot of datasets. The variables in question have the same name accross the datasets but are measured for different years. As an example, imagine you have this situation:\n\n\ndata2000 &lt;- mtcars\ndata2001 &lt;- mtcars\n\n\nFor the sake of argument, imagine that data2000 is data from a survey conducted in the year 2000 and data2001 is the same survey but conducted in the year 2001. For illustration purposes, I use the mtcars dataset, but I could have used any other example. In these sort of situations, the variables are named the same in both datasets. Now if I want to check the summary statistics of a variable, I might do it by running:\n\n\nsummary(data2000$cyl)\nsummary(data2001$cyl)\n\n\nbut this can get quite tedious, especially if instead of only having two years of data, you have 20 years. Another possibility is to merge both datasets and then check the summary statistics of the variable of interest. But this might require a lot of preprocessing, and sometimes you really just want to do a quick check, or some dirty graphs. So you might be tempted to write a loop, which would require to put these two datasets in some kind of structure, such as a list:\n\n\nlist_data &lt;- list(\"data2000\" = data2000, \"data2001\" = data2001)\n\nfor (i in 1:2){\n    print(summary(list_data[[i]]$cyl))\n}\n\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000 \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000\n\nBut this also might get tedious, especially if you want to do this for a lot of different variables, and want to use different functions than summary().\n\n\nAnother, simpler way of doing this, is to use purrr::map() or lapply(). But there is a catch though: how do we specify the column we want to work on? Let’s try some things out:\n\n\nlibrary(purrr)\n\nmap(list_data, summary(cyl))\n\nError in summary(cyl) : object 'cyl' not found\n\nMaybe this will work:\n\n\nmap(list_data, summary, cyl)\n\n## $data2000\n     mpg             cyl             disp             hp       \nMin.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \nMedian :19.20   Median :6.000   Median :196.3   Median :123.0  \nMean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \nMax.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n     drat             wt             qsec             vs        \nMin.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \nMedian :3.695   Median :3.325   Median :17.71   Median :0.0000  \nMean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \nMax.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n      am              gear            carb      \nMin.   :0.0000   Min.   :3.000   Min.   :1.000  \n1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \nMedian :0.0000   Median :4.000   Median :2.000  \nMean   :0.4062   Mean   :3.688   Mean   :2.812  \n3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \nMax.   :1.0000   Max.   :5.000   Max.   :8.000  \n\ndata2001\n     mpg             cyl             disp             hp       \nMin.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \nMedian :19.20   Median :6.000   Median :196.3   Median :123.0  \nMean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \nMax.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n     drat             wt             qsec             vs        \nMin.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \nMedian :3.695   Median :3.325   Median :17.71   Median :0.0000  \nMean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \nMax.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n      am              gear            carb      \nMin.   :0.0000   Min.   :3.000   Min.   :1.000  \n1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \nMedian :0.0000   Median :4.000   Median :2.000  \nMean   :0.4062   Mean   :3.688   Mean   :2.812  \n3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \nMax.   :1.0000   Max.   :5.000   Max.   :8.000\n\nNot quite! You get the summary statistics of every variable, cyl simply gets ignored. This might be ok in our small toy example, but if you have dozens of datasets with hundreds of variables, the output becomes unreadable. The solution is to use an anonymous functions:\n\n\nmap(list_data, (function(x) summary(x$cyl)))\n\n## $data2000\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000 \n\n$data2001\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000\n\nThis is, in my opinion, much more readable than a loop, and the output of this is another list, so it’s easy to save it:\n\n\nsummary_cyl &lt;- map(list_data, (function(x) summary(x$cyl)))\nstr(summary_cyl)\n\n## List of 2\n$ data2000:Classes 'summaryDefault', 'table'  Named num [1:6] 4 4 6 6.19 8 ...\n .. ..- attr(*, \"names\")= chr [1:6] \"Min.\" \"1st Qu.\" \"Median\" \"Mean\" ...\n$ data2001:Classes 'summaryDefault', 'table'  Named num [1:6] 4 4 6 6.19 8 ...\n .. ..- attr(*, \"names\")= chr [1:6] \"Min.\" \"1st Qu.\" \"Median\" \"Mean\" ...\n\nWith the loop, you would need to “allocate” an empty list that you would fill at each iteration.\n\n\nSo this is already nice, but wouldn’t it be nicer to simply have to type:\n\n\nsummary(list_data$cyl)\n\n\nand have the summary of variable cyl for each dataset in the list? Well it is possible with the following function I wrote to make my life easier:\n\n\nto_map &lt;- function(func){\n  function(list, column, ...){\n    if(missing(column)){\n        res &lt;- purrr::map(list, (function(x) func(x, ...)))\n      } else {\n        res &lt;- purrr::map(list, (function(x) func(x[column], ...)))\n             }\n    res\n  }\n}\n\n\nBy following this chapter of Hadley Wickham’s book, Advanced R, I was able to write this function. What does it do? It basically generalizes a function to work on a list of datasets instead of just on a dataset. So for example, in the case of summary():\n\n\nsummarymap &lt;- to_map(summary)\n\nsummarymap(list_data, \"cyl\")\n\n$data2000\n     cyl       \nMin.   :4.000  \n1st Qu.:4.000  \nMedian :6.000  \nMean   :6.188  \n3rd Qu.:8.000  \nMax.   :8.000  \n\n$data2001\n     cyl       \nMin.   :4.000  \n1st Qu.:4.000  \nMedian :6.000  \nMean   :6.188  \n3rd Qu.:8.000  \nMax.   :8.000\n\nSo now everytime I want to have summary statistics for a variable, I just need to use summarymap():\n\n\nsummarymap(list_data, \"mpg\")\n\n## $data2000\n      mpg       \n Min.   :10.40  \n 1st Qu.:15.43  \n Median :19.20  \n Mean   :20.09  \n 3rd Qu.:22.80  \n Max.   :33.90  \n\n$data2001\n      mpg       \n Min.   :10.40  \n 1st Qu.:15.43  \n Median :19.20  \n Mean   :20.09  \n 3rd Qu.:22.80  \n Max.   :33.90\n\nIf I want the summary statistics for every variable, I simply omit the column name:\n\n\nsummarymap(list_data)\n\n$data2000\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n$data2001\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000\n\nI can use any function:\n\n\ntablemap &lt;- to_map(table)\n\ntablemap(list_data, \"cyl\")\n\n## $data2000\n\n 4  6  8 \n11  7 14 \n\n$data2001\n\n 4  6  8 \n11  7 14\n\ntablemap(list_data, \"mpg\")\n\n## $data2000\n\n10.4 13.3 14.3 14.7   15 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7 19.2 19.7 \n   2    1    1    1    1    2    1    1    1    1    1    1    1    2    1 \n  21 21.4 21.5 22.8 24.4   26 27.3 30.4 32.4 33.9 \n   2    2    1    2    1    1    1    2    1    1 \n\n$data2001\n\n10.4 13.3 14.3 14.7   15 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7 19.2 19.7 \n   2    1    1    1    1    2    1    1    1    1    1    1    1    2    1 \n  21 21.4 21.5 22.8 24.4   26 27.3 30.4 32.4 33.9 \n   2    2    1    2    1    1    1    2    1    1\n\nI hope you will find this little function useful, and as usual, for any comments just drop me an email by clicking the red enveloppe in the top right corner or tweet me."
  },
  {
    "objectID": "posts/2013-01-16-simulated-maximum-likelihood-with-r.html",
    "href": "posts/2013-01-16-simulated-maximum-likelihood-with-r.html",
    "title": "Simulated Maximum Likelihood with R",
    "section": "",
    "text": "This document details section 12.4.5. Unobserved Heterogeneity Example from Cameron and Trivedi’s book - MICROECONOMETRICS: Methods and Applications. The original source code giving the results from table 12.2 are available from the authors' site here and written for Stata. This is an attempt to translate the code to R. I’d like to thank Reddit user anonemouse2010 for his advice which helped me write the function.\n\n\nConsult the original source code if you want to read the authors' comments. If you want the R source code without all the commentaries, grab it here. This is not guaranteed to work, nor to be correct. It could set your pet on fire and/or eat your first born. Use at your own risk. I may, or may not, expand this example. Corrections, constructive criticism are welcome.\n\n\nThe model is \\(y=\\theta+u+\\varepsilon\\) where \\(\\theta\\) is a scalar parameter equal to 1. \\(u\\) is extreme value type 1 (Gumbel distribution), \\(\\varepsilon \\leadsto \\mathbb{N}(0,1)\\). For more details, consult the book.\n\n\nImport the data\n\n\nYou can consult the original source code to see how the authors simulated the data. To get the same results, and verify that I didn't make mistakes I prefer importing their data directly from their website.\n\n\ndata &lt;- read.table(\"http://cameron.econ.ucdavis.edu/mmabook/mma12p2mslmsm.asc\")\nu &lt;- data[, 1]\ne &lt;- data[, 2]\ny &lt;- data[, 3]\nnumobs &lt;- length(u)\nsimreps &lt;- 10000\n\n\nSimulation\n\n\nIn the code below, the following likelihood function:\n\\[\\log{\\hat{L}_N(\\theta)} = \\dfrac{1}{N} \\sum_{i=1}^N\\log{\\big( \\dfrac{1}{S}\\sum_{s=1}^S \\dfrac{1}{\\sqrt{2\\pi}} \\exp \\{ -(-y_i-\\theta-u_i^s)^2/2 \\}\\big)}\\]\nwhich can be found on page 397 is programmed using the function sapply.\n\n\ndenssim &lt;- function(theta) {\n    loglik &lt;- mean(sapply(y, function(y) log(mean((1/sqrt(2 * pi)) * exp(-(y - theta + log(-log(runif(simreps))))^2/2)))))\n    return(-loglik)\n}\n\n\nThis likelihood is then maximized:\n\n\nsystem.time(res &lt;- optim(0.1, denssim, method = \"BFGS\", control = list(maxit = simreps)))\n\n   user  system elapsed \n  1.460   0.197   1.657 \n\n\n\nConvergence is achieved pretty rapidly, to\n\n\nres\n\n$par\n[1] 1.193006\n\n$value\n[1] 1.921685\n\n$counts\nfunction gradient \n      58       10 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\nwhich is close to the true value of the parameter 1 (which was used to generate the data).\n\n\nLet's try again with another parameter value, for example ( ). We have to generate y again:\n\n\ny2 &lt;- 2.5 + u + e\n\n\nand slightly modify the likelihood:\n\n\ndenssim2 &lt;- function(theta) {\n  loglik &lt;- mean(\n    sapply(\n      y2,\n      function(y2) log(mean((1/sqrt(2 * pi)) * exp(-(y2 - theta + log(-log(runif(simreps))))^2/2)))))\n  return(-loglik)\n}\n\n\nwhich can then be maximized:\n\n\nsystem.time(res2 &lt;- optim(0.1, denssim2, method = \"BFGS\", control = list(maxit = simreps)))\n\n   user  system elapsed \n  1.321   0.000   1.322 \n\n\n\nThe value that maximizes the likelihood is:\n\n\nres2\n\n$par\n[1] 2.73753\n\n$value\n[1] 1.92257\n\n$counts\nfunction gradient \n      49       10 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\nwhich is close to the true value of the parameter 2.5 (which was used to generate the data)."
  },
  {
    "objectID": "posts/2013-11-07-gmm-with-rmd.html",
    "href": "posts/2013-11-07-gmm-with-rmd.html",
    "title": "Nonlinear Gmm with R - Example with a logistic regression",
    "section": "",
    "text": "In this post, I will explain how you can use the R gmm package to estimate a non-linear model, and more specifically a logit model. For my research, I have to estimate Euler equations using the Generalized Method of Moments. I contacted Pierre Chaussé, the creator of the gmm library for help, since I was having some difficulties. I am very grateful for his help (without him, I'd still probably be trying to estimate my model!).\n\n\nTheoretical background, motivation and data set\n\n\nI will not dwell in the theory too much, because you can find everything you need here. I think it’s more interesting to try to understand why someone would use the Generalized Method of Moments instead of maximization of the log-likelihood. Well, in some cases, getting the log-likelihood can be quite complicated, as can be the case for arbitrary, non-linear models (for example if you want to estimate the parameters of a very non-linear utility function). Also, moment conditions can sometimes be readily available, so using GMM instead of MLE is trivial. And finally, GMM is… well, a very general method: every popular estimator can be obtained as a special case of the GMM estimator, which makes it quite useful.\n\n\nAnother question that I think is important to answer is: why this post? Well, because that’s exactly the kind of post I would have loved to have found 2 months ago, when I was beginning to work with the GMM. Most posts I found presented the gmm package with very simple and trivial examples, which weren’t very helpful. The example presented below is not very complicated per se, but much more closer to a real-world problem than most stuff that is out there. At least, I hope you will find it useful!\n\n\nFor illustration purposes, I'll use data from Marno Verbeek's A guide to modern Econometrics, used in the illustration on page 197. You can download the data from the book's companion page here under the section Data sets or from the Ecdat package in R, which I’ll be using.\n\n\nImplementation in R\n\n\nI don't estimate the exact same model, but only use a subset of the variables available in the data set. Keep in mind that this post is just for illustration purposes.\n\n\nFirst load the gmm package and load the data set:\n\n\nlibrary(gmm)\nlibrary(Ecdat)\ndata(\"Benefits\")\n\nBenefits &lt;- transform(\n  Benefits,\n  age2 = age**2,\n  rr2 = rr**2\n  )\n\n\nWe can then estimate a logit model with the glm() function:\n\n\nnative &lt;- glm(ui ~ age + age2 + dkids + dykids + head + male + married + rr + rr2,\n              data = Benefits,\n              family = binomial(link = \"logit\"),\n              na.action = na.pass)\n\nsummary(native)\n\n## \n## Call:\n## glm(formula = y ~ age + age2 + dkids + dykids + head + male + \n##     married + rr + rr2, family = binomial(link = \"logit\"), na.action = na.pass)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -1.889  -1.379   0.788   0.896   1.237  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)   \n## (Intercept) -1.00534    0.56330   -1.78   0.0743 . \n## age          0.04909    0.02300    2.13   0.0328 * \n## age2        -0.00308    0.00293   -1.05   0.2924   \n## dkids       -0.10922    0.08374   -1.30   0.1921   \n## dykids       0.20355    0.09490    2.14   0.0320 * \n## head        -0.21534    0.07941   -2.71   0.0067 **\n## male        -0.05988    0.08456   -0.71   0.4788   \n## married      0.23354    0.07656    3.05   0.0023 **\n## rr           3.48590    1.81789    1.92   0.0552 . \n## rr2         -5.00129    2.27591   -2.20   0.0280 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 6086.1  on 4876  degrees of freedom\n## Residual deviance: 5983.9  on 4867  degrees of freedom\n## AIC: 6004\n## \n## Number of Fisher Scoring iterations: 4\n\n\nNow comes the interesting part: how can you estimate such a non-linear model with the gmm() function from the gmm package?\n\n\nFor every estimation with the Generalized Method of Moments, you will need valid moment conditions. It turns out that in the case of the logit model, this moment condition is quite simple:\n\n\\[E[X' * (Y-\\Lambda(X'\\theta))] = 0\\]\n\nwhere ( () ) is the logistic function. Let's translate this condition into code. First, we need the logistic function:\n\n\nlogistic &lt;- function(theta, data) {\n    return(1/(1 + exp(-data %*% theta)))\n}\n\n\nand let's also define a new data frame, to make our life easier with the moment conditions (don’t forget to add a column of ones to the matrix, hence the 1 after y):\n\n\ndat &lt;- data.matrix(with(Benefits,\n                        cbind(ui, 1, age, age2, dkids,\n                              dykids, head, sex,\n                              married, rr, rr2)))\n\n\nand now the moment condition itself:\n\n\nmoments &lt;- function(theta, data) {\n  y &lt;- as.numeric(data[, 1])\n  x &lt;- data.matrix(data[, 2:11])\n  m &lt;- x * as.vector((y - logistic(theta, x)))\n  return(cbind(m))\n}\n\n\nThe moment condition(s) are given by a function which returns a matrix with as many columns as moment conditions (same number of columns as parameters for just-identified models).\n\n\nTo use the gmm() function to estimate our model, we need to specify some initial values to get the maximization routine going. One neat trick is simply to use the coefficients of a linear regression; I found it to work well in a lot of situations:\n\n\ninit &lt;- (lm(ui ~ age + age2 + dkids + dykids + head + sex + married + rr + rr2,\n            data = Benefits))$coefficients\n\n\nAnd finally, we have everything to use gmm():\n\n\nmy_gmm &lt;- gmm(moments, x = dat, t0 = init, type = \"iterative\", crit = 1e-25, wmatrix = \"optimal\", method = \"Nelder-Mead\", control = list(reltol = 1e-25, maxit = 20000))\n\nsummary(my_gmm)\n\n\nPlease, notice the options crit=1e-25,method=“Nelder-Mead”,control=list(reltol=1e-25,maxit=20000): these options mean that the Nelder-Mead algorithm is used, and to specify further options to the Nelder-Mead algorithm, the control option is used. This is very important, as Pierre Chaussé explained to me: non-linear optimization is an art, and most of the time the default options won't cut it and will give you false results. To add insult to injury, the Generalized Method of Moments itself is very capricious and you will also have to play around with different initial values to get good results. As you can see, the Convergence code equals 10, which is a code specific to the Nelder-Mead method which indicates «degeneracy of the Nelder–Mead simplex.» . I’m not sure if this is a bad thing though, but other methods can give you better results. I’d suggest you try always different maximization routines with different starting values to see if your estimations are robust. Here, the results are very similar to what we obtained with the built-in function glm() so we can stop here.\n\n\nShould you notice any error whatsoever, do not hesitate to tell me."
  },
  {
    "objectID": "posts/2019-02-04-newspapers_shiny_app_tutorial.html#introduction",
    "href": "posts/2019-02-04-newspapers_shiny_app_tutorial.html#introduction",
    "title": "Building a shiny app to explore historical newspapers: a step-by-step guide",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nI started off this year by exploring a world that was unknown to me, the world of historical newspapers. I did not know that historical newspapers data was a thing, and have been thoroughly enjoying myself exploring the different datasets published by the National Library of Luxembourg. You can find the data here.\n\n\nIn my first blog post, I analyzed data from L’indépendence Luxembourgeoise. I focused on the ads, which were for the most part in the 4th and last page of the newspaper. I did so by extracting the data from the ALTO files. ALTO files contain the content of the newspapers, (basically, the words that make up the article). For this first exercise, I disregarded the METS files, for two reasons. First, I simply wanted to have something quick, and get used to the data. And second, I did not know about ALTO and METS files enough to truly make something out of them. The problem of disregarding the METS file is that I only had a big dump of words, and did not know which words came from which article, or ad in this case.\n\n\nIn the second blog post), I extracted data from the L’Union newspaper, this time by using the metadata from the METS files too. By combining the data from the ALTO files with the metadata from the METS files, I know which words came from which article, which would make further analysis much more interesting.\n\n\nIn the third blog post of this series, I built a Shiny app which makes it easy to explore the 10 years of publications of L’Union. In this blog post, I will explain in great detail how I created this app."
  },
  {
    "objectID": "posts/2019-02-04-newspapers_shiny_app_tutorial.html#part-1-getting-the-data-ready-for-the-shiny-app",
    "href": "posts/2019-02-04-newspapers_shiny_app_tutorial.html#part-1-getting-the-data-ready-for-the-shiny-app",
    "title": "Building a shiny app to explore historical newspapers: a step-by-step guide",
    "section": "\nPart 1: Getting the data ready for the Shiny app\n",
    "text": "Part 1: Getting the data ready for the Shiny app\n\n\n\nStep 1: Extracting the needed data\n\n\nIf you want to follow along with a dataset from a single publication, you can download the following archive on dropbox. Extract this archive, and you will find the data exactly as you would get it from the the big archive you can download from the website of the National Library of Luxembourg. However, to keep the size of the archive small, I removed the .pdf and .jpeg scans.\n\n\nIn the second blog post) I wrote some functions that made extracting the needed data from the files easy. However, after I wrote the article, I noticed that in some cases these functions were not working exactly as intended. I rewrote them a little bit to overcome these issues. You can find the code I used right below. I won’t explain it too much, because you can read the details in the previous blog post. However, should something be unclear, just drop me an email or a tweet!\n\n\n\n\nClick if you want to see the code\n\n\n# This functions will be used within the next functions to extract the relevant pieces\n\nextractor &lt;- function(string, regex, all = FALSE){\n    if(all) {\n        string %&gt;%\n            str_extract_all(regex) %&gt;%\n            flatten_chr() %&gt;%\n            str_remove_all(\"=|\\\\\\\"\") %&gt;%\n            #str_extract_all(\"[:alnum:]+|.|,|\\\\?|!\", simplify = FALSE) %&gt;%\n            map(paste, collapse = \"\") %&gt;%\n            flatten_chr()\n    } else {\n        string %&gt;%\n            str_extract(regex) %&gt;%\n            str_remove_all(\"=|\\\\\\\"\") %&gt;%\n            #str_extract_all(\"[:alnum:]+|.|,|\\\\?|!\", simplify = TRUE) %&gt;%\n            paste(collapse = \" \") %&gt;%\n            tolower()\n    }\n}\n\n# This function extracts the data from the METS files, and returns a tibble:\n\nextract_mets &lt;- function(article){\n    id &lt;- article %&gt;%\n        extractor(\"(?&lt;=ID)(.*?)(?=LABEL)\")\n\n    label &lt;- article %&gt;%\n        extractor(\"(?&lt;=LABEL)(.*?)(?=TYPE)\")\n\n    type &lt;- article %&gt;%\n        extractor(\"(?&lt;=TYPE)(.*?)(?=&gt;)\")\n\n    begins &lt;- article %&gt;%\n        extractor(\"(?&lt;=BEGIN)(.*?)(?=BETYPE)\", all = TRUE)\n\n    tibble::tribble(~label, ~type, ~begins, ~id,\n                    label, type, begins, id) %&gt;%\n        unnest()\n}\n\n# This function extracts the data from the ALTO files, and also returns a tibble:\n\nextract_alto &lt;- function(article){\n    begins &lt;- article[1] %&gt;%\n        extractor(\"(?&lt;=^ID)(.*?)(?=HPOS)\", all = TRUE)\n\n    content &lt;- article %&gt;%\n        extractor(\"(?&lt;=CONTENT)(.*?)(?=WC)\", all = TRUE)\n\n    tibble::tribble(~begins, ~content,\n                    begins, content) %&gt;%\n        unnest()\n}\n\n# This function takes the path to a page as an argument, and extracts the data from \n# each article using the function defined above. It then writes a flat CSV to disk.\n\nalto_csv &lt;- function(page_path){\n\n    page &lt;- read_file(page_path)\n\n    doc_name &lt;- str_extract(page_path, \"(?&lt;=/text/).*\")\n\n    alto_articles &lt;- page %&gt;%\n        str_split(\"TextBlock \") %&gt;%\n        flatten_chr()\n\n    alto_df &lt;- map_df(alto_articles, extract_alto)\n\n    alto_df &lt;- alto_df %&gt;%\n        mutate(document = doc_name)\n\n    write_csv(alto_df, paste0(page_path, \".csv\"))\n}\n\n# Same as above, but for the METS file:\n\nmets_csv &lt;- function(page_path){\n\n    page &lt;- read_file(page_path)\n\n    doc_name &lt;- str_extract(page_path, \"(?&lt;=/).*\")\n\n    mets_articles &lt;- page %&gt;%\n        str_split(\"DMDID\") %&gt;%\n        flatten_chr()\n\n    mets_df &lt;- map_df(mets_articles, extract_mets)\n\n    mets_df &lt;- mets_df %&gt;%\n        mutate(document = doc_name)\n\n    write_csv(mets_df, paste0(page_path, \".csv\"))\n}\n\n# Time to use the above defined functions. First, let's save the path of all the ALTO files\n# into a list:\n\npages_alto &lt;- str_match(list.files(path = \"./\", all.files = TRUE, recursive = TRUE), \".*/text/.*.xml\") %&gt;%\n    discard(is.na)\n\n# I use the {furrr} library to do the extraction in parallel, using 8 cores:\n\nlibrary(furrr)\n\nplan(multiprocess, workers = 8)\n\ntic &lt;- Sys.time()\nfuture_map(pages_alto, alto_csv)\ntoc &lt;- Sys.time()\n\ntoc - tic\n\n#Time difference of 18.64776 mins\n\n\n# Same for the METS files:\n\npages_mets &lt;- str_match(list.files(path = \"./\", all.files = TRUE, recursive = TRUE), \".*mets.xml\") %&gt;%\n    discard(is.na)\n\n\nlibrary(furrr)\n\nplan(multiprocess, workers = 8)\n\ntic &lt;- Sys.time()\nfuture_map(pages_mets, mets_csv)\ntoc &lt;- Sys.time()\n\ntoc - tic\n\n#Time difference of 18.64776 mins\n\n\nIf you want to try the above code for one ALTO and METS files, you can use the following lines (use the download link in the beginning of the blog post to get the required data):\n\n\n\n\nClick if you want to see the code\n\n\nmets &lt;- read_file(\"1533660_newspaper_lunion_1860-11-14/1533660_newspaper_lunion_1860-11-14-mets.xml\")\n\nmets_articles2 &lt;- mets %&gt;%\n    str_split(\"DMDID\") %&gt;%\n    flatten_chr()\n\n\nalto &lt;- read_file(\"1533660_newspaper_lunion_1860-11-14/text/1860-11-14_01-00001.xml\")\n\nalto_articles &lt;- alto %&gt;%\n    str_split(\"TextBlock \") %&gt;%\n    flatten_chr()\n\nmets_df2 &lt;- mets_articles2 %&gt;%\n    map_df(extract_mets)\n\n# Same exercice for ALTO\n\nalto_df &lt;- alto_articles %&gt;%\n    map_df(extract_alto)\n\n\n\n\nStep 2: Joining the data and the metadata\n\n\nNow that I extracted the data from the ALTO files, and the metadata from the METS files, I still need to join both data sets and do some cleaning. What is the goal of joining these two sources? Remember, by doing this I will know which words come from which article, which will make things much easier later on. I explain how the code works as comments in the code block below:\n\n\n\n\nClick if you want to see the code\n\n\nlibrary(tidyverse)\nlibrary(udpipe)\nlibrary(textrank)\nlibrary(tidytext)\n\n# First, I need the path to each folder that contains the ALTO and METS files. Each newspaper\n# data is inside its own folder, one folder per publication. Inside, there's `text` folder that\n# contains the ALTO and METS files. This is also where I saved the .csv files from before.\n\npathdirs &lt;- list.dirs(recursive = FALSE) %&gt;%\n    str_match(\".*lunion.*\") %&gt;%\n    discard(is.na)\n\n# The following function imports the METS and the ALTO csv files, joins them, and does some \n# basic cleaning. I used a trick to detect German articles (even though L'Union is a French publication\n# some articles are in German) and then remove them.\n\ntidy_papers &lt;- function(path){\n    mets_path &lt;- paste0(path, \"/\", list.files(path, \".*.xml.csv\"))\n    mets_csv &lt;- data.table::fread(mets_path)\n\n    alto_path &lt;- paste0(path, \"/text/\", list.files(paste0(path, \"/text/\"), \".*.csv\"))\n    alto_csv &lt;- map_dfr(alto_path, data.table::fread)\n\n    final &lt;- full_join(alto_csv, mets_csv, by = \"begins\") %&gt;%\n        mutate(content = tolower(content)) %&gt;%\n        mutate(content = if_else(str_detect(content, \"hyppart1\"), str_extract_all(content, \"(?&lt;=CONTENT_).*\", simplify = TRUE), content)) %&gt;%\n        mutate(content = if_else(str_detect(content, \"hyppart2\"), NA_character_, content)) %&gt;%\n        # When words are separated by a hyphen and split over two lines, it looks like this in the data.\n        # ex SUBS_TYPEHypPart1 SUBS_CONTENTexceptée\n        # ceptée SUBS_TYPEHypPart2 SUBS_CONTENTexceptée\n        # Here, the word `exceptée` is split over two lines, so using a regular expression, I keep\n        # the string `exceptée`, which comes after the string `CONTENT`,  from the first line and \n        # replace the second line by an NA_character_\n        mutate(content = if_else(str_detect(content, \"superscript\"), NA_character_, content)) %&gt;%\n        mutate(content = if_else(str_detect(content, \"subscript\"), NA_character_, content)) %&gt;%\n        filter(!is.na(content)) %&gt;%\n        filter(type == \"article\") %&gt;%\n        group_by(id) %&gt;%\n        nest %&gt;%\n        # Below I create a list column with all the content of the article in a single string.\n        mutate(article_text = map(data, ~paste(.$content, collapse = \" \"))) %&gt;%\n        mutate(article_text = as.character(article_text)) %&gt;%\n        # Detecting and removing german articles\n        mutate(german = str_detect(article_text, \"wenn|wird|und\")) %&gt;%\n        filter(german == FALSE) %&gt;%\n        select(-german) %&gt;%\n        # Finally, creating the label of the article (the title), and removing things that are \n        # not articles, such as the daily feuilleton.\n        mutate(label = map(data, ~`[`(.$label, 1))) %&gt;%\n        filter(!str_detect(label, \"embranchement|ligne|bourse|abonnés|feuilleton\")) %&gt;%\n        filter(label != \"na\")\n\n    # Save the data in the rds format, as it is not a flat file\n    saveRDS(final, paste0(path, \"/\", str_sub(path, 11, -1), \".rds\"))\n}\n\n# Here again, I do this in parallel\n\nlibrary(furrr)\n\nplan(multiprocess, workers = 8)\n\nfuture_map(pathdirs, tidy_papers)\n\n\nThis is how one of these files looks like, after passing through this function:\n\n\n\n\n\nOne line is one article. The first column is the id of the article, the second column contains a data frame, the text of the article and finally the title of the article. Let’s take a look at the content of the first element of the data column:\n\n\n\n\n\nThis is the result of the merger of the METS and ALTO csv files. The first column is the id of the article, the second column contains each individual word of the article, the label column the label, or title of the article.\n\n\n\n\nStep 3: Part-of-speech annotation\n\n\nPart-of-speech annotation is a technique with the aim of assigning to each word its part of speech. Basically, Pos annotation tells us whether a word is a verb, a noun, an adjective… This will be quite useful for the analysis. To perform Pos annotation, you need to install the {udpipe} package, and download the pre-trained model for the language you want to annotate, in my case French:\n\n\n\n\nClick if you want to see the code\n\n\n# Only run this once. This downloads the model for French\nudpipe_download_model(language = \"french\")\n\n# Load the model\nudmodel_french &lt;- udpipe_load_model(file = 'french-gsd-ud-2.3-181115.udpipe')\n\n# Save the path of the files to annotate in a list:\npathrds &lt;- list.files(path = \"./\", all.files = TRUE, recursive = TRUE) %&gt;% \n  str_match(\".*.rds\") %&gt;%\n  discard(is.na)\n\nannotate_rds &lt;- function(path, udmodel){\n\n    newspaper &lt;- readRDS(path)\n\n    s &lt;- udpipe_annotate(udmodel, newspaper$article_text, doc_id = newspaper$label)\n    x &lt;- data.frame(s)\n\n    saveRDS(x, str_replace(path, \".rds\", \"_annotated.rds\"))\n}\n\nlibrary(furrr)\nplan(multiprocess, workers = 8)\ntic &lt;- Sys.time()\nfuture_map(pathrds, annotate_rds, udmodel = udmodel_french)\ntoc &lt;- Sys.time()\ntoc - tic\n\n\nAnd here is the result:\n\n\n\n\n\nThe upos column contains the tags. Now I know which words are nouns, verbs, adjectives, stopwords… Meaning that I can easily focus on the type of words that interest me. Plus, as an added benefit, I can focus on the lemma of the words. For example, the word viennent, is the conjugated form of the verb venir. venir is thus the lemma of viennent. This means that I can focus my analysis on lemmata. This is useful, because if I compute the frequency of words, viennent would be different from venir, which is not really what we want.\n\n\n\n\nStep 4: tf-idf\n\n\nJust like what I did in my first blog post, I compute the tf-idf of words. The difference, is that here the “document” is the article. This means that I will get the most frequent words inside each article, but who are at the same time rare in the other articles. Doing this ensures that I will only get very relevant words for each article.\n\n\nIn the lines below, I prepare the data to then make the plots. The files that are created using the code below are available in the following Github link.\n\n\nIn the Shiny app, I read the data directly from the repo. This way, I can keep the app small in size.\n\n\n\n\nClick if you want to see the code\n\n\npath_annotatedrds &lt;- list.files(path = \"./\", all.files = TRUE, recursive = TRUE) %&gt;% str_match(\".*_annotated.rds\") %&gt;%\n    discard(is.na)\n\nprepare_tf_idf &lt;- function(path){\n\n    annotated_newspaper &lt;- readRDS(path)\n\n    tf_idf_data &lt;- annotated_newspaper %&gt;%\n        filter(upos %in% c(\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\")) %&gt;%\n        filter(nchar(lemma) &gt; 3) %&gt;%\n        count(doc_id, lemma) %&gt;%\n        bind_tf_idf(lemma, doc_id, n) %&gt;%\n        arrange(desc(tf_idf)) %&gt;%\n        group_by(doc_id)\n\n    name_tf_idf_data &lt;- str_split(path, \"/\", simplify = 1)[1] %&gt;%\n        paste0(\"_tf_idf_data.rds\")  %&gt;%\n        str_sub(start = 9, -1)\n\n    saveRDS(tf_idf_data, paste0(\"tf_idf_data/\", name_tf_idf_data))\n}\n\nlibrary(furrr)\nplan(multiprocess, workers = 8)\n\nfuture_map(path_annotatedrds, prepare_tf_idf)\n\n\n\n\nStep 5: Summarizing articles by extracting the most relevant sentences, using {textrank}\n\n\nThe last step in data preparation is to extract the most relevant sentences of each articles, using the {textrank} package. This packages implements the PageRank algorithm developed by Larry Page and Sergey Brin in 1995. This algorithm ranks pages by the number of links that point to the pages; the most popular and important pages are also the ones with more links to them. A similar approach is used by the implementation of {textrank}. The algorithm is explained in detail in the following paper.\n\n\nHowever, I cannot simply apply {textrank} to the annotated data frame as it is. Because I have several articles, I have to run the textrank_sentences() function, which extracts the relevant sentences, article by article. For this I still need to transform the data set and also need to prepare the data in a way that makes it digestible by the function. I will not explain the code below line by line, since the documentation of the package is quite straightforward. However, keep in mind that I have to run the textrank_sentences() function for each article, which explains that as some point I use the following:\n\ngroup_by(doc_id) %&gt;%\n    nest() %&gt;%\n\nwhich then makes it easy to work by article (doc_id is the id of the articles). This part is definitely the most complex, so if you’re interested in the methodology described here, really take your time to understand this function. Let me know if I can clarify things!\n\n\n\n\nClick if you want to see the code\n\n\nlibrary(textrank)\nlibrary(brotools)\n\npath_annotatedrds &lt;- list.files(path = \"./\", all.files = TRUE, recursive = TRUE) %&gt;% str_match(\".*_annotated.rds\") %&gt;%\n    discard(is.na)\n\nprepare_textrank &lt;- function(path){\n\n    annotated_newspaper &lt;- readRDS(path)\n\n    # sentences summary\n    x_text_rank &lt;- annotated_newspaper %&gt;%\n        group_by(doc_id) %&gt;%\n        nest() %&gt;%\n        mutate(textrank_id = map(data, ~unique_identifier(., c(\"paragraph_id\", \"sentence_id\")))) %&gt;%\n        mutate(cleaned = map2(.x = data, .y = textrank_id, ~cbind(.x, \"textrank_id\" = .y))) %&gt;%\n        select(doc_id, cleaned)\n\n    x_text_rank2 &lt;- x_text_rank %&gt;%\n        mutate(sentences = map(cleaned, ~select(., textrank_id, sentence))) %&gt;%\n        # one_row() is a function from my own package, which eliminates duplicates rows\n        # from a data frame\n        mutate(sentences = map(sentences, ~one_row(., c(\"textrank_id\", \"sentence\"))))\n\n    x_terminology &lt;- x_text_rank %&gt;%\n        mutate(terminology = map(cleaned, ~filter(., upos %in% c(\"NOUN\", \"ADJ\")))) %&gt;%\n        mutate(terminology = map(terminology, ~select(., textrank_id, \"lemma\"))) %&gt;%\n        select(terminology)\n\n    x_final &lt;- bind_cols(x_text_rank2, x_terminology)\n\n    possibly_textrank_sentences &lt;- possibly(textrank_sentences, otherwise = NULL)\n\n    x_final &lt;- x_final %&gt;%\n        mutate(summary = map2(sentences, terminology, possibly_textrank_sentences)) %&gt;%\n        select(doc_id, summary)\n\n    name_textrank_data &lt;- str_split(path, \"/\", simplify = 1)[1] %&gt;%\n        paste0(\"_textrank_data.rds\") %&gt;%\n        str_sub(start = 9, -1)\n\n    saveRDS(x_final, paste0(\"textrank_data/\", name_textrank_data))\n}\n\nlibrary(furrr)\nplan(multiprocess, workers = 8)\n\nfuture_map(path_annotatedrds, prepare_textrank)\n\n\nYou can download the annotated data sets from the following link. This is how the data looks like:\n\n\n\n\n\nUsing the summary() function on an element of the summary column returns the 5 most relevant sentences as extracted by {textrank}."
  },
  {
    "objectID": "posts/2019-02-04-newspapers_shiny_app_tutorial.html#part-2-building-the-shiny-app",
    "href": "posts/2019-02-04-newspapers_shiny_app_tutorial.html#part-2-building-the-shiny-app",
    "title": "Building a shiny app to explore historical newspapers: a step-by-step guide",
    "section": "\nPart 2: Building the shiny app\n",
    "text": "Part 2: Building the shiny app\n\n\nThe most difficult parts are behind us! Building a dashboard is quite easy thanks to the {flexdashboard} package. You need to know Markdown and some Shiny, but it’s way easier than building a complete Shiny app. First of all, install the {fleshdashboard} package, and start from a template, or from this list of layouts.\n\n\nI think that the only trick worth mentioning is that I put the data in a Github repo, and read it directly from the Shiny app. Users choose a date, which I save in a reactive variable. I then build the right url that points towards the right data set, and read it:\n\npath_tf_idf &lt;- reactive({\n    paste0(\"https://raw.githubusercontent.com/b-rodrigues/newspapers_shinyapp/master/tf_idf_data/newspaper_lunion_\", as.character(input$date2), \"_tf_idf_data.rds\")\n})\n\ndfInput &lt;- reactive({\n        read_rds(url(path_tf_idf())) %&gt;%\n        top_n(as.numeric(input$tf_df_words), tf_idf) %&gt;%\n        mutate(word = reorder(lemma, tf_idf)) \n})\n\nBecause I did all the computations beforehand, the app simply reads the data and creates the bar plots for the tf-idf data, or prints the sentences for the textrank data. To print the sentences correcly, I had to use some html tags, using the {htmltools} package. Below you can find the source code of the app:\n\n\n\n\nClick if you want to see the code\n\n\n---\ntitle: \"Exploring 10 years of daily publications of the Luxembourguish newspaper, *L'Union*\"\noutput: \n  flexdashboard::flex_dashboard:\n    theme: yeti\n    orientation: columns\n    vertical_layout: fill\nruntime: shiny\n\n---\n\n`` `{r setup, include=FALSE}\nlibrary(flexdashboard)\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(textrank)\nlibrary(tidytext)\nlibrary(udpipe)\nlibrary(plotly)\nlibrary(ggthemes)\n`` `\n\nSidebar {.sidebar}\n=====================================\n\n`` `{r}\ndateInput('date2',\n      label = paste('Select date'),\n      value = as.character(as.Date(\"1860-11-14\")),\n      min = as.Date(\"1860-11-12\"), max = as.Date(\"1869-12-31\"),\n      format = \"yyyy/mm/dd\",\n      startview = 'year', language = 'en-GB', weekstart = 1\n    )\nselectInput(inputId = \"tf_df_words\", \n            label = \"Select number of unique words for tf-idf\", \n            choices = seq(1:10),\n            selected = 5)\nselectInput(inputId = \"textrank_n_sentences\", \n            label = \"Select the number of sentences for the summary of the article\", \n            choices = seq(1:20), \n            selected = 5)\n`` `\n\n*The BnL has digitised over 800.000 pages of Luxembourg newspapers. From those, more than 700.000 \npages have rich metadata using international XML standards such as METS and ALTO. \nMultiple datasets are available for download. Each one is of different size and contains different\nnewspapers. All the digitised material can also be found on our search platform a-z.lu \n(Make sure to filter by “eluxemburgensia”). All datasets contain XML (METS + ALTO), PDF, original \nTIFF and PNG files for every newspaper issue.* \nSource: https://data.bnl.lu/data/historical-newspapers/\n\nThis Shiny app allows you to get summaries of the 10 years of daily issues of the \"L'Union\" newspaper.\nIn the first tab, a simple word frequency per article is shown, using the tf-idf method. In the \nsecond tab, summary sentences have been extracted using the `{textrank}` package.\n\n\nWord frequency per article\n===================================== \nRow\n-----------------------------------------------------------------------\n\n### Note: there might be days without any publication. In case of an error, select another date.\n    \n`` `{r}\npath_tf_idf &lt;- reactive({\n    paste0(\"https://raw.githubusercontent.com/b-rodrigues/newspapers_shinyapp/master/tf_idf_data/newspaper_lunion_\", as.character(input$date2), \"_tf_idf_data.rds\")\n})\ndfInput &lt;- reactive({\n        read_rds(url(path_tf_idf())) %&gt;%\n        top_n(as.numeric(input$tf_df_words), tf_idf) %&gt;%\n        mutate(word = reorder(lemma, tf_idf)) \n})\nrenderPlotly({\n    df_tf_idf &lt;- dfInput()\n    p1 &lt;- ggplot(df_tf_idf,\n                 aes(word, tf_idf)) +\n                 geom_col(show.legend = FALSE, fill = \"#82518c\") +\n                 labs(x = NULL, y = \"tf-doc_idf\") +\n                 facet_wrap(~doc_id, ncol = 2, scales = \"free\") +\n                 coord_flip() +\n                 theme_dark()\n    ggplotly(p1)\n})\n`` `\n\nSummary of articles {data-orientation=rows}\n===================================== \nRow \n-----------------------------------------------------------------------\n\n### The sentence in bold is the title of the article. You can show more sentences in the summary by using the input in the sidebar.\n    \n`` `{r}\nprint_summary_textrank &lt;- function(doc_id, summary, n_sentences){\n    htmltools::HTML(paste0(\"&lt;b&gt;\", doc_id, \"&lt;/b&gt;\"), paste(\"&lt;p&gt;\", summary(summary, n_sentences), sep = \"\", collapse = \"&lt;br/&gt;\"), \"&lt;/p&gt;\")\n}\npath_textrank &lt;- reactive({\n    paste0(\"https://raw.githubusercontent.com/b-rodrigues/newspapers_shinyapp/master/textrank_data/newspaper_lunion_\", as.character(input$date2), \"_textrank_data.rds\")\n})\ndfInput2 &lt;- reactive({\n        read_rds(url(path_textrank()))\n})\nrenderUI({\n    df_textrank &lt;- dfInput2()\n    \ndf_textrank &lt;- df_textrank %&gt;% \n    mutate(to_print = map2(doc_id, summary, print_summary_textrank, n_sentences = as.numeric(input$textrank_n_sentences)))\ndf_textrank$to_print\n})\n`` `\n\n\n\nI host the app on Shinyapps.io, which is really easy to do from within Rstudio.\n\n\nThat was quite long, I’m not sure that anyone will read this blog post completely, but oh well. Better to put the code online, might help someone one day, that leave it to rot on my hard drive."
  },
  {
    "objectID": "posts/2016-12-24-functional-programming-and-unit-testing-for-data-munging-with-r-available-on-leanpub.html",
    "href": "posts/2016-12-24-functional-programming-and-unit-testing-for-data-munging-with-r-available-on-leanpub.html",
    "title": "Functional programming and unit testing for data munging with R available on Leanpub",
    "section": "",
    "text": "The book I’ve been working on these pasts months (you can read about it here, and read it for free here) is now available on Leanpub! You can grab a copy and read it on your ebook reader or on your computer, and what’s even better is that it is available for free (but you can also decide to buy it if you really like it). Here is the link on Leanpub.\nIn the book, I show you the basics of functional programming, unit testing and package development for the R programming language. The end goal is to make your data tidy in a reproducible way!\nJust a heads up: as the book is right now, the formatting is not perfect and images are missing. This is because I use bookdown to write the book and convert it to Leanpub’s markdown flavour is not trivial. I will find a solution to automate the conversion from bookdown’s version to Leanpub’s markdown and try to keep both versions in sync. Of course, once the book will be finished, the version on Leanpub and on my website are going to be completely identical. If you want to read it on your computer offline, you can also download a pdf from the book’s website, by clicking on the pdf icon in the top left corner. Do not hesitate to send me an email if you want to give me feedback (just click on the red envelope in the top right corner) or tweet me @brodriguesco."
  },
  {
    "objectID": "posts/2017-07-02-dplyr-0-70-tutorial.html",
    "href": "posts/2017-07-02-dplyr-0-70-tutorial.html",
    "title": "Lesser known dplyr 0.7* tricks",
    "section": "",
    "text": "This blog post is an update to an older one I wrote in March. In the post from March, dplyr was at version 0.50, but since then a major update introduced some changes that make some of the tips in that post obsolete. So here I revisit the blog post from March by using dplyr 0.70."
  },
  {
    "objectID": "posts/2017-07-02-dplyr-0-70-tutorial.html#create-new-columns-with-mutate-and-case_when",
    "href": "posts/2017-07-02-dplyr-0-70-tutorial.html#create-new-columns-with-mutate-and-case_when",
    "title": "Lesser known dplyr 0.7* tricks",
    "section": "\nCreate new columns with mutate() and case_when()\n",
    "text": "Create new columns with mutate() and case_when()\n\n\nThe basic things such as selecting columns, renaming them, filtering, etc did not change with this new version. What did change however is creating new columns using case_when(). First, load dplyr and the mtcars dataset:\n\nlibrary(\"dplyr\")\ndata(mtcars)\n\nThis was how it was done in version 0.50 (notice the ’.\\(’ symbol before the variable ‘carb’):&lt;/p&gt;\n&lt;pre class=\"r\"&gt;&lt;code&gt;mtcars %&gt;%\n    mutate(carb_new = case_when(.\\)carb == 1 ~ \"one\", .\\(carb == 2 ~ &quot;two&quot;,\n                                .\\)carb == 4 ~ \"four\", TRUE ~ \"other\")) %&gt;% head(5)\n\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb carb_new\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4     four\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4     four\n## 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1      one\n## 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1      one\n## 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2      two\n\nThis has been simplified to:\n\nmtcars %&gt;%\n    mutate(carb_new = case_when(carb == 1 ~ \"one\",\n                                carb == 2 ~ \"two\",\n                                carb == 4 ~ \"four\",\n                                TRUE ~ \"other\")) %&gt;%\n    head(5)\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb carb_new\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4     four\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4     four\n## 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1      one\n## 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1      one\n## 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2      two\n\nNo need for .$ anymore."
  },
  {
    "objectID": "posts/2017-07-02-dplyr-0-70-tutorial.html#apply-a-function-to-certain-columns-only-by-rows-with-purrrlyr",
    "href": "posts/2017-07-02-dplyr-0-70-tutorial.html#apply-a-function-to-certain-columns-only-by-rows-with-purrrlyr",
    "title": "Lesser known dplyr 0.7* tricks",
    "section": "\nApply a function to certain columns only, by rows, with purrrlyr\n",
    "text": "Apply a function to certain columns only, by rows, with purrrlyr\n\n\ndplyr wasn’t the only package to get an overhaul, purrr also got the same treatment.\n\n\nIn the past, I applied a function to certains columns like this:\n\nmtcars %&gt;%\n    select(am, gear, carb) %&gt;%\n    purrr::by_row(sum, .collate = \"cols\", .to = \"sum_am_gear_carb\") -&gt; mtcars2\nhead(mtcars2)\n\nNow, by_row() does not exist in purrr anymore, but instead a new package called purrrlyr was introduced with functions that don’t really fit inside purrr nor dplyr:\n\nmtcars %&gt;%\n    select(am, gear, carb) %&gt;%\n    purrrlyr::by_row(sum, .collate = \"cols\", .to = \"sum_am_gear_carb\") -&gt; mtcars2\nhead(mtcars2)\n## # A tibble: 6 x 4\n##      am  gear  carb sum_am_gear_carb\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;\n## 1     1     4     4                9\n## 2     1     4     4                9\n## 3     1     4     1                6\n## 4     0     3     1                4\n## 5     0     3     2                5\n## 6     0     3     1                4\n\nThink of purrrlyr as purrrs and dplyrs love child."
  },
  {
    "objectID": "posts/2017-07-02-dplyr-0-70-tutorial.html#using-dplyr-functions-inside-your-own-functions-or-what-is-tidyeval",
    "href": "posts/2017-07-02-dplyr-0-70-tutorial.html#using-dplyr-functions-inside-your-own-functions-or-what-is-tidyeval",
    "title": "Lesser known dplyr 0.7* tricks",
    "section": "\nUsing dplyr functions inside your own functions, or what is tidyeval\n",
    "text": "Using dplyr functions inside your own functions, or what is tidyeval\n\n\nProgramming with dplyr has been simplified a lot. Before version 0.70, one needed to use dplyr in conjuction with lazyeval to use dplyr functions inside one’s own fuctions. It was not always very easy, especially if you mixed columns and values inside your functions. Here’s the example from the March blog post:\n\nextract_vars &lt;- function(data, some_string){\n\n  data %&gt;%\n    select_(lazyeval::interp(~contains(some_string))) -&gt; data\n\n  return(data)\n}\n\nextract_vars(mtcars, \"spam\")\n\nMore examples are available in this other blog post.\n\n\nI will revisit them now with dplyr’s new tidyeval syntax. I’d recommend you read the Tidy evaluation vignette here. This vignette is part of the rlang package, which gets used under the hood by dplyr for all your programming needs. Here is the function I called simpleFunction(), written with the old dplyr syntax:\n\nsimpleFunction &lt;- function(dataset, col_name){\n  dataset %&gt;%\n    group_by_(col_name) %&gt;%\n    summarise(mean_mpg = mean(mpg)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, \"cyl\")\n## # A tibble: 3 x 2\n##     cyl mean_mpg\n##   &lt;dbl&gt;    &lt;dbl&gt;\n## 1     4     26.7\n## 2     6     19.7\n## 3     8     15.1\n\nWith the new synax, it must be rewritten a little bit:\n\nsimpleFunction &lt;- function(dataset, col_name){\n  col_name &lt;- enquo(col_name)\n  dataset %&gt;%\n    group_by(!!col_name) %&gt;%\n    summarise(mean_mpg = mean(mpg)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, cyl)\n## # A tibble: 3 x 2\n##     cyl mean_mpg\n##   &lt;dbl&gt;    &lt;dbl&gt;\n## 1     4     26.7\n## 2     6     19.7\n## 3     8     15.1\n\nWhat has changed? Forget the underscore versions of the usual functions such as select_(), group_by_(), etc. Now, you must quote the column name using enquo() (or just quo() if working interactively, outside a function), which returns a quosure. This quosure can then be evaluated using !! in front of the quosure and inside the usual dplyr functions.\n\n\nLet’s look at another example:\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  filter_criteria &lt;- lazyeval::interp(~y == x, .values=list(y = as.name(col_name), x = value))\n  dataset %&gt;%\n    filter_(filter_criteria) %&gt;%\n    summarise(mean_cyl = mean(cyl)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, \"am\", 1)\n##   mean_cyl\n## 1 5.076923\n\nAs you can see, it’s a bit more complicated, as you needed to use lazyeval::interp() to make it work. With the improved dplyr, here’s how it’s done:\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  col_name &lt;- enquo(col_name)\n  dataset %&gt;%\n    filter((!!col_name) == value) %&gt;%\n    summarise(mean_cyl = mean(cyl)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, am, 1)\n##   mean_cyl\n## 1 5.076923\n\nMuch, much easier! There is something that you must pay attention to though. Notice that I’ve written:\n\nfilter((!!col_name) == value)\n\nand not:\n\nfilter(!!col_name == value)\n\nI have enclosed !!col_name inside parentheses. I struggled with this, but thanks to help from @dmi3k and @_lionelhenry I was able to understand what was happening (isn’t the #rstats community on twitter great?).\n\n\nOne last thing: let’s make this function a bit more general. I hard-coded the variable cyl inside the body of the function, but maybe you’d like the mean of another variable? Easy:\n\nsimpleFunction &lt;- function(dataset, group_col, mean_col, value){\n  group_col &lt;- enquo(group_col)\n  mean_col &lt;- enquo(mean_col)\n  dataset %&gt;%\n    filter((!!group_col) == value) %&gt;%\n    summarise(mean((!!mean_col))) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, am, cyl, 1)\n##   mean(cyl)\n## 1  5.076923\n\n«That’s very nice Bruno, but mean((cyl)) in the output looks ugly as sin» you might think, and you’d be right. It is possible to set the name of the column in the output using := instead of =:\n\nsimpleFunction &lt;- function(dataset, group_col, mean_col, value){\n  group_col &lt;- enquo(group_col)\n  mean_col &lt;- enquo(mean_col)\n  mean_name &lt;- paste0(\"mean_\", mean_col)[2]\n  dataset %&gt;%\n    filter((!!group_col) == value) %&gt;%\n    summarise(!!mean_name := mean((!!mean_col))) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, am, cyl, 1)\n##   mean_cyl\n## 1 5.076923\n\nTo get the name of the column I added this line:\n\nmean_name &lt;- paste0(\"mean_\", mean_col)[2]\n\nTo see what it does, try the following inside an R interpreter (remember to us quo() instead of enquo() outside functions!):\n\npaste0(\"mean_\", quo(cyl))\n## [1] \"mean_~\"   \"mean_cyl\"\n\nenquo() quotes the input, and with paste0() it gets converted to a string that can be used as a column name. However, the ~ is in the way and the output of paste0() is a vector of two strings: the correct name is contained in the second element, hence the [2]. There might be a more elegant way of doing that, but for now this has been working well for me.\n\n\nThat was it folks! I do recommend you read the Programming with dplyr vignette here as well as other blog posts, such as the one recommended to me by @dmi3k here.\n\n\nHave fun with dplyr 0.70!"
  },
  {
    "objectID": "posts/2019-06-04-cosine_sim.html",
    "href": "posts/2019-06-04-cosine_sim.html",
    "title": "Using cosine similarity to find matching documents: a tutorial using Seneca’s letters to his friend Lucilius",
    "section": "",
    "text": "Lately I’ve been interested in trying to cluster documents, and to find similar documents based on their contents. In this blog post, I will use Seneca’s Moral letters to Lucilius and compute the pairwise cosine similarity of his 124 letters. Computing the cosine similarity between two vectors returns how similar these vectors are. A cosine similarity of 1 means that the angle between the two vectors is 0, and thus both vectors have the same direction. Seneca’s Moral letters to Lucilius deal mostly with philosophical topics, as Seneca was, among many other things, a philosopher of the stoic school. The stoic school of philosophy is quite interesting, but it has been unfortunately misunderstood, especially in modern times. There is now a renewed interest for this school, see Modern Stoicism.\n\n\nThe first step is to scrape the letters. The code below scrapes the letters, and saves them into a list. I first start by writing a function that gets the raw text. Note the xpath argument of the html_nodes() function. I obtained this complex expression by using the SelectorGadget extension for Google Chrome, and then selecting the right element of the web page. See this screenshot if my description was not very clear.\n\n\nThen, the extract_text() function extracts the text from the letter. The only line that might be a bit complex is discard(~==(., \"\")) which removes every empty line.\n\n\nFinally, there’s the get_letter() function that actually gets the letter by calling the first two functions. In the last line, I get all the letters into a list by mapping the list of urls to the get_letter() function.\n\nlibrary(tidyverse)\nlibrary(rvest)\n\nbase_url &lt;- \"https://en.wikisource.org/wiki/Moral_letters_to_Lucilius/Letter_\"\n\nletter_numbers &lt;- seq(1, 124)\n\nletter_urls &lt;- paste0(base_url, letter_numbers)\n\nget_raw_text &lt;- function(base_url, letter_number){\n  paste0(base_url, letter_number) %&gt;%\n    read_html() %&gt;%\n    html_nodes(xpath ='//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"mw-parser-output\", \" \" ))]') %&gt;%  \n    html_text()\n}\n\n\nextract_text &lt;- function(raw_text, letter_number){\n  raw_text &lt;- raw_text %&gt;%\n    str_split(\"\\n\") %&gt;%  \n    flatten_chr() %&gt;%  \n    discard(~`==`(., \"\"))\n\n  start &lt;- 5\n\n  end &lt;- str_which(raw_text, \"Footnotes*\")\n\n  raw_text[start:(end-1)] %&gt;%\n    str_remove_all(\"\\\\[\\\\d{1,}\\\\]\") %&gt;%\n    str_remove_all(\"\\\\[edit\\\\]\")\n}\n\nget_letter &lt;- function(base_url, letter_number){\n\n  raw_text &lt;- get_raw_text(base_url, letter_number)\n\n  extract_text(raw_text, letter_number)\n}\n\nletters_to_lucilius &lt;- map2(base_url, letter_numbers, get_letter)\n\nNow that we have the letters saved in a list, we need to process the text a little bit. In order to compute the cosine similarity between the letters, I need to somehow represent them as vectors. There are several ways of doing this, and I am going to compute the tf-idf of each letter. The tf-idf will give me a vector for each letter, with zero and non-zero values. Zero values represent words that are common to all letters, and thus do not have any predictive power. Non-zero values are words that are not present in all letters, but maybe only a few. I expect that letters that discuss death for example, will have the word death in them, and letters that do not discuss death will not have this word. The word death thus has what I call predictive power, in that it helps us distinguish the letters discussing death from the other letters that do not discuss it. The same reasoning can be applied for any topic.\n\n\nSo, to get the tf-idf of each letter, I first need to put them in a tidy dataset. I will use the {tidytext} package for this. First, I load the required packages, convert each letter to a dataframe of one column that contains the text, and save the letter’s titles into another list:\n\nlibrary(tidytext)\nlibrary(SnowballC)\nlibrary(stopwords)\nlibrary(text2vec)\n\nletters_to_lucilius_df &lt;- map(letters_to_lucilius, ~tibble(\"text\" = .))\n\nletter_titles &lt;- letters_to_lucilius_df %&gt;%\n  map(~slice(., 1)) %&gt;%\n  map(pull)\n\nNow, I add this title to each dataframe as a new column, called title:\n\nletters_to_lucilius_df &lt;-  map2(.x = letters_to_lucilius_df, .y = letter_titles,\n                                ~mutate(.x, title = .y)) %&gt;%\n  map(~slice(., -1))\n\nI can now use unnest_tokens() to transform the datasets. Before, I had the whole text of the letter in one column. After using unnest_tokens() I now have a dataset with one row per word. This will make it easy to compute frequencies by letters, or what I am interested in, the tf-idf of each letter:\n\ntokenized_letters &lt;- letters_to_lucilius_df %&gt;%\n  bind_rows() %&gt;%\n  group_by(title) %&gt;%\n  unnest_tokens(word, text)\n\nI can now remove stopwords, using the data containing in the {stopwords} package:\n\nstopwords_en &lt;- tibble(\"word\" = stopwords(\"en\", source  = \"smart\"))\n\ntokenized_letters &lt;- tokenized_letters %&gt;%\n  anti_join(stopwords_en) %&gt;%\n  filter(!str_detect(word, \"\\\\d{1,}\"))\n## Joining, by = \"word\"\n\nNext step, wordstemming, meaning, going from “dogs” to “dog”, or from “was” to “be”. If you do not do wordstemming, “dogs” and “dog” will be considered different words, even though they are not. wordStem() is a function from {SnowballC}.\n\ntokenized_letters &lt;- tokenized_letters %&gt;%\n  mutate(word = wordStem(word, language = \"en\"))\n\nFinally, I can compute the tf-idf of each letter and cast the data as a sparse matrix:\n\ntfidf_letters &lt;- tokenized_letters %&gt;%\n  count(title, word, sort  = TRUE) %&gt;%\n  bind_tf_idf(word, title, n)\n\nsparse_matrix &lt;- tfidf_letters %&gt;%\n  cast_sparse(title, word, tf)\n\nLet’s take a look at the sparse matrix:\n\nsparse_matrix[1:10, 1:4]\n## 10 x 4 sparse Matrix of class \"dgCMatrix\"\n##                                                                   thing\n## CXIII. On the Vitality of the Soul and Its Attributes       0.084835631\n## LXVI. On Various Aspects of Virtue                          0.017079890\n## LXXXVII. Some Arguments in Favour of the Simple Life        0.014534884\n## CXVII. On Real Ethics as Superior to Syllogistic Subtleties 0.025919732\n## LXXVI. On Learning Wisdom in Old Age                        0.021588946\n## CII. On the Intimations of Our Immortality                  0.014662757\n## CXXIV. On the True Good as Attained by Reason               0.010139417\n## XCIV. On the Value of Advice                                0.009266409\n## LXXXI. On Benefits                                          0.007705479\n## LXXXV. On Some Vain Syllogisms                              0.013254786\n##                                                                     live\n## CXIII. On the Vitality of the Soul and Its Attributes       0.0837751856\n## LXVI. On Various Aspects of Virtue                          .           \n## LXXXVII. Some Arguments in Favour of the Simple Life        0.0007267442\n## CXVII. On Real Ethics as Superior to Syllogistic Subtleties 0.0050167224\n## LXXVI. On Learning Wisdom in Old Age                        0.0025906736\n## CII. On the Intimations of Our Immortality                  0.0019550342\n## CXXIV. On the True Good as Attained by Reason               .           \n## XCIV. On the Value of Advice                                0.0023166023\n## LXXXI. On Benefits                                          0.0008561644\n## LXXXV. On Some Vain Syllogisms                              0.0022091311\n##                                                                   good\n## CXIII. On the Vitality of the Soul and Its Attributes       0.01166490\n## LXVI. On Various Aspects of Virtue                          0.04132231\n## LXXXVII. Some Arguments in Favour of the Simple Life        0.04578488\n## CXVII. On Real Ethics as Superior to Syllogistic Subtleties 0.04849498\n## LXXVI. On Learning Wisdom in Old Age                        0.04663212\n## CII. On the Intimations of Our Immortality                  0.05180841\n## CXXIV. On the True Good as Attained by Reason               0.06717364\n## XCIV. On the Value of Advice                                0.01081081\n## LXXXI. On Benefits                                          0.01626712\n## LXXXV. On Some Vain Syllogisms                              0.01472754\n##                                                                 precept\n## CXIII. On the Vitality of the Soul and Its Attributes       .          \n## LXVI. On Various Aspects of Virtue                          .          \n## LXXXVII. Some Arguments in Favour of the Simple Life        .          \n## CXVII. On Real Ethics as Superior to Syllogistic Subtleties .          \n## LXXVI. On Learning Wisdom in Old Age                        .          \n## CII. On the Intimations of Our Immortality                  .          \n## CXXIV. On the True Good as Attained by Reason               0.001267427\n## XCIV. On the Value of Advice                                0.020463320\n## LXXXI. On Benefits                                          .          \n## LXXXV. On Some Vain Syllogisms                              .\n\nWe can consider each row of this matrix as the vector representing a letter, and thus compute the cosine similarity between letters. For this, I am using the sim2() function from the {text2vec} package. I then create the get_similar_letters() function that returns similar letters for a given reference letter:\n\nsimilarities &lt;- sim2(sparse_matrix, method = \"cosine\", norm = \"l2\") \n\nget_similar_letters &lt;- function(similarities, reference_letter, n_recommendations = 3){\n  sort(similarities[reference_letter, ], decreasing = TRUE)[1:(2 + n_recommendations)]\n}\nget_similar_letters(similarities, 19)\n##          XXX. On Conquering the Conqueror \n##                                 1.0000000 \n##                  XXIV. On Despising Death \n##                                 0.6781600 \n##      LXXXII. On the Natural Fear of Death \n##                                 0.6639736 \n## LXX. On the Proper Time to Slip the Cable \n##                                 0.5981706 \n## LXXVIII. On the Healing Power of the Mind \n##                                 0.4709679\nget_similar_letters(similarities, 99)\n##                              LXI. On Meeting Death Cheerfully \n##                                                     1.0000000 \n##                     LXX. On the Proper Time to Slip the Cable \n##                                                     0.5005015 \n## XCIII. On the Quality, as Contrasted with the Length, of Life \n##                                                     0.4631796 \n##                         CI. On the Futility of Planning Ahead \n##                                                     0.4503093 \n##                              LXXVII. On Taking One's Own Life \n##                                                     0.4147019\nget_similar_letters(similarities, 32)\n##                                    LIX. On Pleasure and Joy \n##                                                   1.0000000 \n##          XXIII. On the True Joy which Comes from Philosophy \n##                                                   0.4743672 \n##                          CIX. On the Fellowship of Wise Men \n##                                                   0.4526835 \n## XC. On the Part Played by Philosophy in the Progress of Man \n##                                                   0.4498278 \n##         CXXIII. On the Conflict between Pleasure and Virtue \n##                                                   0.4469312\nget_similar_letters(similarities, 101)\n##                    X. On Living to Oneself \n##                                  1.0000000 \n##          LXXIII. On Philosophers and Kings \n##                                  0.3842292 \n##                  XLI. On the God within Us \n##                                  0.3465457 \n##                       XXXI. On Siren Songs \n##                                  0.3451388 \n## XCV. On the Usefulness of Basic Principles \n##                                  0.3302794\n\nAs we can see from these examples, this seems to be working quite well: the first title is the title of the reference letter, will the next 3 are the suggested letters. The problem is that my matrix is not in the right order, and thus reference letter 19 does not correspond to letter 19 of Seneca… I have to correct that, but not today."
  },
  {
    "objectID": "posts/2019-06-20-tidy_eval_saga.html",
    "href": "posts/2019-06-20-tidy_eval_saga.html",
    "title": "Curly-Curly, the successor of Bang-Bang",
    "section": "",
    "text": "Writing functions that take data frame columns as arguments is a problem that most R users have been confronted with at some point. There are different ways to tackle this issue, and this blog post will focus on the solution provided by the latest release of the {rlang} package. You can read the announcement here, which explains really well what was wrong with the old syntax, and how the new syntax works now.\n\n\nI have written about the problem of writing functions that use data frame columns as arguments three years ago and two year ago too. Last year, I wrote a blog post that showed how to map a list of functions to a list of datasets with a list of columns as arguments that used the !!quo(column_name) syntax (the !! is pronounced bang-bang). Now, there is a new sheriff in town, {{}}, introduced in {rlang} version 0.4.0 that makes things even easier. The suggested pronunciation of {{}} is curly-curly, but there is no consensus yet.\n\n\nFirst, let’s load the {tidyverse}:\n\nlibrary(tidyverse)\n\nLet’s suppose that I need to write a function that takes a data frame, as well as a column from this data frame as arguments:\n\nhow_many_na &lt;- function(dataframe, column_name){\n  dataframe %&gt;%\n    filter(is.na(column_name)) %&gt;%\n    count()\n}\n\nLet’s try this function out on the starwars data:\n\ndata(starwars)\n\nhead(starwars)\n## # A tibble: 6 x 13\n##   name  height  mass hair_color skin_color eye_color birth_year gender\n##   &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; \n## 1 Luke…    172    77 blond      fair       blue            19   male  \n## 2 C-3PO    167    75 &lt;NA&gt;       gold       yellow         112   &lt;NA&gt;  \n## 3 R2-D2     96    32 &lt;NA&gt;       white, bl… red             33   &lt;NA&gt;  \n## 4 Dart…    202   136 none       white      yellow          41.9 male  \n## 5 Leia…    150    49 brown      light      brown           19   female\n## 6 Owen…    178   120 brown, gr… light      blue            52   male  \n## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n## #   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nAs you can see, there are missing values in the hair_color column. Let’s try to count how many missing values are in this column:\n\nhow_many_na(starwars, hair_color)\nError: object 'hair_color' not found\n\nR cannot find the hair_color column, and yet it is in the data! Well, this is actually exactly the issue. The issue is that the column is inside the dataframe, but when calling the function with hair_color as the second argument, R is looking for a variable called hair_color that does not exist. What about trying with \"hair_color\"?\n\nhow_many_na(starwars, \"hair_color\")\n## # A tibble: 1 x 1\n##       n\n##   &lt;int&gt;\n## 1     0\n\nNow we get something, but something wrong!\n\n\nOne way to solve this issue, is to not use the filter() function, and instead rely on base R:\n\nhow_many_na_base &lt;- function(dataframe, column_name){\n  na_index &lt;- is.na(dataframe[, column_name])\n  nrow(dataframe[na_index, column_name])\n}\n\nhow_many_na_base(starwars, \"hair_color\")\n## [1] 5\n\nThis works, but not using the {tidyverse} at all is not an option, at least for me. For instance, the next function, which uses a grouping variable, would be difficult to implement without the {tidyverse}:\n\nsummarise_groups &lt;- function(dataframe, grouping_var, column_name){\n  dataframe %&gt;%\n    group_by(grouping_var) %&gt;%  \n    summarise(mean(column_name, na.rm = TRUE))\n}\n\nCalling this function results in the following error message:\n\nError: Column `grouping_var` is unknown\n\nBefore the release of {rlang} 0.4.0 this is was the solution:\n\nsummarise_groups &lt;- function(dataframe, grouping_var, column_name){\n\n  grouping_var &lt;- enquo(grouping_var)\n  column_name &lt;- enquo(column_name)\n  mean_name &lt;- paste0(\"mean_\", quo_name(column_name))\n\n  dataframe %&gt;%\n    group_by(!!grouping_var) %&gt;%  \n    summarise(!!(mean_name) := mean(!!column_name, na.rm = TRUE))\n}\n\nThe core of the function remained very similar to the version from before, but now one has to use the enquo()-!! syntax. While not overly difficult to use, it is cumbersome.\n\n\nNow this can be simplified using the new {{}} syntax:\n\nsummarise_groups &lt;- function(dataframe, grouping_var, column_name){\n\n  dataframe %&gt;%\n    group_by({{grouping_var}}) %&gt;%  \n    summarise({{column_name}} := mean({{column_name}}, na.rm = TRUE))\n}\n\nMuch easier and cleaner! You still have to use the := operator instead of = for the column name however. Also, from my understanding, if you want to modify the column names, for instance in this case return \"mean_height\" instead of height you have to keep using the enquo()-!! syntax."
  },
  {
    "objectID": "posts/2017-12-17-teaching_tidyverse.html",
    "href": "posts/2017-12-17-teaching_tidyverse.html",
    "title": "Teaching the tidyverse to beginners",
    "section": "",
    "text": "End October I tweeted this:\n\n\n\nwill teach #rstats soon again but this time following @drob 's suggestion of the tidyverse first as laid out here: https://t.co/js8SsUs8Nv\n\n— Bruno Rodrigues (@brodriguesco) October 24, 2017\n\n\n\nand it generated some discussion. Some people believe that this is the right approach, and some others think that one should first present base R and then show how the tidyverse complements it. This year, I taught three classes; a 12-hour class to colleagues that work with me, a 15-hour class to master’s students and 3 hours again to some of my colleagues. Each time, I decided to focus on the tidyverse(almost) entirely, and must say that I am not disappointed with the results!\n\n\nThe 12 hour class was divided in two 6 hours days. It was a bit intense, especially the last 3 hours that took place Friday afternoon. The crowd was composed of some economists that had experience with STATA, some others that were mostly using Excel and finally some colleagues from the IT department that sometimes need to dig into some data themselves. Apart from 2 people, all the other never had any experience with R.\n\n\nWe went from 0 to being able to do the plot below after the end of the first day (so 6 hours in). Keep in mind that practically none of them even had opened RStudio before. I show the code so you can see the progress made in just a few hours:\n\nlibrary(Ecdat)\nlibrary(tidyverse)\nlibrary(ggthemes)\ndata(Bwages)\nbwages = Bwages %&gt;%\n  mutate(educ_level = case_when(educ == 1 ~ \"Primary School\",\n                                educ == 2 ~ \"High School\",\n                                educ == 3 ~ \"Some university\",\n                                educ == 4 ~ \"Master's degree\",\n                                educ == 5 ~ \"Doctoral degree\"))\n\nggplot(bwages) +\n  geom_smooth(aes(exper, wage, colour = educ_level)) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n## `geom_smooth()` using method = 'loess'\n\n\n\n\nOf course some of them needed some help here and there, and I also gave them hints (for example I told them about case_when() and try to use it inside mutate() instead of nested ifs) but it was mostly due to lack of experience and because they hadn’t had the time to fully digest R’s syntax which was for most people involved completely new.\n\n\nOn the second day I showed purrr::map() and purrr::reduce() and overall it went quite well too. I even showed list-columns, and this is where I started losing some of them; I did not insist too much on it though, only wanted to show them the flexibility of data.frame objects. Some of them were quite impressed by list-columns! Then I started showing (for and while) loops and writing functions. I even showed them tidyeval and again, it went relatively well. Once they had the opportunity to play a bit around with it, I think it clicked (plus they have lots of code examples to go back too).\n\n\nAt the end, people seemed to have enjoyed the course, but told me that Friday was heavy; indeed it was, but I feel that it was mostly because 12 hours spread on 2 days is not the best format for this type of material, but we all had time constraints.\n\n\nThe 15 hour Master’s course was spread over 4 days, and covered basically the same. I just used the last 3 hours to show the students some basic functions for model estimation (linear, count, logit/probit and survival models). Again, the students were quite impressed by how easily they could get descriptive statistics by first grouping by some variables. Through their questions, I even got to show them scoped versions of dplyr verbs, such as select_if() and summarise_at(). I was expecting to lose them there, but actually most of them got these scoped versions quite fast. These students already had some experience with R though, but none with the tidyverse.\n\n\nFinally the 3 hour course was perhaps the most interesting; I only had 100% total beginners. Some just knew R by name and had never heard/seen/opened RStudio (with the exception of one person)! I did not show them any loops, function definitions and no plots. I only showed them how RStudio looked and worked, what were (and how to install) packages (as well as the CRAN Task Views) and then how to import data with rio and do descriptive statistics only with dplyr. They were really interested and quite impressed by rio (“what do you mean I can use the same code for importing any dataset, in any format?”) but also by the simplicity of dplyr.\n\n\nIn all the courses, I did show the $ primitive to refer to columns inside a data.frame. First I showed them lists which is where I introduced $. Then it was easy to explain to them why it was the same for a column inside a data.frame; a data.frame is simply a list! This is also the distinction I made from the previous years; I simply mentioned (and showed really quickly) matrices and focused almost entirely on lists. Most participants, if not all, had learned to program statistics by thinking about linear algebra and matrices. Nothing wrong with that, but I feel that R really shines when you focus on lists and on how to work with them.\n\n\nOverall as the teacher, I think that focusing on the tidyverse might be a very good strategy. I might have to do some adjustments here and there for the future courses, but my hunch is that the difficulties that some participants had were not necessarily due to the tidyverse but simply to lack of time to digest what was shown, as well as a total lack of experience with R. I do not think that these participants would have better understood a more traditional, base, matrix-oriented course."
  },
  {
    "objectID": "posts/2015-11-11-bootstrapping-did-with-r.html",
    "href": "posts/2015-11-11-bootstrapping-did-with-r.html",
    "title": "Bootstrapping standard errors for difference-in-differences estimation with R",
    "section": "",
    "text": "I’m currently working on a paper (with my colleague Vincent Vergnat who is also a Phd candidate at BETA) where I want to estimate the causal impact of the birth of a child on hourly and daily wages as well as yearly worked hours. For this we are using non-parametric difference-in-differences (henceforth DiD) and thus have to bootstrap the standard errors. In this post, I show how this is possible using the function boot.\n\n\nFor this we are going to replicate the example from Wooldridge’s Econometric Analysis of Cross Section and Panel Data and more specifically the example on page 415. You can download the data for R here. The question we are going to try to answer is how much does the price of housing decrease due to the presence of an incinerator in the neighborhood?\n\n\nFirst put the data in a folder and set the correct working directory and load the boot library.\n\nlibrary(boot)\nsetwd(\"/home/path/to/data/kiel data/\")\nload(\"kielmc.RData\")\n\nNow you need to write a function that takes the data as an argument, as well as an indices argument. This argument is used by the boot function to select samples. This function should return the statistic you’re interested in, in our case, the DiD estimate.\n\nrun_DiD &lt;- function(my_data, indices){\n    d &lt;- my_data[indices,]\n    return(\n        mean(d$rprice[d$year==1981 & d$nearinc==1]) - \n        mean(d$rprice[d$year==1981 & d$nearinc==0]) - \n        (mean(d$rprice[d$year==1978 & d$nearinc==1]) - \n        mean(d$rprice[d$year==1978 & d$nearinc==0]))\n    )\n}\n\nYou’re almost done! To bootstrap your DiD estimate you just need to use the boot function. If you have cpu with multiple cores (which you should, single core machines are quite outdated by now) you can even parallelize the bootstrapping.\n\nboot_est &lt;- boot(data, run_DiD, R=1000, parallel=\"multicore\", ncpus = 2)\n\nNow you should just take a look at your estimates:\n\nboot_est\n \nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = data, statistic = run_DiD, R = 1000, parallel = \"multicore\", \n ncpus = 2)\n\n\nBootstrap Statistics :\n original    bias    std. error\nt1* -11863.9 -553.3393    8580.435\n\nThese results are very similar to the ones in the book, only the standard error is higher.\n\n\nYou can get confidence intervals like this:\n\nquantile(boot_est$t, c(0.025, 0.975))\n##       2.5%      97.5% \n## -30186.397   3456.133\n\nor a t-statistic:\n\nboot_est$t0/sd(boot_est$t)\n## [1] -1.382669\n\nOr the density of the replications:\n\nplot(density(boot_est$t))\n\n&lt;img src=\"/img/density_did.png\" width=\"670\" height=\"450\" /&gt;&lt;/a&gt;\n\n\nJust as in the book, we find that the DiD estimate is not significant to the 5% level."
  },
  {
    "objectID": "posts/2019-05-19-spacemacs.html",
    "href": "posts/2019-05-19-spacemacs.html",
    "title": "The never-ending editor war (?)",
    "section": "",
    "text": "The creation of this blog post was prompted by this tweet, asking an age-old question:\n\n{{% tweet “1128981852558123008” %}}\n\nThis is actually a very important question, that I have been asking myself for a long time. An IDE, and plain text editors, are a very important tools to anyone writing code. Most working hours are spent within such a program, which means that one has to be careful about choosing the right one, and once a choice is made, one has, in my humble opinion, learn as many features of this program as possible to become as efficient as possible.\n\n\nAs you can notice from the tweet above, I suggested the use of Spacemacs… and my tweet did not get any likes or retweets (as of the 19th of May, sympathetic readers of this blog have liked the tweet). It is to set this great injustice straight that I decided to write this blog post.\n\n\nSpacemacs is a strange beast; if vi and Emacs had a baby, it would certainly look like Spacemacs. So first of all, to understand what is Spacemacs, one has to know a bit about vi and Emacs.\n\n\n\n\n\nvi is a text editor with 43 years of history now. You might have heard of Vim (Vi IMproved) which is a modern clone of vi, from 1991. More recently, another clone has been getting popular, Neovim, started in 2014. Whatever version of vi however, its basic way of functioning remains the same. vi is a modal editor, meaning that the user has to switch between different modes to work on a text file. When vi is first started, the program will be in Normal mode. In this mode, trying to type a word will likely result in nothing, or unexpected behaviour; unexpected, if you’re not familiar with vi. For instance, in Normal mode, typing j will not show the character j on your screen. Instead, this will move the cursor down one line. Typing p will paste, u will undo the last action, y will yank (copy) etc…\n\n\nTo type text, first, one has to enter Insert mode, by typing i while in Normal mode. Only then is it possible to write text. To go back to Normal mode, type ESC. Other modes are Visual mode (from Normal mode press v), which allows the user to select text and Command-line mode which can be entered by keying : from Normal mode and allows to enter commands.\n\n\nNow you might be wondering why anyone would use such a convoluted way to type text. Well, this is because one can chain these commands quite easily to perform repetitive tasks very quickly. For instance, to delete a word, one types daw (in Normal mode), delete a word. To delete the next 3 words, you can type 3daw. To edit the text between, for instance, () you would type ci( (while in Normal mode and anywhere between the braces containing the text to edit), change in (. Same logic applies for ci[ for instance. Can you guess what ciw does? If you are in Normal mode, and you want to change the word the cursor is on, this command will erase the word and put you in Insert mode so that you can write the new word.\n\n\nThese are just basic reasons why vi (or its clones) are awesome. It is also possible to automate very long and complex tasks using macros. One starts a macro by typing q and then any letter of the alphabet to name it, for instance a. The user then performs the actions needed, types q again to stop the recording of the macro, and can then execute the macro with @a. If the user needs to execute the macro say, 10 times, 10@‌‌a does the trick. It is possible to extend vi’s functionalities by using plugins, but more on that down below.\n\n\nvi keybindings have inspired a lot of other programs. For instance, you can get extensions for popular web browsers that mimick vi keybindings, such as Tridayctl for Firefox, or Vivium for Chromium (or Google Chrome). There are even browsers that are built from scratch with support for vi keybinds, such as my personal favorite, qutebrowser. You can even go further and use a tiling window manager on GNU-Linux, for instance i3, which I use, or xmonad. You might need to configure those to behave more like vi, but it is possible. This means that by learning one set of keyboard shortcuts, (and the logic behind chaining the keystrokes to achieve what you want), you can master several different programs. This blog post only deals with the editor part, but as you can see, if you go down the rabbit hole enough, a new exciting world opens up.\n\n\nI will show some common vi operations below, but before that let’s discuss Emacs.\n\n\n\n\n\nI am not really familiar with Emacs; I know that Emacs users only swear by it (just like vi users only swear by vi), and that Emacs is not a modal editor. However, it contains a lot of functions that you can use by pressing ESC, CTRL, ALT or META (META is the Windows key on a regular PC keyboard) followed by regular keys. So the approach is different, but it is widely accepted that productivity of proficient Emacs users is very high too. Emacs was started in 1985, and the most popular clone is GNU Emacs. Emacs also features modes, but not in the same sense as vi. There are major and minor modes. For instance, if you’re editing a Python script, Emacs will be in Python mode, or if editing a Markdown file Emacs will be in Markdown mode. This will change the available functions to the user, as well as provide other niceties, such as auto-completion. Emacs is also easily extensible, which is another reason why it is so popular. Users can install packages for Emacs, just like R users would do for R, to extend Emacs’ capabilities. For instance, a very important package if you plan to use Emacs for statistics or data science is ESS, Emacs Speaks Statistics. Emacs contains other very high quality packages, and it seems to me (but don’t quote me on that) that Emacs’ packages are more mature and feature-rich than vi’s plugins. However, vi keybindings are really awesome. This is, I believe, what Sylvain Benner was thinking when he developed Spacemacs.\n\n\n\n\n\nSpacemacs’ motto is that The best editor is neither Emacs nor Vim, it’s Emacs and Vim!. Spacemacs is a version, or distribution of Emacs, that has a very specific way of doing things. However, since it’s built on top of Emacs, all of Emacs’ packages are available to the user, notably Evil, which is a package that makes Emacs mimick vi’s modal mode and keybindings (the name of this package tells you everything you need to know about what Emacs users think of vi users 😀)\n\n\nNot only does Spacemacs support Emacs packages, but Spacemacs also features so-called layers, which are configuration files that integrate one, or several packages, seamlessly into Spacemacs particular workflow. This particular workflow is what gave Spacemacs its name. Instead of relying on ESC, CTRL, ALT or META like Emacs, users can launch functions by typing Space in Normal mode and then a sequence of letters. For instance, Spaceqr restarts Spacemacs. And what’s more, you don’t actually need to learn these new key sequences. When you type Space, the minibuffer, a little popup window at the bottom of Spacemacs, appears and shows you all the options that you can type. For instance, typing b after Space opens up the buffer menu. Buffers are what could be called tabs in Rstudio. Here you can chose to delete a buffer, with d, create a new buffer with N, and many more options.\n\n\n\n\n\nEnough text, let’s get into the videos. But keep in mind the following: the videos below show the keystrokes I am typing to perform the actions. However, because I use the BÉPO keyboard layout, which is the french equivalent of the DVORAK layout, the keystrokes will be different than those in a regular vi guide, which are mainly written for the QWERTY layout. Also, to use Spacemacs for R, you need to enable the ESS layer, which I show how to do at the end. Enabling this layer will turn on auto-completion, as well as provide documentation in real time for your function in the minibuffer:\n\n\n\n\n\n\n\n\nThe first video shows Spacemacs divided into two windows. On the left, I am navigating around code using the T (move down) and S (move up) keys. To execute a region that I select, I type Spacemrr (this stands for Major mode Run Region). Then around second 5, I key O which switches to Insert mode one line below the line I was, type head(mtcars) and then ESC to switch back to Normal mode and run the line with Spacemrl (Major mode Run Line).\n\n\n\n\nYour browser does not support the video tag. \n\n\nIn this video, I show you how to switch between windows. Type SpaceN to switch to window N. At the end, I key dd which deletes a whole line.\n\n\n\n\nYour browser does not support the video tag. \n\n\nIn the video below, I show how to use the pipe operator with Spacemm. This is a keyboard shortcut that I have defined myself. You can also spot the auto-completion at work in this video. To run the code, I first select it with V, which selects the whole line the cursor is currently at and enters Visual mode. I then select the lines below with T and run the region with Spacemrr.\n\n\n\n\nYour browser does not support the video tag. \n\n\nHere I show how plotting behaves. When a plot is created, a new window is opened with the plot. This is a major shortcoming of using Spacemacs for R programming; there is not a dedicated buffer for plots, and it only shows the very last one created, so there is no way to keep all the plots created in the current session in a neat, dedicated buffer. It seems to be possible using Org-mode, which is an Emacs mode for writing notes, todos, and authoring documents. But I haven’t explored this option yet, mainly because in my case, only looking at one plot at a time is ok.\n\n\n\n\nYour browser does not support the video tag. \n\n\nHere I show how to quickly add text to the top of the document when at the cursor is at the bottom: I try to use the tabyl() function found in the {janitor} package, which I forgot to load. I quickly go all the way up with gg, then key yy to copy the first line, then P to paste it on the line below (p would paste it on the same line), type fv, to find the letter v from the word “tidyverse”, then type liw (which is the BÉPO equivalent of ciw for Change In Word) and finally change “tidyverse” to “janitor”. This seems overly complex, but once you get used to this way of working, you will wonder why you hadn’t tried vi sooner.\n\n\n\n\nYour browser does not support the video tag. \n\n\nHere I show how to do block comment. 8gg jumps to the 8th line, CTRLv starts block visual mode, which allows me to select a block of text. I select the first column of the text, G to jump all the way down, then A to enter insert mode at the end of the selection (actually, it would have been more logical to use I, which enters insert mode at the beginning of the selection) of the line and then add “#” to comment.\n\n\n\n\nYour browser does not support the video tag. \n\n\nHere I show how to delete a block of text:\n\n\n\n\nYour browser does not support the video tag. \n\n\nSearch and replace, by entering command-line mode (look at the very bottom of the window):\n\n\n\n\nYour browser does not support the video tag. \n\n\nI forgot to add “,” characters on a bunch of lines. I add the first “,” to the first line, go down and press ESC to exit Insert mode. Now in Normal mode, I type . to execute the last command, which is inserting a “,” character and going down a line. This dot command is a feature of vi, and it will always redo the last performed change.\n\n\n\n\nYour browser does not support the video tag. \n\n\nBut instead of typing . six times, just type 6. and be done with it:\n\n\n\n\nYour browser does not support the video tag. \n\n\nWhat if you want to do something more complex, involving several commands? Here the dot command won’t be enough, since it only replicates the last command, not more. For this you can define macros with **@**. I look for the “,” character, twice, and put the rest of the characters in the next line with enter. I then repeat this operation by executing the macro using @‌‌a repeatedly (@‌‌a because I saved the actions in a, but it could have been any other letter). I then undo my changes and execute the macro 5 times with 5@‌‌a.\n\n\n\n\nYour browser does not support the video tag. \n\n\nHere I show the undo tree (by typing Spaceua), which is a feature Spacemacs inherited from Emacs: it makes undoing changes and going back to a previous version of your script very easily:\n\n\n\n\nYour browser does not support the video tag. \n\n\nFinally, I show my Spacemacs configuration file. I show where one needs to specify the layers one wishes to use. For R, the ESS layer (which is a configuration file for the ESS Emacs package) is mandatory. As I explained above, it is also possible to use Emacs packages for which no layer is available. These are the packages under dotspacemacs-additional-packages. In my case I use:\n\ndotspacemacs-additional-packages '(polymode\n                                  poly-R\n                                  poly-noweb\n                                  poly-markdown)\n\nwhich makes working with RMarkdown possible. polymode enables simultaneous Major modes, which is needed for RMarkdown (because RMarkdown files mix Markdown and R).\n\n\n\n\nYour browser does not support the video tag. \n\n\nThat’s the end of this long post. Spacemacs is really a joy to use, but the learning curve is quite steep. However, it is definitely worth it. There are so many packages available for Emacs (and hence Spacemacs) that allow you to browse the web, play games, listen to music, send and read emails… that a recurrent joke is that Emacs is a very nice operating system, but it lacks a decent editor. If that’s the case, Spacemacs is the perfect operating system, because it includes the greatest editor, vi.\n\n\nIf you’re interested and and want to learn more about vi, I advise you to read the following book Vim Recipes (pdf warning, free) or Practical Vim, Edit Text at the Speed of thought (not free, but worth every cent), and Use Vim Like a Pro, which I have not read, but it looks quite good, and is free too if you want. Now this only covers the vi part, not the Emacs aspects of Spacemacs, but you don’t really need to know about Emacs to use Spacemacs. I had 0 experience with Emacs, and still have 0 experience with it. I only learned how to configure Spacemacs, which does not require any previous experience. To find the packages you need, as usual, use any search engine of your liking.\n\n\nThe last point I want to address is the built-in Vim mode of Rstudio. While it works, it does not work 100% as regular Vim, and worst of all, does not support, as far as I know, any other keyboard layout than QWERTY, which is a nogo for me.\n\n\nIn any case, if you’re looking to learn something new that you can use for many programs, including Rstudio, learn Vim, and then give Spacemacs a try. Chaining keystrokes to edit text gets addictive very quickly.\n\n\nFor reference, here is my dotspacemacs/user-config, which is where I defined the shortcut for the %&gt;% operator.\n\n(defun dotspacemacs/user-config ()\n  \"Configuration for user code:\nThis function is called at the very end of Spacemacs startup, after layer\nconfiguration.\nPut your configuration code here, except for variables that should be set\nbefore packages are loaded.\"\n;;; R modes\n  (add-to-list 'auto-mode-alist '(\"\\\\.md\" . poly-markdown-mode))\n  (add-to-list 'auto-mode-alist '(\"\\\\.Snw\" . poly-noweb+r-mode))\n  (add-to-list 'auto-mode-alist '(\"\\\\.Rnw\" . poly-noweb+r-mode))\n  (add-to-list 'auto-mode-alist '(\"\\\\.Rmd\" . poly-markdown+r-mode))\n\n  ;; (require 'poly-R)\n  ;; (require 'poly-markdown)\n  ;; (add-to-list 'auto-mode-alist '(\"\\\\.Rmd\" . poly-markdown+r-mode))\n\n  (global-company-mode t)\n  (global-hl-line-mode 1) ; Enable/Disable current line highlight\n  (setq-default fill-column 99)\n  (setq-default auto-fill-mode t)\n  ;; ESS shortcuts\n  (spacemacs/set-leader-keys \"mdt\" 'ess-r-devtools-test-package)\n  (spacemacs/set-leader-keys \"mrl\" 'ess-eval-line)\n  (spacemacs/set-leader-keys \"mrr\" 'ess-eval-region)\n  (spacemacs/set-leader-keys \"mdb\" 'ess-r-devtools-build-package)\n  (spacemacs/set-leader-keys \"mdd\" 'ess-r-devtools-document-package)\n  (spacemacs/set-leader-keys \"mdl\" 'ess-r-devtools-load-package)\n  (spacemacs/set-leader-keys \"mdc\" 'ess-r-devtools-check-package)\n  (spacemacs/set-leader-keys \"mdp\" 'ess-r-package-mode)\n  (add-hook 'ess-mode-hook\n            (lambda ()\n              (ess-toggle-underscore nil)))\n  (define-key evil-normal-state-map (kbd \"SPC mm\")\n            (lambda ()\n              (interactive)\n              (insert \" %&gt;% \")\n              (evil-insert-state)\n              ))\n  ;; Move lines around\n  (spacemacs/set-leader-keys \"MS\" 'move-text-line-up)\n  (spacemacs/set-leader-keys \"MT\" 'move-text-line-down)\n  (setq-default whitespace-mode t)\n  (setq-default whitespace-style (quote (spaces tabs newline space-mark tab-mark newline-mark)))\n  (setq-default whitespace-display-mappings\n        ;; all numbers are Unicode codepoint in decimal. try (insert-char 182 ) to see it\n        '(\n          (space-mark 32 [183] [46]) ; 32 SPACE, 183 MIDDLE DOT 「·」, 46 FULL STOP 「.」\n          (newline-mark 10 [9226 10]) ; 10 LINE FEED\n          (tab-mark 9 [9655 9] [92 9]) ; 9 TAB, 9655 WHITE RIGHT-POINTING TRIANGLE 「▷」\n          ))\n  (setq-default TeX-view-program-selection\n         '((output-pdf \"PDF Viewer\")))\n  (setq-default TeX-view-program-list\n        '((\"PDF Viewer\" \"okular %o\")))\n  (setq-default indent-tabs-mode nil)\n  (setq-default tab-width 2)\n   ;; (setq org-default-notes-file (concat org-directory \"/agenda/notes.org\"))\n   (add-hook 'prog-mode-hook 'spacemacs/toggle-fill-column-indicator-on)\n   (add-hook 'text-mode-hook 'spacemacs/toggle-fill-column-indicator-on)\n   (add-hook 'markdown-mode-hook 'spacemacs/toggle-fill-column-indicator-on)\n  )"
  },
  {
    "objectID": "posts/2018-04-14-playing_with_furrr.html",
    "href": "posts/2018-04-14-playing_with_furrr.html",
    "title": "Imputing missing values in parallel using {furrr}",
    "section": "",
    "text": "Today I saw this tweet on my timeline:\n\n\n\nFor those of us that just can't wait until RStudio officially supports parallel purrr in #rstats, boy have I got something for you. Introducing furrr, parallel purrr through the use of futures. Go ahead, break things, you know you want to:https://t.co/l9z1UC2Tew\n\n— Davis Vaughan (@dvaughan32) April 13, 2018\n\n\n\nand as a heavy {purrr} user, as well as the happy owner of a 6-core AMD Ryzen 5 1600X cpu, I was very excited to try out {furrr}. For those unfamiliar with {purrr}, you can read some of my previous blog posts on it here, here or here.\n\n\nTo summarize very quickly: {purrr} contains so-called higher order functions, which are functions that take other functions as argument. One such function is map(). Consider the following simple example:\n\nnumbers &lt;- seq(1, 10)\n\nIf you want the square root of this numbers, you can of course simply use the sqrt() function, because it is vectorized:\n\nsqrt(numbers)\n##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751\n##  [8] 2.828427 3.000000 3.162278\n\nBut in a lot of situations, the solution is not so simple. Sometimes you have to loop over the values. This is what we would need to do if sqrt() was not vectorized:\n\nsqrt_numbers &lt;- rep(0, 10)\n\nfor(i in length(numbers)){\n  sqrt_numbers[i] &lt;- sqrt(numbers[i])\n}\n\nFirst, you need to initialize a container, and then you have to populate the sqrt_numbers list with the results. Using, {purrr} is way easier:\n\nlibrary(tidyverse)\nmap(numbers, sqrt)\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 1.414214\n## \n## [[3]]\n## [1] 1.732051\n## \n## [[4]]\n## [1] 2\n## \n## [[5]]\n## [1] 2.236068\n## \n## [[6]]\n## [1] 2.44949\n## \n## [[7]]\n## [1] 2.645751\n## \n## [[8]]\n## [1] 2.828427\n## \n## [[9]]\n## [1] 3\n## \n## [[10]]\n## [1] 3.162278\n\nmap() is only one of the nice functions that are bundled inside {purrr}. Mastering {purrr} can really make you a much more efficient R programmer. Anyways, recently, I have been playing around with imputation and the {mice} package. {mice} comes with an example dataset called boys, let’s take a look at it:\n\nlibrary(mice)\n\ndata(boys)\n\nbrotools::describe(boys) %&gt;%\n  select(variable, type, n_missing, everything())\n## # A tibble: 9 x 13\n##   variable type    n_missing  nobs   mean    sd mode     min   max   q25\n##   &lt;chr&gt;    &lt;chr&gt;       &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numeric         0   748   9.16  6.89 0.035  0.035  21.2  1.58\n## 2 bmi      Numeric        21   748  18.1   3.05 14.54 11.8    31.7 15.9 \n## 3 hc       Numeric        46   748  51.5   5.91 33.7  33.7    65   48.1 \n## 4 hgt      Numeric        20   748 132.   46.5  50.1  50     198   84.9 \n## 5 tv       Numeric       522   748  11.9   7.99 &lt;NA&gt;   1      25    4   \n## 6 wgt      Numeric         4   748  37.2  26.0  3.65   3.14  117.  11.7 \n## 7 gen      Factor        503   748  NA    NA    &lt;NA&gt;  NA      NA   NA   \n## 8 phb      Factor        503   748  NA    NA    &lt;NA&gt;  NA      NA   NA   \n## 9 reg      Factor          3   748  NA    NA    south NA      NA   NA   \n## # ... with 3 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, n_unique &lt;int&gt;\n\nIn the code above I use the describe() function from my personal package to get some summary statistics of the boys dataset (you can read more about this function here). I am especially interested in the number of missing values, which is why I re-order the columns. If I did not re-order the columns, it would not appear in the output on my blog.\n\n\nWe see that some columns have a lot of missing values. Using the mice function, it is very easy to impute them:\n\nstart &lt;- Sys.time()\nimp_boys &lt;- mice(boys, m = 10, maxit = 100, printFlag = FALSE)\nend &lt;- Sys.time() - start\n\nprint(end)\n## Time difference of 3.290611 mins\n\nImputation on a single core took around 3 minutes on my computer. This might seem ok, but if you have a larger data set with more variables, 3 minutes can become 3 hours. And if you increase maxit, which helps convergence, or the number of imputations, 3 hours can become 30 hours. With a 6-core CPU this could potentially be brought down to 5 hours (in theory). Let’s see if we can go faster, but first let’s take a look at the imputed data.\n\n\nThe mice() function returns a mids object. If you want to look at the data, you have to use the complete() function (careful, there is also a complete() function in the {tidyr} package, so to avoid problems, I suggest you explicitely call mice::complete()):\n\nimp_boys &lt;- mice::complete(imp_boys, \"long\")\n\nbrotools::describe(imp_boys) %&gt;%\n  select(variable, type, n_missing, everything())\n## # A tibble: 11 x 13\n##    variable type   n_missing  nobs   mean     sd mode     min   max    q25\n##    &lt;chr&gt;    &lt;chr&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n##  1 .id      Numer…         0  7480 374.   216.   1      1     748   188.  \n##  2 .imp     Numer…         0  7480   5.5    2.87 1      1      10     3   \n##  3 age      Numer…         0  7480   9.16   6.89 0.035  0.035  21.2   1.58\n##  4 bmi      Numer…         0  7480  18.0    3.03 14.54 11.8    31.7  15.9 \n##  5 hc       Numer…         0  7480  51.6    5.89 33.7  33.7    65    48.3 \n##  6 hgt      Numer…         0  7480 131.    46.5  50.1  50     198    83   \n##  7 tv       Numer…         0  7480   8.39   8.09 2      1      25     2   \n##  8 wgt      Numer…         0  7480  37.1   26.0  3.65   3.14  117.   11.7 \n##  9 gen      Factor         0  7480  NA     NA    G1    NA      NA    NA   \n## 10 phb      Factor         0  7480  NA     NA    P1    NA      NA    NA   \n## 11 reg      Factor         0  7480  NA     NA    south NA      NA    NA   \n## # ... with 3 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, n_unique &lt;int&gt;\n\nAs expected, no more missing values. The “long” argument inside mice::complete() is needed if you want the complete() function to return a long dataset. Doing the above “manually” using {purrr} is possible with the following code:\n\nstart &lt;- Sys.time()\nimp_boys_purrr &lt;- map(rep(1, 10), ~mice(data = boys, m = ., maxit = 100, printFlag = FALSE))\nend &lt;- Sys.time() - start\n\nprint(end)\n## Time difference of 3.393966 mins\n\nWhat this does is map the function ~mice(data = boys, m = ., maxit = 100, printFlag = FALSE) to a list of 1s, and creates 10 imputed data sets. m = . means that m will be equal to whatever is inside the list we are mapping our function over, so 1, then 1 then another 1 etc…. It took around the same amount of time as using mice() directly.\n\n\nimp_boys_purrr is now a list of 10 mids objects. We thus need to map mice::complete() to imp_boys_purrr to get the data:\n\nimp_boys_purrr_complete &lt;- map(imp_boys_purrr, mice::complete)\n\nNow, imp_boys_purrr_complete is a list of 10 datasets. Let’s map brotools::describe() to it:\n\nmap(imp_boys_purrr_complete, brotools::describe)\n## [[1]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.03 14.54 11.8    31.7 15.9    17.4  19.5\n## 3 hc       Numer…   748  51.7   5.90 33.7  33.7    65   48.3    53.1  56  \n## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   84     146.  175. \n## 5 tv       Numer…   748   8.35  8.00 3      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.2  26.0  3.65   3.14  117.  11.7    34.7  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[2]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.03 14.54 11.8    31.7 15.9    17.5  19.5\n## 3 hc       Numer…   748  51.6   5.88 33.7  33.7    65   48.3    53.2  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.5   145.  175  \n## 5 tv       Numer…   748   8.37  8.02 1      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.1  26.0  3.65   3.14  117.  11.9    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P2    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[3]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.1   3.04 14.54 11.8    31.7 15.9    17.5  19.5\n## 3 hc       Numer…   748  51.6   5.87 33.7  33.7    65   48.5    53.3  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.0   145.  175  \n## 5 tv       Numer…   748   8.46  8.14 2      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.2  26.1  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[4]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.1   3.02 14.54 11.8    31.7 15.9    17.5  19.4\n## 3 hc       Numer…   748  51.7   5.93 33.7  33.7    65   48.5    53.4  56  \n## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   82.9   145.  175  \n## 5 tv       Numer…   748   8.45  8.11 2      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.2  26.0  3.65   3.14  117.  11.7    34.7  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[5]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.03 14.54 11.8    31.7 15.9    17.5  19.5\n## 3 hc       Numer…   748  51.6   5.91 33.7  33.7    65   48.3    53.2  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.0   146.  175. \n## 5 tv       Numer…   748   8.21  8.02 3      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.1  26.0  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[6]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.05 14.54 11.8    31.7 15.9    17.4  19.5\n## 3 hc       Numer…   748  51.7   5.89 33.7  33.7    65   48.3    53.2  56  \n## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   83.0   146.  175  \n## 5 tv       Numer…   748   8.44  8.24 3      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.1  26.0  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[7]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.1   3.04 14.54 11.8    31.7 15.9    17.4  19.5\n## 3 hc       Numer…   748  51.6   5.88 33.7  33.7    65   48.2    53.2  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.5   146.  175  \n## 5 tv       Numer…   748   8.47  8.15 2      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.2  26.1  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[8]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.04 14.54 11.8    31.7 15.9    17.4  19.4\n## 3 hc       Numer…   748  51.6   5.85 33.7  33.7    65   48.2    53.3  56  \n## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   83.0   146.  175  \n## 5 tv       Numer…   748   8.36  8.06 2      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.2  26.1  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[9]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.05 14.54 11.8    31.7 15.9    17.4  19.5\n## 3 hc       Numer…   748  51.6   5.90 33.7  33.7    65   48.3    53.2  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.9   146.  175  \n## 5 tv       Numer…   748   8.57  8.25 1      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.1  26.1  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[10]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.04 14.54 11.8    31.7 15.9    17.4  19.5\n## 3 hc       Numer…   748  51.6   5.89 33.7  33.7    65   48.3    53.1  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.0   146.  175  \n## 5 tv       Numer…   748   8.49  8.18 2      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.1  26.1  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n\nBefore merging this 10 datasets together into one, it would be nice to have a column with the id of the datasets. This can easily be done with a variant of purrr::map(), called map2():\n\nimp_boys_purrr &lt;- map2(.x = seq(1,10), .y = imp_boys_purrr_complete, ~mutate(.y, imp_id = as.character(.x)))\n\nmap2() applies a function, say f(), to 2 lists sequentially: f(x_1, y_1), then f(x_2, y_2), etc… So here I map mutate() to create a new column, imp_id in each dataset. Now let’s bind the rows and take a look at the data:\n\nimp_boys_purrr &lt;- bind_rows(imp_boys_purrr)\n\nimp_boys_purrr %&gt;%\n  brotools::describe() %&gt;%\n  select(variable, type, n_missing, everything())\n## # A tibble: 10 x 13\n##    variable type     n_missing  nobs   mean    sd mode     min   max   q25\n##    &lt;chr&gt;    &lt;chr&gt;        &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 age      Numeric          0  7480   9.16  6.89 0.035  0.035  21.2  1.58\n##  2 bmi      Numeric          0  7480  18.0   3.04 14.54 11.8    31.7 15.9 \n##  3 hc       Numeric          0  7480  51.6   5.89 33.7  33.7    65   48.3 \n##  4 hgt      Numeric          0  7480 131.   46.5  50.1  50     198   83   \n##  5 tv       Numeric          0  7480   8.42  8.11 3      1      25    2   \n##  6 wgt      Numeric          0  7480  37.1  26.0  3.65   3.14  117.  11.7 \n##  7 imp_id   Charact…         0  7480  NA    NA    1     NA      NA   NA   \n##  8 gen      Factor           0  7480  NA    NA    G1    NA      NA   NA   \n##  9 phb      Factor           0  7480  NA    NA    P1    NA      NA   NA   \n## 10 reg      Factor           0  7480  NA    NA    south NA      NA   NA   \n## # ... with 3 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, n_unique &lt;int&gt;\n\nYou may ask yourself why I am bothering with all this. This will become apparent now. We can now use the code we wrote to get our 10 imputed datasets using purrr::map() and simply use furrr::future_map() to parallelize the imputation process:\n\nlibrary(furrr)\n## Loading required package: future\nplan(multiprocess)\n\nstart &lt;- Sys.time()\nimp_boys_future &lt;- future_map(rep(1, 10), ~mice(data = boys, m = ., maxit = 100, printFlag = FALSE))\nend &lt;- Sys.time() - start\n\nprint(end)\n## Time difference of 33.73772 secs\n\nBoooom! Much faster! And simply by loading {furrr}, then using plan(multiprocess) to run the code in parallel (if you forget that, the code will run on a single core) and using future_map() instead of map().\n\n\nLet’s take a look at the data:\n\nimp_boys_future_complete &lt;- map(imp_boys_future, mice::complete)\n\nimp_boys_future &lt;- map2(.x = seq(1,10), .y = imp_boys_future_complete, ~mutate(.y, imp_id = as.character(.x)))\n\nimp_boys_future &lt;- bind_rows(imp_boys_future)\n\nimp_boys_future %&gt;%\n  brotools::describe() %&gt;%\n  select(variable, type, n_missing, everything())\n## # A tibble: 10 x 13\n##    variable type     n_missing  nobs   mean    sd mode     min   max   q25\n##    &lt;chr&gt;    &lt;chr&gt;        &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 age      Numeric          0  7480   9.16  6.89 0.035  0.035  21.2  1.58\n##  2 bmi      Numeric          0  7480  18.0   3.04 14.54 11.8    31.7 15.9 \n##  3 hc       Numeric          0  7480  51.6   5.89 33.7  33.7    65   48.4 \n##  4 hgt      Numeric          0  7480 131.   46.5  50.1  50     198   83   \n##  5 tv       Numeric          0  7480   8.35  8.09 3      1      25    2   \n##  6 wgt      Numeric          0  7480  37.1  26.0  3.65   3.14  117.  11.7 \n##  7 imp_id   Charact…         0  7480  NA    NA    1     NA      NA   NA   \n##  8 gen      Factor           0  7480  NA    NA    G1    NA      NA   NA   \n##  9 phb      Factor           0  7480  NA    NA    P1    NA      NA   NA   \n## 10 reg      Factor           0  7480  NA    NA    south NA      NA   NA   \n## # ... with 3 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, n_unique &lt;int&gt;\n\nSo imputation went from 3.4 minutes (around 200 seconds) to 30 seconds. How cool is that? If you want to play around with {furrr} you must install it from Github, as it is not yet available on CRAN:\n\ndevtools::install_github(\"DavisVaughan/furrr\")\n\nIf you are not comfortable with map() (and thus future_map()) but still want to impute in parallel, there is this very nice script here to do just that. I created a package around this script, called parlMICE (the same name as the script), to make installation and usage easier. You can install it like so:\n\ndevtools::install_github(\"b-rodrigues/parlMICE\")\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2018-07-01-tidy_ive.html",
    "href": "posts/2018-07-01-tidy_ive.html",
    "title": "Missing data imputation and instrumental variables regression: the tidy approach",
    "section": "",
    "text": "In this blog post I will discuss missing data imputation and instrumental variables regression. This is based on a short presentation I will give at my job. You can find the data used here on this website: http://eclr.humanities.manchester.ac.uk/index.php/IV_in_R\n\n\nThe data is used is from Wooldridge’s book, Econometrics: A modern Approach. You can download the data by clicking here.\n\n\nThis is the variable description:\n\n 1. inlf                     =1 if in labor force, 1975\n 2. hours                    hours worked, 1975\n 3. kidslt6                  # kids &lt; 6 years\n 4. kidsge6                  # kids 6-18\n 5. age                      woman's age in yrs\n 6. educ                     years of schooling\n 7. wage                     estimated wage from earns., hours\n 8. repwage                  reported wage at interview in 1976\n 9. hushrs                   hours worked by husband, 1975\n10. husage                   husband's age\n11. huseduc                  husband's years of schooling\n12. huswage                  husband's hourly wage, 1975\n13. faminc                   family income, 1975\n14. mtr                      fed. marginal tax rate facing woman\n15. motheduc                 mother's years of schooling\n16. fatheduc                 father's years of schooling\n17. unem                     unem. rate in county of resid.\n18. city                     =1 if live in SMSA\n19. exper                    actual labor mkt exper\n20. nwifeinc                 (faminc - wage*hours)/1000\n21. lwage                    log(wage)\n22. expersq                  exper^2\n\nThe goal is to first impute missing data in the data set, and then determine the impact of one added year of education on wages. If one simply ignores missing values, bias can be introduced depending on the missingness mechanism. The second problem here is that education is likely to be endogeneous (and thus be correlated to the error term), as it is not randomly assigned. This causes biased estimates and may lead to seriously wrong conclusions. So missingness and endogeneity should be dealt with, but dealing with both issues is more of a programming challenge than an econometrics challenge. Thankfully, the packages contained in the {tidyverse} as well as {mice} will save the day!\n\n\nIf you inspect the data, you will see that there are no missing values. So I will use the {mice} package to first ampute the data (which means adding missing values). This, of course, is done for education purposes. If you’re lucky enough to not have missing values in your data, you shouldn’t add them!\n\n\nLet’s load all the packages needed:\n\nlibrary(tidyverse)\nlibrary(AER)\nlibrary(naniar)\nlibrary(mice)\n\nSo first, let’s read in the data, and ampute it:\n\nwages_data &lt;- read_csv(\"http://eclr.humanities.manchester.ac.uk/images/5/5f/Mroz.csv\")\n## Parsed with column specification:\n## cols(\n##   .default = col_integer(),\n##   wage = col_character(),\n##   repwage = col_double(),\n##   huswage = col_double(),\n##   mtr = col_double(),\n##   unem = col_double(),\n##   nwifeinc = col_double(),\n##   lwage = col_character()\n## )\n## See spec(...) for full column specifications.\n\nFirst, I only select the variables I want to use and convert them to the correct class:\n\nwages_data &lt;- wages_data %&gt;% \n    select(wage, educ, fatheduc, motheduc, inlf, hours, \n               kidslt6, kidsge6, age, huswage, \n               mtr, unem, city, exper) %&gt;% \n    mutate_at(vars(kidslt6, kidsge6, hours, educ, age, wage, huswage, mtr,\n                    motheduc, fatheduc, unem, exper), as.numeric) %&gt;% \n    mutate_at(vars(city, inlf), as.character)\n## Warning in evalq(as.numeric(wage), &lt;environment&gt;): NAs introduced by\n## coercion\n\nIn the data, some women are not in the labour force, and thus do not have any wages; meaning they should have a 0 there. Instead, this is represented with the following symbol: “.”. So I convert these dots to 0. One could argue that the wages should not be 0, but that they’re truly missing. This is true, and there are ways to deal with such questions (Heckman’s selection model for instance), but this is not the point here.\n\nwages_data &lt;- wages_data %&gt;% \n    mutate(wage = ifelse(is.na(wage), 0, wage))\n\nLet’s double check if there are any missing values in the data, using naniar::vis_miss():\n\nvis_miss(wages_data)\n\n\n\n\nNope! Let’s ampute it:\n\nwages_mis &lt;- ampute(wages_data)$amp\n## Warning: Data is made numeric because the calculation of weights requires\n## numeric data\n\nampute() returns an object where the amp element is the amputed data. This is what I save into the new variable wages_mis.\n\n\nLet’s take a look:\n\nvis_miss(wages_mis)\n\n\n\n\nOk, so now we have missing values. Let’s use the recently added mice::parlmice() function to impute the dataset, in parallel:\n\nimp_wages &lt;- parlmice(data = wages_mis, m = 10, maxit = 20, cl.type = \"FORK\")\n\nFor reproducibility, I save these objects to disk:\n\nwrite_csv(wages_mis, \"wages_mis.csv\")\n\nsaveRDS(imp_wages, \"imp_wages.rds\")\n\nAs a sanity check, let’s look at the missingness pattern for the first completed dataset:\n\nvis_miss(complete(imp_wages))\n\n\n\n\nmice::parlmice() was able to impute the dataset. I imputed it 10 times, so now I have 10 imputed datasets. If I want to estimate a model using this data, I will need to do so 10 times. This is where the tidyverse comes into play. First, let’s combine all the 10 imputed datasets into one long dataset, with an index to differentiate them. This is done easily with mice::complete():\n\nimp_wages_df &lt;- mice::complete(imp_wages, \"long\")\n\nLet’s take a look at the data:\n\nhead(imp_wages_df)\n##   .imp .id   wage educ fatheduc motheduc inlf hours kidslt6 kidsge6 age\n## 1    1   1 3.3540   12        7       12    1  1610       1       0  32\n## 2    1   2 1.3889   12        7        7    1  1656       0       2  30\n## 3    1   3 4.5455   12        7       12    1  1980       0       3  35\n## 4    1   4 1.0965   12        7        7    1   456       0       3  34\n## 5    1   5 4.5918   14       14       12    1  1568       1       2  31\n## 6    1   6 4.7421   12        7       14    1  2032       0       0  54\n##   huswage    mtr unem city exper\n## 1  4.0288 0.7215  5.0    0    14\n## 2  8.4416 0.6615 11.0    1     5\n## 3  3.5807 0.6915  5.0    0    15\n## 4  3.5417 0.7815  5.0    0     6\n## 5 10.0000 0.6215  9.5    1    14\n## 6  4.7364 0.6915  7.5    1    33\n\nAs you can see, there are two new columns, .id and .imp. .imp equals i means that it is the ith imputed dataset.\n\n\nBecause I have 0’s in my dependent variable, I will not log the wages but instead use the Inverse Hyperbolic Sine transformation. Marc F. Bellemare wrote a nice post about it here.\n\nihs &lt;- function(x){\n    log(x + sqrt(x**2 + 1))\n}\n\nI can now apply this function, but first I have to group by .imp. Remember, these are 10 separated datasets. I also create the experience squared:\n\nimp_wages_df &lt;- imp_wages_df %&gt;% \n    group_by(.imp) %&gt;% \n    mutate(ihs_wage = ihs(wage),\n           exper2 = exper**2)\n\nNow comes some tidyverse magic. I will create a new dataset by using the nest() function from tidyr.\n\n(imp_wages &lt;- imp_wages_df %&gt;% \n    group_by(.imp) %&gt;% \n    nest())\n## # A tibble: 10 x 2\n##     .imp data               \n##    &lt;int&gt; &lt;list&gt;             \n##  1     1 &lt;tibble [753 × 17]&gt;\n##  2     2 &lt;tibble [753 × 17]&gt;\n##  3     3 &lt;tibble [753 × 17]&gt;\n##  4     4 &lt;tibble [753 × 17]&gt;\n##  5     5 &lt;tibble [753 × 17]&gt;\n##  6     6 &lt;tibble [753 × 17]&gt;\n##  7     7 &lt;tibble [753 × 17]&gt;\n##  8     8 &lt;tibble [753 × 17]&gt;\n##  9     9 &lt;tibble [753 × 17]&gt;\n## 10    10 &lt;tibble [753 × 17]&gt;\n\nAs you can see, imp_wages is now a dataset with two columns: .imp, indexing the imputed datasets, and a column called data, where each element is itself a tibble! data is a so-called list-column. You can read more about it on the purrr tutorial written by Jenny Bryan.\n\n\nEstimating a model now is easy, if you’re familiar with purrr. This is how you do it:\n\nimp_wages_reg = imp_wages %&gt;% \n    mutate(lin_reg = map(data, \n                         ~lm(ihs_wage ~ educ + inlf + hours + \n                                 kidslt6 + kidsge6 + age + huswage + \n                                 mtr + unem + city + exper + exper2, \n                             data = .)))\n\nOk, so what happened here? imp_wages is a data frame, so it’s possible to add a column to it with mutate(). I call that column lin_reg and use map() on the column called data (remember, this column is actually a list of data frame objects, and map() takes a list as an argument, and then a function or formula) with the following formula:\n\n~lm(ihs_wage ~ educ + inlf + hours + \n        kidslt6 + kidsge6 + age + huswage + \n        mtr + unem + city + exper + exper2, \n    data = .)\n\nThis formula is nothing more that a good old linear regression. The last line data = . means that the data to be used inside lm() should be coming from the list called data, which is the second column of imp_wages. As I’m writing these lines, I realize it is confusing as hell. But I promise you that learning to use purrr is a bit like learning how to use a bicycle. Very difficult to explain, but once you know how to do it, it feels super natural. Take some time to play with the lines above to really understand what happened.\n\n\nNow, let’s take a look at the result:\n\nimp_wages_reg\n## # A tibble: 10 x 3\n##     .imp data                lin_reg\n##    &lt;int&gt; &lt;list&gt;              &lt;list&gt; \n##  1     1 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  2     2 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  3     3 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  4     4 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  5     5 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  6     6 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  7     7 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  8     8 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  9     9 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n## 10    10 &lt;tibble [753 × 17]&gt; &lt;lm&gt;\n\nimp_wages_reg now has a third column called lin_reg where each element is a linear model, estimated on the data from the data column! We can now pool the results of these 10 regressions using mice::pool():\n\npool_lin_reg &lt;- pool(imp_wages_reg$lin_reg)\n\nsummary(pool_lin_reg)\n##                  estimate    std.error  statistic       df      p.value\n## (Intercept)  1.2868701172 3.214473e-01  4.0033628 737.9337 6.876133e-05\n## educ         0.0385310276 8.231906e-03  4.6806931 737.9337 3.401935e-06\n## inlf         1.8845418354 5.078235e-02 37.1101707 737.9337 0.000000e+00\n## hours       -0.0001164143 3.011378e-05 -3.8658143 737.9337 1.204773e-04\n## kidslt6     -0.0438925013 3.793152e-02 -1.1571510 737.9337 2.475851e-01\n## kidsge6     -0.0117978229 1.405226e-02 -0.8395678 737.9337 4.014227e-01\n## age         -0.0030084595 2.666614e-03 -1.1281946 737.9337 2.596044e-01\n## huswage     -0.0231736955 5.607364e-03 -4.1327255 737.9337 3.995866e-05\n## mtr         -2.2109176781 3.188827e-01 -6.9333267 737.9337 8.982592e-12\n## unem         0.0028775444 5.462973e-03  0.5267360 737.9337 5.985352e-01\n## city         0.0157414671 3.633755e-02  0.4332011 737.9337 6.649953e-01\n## exper        0.0164364027 6.118875e-03  2.6861806 737.9337 7.389936e-03\n## exper2      -0.0002022602 1.916146e-04 -1.0555575 737.9337 2.915159e-01\n\nThis function averages the results from the 10 regressions and computes correct standard errors. This is based on Rubin’s rules (Rubin, 1987, p. 76). As you can see, the linear regression indicates that one year of added education has a positive, significant effect of log wages (they’re not log wages, I used the IHS transformation, but log wages just sounds better than inverted hyperbolic sined wages). This effect is almost 4%.\n\n\nBut education is not randomly assigned, and as such might be endogenous. This is where instrumental variables come into play. An instrument is a variables that impacts the dependent variable only through the endogenous variable (here, education). For example, the education of the parents do not have a direct impact over one’s wage, but having college-educated parents means that you are likely college-educated yourself, and thus have a higher wage that if you only have a high school diploma.\n\n\nI am thus going to instrument education with both parents’ education:\n\nimp_wages_reg = imp_wages_reg %&gt;% \n    mutate(iv_reg = map(data, \n                         ~ivreg(ihs_wage ~ educ + inlf + hours + \n                                 kidslt6 + kidsge6 + age + huswage + \n                                 mtr + unem + city + exper + exper2 |.-educ + fatheduc + motheduc, \n                             data = .)))\n\nThe only difference from before is the formula:\n\n~ivreg(ihs_wage ~ educ + inlf + hours + \n           kidslt6 + kidsge6 + age + huswage + \n           mtr + unem + city + exper + exper2 |.-educ + fatheduc + motheduc, \n       data = .)\n## ~ivreg(ihs_wage ~ educ + inlf + hours + kidslt6 + kidsge6 + age + \n##     huswage + mtr + unem + city + exper + exper2 | . - educ + \n##     fatheduc + motheduc, data = .)\n\nInstead of lm() I use AER::ivreg() and the formula has a second part, after the | symbol. This is where I specify that I instrument education with the parents’ education.\n\n\nimp_wages_reg now looks like this:\n\nimp_wages_reg\n## # A tibble: 10 x 4\n##     .imp data                lin_reg iv_reg \n##    &lt;int&gt; &lt;list&gt;              &lt;list&gt;  &lt;list&gt; \n##  1     1 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  2     2 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  3     3 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  4     4 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  5     5 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  6     6 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  7     7 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  8     8 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  9     9 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n## 10    10 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n\nLet’s take a look at the results:\n\npool_iv_reg &lt;- pool(imp_wages_reg$iv_reg)\n\nsummary(pool_iv_reg)\n##                  estimate    std.error  statistic       df      p.value\n## (Intercept)  2.0091904157 5.146812e-01  3.9037568 737.9337 1.033832e-04\n## educ         0.0038859137 2.086592e-02  0.1862326 737.9337 8.523136e-01\n## inlf         1.9200207113 5.499457e-02 34.9129122 737.9337 0.000000e+00\n## hours       -0.0001313866 3.157375e-05 -4.1612608 737.9337 3.537881e-05\n## kidslt6     -0.0234593391 4.000689e-02 -0.5863824 737.9337 5.577979e-01\n## kidsge6     -0.0123239220 1.422241e-02 -0.8665145 737.9337 3.864897e-01\n## age         -0.0040874625 2.763340e-03 -1.4791748 737.9337 1.395203e-01\n## huswage     -0.0242737100 5.706497e-03 -4.2536970 737.9337 2.373189e-05\n## mtr         -2.6385172445 3.998419e-01 -6.5989008 737.9337 7.907430e-11\n## unem         0.0047331976 5.622137e-03  0.8418859 737.9337 4.001246e-01\n## city         0.0255647706 3.716783e-02  0.6878197 737.9337 4.917824e-01\n## exper        0.0180917073 6.258779e-03  2.8906127 737.9337 3.957817e-03\n## exper2      -0.0002291007 1.944599e-04 -1.1781381 737.9337 2.391213e-01\n\nAs you can see, education is not statistically significant anymore! This is why it is quite important to think about endogeneity issues. However, it is not always very easy to find suitable instruments. A series of tests exist to determine if you have relevant and strong instruments, but this blog post is already long enough. I will leave this for a future blog post."
  },
  {
    "objectID": "posts/2018-10-21-lux_elections.html",
    "href": "posts/2018-10-21-lux_elections.html",
    "title": "Getting the data from the Luxembourguish elections out of Excel",
    "section": "",
    "text": "In this blog post, similar to a previous blog post I am going to show you how we can go from an Excel workbook that contains data to flat file. I will taking advantage of the structure of the tables inside the Excel sheets by writing a function that extracts the tables and then mapping it to each sheet!\n\n\nLast week, October 14th, Luxembourguish nationals went to the polls to elect the Grand Duke! No, actually, the Grand Duke does not get elected. But Luxembourguish citizen did go to the polls to elect the new members of the Chamber of Deputies (a sort of parliament if you will). The way the elections work in Luxembourg is quite interesting; you can vote for a party, or vote for individual candidates from different parties. The candidates that get the most votes will then seat in the parliament. If you vote for a whole party, each of the candidates get a vote. You get as many votes as there are candidates to vote for. So, for example, if you live in the capital city, also called Luxembourg, you get 21 votes to distribute. You could decide to give 10 votes to 10 candidates of party A and 11 to 11 candidates of party B. Why 21 votes? The chamber of Deputies is made up 60 deputies, and the country is divided into four legislative circonscriptions. So each voter in a circonscription gets an amount of votes that is proportional to the population size of that circonscription.\n\n\nNow you certainly wonder why I put the flag of Gambia on top of this post? This is because the government that was formed after the 2013 elections was made up of a coalition of 3 parties; the Luxembourg Socialist Worker’s Party, the Democratic Party and The Greens. The LSAP managed to get 13 seats in the Chamber, while the DP got 13 and The Greens 6, meaning 32 seats out of 60. So because they made this coalition, they could form the government, and this coalition was named the Gambia coalition because of the colors of these 3 parties: red, blue and green. If you want to take a look at the ballot from 2013 for the southern circonscription, click here.\n\n\nNow that you have the context, we can go back to some data science. The results of the elections of last week can be found on Luxembourg’s Open Data portal, right here. The data is trapped inside Excel sheets; just like I explained in a previous blog post the data is easily read by human, but not easily digested by any type of data analysis software. So I am going to show you how we are going from this big Excel workbook to a flat file.\n\n\nFirst of all, if you open the Excel workbook, you will notice that there are a lot of sheets; there is one for the whole country, named “Le Grand-Duché de Luxembourg”, one for the four circonscriptions, “Centre”, “Nord”, “Sud”, “Est” and 102 more for each commune of the country (a commune is an administrative division). However, the tables are all very similarly shaped, and roughly at the same position.\n\n\n\n\n\nThis is good, because we can write a function to extracts the data and then map it over all the sheets. First, let’s load some packages and the data for the country:\n\nlibrary(\"tidyverse\")\nlibrary(\"tidyxl\")\nlibrary(\"brotools\")\n# National Level 2018\nelections_raw_2018 &lt;- xlsx_cells(\"leg-2018-10-14-22-58-09-737.xlsx\",\n                        sheets = \"Le Grand-Duché de Luxembourg\")\n\n{brotools} is my own package. You can install it with:\n\ndevtools::install_github(\"b-rodrigues/brotools\")\n\nit contains a function that I will use down below. The function I wrote to extract the tables is not very complex, but requires that you are familiar with how {tidyxl} imports Excel workbooks. So if you are not familiar with it, study the imported data frame for a few minutes. It will make understanding the next function easier:\n\nextract_party &lt;- function(dataset, starting_col, target_rows){\n\n    almost_clean &lt;- dataset %&gt;%\n        filter(row %in% target_rows) %&gt;%\n        filter(col %in% c(starting_col, starting_col + 1)) %&gt;%\n        select(character, numeric) %&gt;%\n        fill(numeric, .direction = \"up\") %&gt;%\n        filter(!is.na(character))\n\n    party_name &lt;- almost_clean$character[1] %&gt;%\n        str_split(\"-\", simplify = TRUE) %&gt;%\n        .[2] %&gt;%\n        str_trim()\n\n    almost_clean$character[1] &lt;- \"Pourcentage\"\n\n    almost_clean$party &lt;- party_name\n\n    colnames(almost_clean) &lt;- c(\"Variables\", \"Values\", \"Party\")\n\n    almost_clean %&gt;%\n        mutate(Year = 2018) %&gt;%\n        select(Party, Year, Variables, Values)\n\n}\n\nThis function has three arguments, dataset, starting_col and target_rows. dataset is the data I loaded with xlsx_cells from the {tidyxl} package. I think the following picture illustrates easily what the function does:\n\n\n\n\n\nSo the function first filters only the rows we are interested in, then the cols. I then select the columns I want which are called character and numeric (if the Excel cell contains characters then you will find them in the character column, if it contains numbers you will them in the numeric column), then I fill the empty cells with the values from the numeric column and the I remove the NA’s. These two last steps might not be so clear; this is how the data looks like up until the select() function:\n\n&gt; elections_raw_2018 %&gt;%\n+     filter(row %in% seq(11,19)) %&gt;%\n+     filter(col %in% c(1, 2)) %&gt;%\n+     select(character, numeric)\n# A tibble: 18 x 2\n   character                       numeric\n   &lt;chr&gt;                             &lt;dbl&gt;\n 1 1 - PIRATEN - PIRATEN           NA     \n 2 NA                               0.0645\n 3 Suffrage total                  NA     \n 4 NA                          227549     \n 5 Suffrages de liste              NA     \n 6 NA                          181560     \n 7 Suffrage nominatifs             NA     \n 8 NA                           45989     \n 9 Pourcentage pondéré             NA     \n10 NA                               0.0661\n11 Suffrage total pondéré          NA     \n12 NA                           13394.    \n13 Suffrages de liste pondéré      NA     \n14 NA                           10308     \n15 Suffrage nominatifs pondéré     NA     \n16 NA                            3086.    \n17 Mandats attribués               NA     \n18 NA                               2  \n\nSo by filling the NA’s in the numeric the data now looks like this:\n\n&gt; elections_raw_2018 %&gt;%\n+     filter(row %in% seq(11,19)) %&gt;%\n+     filter(col %in% c(1, 2)) %&gt;%\n+     select(character, numeric) %&gt;%\n+     fill(numeric, .direction = \"up\")\n# A tibble: 18 x 2\n   character                       numeric\n   &lt;chr&gt;                             &lt;dbl&gt;\n 1 1 - PIRATEN - PIRATEN            0.0645\n 2 NA                               0.0645\n 3 Suffrage total              227549     \n 4 NA                          227549     \n 5 Suffrages de liste          181560     \n 6 NA                          181560     \n 7 Suffrage nominatifs          45989     \n 8 NA                           45989     \n 9 Pourcentage pondéré              0.0661\n10 NA                               0.0661\n11 Suffrage total pondéré       13394.    \n12 NA                           13394.    \n13 Suffrages de liste pondéré   10308     \n14 NA                           10308     \n15 Suffrage nominatifs pondéré   3086.    \n16 NA                            3086.    \n17 Mandats attribués                2     \n18 NA                               2 \n\nAnd then I filter out the NA’s from the character column, and that’s almost it! I simply need to add a new column with the party’s name and rename the other columns. I also add a “Year” colmun.\n\n\nNow, each party will have a different starting column. The table with the data for the first party starts on column 1, for the second party it starts on column 4, column 7 for the third party… So the following vector contains all the starting columns:\n\nposition_parties_national &lt;- seq(1, 24, by = 3)\n\n(If you study the Excel workbook closely, you will notice that I do not extract the last two parties. This is because these parties were not present in all of the 4 circonscriptions and are very, very, very small.)\n\n\nThe target rows are always the same, from 11 to 19. Now, I simply need to map this function to this list of positions and I get the data for all the parties:\n\nelections_national_2018 &lt;- map_df(position_parties_national, extract_party, \n                         dataset = elections_raw_2018, target_rows = seq(11, 19)) %&gt;%\n    mutate(locality = \"Grand-Duchy of Luxembourg\", division = \"National\")\n\nI also added the locality and division columns to the data.\n\n\nLet’s take a look:\n\nglimpse(elections_national_2018)\n## Observations: 72\n## Variables: 6\n## $ Party     &lt;chr&gt; \"PIRATEN\", \"PIRATEN\", \"PIRATEN\", \"PIRATEN\", \"PIRATEN\",…\n## $ Year      &lt;dbl&gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, …\n## $ Variables &lt;chr&gt; \"Pourcentage\", \"Suffrage total\", \"Suffrages de liste\",…\n## $ Values    &lt;dbl&gt; 6.446204e-02, 2.275490e+05, 1.815600e+05, 4.598900e+04…\n## $ locality  &lt;chr&gt; \"Grand-Duchy of Luxembourg\", \"Grand-Duchy of Luxembour…\n## $ division  &lt;chr&gt; \"National\", \"National\", \"National\", \"National\", \"Natio…\n\nVery nice.\n\n\nNow we need to do the same for the 4 electoral circonscriptions. First, let’s load the data:\n\n# Electoral districts 2018\ndistricts &lt;- c(\"Centre\", \"Nord\", \"Sud\", \"Est\")\n\nelections_district_raw_2018 &lt;- xlsx_cells(\"leg-2018-10-14-22-58-09-737.xlsx\",\n                                      sheets = districts)\n\nNow things get trickier. Remember I said that the number of seats is proportional to the population of each circonscription? We simply can’t use the same target rows as before. For example, for the “Centre” circonscription, the target rows go from 12 to 37, but for the “Est” circonscription only from 12 to 23. Ideally, we would need a function that would return the target rows.\n\n\nThis is that function:\n\n# The target rows I need to extract are different from district to district\nget_target_rows &lt;- function(dataset, sheet_to_extract, reference_address){\n\n    last_row &lt;- dataset %&gt;%\n        filter(sheet == sheet_to_extract) %&gt;%\n        filter(address == reference_address) %&gt;%\n        pull(numeric)\n\n    seq(12, (11 + 5 + last_row))\n}\n\nThis function needs a dataset, a sheet_to_extract and a reference_address. The reference address is a cell that actually contains the number of seats in that circonscription, in our case “B5”. We can easily get the list of target rows now:\n\n# Get the target rows\nlist_targets &lt;- map(districts, get_target_rows, dataset = elections_district_raw_2018, \n                    reference_address = \"B5\")\n\nlist_targets\n## [[1]]\n##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34\n## [24] 35 36 37\n## \n## [[2]]\n##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n## \n## [[3]]\n##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34\n## [24] 35 36 37 38 39\n## \n## [[4]]\n##  [1] 12 13 14 15 16 17 18 19 20 21 22 23\n\nNow, let’s split the data we imported into a list, where each element of the list is a dataframe with the data from one circonscription:\n\nlist_data_districts &lt;- map(districts, ~filter(.data = elections_district_raw_2018, sheet == .)) \n\nNow I can easily map the function I defined above, extract_party to this list of datasets. Well, I say easily, but it’s a bit more complicated than before because I have now a list of datasets and a list of target rows:\n\nelections_district_2018 &lt;- map2(.x = list_data_districts, .y = list_targets,\n     ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = .y))\n\nThe way to understand this is that for each element of list_data_districts and list_targets, I have to map extract_party to each element of position_parties_national. This gives the intented result:\n\nelections_district_2018\n## [[1]]\n## # A tibble: 208 x 4\n##    Party    Year Variables               Values\n##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;\n##  1 PIRATEN  2018 Pourcentage             0.0514\n##  2 PIRATEN  2018 CLEMENT Sven (1)     8007     \n##  3 PIRATEN  2018 WEYER Jerry (2)      3446     \n##  4 PIRATEN  2018 CLEMENT Pascal (3)   3418     \n##  5 PIRATEN  2018 KUNAKOVA Lucie (4)   2860     \n##  6 PIRATEN  2018 WAMPACH Jo (14)      2693     \n##  7 PIRATEN  2018 LAUX Cynthia (6)     2622     \n##  8 PIRATEN  2018 ISEKIN Christian (5) 2610     \n##  9 PIRATEN  2018 SCHWEICH Georges (9) 2602     \n## 10 PIRATEN  2018 LIESCH Mireille (8)  2551     \n## # … with 198 more rows\n## \n## [[2]]\n## # A tibble: 112 x 4\n##    Party    Year Variables                             Values\n##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                                  &lt;dbl&gt;\n##  1 PIRATEN  2018 Pourcentage                           0.0767\n##  2 PIRATEN  2018 COLOMBERA Jean (2)                 5074     \n##  3 PIRATEN  2018 ALLARD Ben (1)                     4225     \n##  4 PIRATEN  2018 MAAR Andy (3)                      2764     \n##  5 PIRATEN  2018 GINTER Joshua (8)                  2536     \n##  6 PIRATEN  2018 DASBACH Angelika (4)               2473     \n##  7 PIRATEN  2018 GRÜNEISEN Sam (6)                  2408     \n##  8 PIRATEN  2018 BAUMANN Roy (5)                    2387     \n##  9 PIRATEN  2018 CONRAD Pierre (7)                  2280     \n## 10 PIRATEN  2018 TRAUT ép. MOLITOR Angela Maria (9) 2274     \n## # … with 102 more rows\n## \n## [[3]]\n## # A tibble: 224 x 4\n##    Party    Year Variables                    Values\n##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt;\n##  1 PIRATEN  2018 Pourcentage                  0.0699\n##  2 PIRATEN  2018 GOERGEN Marc (1)          9818     \n##  3 PIRATEN  2018 FLOR Starsky (2)          6737     \n##  4 PIRATEN  2018 KOHL Martine (3)          6071     \n##  5 PIRATEN  2018 LIESCH Camille (4)        6025     \n##  6 PIRATEN  2018 KOHL Sylvie (6)           5628     \n##  7 PIRATEN  2018 WELTER Christian (5)      5619     \n##  8 PIRATEN  2018 DA GRAÇA DIAS Yanick (10) 5307     \n##  9 PIRATEN  2018 WEBER Jules (7)           5301     \n## 10 PIRATEN  2018 CHMELIK Libor (8)         5247     \n## # … with 214 more rows\n## \n## [[4]]\n## # A tibble: 96 x 4\n##    Party    Year Variables                           Values\n##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                                &lt;dbl&gt;\n##  1 PIRATEN  2018 Pourcentage                         0.0698\n##  2 PIRATEN  2018 FRÈRES Daniel (1)                4152     \n##  3 PIRATEN  2018 CLEMENT Jill (7)                 1943     \n##  4 PIRATEN  2018 HOUDREMONT Claire (2)            1844     \n##  5 PIRATEN  2018 BÖRGER Nancy (3)                 1739     \n##  6 PIRATEN  2018 MARTINS DOS SANTOS Catarina (6)  1710     \n##  7 PIRATEN  2018 BELLEVILLE Tatjana (4)           1687     \n##  8 PIRATEN  2018 CONTRERAS Gerald (5)             1687     \n##  9 PIRATEN  2018 Suffrages total                 14762     \n## 10 PIRATEN  2018 Suffrages de liste              10248     \n## # … with 86 more rows\n\nI now need to add the locality and division columns:\n\nelections_district_2018 &lt;- map2(.y = elections_district_2018, .x = districts, \n     ~mutate(.y, locality = .x, division = \"Electoral district\")) %&gt;%\n    bind_rows()\n\nWe’re almost done! Now we need to do the same for the 102 remaining sheets, one for each commune of Luxembourg. This will now go very fast, because we got all the building blocks from before:\n\ncommunes &lt;- xlsx_sheet_names(\"leg-2018-10-14-22-58-09-737.xlsx\")\n\ncommunes &lt;- communes %-l% \n    c(\"Le Grand-Duché de Luxembourg\", \"Centre\", \"Est\", \"Nord\", \"Sud\", \"Sommaire\")\n\nLet me introduce the following function: %-l%. This function removes elements from lists:\n\nc(\"a\", \"b\", \"c\", \"d\") %-l% c(\"a\", \"d\")\n## [1] \"b\" \"c\"\n\nYou can think of it as “minus for lists”. This is called an infix operator.\n\n\nSo this function is very useful to get the list of communes, and is part of my package, {brotools}.\n\n\nAs before, I load the data:\n\nelections_communes_raw_2018 &lt;- xlsx_cells(\"leg-2018-10-14-22-58-09-737.xlsx\",\n                                 sheets = communes)\n\nThen get my list of targets, but I need to change the reference address. It’s “B8” now, not “B7”.\n\n# Get the target rows\nlist_targets &lt;- map(communes, get_target_rows, \n                    dataset = elections_communes_raw_2018, reference_address = \"B8\")\n\nI now create a list of communes by mapping a filter function to the data:\n\nlist_data_communes &lt;- map(communes, ~filter(.data = elections_communes_raw_2018, sheet == .)) \n\nAnd just as before, I get the data I need by using extract_party, and adding the “locality” and “division” columns:\n\nelections_communes_2018 &lt;- map2(.x = list_data_communes, .y = list_targets,\n                                ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = .y))\n\nelections_communes_2018 &lt;- map2(.y = elections_communes_2018, .x = communes,\n                                ~mutate(.y, locality = .x, division = \"Commune\")) %&gt;%\n    bind_rows()\n\nThe steps are so similar for the four circonscriptions and for the 102 communes that I could have write a big wrapper function and the use it for the circonscription and communes at once. But I was lazy.\n\n\nFinally, I bind everything together and have a nice, tidy, flat file:\n\n# Final results\n\nelections_2018 &lt;- bind_rows(list(elections_national_2018, elections_district_2018, elections_communes_2018))\n\nglimpse(elections_2018)\n## Observations: 15,544\n## Variables: 6\n## $ Party     &lt;chr&gt; \"PIRATEN\", \"PIRATEN\", \"PIRATEN\", \"PIRATEN\", \"PIRATEN\",…\n## $ Year      &lt;dbl&gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, …\n## $ Variables &lt;chr&gt; \"Pourcentage\", \"Suffrage total\", \"Suffrages de liste\",…\n## $ Values    &lt;dbl&gt; 6.446204e-02, 2.275490e+05, 1.815600e+05, 4.598900e+04…\n## $ locality  &lt;chr&gt; \"Grand-Duchy of Luxembourg\", \"Grand-Duchy of Luxembour…\n## $ division  &lt;chr&gt; \"National\", \"National\", \"National\", \"National\", \"Natio…\n\nThis blog post is already quite long, so I will analyze the data now that R can easily ingest it in a future blog post."
  },
  {
    "objectID": "posts/2018-09-08-steam_linux.html",
    "href": "posts/2018-09-08-steam_linux.html",
    "title": "The year of the GNU+Linux desktop is upon us: using user ratings of Steam Play compatibility to play around with regex and the tidyverse",
    "section": "",
    "text": "I’ve been using GNU+Linux distros for about 10 years now, and have settled for openSUSE as my main operating system around 3 years ago, perhaps even more. If you’re a gamer, you might have heard about SteamOS and how more and more games are available on GNU+Linux. I don’t really care about games, I play the occasional one (currently Tangledeep) when I find the time, but still follow the news about gaming on GNU+Linux. Last week, Valve announced something quite big; it is now possible to run Windows games on GNU+Linux directly from Steam, using a modified version of Wine they call Proton. The feature is still in Beta, and Valve announced that they guarantee around 30 games to work already flawlessly. Of course, people have tried running a lot of other games, and, as was to be expected from Free Software and Open Source fans, GNU+Linux gamers created a Google Sheet that lists which games were tried and how they run. You can take a look at the sheet here.\n\n\nIn this blog post, I will play around with this sheet. This blog post lists some {tidyverse} tricks I find useful and use often. Perhaps these tricks will be useful to you too! Let’s start by loading the needed packages:\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(readxl)\n\nSince I’m lazy and don’t want to type the whole name of the file I’ll be using some little regex:\n\nsteam &lt;- read_excel(Sys.glob(\"Steam*\"), sheet = \"Main\", skip = 2)\n\nglimpse(steam)\n## Observations: 8,570\n## Variables: 9\n## $ SteamDB   &lt;chr&gt; \"LINK\", \"LINK\", \"LINK\", \"LINK\", \"LINK\", \"LINK\", \"LINK\"…\n## $ Game      &lt;chr&gt; \"64\", \"1849\", \"1982\", \"1982\", \"am Weapon: Revival\", \".…\n## $ Submitted &lt;chr&gt; \"5 days ago\", \"12 days ago\", \"11 days ago\", \"11 days a…\n## $ Status    &lt;chr&gt; \"Garbage\", \"Platinum\", \"Gold\", \"Platinum\", \"Platinum\",…\n## $ Notes     &lt;chr&gt; \"Crashes with a debug log\", \"Plays OK.\", \"Gamepad supp…\n## $ Distro    &lt;chr&gt; \"Arch (4.18.5)\", \"Manjaro XFCE\", \"Gentoo AMD64 (Kernel…\n## $ Driver    &lt;chr&gt; \"Nvidia 396.54 / Intel xf86-video-intel (1:2.99.917+83…\n## $ Specs     &lt;chr&gt; \"Intel Core i7-7700HQ / Nvidia GTX 1050 (Mobile)\", \"Ry…\n## $ Proton    &lt;chr&gt; \"3.7 Beta\", \"3.7-4 Beta\", \"3.7-4 Beta\", \"Default\", \"3.…\n\nLet’s count how many unique games are in the data:\n\nsteam %&gt;%\n    count(Game)\n## # A tibble: 3,855 x 2\n##    Game                                                                   n\n##    &lt;chr&gt;                                                              &lt;int&gt;\n##  1 .hack//G.U. Last Recode                                                2\n##  2 $1 Ride                                                                1\n##  3 0rbitalis                                                              1\n##  4 10 Second Ninja                                                        4\n##  5 100% Orange Juice                                                     17\n##  6 1000 Amps                                                              3\n##  7 12 Labours of Hercules VII: Fleecing the Fleece (Platinum Edition)     1\n##  8 16bit trader                                                           1\n##  9 1849                                                                   1\n## 10 1953 - KGB Unleased                                                    1\n## # … with 3,845 more rows\n\nThat’s quite a lot of games! However, not everyone of them is playable:\n\nsteam %&gt;%\n    count(Status)\n## # A tibble: 8 x 2\n##   Status       n\n##   &lt;chr&gt;    &lt;int&gt;\n## 1 Borked     205\n## 2 bronze       1\n## 3 Bronze     423\n## 4 Garbage   2705\n## 5 Gold       969\n## 6 Platinum  2596\n## 7 Primary      1\n## 8 Silver    1670\n\nAround 2500 have the status “Platinum”, but some games might have more than one status:\n\nsteam %&gt;%\n    filter(Game == \"100% Orange Juice\") %&gt;%\n    count(Status)\n## # A tibble: 5 x 2\n##   Status       n\n##   &lt;chr&gt;    &lt;int&gt;\n## 1 Bronze       5\n## 2 Garbage      3\n## 3 Gold         2\n## 4 Platinum     6\n## 5 Silver       1\n\nMore games run like Garbage than Platinum. But perhaps we can dig a little deeper and see if we find some patterns.\n\n\nLet’s take a look at the GNU+Linux distros:\n\nsteam %&gt;%\n    count(Distro) \n## # A tibble: 2,085 x 2\n##    Distro                                         n\n##    &lt;chr&gt;                                      &lt;int&gt;\n##  1 &lt;NA&gt;                                           1\n##  2 ?                                              2\n##  3 \"\\\"Arch Linux\\\" (64 bit)\"                      1\n##  4 \"\\\"Linux Mint 18.3 Sylvia 64bit\"               1\n##  5 \"\\\"Manjaro Stable 64-bit (Kernel 4.14.66)\"     1\n##  6 \"\\\"Solus\\\" (64 bit)\"                           2\n##  7 (K)ubuntu 18.04 64-bit (Kernel 4.15.0)         2\n##  8 (L)Ubuntu 18.04.1 LTS                          1\n##  9 18.04.1                                        1\n## 10 18.04.1 LTS                                    2\n## # … with 2,075 more rows\n\nOk the distro column is pretty messy. Let’s try to bring some order to it:\n\nsteam %&lt;&gt;%\n    mutate(distribution = as_factor(case_when(\n        grepl(\"buntu|lementary|antergos|steam|mint|18.|pop|neon\", Distro, ignore.case = TRUE) ~ \"Ubuntu\",\n        grepl(\"arch|manjaro\", Distro, ignore.case = TRUE) ~ \"Arch Linux\",\n        grepl(\"gentoo\", Distro, ignore.case = TRUE) ~ \"Gentoo\",\n        grepl(\"fedora\", Distro, ignore.case = TRUE) ~ \"Fedora\",\n        grepl(\"suse\", Distro, ignore.case = TRUE) ~ \"openSUSE\",\n        grepl(\"debian|sid|stretch|lmde\", Distro, ignore.case = TRUE) ~ \"Debian\",\n        grepl(\"solus\", Distro, ignore.case = TRUE) ~ \"Solus\",\n        grepl(\"slackware\", Distro, ignore.case = TRUE) ~ \"Slackware\",\n        grepl(\"void\", Distro, ignore.case = TRUE) ~ \"Void Linux\",\n        TRUE ~ \"Other\"\n    )))\n\nThe %&lt;&gt;% operator is shorthand for a &lt;- a %&gt;% f(). It passes a to f() and assigns the result back to a. Anyways, let’s take a look at the distribution column:\n\nsteam %&gt;%\n    count(distribution)\n## # A tibble: 10 x 2\n##    distribution     n\n##    &lt;fct&gt;        &lt;int&gt;\n##  1 Ubuntu        6632\n##  2 Arch Linux     805\n##  3 Solus          175\n##  4 Debian         359\n##  5 Fedora         355\n##  6 Gentoo          42\n##  7 Void Linux      38\n##  8 Other           76\n##  9 openSUSE        66\n## 10 Slackware       22\n\nI will group distributions that have less than 100 occurrences into a single category (meaning I will keep the 5 more common values):\n\nsteam %&lt;&gt;%\n    mutate(distribution = fct_lump(distribution, n = 5, other_level = \"Other\")) \n\nsteam %&gt;%\n    count(distribution)\n## # A tibble: 6 x 2\n##   distribution     n\n##   &lt;fct&gt;        &lt;int&gt;\n## 1 Ubuntu        6632\n## 2 Arch Linux     805\n## 3 Solus          175\n## 4 Debian         359\n## 5 Fedora         355\n## 6 Other          244\n\nLet’s do the same for the CPUs:\n\nsteam %&lt;&gt;%\n    mutate(CPU = as_factor(case_when(\n        grepl(\"intel|i\\\\d|xeon|core2|\\\\d{4}k|q\\\\d{4}|pentium\", Specs, ignore.case = TRUE) ~ \"Intel\",\n        grepl(\"ryzen|threadripper|tr|amd|fx|r\\\\d|\\\\d{4}x|phenom\", Specs, ignore.case = TRUE) ~ \"AMD\",\n        TRUE ~ NA_character_\n    )))\n\nsteam %&gt;%\n    count(CPU)\n## Warning: Factor `CPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n## # A tibble: 3 x 2\n##   CPU       n\n##   &lt;fct&gt; &lt;int&gt;\n## 1 Intel  5768\n## 2 AMD    2319\n## 3 &lt;NA&gt;    483\n\nAnd the same for the GPUs:\n\nsteam %&lt;&gt;%\n    mutate(GPU = as_factor(case_when(\n        grepl(\"nvidia|geforce|3\\\\d{2}|nouveau|gtx|gt\\\\s?\\\\d{1,}|9\\\\d0|1060|1070|1080\", Specs, ignore.case = TRUE) ~ \"Nvidia\",\n        grepl(\"amd|radeon|ati|rx|vega|r9\", Specs, ignore.case = TRUE) ~ \"AMD\",\n        grepl(\"intel|igpu|integrated|hd\\\\d{4}|hd\\\\sgraphics\", Specs, ignore.case = TRUE) ~ \"Intel\",\n        TRUE ~ NA_character_\n    )))\n\nsteam %&gt;%\n    count(GPU)\n## Warning: Factor `GPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n## # A tibble: 4 x 2\n##   GPU        n\n##   &lt;fct&gt;  &lt;int&gt;\n## 1 Nvidia  6086\n## 2 AMD     1374\n## 3 Intel    413\n## 4 &lt;NA&gt;     697\n\nI will also add a rank for the Status column:\n\nsteam %&lt;&gt;%\n    mutate(rank_status = case_when(\n        Status == \"Platinum\" ~ 5,\n        Status == \"Gold\" ~ 4,\n        Status == \"Silver\" ~ 3,\n        Status == \"Bronze\" ~ 2,\n        Status == \"Garbage\" ~ 1\n    ))\n\nNow, what are the top 5 most frequent combinations of Status, distribution, CPU and GPU?\n\nsteam %&gt;%\n    filter(!is.na(CPU), !is.na(GPU)) %&gt;%\n    count(Status, distribution, CPU, GPU) %&gt;%\n    mutate(total = sum(n)) %&gt;%\n    mutate(freq = n / total) %&gt;%\n    top_n(5)\n## Selecting by freq\n## # A tibble: 5 x 7\n##   Status   distribution CPU   GPU        n total   freq\n##   &lt;chr&gt;    &lt;fct&gt;        &lt;fct&gt; &lt;fct&gt;  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n## 1 Garbage  Ubuntu       Intel Nvidia  1025  7443 0.138 \n## 2 Gold     Ubuntu       Intel Nvidia   361  7443 0.0485\n## 3 Platinum Ubuntu       Intel Nvidia  1046  7443 0.141 \n## 4 Platinum Ubuntu       AMD   Nvidia   338  7443 0.0454\n## 5 Silver   Ubuntu       Intel Nvidia   650  7443 0.0873\n\nUnsurprisingly, Ubuntu, or distributions using Ubuntu as a base, are the most popular ones. Nvidia is the most popular GPU, Intel for CPUs and in most cases, this combo of hardware and distribution is associated with positive ratings (even though there are almost as many “Garbage” ratings than “Platinum” ratings).\n\n\nNow let’s compute some dumb averages of Statuses by distribution, CPU and GPU. Since I’m going to run the same computation three times, I’ll write a function to do that.\n\ncompute_avg &lt;- function(dataset, var){\n    var &lt;- enquo(var)\n    dataset %&gt;%\n        select(rank_status, (!!var)) %&gt;%\n        group_by((!!var)) %&gt;%\n        mutate(wt = n()) %&gt;%\n        summarise(average_rating = weighted.mean(rank_status, (!!var), wt, na.rm = TRUE))\n}\n\nLet’s see now if we can rank distribution by Steam play rating:\n\ncompute_avg(steam, distribution)\n## # A tibble: 6 x 2\n##   distribution average_rating\n##   &lt;fct&gt;                 &lt;dbl&gt;\n## 1 Ubuntu                 3.03\n## 2 Arch Linux             3.05\n## 3 Solus                  3.03\n## 4 Debian                 3.01\n## 5 Fedora                 3.07\n## 6 Other                  3.16\n\nHow about for hardware?\n\ncompute_avg(steam, GPU)\n## Warning: Factor `GPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n\n## Warning: Factor `GPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n## # A tibble: 4 x 2\n##   GPU    average_rating\n##   &lt;fct&gt;           &lt;dbl&gt;\n## 1 Nvidia           3.07\n## 2 AMD              2.90\n## 3 Intel            3.01\n## 4 &lt;NA&gt;            NA\ncompute_avg(steam, CPU)\n## Warning: Factor `CPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n\n## Warning: Factor `CPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n## # A tibble: 3 x 2\n##   CPU   average_rating\n##   &lt;fct&gt;          &lt;dbl&gt;\n## 1 Intel           3.03\n## 2 AMD             3.06\n## 3 &lt;NA&gt;           NA\n\nTo wrap this up, what are the games with the most ratings? Perhaps this can give us a hint about which games GNU+Linux users prefer:\n\nsteam %&gt;%\n    count(Game) %&gt;%\n    top_n(10)\n## Selecting by n\n## # A tibble: 10 x 2\n##    Game                              n\n##    &lt;chr&gt;                         &lt;int&gt;\n##  1 Age of Empires II: HD Edition    43\n##  2 Borderlands                      39\n##  3 DiRT 3 Complete Edition          32\n##  4 DOOM                             62\n##  5 Fallout: New Vegas               45\n##  6 Grim Dawn                        34\n##  7 No Man's Sky                     40\n##  8 Path of Exile                    35\n##  9 Quake Champions                  32\n## 10 The Elder Scrolls V: Skyrim      46\n\nI actually laughed out loud when I saw that DOOM was the game with the most ratings! What else was I expecting, really."
  },
  {
    "objectID": "posts/2019-01-04-newspapers.html",
    "href": "posts/2019-01-04-newspapers.html",
    "title": "Looking into 19th century ads from a Luxembourguish newspaper with R",
    "section": "",
    "text": "The national library of Luxembourg published some very interesting data sets; scans of historical newspapers! There are several data sets that you can download, from 250mb up to 257gb. I decided to take a look at the 32gb “ML Starter Pack”. It contains high quality scans of one year of the L’indépendence Luxembourgeoise (Luxembourguish independence) from the year 1877. To make life easier to data scientists, the national library also included ALTO and METS files (which is a XML schema that is used to describe the layout and contents of physical text sources, such as pages of a book or newspaper) which can be easily parsed by R.\n\n\nL’indépendence Luxembourgeoise is quite interesting in that it is a Luxembourguish newspaper written in French. Luxembourg always had 3 languages that were used in different situations, French, German and Luxembourguish. Luxembourguish is the language people used (and still use) for day to day life and to speak to their baker. Historically however, it was not used for the press or in politics. Instead it was German that was used for the press (or so I thought) and French in politics (only in 1984 was Luxembourguish made an official Language of Luxembourg). It turns out however that L’indépendence Luxembourgeoise, a daily newspaper that does not exist anymore, was in French. This piqued my interest, and it also made analysis easier, for 2 reasons: I first started with the Luxemburger Wort (Luxembourg’s Word I guess would be a translation), which still exists today, but which is in German. And at that time, German was written using the Fraktur font, which makes it barely readable. Look at the alphabet in Fraktur:\n\n𝕬 𝕭 𝕮 𝕯 𝕰 𝕱 𝕲 𝕳 𝕴 𝕵 𝕶 𝕷 𝕸 𝕹 𝕺 𝕻 𝕼 𝕽 𝕾 𝕿 𝖀 𝖁 𝖂 𝖃 𝖄 𝖅\n𝖆 𝖇 𝖈 𝖉 𝖊 𝖋 𝖌 𝖍 𝖎 𝖏 𝖐 𝖑 𝖒 𝖓 𝖔 𝖕 𝖖 𝖗 𝖘 𝖙 𝖚 𝖛 𝖜 𝖝 𝖞 𝖟\n\nIt’s not like German is already hard enough, they had to invent the least readable font ever to write German in, to make extra sure it would be hell to decipher.\n\n\nSo basically I couldn’t be bothered to try to read a German newspaper in Fraktur. That’s when I noticed the L’indépendence Luxembourgeoise… A Luxembourguish newspaper? Written in French? Sounds interesting.\n\n\nAnd oh boy. Interesting it was.\n\n\n19th century newspapers articles were something else. There’s this article for instance:\n\n\n\n\n\nFor those of you that do not read French, this article relates that in France, the ministry of justice required priests to include prayers on the Sunday that follows the start of the new season of parliamentary discussions, in order for God to provide senators his help.\n\n\nThere this gem too:\n\n\n\n\n\nThis article presents the tallest soldier of the German army, called Emhke, and nominated by the German Emperor himself to accompany him during his visit to Palestine. Emhke was 2.08 meters tall and weighted 236 pounds (apparently at the time Luxembourg was not fully sold on the metric system).\n\n\nAnyway, I decided to take a look at ads. The last paper of this 4 page newspaper always contained ads and other announcements. For example, there’s this ad for a pharmacy:\n\n\n\n\n\nthat sells tea, and mineral water. Yes, tea and mineral water. In a pharmacy. Or this one:\n\n\n\n\n\nwhich is literally upside down in the newspaper (the one from the 10th of April 1877). I don’t know if it’s a mistake or if it’s a marketing ploy, but it did catch my attention, 140 years later, so bravo. This is an announcement made by a shop owner that wants to sell all his merchandise for cheap, perhaps to make space for new stuff coming in?\n\n\nSo I decided brush up on my natural language processing skills with R and do topic modeling on these ads. The challenge here is that a single document, the 4th page of the newspaper, contains a lot of ads. So it will probably be difficult to clearly isolate topics. But let’s try nonetheless. First of all, let’s load all the .xml files that contain the data. These files look like this:\n\n&lt;TextLine ID=\"LINE6\" STYLEREFS=\"TS11\" HEIGHT=\"42\" WIDTH=\"449\" HPOS=\"165\" VPOS=\"493\"&gt;\n                                    &lt;String ID=\"S16\" CONTENT=\"l’après-midi,\" WC=\"0.638\" CC=\"0803367024653\" HEIGHT=\"42\" WIDTH=\"208\" HPOS=\"165\" VPOS=\"493\"/&gt;\n                                    &lt;SP ID=\"SP11\" WIDTH=\"24\" HPOS=\"373\" VPOS=\"493\"/&gt;\n                                    &lt;String ID=\"S17\" CONTENT=\"le\" WC=\"0.8\" CC=\"40\" HEIGHT=\"30\" WIDTH=\"29\" HPOS=\"397\" VPOS=\"497\"/&gt;\n                                    &lt;SP ID=\"SP12\" WIDTH=\"14\" HPOS=\"426\" VPOS=\"497\"/&gt;\n                                    &lt;String ID=\"S18\" CONTENT=\"Gouverne\" WC=\"0.638\" CC=\"72370460\" HEIGHT=\"31\" WIDTH=\"161\" HPOS=\"440\" VPOS=\"496\" SUBS_TYPE=\"HypPart1\" SUBS_CONTENT=\"Gouvernement\"/&gt;\n                                    &lt;HYP CONTENT=\"-\" WIDTH=\"11\" HPOS=\"603\" VPOS=\"514\"/&gt;\n                                  &lt;/TextLine&gt;\n                        &lt;TextLine ID=\"LINE7\" STYLEREFS=\"TS11\" HEIGHT=\"41\" WIDTH=\"449\" HPOS=\"166\" VPOS=\"541\"&gt;\n                                    &lt;String ID=\"S19\" CONTENT=\"ment\" WC=\"0.725\" CC=\"0074\" HEIGHT=\"26\" WIDTH=\"81\" HPOS=\"166\" VPOS=\"545\" SUBS_TYPE=\"HypPart2\" SUBS_CONTENT=\"Gouvernement\"/&gt;\n                                    &lt;SP ID=\"SP13\" WIDTH=\"24\" HPOS=\"247\" VPOS=\"545\"/&gt;\n                                    &lt;String ID=\"S20\" CONTENT=\"Royal\" WC=\"0.62\" CC=\"74503\" HEIGHT=\"41\" WIDTH=\"100\" HPOS=\"271\" VPOS=\"541\"/&gt;\n                                    &lt;SP ID=\"SP14\" WIDTH=\"26\" HPOS=\"371\" VPOS=\"541\"/&gt;\n                                    &lt;String ID=\"S21\" CONTENT=\"Grand-Ducal\" WC=\"0.682\" CC=\"75260334005\" HEIGHT=\"32\" WIDTH=\"218\" HPOS=\"397\" VPOS=\"541\"/&gt;\n                                  &lt;/TextLine&gt;\n\nI’m interested in the “CONTENT” tag, which contains the words. Let’s first get that into R.\n\n\nLoad the packages, and the files:\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(topicmodels)\nlibrary(brotools)\n\nad_pages &lt;- str_match(list.files(path = \"./\", all.files = TRUE, recursive = TRUE), \".*4-alto.xml\") %&gt;%\n    discard(is.na)\n\nI save the path of all the pages at once into the ad_pages variables. To understand how and why this works, you must take a look at the hierarchy of the folder:\n\n\n\n\n\nInside each of these folder, there is a text folder, and inside this folder there are the .xml files. Because this structure is bit complex, I use the list.files() function with the all.files and recursive argument set to TRUE which allow me to dig deep into the folder structure and list every single file. I am only interested into the 4th page though, so that’s why I use str_match() to only keep the 4th page using the “.*4-alto.xml” regular expression. This is the right regular expression, because the files are named like so:\n\n1877-12-29_01-00004-alto.xml\n\nSo in the end, ad_pages is a list of all the paths to these files. I then write a function to extract the contents of the “CONTENT” tag. Here is the function.\n\nget_words &lt;- function(page_path){\n    \n    page &lt;- read_file(page_path)\n    \n    page_name &lt;- str_extract(page_path, \"1.*(?=-0000)\") \n    \n    page %&gt;%  \n        str_split(\"\\n\", simplify = TRUE) %&gt;% \n        keep(str_detect(., \"CONTENT\")) %&gt;% \n        str_extract(\"(?&lt;=CONTENT)(.*?)(?=WC)\") %&gt;% \n        discard(is.na) %&gt;% \n        str_extract(\"[:alpha:]+\") %&gt;% \n        tolower %&gt;% \n        as_tibble %&gt;% \n        rename(tokens = value) %&gt;% \n        mutate(page = page_name)\n}\n\nThis function takes the path to a page as argument, and returns a tibble with the two columns: one containing the words, which I called tokens and the second the name of the document this word was found. I uploaded on .xml file here so that you can try the function yourself. The difficult part is str_extract(“(?&lt;=CONTENT)(.*?)(?=WC)“) which is were the words inside the “CONTENT” tag get extracted.\n\n\nI then map this function to all the pages, and get a nice tibble with all the words:\n\nad_words &lt;- map_dfr(ad_pages, get_words)\nad_words\n## # A tibble: 1,114,662 x 2\n##    tokens     page                            \n##    &lt;chr&gt;      &lt;chr&gt;                           \n##  1 afin       1877-01-05_01/text/1877-01-05_01\n##  2 de         1877-01-05_01/text/1877-01-05_01\n##  3 mettre     1877-01-05_01/text/1877-01-05_01\n##  4 mes        1877-01-05_01/text/1877-01-05_01\n##  5 honorables 1877-01-05_01/text/1877-01-05_01\n##  6 clients    1877-01-05_01/text/1877-01-05_01\n##  7 à          1877-01-05_01/text/1877-01-05_01\n##  8 même       1877-01-05_01/text/1877-01-05_01\n##  9 d          1877-01-05_01/text/1877-01-05_01\n## 10 avantages  1877-01-05_01/text/1877-01-05_01\n## # … with 1,114,652 more rows\n\nI then do some further cleaning, removing stop words (French and German, because there are some ads in German) and a bunch of garbage characters and words, which are probably when the OCR failed. I also remove some German words from the few German ads that are in the paper, because they have a very high tf-idf (I’ll explain below what that is). I also remove very common words in ads that were just like stopwords. Every ad of a shop mentioned their clients with honorable clientèle, or used the word vente, and so on. This is what you see below in the very long calls to str_remove_all. I also compute the tf_idf and I am grateful to ThinkR blog post on that, which you can read here. It’s in French though, but the idea of the blog post is to present topic modeling with Wikipedia articles. You can also read the section on tf-idf from the Text Mining with R ebook, here. tf-idf gives a measure of how common words are. Very common words, like stopwords, have a tf-idf of 0. So I use this to further remove very common words, by only keeping words with a tf-idf greater than 0.01. This is why I manually remove garbage words and German words below, because they are so uncommon that they have a very high tf-idf and mess up the rest of the analysis. To find these words I had to go back and forth between the tibble of cleaned words and my code, and manually add all these exceptions. It took some time, but definitely made the results of the next steps better. I then use cast_dtm to cast the tibble into a DocumentTermMatrix object, which is needed for the LDA() function that does the topic modeling:\n\nstopwords_fr &lt;- read_csv(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-fr/master/stopwords-fr.txt\",\n                         col_names = FALSE)\n## Parsed with column specification:\n## cols(\n##   X1 = col_character()\n## )\nstopwords_de &lt;- read_csv(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/stopwords-de.txt\",\n                         col_names = FALSE)\n## Parsed with column specification:\n## cols(\n##   X1 = col_character()\n## )\n## Warning: 1 parsing failure.\n## row col  expected    actual                                                                                   file\n## 157  -- 1 columns 2 columns 'https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/stopwords-de.txt'\nad_words2 &lt;- ad_words %&gt;% \n    filter(!is.na(tokens)) %&gt;% \n    mutate(tokens = str_remove_all(tokens, \n                                   '[|\\\\|!|\"|#|$|%|&|\\\\*|+|,|-|.|/|:|;|&lt;|=|&gt;|?|@|^|_|`|’|\\'|‘|(|)|\\\\||~|=|]|°|&lt;|&gt;|«|»|\\\\d{1,100}|©|®|•|—|„|“|-|¦\\\\\\\\|”')) %&gt;%\n    mutate(tokens = str_remove_all(tokens,\n                                   \"j'|j’|m’|m'|n’|n'|c’|c'|qu’|qu'|s’|s'|t’|t'|l’|l'|d’|d'|luxembourg|honneur|rue|prix|maison|frs|ber|adresser|unb|mois|vente|informer|sann|neben|rbudj|artringen|salz|eingetragen|ort|ftofjenb|groifdjen|ort|boch|chem|jahrgang|uoa|genannt|neuwahl|wechsel|sittroe|yerlorenkost|beichsmark|tttr|slpril|ofto|rbudj|felben|acferftücf|etr|eft|sbege|incl|estce|bes|franzosengrund|qne|nne|mme|qni|faire|id|kil\")) %&gt;%\n    anti_join(stopwords_de, by = c(\"tokens\" = \"X1\")) %&gt;% \n    filter(!str_detect(tokens, \"§\")) %&gt;% \n    mutate(tokens = ifelse(tokens == \"inédite\", \"inédit\", tokens)) %&gt;% \n    filter(tokens != \"\") %&gt;% \n    anti_join(stopwords_fr, by = c(\"tokens\" = \"X1\")) %&gt;% \n    count(page, tokens) %&gt;% \n    bind_tf_idf(tokens, page, n) %&gt;% \n    arrange(desc(tf_idf))\n\ndtm_long &lt;- ad_words2 %&gt;% \n    filter(tf_idf &gt; 0.01) %&gt;% \n    cast_dtm(page, tokens, n)\n\nTo read more details on this, I suggest you take a look at the following section of the Text Mining with R ebook: Latent Dirichlet Allocation.\n\n\nI choose to model 10 topics (k = 10), and set the alpha parameter to 5. This hyperparamater controls how many topics are present in one document. Since my ads are all in one page (one document), I increased it. Let’s fit the model, and plot the results:\n\nlda_model_long &lt;- LDA(dtm_long, k = 10, control = list(alpha = 5))\n\nI plot the per-topic-per-word probabilities, the “beta” from the model and plot the 5 words that contribute the most to each topic:\n\nresult &lt;- tidy(lda_model_long, \"beta\")\n\nresult %&gt;%\n    group_by(topic) %&gt;%\n    top_n(5, beta) %&gt;%\n    ungroup() %&gt;%\n    arrange(topic, -beta) %&gt;% \n    mutate(term = reorder(term, beta)) %&gt;%\n    ggplot(aes(term, beta, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\") +\n    coord_flip() +\n    theme_blog()\n\n\n\n\nSo some topics seem clear to me, other not at all. For example topic 4 seems to be about shoes made out of leather. The word semelle, sole, also appears. Then there’s a lot of topics that reference either music, bals, or instruments. I guess these are ads for local music festivals, or similar events. There’s also an ad for what seems to be bundles of sticks, topic 3: chêne is oak, copeaux is shavings and you know what fagots is. The first word stère which I did not know is a unit of volume equal to one cubic meter (see Wikipedia). So they were likely selling bundle of oak sticks by the cubic meter. For the other topics, I either lack context or perhaps I just need to adjust k, the number of topics to model, and alpha to get better results. In the meantime, topic 1 is about shoes (chaussures), theatre, fuel (combustible) and farts (pet). Really wonder what they were selling in that shop.\n\n\nIn any case, this was quite an interesting project. I learned a lot about topic modeling and historical newspapers of my country! I do not know if I will continue exploring it myself, but I am really curious to see what others will do with it!"
  },
  {
    "objectID": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html",
    "href": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html",
    "title": "Historical newspaper scraping with {tesseract} and R",
    "section": "",
    "text": "I have been playing around with historical newspapers data for some months now. The “obvious” type of analysis to do is NLP, but there is also a lot of numerical data inside historical newspapers. For instance, you can find these tables that show the market prices of the day in the L’Indépendance Luxembourgeoise:\nI wanted to see how easy it was to extract these tables from the newspapers and then make it available. It was a bit more complicated than anticipated."
  },
  {
    "objectID": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html#download-data",
    "href": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html#download-data",
    "title": "Historical newspaper scraping with {tesseract} and R",
    "section": "\nDownload data\n",
    "text": "Download data\n\n\nThe first step is to download the data. For this, I have used the code @yvesmaurer which you can find here. This code makes it easy to download individual pages of certain newspapers, for instance this one. The pages I am interested in are pages 3, which contain the tables I need, for example here. @yvesmaurer’s code makes it easy to find the download links, which look like this: https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F1drtkj%2Fpages%2F3/full/full/0/default.jpg. It is also possible to crop the image by changing some parameters like so. This is helpful, because it makes the image smaller. The tables I’m interested in are always in the last column, so I can can use this feature to get smaller images. However, not every issue contains these tables, and I only want to download the ones that have these tables. So I wrote the following code to download the images I’m interested in:\n\nlibrary(tidyverse)\nlibrary(magick)\nlibrary(tesseract)\nlibrary(furrr)\n\ndownload_image &lt;- function(link){\n\n    print(link)\n\n    isok &lt;- image_read(link) %&gt;%\n        ocr(engine = \"fra\") %&gt;%\n        str_to_lower() %&gt;%\n        str_detect(\"marché de luxembourg\")\n\n    if(isok){\n        date_link &lt;- link %&gt;%\n            str_replace(\"pages%2f3\", \"pages%2f1\") %&gt;%\n            str_replace(\"pct:74,0,100,100\", \"pct:76,1,17,5\")\n\n        paper_date &lt;- image_read(date_link) %&gt;%\n            ocr(engine = \"fra\") %&gt;%\n            str_squish() %&gt;%\n            str_remove(\"%\") %&gt;%\n            str_remove(\"&\") %&gt;%\n            str_remove(\"/\")\n\n        ark &lt;- link %&gt;%\n            str_sub(53, 60)\n\n        download.file(link, paste0(\"indep_pages/\", ark, \"-\", paper_date, \".jpg\"))\n    } else {\n        NULL\n        }\n}\n\nThis code only downloads an image if the ocr() from the {tesseract} (which does, you guessed it, OCR) detects the string “marché de luxembourg” which is the title of the tables. This is a bit extreme, because if a single letter cannot be correctly detected by the OCR, the page will not be downloaded. But I figured that if this string could not be easily recognized, this would be a canary telling me that the text inside the table would also not be easily recognized. So it might be extreme, but my hope was that it would make detecting the table itself easier. Turned out it wasn’t so easy, but more on this later."
  },
  {
    "objectID": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html#preparing-images",
    "href": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html#preparing-images",
    "title": "Historical newspaper scraping with {tesseract} and R",
    "section": "\nPreparing images\n",
    "text": "Preparing images\n\n\nNow that I have the images, I will prepare them to make character recognition easier. To do this, I’m using the {magick} package:\n\nlibrary(tidyverse)\nlibrary(magick)\nlibrary(tesseract)\nlibrary(furrr)\n\nprepare_image &lt;- function(image_path){\n    image &lt;- image_read(image_path)\n\n    image &lt;- image %&gt;%\n        image_modulate(brightness = 150) %&gt;%\n        image_convolve('DoG:0,0,2', scaling = '1000, 100%') %&gt;%\n        image_despeckle(times = 10)\n\n    image_write(image, paste0(getwd(), \"/edited/\", str_remove(image_path, \".jpg\"), \"edited.jpg\"))\n}\n\n\nimage_paths &lt;- dir(path = \"indep_pages\", pattern = \"*.jpg\", full.names = TRUE)\n\nplan(multiprocess, workers = 8)\n\nimage_paths %&gt;%\n    future_map(prepare_image)\n\nThe picture below shows the result:\n\n\n\n\n\nNow comes the complicated part, which is going from the image above, to the dataset below:\n\ngood_fr,good_en,unit,market_date,price,source_url\nFroment,Wheat,hectolitre,1875-08-28,23,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg\nMétail,Meslin,hectolitre,1875-08-28,21,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg\nSeigle,Rye,hectolitre,1875-08-28,15,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg\nOrge,Barley,hectolitre,1875-08-28,16,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg\nOrge mondé,Pot Barley,kilogram,1875-08-28,0.85,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg\nOrge perlé,Pearl barley,kilogram,1875-08-28,0.8,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg\nAvoine,Oats,hectolitre,1875-08-28,8.5,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg\nPois,Peas,hectolitre,1875-08-28,NA,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg"
  },
  {
    "objectID": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html#ocr-with-tesseract",
    "href": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html#ocr-with-tesseract",
    "title": "Historical newspaper scraping with {tesseract} and R",
    "section": "\nOCR with {tesseract}\n",
    "text": "OCR with {tesseract}\n\n\nThe first step was to get the date. For this, I have used the following function, which will then be used inside another function, which will extract the data and prices.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(magick)\nlibrary(tesseract)\nlibrary(furrr)\nlibrary(janitor)\n\nis_empty_line &lt;- function(line){\n    ifelse(line == \"\", TRUE, FALSE)\n}\n\nSys.setlocale('LC_TIME', \"fr_FR\")\n\nget_date &lt;- function(string, annee){\n\n    liste_mois &lt;- c(\"janvier\", \"février\", \"mars\", \"avril\", \"mai\", \"juin\", \"juillet\",\n                    \"août\", \"septembre\", \"octobre\", \"novembre\", \"décembre\")\n\n    raw_date &lt;- string %&gt;%\n      str_to_lower() %&gt;%\n        str_remove_all(\"\\\\.\") %&gt;%\n        str_extract(\"\\\\d{1,2} .{3,9}(\\\\s+)?\\\\d{0,4}\") %&gt;%\n        str_split(\"\\\\s+\", simplify = TRUE)\n\n    if(ncol(raw_date) == 2){\n        raw_date &lt;- cbind(raw_date, \"annee\")\n    }\n\n    raw_date[1, 3] &lt;- annee\n\n    raw_date &lt;- str_to_lower(raw_date[1:1, 1:3])\n\n    long_month &lt;- case_when(\n      raw_date[2] == \"janv\" ~ \"janvier\",\n      raw_date[2] == \"févr\" ~ \"février\",\n      raw_date[2] == \"sept\" ~ \"septembre\",\n      raw_date[2] == \"oct\" ~ \"octobre\",\n      raw_date[2] == \"nov\" ~ \"novembre\",\n      raw_date[2] == \"dec\" ~ \"décembre\",\n      TRUE ~ as.character(raw_date[2]))\n\n    raw_date[2] &lt;- long_month\n\n    is_it_date &lt;- as.Date(paste0(raw_date, collapse = \"-\"), format = \"%d-%b-%Y\") %&gt;%\n        is.na() %&gt;% `!`()\n\n    if(is_it_date){\n        return(as.Date(paste0(raw_date, collapse = \"-\"), format = \"%d-%b-%Y\"))\n    } else {\n        if(!(raw_date[2] %in% liste_mois)){\n            raw_date[2] &lt;- liste_mois[stringdist::amatch(raw_date[2], liste_mois, maxDist = 2)]\n            return(as.Date(paste0(raw_date, collapse = \"-\"), format = \"%d-%b-%Y\"))\n        }\n    }\n}\n\nThis function is more complicated than I had hoped. This is because dates come in different formats. For example, there are dates written like this “21 Janvier 1872”, or “12 Septembre” or “12 sept.”. The biggest problem here is that sometimes the year is missing. I deal with this in the next function, which is again, more complicated than what I had hoped. I won’t go into details and explain every step of the function above, but the idea is to extract the data from the raw text, replace abbreviated months with the full month name if needed, and then check if I get a valid date. If not, I try my luck with stringdist::amatch(), to try to match, say “jonvier” with “janvier”. This is in case the OCR made a mistake. I am not very happy with this solution, because it is very approximative, but oh well.\n\n\nThe second step is to get the data. I noticed that the rows stay consistent, but do change after June 1st 1876. So I simply hardcoded the goods names, and was only concerned with extracting the prices. I also apply some manual corrections inside the function; mainly dates that were wrongly recognized by the OCR engine, and which were causing problems. Again, not an optimal solution, the other alternative was to simply drop this data, which I did not want to do. Here is the function:\n\nextract_table &lt;- function(image_path){\n\n  image &lt;- image_read(image_path)\n\n  annee &lt;- image_path %&gt;%\n    str_extract(\"187\\\\d\")\n\n  ark &lt;- image_path %&gt;%\n    str_sub(22, 27)\n\n  source_url &lt;- str_glue(\"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F{ark}%2Fpages%2F1/full/full/0/default.jpg\",\n                         ark = ark)\n\n  text &lt;- ocr(image, engine = \"fra\")\n\n    text &lt;- text %&gt;%\n      str_split(\"\\n\") %&gt;%\n      unlist %&gt;%\n      str_squish() %&gt;%\n      str_remove_all(\"^.{1,10}$\") %&gt;%\n      discard(is_empty_line) %&gt;%\n      str_replace(\"Mercuriale du \\\\+ Nov. 1831.\", \"Mercuriale du 4 Nov. 1831.\") %&gt;%\n      str_replace(\"….u .T juillet.\", \"du 7 juillet\") %&gt;%\n      str_replace(\"octobré\", \"octobre\") %&gt;%\n      str_replace(\"AT octobre\", \"17 octobre\") %&gt;% # correction for \"f8g6kq8-18  LUNDI 19 OCTOBRÉ 1874. BUREAUX de fa RÉDACTIGedited.jpg\"\n      str_replace(\"T norembre\", \"7 novembre\") %&gt;%  # correction for fcrhrn5-LE 8  LUNDI 9 NOVEMBRE 1874 BUREAUX de la RÉDedited.jpg\n      str_replace(\"À oc demain 5\", \"27 mai\") %&gt;% # correction for fd61vzp-MARDI 50. MAI 1876 BUREAUX de la. RED, n VE DE L’ADMINISTRAedited.jpg\n      str_replace(\"G\", \"6\") %&gt;%\n      str_replace(\"Hercariale du 80 nov. 1872,\", \"du 30 novembre 1872\") %&gt;%\n      str_replace(\"….u .T juillet.\", \"du 7 juillet\") %&gt;%\n      str_replace(\"Rs ne its du 28-octobré.: :!: :\", \"28 octobre\") %&gt;%\n      str_replace(\"De routes due 98-juilléle. à eat\", \"28 juillet\") %&gt;%\n      str_replace(\"\\\\| Mereariale dn 14 dre. 1872,\", \"14 décembre 1872\")\n\n\n  start &lt;- text %&gt;%\n    str_which(\"MARCH(É|E).*D(E|É).*LUXEMBOUR(G|6)\") + 2\n\n  start &lt;- ifelse(is_empty(start), str_which(text, \".*D.*UXEM.*\") + 2, start)\n\n  end &lt;- start + 40\n\n  pricing_date &lt;- text[start - 1] %&gt;%\n    str_remove(\"%\") %&gt;%\n    str_remove(\"er\") %&gt;%\n    str_remove(\"\\\\.+\") %&gt;%\n    str_remove(\"\\\\*\") %&gt;%\n    str_remove(\"®\") %&gt;%\n    str_remove(\":\") %&gt;%\n    str_remove(\"\\\\?\") %&gt;%\n    str_replace(\"\\\\$\", \"9\") %&gt;%\n    str_remove(\"°\") %&gt;%\n    str_replace(\"‘du 14août.. - ; En\", \"14 août\") %&gt;%\n    str_replace(\"OP PE CN AP PP\", \"du 28 juin\") %&gt;%\n    str_replace(\"‘ du 81 janvi Le\", \"31 janvier\") %&gt;%\n    str_replace(\"\\\\| \\\\| du AT août\", \"17 août\") %&gt;%\n    str_replace(\"Su”  du 81 juillet. L\", \"31 juillet\") %&gt;%\n    str_replace(\"0 du 29 avril \\\" \\\\|\", \"29 avril\") %&gt;%\n    str_replace(\"LU 0 du 28 ail\", \"28 avril\") %&gt;%\n    str_replace(\"Rs ne its du 28-octobre :!: :\", \"23 octobre\") %&gt;%\n    str_replace(\"7 F \\\\|  du 13 octobre LA LOTS\", \"13 octobre\") %&gt;%\n    str_replace(\"À. du 18 juin UT ET\", \"13 juin\")\n\n\n  market_date &lt;- get_date(pricing_date, annee)\n\n  items &lt;- c(\"Froment\", \"Métail\", \"Seigle\", \"Orge\", \"Orge mondé\", \"Orge perlé\", \"Avoine\", \"Pois\", \"Haricots\",\n             \"Lentilles\", \"Pommes de terre\", \"Bois de hêtre\", \"Bois de chêne\", \"Beurre\", \"Oeufs\", \"Foin\",\n             \"Paille\", \"Viande de boeuf\", \"Viande de vache\", \"Viande de veau\", \"Viande de mouton\",\n             \"Viande fraîche de cochon\", \"Viande fumée de cochon\", \"Haricots\", \"Pois\", \"Lentilles\",\n             \"Farines de froment\", \"Farines de méteil\", \"Farines de seigle\")\n\n  items_en &lt;- c(\"Wheat\", \"Meslin\", \"Rye\", \"Barley\", \"Pot Barley\", \"Pearl barley\", \"Oats\", \"Peas\", \"Beans\",\n    \"Lentils\", \"Potatoes\", \"Beech wood\", \"Oak wood\", \"Butter\", \"Eggs\", \"Hay\", \"Straw\", \"Beef meat\",\n    \"Cow meat\", \"Veal meat\", \"Sheep meat\", \"Fresh pig meat\", \"Smoked pig meat\", \"Beans\", \"Peas\",\n    \"Lentils\", \"Wheat flours\", \"Meslin flours\", \"Rye flours\")\n\n\n  unit &lt;- c(\"hectolitre\", \"hectolitre\", \"hectolitre\", \"hectolitre\", \"kilogram\", \"kilogram\", \"hectolitre\",\n            \"hectolitre\", \"hectolitre\", \"hectolitre\", \"hectolitre\", \"stere\", \"stere\", \"kilogram\", \"dozen\",\n            \"500 kilogram\", \"500 kilogram\", \"kilogram\", \"kilogram\", \"kilogram\", \"kilogram\", \"kilogram\",\n            \"kilogram\", \"litre\", \"litre\", \"litre\", \"kilogram\", \"kilogram\", \"kilogram\")\n\n  # starting with june 1876, the order of the items changes\n  items_06_1876 &lt;- c(\"Froment\", \"Métail\", \"Seigle\", \"Orge\", \"Avoine\", \"Pois\", \"Haricots\", \"Lentilles\",\n                     \"Pommes de terre\", \"Farines de froment\", \"Farines de méteil\", \"Farines de seigle\", \"Orge mondé\",\n                     \"Beurre\", \"Oeufs\", \"Foins\", \"Paille\", \"Bois de hêtre\", \"Bois de chêne\", \"Viande de boeuf\", \"Viande de vache\",\n                     \"Viande de veau\", \"Viande de mouton\", \"Viande fraîche de cochon\", \"Viande fumée de cochon\")\n\n  items_06_1876_en &lt;- c(\"Wheat\", \"Meslin\", \"Rye\", \"Barley\", \"Oats\", \"Peas\", \"Beans\", \"Lentils\",\n                        \"Potatoes\", \"Wheat flours\", \"Meslin flours\", \"Rye flours\", \"Pot barley\",\n                        \"Butter\", \"Eggs\", \"Hay\", \"Straw\", \"Beechwood\", \"Oakwood\", \"Beef meat\", \"Cow meat\",\n                        \"Veal meat\", \"Sheep meat\", \"Fresh pig meat\", \"Smoked pig meat\")\n\n  units_06_1876 &lt;- c(rep(\"hectolitre\", 9), rep(\"kilogram\", 5), \"douzaine\", rep(\"500 kilogram\", 2),\n                     \"stere\", \"stere\", rep(\"kilogram\", 6))\n\n  raw_data &lt;- text[start:end]\n\n  prices &lt;- raw_data %&gt;%\n    str_replace_all(\"©\", \"0\") %&gt;%\n    str_extract(\"\\\\d{1,2}\\\\s\\\\d{2}\") %&gt;%\n    str_replace(\"\\\\s\", \"\\\\.\") %&gt;%\n    as.numeric\n\n  if(is.na(prices[1])){\n    prices &lt;- tail(prices, -1)\n  } else {\n    prices &lt;- prices\n  }\n\n  if(market_date &lt; as.Date(\"01-06-1876\", format = \"%d-%m-%Y\")){\n    prices &lt;- prices[1:length(items)]\n    tibble(\"good_fr\" = items, \"good_en\" = items_en, \"unit\" = unit, \"market_date\" = market_date,\n           \"price\" = prices, \"source_url\" = source_url)\n  } else {\n    prices &lt;- prices[1:length(items_06_1876_en)]\n    tibble(\"good_fr\" = items_06_1876, \"good_en\" = items_06_1876_en, \"unit\" = units_06_1876,\n           \"market_date\" = market_date, \"price\" = prices, \"source_url\" = source_url)\n  }\n}\n\nAs I wrote previously, I had to deal with the missing year in the date inside this function. To do that, I extracted the year from the name of the file, and pasted it then into the date. The file name contains the data because the function in the function that downloads the files I also performed OCR on the first page, to get the date of the newspaper issue. The sole purpose of this was to get the year. Again, the function is more complex than what I hoped, but it did work well overall. There are still mistakes in the data, for example sometimes the prices are in the wrong order; meaning that they’re “shifted”, for example instead of the prices for eggs, I have the prices of the good that comes next. So obviously be careful if you decide to analyze the data, and double-check if something seems weird. I have made the data available on Luxembourg Open Data Portal, here."
  },
  {
    "objectID": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html#analyzing-the-data",
    "href": "posts/2019-04-07-historical_newspaper_scraping_tesseract.html#analyzing-the-data",
    "title": "Historical newspaper scraping with {tesseract} and R",
    "section": "\nAnalyzing the data\n",
    "text": "Analyzing the data\n\n\nAnd now, to the fun part. I want to know what was the price of smoked pig meat, and how it varied through time:\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(brotools)\nmarket_price &lt;- read_csv(\"https://download.data.public.lu/resources/digitised-luxembourg-historical-newspapers-journaux-historiques-luxembourgeois-numerises/20190407-183605/market-price.csv\")\n## Parsed with column specification:\n## cols(\n##   good_fr = col_character(),\n##   good_en = col_character(),\n##   unit = col_character(),\n##   market_date = col_date(format = \"\"),\n##   price = col_double(),\n##   source_url = col_character()\n## )\nmarket_price %&gt;%\n    filter(good_en == \"Smoked pig meat\") %&gt;%\n    ggplot(aes(x = market_date, y = price)) +\n    geom_line(aes(group = 1), colour = \"#82518c\") + \n    theme_blog() + \n    labs(title = \"Prices of smoked pig meat at the Luxembourg-City market in the 19th century\")\n## Warning: Removed 2 rows containing missing values (geom_path).\n\n\n\n\nAs you can see, there is a huge spike somewhere in 1874. Maybe there was a very severe smoked pig meat shortage that caused the prices to increase dramatically, but the more likely explanation is that there was some sort of mistake, either in the OCR step, or when I extracted the prices, and somehow that particular price of smoked pig meat is actually the price of another, more expensive good.\n\n\nSo let’s only consider prices that are below, say, 20 franks, which is already very high:\n\nmarket_price %&gt;%\n    filter(good_en == \"Smoked pig meat\") %&gt;%\n    filter(price &lt; 20) %&gt;% \n    ggplot(aes(x = market_date, y = price)) +\n    geom_line(aes(group = 1), colour = \"#82518c\") + \n    theme_blog() + \n    labs(title = \"Prices of smoked pig meat at the Luxembourg-City market in the 1870s\")\n\n\n\n\nNow, some prices are very high. Let’s check if it’s a mistake:\n\nmarket_price %&gt;% \n    filter(good_en == \"Smoked pig meat\") %&gt;% \n    filter(between(price, 5, 20)) %&gt;% \n    pull(source_url)\n## [1] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fbs2fs6%2Fpages%2F1/full/full/0/default.jpg\"\n## [2] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fd61vzp%2Fpages%2F1/full/full/0/default.jpg\"\n## [3] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fjdwb6m%2Fpages%2F1/full/full/0/default.jpg\"\n## [4] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fng14m3%2Fpages%2F1/full/full/0/default.jpg\"\n## [5] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fw9jdrb%2Fpages%2F1/full/full/0/default.jpg\"\n\nIf you go to the first url, you will land on the first page of the newspaper. To check the table, you need to check the third page, by changing this part of the url “pages%2F1” to this “pages%2F3”.\n\n\nYou will then find the following:\n\n\n\n\n\nAs you can see, the price was 2.5, but the OCR returned 7.5. This is a problem that is unavoidable with OCR; there is no way of knowing a priori if characters were not well recognized. It is actually quite interesting how the price for smoked pig meat stayed constant through all these years. A density plot shows that most prices were around 2.5:\n\nmarket_price %&gt;% \n    filter(good_en == \"Smoked pig meat\") %&gt;% \n    filter(price &lt; 20) %&gt;% \n    ggplot() + \n    geom_density(aes(price), colour = \"#82518c\") + \n    theme_blog()\n\n\n\n\nWhat about another good, say, barley?\n\nmarket_price %&gt;%\n    filter(good_en == \"Barley\") %&gt;%\n    ggplot(aes(x = market_date, y = price)) +\n    geom_line(aes(group = 1), colour = \"#82518c\") + \n    theme_blog() + \n    labs(title = \"Prices of barley at the Luxembourg-City market in the 1870s\")\n\n\n\n\nHere again, we see some very high spikes, most likely due to errors. Let’s try to limit the prices to likely values:\n\nmarket_price %&gt;%\n    filter(good_en == \"Barley\") %&gt;%\n    filter(between(price, 10, 40)) %&gt;%\n    ggplot(aes(x = market_date, y = price)) +\n    geom_line(aes(group = 1), colour = \"#82518c\") + \n    theme_blog() + \n    labs(title = \"Prices of barley at the Luxembourg-City market in the 1870s\")\n\n\n\nmarket_price %&gt;% \n    filter(good_en == \"Barley\") %&gt;% \n    ggplot() + \n    geom_density(aes(price), colour = \"#82518c\") + \n    theme_blog()\n## Warning: Removed 39 rows containing non-finite values (stat_density).\n\n\n\n\nLet’s finish this with one of my favourite legume, lentils:\n\nmarket_price %&gt;%\n    filter(good_en == \"Lentils\") %&gt;%\n    ggplot(aes(x = market_date, y = price)) +\n    geom_line(aes(group = 1), colour = \"#82518c\") + \n    theme_blog() + \n    labs(title = \"Prices of lentils at the Luxembourg-City market in the 1870s\")\n\n\n\nmarket_price %&gt;% \n    filter(good_en == \"Lentils\") %&gt;% \n    ggplot() + \n    geom_density(aes(price), colour = \"#82518c\") + \n    theme_blog()\n## Warning: Removed 79 rows containing non-finite values (stat_density).\n\n\n\n\nAll these 0’s might be surprising, but in most cases, they are actually true zeros! For example, you can check this issue. This very likely means that no lentils were available that day at the market. Let’s get rid of the 0s and other extreme values:\n\nmarket_price %&gt;%\n    filter(good_en == \"Lentils\") %&gt;%\n    filter(between(price, 1, 40)) %&gt;% \n    ggplot(aes(x = market_date, y = price)) +\n    geom_line(aes(group = 1), colour = \"#82518c\") + \n    theme_blog() + \n    labs(title = \"Prices of lentils at the Luxembourg-City market in the 1870s\")\n\n\n\n\nI would like to see if the spikes above 30 are errors or not:\n\nmarket_price %&gt;% \n    filter(good_en == \"Lentils\") %&gt;% \n    filter(between(price, 30, 40)) %&gt;% \n    pull(source_url)\n## [1] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F04mb5t%2Fpages%2F1/full/full/0/default.jpg\"\n## [2] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fb8zp31%2Fpages%2F1/full/full/0/default.jpg\"\n## [3] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fkzrj53%2Fpages%2F1/full/full/0/default.jpg\"\n## [4] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fs8sw2v%2Fpages%2F1/full/full/0/default.jpg\"\n## [5] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fsjptsk%2Fpages%2F1/full/full/0/default.jpg\"\n## [6] \"https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fwk65b6%2Fpages%2F1/full/full/0/default.jpg\"\n\nThe price was recognized as being 35, and turns out it was correct as you can see here. This is quite interesting, because the average price was way lower than that:\n\nmarket_price %&gt;%\n    filter(good_en == \"Lentils\") %&gt;%\n    filter(between(price, 1, 40)) %&gt;% \n    summarise(mean_price = mean(price), \n              sd_price = sd(price))\n## # A tibble: 1 x 2\n##   mean_price sd_price\n##        &lt;dbl&gt;    &lt;dbl&gt;\n## 1       20.8     5.82\n\nI’m going to finish here; it was an interesting project, and I can’t wait for more newspapers to be digitized and OCR to work even better. There is a lot more historical data trapped in these newspapers that could provide a lot insights on Luxembourg’s society in the 19th century."
  },
  {
    "objectID": "posts/2018-12-15-lubridate_africa.html",
    "href": "posts/2018-12-15-lubridate_africa.html",
    "title": "Manipulate dates easily with {lubridate}",
    "section": "",
    "text": "This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free here. This is taken from Chapter 5, which presents the {tidyverse} packages and how to use them to compute descriptive statistics and manipulate data. In the text below, I scrape a table from Wikipedia, which shows when African countries gained independence from other countries. Then, using {lubridate} functions I show you how you can answers questions such as Which countries gained independence before 1960?."
  },
  {
    "objectID": "posts/2018-12-15-lubridate_africa.html#set-up-scraping-some-data-from-wikipedia",
    "href": "posts/2018-12-15-lubridate_africa.html#set-up-scraping-some-data-from-wikipedia",
    "title": "Manipulate dates easily with {lubridate}",
    "section": "\nSet-up: scraping some data from Wikipedia\n",
    "text": "Set-up: scraping some data from Wikipedia\n\n\n{lubridate} is yet another tidyverse package, that makes dealing with dates or duration data (and intervals) as painless as possible. I do not use every function contained in the package daily, and as such will only focus on some of the functions. However, if you have to deal with dates often, you might want to explore the package thoroughly.\n\n\nLet’s get some data from a Wikipedia table:\n\nlibrary(tidyverse)\nlibrary(rvest)\npage &lt;- read_html(\"https://en.wikipedia.org/wiki/Decolonisation_of_Africa\")\n\nindependence &lt;- page %&gt;%\n    html_node(\".wikitable\") %&gt;%\n    html_table(fill = TRUE)\n\nindependence &lt;- independence %&gt;%\n    select(-Rank) %&gt;%\n    map_df(~str_remove_all(., \"\\\\[.*\\\\]\")) %&gt;%\n    rename(country = `Country[a]`,\n           colonial_name = `Colonial name`,\n           colonial_power = `Colonial power[b]`,\n           independence_date = `Independence date[c]`,\n           first_head_of_state = `First head of state[d]`,\n           independence_won_through = `Independence won through`)\n\nThis dataset was scraped from the following Wikipedia table. It shows when African countries gained independence from which colonial powers. In Chapter 11, I will show you how to scrape Wikipedia pages using R. For now, let’s take a look at the contents of the dataset:\n\nindependence\n## # A tibble: 54 x 6\n##    country colonial_name colonial_power independence_da… first_head_of_s…\n##    &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;            &lt;chr&gt;           \n##  1 Liberia Liberia       United States  26 July 1847     Joseph Jenkins …\n##  2 South … Cape Colony … United Kingdom 31 May 1910      Louis Botha     \n##  3 Egypt   Sultanate of… United Kingdom 28 February 1922 Fuad I          \n##  4 Eritrea Italian Erit… Italy          10 February 1947 Haile Selassie  \n##  5 Libya   British Mili… United Kingdo… 24 December 1951 Idris           \n##  6 Sudan   Anglo-Egypti… United Kingdo… 1 January 1956   Ismail al-Azhari\n##  7 Tunisia French Prote… France         20 March 1956    Muhammad VIII a…\n##  8 Morocco French Prote… France Spain   2 March 19567 A… Mohammed V      \n##  9 Ghana   Gold Coast    United Kingdom 6 March 1957     Kwame Nkrumah   \n## 10 Guinea  French West … France         2 October 1958   Ahmed Sékou Tou…\n## # … with 44 more rows, and 1 more variable: independence_won_through &lt;chr&gt;\n\nas you can see, the date of independence is in a format that might make it difficult to answer questions such as Which African countries gained independence before 1960 ? for two reasons. First of all, the date uses the name of the month instead of the number of the month (well, this is not such a big deal, but still), and second of all the type of the independence day column is character and not “date”. So our first task is to correctly define the column as being of type date, while making sure that R understands that January is supposed to be “01”, and so on."
  },
  {
    "objectID": "posts/2018-12-15-lubridate_africa.html#using-lubridate",
    "href": "posts/2018-12-15-lubridate_africa.html#using-lubridate",
    "title": "Manipulate dates easily with {lubridate}",
    "section": "\nUsing {lubridate}\n",
    "text": "Using {lubridate}\n\n\nThere are several helpful functions included in {lubridate} to convert columns to dates. For instance if the column you want to convert is of the form “2012-11-21”, then you would use the function ymd(), for “year-month-day”. If, however the column is “2012-21-11”, then you would use ydm(). There’s a few of these helper functions, and they can handle a lot of different formats for dates. In our case, having the name of the month instead of the number might seem quite problematic, but it turns out that this is a case that {lubridate} handles painfully:\n\nlibrary(lubridate)\n## \n## Attaching package: 'lubridate'\n## The following object is masked from 'package:base':\n## \n##     date\nindependence &lt;- independence %&gt;%\n  mutate(independence_date = dmy(independence_date))\n## Warning: 5 failed to parse.\n\nSome dates failed to parse, for instance for Morocco. This is because these countries have several independence dates; this means that the string to convert looks like:\n\n\"2 March 1956\n7 April 1956\n10 April 1958\n4 January 1969\"\n\nwhich obviously cannot be converted by {lubridate} without further manipulation. I ignore these cases for simplicity’s sake.\n\n\nLet’s take a look at the data now:\n\nindependence\n## # A tibble: 54 x 6\n##    country colonial_name colonial_power independence_da… first_head_of_s…\n##    &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;          &lt;date&gt;           &lt;chr&gt;           \n##  1 Liberia Liberia       United States  1847-07-26       Joseph Jenkins …\n##  2 South … Cape Colony … United Kingdom 1910-05-31       Louis Botha     \n##  3 Egypt   Sultanate of… United Kingdom 1922-02-28       Fuad I          \n##  4 Eritrea Italian Erit… Italy          1947-02-10       Haile Selassie  \n##  5 Libya   British Mili… United Kingdo… 1951-12-24       Idris           \n##  6 Sudan   Anglo-Egypti… United Kingdo… 1956-01-01       Ismail al-Azhari\n##  7 Tunisia French Prote… France         1956-03-20       Muhammad VIII a…\n##  8 Morocco French Prote… France Spain   NA               Mohammed V      \n##  9 Ghana   Gold Coast    United Kingdom 1957-03-06       Kwame Nkrumah   \n## 10 Guinea  French West … France         1958-10-02       Ahmed Sékou Tou…\n## # … with 44 more rows, and 1 more variable: independence_won_through &lt;chr&gt;\n\nAs you can see, we now have a date column in the right format. We can now answer questions such as Which countries gained independence before 1960? quite easily, by using the functions year(), month() and day(). Let’s see which countries gained independence before 1960:\n\nindependence %&gt;%\n  filter(year(independence_date) &lt;= 1960) %&gt;%\n  pull(country)\n##  [1] \"Liberia\"                          \"South Africa\"                    \n##  [3] \"Egypt\"                            \"Eritrea\"                         \n##  [5] \"Libya\"                            \"Sudan\"                           \n##  [7] \"Tunisia\"                          \"Ghana\"                           \n##  [9] \"Guinea\"                           \"Cameroon\"                        \n## [11] \"Togo\"                             \"Mali\"                            \n## [13] \"Madagascar\"                       \"Democratic Republic of the Congo\"\n## [15] \"Benin\"                            \"Niger\"                           \n## [17] \"Burkina Faso\"                     \"Ivory Coast\"                     \n## [19] \"Chad\"                             \"Central African Republic\"        \n## [21] \"Republic of the Congo\"            \"Gabon\"                           \n## [23] \"Mauritania\"\n\nYou guessed it, year() extracts the year of the date column and converts it as a numeric so that we can work on it. This is the same for month() or day(). Let’s try to see if countries gained their independence on Christmas Eve:\n\nindependence %&gt;%\n  filter(month(independence_date) == 12,\n         day(independence_date) == 24) %&gt;%\n  pull(country)\n## [1] \"Libya\"\n\nSeems like Libya was the only one! You can also operate on dates. For instance, let’s compute the difference between two dates, using the interval() column:\n\nindependence %&gt;%\n  mutate(today = lubridate::today()) %&gt;%\n  mutate(independent_since = interval(independence_date, today)) %&gt;%\n  select(country, independent_since)\n## # A tibble: 54 x 2\n##    country      independent_since             \n##    &lt;chr&gt;        &lt;S4: Interval&gt;                \n##  1 Liberia      1847-07-26 UTC--2019-02-10 UTC\n##  2 South Africa 1910-05-31 UTC--2019-02-10 UTC\n##  3 Egypt        1922-02-28 UTC--2019-02-10 UTC\n##  4 Eritrea      1947-02-10 UTC--2019-02-10 UTC\n##  5 Libya        1951-12-24 UTC--2019-02-10 UTC\n##  6 Sudan        1956-01-01 UTC--2019-02-10 UTC\n##  7 Tunisia      1956-03-20 UTC--2019-02-10 UTC\n##  8 Morocco      NA--NA                        \n##  9 Ghana        1957-03-06 UTC--2019-02-10 UTC\n## 10 Guinea       1958-10-02 UTC--2019-02-10 UTC\n## # … with 44 more rows\n\nThe independent_since column now contains an interval object that we can convert to years:\n\nindependence %&gt;%\n  mutate(today = lubridate::today()) %&gt;%\n  mutate(independent_since = interval(independence_date, today)) %&gt;%\n  select(country, independent_since) %&gt;%\n  mutate(years_independent = as.numeric(independent_since, \"years\"))\n## # A tibble: 54 x 3\n##    country      independent_since              years_independent\n##    &lt;chr&gt;        &lt;S4: Interval&gt;                             &lt;dbl&gt;\n##  1 Liberia      1847-07-26 UTC--2019-02-10 UTC             172. \n##  2 South Africa 1910-05-31 UTC--2019-02-10 UTC             109. \n##  3 Egypt        1922-02-28 UTC--2019-02-10 UTC              97.0\n##  4 Eritrea      1947-02-10 UTC--2019-02-10 UTC              72  \n##  5 Libya        1951-12-24 UTC--2019-02-10 UTC              67.1\n##  6 Sudan        1956-01-01 UTC--2019-02-10 UTC              63.1\n##  7 Tunisia      1956-03-20 UTC--2019-02-10 UTC              62.9\n##  8 Morocco      NA--NA                                      NA  \n##  9 Ghana        1957-03-06 UTC--2019-02-10 UTC              61.9\n## 10 Guinea       1958-10-02 UTC--2019-02-10 UTC              60.4\n## # … with 44 more rows\n\nWe can now see for how long the last country to gain independence has been independent. Because the data is not tidy (in some cases, an African country was colonized by two powers, see Libya), I will only focus on 4 European colonial powers: Belgium, France, Portugal and the United Kingdom:\n\nindependence %&gt;%\n  filter(colonial_power %in% c(\"Belgium\", \"France\", \"Portugal\", \"United Kingdom\")) %&gt;%\n  mutate(today = lubridate::today()) %&gt;%\n  mutate(independent_since = interval(independence_date, today)) %&gt;%\n  mutate(years_independent = as.numeric(independent_since, \"years\")) %&gt;%\n  group_by(colonial_power) %&gt;%\n  summarise(last_colony_independent_for = min(years_independent, na.rm = TRUE))\n## # A tibble: 4 x 2\n##   colonial_power last_colony_independent_for\n##   &lt;chr&gt;                                &lt;dbl&gt;\n## 1 Belgium                               56.6\n## 2 France                                41.6\n## 3 Portugal                              43.2\n## 4 United Kingdom                        42.6\n\n{lubridate} contains many more functions. If you often work with dates, duration or interval data, {lubridate} is a package that you have to master."
  },
  {
    "objectID": "posts/2018-09-11-human_to_machine.html",
    "href": "posts/2018-09-11-human_to_machine.html",
    "title": "Going from a human readable Excel file to a machine-readable csv with {tidyxl}",
    "section": "",
    "text": "I won’t write a very long introduction; we all know that Excel is ubiquitous in business, and that it has a lot of very nice features, especially for business practitioners that do not know any programming. However, when people use Excel for purposes it was not designed for, it can be a hassle. Often, people use Excel as a reporting tool, which it is not; they create very elaborated and complicated spreadsheets that are human readable, but impossible to import within any other tool.\n\n\nIn this blog post (which will probably be part of a series), I show you how you can go from this:\n\n\n\n\n\nto this:\n\n\n\n\n\nYou can find the data I will use here. Click on the “Time use” folder and you can download the workbook.\n\n\nThe Excel workbook contains several sheets (in French and English) of the amount of time Luxembourguish citizens spend from Monday to Sunday. For example, on average, people that are in employment spend almost 8 hours sleeping during the week days, and 8:45 hours on Saturday.\n\n\nAs you can see from the screenshot, each sheet contains several tables that have lots of headers and these tables are next to one another. Trying to import these sheets with good ol’ readxl::read_excel() produces a monster.\n\n\nThis is where {tidyxl} comes into play. Let’s import the workbook with {tidyxl}:\n\nlibrary(tidyverse)\nlibrary(tidyxl)\n\ntime_use_xl &lt;- xlsx_cells(\"time-use.xlsx\")\n\nLet’s see what happened:\n\nhead(time_use_xl)\n## # A tibble: 6 x 21\n##   sheet address   row   col is_blank data_type error logical numeric\n##   &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;lgl&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;     &lt;dbl&gt;\n## 1 Index A1          1     1 FALSE    character &lt;NA&gt;  NA           NA\n## 2 Index B1          1     2 TRUE     blank     &lt;NA&gt;  NA           NA\n## 3 Index C1          1     3 TRUE     blank     &lt;NA&gt;  NA           NA\n## 4 Index D1          1     4 TRUE     blank     &lt;NA&gt;  NA           NA\n## 5 Index E1          1     5 TRUE     blank     &lt;NA&gt;  NA           NA\n## 6 Index F1          1     6 TRUE     blank     &lt;NA&gt;  NA           NA\n## # … with 12 more variables: date &lt;dttm&gt;, character &lt;chr&gt;,\n## #   character_formatted &lt;list&gt;, formula &lt;chr&gt;, is_array &lt;lgl&gt;,\n## #   formula_ref &lt;chr&gt;, formula_group &lt;int&gt;, comment &lt;chr&gt;, height &lt;dbl&gt;,\n## #   width &lt;dbl&gt;, style_format &lt;chr&gt;, local_format_id &lt;int&gt;\n\nAs you can see, the sheet was imported, but the result might be unexpected. Actually, time_use_xl is a tibble object, where each row is one cell of the Excel sheet. This might seem very complicated to handle, but you will see that it actually makes things way easier.\n\n\nI only want to work on the English sheets so I use the following code to ignore the French ones:\n\nsheets &lt;- xlsx_sheet_names(\"time-use.xlsx\") %&gt;%\n    keep(grepl(pattern = \".*day$\", .))\n\nAlso, there’s a sheet that aggregates the results for week days and weekends, which I also ignore.\n\n\nNow, to extract the tables from each sheet I wrote the following function:\n\nextract_data &lt;- function(sheet){\n    activities &lt;- sheet %&gt;%\n        filter(col == 2) %&gt;%\n        select(row, character) %&gt;%\n        filter(row %in% seq(6,58)) %&gt;%\n        rename(activities = character) %&gt;%\n        select(-row)\n    \n    cols_to_extract &lt;- sheet %&gt;% \n        filter(grepl(\"Population who completed.*\", character)) %&gt;% \n        pull(col)\n    \n    headers_pos &lt;- cols_to_extract - 1\n    \n    headers &lt;- sheet %&gt;%\n        filter(col %in% headers_pos, row == 3) %&gt;%\n        pull(character)\n    \n    cols_to_extract %&gt;% \n        map(~filter(sheet, col %in% .)) %&gt;%\n        map(~select(., sheet, address, row, col, character)) %&gt;%\n        map(~filter(., row %in% seq(6,58))) %&gt;%\n        map(~select(., character)) %&gt;%\n        map2(.x = ., .y = headers, ~mutate(.x, \"population\" = .y)) %&gt;%\n        map(., ~bind_cols(activities, .)) %&gt;%\n        bind_rows()\n}\n\nLet’s study it step by step and see how it works. First, there’s the argument, sheet. This function will be mapped to each sheet of the workbook. Then, the first block I wrote, extracts the activities:\n\n    activities &lt;- sheet %&gt;%\n        filter(col == 2) %&gt;%\n        select(row, character) %&gt;%\n        filter(row %in% seq(6,58)) %&gt;%\n        rename(activities = character) %&gt;%\n        select(-row)\n\nI only keep the second column (filter(col == 2)); col is a column of the tibble and if you look inside the workbook, you will notice that the activities are on the second column, or the B column. Then, I select two columns, the row and the character column. row is self-explanatory and character actually contains whatever is written inside the cells. Then, I only keep rows 6 to 58, because that is what interests me; the rest is either empty cells, or unneeded. Finally, I rename the character column to activities and remove the row column.\n\n\nThe second block:\n\n    cols_to_extract &lt;- sheet %&gt;% \n        filter(grepl(\"Population who completed.*\", character)) %&gt;% \n        pull(col)\n\nreturns the index of the columns I want to extract. I am only interested in the people that have completed the activities, so using grepl() inside filter(), I located these columns, and use pull()… to pull them out of the data frame! cols_to_extract is thus a nice atomic vector of columns that I want to keep.\n\n\nIn the third block, I extract the headers:\n\n    headers_pos &lt;- cols_to_extract - 1\n\nWhy - 1? This is because if you look in the Excel, you will see that the headers are one column before the column labeled “People who completed the activity”. For example on column G, I have “People who completed the activity” and on column F I have the header, in this case “Male”.\n\n\nNow I actually extract the headers:\n\n    headers &lt;- sheet %&gt;%\n        filter(col %in% headers_pos, row == 3) %&gt;%\n        pull(character)\n\nHeaders are always on the third row, but on different columns, hence the col %in% headers_pos. I then pull out the values inside the cells with pull(character). So my headers object will be an atomic vector with “All”, “Male”, “Female”, “10 - 19 years”, etc… everything on row 3.\n\n\nFinally, the last block, actually extracts the data:\n\n    cols_to_extract %&gt;% \n        map(~filter(sheet, col %in% .)) %&gt;%\n        map(~select(., sheet, address, row, col, character)) %&gt;%\n        map(~filter(., row %in% seq(6,58))) %&gt;%\n        map(~select(., character)) %&gt;%\n        map2(.x = ., .y = headers, ~mutate(.x, \"population\" = .y)) %&gt;%\n        map(., ~bind_cols(activities, .)) %&gt;%\n        bind_rows()\n\ncols_to_extract is a vector with the positions of the columns that interest me. So for example “4”, “7”, “10” and so on. I map this vector to the sheet, which returns me a list of extracted data frames. I pass this down to a select() (which is inside map()… why? Because the input parameter is a list of data frames). So for each data frame inside the list, I select the columns sheet, address, row, col and character. Then, for each data frame inside the list, I use filter() to only keep the rows from position 6 to 58. Then, I only select the character column, which actually contains the text inside the cell. Then, using map2(), I add the values inside the headers object as a new column, called population. Then, I bind the activities column to the data frame and bind all the rows together.\n\n\nTime to use this function! Let’s see:\n\nclean_data &lt;- sheets %&gt;%\n    map(~filter(time_use_xl, sheet %in% .)) %&gt;%\n    set_names(sheets) %&gt;%\n    map(extract_data) %&gt;%\n    map2(.x = ., .y = sheets, ~mutate(.x, \"day\" = .y)) %&gt;%\n    bind_rows() %&gt;%\n    select(day, population, activities, time = character)\n\nglimpse(clean_data)\n## Observations: 2,968\n## Variables: 4\n## $ day        &lt;chr&gt; \"Year 2014_Monday til Friday\", \"Year 2014_Monday til …\n## $ population &lt;chr&gt; \"All\", \"All\", \"All\", \"All\", \"All\", \"All\", \"All\", \"All…\n## $ activities &lt;chr&gt; \"Personal care\", \"Sleep\", \"Eating\", \"Other personal c…\n## $ time       &lt;chr&gt; \"11:07\", \"08:26\", \"01:47\", \"00:56\", \"07:37\", \"07:47\",…\n\nSo I map my list of sheets to the tibble I imported with readxl, use set_names to name the elements of my list (which is superfluous, but I wanted to show this; might interest you!) and then map this result to my little function. I could stop here, but I then add a new column to each data frame that contains the day on which the data was measured, bind the rows together and reorder the columns. Done!\n\n\nNow, how did I come up with this function? I did not start with a function. I started by writing some code that did what I wanted for one table only, inside one sheet only. Only when I got something that worked, did I start to generalize to several tables and then to several sheets. Most of the time spent was actually in trying to find patterns in the Excel sheet that I could use to write my function (for example noticing that the headers I wanted where always one column before the column I was interested in). This is my advice when working with function programming; always solve the issue for one element, wrap this code inside a function, and then simply map this function to a list of elements!"
  },
  {
    "objectID": "posts/2014-11-11-benchmarks-r-blas-atlas-rro.html",
    "href": "posts/2014-11-11-benchmarks-r-blas-atlas-rro.html",
    "title": "R, R with Atlas, R with OpenBLAS and Revolution R Open: which is fastest?",
    "section": "",
    "text": "In this short post, I benchmark different “versions” of R. I compare the execution speeds of R, R linked against OpenBLAS, R linked against ATLAS and Revolution R Open. Revolution R Open is a new open source version of R made by Revolution Analytics. It is linked against MKL and should offer huge speed improvements over vanilla R. Also, it uses every cores of your computer by default, without any change whatsoever to your code.\n\n\nTL;DR: Revolution R Open is the fastest of all the benchmarked versions (with R linked against OpenBLAS and ATLAS just behind), and easier to setup.\n\n\nSetup\n\n\nI benchmarked these different versions of R using R-benchmark-25.R that you can download here. This benchmark file was created by Simon Urbanek.\n\n\nI ran the benchmarks on my OpenSUSE 13.2 computer with a Pentium Dual-Core CPU E6500@2.93GHz with 4GB of Ram. It's outdated, but it's still quite fast for most of my numerical computation needs. I installed “vanilla” R from the official OpenSUSE repositories which is currently at version 3.1.2.\n\n\nThen, I downloaded OpenBLAS and ATLAS also from the official OpenSUSE repositories and made R use these libraries instead of its own implementation of BLAS. The way I did that is a bit hacky, but works: first, go to /usr/lib64/R/lib and backup libRblas.so (rename it to libRblas.soBackup for instance). Then link /usr/lib64/libopenblas.so.0 to /usr/lib64/R/lib/libRblas, and that's it, R will use OpenBLAS. For ATLAS, you can do it in the same fashion, but you'll find the library in /usr/lib64/atlas/. These paths should be the same for any GNU/Linux distribution. For other operating systems, I'm sure you can find where these libraries are with Google.\n\n\nThe last version I benchmarked was Revolution R Open. This is a new version of R released by Revolution Analytics. Revolution Analytics had their own version of R, called Revolution R, for quite some time now. They decided to release a completely free as in freedom and free as in free beer version of this product which they now renamed Revolution R Open. You can download Revolution R Open here. You can have both “vanilla” R and Revolution R Open installed on your system.\n\n\nResults\n\n\nI ran the R-benchmark-25.R 6 times for every version but will only discuss the 4 best runs.\n\n\n\n\n\nR version\n\n\nFastest run\n\n\nSlowest run\n\n\nMean Run\n\n\n\n\nVanilla R\n\n\n63.65\n\n\n66.21\n\n\n64.61\n\n\n\n\nOpenBLAS R\n\n\n15.63\n\n\n18.96\n\n\n16.94\n\n\n\n\nATLAS R\n\n\n16.92\n\n\n21.57\n\n\n18.24\n\n\n\n\nRRO\n\n\n14.96\n\n\n16.08\n\n\n15.49\n\n\n\n\nAs you can read from the table above, Revolution R Open was the fastest of the four versions, but not significantly faster than BLAS or ATLAS R. However, RRO uses all the available cores by default, so if your code relies on a lot matrix algebra, RRO might be actually a lot more faster than OpenBLAS and ATLAS R. Another advantage of RRO is that it is very easy to install, and also works with Rstudio and is compatible with every R package to existence. “Vanilla” R is much slower than the other three versions, more than 3 times as slow!\n\n\nConclusion\n\n\nWith other benchmarks, you could get other results, but I don’t think that “vanilla” R could beat any of the other three versions. Whatever your choice, I recommend not using plain, “vanilla” R. The other options are much faster than standard R, and don't require much work to set up. I'd personally recommend Revolution R Open, as it is free software and compatible with CRAN packages and Rstudio."
  },
  {
    "objectID": "posts/2019-03-05-historical_vowpal_part2.html",
    "href": "posts/2019-03-05-historical_vowpal_part2.html",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2",
    "section": "",
    "text": "In part 1 of this series I set up Vowpal Wabbit to classify newspapers content. Now, let’s use the model to make predictions and see how and if we can improve the model. Then, let’s train the model on the whole data."
  },
  {
    "objectID": "posts/2019-03-05-historical_vowpal_part2.html#step-1-prepare-the-data",
    "href": "posts/2019-03-05-historical_vowpal_part2.html#step-1-prepare-the-data",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2",
    "section": "\nStep 1: prepare the data\n",
    "text": "Step 1: prepare the data\n\n\nThe first step consists in importing the test data and preparing it. The test data need not be large and thus can be imported and worked on in R.\n\n\nI need to remove the target column from the test set, or else it will be used to make predictions. If you do not remove this column the accuracy of the model will be very high, but it will be wrong since, of course, you do not have the target column at running time… because it is the column that you want to predict!\n\nlibrary(\"tidyverse\")\nlibrary(\"yardstick\")\n\nsmall_test &lt;- read_delim(\"data_split/small_test.txt\", \"|\",\n                      escape_double = FALSE, col_names = FALSE,\n                      trim_ws = TRUE)\n\nsmall_test %&gt;%\n    mutate(X1= \" \") %&gt;%\n    write_delim(\"data_split/small_test2.txt\", col_names = FALSE, delim = \"|\")\n\nI wrote the data in a file called small_test2.txt and can now use my model to make predictions:\n\nsystem2(\"/home/cbrunos/miniconda3/bin/vw\", args = \"-t -i vw_models/small_oaa.model data_split/small_test2.txt -p data_split/small_oaa.predict\")\n\nThe predictions get saved in the file small_oaa.predict, which is a plain text file. Let’s add these predictions to the original test set:\n\nsmall_predictions &lt;- read_delim(\"data_split/small_oaa.predict\", \"|\",\n                          escape_double = FALSE, col_names = FALSE,\n                          trim_ws = TRUE)\n\nsmall_test &lt;- small_test %&gt;%\n    rename(truth = X1) %&gt;%\n    mutate(truth = factor(truth, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")))\n\nsmall_predictions &lt;- small_predictions %&gt;%\n    rename(predictions = X1) %&gt;%\n    mutate(predictions = factor(predictions, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")))\n\nsmall_test &lt;- small_test %&gt;%\n    bind_cols(small_predictions)"
  },
  {
    "objectID": "posts/2019-03-05-historical_vowpal_part2.html#step-2-use-the-model-and-test-data-to-evaluate-performance",
    "href": "posts/2019-03-05-historical_vowpal_part2.html#step-2-use-the-model-and-test-data-to-evaluate-performance",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2",
    "section": "\nStep 2: use the model and test data to evaluate performance\n",
    "text": "Step 2: use the model and test data to evaluate performance\n\n\nWe can use the several metrics included in {yardstick} to evaluate the model’s performance:\n\nconf_mat(small_test, truth = truth, estimate = predictions)\n\naccuracy(small_test, truth = truth, estimate = predictions)\n          Truth\nPrediction  1  2  3  4  5\n         1 51 15  2 10  1\n         2 11  6  3  1  0\n         3  0  0  0  0  0\n         4  0  0  0  0  0\n         5  0  0  0  0  0\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.570\n\nWe can see that the model never predicted class 3, 4 or 5. Can we improve by adding some regularization? Let’s find out!"
  },
  {
    "objectID": "posts/2019-03-05-historical_vowpal_part2.html#step-3-adding-regularization",
    "href": "posts/2019-03-05-historical_vowpal_part2.html#step-3-adding-regularization",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2",
    "section": "\nStep 3: adding regularization\n",
    "text": "Step 3: adding regularization\n\n\nBefore trying regularization, let’s try changing the cost function from the logistic function to the hinge function:\n\n# Train the model\nhinge_oaa_fit &lt;- system2(\"/home/cbrunos/miniconda3/bin/vw\", args = \"--oaa 5 -d data_split/small_train.txt --loss_function hinge -f vw_models/hinge_oaa.model\", stderr = TRUE)\n\nsystem2(\"/home/cbrunos/miniconda3/bin/vw\", args = \"-i vw_models/hinge_oaa.model -t -d data_split/small_test2.txt -p data_split/hinge_oaa.predict\")\n\n\npredictions &lt;- read_delim(\"data_split/hinge_oaa.predict\", \"|\",\n                          escape_double = FALSE, col_names = FALSE,\n                          trim_ws = TRUE)\n\ntest &lt;- test %&gt;%\n    select(-predictions)\n\npredictions &lt;- predictions %&gt;%\n    rename(predictions = X1) %&gt;%\n    mutate(predictions = factor(predictions, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")))\n\ntest &lt;- test %&gt;%\n    bind_cols(predictions)\nconf_mat(test, truth = truth, estimate = predictions)\n\naccuracy(test, truth = truth, estimate = predictions)\n          Truth\nPrediction   1   2   3   4   5\n         1 411 120  45  92   1\n         2 355 189  12  17   0\n         3  11   2   0   0   0\n         4  36   4   0   1   0\n         5   3   0   3   0   0\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.462\n\nWell, didn’t work out so well, but at least we now know how to change the loss function. Let’s go back to the logistic loss and add some regularization. First, let’s train the model:\n\nregul_oaa_fit &lt;- system2(\"/home/cbrunos/miniconda3/bin/vw\", args = \"--oaa 5 --l1 0.005 --l2 0.005 -d data_split/small_train.txt -f vw_models/small_regul_oaa.model\", stderr = TRUE)\n\nNow we can use it for prediction:\n\nsystem2(\"/home/cbrunos/miniconda3/bin/vw\", args = \"-i vw_models/small_regul_oaa.model -t -d data_split/test2.txt -p data_split/small_regul_oaa.predict\")\n\n\npredictions &lt;- read_delim(\"data_split/small_regul_oaa.predict\", \"|\",\n                          escape_double = FALSE, col_names = FALSE,\n                          trim_ws = TRUE)\n\ntest &lt;- test %&gt;%\n    select(-predictions)\n\npredictions &lt;- predictions %&gt;%\n    rename(predictions = X1) %&gt;%\n    mutate(predictions = factor(predictions, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")))\n\ntest &lt;- test %&gt;%\n    bind_cols(predictions)\n\nWe can now use it for predictions:\n\nconf_mat(test, truth = truth, estimate = predictions)\n\naccuracy(test, truth = truth, estimate = predictions)\n          Truth\nPrediction   1   2   3   4   5\n         1 816 315  60 110   1\n         2   0   0   0   0   0\n         3   0   0   0   0   0\n         4   0   0   0   0   0\n         5   0   0   0   0   0\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.627\n\nSo accuracy improved, but the model only predicts class 1 now… let’s try with other hyper-parameters values:\n\nregul_oaa_fit &lt;- system2(\"/home/cbrunos/miniconda3/bin/vw\", args = \"--oaa 5 --l1 0.00015 --l2 0.00015 -d data_split/small_train.txt -f vw_models/small_regul_oaa.model\", stderr = TRUE)\nconf_mat(test, truth = truth, estimate = predictions)\n\naccuracy(test, truth = truth, estimate = predictions)\n          Truth\nPrediction   1   2   3   4   5\n         1 784 300  57 108   1\n         2  32  14   3   2   0\n         3   0   1   0   0   0\n         4   0   0   0   0   0\n         5   0   0   0   0   0\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.613\n\nSo accuracy is lower than previously, but at least more categories get correctly predicted. Depending on your needs, you should consider different metrics. Especially for classification problems, you might not be interested in accuracy, in particular if the data is severely unbalanced.\n\n\nAnyhow, to finish this blog post, let’s train the model on the whole data and measure the time it takes to run the full model."
  },
  {
    "objectID": "posts/2019-03-05-historical_vowpal_part2.html#step-4-training-on-the-whole-data",
    "href": "posts/2019-03-05-historical_vowpal_part2.html#step-4-training-on-the-whole-data",
    "title": "Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2",
    "section": "\nStep 4: Training on the whole data\n",
    "text": "Step 4: Training on the whole data\n\n\nLet’s first split the whole data into a training and a testing set:\n\nnb_lines &lt;- system2(\"cat\", args = \"text_fr.txt | wc -l\", stdout = TRUE)\n\nsystem2(\"split\", args = paste0(\"-l\", floor(as.numeric(nb_lines)*0.995), \" text_fr.txt data_split/\"))\n\nsystem2(\"mv\", args = \"data_split/aa data_split/train.txt\")\nsystem2(\"mv\", args = \"data_split/ab data_split/test.txt\")\n\nThe whole data contains 260247 lines, and the training set weighs 667MB, which is quite large. Let’s train the simple multiple classifier on the data and see how long it takes:\n\ntic &lt;- Sys.time()\noaa_fit &lt;- system2(\"/home/cbrunos/miniconda3/bin/vw\", args = \"--oaa 5 -d data_split/train.txt -f vw_models/oaa.model\", stderr = TRUE)\nSys.time() - tic\nTime difference of 4.73266 secs\n\nYep, you read that right. Training the classifier on 667MB of data took less than 5 seconds!\n\n\nLet’s take a look at the final object:\n\noaa_fit\n [1] \"final_regressor = vw_models/oaa.model\"                                   \n [2] \"Num weight bits = 18\"                                                    \n [3] \"learning rate = 0.5\"                                                     \n [4] \"initial_t = 0\"                                                           \n [5] \"power_t = 0.5\"                                                           \n [6] \"using no cache\"                                                          \n [7] \"Reading datafile = data_split/train.txt\"                                 \n [8] \"num sources = 1\"                                                         \n [9] \"average  since         example        example  current  current  current\"\n[10] \"loss     last          counter         weight    label  predict features\"\n[11] \"1.000000 1.000000            1            1.0        2        1      253\"\n[12] \"0.500000 0.000000            2            2.0        2        2      499\"\n[13] \"0.250000 0.000000            4            4.0        2        2        6\"\n[14] \"0.250000 0.250000            8            8.0        1        1     2268\"\n[15] \"0.312500 0.375000           16           16.0        1        1      237\"\n[16] \"0.250000 0.187500           32           32.0        1        1      557\"\n[17] \"0.171875 0.093750           64           64.0        1        1      689\"\n[18] \"0.179688 0.187500          128          128.0        2        2      208\"\n[19] \"0.144531 0.109375          256          256.0        1        1      856\"\n[20] \"0.136719 0.128906          512          512.0        4        4        4\"\n[21] \"0.122070 0.107422         1024         1024.0        1        1     1353\"\n[22] \"0.106934 0.091797         2048         2048.0        1        1      571\"\n[23] \"0.098633 0.090332         4096         4096.0        1        1       43\"\n[24] \"0.080566 0.062500         8192         8192.0        1        1      885\"\n[25] \"0.069336 0.058105        16384        16384.0        1        1      810\"\n[26] \"0.062683 0.056030        32768        32768.0        2        2      467\"\n[27] \"0.058167 0.053650        65536        65536.0        1        1       47\"\n[28] \"0.056061 0.053955       131072       131072.0        1        1      495\"\n[29] \"\"                                                                        \n[30] \"finished run\"                                                            \n[31] \"number of examples = 258945\"                                             \n[32] \"weighted example sum = 258945.000000\"                                    \n[33] \"weighted label sum = 0.000000\"                                           \n[34] \"average loss = 0.054467\"                                                 \n[35] \"total feature number = 116335486\"  \n\nLet’s use the test set and see how the model fares:\n\nconf_mat(test, truth = truth, estimate = predictions)\n\naccuracy(test, truth = truth, estimate = predictions)\n          Truth\nPrediction   1   2   3   4   5\n         1 537 175  52 100   1\n         2 271 140   8   9   0\n         3   1   0   0   0   0\n         4   7   0   0   1   0\n         5   0   0   0   0   0\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.521\n\nBetter accuracy can certainly be achieved with hyper-parameter tuning… maybe the subject for a future blog post? In any case I am very impressed with Vowpal Wabbit and am certainly looking forward to future developments of {RVowpalWabbit}!"
  },
  {
    "objectID": "posts/2018-12-30-reticulate.html",
    "href": "posts/2018-12-30-reticulate.html",
    "title": "R or Python? Why not both? Using Anaconda Python within R with {reticulate}",
    "section": "",
    "text": "This short blog post illustrates how easy it is to use R and Python in the same R Notebook thanks to the {reticulate} package. For this to work, you might need to upgrade RStudio to the current preview version. Let’s start by importing {reticulate}:\n\nlibrary(reticulate)\n\n{reticulate} is an RStudio package that provides “a comprehensive set of tools for interoperability between Python and R”. With it, it is possible to call Python and use Python libraries within an R session, or define Python chunks in R markdown. I think that using R Notebooks is the best way to work with Python and R; when you want to use Python, you simply use a Python chunk:\n\n```{python}\nyour python code here\n```\n\nThere’s even autocompletion for Python object methods:\n\n\n\n\n\nFantastic!\n\n\nHowever, if you wish to use Python interactively within your R session, you must start the Python REPL with the repl_python() function, which starts a Python REPL. You can then do whatever you want, even access objects from your R session, and then when you exit the REPL, any object you created in Python remains accessible in R. I think that using Python this way is a bit more involved and would advise using R Notebooks if you need to use both languages.\n\n\nI installed the Anaconda Python distribution to have Python on my system. To use it with {reticulate} I must first use the use_python() function that allows me to set which version of Python I want to use:\n\n# This is an R chunk\nuse_python(\"~/miniconda3/bin/python\")\n\nI can now load a dataset, still using R:\n\n# This is an R chunk\ndata(mtcars)\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nand now, to access the mtcars data frame, I simply use the r object:\n\n# This is a Python chunk\nprint(r.mtcars.describe())\n##              mpg        cyl        disp   ...            am       gear     carb\n## count  32.000000  32.000000   32.000000   ...     32.000000  32.000000  32.0000\n## mean   20.090625   6.187500  230.721875   ...      0.406250   3.687500   2.8125\n## std     6.026948   1.785922  123.938694   ...      0.498991   0.737804   1.6152\n## min    10.400000   4.000000   71.100000   ...      0.000000   3.000000   1.0000\n## 25%    15.425000   4.000000  120.825000   ...      0.000000   3.000000   2.0000\n## 50%    19.200000   6.000000  196.300000   ...      0.000000   4.000000   2.0000\n## 75%    22.800000   8.000000  326.000000   ...      1.000000   4.000000   4.0000\n## max    33.900000   8.000000  472.000000   ...      1.000000   5.000000   8.0000\n## \n## [8 rows x 11 columns]\n\n.describe() is a Python Pandas DataFrame method to get summary statistics of our data. This means that mtcars was automatically converted from a tibble object to a Pandas DataFrame! Let’s check its type:\n\n# This is a Python chunk\nprint(type(r.mtcars))\n## &lt;class 'pandas.core.frame.DataFrame'&gt;\n\nLet’s save the summary statistics in a variable:\n\n# This is a Python chunk\nsummary_mtcars = r.mtcars.describe()\n\nLet’s access this from R, by using the py object:\n\n# This is an R chunk\nclass(py$summary_mtcars)\n## [1] \"data.frame\"\n\nLet’s try something more complex. Let’s first fit a linear model in Python, and see how R sees it:\n\n# This is a Python chunk\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nmodel = smf.ols('mpg ~ hp', data = r.mtcars).fit()\nprint(model.summary())\n##                             OLS Regression Results                            \n## ==============================================================================\n## Dep. Variable:                    mpg   R-squared:                       0.602\n## Model:                            OLS   Adj. R-squared:                  0.589\n## Method:                 Least Squares   F-statistic:                     45.46\n## Date:                Sun, 10 Feb 2019   Prob (F-statistic):           1.79e-07\n## Time:                        00:25:51   Log-Likelihood:                -87.619\n## No. Observations:                  32   AIC:                             179.2\n## Df Residuals:                      30   BIC:                             182.2\n## Df Model:                           1                                         \n## Covariance Type:            nonrobust                                         \n## ==============================================================================\n##                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n## ------------------------------------------------------------------------------\n## Intercept     30.0989      1.634     18.421      0.000      26.762      33.436\n## hp            -0.0682      0.010     -6.742      0.000      -0.089      -0.048\n## ==============================================================================\n## Omnibus:                        3.692   Durbin-Watson:                   1.134\n## Prob(Omnibus):                  0.158   Jarque-Bera (JB):                2.984\n## Skew:                           0.747   Prob(JB):                        0.225\n## Kurtosis:                       2.935   Cond. No.                         386.\n## ==============================================================================\n## \n## Warnings:\n## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nJust for fun, I ran the linear regression with the Scikit-learn library too:\n\n# This is a Python chunk\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression  \nregressor = LinearRegression()  \nx = r.mtcars[[\"hp\"]]\ny = r.mtcars[[\"mpg\"]]\nmodel_scikit = regressor.fit(x, y)\nprint(model_scikit.intercept_)\n## [30.09886054]\nprint(model_scikit.coef_)\n## [[-0.06822828]]\n\nLet’s access the model variable in R and see what type of object it is in R:\n\n# This is an R chunk\nmodel_r &lt;- py$model\nclass(model_r)\n## [1] \"statsmodels.regression.linear_model.RegressionResultsWrapper\"\n## [2] \"statsmodels.base.wrapper.ResultsWrapper\"                     \n## [3] \"python.builtin.object\"\n\nSo because this is a custom Python object, it does not get converted into the equivalent R object. This is described here. However, you can still use Python methods from within an R chunk!\n\n# This is an R chunk\nmodel_r$aic\n## [1] 179.2386\nmodel_r$params\n##   Intercept          hp \n## 30.09886054 -0.06822828\n\nI must say that I am very impressed with the {reticulate} package. I think that even if you are primarily a Python user, this is still very interesting to know in case you need a specific function from an R package. Just write all your script inside a Python Markdown chunk and then use the R function you need from an R chunk! Of course there is also a way to use R from Python, a Python library called rpy2 but I am not very familiar with it. From what I read, it seems to be also quite simple to use."
  },
  {
    "objectID": "posts/2015-02-22-export-r-output-to-file.html",
    "href": "posts/2015-02-22-export-r-output-to-file.html",
    "title": "Export R output to a file",
    "section": "",
    "text": "Sometimes it is useful to export the output of a long-running R command. For example, you might want to run a time consuming regression just before leaving work on Friday night, but would like to get the output saved inside your Dropbox folder to take a look at the results before going back to work on Monday.\n\n\nThis can be achieved very easily using capture.output() and cat() like so:\n\nout &lt;- capture.output(summary(my_very_time_consuming_regression))\n\ncat(\"My title\", out, file=\"summary_of_my_very_time_consuming_regression.txt\", sep=\"\\n\", append=TRUE)\n\nmy_very_time_consuming_regression is an object of class lm for example. I save the output of summary(my_very_time_consuming_regression) as text using capture.output and save it in a variable called out. Finally, I save out to a file called summary_of_my_very_time_consuming_regression.txt with the first sentence being My title (you can put anything there). The file summary_of_my_very_time_consuming_regression.txt doesn’t have to already exist in your working directory. The option sep=\"\" is important or else the whole output will be written in a single line. Finally, append=TRUE makes sure your file won’t be overwritten; additional output will be appended to the file, which can be nice if you want to compare different versions of your model."
  },
  {
    "objectID": "posts/2018-01-03-lists_all_the_way.html",
    "href": "posts/2018-01-03-lists_all_the_way.html",
    "title": "It’s lists all the way down",
    "section": "",
    "text": "There’s a part 2 to this post: read it here.\n\n\nToday, I had the opportunity to help someone over at the R for Data Science Slack group (read more about this group here) and I thought that the question asked could make for an interesting blog post, so here it is!\n\n\nDisclaimer: the way I’m doing things here is totally not optimal, but I want to illustrate how to map functions over nested lists. But I show the optimal way at the end, so for the people that are familiar with purrr don’t get mad at me.\n\n\nSuppose you have to do certain data transformation tasks on a data frame, and you write a nice function that does that for you:\n\nlibrary(tidyverse)\ndata(mtcars)\n\nnice_function = function(df, param1, param2){\n  df = df %&gt;%\n    filter(cyl == param1, am == param2) %&gt;%\n    mutate(result = mpg * param1 * (2 - param2))\n\n  return(df)\n}\n\nnice_function(mtcars, 4, 0)\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\n\nThis might seem like a silly function and not a nice function, but it will illustrate the point I want to make (and the question that was asked) very well. This function is completely useless, but bear with me. Now, suppose that you want to do these operations for each value of cyl and am (of course you can do that without using nice_function()…). First, you might want to fix the value of am to 0, and then loop over the values of cyl. But as I have explained in this other blog post I prefer using the map() functions included in purrr. For example:\n\nvalues_cyl = c(4, 6, 8)\n\n(result = map(values_cyl, nice_function, df = mtcars, param2 = 0))\n## [[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\n## \n## [[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## \n## [[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n\nWhat you get here is a list for each value in values_cyl; so one list for 4, one for 6 and one for 8. Suppose now that you are feeling adventurous, and want to loop over the values of am too:\n\nvalues_am = c(0, 1)\n\nSo first, we need to map a function to each element of values_am. But which function? Well, for given value of am, our problem is the same as before; we need to map nice_function() to each value of cyl. So, that’s what we’re going to do:\n\n(result = map(values_am, ~map(values_cyl, nice_function, df = mtcars, param2 = .)))\n## [[1]]\n## [[1]][[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\n## \n## [[1]][[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## \n## [[1]][[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## \n## \n## [[2]]\n## [[2]][[1]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## \n## [[2]][[2]]\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2\n## \n## [[2]][[3]]\n##    mpg cyl disp  hp drat   wt qsec vs am gear carb result\n## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4\n## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0\n\nWe now have a list of size 2 (for each value of am) where each element is itself a list of size 3 (for each value of cyl) where each element is a data frame. Are you still with me? Also, notice that the second map is given as a formula (notice the ~ in front of the second map). This creates an anonymous function, where the parameter is given by the . (think of the . as being the x in f(x)). So the . is the stand-in for the values contained inside values_am.\n\n\nThe people that are familiar with the map() functions must be fuming right now; there is a way to avoid this nested hell. I will talk about it soon, but first I want to play around with this list of lists.\n\n\nIf you have a list of data frames, you can bind their rows together with reduce(list_of_dfs, rbind). You would like to this here, but because your lists of data frames are contained inside another list… you guessed it, you have to map over it!\n\n(result2 = map(result, ~reduce(., rbind)))\n## [[1]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## \n## [[2]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 2  32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 3  30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 4  33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 5  27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 6  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 7  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 8  21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 9  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 10 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 11 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 12 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 13 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nHere again, I pass reduce() as a formula to map() to create an anonymous function. Again, the . is used as the stand-in for each element contained in result; a list of data frames, where reduce(., rbind) knows what to do. Now that we have this we can use reduce() with rbind() again to get a single data frame:\n\n(result3 = reduce(result2, rbind))\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## 20 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 21 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 22 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 23 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 24 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 25 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 26 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 27 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 28 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 29 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 31 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 32 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nOf course, since reduce(list_of_dfs, rbind) is such a common operation, you could have simply used dplyr::bind_rows, which does exactly this:\n\n(result2 = map(result, bind_rows))\n## [[1]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## \n## [[2]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 2  32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 3  30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 4  33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 5  27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 6  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 7  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 8  21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 9  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 10 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 11 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 12 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 13 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nand then:\n\n(result3 = bind_rows(result2))\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## 20 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 21 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 22 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 23 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 24 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 25 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 26 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 27 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 28 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 29 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 31 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 32 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nOf course, things are even simpler: you can avoid this deeply nested monstrosity by using map_df() instead of map()! map_df() works just like map() but return a data frame (hence the _df in the name) instead of a list:\n\n(result_df = map_df(values_am, ~map_df(values_cyl, nice_function, df = mtcars, param2 = .)))\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## 20 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 21 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 22 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 23 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 24 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 25 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 26 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 27 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 28 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 29 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 31 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 32 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nIf you look at the source code of map_df() you see that dplyr::bind_rows gets called at the end:\n\nmap_df\n## function (.x, .f, ..., .id = NULL) \n## {\n##     if (!is_installed(\"dplyr\")) {\n##         abort(\"`map_df()` requires dplyr\")\n##     }\n##     .f &lt;- as_mapper(.f, ...)\n##     res &lt;- map(.x, .f, ...)\n##     dplyr::bind_rows(res, .id = .id)\n## }\n## &lt;bytecode: 0x55dad486e6a0&gt;\n## &lt;environment: namespace:purrr&gt;\n\nSo moral of the story? There are a lot of variants of the common purrr::map() functions (as well as of dplyr verbs, such as filter_at, select_if, etc…) and learning about them can save you from a lot of pain! However, if you need to apply a function to nested lists this is still possible; you just have to think about the structure of the nested list for a bit. There is also another function that you might want to study, modify_depth() which solves related issues but I will end the blog post here. I might talk about it in a future blog post.\n\n\nAlso, if you want to learn more about R and the tidyverse, do read the link I posted in the introduction of the post and join the R4ds slack group! There are a lot of very nice people there that want to help you get better with your R-fu. Also, this is where I got the inspiration to write this blog post and I am thankful to the people there for the discussions; I feel comfortable with R, but I still learn new tips and tricks every day!\n\n\nIf you enjoy these blog posts, you can follow me on twitter. And happy new yeaR!"
  },
  {
    "objectID": "posts/2018-10-27-lux_elections_analysis.html#abstract",
    "href": "posts/2018-10-27-lux_elections_analysis.html#abstract",
    "title": "Maps with pie charts on top of each administrative division: an example with Luxembourg’s elections data",
    "section": "\nAbstract\n",
    "text": "Abstract\n\n\nYou can find the data used in this blog post here: https://github.com/b-rodrigues/elections_lux\n\n\nThis is a follow up to a previous blog post where I extracted data of the 2018 Luxembourguish elections from Excel Workbooks. Now that I have the data, I will create a map of Luxembourg by commune, with pie charts of the results on top of each commune! To do this, I use good ol’ {ggplot2} and another packages called {scatterpie}. As a bonus, I have added the code to extract the data from the 2013 elections from Excel. You’ll find this code in the appendix at the end of the blog post."
  },
  {
    "objectID": "posts/2018-10-27-lux_elections_analysis.html#introduction",
    "href": "posts/2018-10-27-lux_elections_analysis.html#introduction",
    "title": "Maps with pie charts on top of each administrative division: an example with Luxembourg’s elections data",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nBefore importing the data for the elections of 2018, let’s install some packages:\n\ninstall.packages('rgeos', type='source') # Dependency of rgdal\ninstall.packages('rgdal', type='source') # To read in the shapefile\n\nThese packages might be very tricky to install on OSX and Linux, but they’re needed to import the shapefile of the country, which is needed to draw a map. So to make things easier, I have created an rds object, from the shapefile of Luxembourg, that you can import natively in R without needing these two packages. But if you want to use them, here is how:\n\ncommunes &lt;- readOGR(\"Limadmin_SHP/LIMADM_COMMUNES.shp\")\n\nBy the way, you can download the shapefile for Luxembourg here.\n\n\nI’ll use my shapefile though (that you can download from the same github repo as the data):\n\ncommunes_df &lt;- readRDS(\"commune_shapefile.rds\")\n\nHere’s how it looks like:\n\nhead(communes_df)\n##       long      lat order  hole piece      group       id\n## 1 91057.65 101536.6     1 FALSE     1 Beaufort.1 Beaufort\n## 2 91051.79 101487.3     2 FALSE     1 Beaufort.1 Beaufort\n## 3 91043.43 101461.7     3 FALSE     1 Beaufort.1 Beaufort\n## 4 91043.37 101449.8     4 FALSE     1 Beaufort.1 Beaufort\n## 5 91040.42 101432.1     5 FALSE     1 Beaufort.1 Beaufort\n## 6 91035.44 101405.6     6 FALSE     1 Beaufort.1 Beaufort\n\nNow let’s load some packages:\n\nlibrary(\"tidyverse\")\nlibrary(\"tidyxl\")\nlibrary(\"ggplot2\")\nlibrary(\"scatterpie\")\n\nOk, now, let’s import the elections results data, which is the output of last week’s blog post:\n\nelections &lt;- read_csv(\"elections_2018.csv\")\n## Parsed with column specification:\n## cols(\n##   Party = col_character(),\n##   Year = col_double(),\n##   Variables = col_character(),\n##   Values = col_double(),\n##   locality = col_character(),\n##   division = col_character()\n## )\n\nI will only focus on the data at the commune level, and only use the share of votes for each party:\n\nelections_map &lt;- elections %&gt;%\n    filter(division == \"Commune\",\n           Variables == \"Pourcentage\")\n\nNow I need to make sure that the names of the communes are the same between the elections data and the shapefile. Usual suspects are the “Haute-Sûre” and the “Redange-sur-Attert” communes, but let’s take a look:\n\nlocality_elections &lt;- unique(elections_map$locality)\nlocality_shapefile &lt;- unique(communes_df$id)\n\nsetdiff(locality_elections, locality_shapefile)\n## [1] \"Lac de la Haute-Sûre\" \"Redange Attert\"\n\nYep, exactly as expected. I’ve had problems with the names of these two communes in the past already. Let’s rename these two communes in the elections data:\n\nelections_map &lt;- elections_map %&gt;%\n    mutate(commune = case_when(locality == \"Lac de la Haute-Sûre\" ~ \"Lac de la Haute Sûre\",\n                          locality == \"Redange Attert\" ~ \"Redange\",\n                          TRUE ~ locality))\n\nNow, I can select the relevant columns from the shapefile:\n\ncommunes_df &lt;- communes_df %&gt;%\n    select(long, lat, commune = id)\n\nand from the elections data:\n\nelections_map &lt;- elections_map %&gt;%\n    select(commune, Party, Variables, Values)"
  },
  {
    "objectID": "posts/2018-10-27-lux_elections_analysis.html#plotting-the-data-on-a-map",
    "href": "posts/2018-10-27-lux_elections_analysis.html#plotting-the-data-on-a-map",
    "title": "Maps with pie charts on top of each administrative division: an example with Luxembourg’s elections data",
    "section": "\nPlotting the data on a map\n",
    "text": "Plotting the data on a map\n\n\nNow, for the type of plot I want to make, using the {scatterpie} package, I need the data to be in the wide format, not long. For this I will use tidyr::spread():\n\nelections_map &lt;- elections_map %&gt;% \n    spread(Party, Values)\n\nThis is how the data looks now:\n\nglimpse(elections_map)\n## Observations: 102\n## Variables: 10\n## $ commune     &lt;chr&gt; \"Beaufort\", \"Bech\", \"Beckerich\", \"Berdorf\", \"Bertran…\n## $ Variables   &lt;chr&gt; \"Pourcentage\", \"Pourcentage\", \"Pourcentage\", \"Pource…\n## $ ADR         &lt;dbl&gt; 0.12835106, 0.09848661, 0.08596748, 0.16339234, 0.04…\n## $ CSV         &lt;dbl&gt; 0.2426239, 0.2945285, 0.3004751, 0.2604552, 0.290278…\n## $ `déi gréng` &lt;dbl&gt; 0.15695672, 0.21699651, 0.24072721, 0.15619529, 0.15…\n## $ `déi Lénk`  &lt;dbl&gt; 0.04043732, 0.03934808, 0.05435776, 0.02295273, 0.04…\n## $ DP          &lt;dbl&gt; 0.15875393, 0.19394645, 0.12899689, 0.15444466, 0.30…\n## $ KPL         &lt;dbl&gt; 0.015875393, 0.006519208, 0.004385164, 0.011476366, …\n## $ LSAP        &lt;dbl&gt; 0.11771754, 0.11455180, 0.08852549, 0.16592103, 0.09…\n## $ PIRATEN     &lt;dbl&gt; 0.13928411, 0.03562282, 0.09656496, 0.06516242, 0.04…\n\nFor this to work, I need two datasets; one to draw the map (commune_df) and one to draw the pie charts over each commune, with the data to draw the charts, but also the position of where I want the pie charts. For this, I will compute the average of the longitude and latitude, which should be good enough:\n\nscatterpie_data &lt;- communes_df %&gt;%\n    group_by(commune) %&gt;%\n    summarise(long = mean(long),\n              lat = mean(lat))\n\nNow, let’s join the two datasets:\n\nfinal_data &lt;- left_join(scatterpie_data, elections_map, by = \"commune\") \n\nI have all the ingredients to finally plot the data:\n\nggplot() +\n    geom_polygon(data = communes_df, aes(x = long, y = lat, group = commune), colour = \"grey\", fill = NA) +\n    geom_scatterpie(data = final_data, aes(x=long, y=lat, group=commune), \n                    cols = c(\"ADR\", \"CSV\", \"déi gréng\", \"déi Lénk\", \"DP\", \"KPL\", \"LSAP\", \"PIRATEN\")) +\n    labs(title = \"Share of total vote in each commune, 2018 elections\") +\n    theme_void() +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank(),\n          legend.text = element_text(colour = \"white\"),\n          plot.background = element_rect(\"#272b30\"),\n          plot.title = element_text(colour = \"white\")) +\n    scale_fill_manual(values = c(\"ADR\" = \"#009dd1\",\n                                 \"CSV\" = \"#ee7d00\",\n                                 \"déi gréng\" = \"#45902c\",\n                                 \"déi Lénk\" = \"#e94067\",\n                                 \"DP\" = \"#002a54\",\n                                 \"KPL\" = \"#ff0000\",\n                                 \"LSAP\" = \"#ad3648\",\n                                 \"PIRATEN\" = \"#ad5ea9\"))\n\n\n\n\nNot too bad, but we can’t really read anything from the pie charts. I will now make their size proportional to the number of voters in each commune. For this, I need to go back to the Excel sheets, and look for the right cell:\n\n\n\n\n\nIt will be easy to extract this info. It located in cell “E5”:\n\nelections_raw_2018 &lt;- xlsx_cells(\"leg-2018-10-14-22-58-09-737.xlsx\")\n\nelectors_commune &lt;- elections_raw_2018 %&gt;%\n    filter(!(sheet %in% c(\"Le Grand-Duché de Luxembourg\", \"Centre\", \"Est\", \"Nord\", \"Sud\", \"Sommaire\"))) %&gt;%\n    filter(address == \"E5\") %&gt;%\n    select(sheet, numeric) %&gt;%\n    rename(commune = sheet,\n           electors = numeric)\n\nI can now add this to the data:\n\nfinal_data &lt;- final_data %&gt;% \n    full_join(electors_commune) %&gt;%\n    mutate(log_electors = log(electors) * 200)\n## Joining, by = \"commune\"\n\nIn the last line, I create a new column called log_electors that I then multiply by 200. This will be useful later.\n\n\nNow I can add the r argument inside the aes() function on the third line, to make the pie chart size proportional to the number of electors in that commune:\n\nggplot() +\n  geom_polygon(data = communes_df, aes(x = long, y = lat, group = commune), colour = \"grey\", fill = NA) +\n    geom_scatterpie(data = final_data, aes(x=long, y=lat, group = commune, r = electors), \n                    cols = c(\"ADR\", \"CSV\", \"déi gréng\", \"déi Lénk\", \"DP\", \"KPL\", \"LSAP\", \"PIRATEN\")) +\n    labs(title = \"Share of total vote in each commune, 2018 elections\") +\n    theme_void() +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank(),\n          legend.text = element_text(colour = \"white\"),\n          plot.background = element_rect(\"#272b30\"),\n          plot.title = element_text(colour = \"white\")) +\n    scale_fill_manual(values = c(\"ADR\" = \"#009dd1\",\n                                 \"CSV\" = \"#ee7d00\",\n                                 \"déi gréng\" = \"#45902c\",\n                                 \"déi Lénk\" = \"#182024\",\n                                 \"DP\" = \"#002a54\",\n                                 \"KPL\" = \"#ff0000\",\n                                 \"LSAP\" = \"#ad3648\",\n                                 \"PIRATEN\" = \"#ad5ea9\"))\n## Warning: Removed 32 rows containing non-finite values (stat_pie).\n\n\n\n\nOk, that was not a good idea! Perhaps the best option would be to have one map per circonscription. For this, I need the list of communes by circonscription. This is available on Wikipedia. Here are the lists:\n\ncentre &lt;- c(\"Bissen\", \"Colmar-Berg\", \"Fischbach\", \"Heffingen\", \"Larochette\",\n            \"Lintgen\", \"Lorentzweiler\", \"Mersch\", \"Nommern\", \"Helperknapp\", \"Bertrange\", \"Contern\", \n            \"Hesperange\", \"Luxembourg\", \"Niederanven\", \"Sandweiler\", \"Schuttrange\", \"Steinsel\", \n            \"Strassen\", \"Walferdange\", \"Weiler-la-Tour\")\n\nest &lt;- c(\"Beaufort\", \"Bech\", \"Berdorf\", \"Consdorf\", \"Echternach\", \"Rosport-Mompach\", \"Waldbillig\",\n         \"Betzdorf\", \"Biwer\", \"Flaxweiler\", \"Grevenmacher\", \"Junglinster\", \"Manternach\", \"Mertert\",\n         \"Wormeldange\",\"Bous\", \"Dalheim\", \"Lenningen\", \"Mondorf-les-Bains\", \"Remich\", \"Schengen\",\n         \"Stadtbredimus\", \"Waldbredimus\")\n\nnord &lt;- c(\"Clervaux\", \"Parc Hosingen\", \"Troisvierges\", \"Weiswampach\", \"Wincrange\", \"Bettendorf\", \n          \"Bourscheid\", \"Diekirch\", \"Erpeldange-sur-Sûre\", \"Ettelbruck\", \"Feulen\", \"Mertzig\", \"Reisdorf\", \n          \"Schieren\", \"Vallée de l'Ernz\", \"Beckerich\", \"Ell\", \"Grosbous\", \"Préizerdaul\", \n          \"Rambrouch\", \"Redange\", \"Saeul\", \"Useldange\", \"Vichten\", \"Wahl\", \"Putscheid\", \"Tandel\",\n          \"Vianden\", \"Boulaide\", \"Esch-sur-Sûre\", \"Goesdorf\", \"Kiischpelt\", \"Lac de la Haute Sûre\",\n          \"Wiltz\", \"Winseler\")\n\nsud &lt;- c(\"Dippach\", \"Garnich\", \"Käerjeng\", \"Kehlen\", \"Koerich\", \"Kopstal\", \"Mamer\", \n         \"Habscht\", \"Steinfort\", \"Bettembourg\", \"Differdange\", \"Dudelange\", \"Esch-sur-Alzette\", \n         \"Frisange\", \"Kayl\", \"Leudelange\", \"Mondercange\", \"Pétange\", \"Reckange-sur-Mess\", \"Roeser\",\n         \"Rumelange\", \"Sanem\", \"Schifflange\")\n\ncirconscriptions &lt;- list(\"centre\" = centre, \"est\" = est,\n                         \"nord\" = nord, \"sud\" = sud)\n\nNow, I can make one map per circonscription. First, let’s split the data sets by circonscription:\n\ncommunes_df_by_circonscription &lt;- circonscriptions %&gt;%\n    map(~filter(communes_df, commune %in% .))\n\nfinal_data_by_circonscription &lt;- circonscriptions %&gt;%\n    map(~filter(final_data, commune %in% .))\n\nBy using pmap(), I can reuse the code to generate the plot to each element of the two lists. This is nice because I do not need to copy and paste the code 4 times:\n\npmap(list(x = communes_df_by_circonscription,\n          y = final_data_by_circonscription,\n          z = names(communes_df_by_circonscription)),\n     function(x, y, z){\n         ggplot() +\n        geom_polygon(data = x, aes(x = long, y = lat, group = commune), \n                     colour = \"grey\", fill = NA) +\n        geom_scatterpie(data = y, aes(x=long, y=lat, group = commune), \n                        cols = c(\"ADR\", \"CSV\", \"déi gréng\", \"déi Lénk\", \"DP\", \"KPL\", \"LSAP\", \"PIRATEN\")) +\n        labs(title = paste0(\"Share of total vote in each commune, 2018 elections for circonscription \", z)) +\n        theme_void() +\n        theme(legend.position = \"bottom\",\n              legend.title = element_blank(),\n              legend.text = element_text(colour = \"white\"),\n              plot.background = element_rect(\"#272b30\"),\n              plot.title = element_text(colour = \"white\")) + \n        scale_fill_manual(values = c(\"ADR\" = \"#009dd1\",\n                                     \"CSV\" = \"#ee7d00\",\n                                     \"déi gréng\" = \"#45902c\",\n                                     \"déi Lénk\" = \"#182024\",\n                                     \"DP\" = \"#002a54\",\n                                     \"KPL\" = \"#ff0000\",\n                                     \"LSAP\" = \"#ad3648\",\n                                     \"PIRATEN\" = \"#ad5ea9\"))\n     }\n)\n## $centre\n\n\n\n## \n## $est\n\n\n\n## \n## $nord\n\n\n\n## \n## $sud\n\n\n\n\nI created an anonymous function of three argument, x, y and z. If you are unfamiliar with pmap(), study the above code closely. If you have questions, do not hesitate to reach out!\n\n\nThe pie charts are still quite small, but if I try to change the size of the pie charts, I’ll have the same problem as before: inside the same circonscription, some communes have really a lot of electors, and some a very small number. Perhaps I can try with the log of the electors?\n\npmap(list(x = communes_df_by_circonscription,\n          y = final_data_by_circonscription,\n          z = names(communes_df_by_circonscription)),\n     function(x, y, z){\n         ggplot() +\n        geom_polygon(data = x, aes(x = long, y = lat, group = commune), \n                     colour = \"grey\", fill = NA) +\n        geom_scatterpie(data = y, aes(x=long, y=lat, group = commune, r = log_electors), \n                        cols = c(\"ADR\", \"CSV\", \"déi gréng\", \"déi Lénk\", \"DP\", \"KPL\", \"LSAP\", \"PIRATEN\")) +\n        labs(title = paste0(\"Share of total vote in each commune, 2018 elections for circonscription \", z)) +\n        theme_void() +\n        theme(legend.position = \"bottom\",\n              legend.title = element_blank(),\n              legend.text = element_text(colour = \"white\"),\n              plot.background = element_rect(\"#272b30\"),\n              plot.title = element_text(colour = \"white\")) + \n        scale_fill_manual(values = c(\"ADR\" = \"#009dd1\",\n                                     \"CSV\" = \"#ee7d00\",\n                                     \"déi gréng\" = \"#45902c\",\n                                     \"déi Lénk\" = \"#182024\",\n                                     \"DP\" = \"#002a54\",\n                                     \"KPL\" = \"#ff0000\",\n                                     \"LSAP\" = \"#ad3648\",\n                                     \"PIRATEN\" = \"#ad5ea9\"))\n     }\n)\n## $centre\n\n\n\n## \n## $est\n\n\n\n## \n## $nord\n## Warning: Removed 16 rows containing non-finite values (stat_pie).\n\n\n\n## \n## $sud\n\n\n\n\nThis looks better now!"
  },
  {
    "objectID": "posts/2018-10-27-lux_elections_analysis.html#conclusion",
    "href": "posts/2018-10-27-lux_elections_analysis.html#conclusion",
    "title": "Maps with pie charts on top of each administrative division: an example with Luxembourg’s elections data",
    "section": "\nConclusion\n",
    "text": "Conclusion\n\n\nHaving data in a machine readable format is really important. The amount of code I had to write to go from the Excel Workbooks that contained the data to this plots is quite large, but if the data was in a machine readable format to start with, I could have focused on the plots immediately.\n\n\nThe good thing is that I got to practice my skills and discovered {scatterpie}!\n\n\n\nAppendix\n\n\nThe following lines of code extract the data (from the 2013 elections) from the Excel Workbooks that can be found in Luxembourguish Open Data Portal.\n\n\nI will not comment them, as they work in a similar way than in the previous blog post where I extracted the data from the 2018 elections. The only difference, is that the sheet with the national level data was totally different, so I did not extract it. The first reason is because I don’t need it for this blog post, the second is because I was lazy. For me, that’s two pretty good reasons not to do something. If you have a question concerning the code below, don’t hesitate to reach out though!\n\nlibrary(\"tidyverse\")\nlibrary(\"tidyxl\")\nlibrary(\"brotools\")\n\npath &lt;- Sys.glob(\"content/blog/2013*xlsx\")[-5]\n\nelections_raw_2013 &lt;- map(path, xlsx_cells) %&gt;%\n    map(~filter(., sheet != \"Sommaire\"))\n\nelections_sheets_2013 &lt;- map(map(path, xlsx_sheet_names), ~`%-l%`(., \"Sommaire\"))\n\nlist_targets &lt;- list(\"Centre\" = seq(9, 32),\n                    \"Est\" = seq(9, 18),\n                    \"Nord\" = seq(9, 20),\n                    \"Sud\" = seq(9, 34))\n\nposition_parties_national &lt;- seq(1, 24, by = 3)\n\nextract_party &lt;- function(dataset, starting_col, target_rows){\n    \n    almost_clean &lt;- dataset %&gt;%\n        filter(row %in% target_rows) %&gt;%\n        filter(col %in% c(starting_col, starting_col + 1)) %&gt;%\n        select(character, numeric) %&gt;%\n        fill(numeric, .direction = \"up\") %&gt;%\n        filter(!is.na(character))\n    \n    party_name &lt;- almost_clean$character[1]\n    \n    almost_clean$character[1] &lt;- \"Pourcentage\"\n    \n    almost_clean$party &lt;- party_name\n    \n    colnames(almost_clean) &lt;- c(\"Variables\", \"Values\", \"Party\")\n    \n    almost_clean %&gt;%\n        mutate(Year = 2013) %&gt;%\n        select(Party, Year, Variables, Values)\n    \n}\n\n\n# Treat one district\n\nextract_district &lt;- function(dataset, sheets, target_rows, position_parties_national){\n\n    list_data_districts &lt;- map(sheets, ~filter(.data = dataset, sheet == .)) \n\n    elections_districts_2013 &lt;- map(.x = list_data_districts,\n                                    ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = target_rows))\n\n    map2(.y = elections_districts_2013, .x = sheets,\n         ~mutate(.y, locality = .x, division = \"Commune\", Year = \"2013\")) %&gt;%\n        bind_rows()\n}\n\nelections_2013 &lt;- pmap_dfr(list(x = elections_raw_2013, \n          y = elections_sheets_2013,\n          z = list_targets), \n     function(x, y, z){\n         map_dfr(position_parties_national, \n             ~extract_district(dataset = x, sheets = y, target_rows = z, position_parties_national = .))\n     })\n\n# Correct districts\nelections_2013 &lt;- elections_2013 %&gt;%\n    mutate(division = case_when(locality == \"CENTRE\" ~ \"Electoral district\",\n                                locality == \"EST\" ~ \"Electoral district\",\n                                locality == \"NORD\" ~ \"Electoral district\",\n                                locality == \"SUD\" ~ \"Electoral district\",\n                                TRUE ~ division))"
  },
  {
    "objectID": "posts/2018-10-27-lux_elections_analysis.html#appendix",
    "href": "posts/2018-10-27-lux_elections_analysis.html#appendix",
    "title": "Maps with pie charts on top of each administrative division: an example with Luxembourg’s elections data",
    "section": "\nAppendix\n",
    "text": "Appendix\n\n\nThe following lines of code extract the data (from the 2013 elections) from the Excel Workbooks that can be found in Luxembourguish Open Data Portal.\n\n\nI will not comment them, as they work in a similar way than in the previous blog post where I extracted the data from the 2018 elections. The only difference, is that the sheet with the national level data was totally different, so I did not extract it. The first reason is because I don’t need it for this blog post, the second is because I was lazy. For me, that’s two pretty good reasons not to do something. If you have a question concerning the code below, don’t hesitate to reach out though!\n\nlibrary(\"tidyverse\")\nlibrary(\"tidyxl\")\nlibrary(\"brotools\")\n\npath &lt;- Sys.glob(\"content/blog/2013*xlsx\")[-5]\n\nelections_raw_2013 &lt;- map(path, xlsx_cells) %&gt;%\n    map(~filter(., sheet != \"Sommaire\"))\n\nelections_sheets_2013 &lt;- map(map(path, xlsx_sheet_names), ~`%-l%`(., \"Sommaire\"))\n\nlist_targets &lt;- list(\"Centre\" = seq(9, 32),\n                    \"Est\" = seq(9, 18),\n                    \"Nord\" = seq(9, 20),\n                    \"Sud\" = seq(9, 34))\n\nposition_parties_national &lt;- seq(1, 24, by = 3)\n\nextract_party &lt;- function(dataset, starting_col, target_rows){\n    \n    almost_clean &lt;- dataset %&gt;%\n        filter(row %in% target_rows) %&gt;%\n        filter(col %in% c(starting_col, starting_col + 1)) %&gt;%\n        select(character, numeric) %&gt;%\n        fill(numeric, .direction = \"up\") %&gt;%\n        filter(!is.na(character))\n    \n    party_name &lt;- almost_clean$character[1]\n    \n    almost_clean$character[1] &lt;- \"Pourcentage\"\n    \n    almost_clean$party &lt;- party_name\n    \n    colnames(almost_clean) &lt;- c(\"Variables\", \"Values\", \"Party\")\n    \n    almost_clean %&gt;%\n        mutate(Year = 2013) %&gt;%\n        select(Party, Year, Variables, Values)\n    \n}\n\n\n# Treat one district\n\nextract_district &lt;- function(dataset, sheets, target_rows, position_parties_national){\n\n    list_data_districts &lt;- map(sheets, ~filter(.data = dataset, sheet == .)) \n\n    elections_districts_2013 &lt;- map(.x = list_data_districts,\n                                    ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = target_rows))\n\n    map2(.y = elections_districts_2013, .x = sheets,\n         ~mutate(.y, locality = .x, division = \"Commune\", Year = \"2013\")) %&gt;%\n        bind_rows()\n}\n\nelections_2013 &lt;- pmap_dfr(list(x = elections_raw_2013, \n          y = elections_sheets_2013,\n          z = list_targets), \n     function(x, y, z){\n         map_dfr(position_parties_national, \n             ~extract_district(dataset = x, sheets = y, target_rows = z, position_parties_national = .))\n     })\n\n# Correct districts\nelections_2013 &lt;- elections_2013 %&gt;%\n    mutate(division = case_when(locality == \"CENTRE\" ~ \"Electoral district\",\n                                locality == \"EST\" ~ \"Electoral district\",\n                                locality == \"NORD\" ~ \"Electoral district\",\n                                locality == \"SUD\" ~ \"Electoral district\",\n                                TRUE ~ division))"
  },
  {
    "objectID": "posts/2017-10-26-margins_r.html",
    "href": "posts/2017-10-26-margins_r.html",
    "title": "Easy peasy STATA-like marginal effects with R",
    "section": "",
    "text": "Model interpretation is essential in the social sciences. If one wants to know the effect of variable x on the dependent variable y, marginal effects are an easy way to get the answer. STATA includes a margins command that has been ported to R by Thomas J. Leeper of the London School of Economics and Political Science. You can find the source code of the package on github. In this short blog post, I demo some of the functionality of margins.\n\n\nFirst, let’s load some packages:\n\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(broom)\nlibrary(margins)\nlibrary(Ecdat)\n\nAs an example, we are going to use the Participation data from the Ecdat package:\n\ndata(Participation)\n?Participation\nLabor Force Participation\n\nDescription\n\na cross-section\n\nnumber of observations : 872\n\nobservation : individuals\n\ncountry : Switzerland\n\nUsage\n\ndata(Participation)\nFormat\n\nA dataframe containing :\n\nlfp\nlabour force participation ?\n\nlnnlinc\nthe log of nonlabour income\n\nage\nage in years divided by 10\n\neduc\nyears of formal education\n\nnyc\nthe number of young children (younger than 7)\n\nnoc\nnumber of older children\n\nforeign\nforeigner ?\n\nSource\n\nGerfin, Michael (1996) “Parametric and semiparametric estimation of the binary response”, Journal of Applied Econometrics, 11(3), 321-340.\n\nReferences\n\nDavidson, R. and James G. MacKinnon (2004) Econometric Theory and Methods, New York, Oxford University Press, http://www.econ.queensu.ca/ETM/, chapter 11.\n\nJournal of Applied Econometrics data archive : http://qed.econ.queensu.ca/jae/.\n\nThe variable of interest is lfp: whether the individual participates in the labour force or not. To know which variables are relevant in the decision to participate in the labour force, one could estimate a logit model, using glm().\n\nlogit_participation = glm(lfp ~ ., data = Participation, family = \"binomial\")\n\nNow that we ran the regression, we can take a look at the results. I like to use broom::tidy() to look at the results of regressions, as tidy() returns a nice data.frame, but you could use summary() if you’re only interested in reading the output:\n\ntidy(logit_participation)\n##          term    estimate  std.error  statistic      p.value\n## 1 (Intercept) 10.37434616 2.16685216  4.7877499 1.686617e-06\n## 2     lnnlinc -0.81504064 0.20550116 -3.9661122 7.305449e-05\n## 3         age -0.51032975 0.09051783 -5.6378920 1.721444e-08\n## 4        educ  0.03172803 0.02903580  1.0927211 2.745163e-01\n## 5         nyc -1.33072362 0.18017027 -7.3859224 1.514000e-13\n## 6         noc -0.02198573 0.07376636 -0.2980454 7.656685e-01\n## 7  foreignyes  1.31040497 0.19975784  6.5599678 5.381941e-11\n\nFrom the results above, one can only interpret the sign of the coefficients. To know how much a variable influences the labour force participation, one has to use margins():\n\neffects_logit_participation = margins(logit_participation) \n\nprint(effects_logit_participation)\n## Average marginal effects\n## glm(formula = lfp ~ ., family = \"binomial\", data = Participation)\n##  lnnlinc     age     educ     nyc       noc foreignyes\n##  -0.1699 -0.1064 0.006616 -0.2775 -0.004584     0.2834\n\nUsing summary() on the object returned by margins() provides more details:\n\nsummary(effects_logit_participation)\n##      factor     AME     SE       z      p   lower   upper\n##         age -0.1064 0.0176 -6.0494 0.0000 -0.1409 -0.0719\n##        educ  0.0066 0.0060  1.0955 0.2733 -0.0052  0.0185\n##  foreignyes  0.2834 0.0399  7.1102 0.0000  0.2053  0.3615\n##     lnnlinc -0.1699 0.0415 -4.0994 0.0000 -0.2512 -0.0887\n##         noc -0.0046 0.0154 -0.2981 0.7656 -0.0347  0.0256\n##         nyc -0.2775 0.0333 -8.3433 0.0000 -0.3426 -0.2123\n\nAnd it is also possible to plot the effects with base graphics:\n\nplot(effects_logit_participation)\n\n\n\n\nThis uses the basic R plotting capabilities, which is useful because it is a simple call to the function plot() but if you’ve been using ggplot2 and want this graph to have the same look as the others made with ggplot2 you first need to save the summary in a variable. Let’s overwrite this effects_logit_participation variable with its summary:\n\neffects_logit_participation = summary(effects_logit_participation)\n\nAnd now it is possible to use ggplot2 to create the same plot:\n\nggplot(data = effects_logit_participation) +\n  geom_point(aes(factor, AME)) +\n  geom_errorbar(aes(x = factor, ymin = lower, ymax = upper)) +\n  geom_hline(yintercept = 0) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45))\n\n\n\n\nSo an infinitesimal increase, in say, non-labour income (lnnlinc) of 0.001 is associated with a decrease of the probability of labour force participation by 0.001*17 percentage points.\n\n\nYou can also extract the marginal effects of a single variable, with dydx():\n\nhead(dydx(Participation, logit_participation, \"lnnlinc\"))\n##   dydx_lnnlinc\n## 1  -0.15667764\n## 2  -0.20014487\n## 3  -0.18495109\n## 4  -0.05377262\n## 5  -0.18710476\n## 6  -0.19586986\n\nWhich makes it possible to extract the effects for a list of individuals that you can create yourself:\n\nmy_subjects = tribble(\n    ~lfp,  ~lnnlinc, ~age, ~educ, ~nyc, ~noc, ~foreign,\n    \"yes\",   10.780,  7.0,     4,    1,    1,    \"yes\",\n     \"no\",     1.30,  9.0,     1,    4,    1,    \"yes\"\n)\n\ndydx(my_subjects, logit_participation, \"lnnlinc\")\n##   dydx_lnnlinc\n## 1  -0.09228119\n## 2  -0.17953451\n\nI used the tribble() function from the tibble package to create this test data set, row by row. Then, using dydx(), I get the marginal effect of variable lnnlinc for these two individuals. No doubt that this package will be a huge help convincing more social scientists to try out R and make a potential transition from STATA easier."
  },
  {
    "objectID": "posts/2015-01-12-introduction-to-programming-econometrics-with-r.html",
    "href": "posts/2015-01-12-introduction-to-programming-econometrics-with-r.html",
    "title": "Introduction to programming econometrics with R",
    "section": "",
    "text": "This semester, I’ll be teaching an introduction to applied econometrics with R, so I’ve decided to write a very small book called “Introduction to programming Econometrics with R”. This is primarily intended for bachelor students and the focus is not much on econometric theory, but more on how to implement econometric theory into computer code, using the R programming language. It’s very basic and doesn’t cover any advanced topics in econometrics and is intended for people with 0 previous programming knowledge. It is still very rough around the edges, and it’s missing the last chapter about reproducible research, and the references, but I think it’s time to put it out there; someone else than my students may find it useful. The book’s probably full of typos and mistakes, so don’t hesitate to drop me an e-mail if you find something fishy: contact@brodrigues.co\nAlso there might be some sections at the beginning that only concern my students. Just ignore that.\nGet it here: download\n\nUpdate (2017-01-22)\nYou might find the book useful as it is now, but I never had a chance to finish it. I might get back to it once I’ll have more time, and port it to bookdown."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html",
    "href": "posts/2017-03-08-lesser_known_tricks.html",
    "title": "Lesser known dplyr tricks",
    "section": "",
    "text": "In this blog post I share some lesser-known (at least I believe they are) tricks that use mainly functions from dplyr."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#removing-unneeded-columns",
    "href": "posts/2017-03-08-lesser_known_tricks.html#removing-unneeded-columns",
    "title": "Lesser known dplyr tricks",
    "section": "\nRemoving unneeded columns\n",
    "text": "Removing unneeded columns\n\n\nDid you know that you can use - in front of a column name to remove it from a data frame?\n\nmtcars %&gt;% \n    select(-disp) %&gt;% \n    head()\n##                    mpg cyl  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#re-ordering-columns",
    "href": "posts/2017-03-08-lesser_known_tricks.html#re-ordering-columns",
    "title": "Lesser known dplyr tricks",
    "section": "\nRe-ordering columns\n",
    "text": "Re-ordering columns\n\n\nStill using select(), it is easy te re-order columns in your data frame:\n\nmtcars %&gt;% \n    select(cyl, disp, hp, everything()) %&gt;% \n    head()\n##                   cyl disp  hp  mpg drat    wt  qsec vs am gear carb\n## Mazda RX4           6  160 110 21.0 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag       6  160 110 21.0 3.90 2.875 17.02  0  1    4    4\n## Datsun 710          4  108  93 22.8 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive      6  258 110 21.4 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout   8  360 175 18.7 3.15 3.440 17.02  0  0    3    2\n## Valiant             6  225 105 18.1 2.76 3.460 20.22  1  0    3    1\n\nAs its name implies everything() simply means all the other columns."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#renaming-columns-with-rename",
    "href": "posts/2017-03-08-lesser_known_tricks.html#renaming-columns-with-rename",
    "title": "Lesser known dplyr tricks",
    "section": "\nRenaming columns with rename()\n",
    "text": "Renaming columns with rename()\n\nmtcars &lt;- rename(mtcars, spam_mpg = mpg)\nmtcars &lt;- rename(mtcars, spam_disp = disp)\nmtcars &lt;- rename(mtcars, spam_hp = hp)\n\nhead(mtcars)\n##                   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am\n## Mazda RX4             21.0   6       160     110 3.90 2.620 16.46  0  1\n## Mazda RX4 Wag         21.0   6       160     110 3.90 2.875 17.02  0  1\n## Datsun 710            22.8   4       108      93 3.85 2.320 18.61  1  1\n## Hornet 4 Drive        21.4   6       258     110 3.08 3.215 19.44  1  0\n## Hornet Sportabout     18.7   8       360     175 3.15 3.440 17.02  0  0\n## Valiant               18.1   6       225     105 2.76 3.460 20.22  1  0\n##                   gear carb\n## Mazda RX4            4    4\n## Mazda RX4 Wag        4    4\n## Datsun 710           4    1\n## Hornet 4 Drive       3    1\n## Hornet Sportabout    3    2\n## Valiant              3    1"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#selecting-columns-with-a-regexp",
    "href": "posts/2017-03-08-lesser_known_tricks.html#selecting-columns-with-a-regexp",
    "title": "Lesser known dplyr tricks",
    "section": "\nSelecting columns with a regexp\n",
    "text": "Selecting columns with a regexp\n\n\nIt is easy to select the columns that start with “spam” with some helper functions:\n\nmtcars %&gt;% \n    select(contains(\"spam\")) %&gt;% \n    head()\n##                   spam_mpg spam_disp spam_hp\n## Mazda RX4             21.0       160     110\n## Mazda RX4 Wag         21.0       160     110\n## Datsun 710            22.8       108      93\n## Hornet 4 Drive        21.4       258     110\n## Hornet Sportabout     18.7       360     175\n## Valiant               18.1       225     105\n\ntake also a look at starts_with(), ends_with(), contains(), matches(), num_range(), one_of() and everything()."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#create-new-columns-with-mutate-and-if_else",
    "href": "posts/2017-03-08-lesser_known_tricks.html#create-new-columns-with-mutate-and-if_else",
    "title": "Lesser known dplyr tricks",
    "section": "\nCreate new columns with mutate() and if_else()\n",
    "text": "Create new columns with mutate() and if_else()\n\nmtcars %&gt;% \n    mutate(vs_new = if_else(\n        vs == 1, \n        \"one\", \n        \"zero\", \n        NA_character_)) %&gt;% \n    head()\n##   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am gear carb vs_new\n## 1     21.0   6       160     110 3.90 2.620 16.46  0  1    4    4   zero\n## 2     21.0   6       160     110 3.90 2.875 17.02  0  1    4    4   zero\n## 3     22.8   4       108      93 3.85 2.320 18.61  1  1    4    1    one\n## 4     21.4   6       258     110 3.08 3.215 19.44  1  0    3    1    one\n## 5     18.7   8       360     175 3.15 3.440 17.02  0  0    3    2   zero\n## 6     18.1   6       225     105 2.76 3.460 20.22  1  0    3    1    one\n\nYou might want to create a new variable conditionally on several values of another column:\n\nmtcars %&gt;% \n    mutate(carb_new = case_when(.$carb == 1 ~ \"one\",\n                                .$carb == 2 ~ \"two\",\n                                .$carb == 4 ~ \"four\",\n                                 TRUE ~ \"other\")) %&gt;% \n    head(15)\n##    spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am gear carb\n## 1      21.0   6     160.0     110 3.90 2.620 16.46  0  1    4    4\n## 2      21.0   6     160.0     110 3.90 2.875 17.02  0  1    4    4\n## 3      22.8   4     108.0      93 3.85 2.320 18.61  1  1    4    1\n## 4      21.4   6     258.0     110 3.08 3.215 19.44  1  0    3    1\n## 5      18.7   8     360.0     175 3.15 3.440 17.02  0  0    3    2\n## 6      18.1   6     225.0     105 2.76 3.460 20.22  1  0    3    1\n## 7      14.3   8     360.0     245 3.21 3.570 15.84  0  0    3    4\n## 8      24.4   4     146.7      62 3.69 3.190 20.00  1  0    4    2\n## 9      22.8   4     140.8      95 3.92 3.150 22.90  1  0    4    2\n## 10     19.2   6     167.6     123 3.92 3.440 18.30  1  0    4    4\n## 11     17.8   6     167.6     123 3.92 3.440 18.90  1  0    4    4\n## 12     16.4   8     275.8     180 3.07 4.070 17.40  0  0    3    3\n## 13     17.3   8     275.8     180 3.07 3.730 17.60  0  0    3    3\n## 14     15.2   8     275.8     180 3.07 3.780 18.00  0  0    3    3\n## 15     10.4   8     472.0     205 2.93 5.250 17.98  0  0    3    4\n##    carb_new\n## 1      four\n## 2      four\n## 3       one\n## 4       one\n## 5       two\n## 6       one\n## 7      four\n## 8       two\n## 9       two\n## 10     four\n## 11     four\n## 12    other\n## 13    other\n## 14    other\n## 15     four\n\nMind the .$ before the variable carb. There is a github issue about this, and it is already fixed in the development version of dplyr, which means that in the next version of dplyr, case_when() will work as any other specialized dplyr function inside mutate()."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#apply-a-function-to-certain-columns-only-by-rows",
    "href": "posts/2017-03-08-lesser_known_tricks.html#apply-a-function-to-certain-columns-only-by-rows",
    "title": "Lesser known dplyr tricks",
    "section": "\nApply a function to certain columns only, by rows\n",
    "text": "Apply a function to certain columns only, by rows\n\nmtcars %&gt;%\n    select(am, gear, carb) %&gt;%\n    purrr::by_row(sum, .collate = \"cols\", .to = \"sum_am_gear_carb\") -&gt; mtcars2\nhead(mtcars2)\n\nFor this, I had to use purrr’s by_row() function. You can then add this column to your original data frame:\n\nmtcars &lt;- cbind(mtcars, \"sum_am_gear_carb\" = mtcars2$sum_am_gear_carb)\nhead(mtcars)\n##                   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am\n## Mazda RX4             21.0   6       160     110 3.90 2.620 16.46  0  1\n## Mazda RX4 Wag         21.0   6       160     110 3.90 2.875 17.02  0  1\n## Datsun 710            22.8   4       108      93 3.85 2.320 18.61  1  1\n## Hornet 4 Drive        21.4   6       258     110 3.08 3.215 19.44  1  0\n## Hornet Sportabout     18.7   8       360     175 3.15 3.440 17.02  0  0\n## Valiant               18.1   6       225     105 2.76 3.460 20.22  1  0\n##                   gear carb sum_am_gear_carb\n## Mazda RX4            4    4                9\n## Mazda RX4 Wag        4    4                9\n## Datsun 710           4    1                6\n## Hornet 4 Drive       3    1                4\n## Hornet Sportabout    3    2                5\n## Valiant              3    1                4"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#use-do-to-do-any-arbitrary-operation",
    "href": "posts/2017-03-08-lesser_known_tricks.html#use-do-to-do-any-arbitrary-operation",
    "title": "Lesser known dplyr tricks",
    "section": "\nUse do() to do any arbitrary operation\n",
    "text": "Use do() to do any arbitrary operation\n\nmtcars %&gt;% \n    group_by(cyl) %&gt;% \n    do(models = lm(spam_mpg ~ drat + wt, data = .)) %&gt;% \n    broom::tidy(models)\n## # A tibble: 9 x 6\n## # Groups:   cyl [3]\n##     cyl term        estimate std.error statistic p.value\n##   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1     4 (Intercept)   33.2      17.1       1.94  0.0877 \n## 2     4 drat           1.32      3.45      0.384 0.711  \n## 3     4 wt            -5.24      2.22     -2.37  0.0456 \n## 4     6 (Intercept)   30.7       7.51      4.08  0.0151 \n## 5     6 drat          -0.444     1.17     -0.378 0.725  \n## 6     6 wt            -2.99      1.57     -1.91  0.129  \n## 7     8 (Intercept)   29.7       7.09      4.18  0.00153\n## 8     8 drat          -1.47      1.63     -0.903 0.386  \n## 9     8 wt            -2.45      0.799    -3.07  0.0107\n\ndo() is useful when you want to use any R function (user defined functions work too!) with dplyr functions. First I grouped the observations by cyl and then ran a linear model for each group. Then I converted the output to a tidy data frame using broom::tidy()."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#using-dplyr-functions-inside-your-own-functions",
    "href": "posts/2017-03-08-lesser_known_tricks.html#using-dplyr-functions-inside-your-own-functions",
    "title": "Lesser known dplyr tricks",
    "section": "\nUsing dplyr functions inside your own functions\n",
    "text": "Using dplyr functions inside your own functions\n\nextract_vars &lt;- function(data, some_string){\n    \n  data %&gt;%\n    select_(lazyeval::interp(~contains(some_string))) -&gt; data\n    \n  return(data)\n}\n\nextract_vars(mtcars, \"spam\")\n##                     spam_mpg spam_disp spam_hp\n## Mazda RX4               21.0     160.0     110\n## Mazda RX4 Wag           21.0     160.0     110\n## Datsun 710              22.8     108.0      93\n## Hornet 4 Drive          21.4     258.0     110\n## Hornet Sportabout       18.7     360.0     175\n## Valiant                 18.1     225.0     105\n## Duster 360              14.3     360.0     245\n## Merc 240D               24.4     146.7      62\n## Merc 230                22.8     140.8      95\n## Merc 280                19.2     167.6     123\n## Merc 280C               17.8     167.6     123\n## Merc 450SE              16.4     275.8     180\n## Merc 450SL              17.3     275.8     180\n## Merc 450SLC             15.2     275.8     180\n## Cadillac Fleetwood      10.4     472.0     205\n## Lincoln Continental     10.4     460.0     215\n## Chrysler Imperial       14.7     440.0     230\n## Fiat 128                32.4      78.7      66\n## Honda Civic             30.4      75.7      52\n## Toyota Corolla          33.9      71.1      65\n## Toyota Corona           21.5     120.1      97\n## Dodge Challenger        15.5     318.0     150\n## AMC Javelin             15.2     304.0     150\n## Camaro Z28              13.3     350.0     245\n## Pontiac Firebird        19.2     400.0     175\n## Fiat X1-9               27.3      79.0      66\n## Porsche 914-2           26.0     120.3      91\n## Lotus Europa            30.4      95.1     113\n## Ford Pantera L          15.8     351.0     264\n## Ferrari Dino            19.7     145.0     175\n## Maserati Bora           15.0     301.0     335\n## Volvo 142E              21.4     121.0     109\n\nAbout this last point, you can read more about it here.\n\n\nHope you liked this small list of tricks!"
  },
  {
    "objectID": "posts/2018-11-03-nethack_analysis.html#abstract",
    "href": "posts/2018-11-03-nethack_analysis.html#abstract",
    "title": "Analyzing NetHack data, part 1: What kills the players",
    "section": "\nAbstract\n",
    "text": "Abstract\n\n\nIn this post, I will analyse the data I scraped and put into an R package, which I called {nethack}. NetHack is a roguelike game; for more context, read my previous blog post. You can install the {nethack} package and play around with the data yourself by installing it from github:\n\ndevtools::install_github(\"b-rodrigues/nethack\")\n\nAnd to use it:\n\nlibrary(nethack)\ndata(\"nethack\")\n\nThe data contains information on games played from 2001 to 2018; 322485 rows and 14 columns. I will analyze the data in a future blog post. This post focuses on getting and then sharing the data. By the way, all the content from the public server I scrape is under the CC BY 4.0 license.\n\n\nI built the package by using the very useful {devtools} package."
  },
  {
    "objectID": "posts/2018-11-03-nethack_analysis.html#introduction",
    "href": "posts/2018-11-03-nethack_analysis.html#introduction",
    "title": "Analyzing NetHack data, part 1: What kills the players",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nWhat I want from this first analysis are several, simple things: how many players manage to ascend (meaning, winning), what monster kills most players, and finally extract data from the dumplog column. The dumplog column is a bit special; each element of the dumplog column is a log file that contains a lot of information from the last turns of a player. I will leave this for a future blog post, though.\n\n\nLet’s load some packages first:\n\nlibrary(nethack)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(magrittr)\nlibrary(brotools)\n\n{brotools} is my own package that contains some functions that I use daily. If you want to install it, run the following line:\n\ndevtools::install_github(\"b-rodrigues/brotools\")\n\nThe documentation is not up-to-date, I think I’ll do that and release it on CRAN. Some day.\n\n\nNow, let’s load the “nethack” data, included in the {nethack} package:\n\n##   rank score     name time turns lev_max hp_max role race gender alignment\n## 1    1   360      jkm &lt;NA&gt;    NA     2/2  -2/25  Sam  Hum    Mal       Law\n## 2    2   172 yosemite &lt;NA&gt;    NA     1/1  -1/10  Tou  Hum    Fem       Neu\n## 3    3  2092    dtype &lt;NA&gt;    NA     6/7  -2/47  Val  Hum    Fem       Neu\n## 4    4    32   joorko &lt;NA&gt;    NA     1/1   0/15  Sam  Hum    Mal       Law\n## 5    5   118    jorko &lt;NA&gt;    NA     1/1   0/11  Rog  Orc    Fem       Cha\n## 6    6  1757   aaronl &lt;NA&gt;    NA     5/5   0/60  Bar  Hum    Mal       Neu\n##                                                      death       date\n## 1                                   killed by a brown mold 2001-10-24\n## 2                                       killed by a jackal 2001-10-24\n## 3                                     killed by a fire ant 2001-10-24\n## 4                                       killed by a jackal 2001-10-24\n## 5                                       killed by a jackal 2001-10-24\n## 6 killed by a hallucinogen-distorted ghoul, while helpless 2001-10-24\n##   dumplog\n## 1      NA\n## 2      NA\n## 3      NA\n## 4      NA\n## 5      NA\n## 6      NA\ndata(\"nethack\")\n\nhead(nethack)\n\nLet’s create some variables that might be helpful (or perhaps not, we’ll see):\n\nnethack %&lt;&gt;% \n  mutate(date = ymd(date),\n         year = year(date),\n         month = month(date),\n         day = day(date))\n\nThis makes it easy to look at the data from, say, June 2017:\n\nnethack %&gt;%\n  filter(year == 2017, month == 6) %&gt;%\n  brotools::describe()\n## # A tibble: 17 x 17\n##    variable type   nobs    mean      sd mode    min     max   q05   q25\n##    &lt;chr&gt;    &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 day      Nume…  1451    17.4  9.00e0 1         1      30     2   10 \n##  2 month    Nume…  1451     6    0.     6         6       6     6    6 \n##  3 rank     Nume…  1451    47.1  2.95e1 1         1     100     4   20 \n##  4 score    Nume…  1451 38156.   3.39e5 488       0 5966425    94  402.\n##  5 turns    Nume…  1451  4179.   1.23e4 812       1  291829   204  860.\n##  6 year     Nume…  1451  2017    0.     2017   2017    2017  2017 2017 \n##  7 alignme… Char…  1451    NA   NA      Law      NA      NA    NA   NA \n##  8 death    Char…  1451    NA   NA      kill…    NA      NA    NA   NA \n##  9 gender   Char…  1451    NA   NA      Mal      NA      NA    NA   NA \n## 10 hp_max   Char…  1451    NA   NA      -1/16    NA      NA    NA   NA \n## 11 lev_max  Char…  1451    NA   NA      4/4      NA      NA    NA   NA \n## 12 name     Char…  1451    NA   NA      ohno…    NA      NA    NA   NA \n## 13 race     Char…  1451    NA   NA      Hum      NA      NA    NA   NA \n## 14 role     Char…  1451    NA   NA      Kni      NA      NA    NA   NA \n## 15 time     Char…  1451    NA   NA      01:1…    NA      NA    NA   NA \n## 16 dumplog  List   1451    NA   NA      &lt;NA&gt;     NA      NA    NA   NA \n## 17 date     Date   1451    NA   NA      &lt;NA&gt;     NA      NA    NA   NA \n## # … with 7 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, q95 &lt;dbl&gt;,\n## #   n_missing &lt;int&gt;, n_unique &lt;int&gt;, starting_date &lt;date&gt;,\n## #   ending_date &lt;date&gt;\n\nLet’s also take a look at a dumplog:\n\n\n\n\nClick to expand; the dumplog is quite long\n\n\nnethack %&gt;%\n    filter(year == 2018, month == 10) %&gt;%\n    slice(1) %&gt;%\n    pull(dumplog)\n## [[1]]\n##   [1] \"Unix NetHack Version 3.6.1 - last build Fri Apr 27 19:25:48 2018. (d4ebae12f1a709d1833cf466dd0c553fb97518d2)\"\n##   [2] \"\"                                                                                                            \n##   [3] \"Game began 2018-09-30 22:27:18, ended 2018-10-01 00:01:12.\"                                                  \n##   [4] \"\"                                                                                                            \n##   [5] \"brothertrebius, neutral female gnomish Ranger\"                                                               \n##   [6] \"\"                                                                                                            \n##   [7] \"                  -----\"                                                                                     \n##   [8] \"   --------       |....#     -----      --------\"                                                            \n##   [9] \"   |/..%.=|      #...^|######|...|    ##.......|\"                                                            \n##  [10] \"   |/[%..%|      #|...|     #|...|    # |......|\"                                                            \n##  [11] \"   |......|      #-----     #-...-######....&lt;..|\"                                                            \n##  [12] \"   -----|--      ###         -|-.-  #   |......|\"                                                            \n##  [13] \"        ##         ##           #   #   ----f---\"                                                            \n##  [14] \"         ####       #           #  ##       f@Y\"                                                             \n##  [15] \"            #       #           #  #\"                                                                        \n##  [16] \"       -----.-------#           #  #\"                                                                        \n##  [17] \"       |........%..|#           #  #\"                                                                        \n##  [18] \"       |............#           #  #\"                                                                        \n##  [19] \"       |...........|          0##  #\"                                                                        \n##  [20] \"       |...........|         -.--- #\"                                                                        \n##  [21] \"       -------------         |^..|##\"                                                                        \n##  [22] \"                             |...|#\"                                                                         \n##  [23] \"                             |0&gt;..#\"                                                                         \n##  [24] \"                             -----\"                                                                          \n##  [25] \"\"                                                                                                            \n##  [26] \"Brothertre the Trailblazer   St:15 Dx:12 Co:16 In:13 Wi:15 Ch:6  Neutral\"                                    \n##  [27] \"Dlvl:6  $:59 HP:0(54) Pw:40(40) AC:0  Exp:8 T:7398  Satiated Burdened\"                                       \n##  [28] \"\"                                                                                                            \n##  [29] \"Latest messages:\"                                                                                            \n##  [30] \" In what direction? l\"                                                                                       \n##  [31] \" You shoot 2 arrows.\"                                                                                        \n##  [32] \" The 1st arrow hits the ape.\"                                                                                \n##  [33] \" The 2nd arrow hits the ape!\"                                                                                \n##  [34] \" The ape hits!\"                                                                                              \n##  [35] \" The ape hits!\"                                                                                              \n##  [36] \" The ape bites!\"                                                                                             \n##  [37] \" You ready: q - 9 uncursed arrows.\"                                                                          \n##  [38] \" In what direction? l\"                                                                                       \n##  [39] \" The arrow hits the ape.\"                                                                                    \n##  [40] \" The ape hits!\"                                                                                              \n##  [41] \" The ape hits!\"                                                                                              \n##  [42] \" The ape bites!\"                                                                                             \n##  [43] \" The ape hits!\"                                                                                              \n##  [44] \" The ape hits!\"                                                                                              \n##  [45] \" The ape misses!\"                                                                                            \n##  [46] \" In what direction? l\"                                                                                       \n##  [47] \" You shoot 2 arrows.\"                                                                                        \n##  [48] \" The 1st arrow hits the ape!\"                                                                                \n##  [49] \" The 2nd arrow hits the ape.\"                                                                                \n##  [50] \" The ape misses!\"                                                                                            \n##  [51] \" The ape hits!\"                                                                                              \n##  [52] \" The ape misses!\"                                                                                            \n##  [53] \" In what direction? l\"                                                                                       \n##  [54] \" You shoot 2 arrows.\"                                                                                        \n##  [55] \" The 1st arrow misses the ape.\"                                                                              \n##  [56] \" The 2nd arrow hits the ape.\"                                                                                \n##  [57] \" The ape misses!\"                                                                                            \n##  [58] \" The ape hits!\"                                                                                              \n##  [59] \" The ape bites!\"                                                                                             \n##  [60] \" In what direction? l\"                                                                                       \n##  [61] \" The arrow hits the ape!\"                                                                                    \n##  [62] \" The ape hits!\"                                                                                              \n##  [63] \" The ape misses!\"                                                                                            \n##  [64] \" The ape bites!\"                                                                                             \n##  [65] \" You hear someone cursing shoplifters.\"                                                                      \n##  [66] \" The ape misses!\"                                                                                            \n##  [67] \" The ape hits!\"                                                                                              \n##  [68] \" The ape bites!\"                                                                                             \n##  [69] \" What do you want to write with? [- amnqsvBJM-OWZ or ?*] -\"                                                  \n##  [70] \" You write in the dust with your fingertip.\"                                                                 \n##  [71] \" What do you want to write in the dust here? Elbereth\"                                                       \n##  [72] \" The ape hits!\"                                                                                              \n##  [73] \" The ape hits!\"                                                                                              \n##  [74] \" You die...\"                                                                                                 \n##  [75] \" Do you want your possessions identified? [ynq] (y) y\"                                                       \n##  [76] \" Do you want to see your attributes? [ynq] (y) n\"                                                            \n##  [77] \" Do you want an account of creatures vanquished? [ynaq] (y) n\"                                               \n##  [78] \" Do you want to see your conduct? [ynq] (y) n\"                                                               \n##  [79] \" Do you want to see the dungeon overview? [ynq] (y) q\"                                                       \n##  [80] \"\"                                                                                                            \n##  [81] \"Inventory:\"                                                                                                  \n##  [82] \" Coins\"                                                                                                      \n##  [83] \"  $ - 59 gold pieces\"                                                                                        \n##  [84] \" Weapons\"                                                                                                    \n##  [85] \"  m - 17 blessed +1 arrows\"                                                                                  \n##  [86] \"  n - a blessed +0 arrow\"                                                                                    \n##  [87] \"  q - 3 +0 arrows (in quiver)\"                                                                               \n##  [88] \"  s - a +0 bow (weapon in hand)\"                                                                             \n##  [89] \"  B - 11 +1 darts\"                                                                                           \n##  [90] \"  N - 11 +0 darts\"                                                                                           \n##  [91] \"  a - a +1 dagger (alternate weapon; not wielded)\"                                                           \n##  [92] \" Armor\"                                                                                                      \n##  [93] \"  T - an uncursed +0 dwarvish iron helm (being worn)\"                                                        \n##  [94] \"  z - an uncursed +0 pair of leather gloves (being worn)\"                                                    \n##  [95] \"  U - a cursed -4 pair of iron shoes (being worn)\"                                                           \n##  [96] \"  e - an uncursed +2 cloak of displacement (being worn)\"                                                     \n##  [97] \"  h - a blessed +0 dwarvish mithril-coat (being worn)\"                                                       \n##  [98] \" Comestibles\"                                                                                                \n##  [99] \"  f - 3 uncursed cram rations\"                                                                               \n## [100] \"  j - 2 uncursed food rations\"                                                                               \n## [101] \"  L - an uncursed food ration\"                                                                               \n## [102] \"  P - an uncursed lembas wafer\"                                                                              \n## [103] \"  I - an uncursed lizard corpse\"                                                                             \n## [104] \"  o - an uncursed tin of spinach\"                                                                            \n## [105] \" Scrolls\"                                                                                                    \n## [106] \"  G - 2 uncursed scrolls of blank paper\"                                                                     \n## [107] \"  t - an uncursed scroll of confuse monster\"                                                                 \n## [108] \"  V - an uncursed scroll of identify\"                                                                        \n## [109] \" Potions\"                                                                                                    \n## [110] \"  x - an uncursed potion of gain ability\"                                                                    \n## [111] \"  H - a blessed potion of sleeping\"                                                                          \n## [112] \"  g - 3 uncursed potions of water\"                                                                           \n## [113] \" Rings\"                                                                                                      \n## [114] \"  O - an uncursed ring of slow digestion (on left hand)\"                                                     \n## [115] \"  v - an uncursed ring of stealth (on right hand)\"                                                           \n## [116] \" Tools\"                                                                                                      \n## [117] \"  p - an uncursed magic lamp\"                                                                                \n## [118] \"  k - an uncursed magic whistle\"                                                                             \n## [119] \"  Q - an uncursed mirror\"                                                                                    \n## [120] \"  C - an uncursed saddle\"                                                                                    \n## [121] \"  D - an uncursed stethoscope\"                                                                               \n## [122] \"  y - a +0 unicorn horn\"                                                                                     \n## [123] \"  i - 7 uncursed wax candles\"                                                                                \n## [124] \" Gems/Stones\"                                                                                                \n## [125] \"  W - an uncursed flint stone\"                                                                               \n## [126] \"  M - an uncursed worthless piece of red glass\"                                                              \n## [127] \"  Z - an uncursed worthless piece of violet glass\"                                                           \n## [128] \"  J - an uncursed worthless piece of white glass\"                                                            \n## [129] \"\"                                                                                                            \n## [130] \"Brothertrebius the Ranger's attributes:\"                                                                     \n## [131] \"\"                                                                                                            \n## [132] \"Background:\"                                                                                                 \n## [133] \" You were a Trailblazer, a level 8 female gnomish Ranger.\"                                                   \n## [134] \" You were neutral, on a mission for Venus\"                                                                   \n## [135] \" who was opposed by Mercury (lawful) and Mars (chaotic).\"                                                    \n## [136] \"\"                                                                                                            \n## [137] \"Final Characteristics:\"                                                                                      \n## [138] \" You had 0 hit points (max:54).\"                                                                             \n## [139] \" You had 40 magic power (max:40).\"                                                                           \n## [140] \" Your armor class was 0.\"                                                                                    \n## [141] \" You had 1552 experience points.\"                                                                            \n## [142] \" You entered the dungeon 7398 turns ago.\"                                                                    \n## [143] \" Your strength was 15 (limit:18/50).\"                                                                        \n## [144] \" Your dexterity was 12 (limit:18).\"                                                                          \n## [145] \" Your constitution was 16 (limit:18).\"                                                                       \n## [146] \" Your intelligence was 13 (limit:19).\"                                                                       \n## [147] \" Your wisdom was 15 (limit:18).\"                                                                             \n## [148] \" Your charisma was 6 (limit:18).\"                                                                            \n## [149] \"\"                                                                                                            \n## [150] \"Final Status:\"                                                                                               \n## [151] \" You were satiated.\"                                                                                         \n## [152] \" You were burdened; movement was slightly slowed.\"                                                           \n## [153] \" You were wielding a bow.\"                                                                                   \n## [154] \"\"                                                                                                            \n## [155] \"Final Attributes:\"                                                                                           \n## [156] \" You were piously aligned.\"                                                                                  \n## [157] \" You were telepathic.\"                                                                                       \n## [158] \" You had automatic searching.\"                                                                               \n## [159] \" You had infravision.\"                                                                                       \n## [160] \" You were displaced.\"                                                                                        \n## [161] \" You were stealthy.\"                                                                                         \n## [162] \" You had slower digestion.\"                                                                                  \n## [163] \" You were guarded.\"                                                                                          \n## [164] \" You are dead.\"                                                                                              \n## [165] \"\"                                                                                                            \n## [166] \"Vanquished creatures:\"                                                                                       \n## [167] \"  a warhorse\"                                                                                                \n## [168] \"  a tengu\"                                                                                                   \n## [169] \"  a quivering blob\"                                                                                          \n## [170] \" an iron piercer\"                                                                                            \n## [171] \"  2 black lights\"                                                                                            \n## [172] \"  a gold golem\"                                                                                              \n## [173] \"  a werewolf\"                                                                                                \n## [174] \"  3 lizards\"                                                                                                 \n## [175] \"  2 dingoes\"                                                                                                 \n## [176] \"  a housecat\"                                                                                                \n## [177] \"  a white unicorn\"                                                                                           \n## [178] \"  2 dust vortices\"                                                                                           \n## [179] \"  a plains centaur\"                                                                                          \n## [180] \" an ape\"                                                                                                     \n## [181] \"  a Woodland-elf\"                                                                                            \n## [182] \"  2 soldier ants\"                                                                                            \n## [183] \"  a bugbear\"                                                                                                 \n## [184] \" an imp\"                                                                                                     \n## [185] \"  a wood nymph\"                                                                                              \n## [186] \"  a water nymph\"                                                                                             \n## [187] \"  a rock piercer\"                                                                                            \n## [188] \"  a pony\"                                                                                                    \n## [189] \"  3 fog clouds\"                                                                                              \n## [190] \"  a yellow light\"                                                                                            \n## [191] \"  a violet fungus\"                                                                                           \n## [192] \"  2 gnome lords\"                                                                                             \n## [193] \"  2 gnomish wizards\"                                                                                         \n## [194] \"  2 gray oozes\"                                                                                              \n## [195] \"  2 elf zombies\"                                                                                             \n## [196] \"  a straw golem\"                                                                                             \n## [197] \"  a paper golem\"                                                                                             \n## [198] \"  2 giant ants\"                                                                                              \n## [199] \"  2 little dogs\"                                                                                             \n## [200] \"  3 floating eyes\"                                                                                           \n## [201] \"  8 dwarves\"                                                                                                 \n## [202] \"  a homunculus\"                                                                                              \n## [203] \"  3 kobold lords\"                                                                                            \n## [204] \"  3 kobold shamans\"                                                                                          \n## [205] \" 13 hill orcs\"                                                                                               \n## [206] \"  4 rothes\"                                                                                                  \n## [207] \"  2 centipedes\"                                                                                              \n## [208] \"  3 giant bats\"                                                                                              \n## [209] \"  6 dwarf zombies\"                                                                                           \n## [210] \"  a werejackal\"                                                                                              \n## [211] \"  3 iguanas\"                                                                                                 \n## [212] \" 23 killer bees\"                                                                                             \n## [213] \" an acid blob\"                                                                                               \n## [214] \"  a coyote\"                                                                                                  \n## [215] \"  3 gas spores\"                                                                                              \n## [216] \"  5 hobbits\"                                                                                                 \n## [217] \"  7 manes\"                                                                                                   \n## [218] \"  2 large kobolds\"                                                                                           \n## [219] \"  a hobgoblin\"                                                                                               \n## [220] \"  2 giant rats\"                                                                                              \n## [221] \"  2 cave spiders\"                                                                                            \n## [222] \"  a yellow mold\"                                                                                             \n## [223] \"  6 gnomes\"                                                                                                  \n## [224] \"  8 garter snakes\"                                                                                           \n## [225] \"  2 gnome zombies\"                                                                                           \n## [226] \"  8 geckos\"                                                                                                  \n## [227] \" 11 jackals\"                                                                                                 \n## [228] \"  5 foxes\"                                                                                                   \n## [229] \"  2 kobolds\"                                                                                                 \n## [230] \"  2 goblins\"                                                                                                 \n## [231] \"  a sewer rat\"                                                                                               \n## [232] \"  6 grid bugs\"                                                                                               \n## [233] \"  3 lichens\"                                                                                                 \n## [234] \"  2 kobold zombies\"                                                                                          \n## [235] \"  5 newts\"                                                                                                   \n## [236] \"206 creatures vanquished.\"                                                                                   \n## [237] \"\"                                                                                                            \n## [238] \"No species were genocided or became extinct.\"                                                                \n## [239] \"\"                                                                                                            \n## [240] \"Voluntary challenges:\"                                                                                       \n## [241] \" You never genocided any monsters.\"                                                                          \n## [242] \" You never polymorphed an object.\"                                                                           \n## [243] \" You never changed form.\"                                                                                    \n## [244] \" You used no wishes.\"                                                                                        \n## [245] \"\"                                                                                                            \n## [246] \"The Dungeons of Doom: levels 1 to 6\"                                                                         \n## [247] \"   Level 1:\"                                                                                                 \n## [248] \"      A fountain.\"                                                                                           \n## [249] \"   Level 2:\"                                                                                                 \n## [250] \"      A sink.\"                                                                                               \n## [251] \"   Level 3:\"                                                                                                 \n## [252] \"      A general store, a fountain.\"                                                                          \n## [253] \"   Level 4:\"                                                                                                 \n## [254] \"      A general store, a fountain.\"                                                                          \n## [255] \"      Stairs down to The Gnomish Mines.\"                                                                     \n## [256] \"   Level 5:\"                                                                                                 \n## [257] \"      A fountain.\"                                                                                           \n## [258] \"   Level 6: &lt;- You were here.\"                                                                               \n## [259] \"      A general store.\"                                                                                      \n## [260] \"      Final resting place for\"                                                                               \n## [261] \"         you, killed by an ape.\"                                                                             \n## [262] \"The Gnomish Mines: levels 5 to 8\"                                                                            \n## [263] \"   Level 5:\"                                                                                                 \n## [264] \"   Level 6:\"                                                                                                 \n## [265] \"   Level 7:\"                                                                                                 \n## [266] \"      Many shops, a temple, some fountains.\"                                                                 \n## [267] \"   Level 8:\"                                                                                                 \n## [268] \"\"                                                                                                            \n## [269] \"Game over:\"                                                                                                  \n## [270] \"                       ----------\"                                                                           \n## [271] \"                      /          \\\\\"                                                                         \n## [272] \"                     /    REST    \\\\\"                                                                        \n## [273] \"                    /      IN      \\\\\"                                                                       \n## [274] \"                   /     PEACE      \\\\\"                                                                      \n## [275] \"                  /                  \\\\\"                                                                     \n## [276] \"                  |  brothertrebius  |\"                                                                      \n## [277] \"                  |      59 Au       |\"                                                                      \n## [278] \"                  | killed by an ape |\"                                                                      \n## [279] \"                  |                  |\"                                                                      \n## [280] \"                  |                  |\"                                                                      \n## [281] \"                  |                  |\"                                                                      \n## [282] \"                  |       2018       |\"                                                                      \n## [283] \"                 *|     *  *  *      | *\"                                                                    \n## [284] \"        _________)/\\\\\\\\_//(\\\\/(/\\\\)/\\\\//\\\\/|_)_______\"                                                       \n## [285] \"\"                                                                                                            \n## [286] \"Goodbye brothertrebius the Ranger...\"                                                                        \n## [287] \"\"                                                                                                            \n## [288] \"You died in The Dungeons of Doom on dungeon level 6 with 6652 points,\"                                       \n## [289] \"and 59 pieces of gold, after 7398 moves.\"                                                                    \n## [290] \"You were level 8 with a maximum of 54 hit points when you died.\"                                             \n## [291] \"\"\n\n\nNow, I am curious to see how many games are played per day:\n\nruns_per_day &lt;- nethack %&gt;%\n  group_by(date) %&gt;%\n  count() %&gt;%\n  ungroup() \n\n\nggplot(runs_per_day, aes(y = n, x = date)) + \n  geom_point(colour = \"#0f4150\") + \n  geom_smooth(colour = \"#82518c\") + \n  theme_blog()\n## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\nThe number of games seems to be stable since 2015, around 50. But what is also interesting is not only the number of games played, but also how many of these games resulted in a win.\n\n\nFor this, let’s also add a new column that tells us whether the played ascended (won the game) or not:\n\nnethack %&lt;&gt;%\n  mutate(Ascended = ifelse(death == \"ascended\", \"Ascended\", \"Died an horrible death\"))\n\nI’m curious to see how many players managed to ascend… NetHack being as hard as diamonds, probably not a lot:\n\nascensions_per_day &lt;- nethack %&gt;%\n  group_by(date, Ascended) %&gt;%\n  count() %&gt;%\n  rename(Total = n)\n\nggplot(ascensions_per_day) + \n  geom_area(aes(y = Total, x = as.Date(date), fill = Ascended)) +\n  theme_blog() +\n  labs(y = \"Number of runs\", x = \"Date\") +\n  scale_fill_blog() +\n  theme(legend.title = element_blank())\n\n\n\n\nYeah, just as expected. Because there is so much data, it’s difficult to see clearly, though. Depending on the size of the screen you’re reading this, it might seem that in some days there are a lot of ascensions. This is only an impression due to the resolution of the picture. Let’s see the share of ascensions per year (and how many times the quests fail miserably), and this will become more apparent:\n\nascensions_per_day %&gt;%\n  mutate(Year = year(as.Date(date))) %&gt;%\n  group_by(Year, Ascended) %&gt;%\n  summarise(Total = sum(Total, na.rm = TRUE)) %&gt;%\n  group_by(Year) %&gt;%\n  mutate(denom = sum(Total, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Share = Total/denom) %&gt;%\n  ggplot() + \n  geom_col(aes(y = Share, x = Year, fill = Ascended)) + \n  theme_blog() + \n  scale_fill_blog() + \n  theme(legend.title = element_blank())\n\n\n\n\nI will now convert the “time” column to seconds. I am not yet sure that this column is really useful, because NetHack is a turn based game. This means that when the player does not move, neither do the monsters. So the seconds spent playing might not be a good proxy for actual time spent playing. But it makes for a good exercise:\n\nconvert_to_seconds &lt;- function(time_string){\n    time_numeric &lt;- time_string %&gt;%\n        str_split(\":\", simplify = TRUE) %&gt;%\n        as.numeric\n\n    time_in_seconds &lt;- sum(time_numeric * c(3600, 60, 1))\n\n    time_in_seconds \n}\n\nThe strings I want to convert are of the form “01:34:43”, so I split at the “:” and then convert the result to numeric. I end up with an atomic vector (c(1, 34, 43)). Then I multiple each element by the right number of seconds, and sum that to get the total. Let’s apply it to my data:\n\nnethack %&lt;&gt;%\n  mutate(time_in_seconds = map_dbl(time, convert_to_seconds))\n\nWhat is the distribution of “time_in_seconds”?\n\nnethack %&gt;%\n  describe(time_in_seconds)\n## # A tibble: 1 x 15\n##   variable type    nobs   mean     sd mode    min    max   q05   q25 median\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n## 1 time_in… Nume… 322485 23529. 2.73e5 &lt;NA&gt;     61 2.72e7   141   622   1689\n## # … with 4 more variables: q75 &lt;dbl&gt;, q95 &lt;dbl&gt;, n_missing &lt;int&gt;,\n## #   n_unique &lt;lgl&gt;\n\nWe see that the minimum of time_in_seconds is 61 whereas the maximum is of the order of 27200000… This must be a mistake, because that is almost one year!\n\nnethack %&gt;%\n  filter(time_in_seconds == max(time_in_seconds, na.rm = TRUE))\n##   rank      score   name       time   turns lev_max  hp_max role race\n## 1   28 3173960108 fisted 7553:41:49 6860357    4/47 362/362  Wiz  Elf\n##   gender alignment                      death       date dumplog year\n## 1    Mal       Neu drowned in a pool of water 2017-02-02      NA 2017\n##   month day               Ascended time_in_seconds\n## 1     2   2 Died an horrible death        27193309\n\nWell… maybe “fisted” wanted to break the record of the longest NetHack game ever. Congratulations!\n\n\nLet’s take a look at the density but cut it at 90th percentile:\n\nnethack %&gt;%\n  filter(!is.na(time_in_seconds),\n         time_in_seconds &lt; quantile(time_in_seconds, 0.9, na.rm = TRUE)) %&gt;%\n  ggplot() + \n  geom_density(aes(x = time_in_seconds), colour = \"#82518c\") + \n  theme_blog()\n\n\n\n\nAs expected, the distribution is right skewed. However, as explained above NetHack is a turn based game, meaning that if the player does not move, the monsters won’t move either. Perhaps it makes more sense to look at the turns column:\n\nnethack %&gt;%\n  describe(turns)\n## # A tibble: 1 x 15\n##   variable type    nobs  mean     sd mode    min    max   q05   q25 median\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n## 1 turns    Nume… 322485 4495. 19853. &lt;NA&gt;      1 6.86e6   202   871   1818\n## # … with 4 more variables: q75 &lt;dbl&gt;, q95 &lt;dbl&gt;, n_missing &lt;int&gt;,\n## #   n_unique &lt;lgl&gt;\n\nThe maximum is quite large too. Just like before, let’s focus by cutting the variable at the 90th percentile:\n\nnethack %&gt;%\n  filter(!is.na(turns),\n         turns &lt; quantile(turns, 0.9, na.rm = TRUE)) %&gt;% \n  ggplot() + \n  geom_density(aes(x = turns), colour = \"#82518c\") + \n  theme_blog()\n\n\n\n\nI think that using turns makes more sense. In the a future blog post, I will estimate a survival model and see how long players survive, and will use turns instead of time_in_seconds."
  },
  {
    "objectID": "posts/2018-11-03-nethack_analysis.html#analysis",
    "href": "posts/2018-11-03-nethack_analysis.html#analysis",
    "title": "Analyzing NetHack data, part 1: What kills the players",
    "section": "\nAnalysis\n",
    "text": "Analysis\n\n\n\nWhat kills the players\n\n\nTo know what kills players so much, some cleaning of the death column is in order. Death can occur from poisoning, starvation, accidents, drowning… of course monsters can kill the player too. Here are some values of the death variable:\n\nburned by a tower of flame\nchoked on a lichen corpse\ndied of starvation\nfell into a pit of iron spikes\nkilled by a gnome\nkilled by a gnome called Blabla\nkilled by a gnome called Blabla while sleeping\nslipped while mounting a saddled pony\nslipped while mounting a saddled pony called Jolly Jumper\nzapped her/himself with a spell\n\nTo know what is the most frequent cause of death, I have to do some cleaning, because if not, “killed by a gnome” and “killed by a gnome called Blabla” would be two different causes of death. In the end, what interests me is to know how many times the player got killed by a gnome.\n\n\nThe following lines do a cleanup of the death variable:\n\nnethack %&lt;&gt;% \n  mutate(death2 = case_when(str_detect(death, \"poisoned\") ~ \"poisoned\",\n                            str_detect(death, \"slipped\") ~ \"accident\",\n                            str_detect(death, \"petrified\") ~ \"petrified\",\n                            str_detect(death, \"choked\") ~ \"accident\",\n                            str_detect(death, \"caught.*self\") ~ \"accident\",\n                            str_detect(death, \"starvation\") ~ \"starvation\",\n                            str_detect(death, \"drowned\") ~ \"drowned\",\n                            str_detect(death, \"fell\") ~ \"fell\",\n                            str_detect(death, \"zapped\") ~ \"zapped\",\n                            str_detect(death, \"killed\") ~ \"killed\",\n                            TRUE ~ death)) %&gt;%\n  mutate(death3 = str_extract(death, \"(?&lt;=by|while).*\")) %&gt;%\n  mutate(death3 = case_when(str_detect(death3, \",|\\\\bcalled\\\\b\") ~ str_extract(death3, \"(.*?),|(.*?)\\\\bcalled\\\\b\"), \n                            TRUE ~ death3)) %&gt;%\n  mutate(death3 = str_remove(death3, \",|called|\\\\ban?\"),\n         death3 = str_trim(death3))\n\ndeath2 is a new variable, in which I broadly categorize causes of death. Using regular expressions I detect causes of death and aggregate some categories, for instance “slipped” and “chocked” into “accident”. Then, I want to extract everything that comes after the strings “by” or while, and put the result into a new variable called death3. Then I detect the string “,” or “called”; if one of these strings is present, I extract everything that comes before “,” or that comes before “called”. Finally, I remove “,”, “called” or “a” or “an” from the string and trim the whitespaces.\n\n\nLet’s take a look at these new variables:\n\nset.seed(123)\nnethack %&gt;%\n    select(name, death, death2, death3) %&gt;%\n    sample_n(10)\n##              name                     death   death2        death3\n## 92740   DianaFury     killed by a death ray   killed     death ray\n## 254216    Oddabit         killed by a tiger   killed         tiger\n## 131889    shachaf      killed by a fire ant   killed      fire ant\n## 284758        a43  poisoned by a killer bee poisoned    killer bee\n## 303283      goast         killed by a gecko   killed         gecko\n## 14692     liberty    killed by a gnome king   killed    gnome king\n## 170303     arch18                  ascended ascended          &lt;NA&gt;\n## 287786 foolishwtf           killed by a bat   killed           bat\n## 177826    Renleve     killed by a giant bat   killed     giant bat\n## 147248      TheOV killed by a black unicorn   killed black unicorn\n\nNow, it is quite easy to know what monsters are the meanest buttholes; let’s focus on the top 15. Most likely, these are going to be early game monsters. Let’ see:\n\nnethack %&gt;%\n    filter(!is.na(death3)) %&gt;%\n    count(death3) %&gt;%\n    top_n(15) %&gt;%\n    mutate(death3 = fct_reorder(death3, n, .desc = FALSE)) %&gt;%\n    ggplot() + \n    geom_col(aes(y = n, x = death3)) + \n    coord_flip() + \n    theme_blog() + \n    scale_fill_blog() + \n    ylab(\"Number of deaths caused\") +\n    xlab(\"Monster\")\n## Selecting by n\n\n\n\n\nSeems like soldier ants are the baddest, followed by jackals and dwarfs. As expected, these are mostly early game monsters. Thus, it would be interesting to look at this distribution, but at different stages in the game. Let’s create a categorical variable that discretizes turns, and then create one plot per category:\n\n\n\n\nClick to expand\n\n\nnethack %&gt;%\n    filter(!is.na(death3)) %&gt;%\n    filter(!is.na(turns)) %&gt;%\n    mutate(turn_flag = case_when(between(turns, 1, 5000) ~ \"Less than 5000\",\n                                 between(turns, 5001, 10000) ~ \"Between 5001 and 10000\",\n                                 between(turns, 10001, 20000) ~ \"Between 10001 and 20000\",\n                                 between(turns, 20001, 40000) ~ \"Between 20001 and 40000\",\n                                 between(turns, 40001, 60000) ~ \"Between 40001 and 60000\",\n                                 turns &gt; 60000 ~ \"More than 60000\")) %&gt;%\n    mutate(turn_flag = factor(turn_flag, levels = c(\"Less than 5000\", \n                                                    \"Between 5001 and 10000\",\n                                                    \"Between 10001 and 20000\",\n                                                    \"Between 20001 and 40000\",\n                                                    \"Between 40001 and 60000\",\n                                                    \"More than 60000\"), ordered = TRUE)) %&gt;%\n    group_by(turn_flag) %&gt;%\n    count(death3) %&gt;%\n    top_n(15) %&gt;%\n    nest() %&gt;%\n    mutate(data = map(data, ~mutate(., death3 = fct_reorder(death3, n, .desc = TRUE))))  %&gt;%\n    mutate(plots = map2(.x = turn_flag,\n                         .y = data,\n                         ~ggplot(data = .y) + \n                             geom_col(aes(y = n, x = death3)) + \n                             coord_flip() + \n                             theme_blog() + \n                             scale_fill_blog() + \n                             ylab(\"Number of deaths caused\") +\n                             xlab(\"Monster\") + \n                             ggtitle(.x))) %&gt;%\n    pull(plots)\n## Selecting by n\n## [[1]]\n\n\n\n## \n## [[2]]\n\n\n\n## \n## [[3]]\n\n\n\n## \n## [[4]]\n\n\n\n## \n## [[5]]\n\n\n\n## \n## [[6]]\n\n\n\nFinally, for this section, I want to know if there are levels, or floors, where players die more often than others. For this, we can take a look at the lev_max column. Observations in this column are of the form “8/10”. This means that the player died on level 8, but the lowest level that was explored was the 10th. Let’s do this for the year 2017 first. Before anything, I have to explain the layout of the levels of the game. You can see a diagram here. The player starts on floor 1, and goes down to level 53. Then, the player can ascend, by going on levels -1 to -5. But there are more levels than these ones. -6 and -9 are the sky, and the player can teleport there (but will fall to his death). If the player teleports to level -10, he’ll enter heaven (and die too). Because these levels are special, I do not consider them here. I do not consider level 0 either, which is “Nowhere”. Let’s get the number of players who died on each floor, but also compute the cumulative death count:\n\ndied_on_level &lt;- nethack %&gt;%\n    filter(Ascended == \"Died an horrible death\") %&gt;%\n    mutate(died_on = str_extract(lev_max, \"-?\\\\d{1,}\")) %&gt;%\n    mutate(died_on = as.numeric(died_on)) %&gt;%\n    group_by(year) %&gt;%\n    count(died_on) %&gt;% \n    filter(died_on &gt;= -5, died_on != 0) %&gt;%\n    mutate(died_on = case_when(died_on == -1 ~ 54,\n                               died_on == -2 ~ 55,\n                               died_on == -3 ~ 56,\n                               died_on == -4 ~ 57,\n                               died_on == -5 ~ 58,\n                               TRUE ~ died_on)) %&gt;%\n    arrange(desc(died_on)) %&gt;%\n    mutate(cumul_deaths = cumsum(n))\n\nLet’s take a look:\n\nhead(died_on_level)\n## # A tibble: 6 x 4\n## # Groups:   year [6]\n##    year died_on     n cumul_deaths\n##   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;        &lt;int&gt;\n## 1  2002      58     5            5\n## 2  2003      58    11           11\n## 3  2004      58    19           19\n## 4  2005      58    28           28\n## 5  2006      58    25           25\n## 6  2007      58    22           22\n\nNow, let’s compute the number of players who ascended and add this to the cumulative count:\n\nascended_yearly &lt;- nethack %&gt;%\n    filter(Ascended == \"Ascended\") %&gt;%\n    group_by(year) %&gt;%\n    count(Ascended)\n\nLet’s take a look:\n\nhead(ascended_yearly)\n## # A tibble: 6 x 3\n## # Groups:   year [6]\n##    year Ascended     n\n##   &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;\n## 1  2001 Ascended     4\n## 2  2002 Ascended    38\n## 3  2003 Ascended   132\n## 4  2004 Ascended   343\n## 5  2005 Ascended   329\n## 6  2006 Ascended   459\n\nI will modify the dataset a little bit and merge it with the previous one:\n\nascended_yearly %&lt;&gt;%\n  rename(ascended_players = `n`) %&gt;%\n  select(-Ascended)\n\nLet’s add this to the data frame from before by merging both, and then we can compute the surviving players:\n\ndied_on_level %&lt;&gt;%\n  full_join(ascended_yearly, by = \"year\") %&gt;%\n  mutate(surviving_players = cumul_deaths + ascended_players)\n\nNow we can compute the share of players who died on each level:\n\ndied_on_level %&gt;%\n    mutate(death_rate = n/surviving_players) %&gt;% \n    ggplot(aes(y = death_rate, x = as.factor(died_on))) + \n    geom_line(aes(group = year, alpha = year), colour = \"#82518c\") +\n    theme_blog() + \n    ylab(\"Death rate\") +\n    xlab(\"Level\") + \n    theme(axis.text.x = element_text(angle = 90),\n          legend.position = \"none\") + \n    scale_y_continuous(labels = scales::percent)\n\n\n\n\nLooks like level 7 is consistently the most dangerous! The death rate there is more than 35%!\n\n\nThat’s it for this blog post, in the next one, I will focus on what players kill!"
  },
  {
    "objectID": "posts/2018-12-02-hyper-parameters.html",
    "href": "posts/2018-12-02-hyper-parameters.html",
    "title": "What hyper-parameters are, and what to do with them; an illustration with ridge regression",
    "section": "",
    "text": "This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free here. This is taken from Chapter 7, which deals with statistical models. In the text below, I explain what hyper-parameters are, and as an example I run a ridge regression using the {glmnet} package. The book is still being written, so comments are more than welcome!"
  },
  {
    "objectID": "posts/2018-12-02-hyper-parameters.html#hyper-parameters",
    "href": "posts/2018-12-02-hyper-parameters.html#hyper-parameters",
    "title": "What hyper-parameters are, and what to do with them; an illustration with ridge regression",
    "section": "\nHyper-parameters\n",
    "text": "Hyper-parameters\n\n\nHyper-parameters are parameters of the model that cannot be directly learned from the data. A linear regression does not have any hyper-parameters, but a random forest for instance has several. You might have heard of ridge regression, lasso and elasticnet. These are extensions to linear models that avoid over-fitting by penalizing large models. These extensions of the linear regression have hyper-parameters that the practitioner has to tune. There are several ways one can tune these parameters, for example, by doing a grid-search, or a random search over the grid or using more elaborate methods. To introduce hyper-parameters, let’s get to know ridge regression, also called Tikhonov regularization.\n\n\n\nRidge regression\n\n\nRidge regression is used when the data you are working with has a lot of explanatory variables, or when there is a risk that a simple linear regression might overfit to the training data, because, for example, your explanatory variables are collinear. If you are training a linear model and then you notice that it generalizes very badly to new, unseen data, it is very likely that the linear model you trained overfits the data. In this case, ridge regression might prove useful. The way ridge regression works might seem counter-intuititive; it boils down to fitting a worse model to the training data, but in return, this worse model will generalize better to new data.\n\n\nThe closed form solution of the ordinary least squares estimator is defined as:\n\n\\[\\widehat{\\beta} = (X'X)^{-1}X'Y\\]\n\nwhere \\(X\\) is the design matrix (the matrix made up of the explanatory variables) and \\(Y\\) is the dependent variable. For ridge regression, this closed form solution changes a little bit:\n\n\\[\\widehat{\\beta} = (X'X + \\lambda I_p)^{-1}X'Y\\]\n\nwhere \\(lambda \\in \\mathbb{R}\\) is an hyper-parameter and \\(I_p\\) is the identity matrix of dimension \\(p\\) (\\(p\\) is the number of explanatory variables). This formula above is the closed form solution to the following optimisation program:\n\n\\[ \\sum_{i=1}^n \\left(y_i - \\sum_{j=1}^px_{ij}\\beta_j\\right)^2 \\]\n\nsuch that:\n\n\\[ \\sum_{j=1}^p(\\beta_j)^2 &lt; c\\]\n\nfor any strictly positive (c).\n\n\nThe glmnet() function from the {glmnet} package can be used for ridge regression, by setting the alpha argument to 0 (setting it to 1 would do LASSO, and setting it to a number between 0 and 1 would do elasticnet). But in order to compare linear regression and ridge regression, let me first divide the data into a training set and a testing set. I will be using the Housing data from the {Ecdat} package:\n\nlibrary(tidyverse)\nlibrary(Ecdat)\nlibrary(glmnet)\nindex &lt;- 1:nrow(Housing)\n\nset.seed(12345)\ntrain_index &lt;- sample(index, round(0.90*nrow(Housing)), replace = FALSE)\n\ntest_index &lt;- setdiff(index, train_index)\n\ntrain_x &lt;- Housing[train_index, ] %&gt;% \n    select(-price)\n\ntrain_y &lt;- Housing[train_index, ] %&gt;% \n    pull(price)\n\ntest_x &lt;- Housing[test_index, ] %&gt;% \n    select(-price)\n\ntest_y &lt;- Housing[test_index, ] %&gt;% \n    pull(price)\n\nI do the train/test split this way, because glmnet() requires a design matrix as input, and not a formula. Design matrices can be created using the model.matrix() function:\n\ntrain_matrix &lt;- model.matrix(train_y ~ ., data = train_x)\n\ntest_matrix &lt;- model.matrix(test_y ~ ., data = test_x)\n\nTo run an unpenalized linear regression, we can set the penalty to 0:\n\nmodel_lm_ridge &lt;- glmnet(y = train_y, x = train_matrix, alpha = 0, lambda = 0)\n\nThe model above provides the same result as a linear regression. Let’s compare the coefficients between the two:\n\ncoef(model_lm_ridge)\n## 13 x 1 sparse Matrix of class \"dgCMatrix\"\n##                       s0\n## (Intercept) -3247.030393\n## (Intercept)     .       \n## lotsize         3.520283\n## bedrooms     1745.211187\n## bathrms     14337.551325\n## stories      6736.679470\n## drivewayyes  5687.132236\n## recroomyes   5701.831289\n## fullbaseyes  5708.978557\n## gashwyes    12508.524241\n## aircoyes    12592.435621\n## garagepl     4438.918373\n## prefareayes  9085.172469\n\nand now the coefficients of the linear regression (because I provide a design matrix, I have to use lm.fit() instead of lm() which requires a formula, not a matrix.)\n\ncoef(lm.fit(x = train_matrix, y = train_y))\n##  (Intercept)      lotsize     bedrooms      bathrms      stories \n## -3245.146665     3.520357  1744.983863 14336.336858  6737.000410 \n##  drivewayyes   recroomyes  fullbaseyes     gashwyes     aircoyes \n##  5686.394123  5700.210775  5709.493884 12509.005265 12592.367268 \n##     garagepl  prefareayes \n##  4439.029607  9085.409155\n\nas you can see, the coefficients are the same. Let’s compute the RMSE for the unpenalized linear regression:\n\npreds_lm &lt;- predict(model_lm_ridge, test_matrix)\n\nrmse_lm &lt;- sqrt(mean((preds_lm - test_y)^2))\n\nThe RMSE for the linear unpenalized regression is equal to 14463.08.\n\n\nLet’s now run a ridge regression, with lambda equal to 100, and see if the RMSE is smaller:\n\nmodel_ridge &lt;- glmnet(y = train_y, x = train_matrix, alpha = 0, lambda = 100)\n\nand let’s compute the RMSE again:\n\npreds &lt;- predict(model_ridge, test_matrix)\n\nrmse &lt;- sqrt(mean((preds - test_y)^2))\n\nThe RMSE for the linear penalized regression is equal to 14460.71, which is smaller than before. But which value of lambda gives smallest RMSE? To find out, one must run model over a grid of lambda values and pick the model with lowest RMSE. This procedure is available in the cv.glmnet() function, which picks the best value for lambda:\n\nbest_model &lt;- cv.glmnet(train_matrix, train_y)\n# lambda that minimises the MSE\nbest_model$lambda.min\n## [1] 66.07936\n\nAccording to cv.glmnet() the best value for lambda is 66.0793576. In the next section, we will implement cross validation ourselves, in order to find the hyper-parameters of a random forest."
  },
  {
    "objectID": "posts/2019-04-28-diffindiff_part1.html",
    "href": "posts/2019-04-28-diffindiff_part1.html",
    "title": "Fast food, causality and R packages, part 1",
    "section": "",
    "text": "I am currently working on a package for the R programming language; its initial goal was to simply distribute the data used in the Card and Krueger 1994 paper that you can read here (PDF warning).\n\n\nThe gist of the paper is to try to answer the following question: Do increases in minimum wages reduce employment? According to Card and Krueger’s paper from 1994, no. The authors studied a change in legislation in New Jersey which increased the minimum wage from $4.25 an hour to $5.05 an hour. The neighbourghing state of Pennsylvania did not introduce such an increase. The authors thus used the State of Pennsylvania as a control for the State of New Jersey and studied how the increase in minimum wage impacted the employment in fast food restaurants and found, against what economic theory predicted, an increase and not a decrease in employment. The authors used a method called difference-in-differences to asses the impact of the minimum wage increase.\n\n\nThis result was and still is controversial, with subsequent studies finding subtler results. For instance, showing that there is a reduction in employment following an increase in minimum wage, but only for large restaurants (see Ropponen and Olli, 2011).\n\n\nAnyways, this blog post will discuss how to create a package using to distribute the data. In a future blog post, I will discuss preparing the data to make it available as a demo dataset inside the package, and then writing and documenting functions.\n\n\nThe first step to create a package, is to create a new project:\n\n\n\n\n\nSelect “New Directory”:\n\n\n\n\n\nThen “R package”:\n\n\n\n\n\nand on the window that appears, you can choose the name of the package, as well as already some starting source files:\n\n\n\n\n\nAlso, I’d highly recommend you click on the “Create a git repository” box and use git within your project for reproducibility and sharing your code more easily. If you do not know git, there’s a lot of online resources to get you started. It’s not super difficult, but it does require making some new habits, which can take some time.\n\n\nI called my package {diffindiff}, and clicked on “Create Project”. This opens up a new project with a hello.R script, which gives you some pointers:\n\n# Hello, world!\n#\n# This is an example function named 'hello' \n# which prints 'Hello, world!'.\n#\n# You can learn more about package authoring with RStudio at:\n#\n#   http://r-pkgs.had.co.nz/\n#\n# Some useful keyboard shortcuts for package authoring:\n#\n#   Install Package:           'Ctrl + Shift + B'\n#   Check Package:             'Ctrl + Shift + E'\n#   Test Package:              'Ctrl + Shift + T'\n\nhello &lt;- function() {\n  print(\"Hello, world!\")\n}\n\nNow, to simplify the creation of your package, I highly recommend you use the {usethis} package. {usethis} removes a lot of the pain involved in creating packages.\n\n\nFor instance, want to start by adding a README file? Simply run:\n\nusethis::use_readme_md()\n✔ Setting active project to '/path/to/your/package/diffindiff'\n✔ Writing 'README.md'\n● Modify 'README.md'\n\nThis creates a README.md file in the root directory of your package. Simply change that file, and that’s it.\n\n\nThe next step could be setting up your package to work with {roxygen2}, which is very useful for writing documentation:\n\nusethis::use_roxygen_md()\n✔ Setting Roxygen field in DESCRIPTION to 'list(markdown = TRUE)'\n✔ Setting RoxygenNote field in DESCRIPTION to '6.1.1'\n● Run `devtools::document()`\n\nSee how the output tells you to run devtools::document()? This function will document your package, transforming the comments you write to describe your functions to documentation and managing the NAMESPACE file. Let’s run this function too:\n\ndevtools::document()\nUpdating diffindiff documentation\nFirst time using roxygen2. Upgrading automatically...\nLoading diffindiff\nWarning: The existing 'NAMESPACE' file was not generated by roxygen2, and will not be overwritten.\n\nYou might have a similar message than me, telling you that the NAMESPACE file was not generated by {roxygen2}, and will thus not be overwritten. Simply remove the file and run devtools::document() again:\n\ndevtools::document()\nUpdating diffindiff documentation\nFirst time using roxygen2. Upgrading automatically...\nWriting NAMESPACE\nLoading diffindiff\n\nBut what is actually the NAMESPACE file? This file is quite important, as it details where your package’s functions have to look for in order to use other functions. This means that if your package needs function foo() from package {bar}, it will consistently look for foo() inside {bar} and not confuse it with, say, the foo() function from the {barley} package, even if you load {barley} after {bar} in your interactive session. This can seem confusing now, but in the next blog posts I will detail this, and you will see that it’s not that difficult. Just know that it is an important file, and that you do not have to edit it by hand.\n\n\nNext, I like to run the following:\n\nusethis::use_pipe()\n✔ Adding 'magrittr' to Imports field in DESCRIPTION\n✔ Writing 'R/utils-pipe.R'\n● Run `devtools::document()`\n\nThis makes the now famous %&gt;% function available internally to your package (so you can use it to write the functions that will be included in your package) but also available to the users that will load the package.\n\n\nYour package is still missing a license. If you plan on writing a package for your own personal use, for instance, a collection of functions, there is no need to think about licenses. But if you’re making your package available through CRAN, then you definitely need to think about it. For this package, I’ll be using the MIT license, because the package will distribute data which I do not own (I’ve got permission from Card to re-distribute it) and thus I think it would be better to use a permissive license (I don’t know if the GPL, another license, which is stricter in terms of redistribution, could be used in this case).\n\nusethis::use_mit_license()\n✔ Setting License field in DESCRIPTION to 'MIT + file LICENSE'\n✔ Writing 'LICENSE.md'\n✔ Adding '^LICENSE\\\\.md$' to '.Rbuildignore'\n✔ Writing 'LICENSE'\n\nWe’re almost done setting up the structure of the package. If we forget something though, it’s not an issue, we’ll just have to run the right use_* function later on. Let’s finish by preparing the folder that will contains the script to prepare the data:\n\nusethis::use_data_raw()\n✔ Creating 'data-raw/'\n✔ Adding '^data-raw$' to '.Rbuildignore'\n✔ Writing 'data-raw/DATASET.R'\n● Modify 'data-raw/DATASET.R'\n● Finish the data preparation script in 'data-raw/DATASET.R'\n● Use `usethis::use_data()` to add prepared data to package\n\nThis creates the data-raw folder with the DATASET.R script inside. This is the script that will contain the code to download and prepare datasets that you want to include in your package. This will be the subject of the next blog post.\n\n\nLet’s now finish by documenting the package, and pushing everything to Github:\n\ndevtools::document()\n\nThe following lines will only work if you set up the Github repo:\n\ngit add .\ngit commit -am \"first commit\"\ngit push origin master"
  },
  {
    "objectID": "posts/2017-07-27-spread_rename_at.html",
    "href": "posts/2017-07-27-spread_rename_at.html",
    "title": "tidyr::spread() and dplyr::rename_at() in action",
    "section": "",
    "text": "I was recently confronted to a situation that required going from a long dataset to a wide dataset, but with a small twist: there were two datasets, which I had to merge into one. You might wonder what kinda crappy twist that is, right? Well, let’s take a look at the data:\n\ndata1; data2\n## # A tibble: 20 x 4\n##    country date       variable_1       value\n##    &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;            &lt;int&gt;\n##  1 lu      01/01/2005 maybe               22\n##  2 lu      01/07/2005 maybe               13\n##  3 lu      01/01/2006 maybe               40\n##  4 lu      01/07/2006 maybe               25\n##  5 lu      01/01/2005 totally_agree       42\n##  6 lu      01/07/2005 totally_agree       17\n##  7 lu      01/01/2006 totally_agree       25\n##  8 lu      01/07/2006 totally_agree       16\n##  9 lu      01/01/2005 totally_disagree    39\n## 10 lu      01/07/2005 totally_disagree    17\n## 11 lu      01/01/2006 totally_disagree    23\n## 12 lu      01/07/2006 totally_disagree    21\n## 13 lu      01/01/2005 kinda_disagree      69\n## 14 lu      01/07/2005 kinda_disagree      12\n## 15 lu      01/01/2006 kinda_disagree      10\n## 16 lu      01/07/2006 kinda_disagree       9\n## 17 lu      01/01/2005 kinda_agree         38\n## 18 lu      01/07/2005 kinda_agree         31\n## 19 lu      01/01/2006 kinda_agree         19\n## 20 lu      01/07/2006 kinda_agree         12\n## # A tibble: 20 x 4\n##    country date       variable_2       value\n##    &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;            &lt;int&gt;\n##  1 lu      01/01/2005 kinda_agree         22\n##  2 lu      01/07/2005 kinda_agree         13\n##  3 lu      01/01/2006 kinda_agree         40\n##  4 lu      01/07/2006 kinda_agree         25\n##  5 lu      01/01/2005 totally_agree       42\n##  6 lu      01/07/2005 totally_agree       17\n##  7 lu      01/01/2006 totally_agree       25\n##  8 lu      01/07/2006 totally_agree       16\n##  9 lu      01/01/2005 totally_disagree    39\n## 10 lu      01/07/2005 totally_disagree    17\n## 11 lu      01/01/2006 totally_disagree    23\n## 12 lu      01/07/2006 totally_disagree    21\n## 13 lu      01/01/2005 maybe               69\n## 14 lu      01/07/2005 maybe               12\n## 15 lu      01/01/2006 maybe               10\n## 16 lu      01/07/2006 maybe                9\n## 17 lu      01/01/2005 kinda_disagree      38\n## 18 lu      01/07/2005 kinda_disagree      31\n## 19 lu      01/01/2006 kinda_disagree      19\n## 20 lu      01/07/2006 kinda_disagree      12\n\nAs explained in Hadley (2014), this is how you should keep your data… But for a particular purpose, I had to transform these datasets. What I was asked to do was to merge these into a single wide data frame. Doing this for one dataset is easy:\n\ndata1 %&gt;%\n  spread(variable_1, value)\n## # A tibble: 4 x 7\n##   country date       kinda_agree kinda_disagree maybe totally_agree\n##   &lt;chr&gt;   &lt;chr&gt;            &lt;int&gt;          &lt;int&gt; &lt;int&gt;         &lt;int&gt;\n## 1 lu      01/01/2005          38             69    22            42\n## 2 lu      01/01/2006          19             10    40            25\n## 3 lu      01/07/2005          31             12    13            17\n## 4 lu      01/07/2006          12              9    25            16\n## # ... with 1 more variable: totally_disagree &lt;int&gt;\n\nBut because data1 and data2 have the same levels for variable_1 and variable_2, this would not work. So the solution I found online, in this SO thread was to use tidyr::spread() with dplyr::rename_at() like this:\n\ndata1 &lt;- data1 %&gt;%\n  spread(variable_1, value) %&gt;%\n  rename_at(vars(-country, -date), funs(paste0(\"variable1:\", .)))\n\nglimpse(data1)\n## Observations: 4\n## Variables: 7\n## $ country                      &lt;chr&gt; \"lu\", \"lu\", \"lu\", \"lu\"\n## $ date                         &lt;chr&gt; \"01/01/2005\", \"01/01/2006\", \"01/0...\n## $ `variable1:kinda_agree`      &lt;int&gt; 38, 19, 31, 12\n## $ `variable1:kinda_disagree`   &lt;int&gt; 69, 10, 12, 9\n## $ `variable1:maybe`            &lt;int&gt; 22, 40, 13, 25\n## $ `variable1:totally_agree`    &lt;int&gt; 42, 25, 17, 16\n## $ `variable1:totally_disagree` &lt;int&gt; 39, 23, 17, 21\ndata2 &lt;- data2 %&gt;%\n  spread(variable_2, value) %&gt;%\n  rename_at(vars(-country, -date), funs(paste0(\"variable2:\", .)))\n\nglimpse(data2)\n## Observations: 4\n## Variables: 7\n## $ country                      &lt;chr&gt; \"lu\", \"lu\", \"lu\", \"lu\"\n## $ date                         &lt;chr&gt; \"01/01/2005\", \"01/01/2006\", \"01/0...\n## $ `variable2:kinda_agree`      &lt;int&gt; 22, 40, 13, 25\n## $ `variable2:kinda_disagree`   &lt;int&gt; 38, 19, 31, 12\n## $ `variable2:maybe`            &lt;int&gt; 69, 10, 12, 9\n## $ `variable2:totally_agree`    &lt;int&gt; 42, 25, 17, 16\n## $ `variable2:totally_disagree` &lt;int&gt; 39, 23, 17, 21\n\nrename_at() needs variables which you pass to vars(), a helper function to select variables, and a function that will do the renaming, passed to funs(). The function I use is simply paste0(), which pastes a string, for example “variable1:” with the name of the columns, given by the single ‘.’, a dummy argument. Now these datasets can be merged:\n\ndata1 %&gt;%\n  full_join(data2) %&gt;%\n  glimpse()\n## Joining, by = c(\"country\", \"date\")\n## Observations: 4\n## Variables: 12\n## $ country                      &lt;chr&gt; \"lu\", \"lu\", \"lu\", \"lu\"\n## $ date                         &lt;chr&gt; \"01/01/2005\", \"01/01/2006\", \"01/0...\n## $ `variable1:kinda_agree`      &lt;int&gt; 38, 19, 31, 12\n## $ `variable1:kinda_disagree`   &lt;int&gt; 69, 10, 12, 9\n## $ `variable1:maybe`            &lt;int&gt; 22, 40, 13, 25\n## $ `variable1:totally_agree`    &lt;int&gt; 42, 25, 17, 16\n## $ `variable1:totally_disagree` &lt;int&gt; 39, 23, 17, 21\n## $ `variable2:kinda_agree`      &lt;int&gt; 22, 40, 13, 25\n## $ `variable2:kinda_disagree`   &lt;int&gt; 38, 19, 31, 12\n## $ `variable2:maybe`            &lt;int&gt; 69, 10, 12, 9\n## $ `variable2:totally_agree`    &lt;int&gt; 42, 25, 17, 16\n## $ `variable2:totally_disagree` &lt;int&gt; 39, 23, 17, 21\n\nHope this post helps you understand the difference between long and wide datasets better, as well as dplyr::rename_at()!"
  },
  {
    "objectID": "posts/2019-06-12-intermittent.html",
    "href": "posts/2019-06-12-intermittent.html",
    "title": "Intermittent demand, Croston and Die Hard",
    "section": "",
    "text": "I have recently been confronted to a kind of data set and problem that I was not even aware existed: intermittent demand data. Intermittent demand arises when the demand for a certain good arrives sporadically. Let’s take a look at an example, by analyzing the number of downloads for the {RDieHarder} package:\n\nlibrary(tidyverse)\nlibrary(tsintermittent)\nlibrary(nnfor)\nlibrary(cranlogs)\nlibrary(brotools)\nrdieharder &lt;- cran_downloads(\"RDieHarder\", from = \"2017-01-01\")\n\nggplot(rdieharder) +\n  geom_line(aes(y = count, x = date), colour = \"#82518c\") +\n  theme_blog()\n\n\n\n\nLet’s take a look at just one month of data, because the above plot is not very clear, because of the outlier just before 2019… I wonder now, was that on Christmas day?\n\nrdieharder %&gt;%\n  filter(count == max(count))\n##         date count    package\n## 1 2018-12-21   373 RDieHarder\n\nNot exactly on Christmas day, but almost! Anyways, let’s look at one month of data:\n\njanuary_2018 &lt;- rdieharder %&gt;%\n  filter(between(date, as.Date(\"2018-01-01\"), as.Date(\"2018-02-01\")))\n\nggplot(january_2018) +\n  geom_line(aes(y = count, x = date), colour = \"#82518c\") +\n  theme_blog()\n\n\n\n\nNow, it is clear that this will be tricky to forecast. There is no discernible pattern, no trend, no seasonality… nothing that would make it “easy” for a model to learn how to forecast such data.\n\n\nThis is typical intermittent demand data. Specific methods have been developed to forecast such data, the most well-known being Croston, as detailed in this paper. A function to estimate such models is available in the {tsintermittent} package, written by Nikolaos Kourentzes who also wrote another package, {nnfor}, which uses Neural Networks to forecast time series data. I am going to use both to try to forecast the intermittent demand for the {RDieHarder} package for the year 2019.\n\n\nLet’s first load these packages:\n\nlibrary(tsintermittent)\nlibrary(nnfor)\n\nAnd as usual, split the data into training and testing sets:\n\ntrain_data &lt;- rdieharder %&gt;%\n  filter(date &lt; as.Date(\"2019-01-01\")) %&gt;%\n  pull(count) %&gt;%\n  ts()\n\ntest_data &lt;- rdieharder %&gt;%\n  filter(date &gt;= as.Date(\"2019-01-01\"))\n\nLet’s consider three models; a naive one, which simply uses the mean of the training set as the forecast for all future periods, Croston’s method, and finally a Neural Network from the {nnfor} package:\n\nnaive_model &lt;- mean(train_data)\n\ncroston_model &lt;- crost(train_data, h = 163)\n\nnn_model &lt;- mlp(train_data, reps = 1, hd.auto.type = \"cv\")\n## Warning in preprocess(y, m, lags, keep, difforder, sel.lag,\n## allow.det.season, : No inputs left in the network after pre-selection,\n## forcing AR(1).\nnn_model_forecast &lt;- forecast(nn_model, h = 163)\n\nThe crost() function estimates Croston’s model, and the h argument produces the forecast for the next 163 days. mlp() trains a multilayer perceptron, and the hd.auto.type = \"cv\" argument means that 5-fold cross-validation will be used to find the best number of hidden nodes. I then obtain the forecast using the forecast() function. As you can read from the Warning message above, the Neural Network was replaced by an auto-regressive model, AR(1), because no inputs were left after pre-selection… I am not exactly sure what that means, but if I remove the big outlier from before, this warning message disappears, and a Neural Network is successfully trained.\n\n\nIn order to rank the models, I follow this paper from Rob J. Hyndman, who wrote a very useful book titled Forecasting: Principles and Practice, and use the Mean Absolute Scaled Error, or MASE. You can also read this shorter pdf which also details how to use MASE to measure the accuracy for intermittent demand. Here is the function:\n\nmase &lt;- function(train_ts, test_ts, outsample_forecast){\n\n  naive_insample_forecast &lt;- stats::lag(train_ts)\n\n  insample_mae &lt;- mean(abs(train_ts - naive_insample_forecast), na.rm = TRUE)\n  error_outsample &lt;- test_ts - outsample_forecast\n\n  ase &lt;- error_outsample / insample_mae\n  mean(abs(ase), na.rm = TRUE)\n}\n\nIt is now easy to compute the models’ accuracies:\n\nmase(train_data, test_data$count, naive_model)\n## [1] 1.764385\nmase(train_data, test_data$count, croston_model$component$c.out[1])\n## [1] 1.397611\nmase(train_data, test_data$count, nn_model_forecast$mean)\n## [1] 1.767357\n\nCroston’s method is the one that performs best from the three. Maybe surprisingly, the naive method performs just as well as the Neural Network! (or rather, the AR(1) model) Let’s also plot the predictions with the true values from the test set:\n\ntest_data &lt;- test_data %&gt;%\n  mutate(naive_model_forecast = naive_model,\n         croston_model_forecast = croston_model$component$c.out[1],\n         nn_model_forecast = nn_model_forecast$mean) %&gt;%\n  select(-package) %&gt;%\n  rename(actual_value = count)\n\n\ntest_data_longer &lt;- test_data %&gt;%\n  gather(models, value,\n         actual_value, naive_model_forecast, croston_model_forecast, nn_model_forecast)\n## Warning: attributes are not identical across measure variables;\n## they will be dropped\nggplot(test_data_longer) +\n  geom_line(aes(y = value, x = date, colour = models)) +\n  theme_blog()\n\n\n\n\nJust to make sure I didn’t make a mistake when writing the mase() function, let’s use the accuracy() function from the {forecast} package and compare the result for the Neural Network:\n\nlibrary(forecast)\naccuracy(nn_model_forecast, x = test_data$actual_value)\n##                       ME     RMSE      MAE  MPE MAPE      MASE       ACF1\n## Training set 0.001929409 14.81196 4.109577  NaN  Inf 0.8437033 0.05425074\n## Test set     8.211758227 12.40199 8.635563 -Inf  Inf 1.7673570         NA\n\nThe result is the same, so it does seem like the naive method is not that bad, actually! Now, in general, intermittent demand series have a lot of 0 values, which is not really the case here. I still think that the methodology fits to this particular data set.\n\n\nHow else would you have forecast this data? Let me know via twitter!"
  },
  {
    "objectID": "posts/2013-12-31-r-cas.html",
    "href": "posts/2013-12-31-r-cas.html",
    "title": "Using R as a Computer Algebra System with Ryacas",
    "section": "",
    "text": "R is used to perform statistical analysis and doesn't focus on symbolic maths. But it is sometimes useful to let the computer derive a function for you (and have the analytic expression of said derivative), but maybe you don't want to leave your comfy R shell. It is possible to turn R into a full-fledged computer algebra system. CASs are tools that perform symbolic operations, such as getting the expression of the derivative of a user-defined (and thus completely arbitrary) function. Popular CASs include the proprietary Mathematica and Maple. There exists a lot of CASs under a Free Software license, Maxima (based on the very old Macsyma), Yacas, Xcas… In this post I will focus on Yacas and the Ryacas libarary. There is also the possibility to use the rSympy library that uses the Sympy Python library, which has a lot more features than Yacas. However, depending on your operating system installation can be tricky as it also requires rJava as a dependency.\n\n\nEven though Ryacas is quite nice to have, there are some issues though. For example, let's say you want the first derivative of a certain function f. If you use Ryacas to get it, the returned object won't be a function. There is a way to “extract” the text from the returned object and make a function out of it. But there are still other issues; I'll discuss them later.\n\n\nInstallation\n\n\nInstallation should be rather painless. On Linux you need to install Yacas first, which should be available in the major distros' repositories. Then you can install Ryacas from within the R shell. On Windows, you need to run these three commands (don't bother installing Yacas first):\n\n\ninstall.packages(Ryacas)\nlibrary(Ryacas)\nyacasInstall()\n\n\nYou can find more information on the project's page.\n\n\nExample session\n\n\nFirst, you must load Ryacas and define symbols that you will use in your functions.\n\n\nlibrary(Ryacas)\n\n## Loading required package: Ryacas Loading required package: XML\n\n\nx &lt;- Sym(\"x\")\n\n\nYou can then define your fonctions:\n\n\nmy_func &lt;- function(x) {\n  return(x/(x^2 + 3))\n}\n\n\nAnd you can get the derivative for instance:\n\n\nmy_deriv &lt;- yacas(deriv(my_func(x), x))\n\n## [1] \"Starting Yacas!\"\n\n\nIf you check the class of my_deriv, you'll see that it is of class yacas, which is not very useful. Let's «convert» it to a function:\n\n\nmy_deriv2 &lt;- function(x) {\n  eval(parse(text = my_deriv$YacasForm))\n}\n\n\nWe can then evaluate it. A lot of different operations are possible. But there are some problems.\n\n\nIssues with Ryacas\n\n\nYou can't use elements of a vector as parameters of your function, i.e.:\n\n\ntheta &lt;- Sym(\"theta\")\nfunc &lt;- function(x) {\n  return(theta[1] * x + theta[2])\n}\n\n\nLet's integrate this\nFunc &lt;- yacas(Integrate(func(x), x)) \n\n\nreturns (x^2theta)/2+NAx; which is not quite what we want…there is a workaround however. Define your functions like this:\n\n\na &lt;- Sym(\"a\")\nb &lt;- Sym(\"b\")\nfunc2 &lt;- function(x) {\n  return(a * x + b)\n}\n\n# Let&#39;s integrate this\nFunc2 &lt;- yacas(Integrate(func2(x), x))\n\n\nwe get the expected result: (x^2a)/2+bx;. Now replace a and b by the thetas:\n\n\nFunc2 &lt;- gsub(\"a\", \"theta[1]\", Func2$YacasForm)\nFunc2 &lt;- gsub(\"b\", \"theta[2]\", Func2)\n\n\nNow we have what we want:\n\n\nFunc2\n\n## [1] \"(x^2*theta[1])/2+theta[2]*x;\"\n\n\nYou can then copy-paste this result into a function.\n\n\nAnother problem is if you use built-in functions that are different between R and Yacas. For example:\n\n\nmy_log &lt;- function(x) {\n    return(sin(log(2 + x)))\n}\n\n\nNow try to differentiate it:\n\n\ndmy_log &lt;- yacas(deriv(my_log(x), x))\n\n\nyou get: Cos(Ln(x+2))/(x+2);. The problem with this, is that R doesn't recognize Cos as the cosine (which is cos in R) and the same goes for Ln. These are valid Yacas functions, but that is not the case in R. So you'll have to use gsub to replace these functions and then copy paste the end result into a function.\n\n\nConclusion\n\n\nWhile it has some flaws, Ryacas can be quite useful if you need to derive or integrate complicated expressions that you then want to use in R. Using some of the tricks I showed here, you should be able to overcome some of its shortcomings. If installation of rJava and thus rSympy becomes easier, I'll probably also do a short blog-post about it, as it has more features than Ryacas."
  },
  {
    "objectID": "posts/2018-04-15-announcing_pmice.html",
    "href": "posts/2018-04-15-announcing_pmice.html",
    "title": "{pmice}, an experimental package for missing data imputation in parallel using {mice} and {furrr}",
    "section": "",
    "text": "Yesterday I wrote this blog post which showed how one could use {furrr} and {mice} to impute missing data in parallel, thus speeding up the process tremendously.\n\n\nTo make using this snippet of code easier, I quickly cobbled together an experimental package called {pmice} that you can install from Github:\n\ndevtools::install_github(\"b-rodrigues/pmice\")\n\nFor now, it returns a list of mids objects and not a mids object like mice::mice() does, but I’ll be working on it. Contributions welcome!\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2019-03-31-tesseract.html",
    "href": "posts/2019-03-31-tesseract.html",
    "title": "Get text from pdfs or images using OCR: a tutorial with {tesseract} and {magick}",
    "section": "",
    "text": "In this blog post I’m going to show you how you can extract text from scanned pdf files, or pdf files where no text recognition was performed. (For pdfs where text recognition was performed, you can read my other blog post).\n\n\nThe pdf I’m going to use can be downloaded from here. It’s a poem titled, D’Léierchen (Dem Léiweckerche säi Lidd), written by Michel Rodange, arguably Luxembourg’s most well known writer and poet. Michel Rodange is mostly known for his fable, Renert oder De Fuuß am Frack an a Ma’nsgrëßt, starring a central European trickster anthropomorphic red fox.\n\n\n\n\n\nAnyway, back to the point of this blog post. How can we get data from a pdf where no text recognition was performed (or, how can we get text from an image)? The pdf we need the text from looks like this:\n\n\n\n\n\nTo get the text from the pdf, we can use the {tesseract} package, which provides bindings to the tesseract program. tesseract is an open source OCR engine developed by Google. This means that first you will need to install the tesseract program on your system. You can follow the intructions from tesseract’s github page. tesseract is currently at version 4.\n\n\nBefore applying OCR to a pdf, let’s first use the {pdftools} package to convert the pdf to png. This is because {tesseract} requires images as input (if you provide a pdf file, it will converted on the fly). Let’s first load the needed packages:\n\nlibrary(tidyverse)\nlibrary(tesseract)\nlibrary(pdftools)\nlibrary(magick)\n\nAnd now let’s convert the pdf to png files (in plural, because we’ll get one image per page of the pdf):\n\npngfile &lt;- pdftools::pdf_convert(\"path/to/pdf\", dpi = 600)\n\nThis will generate 14 png files. I erase the ones that are not needed, such as the title page. Now, let’s read in all the image files:\n\npath &lt;- dir(path = \"path/to/pngs\", pattern = \"*.png\", full.names = TRUE)\n\nimages &lt;- map(path, magick::image_read)\n\nThe images object is a list of magick-images, which we can parse. BUUUUUT! There’s a problem. The text is laid out in two columns. Which means that the first line after performing OCR will be the first line of the first column, and the first line of the second column joined together. Same for the other lines of course. So ideally, I’d need to split the file in the middle, and then perform OCR. This is easily done with the {magick} package:\n\nfirst_half &lt;- map(images, ~image_crop(., geometry = \"2307x6462\"))\n\nsecond_half &lt;- map(images, ~image_crop(., geometry = \"2307x6462+2307+0\"))\n\nBecause the pngs are 4614 by 6962 pixels, I can get the first half of the png by cropping at “2307x6462” (I decrease the height a bit to get rid of the page number), and the second half by applying the same logic, but starting the cropping at the “2307+0” position. The result looks like this:\n\n\n\n\n\nMuch better! Now I need to join these two lists together. I cannot simply join them. Consider the following example:\n\none &lt;- list(1, 3, 5)\n\ntwo &lt;- list(2, 4, 6)\n\nThis is the setup I currently have; first_half contains odd pages, and second_half contains even pages. The result I want would look like this:\n\nlist(1, 2, 3, 4, 5, 6)\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 2\n## \n## [[3]]\n## [1] 3\n## \n## [[4]]\n## [1] 4\n## \n## [[5]]\n## [1] 5\n## \n## [[6]]\n## [1] 6\n\nThere is a very elegant solution, with reduce2() from the {purrr} package. reduce() takes one list and a function, and … reduces the list to a single element. For instance:\n\nreduce(list(1, 2, 3), paste)\n## [1] \"1 2 3\"\n\nreduce2() is very similar, but takes in two lists, but the second list must be one element shorter:\n\nreduce2(list(1, 2, 3), list(\"a\", \"b\"), paste)\n## [1] \"1 2 a 3 b\"\n\nSo we cannot simply use reduce2() on lists one and two, because they’re the same length. So let’s prepend a value to one, using the prepend() function of {purrr}:\n\nprepend(one, 0) %&gt;% \n    reduce2(two, c)\n## [1] 0 1 2 3 4 5 6\n\nExactly what we need! Let’s apply this trick to our lists:\n\nmerged_list &lt;- prepend(first_half, NA) %&gt;% \n    reduce2(second_half, c) %&gt;% \n    discard(is.na)\n\nI’ve prepended NA to the first list, and then used reduce2() and then used discard(is.na) to remove the NA I’ve added at the start. Now, we can use OCR to get the text:\n\ntext_list &lt;- map(merged_list, ocr)\n\nocr() uses a model trained on English by default, and even though there is a model trained on Luxembourguish, the one trained on English works better! Very likely because the English model was trained on a lot more data than the Luxembourguish one. I was worried the English model was not going to recognize characters such as é, but no, it worked quite well.\n\n\nThis is how it looks like:\n\ntext_list\n\n[[1]]\n[1] \"Lhe\\n| Kaum huet d’Feld dat fréndlecht Feier\\nVun der Aussentssonn gesunn\\nAs mam Plou aus Stall a Scheier\\n* D’lescht e Bauer ausgezunn.\\nFir de Plou em nach ze dreiwen\\nWar sai Jéngelchen alaert,\\nDeen nét wéllt doheem méi bleiwen\\n8 An esouz um viischte Paerd.\\nOp der Schéllche stoung ze denken\\nD’Léierche mam Hierz voll Lidder\\nFir de Béifchen nach ze zanken\\n12 Duckelt s’an de Som sech nidder.\\nBis e laascht war, an du stémmt se\\nUn e Liddchen, datt et kraacht\\nOp der Nouteleder klémmt se\\n16 Datt dem Béifchen d’Haerz alt laacht.\\nAn du sot en: Papp, ech mengen\\nBal de Vull dee kénnt och schwatzen.\\nLauschter, sot de Papp zum Klengen,\\n20 Ech kann d’Liddchen iwersetzen.\\nI\\nBas de do, mii léiwe Fréndchen\\nMa de Wanter dee war laang!\\nKuck, ech hat keng fréilech Sténnchen\\n24 *T war fir dech a mech mer baang.\\nAn du koum ech dech besichen\\nWell du goungs nét méi eraus\\nMann wat hues jo du eng Kichen\\n28 Wat eng Scheier wat en Haus.\\nWi zerguttster, a wat Saachen!\\nAn déng Frache gouf mer Brout.\\nAn déng Kanner, wi se laachen,\\n32, An hir Backelcher, wi rout!\\nJo, bei dir as Rot nét deier!\\nJo a kuck mer wat eng Méscht.\\nDat gét Saache fir an d’Scheier\\n36 An och Sué fir an d’Késcht.\\nMuerges waars de schuns um Dreschen\\nIr der Daudes d’Schung sech stréckt\\nBas am Do duurch Wis a Paschen\\n40 Laascht all Waassergruef geschréckt.\\n\"\n....\n....\n\nWe still need to split at the “” character:\n\ntext_list &lt;- text_list %&gt;% \n    map(., ~str_split(., \"\\n\"))\n\nThe end result:\n\ntext_list\n\n[[1]]\n[[1]][[1]]\n [1] \"Lhe\"                                      \"| Kaum huet d’Feld dat fréndlecht Feier\" \n [3] \"Vun der Aussentssonn gesunn\"              \"As mam Plou aus Stall a Scheier\"         \n [5] \"* D’lescht e Bauer ausgezunn.\"            \"Fir de Plou em nach ze dreiwen\"          \n [7] \"War sai Jéngelchen alaert,\"               \"Deen nét wéllt doheem méi bleiwen\"       \n [9] \"8 An esouz um viischte Paerd.\"            \"Op der Schéllche stoung ze denken\"       \n[11] \"D’Léierche mam Hierz voll Lidder\"         \"Fir de Béifchen nach ze zanken\"          \n[13] \"12 Duckelt s’an de Som sech nidder.\"      \"Bis e laascht war, an du stémmt se\"      \n[15] \"Un e Liddchen, datt et kraacht\"           \"Op der Nouteleder klémmt se\"             \n[17] \"16 Datt dem Béifchen d’Haerz alt laacht.\" \"An du sot en: Papp, ech mengen\"          \n[19] \"Bal de Vull dee kénnt och schwatzen.\"     \"Lauschter, sot de Papp zum Klengen,\"     \n[21] \"20 Ech kann d’Liddchen iwersetzen.\"       \"I\"                                       \n[23] \"Bas de do, mii léiwe Fréndchen\"           \"Ma de Wanter dee war laang!\"             \n[25] \"Kuck, ech hat keng fréilech Sténnchen\"    \"24 *T war fir dech a mech mer baang.\"    \n[27] \"An du koum ech dech besichen\"             \"Well du goungs nét méi eraus\"            \n[29] \"Mann wat hues jo du eng Kichen\"           \"28 Wat eng Scheier wat en Haus.\"         \n[31] \"Wi zerguttster, a wat Saachen!\"           \"An déng Frache gouf mer Brout.\"          \n[33] \"An déng Kanner, wi se laachen,\"           \"32, An hir Backelcher, wi rout!\"         \n[35] \"Jo, bei dir as Rot nét deier!\"            \"Jo a kuck mer wat eng Méscht.\"           \n[37] \"Dat gét Saache fir an d’Scheier\"          \"36 An och Sué fir an d’Késcht.\"          \n[39] \"Muerges waars de schuns um Dreschen\"      \"Ir der Daudes d’Schung sech stréckt\"     \n[41] \"Bas am Do duurch Wis a Paschen\"           \"40 Laascht all Waassergruef geschréckt.\" \n[43] \"\"  \n...\n...\n\nPerfect! Some more cleaning would be needed though. For example, I need to remove the little annotations that are included:\n\n\n\n\n\nI don’t know yet how I’m going to do that.I also need to remove the line numbers at the beginning of every fourth line, but this is easily done with a simple regular expression:\n\nstr_remove_all(c(\"12 bla\", \"blb\", \"123 blc\"), \"^\\\\d{1,}\\\\s+\")\n## [1] \"bla\" \"blb\" \"blc\"\n\nBut this will be left for a future blog post!"
  },
  {
    "objectID": "posts/2018-06-24-fun_ts.html",
    "href": "posts/2018-06-24-fun_ts.html",
    "title": "Forecasting my weight with R",
    "section": "",
    "text": "I’ve been measuring my weight almost daily for almost 2 years now; I actually started earlier, but not as consistently. The goal of this blog post is to get re-acquaiented with time series; I haven’t had the opportunity to work with time series for a long time now and I have seen that quite a few packages that deal with time series have been released on CRAN. In this blog post, I will explore my weight measurements using some functions from the {tsibble} and {tibbletime} packages, and then do some predictions with the {forecast} package.\n\n\nFirst, let’s load the needed packages, read in the data and convert it to a tsibble:\n\nlibrary(\"tidyverse\")\nlibrary(\"readr\")\nlibrary(\"forecast\")\nlibrary(\"tsibble\")\nlibrary(\"tibbletime\")\nlibrary(\"mice\")\nweight &lt;- read_csv(\"https://gist.githubusercontent.com/b-rodrigues/ea60679135f8dbed448ccf66a216811f/raw/18b469f3b0720f76ce5ee2715d0f9574b615f170/gistfile1.txt\") %&gt;% \n    as_tsibble()\n## Parsed with column specification:\n## cols(\n##   Date = col_date(format = \"\"),\n##   Poids = col_double()\n## )\n## The `index` is `Date`.\n\nYou can read more about {tsibble} here. Here, I use {tsibble} mostly for the next step, which is using the function fill_na() on the tsibble. fill_na() turns implicit missing values into explicit missing values. These are implicit missing values:\n\n          Date Poids\n1   2013-01-01 84.10\n2   2013-01-04 85.60\n\nand this is the same view, but with explicit missing values:\n\n          Date Poids\n1   2013-01-01 84.10\n2   2013-01-02 NA\n3   2013-01-03 NA\n4   2013-01-04 85.60\n\nThis is useful to do, because I want to impute the missing values using the {mice} package. Let’s do this:\n\nweight &lt;- weight %&gt;% \n    fill_na()\n\nimp_weight &lt;- mice(data = weight) %&gt;% \n    mice::complete(\"long\")\n## \n##  iter imp variable\n##   1   1  Poids\n##   1   2  Poids\n##   1   3  Poids\n##   1   4  Poids\n##   1   5  Poids\n##   2   1  Poids\n##   2   2  Poids\n##   2   3  Poids\n##   2   4  Poids\n##   2   5  Poids\n##   3   1  Poids\n##   3   2  Poids\n##   3   3  Poids\n##   3   4  Poids\n##   3   5  Poids\n##   4   1  Poids\n##   4   2  Poids\n##   4   3  Poids\n##   4   4  Poids\n##   4   5  Poids\n##   5   1  Poids\n##   5   2  Poids\n##   5   3  Poids\n##   5   4  Poids\n##   5   5  Poids\n\nLet’s take a look at imp_weight:\n\nhead(imp_weight)\n##   .imp .id       Date Poids\n## 1    1   1 2013-10-28  84.1\n## 2    1   2 2013-10-29  84.4\n## 3    1   3 2013-10-30  83.5\n## 4    1   4 2013-10-31  84.1\n## 5    1   5 2013-11-01  85.6\n## 6    1   6 2013-11-02  85.2\n\nLet’s select the relevant data. I filter from the 11th of July 2016, which is where I started weighing myself almost every day, to the 31st of May 2018. I want to predict my weight for the month of June (you might think of the month of June 2018 as the test data, and the rest as training data):\n\nimp_weight_train &lt;- imp_weight %&gt;% \n    filter(Date &gt;= \"2016-07-11\", Date &lt;= \"2018-05-31\")\n\nIn the next lines, I create a column called imputation which is simply the same as the column .imp but of character class, remove unneeded columns and rename some other columns (“Poids” is French for weight):\n\nimp_weight_train &lt;- imp_weight_train %&gt;% \n    mutate(imputation = as.character(.imp)) %&gt;% \n    select(-.id, -.imp) %&gt;% \n    rename(date = Date) %&gt;% \n    rename(weight = Poids)\n\nLet’s take a look at the data:\n\nggplot(imp_weight_train, aes(date, weight, colour = imputation)) +\n    geom_line() + \n    theme(legend.position = \"bottom\")\n\n\n\n\nThis plots gives some info, but it might be better to smooth the lines. This is possible by computing a rolling mean. For this I will use the rollify() function of the {tibbletime} package:\n\nmean_roll_5 &lt;- rollify(mean, window = 5)\nmean_roll_10 &lt;- rollify(mean, window = 10)\n\nrollify() can be seen as an adverb, pretty much like purrr::safely(); rollify() is a higher order function that literally rollifies a function, in this case mean() which means that rollifying the mean creates a function that returns the rolling mean. The window argument lets you decide how smooth you want the curve to be: the higher the smoother. However, you will lose some observations. Let’s use this functions to add the rolling means to the data frame:\n\nimp_weight_train &lt;- imp_weight_train %&gt;% \n    group_by(imputation) %&gt;% \n    mutate(roll_5 = mean_roll_5(weight),\n           roll_10 = mean_roll_10(weight))\n\nNow, let’s plot these new curves:\n\nggplot(imp_weight_train, aes(date, roll_5, colour = imputation)) +\n    geom_line() + \n    theme(legend.position = \"bottom\")\n## Warning: Removed 20 rows containing missing values (geom_path).\n\n\n\nggplot(imp_weight_train, aes(date, roll_10, colour = imputation)) +\n    geom_line() + \n    theme(legend.position = \"bottom\")\n## Warning: Removed 45 rows containing missing values (geom_path).\n\n\n\n\nThat’s easier to read, isn’t it?\n\n\nNow, I will use the auto.arima() function to train a model on the data to forecast my weight for the month of June. However, my data, imp_weight_train is a list of datasets. auto.arima() does not take a data frame as an argument, much less so a list of datasets. I’ll create a wrapper around auto.arima() that works on a dataset, and then map it to the list of datasets:\n\nauto.arima.df &lt;- function(data, y, ...){\n\n    y &lt;- enquo(y)\n\n    yts &lt;- data %&gt;% \n        pull(!!y) %&gt;% \n        as.ts()\n\n    auto.arima(yts, ...)\n}\n\nauto.arima.df() takes a data frame as argument, and then y, which is the column that contains the univariate time series. This column then gets pulled out of the data frame, converted to a time series object with as.ts(), and then passed down to auto.arima(). I can now use this function on my list of data sets. The first step is to nest the data:\n\nnested_data &lt;- imp_weight_train %&gt;% \n    group_by(imputation) %&gt;% \n    nest() \n\nLet’s take a look at nested_data:\n\nnested_data\n## # A tibble: 5 x 2\n##   imputation data              \n##   &lt;chr&gt;      &lt;list&gt;            \n## 1 1          &lt;tibble [690 × 4]&gt;\n## 2 2          &lt;tibble [690 × 4]&gt;\n## 3 3          &lt;tibble [690 × 4]&gt;\n## 4 4          &lt;tibble [690 × 4]&gt;\n## 5 5          &lt;tibble [690 × 4]&gt;\n\nnested_data is a tibble with a column called data, which is a so-called list-column. Each element of data is itself a tibble. This is a useful structure, because now I can map auto.arima.df() to the data frame:\n\nmodels &lt;- nested_data %&gt;% \n    mutate(model = map(data, auto.arima.df, y = weight))\n\nThis trick can be a bit difficult to follow the first time you see it. The idea is the following: nested_data is a tibble. Thus, I can add a column to it using mutate(). So far so good. Now that I am “inside” the mutate call, I can use purrr::map(). Why? purrr::map() takes a list and then a function as arguments. Remember that data is a list column; you can see it above, the type of the column data is list. So data is a list, and thus can be used inside purrr::map(). Great. Now, what is inside data? tibbles, where inside each of them is a column called weight. This is the column that contains my univariate time series I want to model. Let’s take a look at models:\n\nmodels\n## # A tibble: 5 x 3\n##   imputation data               model      \n##   &lt;chr&gt;      &lt;list&gt;             &lt;list&gt;     \n## 1 1          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;\n## 2 2          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;\n## 3 3          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;\n## 4 4          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;\n## 5 5          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;\n\nmodels is a tibble with a column called model, where each element is a model of type ARIMA.\n\n\nAdding forecasts is based on the same trick as above, and we use the forecast() function:\n\nforecasts &lt;- models %&gt;% \n    mutate(predictions = map(model, forecast, h = 24)) %&gt;% \n    mutate(predictions = map(predictions, as_tibble)) %&gt;% \n    pull(predictions) \n\nI forecast 24 days (I am writing this on the 24th of June), and convert the predictions to tibbles, and then pull only the predictions tibble:\n\nforecasts\n## [[1]]\n## # A tibble: 24 x 5\n##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`\n##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1             71.5    70.7    72.3    70.2    72.8\n##  2             71.5    70.7    72.4    70.3    72.8\n##  3             71.5    70.6    72.3    70.1    72.8\n##  4             71.5    70.6    72.4    70.1    72.9\n##  5             71.4    70.5    72.4    70.0    72.9\n##  6             71.5    70.5    72.4    70.0    72.9\n##  7             71.4    70.5    72.4    69.9    72.9\n##  8             71.4    70.4    72.4    69.9    72.9\n##  9             71.4    70.4    72.4    69.9    72.9\n## 10             71.4    70.4    72.4    69.8    73.0\n## # ... with 14 more rows\n## \n## [[2]]\n## # A tibble: 24 x 5\n##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`\n##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1             71.6    70.8    72.3    70.3    72.8\n##  2             71.6    70.8    72.5    70.3    72.9\n##  3             71.5    70.6    72.4    70.2    72.9\n##  4             71.5    70.6    72.5    70.1    72.9\n##  5             71.5    70.5    72.5    70.0    73.0\n##  6             71.5    70.5    72.5    70.0    73.0\n##  7             71.5    70.5    72.5    69.9    73.0\n##  8             71.5    70.4    72.5    69.9    73.1\n##  9             71.5    70.4    72.5    69.8    73.1\n## 10             71.4    70.3    72.6    69.7    73.1\n## # ... with 14 more rows\n## \n## [[3]]\n## # A tibble: 24 x 5\n##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`\n##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1             71.6    70.8    72.4    70.4    72.8\n##  2             71.5    70.7    72.4    70.2    72.8\n##  3             71.5    70.6    72.4    70.2    72.9\n##  4             71.5    70.6    72.4    70.1    72.9\n##  5             71.5    70.5    72.4    70.0    72.9\n##  6             71.5    70.5    72.4    70.0    73.0\n##  7             71.5    70.5    72.5    69.9    73.0\n##  8             71.4    70.4    72.5    69.9    73.0\n##  9             71.4    70.4    72.5    69.8    73.0\n## 10             71.4    70.4    72.5    69.8    73.1\n## # ... with 14 more rows\n## \n## [[4]]\n## # A tibble: 24 x 5\n##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`\n##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1             71.5    70.8    72.3    70.3    72.8\n##  2             71.5    70.7    72.4    70.3    72.8\n##  3             71.5    70.7    72.4    70.2    72.8\n##  4             71.5    70.6    72.4    70.1    72.9\n##  5             71.5    70.6    72.4    70.1    72.9\n##  6             71.5    70.5    72.5    70.0    73.0\n##  7             71.5    70.5    72.5    69.9    73.0\n##  8             71.5    70.4    72.5    69.9    73.0\n##  9             71.4    70.4    72.5    69.8    73.1\n## 10             71.4    70.3    72.5    69.8    73.1\n## # ... with 14 more rows\n## \n## [[5]]\n## # A tibble: 24 x 5\n##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`\n##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1             71.5    70.8    72.3    70.3    72.8\n##  2             71.5    70.7    72.4    70.3    72.8\n##  3             71.5    70.7    72.4    70.2    72.8\n##  4             71.5    70.6    72.4    70.1    72.9\n##  5             71.5    70.6    72.4    70.1    72.9\n##  6             71.5    70.5    72.4    70.0    73.0\n##  7             71.5    70.5    72.5    69.9    73.0\n##  8             71.5    70.4    72.5    69.9    73.0\n##  9             71.4    70.4    72.5    69.8    73.1\n## 10             71.4    70.3    72.5    69.8    73.1\n## # ... with 14 more rows\n\nSo forecasts is a list of tibble, each containing a forecast. Remember that I have 5 tibbles, because I imputed the data 5 times. I will merge this list of data sets together into one, but before I need to add a column that indices the forecasts:\n\nforecasts &lt;- map2(.x = forecasts, .y = as.character(seq(1, 5)), \n     ~mutate(.x, id = .y)) %&gt;% \n    bind_rows() %&gt;% \n    select(-c(`Lo 80`, `Hi 80`))\n\ncolnames(forecasts) &lt;- c(\"point_forecast\", \"low_95\", \"hi_95\", \"id\")\n\nLet’s take a look again at forecasts:\n\nforecasts\n## # A tibble: 120 x 4\n##    point_forecast low_95 hi_95 id   \n##             &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n##  1           71.5   70.2  72.8 1    \n##  2           71.5   70.3  72.8 1    \n##  3           71.5   70.1  72.8 1    \n##  4           71.5   70.1  72.9 1    \n##  5           71.4   70.0  72.9 1    \n##  6           71.5   70.0  72.9 1    \n##  7           71.4   69.9  72.9 1    \n##  8           71.4   69.9  72.9 1    \n##  9           71.4   69.9  72.9 1    \n## 10           71.4   69.8  73.0 1    \n## # ... with 110 more rows\n\nI now select the true values for the month of June. I also imputed this data, but here I will simply keep the average of the imputations:\n\nweight_june &lt;- imp_weight %&gt;% \n    filter(Date &gt;= \"2018-06-01\") %&gt;% \n    select(-.id) %&gt;% \n    group_by(Date) %&gt;% \n    summarise(true_weight = mean(Poids)) %&gt;% \n    rename(date = Date)\n\nLet’s take a look at weight_june:\n\nweight_june\n## # A tibble: 24 x 2\n##    date       true_weight\n##    &lt;date&gt;           &lt;dbl&gt;\n##  1 2018-06-01        71.8\n##  2 2018-06-02        70.8\n##  3 2018-06-03        71.2\n##  4 2018-06-04        71.4\n##  5 2018-06-05        70.9\n##  6 2018-06-06        70.8\n##  7 2018-06-07        70.5\n##  8 2018-06-08        70.1\n##  9 2018-06-09        70.3\n## 10 2018-06-10        71.0\n## # ... with 14 more rows\n\nLet’s repeat weight_june 5 times, and add the index 1 to 5. Why? Because I want to merge the true data with the forecasts, and having the data in this form makes things easier:\n\nweight_june &lt;- modify(list_along(1:5), ~`&lt;-`(., weight_june)) %&gt;% \n    map2(.y = as.character(seq(1, 5)), \n         ~mutate(.x, id = .y)) %&gt;% \n    bind_rows()\n\nThe first line:\n\nmodify(list_along(1:5), ~`&lt;-`(., weight_june)) \n\nlooks quite complicated, but you will see that it is not, once we break it apart. modify() modifies a list. The list to modify is list_along(1:5), which create a list of NULLs:\n\nlist_along(1:5)\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n\nThe second argument of modify() is either a function or a formula. I created the following formula:\n\n~`&lt;-`(., weight_june)\n\nWe all know the function &lt;-(), but are not used to see it that way. But consider the following:\n\na &lt;- 3\n`&lt;-`(a, 3)\n\nThese two formulations are equivalent. So these lines fill the empty element of the list of NULLs with the data frame weight_june. Then I add the id column and then bind the rows together: bind_rows().\n\n\nLet’s bind the columns of weight_june and forecasts and take a look at it:\n\nforecasts &lt;- bind_cols(weight_june, forecasts) %&gt;% \n    select(-id1)\n\nforecasts\n## # A tibble: 120 x 6\n##    date       true_weight id    point_forecast low_95 hi_95\n##    &lt;date&gt;           &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n##  1 2018-06-01        71.8 1               71.5   70.2  72.8\n##  2 2018-06-02        70.8 1               71.5   70.3  72.8\n##  3 2018-06-03        71.2 1               71.5   70.1  72.8\n##  4 2018-06-04        71.4 1               71.5   70.1  72.9\n##  5 2018-06-05        70.9 1               71.4   70.0  72.9\n##  6 2018-06-06        70.8 1               71.5   70.0  72.9\n##  7 2018-06-07        70.5 1               71.4   69.9  72.9\n##  8 2018-06-08        70.1 1               71.4   69.9  72.9\n##  9 2018-06-09        70.3 1               71.4   69.9  72.9\n## 10 2018-06-10        71.0 1               71.4   69.8  73.0\n## # ... with 110 more rows\n\nNow, for the last plot:\n\nggplot(forecasts, aes(x = date, colour = id)) +\n    geom_line(aes(y = true_weight), size = 2) + \n    geom_line(aes(y = hi_95)) + \n    geom_line(aes(y = low_95)) + \n    theme(legend.position = \"bottom\")\n\n\n\n\nThe true data fall within all the confidence intervals, but I am a bit surprised by the intervals, especially the upper confidence intervals; they all are way above 72kg, however my true weight has been fluctuating around 71kg for quite some months now. I think I have to refresh my memory on time series, because I am certainly missing something!\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2018-02-11-census-random_forest.html",
    "href": "posts/2018-02-11-census-random_forest.html",
    "title": "Predicting job search by training a random forest on an unbalanced dataset",
    "section": "",
    "text": "Update 2022: there some literature advising against using techniques to artificially balance a dataset, for example one. Use at your own risks!\nIn this blog post, I am going to train a random forest on census data from the US to predict the probability that someone is looking for a job. To this end, I downloaded the US 1990 census data from the UCI Machine Learning Repository. Having a background in economics, I am always quite interested by such datasets. I downloaded the raw data which is around 820mb uncompressed. You can download it from this folder here.\nBefore training a random forest on it, some preprocessing is needed. First problem: the columns in the data do not have names. Actually, training a random forest on unamed variables is possible, but I like my columns to have names. The names are on a separate file, called USCensus1990raw.attributes.txt. This is how this file looks like:\nThe variable names are always written in upper case and sometimes end with some numbers. Regular expressions will help extract these column names:\nUsing readLines I load this text file into R. Then with stringr::str_extract_all, I can extract the variable names from this text file. The regular expression, 1+(\\d{1,}|[A-Z])\\s+ can seem complicated, but by breaking it up, it’ll be clear:\nThis regular expression matches only the variable names. By using ^ I only limit myself to the uppercase letters at the start of the line, which already removes a lot of unneeded lines from the text. Then, by matching numbers or letters, followed by spaces, I avoid matching strings such as VAR:. There’s probably a shorter way to write this regular expression, but since this one works, I stopped looking for another solution.\nNow that I have a vector called column_names, I can baptize the columns in my dataset:\nI also add a column called caseid to the dataset, but it’s actually not really needed. But it made me look for and find rownames_to_column(), which can be useful:\nNow I select the variables I need. I use dplyr::select() to select the columns I need (actually, I will remove some of these later for the purposes of the blog post, but will continue exploring them. Maybe write a part 2?):\nNow, I convert factor variables to factors and only relevel the race variable:\nSo the variable I want to predict is looking which has 2 levels (I removed the level 0, which stands for NA). I convert all the variables that are supposed to be factors into factors using mutate_at() and then reselect a subsample of the columns. census is now a tibble with 39 columns and 2458285 rows. I will train the forest on a subsample only, because with cross validation it would take forever on the whole dataset.\nI run the training on another script, that I will then run using the Rscript command instead of running it from Spacemacs (yes, I don’t use RStudio at home but Spacemacs + ESS). Here’s the script:\n90% of the individuals in the sample are not looking for a new job. For training purposes, I will only use 50000 observations instead of the whole sample. I’m already thinking about writing another blog post where I show how to use the whole data. But 50000 observations should be more than enough to have a pretty nice model. However, having 90% of observations belonging to a single class can cause problems with the model; the model might predict that everyone should belong to class 2 and in doing so, the model would be 90% accurate! Let’s ignore this for now, but later I am going to tackle this issue with a procedure calleds SMOTE.\nNow, using caret::trainIndex(), I partition the data into a training sample and a testing sample:\nI also save the testing data to disk, because when the training is done I’ll lose my R session (remember, I’ll run the training using Rscript):\nBefore training the model, I’ll change some options; I’ll do 5-fold cross validation that I repeat 5 times. This will further split the training set into training/testing sets which will increase my confidence in the metrics that I get from the training. This will ensure that the best model really is the best, and not a fluke resulting from the splitting of the data that I did beforehand. Then, I will test the best model on the testing data from above:\nA very nice feature from the caret package is the possibility to make the training in parallel. For this, load the doParallel package (which I did above), and then register the number of cores you want to use for training with makeCluster(). You can replace detectCores() by the number of cores you want to use:\nFinally, we can train the model:\nBecause it takes around 1 and a half hours to train, I save the model to disk using saveRDS():\nThe picture below shows all the cores from my computer running and RAM usage being around 20gb during the training process:\nAnd this the results of training the random forest on the unbalanced data:\nIf someone really is looking for a job, the model is able to predict it correctly 92% of the times and 98% of the times if that person is not looking for a job. It’s slightly better than simply saying than no one is looking for a job, which would be right 90% of the times, but not great either.\nTo train to make the model more accurate in predicting class 1, I will resample the training set, but by downsampling class 2 and upsampling class 1. This can be done with the function SMOTE() from the {DMwR} package. However, the testing set should have the same distribution as the population, so I should not apply SMOTE() to the testing set. I will resplit the data, but this time with a 95/5 % percent split; this way I have 5% of the original dataset used for testing, I can use SMOTE() on the 95% remaining training set. Because SMOTEing takes some time, I save the SMOTEd training set using readRDS() for later use:\nThe testing set has 34780 observations and below you can see the distribution of the target variable, looking:\nHere are the results:\nThe balanced accuracy is higher, but unlike what I expected (and hoped), this model is worse in predicting class 1! I will be trying one last thing; since I have a lot of data at my disposal, I will simply sample 25000 observations where the target variable looking equals 1, and then sample another 25000 observations where the target variable equals 2 (without using SMOTE()). Then I’ll simply bind the rows and train the model on that:\nAnd here are the results:\nLooks like it’s not much better than using SMOTE()!\nThere are several ways I could achieve better predictions; tuning the model is one possibility, or perhaps going with another type of model altogether. I will certainly come back to this dataset in future blog posts!\nUsing the best model, let’s take a look at which variables are the most important for predicting job search:\nIt’s also possible to have a plot of the above:\nTo make sense of this, we have to read the description of the features here.\nrlabor3 is the most important variable, and means that the individual is unemployed. rlabor6 means not in the labour force. Then the age of the individual as well as the individual’s income play a role. tmpabsnt is a variable that equals 1 if the individual is temporary absent from work, due to a layoff. All these variables having an influence on the probability of looking for a job make sense, but looks like a very simple model focusing on just a couple of variables would make as good a job as the random forest.\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2018-02-11-census-random_forest.html#footnotes",
    "href": "posts/2018-02-11-census-random_forest.html#footnotes",
    "title": "Predicting job search by training a random forest on an unbalanced dataset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA-Z↩︎\nA-Z↩︎"
  },
  {
    "objectID": "posts/2017-12-27-build_formulae.html",
    "href": "posts/2017-12-27-build_formulae.html",
    "title": "Building formulae",
    "section": "",
    "text": "This Stackoverflow question made me think about how to build formulae. For example, you might want to programmatically build linear model formulae and then map these models on data. For example, suppose the following (output suppressed):\n\ndata(mtcars)\n\nlm(mpg ~ hp, data = mtcars)\nlm(mpg ~I(hp^2), data = mtcars)\nlm(mpg ~I(hp^3), data = mtcars)\nlm(mpg ~I(hp^4), data = mtcars)\nlm(mpg ~I(hp^5), data = mtcars)\nlm(mpg ~I(hp^6), data = mtcars)\n\nTo avoid doing this, one can write a function that builds the formulae:\n\ncreate_form = function(power){\n  rhs = substitute(I(hp^pow), list(pow=power))\n  rlang::new_formula(quote(mpg), rhs)\n}\n\nIf you are not familiar with substitute(), try the following to understand what it does:\n\nsubstitute(y ~ x, list(x = 1))\n## y ~ 1\n\nThen using rlang::new_formula() I build a formula by providing the left hand side, which is quote(mpg) here, and the right hand side, which I built using substitute(). Now I can create a list of formulae:\n\nlibrary(tidyverse)\n\nlist_formulae = map(seq(1, 6), create_form)\n\nstr(list_formulae)\n## List of 6\n##  $ :Class 'formula'  language mpg ~ I(hp^1L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605f897ca0&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^2L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605f891418&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^3L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605da76098&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^4L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605da6a600&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^5L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605da68980&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^6L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605da66d38&gt;\n\nAs you can see, power got replaced by 1, 2, 3,… and each element of the list is a nice formula. Exactly what lm() needs. So now it’s easy to map lm() to this list of formulae:\n\ndata(mtcars)\n\nmap(list_formulae, lm, data = mtcars)\n## [[1]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^1)  \n##    30.09886     -0.06823  \n## \n## \n## [[2]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^2)  \n##  24.3887252   -0.0001649  \n## \n## \n## [[3]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^3)  \n##   2.242e+01   -4.312e-07  \n## \n## \n## [[4]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^4)  \n##   2.147e+01   -1.106e-09  \n## \n## \n## [[5]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^5)  \n##   2.098e+01   -2.801e-12  \n## \n## \n## [[6]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^6)  \n##   2.070e+01   -7.139e-15\n\nThis is still a new topic for me there might be more elegant ways to do that, using tidyeval to remove the hardcoding of the columns in create_form(). I might continue exploring this."
  },
  {
    "objectID": "posts/2019-01-13-newspapers_mets_alto.html",
    "href": "posts/2019-01-13-newspapers_mets_alto.html",
    "title": "Making sense of the METS and ALTO XML standards",
    "section": "",
    "text": "Last week I wrote a blog post where I analyzed one year of newspapers ads from 19th century newspapers. The data is made available by the national library of Luxembourg. In this blog post, which is part 1 of a 2 part series, I extract data from the 257gb archive, which contains 10 years of publications of the L’Union, another 19th century Luxembourguish newspaper written in French. As I explained in the previous post, to make life easier to data scientists, the national library also included ALTO and METS files (which are a XML files used to describe the layout and contents of physical text sources, such as pages of a book or newspaper) which can be easily parsed by R.\nThis is how a ALTO file looks like:\nEach page of the newspaper of a given day has one ALTO file. This is how a METS file looks like:\nFor each daily issue of the newspaper, there is a METS file. So 1 METS file for 4 ALTO files.\nIn my last blog post, I only extracted the words from the ALTO file (red rectangles of the first screenshot) and did not touch the METS file. The problem of doing this is that I get all the words for each page, without knowing which come from the same article. If I want to know which words come from the same article, I need to use the info from the METS file. From the METS file I have the ID of the article, and some other metadata, such as the title of the article and the type of the article (which can be article, advertisement, etc). The information highlighted with the green rectangles in the METS file can be linked to the green rectangles from the ALTO files. My goal is to get the following data frame from the METS file:\nand this data frame from the ALTO files:\nAs you can see, by combining both data frames I can know which words come from the same article, which will be helpful for further analysis. A lot of things happened in the 1860s. I am really curious to see if and how these events where reported in a Luxembourguish newspaper. I am particularly curious about how long it took to report certain news from far away, such as the assassination of Abraham Lincoln. But before that I need to extract the data!\nI will only focus on the METS file. The logic for the ALTO file is the same. All the source code will be in the appendix of this blog post.\nFirst, let’s take a look at a METS file:\nThis is how it looks like:\nAs usual when you import text files like this, it’s always a good idea to split the file. I will split at the “DMDID” character. Take a look back at the second screenshot. The very first tag, first row, first word after div is “DMDID”. By splitting at this level, I will get back a list, where each element is the content of this div DMDID block. This is exactly what I need, since this block contains the information from the green rectangles. So let’s split the mets variable at this level:\nLet’s take a look at mets_articles:\nDoesn’t seem to be very helpful, but actually it is. We can see that mets_articles is a now a list of 25 elements.\nThis means that for each element of mets_articles, I need to get the identifier, the label, the type (the red rectangles from the screenshot), but also the information from the “BEGIN” element (the green rectangle).\nTo do this, I’ll be using regular expressions. In general, I start by experimenting in the console, and then when things start looking good, I write a function. Here is this function:\nThis function may seem complicated, but it simply encapsulates some pretty standard steps to get the data I need. I had to consider two cases. The first case is when I need to extract all the elements with str_extract_all(), or only the first occurrence, with str_extract(). Let’s test it on the first article of the mets_articles list:\nLet’s see what happens with all = TRUE:\nThis seems to work as intended. Since I need to call this function several times, I’ll be writing another function that extracts all I need:\nThis function uses complex regular expressions to extract the strings I need, and then puts the result into a data frame, with the tibble() function. I then use unnest(), because label, type, begins and id are not the same length. label, type and id are of length 1, while begins is longer. This means that when I put them into a data frame it looks like this:\nWith unnest(), I get a nice data frame:\nNow, I simply need to map this function to all the files and that’s it! For this, I will write yet another helper function:\nThis function takes the path to a METS file as input, and processes it using the steps I explained above. The only difference is that I add a column containing the name of the file that was processed, and write the resulting data frame directly to disk as a data frame. Finally, I can map this function to all the METS files:\nI use {furrr} to extract the data from all the files in parallel, by putting 8 cores of my CPU to work. This took around 3 minutes and 20 seconds to finish.\nThat’s it for now, stay tuned for part 2 where I will analyze this fresh data!"
  },
  {
    "objectID": "posts/2019-01-13-newspapers_mets_alto.html#appendix",
    "href": "posts/2019-01-13-newspapers_mets_alto.html#appendix",
    "title": "Making sense of the METS and ALTO XML standards",
    "section": "\nAppendix\n",
    "text": "Appendix\n\nextract_alto &lt;- function(article){\n    begins &lt;- article[1] %&gt;%\n        extractor(\"(?&lt;=^ID)(.*?)(?=HPOS)\", all = TRUE)\n\n    content &lt;- article %&gt;%\n        extractor(\"(?&lt;=CONTENT)(.*?)(?=WC)\", all = TRUE)\n\n    tibble::tribble(~begins, ~content,\n                    begins, content) %&gt;%\n        unnest()\n}\n\nalto_csv &lt;- function(page_path){\n\n    page &lt;- read_file(page_path)\n\n    doc_name &lt;- str_extract(page_path, \"(?&lt;=/text/).*\")\n\n    alto_articles &lt;- page %&gt;%\n        str_split(\"TextBlock \") %&gt;%\n        flatten_chr()\n\n    alto_df &lt;- map_df(alto_articles, extract_alto)\n\n    alto_df &lt;- alto_df %&gt;%\n        mutate(document = doc_name)\n\n    write_csv(alto_df, paste0(page_path, \".csv\"))\n}\n\n\nalto &lt;- read_file(\"1533660_newspaper_lunion_1860-11-14/text/1860-11-14_01-00001.xml\")\n\n\n# Extract content from alto files\n\npages_alto &lt;- str_match(list.files(path = \"./\", all.files = TRUE, recursive = TRUE), \".*/text/.*.xml\") %&gt;%\n    discard(is.na)\n\n\nlibrary(furrr)\n\nplan(multiprocess, workers = 8)\n\ntic &lt;- Sys.time()\nfuture_map(pages_alto, alto_csv)\ntoc &lt;- Sys.time()\n\ntoc - tic\n\n#Time difference of 18.64776 mins"
  },
  {
    "objectID": "posts/2019-05-18-xml2.html",
    "href": "posts/2019-05-18-xml2.html",
    "title": "For posterity: install {xml2} on GNU/Linux distros",
    "section": "",
    "text": "Today I’ve removed my system’s R package and installed MRO instead. While re-installing all packages, I’ve encountered one of the most frustrating error message for someone installing packages from source:\n\nError : /tmp/Rtmpw60aCp/R.INSTALL7819efef27e/xml2/man/read_xml.Rd:47: unable to load shared object\n'/usr/lib64/R/library/xml2/libs/xml2.so': \nlibicui18n.so.58: cannot open shared object file: No such file or directory ERROR: \ninstalling Rd objects failed for package ‘xml2’ \n\nThis library, libicui18n.so.58 is a pain in the butt. However, you can easily install it if you install miniconda. After installing miniconda, you can look for it with:\n\n[19-05-18 18:26] cbrunos in ~/ ➤ locate libicui18n.so.58\n\n/home/cbrunos/miniconda3/lib/libicui18n.so.58\n/home/cbrunos/miniconda3/lib/libicui18n.so.58.2\n/home/cbrunos/miniconda3/pkgs/icu-58.2-h9c2bf20_1/lib/libicui18n.so.58\n/home/cbrunos/miniconda3/pkgs/icu-58.2-h9c2bf20_1/lib/libicui18n.so.58.2\n\n\nSo now you need to tell R where to look for this library. The following Stackoverflow answer saved the day. Add the following lines to R_HOME/etc/ldpaths (in my case, it was in /opt/microsoft/ropen/3.5.2/lib64/R/etc/):\n\nLD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/username/miniconda3/lib/\nexport LD_LIBRARY_PATH\n\nand try to install xml2 again, and it should work! If not, just abandon the idea of using R and switch to doing data science with VBA, it’ll be less frustrating.\n\n\nSomething else, if you install Microsoft R Open, you’ll be stuck with some older packages, because by default MRO uses a snapshot of CRAN from a given day as a mirror. To get the freshest packages, add the following line to your .Rprofile file (which should be located in your HOME):\n\noptions(repos = c(CRAN = \"http://cran.rstudio.com/\"))\n\nAnd to finish this short blog post, add the following line to your .Rprofile if you get the following error messages when trying to install a package from github:\n\nremotes::install_github('rstudio/DT') Downloading GitHub repo rstudio/DT@master tar: \nThis does not look like a tar archive gzip: stdin: unexpected end of file tar: Child returned \nstatus 1 tar: Error is not recoverable: exiting now tar: This does not look like a tar archive \ngzip: stdin: unexpected end of file tar: Child returned status 1 tar: Error is not recoverable: \nexiting now Error in getrootdir(untar(src, list = TRUE)) : length(file_list) &gt; 0 is not TRUE Calls: \n&lt;Anonymous&gt; ... source_pkg -&gt; decompress -&gt; getrootdir -&gt; stopifnot In addition: Warning messages: 1: \nIn utils::untar(tarfile, ...) : ‘tar -xf '/tmp/RtmpitCFRe/file2677442609b8.tar.gz' -C \n'/tmp/RtmpitCFRe/remotes267752f2629f'’ returned error code 2 2: \nIn system(cmd, intern = TRUE) : running command 'tar -tf '/tmp/RtmpitCFRe/file2677442609b8.tar.gz'' \nhad status 2 Execution halted\n\nThe solution, which can found here\n\noptions(\"download.file.method\" = \"libcurl\")"
  }
]