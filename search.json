[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Econometrics and Free Software",
    "section": "",
    "text": "Welcome to my blog where I talk about R, Nix, Econometrics and Data Science. If you enjoy reading what I write, you might enjoy my books or want to follow me on Mastodon or Twitter or Bluesky. If you are 40+, click here instead. I also make videos on youtube.\n\n\n\n\n\n\n2025\n\n\n\ngithub pages setup for this website\n\n\n\n2024\n\n\n\nhuhu\n\n\nNovember blog post\n\n\n\n2018\n\n\n\n{pmice}, an experimental package for missing data imputation in parallel using {mice} and {furrr}\n\n\nThe year of the GNU+Linux desktop is upon us: using user ratings of Steam Play compatibility to play around with regex and the tidyverse\n\n\nPredicting job search by training a random forest on an unbalanced dataset\n\n\nMissing data imputation and instrumental variables regression: the tidy approach\n\n\nMapping a list of functions to a list of datasets with a list of columns as arguments\n\n\nKeep trying that api call with purrr::possibly()\n\n\nIt’s lists all the way down, part 2: We need to go deeper\n\n\nIt’s lists all the way down\n\n\nImputing missing values in parallel using {furrr}\n\n\nImporting 30GB of data into R with sparklyr\n\n\nHow Luxembourguish residents spend their time: a small {flexdashboard} demo using the Time use survey data\n\n\nGoing from a human readable Excel file to a machine-readable csv with {tidyxl}\n\n\nGetting {sparklyr}, {h2o}, {rsparkling} to work together and some fun with bash\n\n\nGetting the data from the Luxembourguish elections out of Excel\n\n\nGetting data from pdfs using the pdftools package\n\n\nGet basic summary statistics for all the variables in a data frame\n\n\nForecasting my weight with R\n\n\nExporting editable plots from R to Powerpoint: making ggplot2 purrr with officer\n\n\nDealing with heteroskedasticity; regression with robust standard errors using R\n\n\n\n2017\n\n\n\ntidyr::spread() and dplyr::rename_at() in action\n\n\nWhy I find tidyeval useful\n\n\nTeaching the tidyverse to beginners\n\n\nPeace of mind with purrr\n\n\nMy free book has a cover!\n\n\nMake ggplot2 purrr\n\n\nLesser known purrr tricks\n\n\nLesser known dplyr tricks\n\n\nLesser known dplyr 0.7* tricks\n\n\nIntroducing brotools\n\n\nHow to use jailbreakr\n\n\nEasy peasy STATA-like marginal effects with R\n\n\nBuilding formulae\n\n\n\n2016\n\n\n\nWork on lists of datasets instead of individual datasets by using functional programming\n\n\nUnit testing with R\n\n\nRead a lot of datasets at once with R\n\n\nMerge a list of datasets together\n\n\nI’ve started writing a ‘book’: Functional programming and unit testing for data munging with R\n\n\nFunctional programming and unit testing for data munging with R available on Leanpub\n\n\nData frame columns as arguments to dplyr functions\n\n\nCareful with tryCatch\n\n\n\n2015\n\n\n\nUpdate to Introduction to programming econometrics with R\n\n\nIntroduction to programming econometrics with R\n\n\nExport R output to a file\n\n\nBootstrapping standard errors for difference-in-differences estimation with R\n\n\n\n2014\n\n\n\nR, R with Atlas, R with OpenBLAS and Revolution R Open: which is fastest?\n\n\nObject Oriented Programming with R: An example with a Cournot duopoly\n\n\n\n2013\n\n\n\nUsing R as a Computer Algebra System with Ryacas\n\n\nSimulated Maximum Likelihood with R\n\n\nNonlinear Gmm with R - Example with a logistic regression\n\n\nMethod of Simulated Moments with R\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2018-09-08-steam_linux.html",
    "href": "posts/2018-09-08-steam_linux.html",
    "title": "The year of the GNU+Linux desktop is upon us: using user ratings of Steam Play compatibility to play around with regex and the tidyverse",
    "section": "",
    "text": "I’ve been using GNU+Linux distros for about 10 years now, and have settled for openSUSE as my main operating system around 3 years ago, perhaps even more. If you’re a gamer, you might have heard about SteamOS and how more and more games are available on GNU+Linux. I don’t really care about games, I play the occasional one (currently Tangledeep) when I find the time, but still follow the news about gaming on GNU+Linux. Last week, Valve announced something quite big; it is now possible to run Windows games on GNU+Linux directly from Steam, using a modified version of Wine they call Proton. The feature is still in Beta, and Valve announced that they guarantee around 30 games to work already flawlessly. Of course, people have tried running a lot of other games, and, as was to be expected from Free Software and Open Source fans, GNU+Linux gamers created a Google Sheet that lists which games were tried and how they run. You can take a look at the sheet here.\n\n\nIn this blog post, I will play around with this sheet. This blog post lists some {tidyverse} tricks I find useful and use often. Perhaps these tricks will be useful to you too! Let’s start by loading the needed packages:\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(readxl)\n\nSince I’m lazy and don’t want to type the whole name of the file I’ll be using some little regex:\n\nsteam &lt;- read_excel(Sys.glob(\"Steam*\"), sheet = \"Main\", skip = 2)\n\nglimpse(steam)\n## Observations: 8,570\n## Variables: 9\n## $ SteamDB   &lt;chr&gt; \"LINK\", \"LINK\", \"LINK\", \"LINK\", \"LINK\", \"LINK\", \"LINK\"…\n## $ Game      &lt;chr&gt; \"64\", \"1849\", \"1982\", \"1982\", \"am Weapon: Revival\", \".…\n## $ Submitted &lt;chr&gt; \"5 days ago\", \"12 days ago\", \"11 days ago\", \"11 days a…\n## $ Status    &lt;chr&gt; \"Garbage\", \"Platinum\", \"Gold\", \"Platinum\", \"Platinum\",…\n## $ Notes     &lt;chr&gt; \"Crashes with a debug log\", \"Plays OK.\", \"Gamepad supp…\n## $ Distro    &lt;chr&gt; \"Arch (4.18.5)\", \"Manjaro XFCE\", \"Gentoo AMD64 (Kernel…\n## $ Driver    &lt;chr&gt; \"Nvidia 396.54 / Intel xf86-video-intel (1:2.99.917+83…\n## $ Specs     &lt;chr&gt; \"Intel Core i7-7700HQ / Nvidia GTX 1050 (Mobile)\", \"Ry…\n## $ Proton    &lt;chr&gt; \"3.7 Beta\", \"3.7-4 Beta\", \"3.7-4 Beta\", \"Default\", \"3.…\n\nLet’s count how many unique games are in the data:\n\nsteam %&gt;%\n    count(Game)\n## # A tibble: 3,855 x 2\n##    Game                                                                   n\n##    &lt;chr&gt;                                                              &lt;int&gt;\n##  1 .hack//G.U. Last Recode                                                2\n##  2 $1 Ride                                                                1\n##  3 0rbitalis                                                              1\n##  4 10 Second Ninja                                                        4\n##  5 100% Orange Juice                                                     17\n##  6 1000 Amps                                                              3\n##  7 12 Labours of Hercules VII: Fleecing the Fleece (Platinum Edition)     1\n##  8 16bit trader                                                           1\n##  9 1849                                                                   1\n## 10 1953 - KGB Unleased                                                    1\n## # … with 3,845 more rows\n\nThat’s quite a lot of games! However, not everyone of them is playable:\n\nsteam %&gt;%\n    count(Status)\n## # A tibble: 8 x 2\n##   Status       n\n##   &lt;chr&gt;    &lt;int&gt;\n## 1 Borked     205\n## 2 bronze       1\n## 3 Bronze     423\n## 4 Garbage   2705\n## 5 Gold       969\n## 6 Platinum  2596\n## 7 Primary      1\n## 8 Silver    1670\n\nAround 2500 have the status “Platinum”, but some games might have more than one status:\n\nsteam %&gt;%\n    filter(Game == \"100% Orange Juice\") %&gt;%\n    count(Status)\n## # A tibble: 5 x 2\n##   Status       n\n##   &lt;chr&gt;    &lt;int&gt;\n## 1 Bronze       5\n## 2 Garbage      3\n## 3 Gold         2\n## 4 Platinum     6\n## 5 Silver       1\n\nMore games run like Garbage than Platinum. But perhaps we can dig a little deeper and see if we find some patterns.\n\n\nLet’s take a look at the GNU+Linux distros:\n\nsteam %&gt;%\n    count(Distro) \n## # A tibble: 2,085 x 2\n##    Distro                                         n\n##    &lt;chr&gt;                                      &lt;int&gt;\n##  1 &lt;NA&gt;                                           1\n##  2 ?                                              2\n##  3 \"\\\"Arch Linux\\\" (64 bit)\"                      1\n##  4 \"\\\"Linux Mint 18.3 Sylvia 64bit\"               1\n##  5 \"\\\"Manjaro Stable 64-bit (Kernel 4.14.66)\"     1\n##  6 \"\\\"Solus\\\" (64 bit)\"                           2\n##  7 (K)ubuntu 18.04 64-bit (Kernel 4.15.0)         2\n##  8 (L)Ubuntu 18.04.1 LTS                          1\n##  9 18.04.1                                        1\n## 10 18.04.1 LTS                                    2\n## # … with 2,075 more rows\n\nOk the distro column is pretty messy. Let’s try to bring some order to it:\n\nsteam %&lt;&gt;%\n    mutate(distribution = as_factor(case_when(\n        grepl(\"buntu|lementary|antergos|steam|mint|18.|pop|neon\", Distro, ignore.case = TRUE) ~ \"Ubuntu\",\n        grepl(\"arch|manjaro\", Distro, ignore.case = TRUE) ~ \"Arch Linux\",\n        grepl(\"gentoo\", Distro, ignore.case = TRUE) ~ \"Gentoo\",\n        grepl(\"fedora\", Distro, ignore.case = TRUE) ~ \"Fedora\",\n        grepl(\"suse\", Distro, ignore.case = TRUE) ~ \"openSUSE\",\n        grepl(\"debian|sid|stretch|lmde\", Distro, ignore.case = TRUE) ~ \"Debian\",\n        grepl(\"solus\", Distro, ignore.case = TRUE) ~ \"Solus\",\n        grepl(\"slackware\", Distro, ignore.case = TRUE) ~ \"Slackware\",\n        grepl(\"void\", Distro, ignore.case = TRUE) ~ \"Void Linux\",\n        TRUE ~ \"Other\"\n    )))\n\nThe %&lt;&gt;% operator is shorthand for a &lt;- a %&gt;% f(). It passes a to f() and assigns the result back to a. Anyways, let’s take a look at the distribution column:\n\nsteam %&gt;%\n    count(distribution)\n## # A tibble: 10 x 2\n##    distribution     n\n##    &lt;fct&gt;        &lt;int&gt;\n##  1 Ubuntu        6632\n##  2 Arch Linux     805\n##  3 Solus          175\n##  4 Debian         359\n##  5 Fedora         355\n##  6 Gentoo          42\n##  7 Void Linux      38\n##  8 Other           76\n##  9 openSUSE        66\n## 10 Slackware       22\n\nI will group distributions that have less than 100 occurrences into a single category (meaning I will keep the 5 more common values):\n\nsteam %&lt;&gt;%\n    mutate(distribution = fct_lump(distribution, n = 5, other_level = \"Other\")) \n\nsteam %&gt;%\n    count(distribution)\n## # A tibble: 6 x 2\n##   distribution     n\n##   &lt;fct&gt;        &lt;int&gt;\n## 1 Ubuntu        6632\n## 2 Arch Linux     805\n## 3 Solus          175\n## 4 Debian         359\n## 5 Fedora         355\n## 6 Other          244\n\nLet’s do the same for the CPUs:\n\nsteam %&lt;&gt;%\n    mutate(CPU = as_factor(case_when(\n        grepl(\"intel|i\\\\d|xeon|core2|\\\\d{4}k|q\\\\d{4}|pentium\", Specs, ignore.case = TRUE) ~ \"Intel\",\n        grepl(\"ryzen|threadripper|tr|amd|fx|r\\\\d|\\\\d{4}x|phenom\", Specs, ignore.case = TRUE) ~ \"AMD\",\n        TRUE ~ NA_character_\n    )))\n\nsteam %&gt;%\n    count(CPU)\n## Warning: Factor `CPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n## # A tibble: 3 x 2\n##   CPU       n\n##   &lt;fct&gt; &lt;int&gt;\n## 1 Intel  5768\n## 2 AMD    2319\n## 3 &lt;NA&gt;    483\n\nAnd the same for the GPUs:\n\nsteam %&lt;&gt;%\n    mutate(GPU = as_factor(case_when(\n        grepl(\"nvidia|geforce|3\\\\d{2}|nouveau|gtx|gt\\\\s?\\\\d{1,}|9\\\\d0|1060|1070|1080\", Specs, ignore.case = TRUE) ~ \"Nvidia\",\n        grepl(\"amd|radeon|ati|rx|vega|r9\", Specs, ignore.case = TRUE) ~ \"AMD\",\n        grepl(\"intel|igpu|integrated|hd\\\\d{4}|hd\\\\sgraphics\", Specs, ignore.case = TRUE) ~ \"Intel\",\n        TRUE ~ NA_character_\n    )))\n\nsteam %&gt;%\n    count(GPU)\n## Warning: Factor `GPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n## # A tibble: 4 x 2\n##   GPU        n\n##   &lt;fct&gt;  &lt;int&gt;\n## 1 Nvidia  6086\n## 2 AMD     1374\n## 3 Intel    413\n## 4 &lt;NA&gt;     697\n\nI will also add a rank for the Status column:\n\nsteam %&lt;&gt;%\n    mutate(rank_status = case_when(\n        Status == \"Platinum\" ~ 5,\n        Status == \"Gold\" ~ 4,\n        Status == \"Silver\" ~ 3,\n        Status == \"Bronze\" ~ 2,\n        Status == \"Garbage\" ~ 1\n    ))\n\nNow, what are the top 5 most frequent combinations of Status, distribution, CPU and GPU?\n\nsteam %&gt;%\n    filter(!is.na(CPU), !is.na(GPU)) %&gt;%\n    count(Status, distribution, CPU, GPU) %&gt;%\n    mutate(total = sum(n)) %&gt;%\n    mutate(freq = n / total) %&gt;%\n    top_n(5)\n## Selecting by freq\n## # A tibble: 5 x 7\n##   Status   distribution CPU   GPU        n total   freq\n##   &lt;chr&gt;    &lt;fct&gt;        &lt;fct&gt; &lt;fct&gt;  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n## 1 Garbage  Ubuntu       Intel Nvidia  1025  7443 0.138 \n## 2 Gold     Ubuntu       Intel Nvidia   361  7443 0.0485\n## 3 Platinum Ubuntu       Intel Nvidia  1046  7443 0.141 \n## 4 Platinum Ubuntu       AMD   Nvidia   338  7443 0.0454\n## 5 Silver   Ubuntu       Intel Nvidia   650  7443 0.0873\n\nUnsurprisingly, Ubuntu, or distributions using Ubuntu as a base, are the most popular ones. Nvidia is the most popular GPU, Intel for CPUs and in most cases, this combo of hardware and distribution is associated with positive ratings (even though there are almost as many “Garbage” ratings than “Platinum” ratings).\n\n\nNow let’s compute some dumb averages of Statuses by distribution, CPU and GPU. Since I’m going to run the same computation three times, I’ll write a function to do that.\n\ncompute_avg &lt;- function(dataset, var){\n    var &lt;- enquo(var)\n    dataset %&gt;%\n        select(rank_status, (!!var)) %&gt;%\n        group_by((!!var)) %&gt;%\n        mutate(wt = n()) %&gt;%\n        summarise(average_rating = weighted.mean(rank_status, (!!var), wt, na.rm = TRUE))\n}\n\nLet’s see now if we can rank distribution by Steam play rating:\n\ncompute_avg(steam, distribution)\n## # A tibble: 6 x 2\n##   distribution average_rating\n##   &lt;fct&gt;                 &lt;dbl&gt;\n## 1 Ubuntu                 3.03\n## 2 Arch Linux             3.05\n## 3 Solus                  3.03\n## 4 Debian                 3.01\n## 5 Fedora                 3.07\n## 6 Other                  3.16\n\nHow about for hardware?\n\ncompute_avg(steam, GPU)\n## Warning: Factor `GPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n\n## Warning: Factor `GPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n## # A tibble: 4 x 2\n##   GPU    average_rating\n##   &lt;fct&gt;           &lt;dbl&gt;\n## 1 Nvidia           3.07\n## 2 AMD              2.90\n## 3 Intel            3.01\n## 4 &lt;NA&gt;            NA\ncompute_avg(steam, CPU)\n## Warning: Factor `CPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n\n## Warning: Factor `CPU` contains implicit NA, consider using\n## `forcats::fct_explicit_na`\n## # A tibble: 3 x 2\n##   CPU   average_rating\n##   &lt;fct&gt;          &lt;dbl&gt;\n## 1 Intel           3.03\n## 2 AMD             3.06\n## 3 &lt;NA&gt;           NA\n\nTo wrap this up, what are the games with the most ratings? Perhaps this can give us a hint about which games GNU+Linux users prefer:\n\nsteam %&gt;%\n    count(Game) %&gt;%\n    top_n(10)\n## Selecting by n\n## # A tibble: 10 x 2\n##    Game                              n\n##    &lt;chr&gt;                         &lt;int&gt;\n##  1 Age of Empires II: HD Edition    43\n##  2 Borderlands                      39\n##  3 DiRT 3 Complete Edition          32\n##  4 DOOM                             62\n##  5 Fallout: New Vegas               45\n##  6 Grim Dawn                        34\n##  7 No Man's Sky                     40\n##  8 Path of Exile                    35\n##  9 Quake Champions                  32\n## 10 The Elder Scrolls V: Skyrim      46\n\nI actually laughed out loud when I saw that DOOM was the game with the most ratings! What else was I expecting, really."
  },
  {
    "objectID": "posts/2016-12-24-functional-programming-and-unit-testing-for-data-munging-with-r-available-on-leanpub.html",
    "href": "posts/2016-12-24-functional-programming-and-unit-testing-for-data-munging-with-r-available-on-leanpub.html",
    "title": "Functional programming and unit testing for data munging with R available on Leanpub",
    "section": "",
    "text": "The book I’ve been working on these pasts months (you can read about it here, and read it for free here) is now available on Leanpub! You can grab a copy and read it on your ebook reader or on your computer, and what’s even better is that it is available for free (but you can also decide to buy it if you really like it). Here is the link on Leanpub.\nIn the book, I show you the basics of functional programming, unit testing and package development for the R programming language. The end goal is to make your data tidy in a reproducible way!\nJust a heads up: as the book is right now, the formatting is not perfect and images are missing. This is because I use bookdown to write the book and convert it to Leanpub’s markdown flavour is not trivial. I will find a solution to automate the conversion from bookdown’s version to Leanpub’s markdown and try to keep both versions in sync. Of course, once the book will be finished, the version on Leanpub and on my website are going to be completely identical. If you want to read it on your computer offline, you can also download a pdf from the book’s website, by clicking on the pdf icon in the top left corner. Do not hesitate to send me an email if you want to give me feedback (just click on the red envelope in the top right corner) or tweet me @brodriguesco."
  },
  {
    "objectID": "posts/2018-10-21-lux_elections.html",
    "href": "posts/2018-10-21-lux_elections.html",
    "title": "Getting the data from the Luxembourguish elections out of Excel",
    "section": "",
    "text": "In this blog post, similar to a previous blog post I am going to show you how we can go from an Excel workbook that contains data to flat file. I will taking advantage of the structure of the tables inside the Excel sheets by writing a function that extracts the tables and then mapping it to each sheet!\n\n\nLast week, October 14th, Luxembourguish nationals went to the polls to elect the Grand Duke! No, actually, the Grand Duke does not get elected. But Luxembourguish citizen did go to the polls to elect the new members of the Chamber of Deputies (a sort of parliament if you will). The way the elections work in Luxembourg is quite interesting; you can vote for a party, or vote for individual candidates from different parties. The candidates that get the most votes will then seat in the parliament. If you vote for a whole party, each of the candidates get a vote. You get as many votes as there are candidates to vote for. So, for example, if you live in the capital city, also called Luxembourg, you get 21 votes to distribute. You could decide to give 10 votes to 10 candidates of party A and 11 to 11 candidates of party B. Why 21 votes? The chamber of Deputies is made up 60 deputies, and the country is divided into four legislative circonscriptions. So each voter in a circonscription gets an amount of votes that is proportional to the population size of that circonscription.\n\n\nNow you certainly wonder why I put the flag of Gambia on top of this post? This is because the government that was formed after the 2013 elections was made up of a coalition of 3 parties; the Luxembourg Socialist Worker’s Party, the Democratic Party and The Greens. The LSAP managed to get 13 seats in the Chamber, while the DP got 13 and The Greens 6, meaning 32 seats out of 60. So because they made this coalition, they could form the government, and this coalition was named the Gambia coalition because of the colors of these 3 parties: red, blue and green. If you want to take a look at the ballot from 2013 for the southern circonscription, click here.\n\n\nNow that you have the context, we can go back to some data science. The results of the elections of last week can be found on Luxembourg’s Open Data portal, right here. The data is trapped inside Excel sheets; just like I explained in a previous blog post the data is easily read by human, but not easily digested by any type of data analysis software. So I am going to show you how we are going from this big Excel workbook to a flat file.\n\n\nFirst of all, if you open the Excel workbook, you will notice that there are a lot of sheets; there is one for the whole country, named “Le Grand-Duché de Luxembourg”, one for the four circonscriptions, “Centre”, “Nord”, “Sud”, “Est” and 102 more for each commune of the country (a commune is an administrative division). However, the tables are all very similarly shaped, and roughly at the same position.\n\n\n\n\n\nThis is good, because we can write a function to extracts the data and then map it over all the sheets. First, let’s load some packages and the data for the country:\n\nlibrary(\"tidyverse\")\nlibrary(\"tidyxl\")\nlibrary(\"brotools\")\n# National Level 2018\nelections_raw_2018 &lt;- xlsx_cells(\"leg-2018-10-14-22-58-09-737.xlsx\",\n                        sheets = \"Le Grand-Duché de Luxembourg\")\n\n{brotools} is my own package. You can install it with:\n\ndevtools::install_github(\"b-rodrigues/brotools\")\n\nit contains a function that I will use down below. The function I wrote to extract the tables is not very complex, but requires that you are familiar with how {tidyxl} imports Excel workbooks. So if you are not familiar with it, study the imported data frame for a few minutes. It will make understanding the next function easier:\n\nextract_party &lt;- function(dataset, starting_col, target_rows){\n\n    almost_clean &lt;- dataset %&gt;%\n        filter(row %in% target_rows) %&gt;%\n        filter(col %in% c(starting_col, starting_col + 1)) %&gt;%\n        select(character, numeric) %&gt;%\n        fill(numeric, .direction = \"up\") %&gt;%\n        filter(!is.na(character))\n\n    party_name &lt;- almost_clean$character[1] %&gt;%\n        str_split(\"-\", simplify = TRUE) %&gt;%\n        .[2] %&gt;%\n        str_trim()\n\n    almost_clean$character[1] &lt;- \"Pourcentage\"\n\n    almost_clean$party &lt;- party_name\n\n    colnames(almost_clean) &lt;- c(\"Variables\", \"Values\", \"Party\")\n\n    almost_clean %&gt;%\n        mutate(Year = 2018) %&gt;%\n        select(Party, Year, Variables, Values)\n\n}\n\nThis function has three arguments, dataset, starting_col and target_rows. dataset is the data I loaded with xlsx_cells from the {tidyxl} package. I think the following picture illustrates easily what the function does:\n\n\n\n\n\nSo the function first filters only the rows we are interested in, then the cols. I then select the columns I want which are called character and numeric (if the Excel cell contains characters then you will find them in the character column, if it contains numbers you will them in the numeric column), then I fill the empty cells with the values from the numeric column and the I remove the NA’s. These two last steps might not be so clear; this is how the data looks like up until the select() function:\n\n&gt; elections_raw_2018 %&gt;%\n+     filter(row %in% seq(11,19)) %&gt;%\n+     filter(col %in% c(1, 2)) %&gt;%\n+     select(character, numeric)\n# A tibble: 18 x 2\n   character                       numeric\n   &lt;chr&gt;                             &lt;dbl&gt;\n 1 1 - PIRATEN - PIRATEN           NA     \n 2 NA                               0.0645\n 3 Suffrage total                  NA     \n 4 NA                          227549     \n 5 Suffrages de liste              NA     \n 6 NA                          181560     \n 7 Suffrage nominatifs             NA     \n 8 NA                           45989     \n 9 Pourcentage pondéré             NA     \n10 NA                               0.0661\n11 Suffrage total pondéré          NA     \n12 NA                           13394.    \n13 Suffrages de liste pondéré      NA     \n14 NA                           10308     \n15 Suffrage nominatifs pondéré     NA     \n16 NA                            3086.    \n17 Mandats attribués               NA     \n18 NA                               2  \n\nSo by filling the NA’s in the numeric the data now looks like this:\n\n&gt; elections_raw_2018 %&gt;%\n+     filter(row %in% seq(11,19)) %&gt;%\n+     filter(col %in% c(1, 2)) %&gt;%\n+     select(character, numeric) %&gt;%\n+     fill(numeric, .direction = \"up\")\n# A tibble: 18 x 2\n   character                       numeric\n   &lt;chr&gt;                             &lt;dbl&gt;\n 1 1 - PIRATEN - PIRATEN            0.0645\n 2 NA                               0.0645\n 3 Suffrage total              227549     \n 4 NA                          227549     \n 5 Suffrages de liste          181560     \n 6 NA                          181560     \n 7 Suffrage nominatifs          45989     \n 8 NA                           45989     \n 9 Pourcentage pondéré              0.0661\n10 NA                               0.0661\n11 Suffrage total pondéré       13394.    \n12 NA                           13394.    \n13 Suffrages de liste pondéré   10308     \n14 NA                           10308     \n15 Suffrage nominatifs pondéré   3086.    \n16 NA                            3086.    \n17 Mandats attribués                2     \n18 NA                               2 \n\nAnd then I filter out the NA’s from the character column, and that’s almost it! I simply need to add a new column with the party’s name and rename the other columns. I also add a “Year” colmun.\n\n\nNow, each party will have a different starting column. The table with the data for the first party starts on column 1, for the second party it starts on column 4, column 7 for the third party… So the following vector contains all the starting columns:\n\nposition_parties_national &lt;- seq(1, 24, by = 3)\n\n(If you study the Excel workbook closely, you will notice that I do not extract the last two parties. This is because these parties were not present in all of the 4 circonscriptions and are very, very, very small.)\n\n\nThe target rows are always the same, from 11 to 19. Now, I simply need to map this function to this list of positions and I get the data for all the parties:\n\nelections_national_2018 &lt;- map_df(position_parties_national, extract_party, \n                         dataset = elections_raw_2018, target_rows = seq(11, 19)) %&gt;%\n    mutate(locality = \"Grand-Duchy of Luxembourg\", division = \"National\")\n\nI also added the locality and division columns to the data.\n\n\nLet’s take a look:\n\nglimpse(elections_national_2018)\n## Observations: 72\n## Variables: 6\n## $ Party     &lt;chr&gt; \"PIRATEN\", \"PIRATEN\", \"PIRATEN\", \"PIRATEN\", \"PIRATEN\",…\n## $ Year      &lt;dbl&gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, …\n## $ Variables &lt;chr&gt; \"Pourcentage\", \"Suffrage total\", \"Suffrages de liste\",…\n## $ Values    &lt;dbl&gt; 6.446204e-02, 2.275490e+05, 1.815600e+05, 4.598900e+04…\n## $ locality  &lt;chr&gt; \"Grand-Duchy of Luxembourg\", \"Grand-Duchy of Luxembour…\n## $ division  &lt;chr&gt; \"National\", \"National\", \"National\", \"National\", \"Natio…\n\nVery nice.\n\n\nNow we need to do the same for the 4 electoral circonscriptions. First, let’s load the data:\n\n# Electoral districts 2018\ndistricts &lt;- c(\"Centre\", \"Nord\", \"Sud\", \"Est\")\n\nelections_district_raw_2018 &lt;- xlsx_cells(\"leg-2018-10-14-22-58-09-737.xlsx\",\n                                      sheets = districts)\n\nNow things get trickier. Remember I said that the number of seats is proportional to the population of each circonscription? We simply can’t use the same target rows as before. For example, for the “Centre” circonscription, the target rows go from 12 to 37, but for the “Est” circonscription only from 12 to 23. Ideally, we would need a function that would return the target rows.\n\n\nThis is that function:\n\n# The target rows I need to extract are different from district to district\nget_target_rows &lt;- function(dataset, sheet_to_extract, reference_address){\n\n    last_row &lt;- dataset %&gt;%\n        filter(sheet == sheet_to_extract) %&gt;%\n        filter(address == reference_address) %&gt;%\n        pull(numeric)\n\n    seq(12, (11 + 5 + last_row))\n}\n\nThis function needs a dataset, a sheet_to_extract and a reference_address. The reference address is a cell that actually contains the number of seats in that circonscription, in our case “B5”. We can easily get the list of target rows now:\n\n# Get the target rows\nlist_targets &lt;- map(districts, get_target_rows, dataset = elections_district_raw_2018, \n                    reference_address = \"B5\")\n\nlist_targets\n## [[1]]\n##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34\n## [24] 35 36 37\n## \n## [[2]]\n##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n## \n## [[3]]\n##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34\n## [24] 35 36 37 38 39\n## \n## [[4]]\n##  [1] 12 13 14 15 16 17 18 19 20 21 22 23\n\nNow, let’s split the data we imported into a list, where each element of the list is a dataframe with the data from one circonscription:\n\nlist_data_districts &lt;- map(districts, ~filter(.data = elections_district_raw_2018, sheet == .)) \n\nNow I can easily map the function I defined above, extract_party to this list of datasets. Well, I say easily, but it’s a bit more complicated than before because I have now a list of datasets and a list of target rows:\n\nelections_district_2018 &lt;- map2(.x = list_data_districts, .y = list_targets,\n     ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = .y))\n\nThe way to understand this is that for each element of list_data_districts and list_targets, I have to map extract_party to each element of position_parties_national. This gives the intented result:\n\nelections_district_2018\n## [[1]]\n## # A tibble: 208 x 4\n##    Party    Year Variables               Values\n##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;\n##  1 PIRATEN  2018 Pourcentage             0.0514\n##  2 PIRATEN  2018 CLEMENT Sven (1)     8007     \n##  3 PIRATEN  2018 WEYER Jerry (2)      3446     \n##  4 PIRATEN  2018 CLEMENT Pascal (3)   3418     \n##  5 PIRATEN  2018 KUNAKOVA Lucie (4)   2860     \n##  6 PIRATEN  2018 WAMPACH Jo (14)      2693     \n##  7 PIRATEN  2018 LAUX Cynthia (6)     2622     \n##  8 PIRATEN  2018 ISEKIN Christian (5) 2610     \n##  9 PIRATEN  2018 SCHWEICH Georges (9) 2602     \n## 10 PIRATEN  2018 LIESCH Mireille (8)  2551     \n## # … with 198 more rows\n## \n## [[2]]\n## # A tibble: 112 x 4\n##    Party    Year Variables                             Values\n##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                                  &lt;dbl&gt;\n##  1 PIRATEN  2018 Pourcentage                           0.0767\n##  2 PIRATEN  2018 COLOMBERA Jean (2)                 5074     \n##  3 PIRATEN  2018 ALLARD Ben (1)                     4225     \n##  4 PIRATEN  2018 MAAR Andy (3)                      2764     \n##  5 PIRATEN  2018 GINTER Joshua (8)                  2536     \n##  6 PIRATEN  2018 DASBACH Angelika (4)               2473     \n##  7 PIRATEN  2018 GRÜNEISEN Sam (6)                  2408     \n##  8 PIRATEN  2018 BAUMANN Roy (5)                    2387     \n##  9 PIRATEN  2018 CONRAD Pierre (7)                  2280     \n## 10 PIRATEN  2018 TRAUT ép. MOLITOR Angela Maria (9) 2274     \n## # … with 102 more rows\n## \n## [[3]]\n## # A tibble: 224 x 4\n##    Party    Year Variables                    Values\n##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt;\n##  1 PIRATEN  2018 Pourcentage                  0.0699\n##  2 PIRATEN  2018 GOERGEN Marc (1)          9818     \n##  3 PIRATEN  2018 FLOR Starsky (2)          6737     \n##  4 PIRATEN  2018 KOHL Martine (3)          6071     \n##  5 PIRATEN  2018 LIESCH Camille (4)        6025     \n##  6 PIRATEN  2018 KOHL Sylvie (6)           5628     \n##  7 PIRATEN  2018 WELTER Christian (5)      5619     \n##  8 PIRATEN  2018 DA GRAÇA DIAS Yanick (10) 5307     \n##  9 PIRATEN  2018 WEBER Jules (7)           5301     \n## 10 PIRATEN  2018 CHMELIK Libor (8)         5247     \n## # … with 214 more rows\n## \n## [[4]]\n## # A tibble: 96 x 4\n##    Party    Year Variables                           Values\n##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                                &lt;dbl&gt;\n##  1 PIRATEN  2018 Pourcentage                         0.0698\n##  2 PIRATEN  2018 FRÈRES Daniel (1)                4152     \n##  3 PIRATEN  2018 CLEMENT Jill (7)                 1943     \n##  4 PIRATEN  2018 HOUDREMONT Claire (2)            1844     \n##  5 PIRATEN  2018 BÖRGER Nancy (3)                 1739     \n##  6 PIRATEN  2018 MARTINS DOS SANTOS Catarina (6)  1710     \n##  7 PIRATEN  2018 BELLEVILLE Tatjana (4)           1687     \n##  8 PIRATEN  2018 CONTRERAS Gerald (5)             1687     \n##  9 PIRATEN  2018 Suffrages total                 14762     \n## 10 PIRATEN  2018 Suffrages de liste              10248     \n## # … with 86 more rows\n\nI now need to add the locality and division columns:\n\nelections_district_2018 &lt;- map2(.y = elections_district_2018, .x = districts, \n     ~mutate(.y, locality = .x, division = \"Electoral district\")) %&gt;%\n    bind_rows()\n\nWe’re almost done! Now we need to do the same for the 102 remaining sheets, one for each commune of Luxembourg. This will now go very fast, because we got all the building blocks from before:\n\ncommunes &lt;- xlsx_sheet_names(\"leg-2018-10-14-22-58-09-737.xlsx\")\n\ncommunes &lt;- communes %-l% \n    c(\"Le Grand-Duché de Luxembourg\", \"Centre\", \"Est\", \"Nord\", \"Sud\", \"Sommaire\")\n\nLet me introduce the following function: %-l%. This function removes elements from lists:\n\nc(\"a\", \"b\", \"c\", \"d\") %-l% c(\"a\", \"d\")\n## [1] \"b\" \"c\"\n\nYou can think of it as “minus for lists”. This is called an infix operator.\n\n\nSo this function is very useful to get the list of communes, and is part of my package, {brotools}.\n\n\nAs before, I load the data:\n\nelections_communes_raw_2018 &lt;- xlsx_cells(\"leg-2018-10-14-22-58-09-737.xlsx\",\n                                 sheets = communes)\n\nThen get my list of targets, but I need to change the reference address. It’s “B8” now, not “B7”.\n\n# Get the target rows\nlist_targets &lt;- map(communes, get_target_rows, \n                    dataset = elections_communes_raw_2018, reference_address = \"B8\")\n\nI now create a list of communes by mapping a filter function to the data:\n\nlist_data_communes &lt;- map(communes, ~filter(.data = elections_communes_raw_2018, sheet == .)) \n\nAnd just as before, I get the data I need by using extract_party, and adding the “locality” and “division” columns:\n\nelections_communes_2018 &lt;- map2(.x = list_data_communes, .y = list_targets,\n                                ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = .y))\n\nelections_communes_2018 &lt;- map2(.y = elections_communes_2018, .x = communes,\n                                ~mutate(.y, locality = .x, division = \"Commune\")) %&gt;%\n    bind_rows()\n\nThe steps are so similar for the four circonscriptions and for the 102 communes that I could have write a big wrapper function and the use it for the circonscription and communes at once. But I was lazy.\n\n\nFinally, I bind everything together and have a nice, tidy, flat file:\n\n# Final results\n\nelections_2018 &lt;- bind_rows(list(elections_national_2018, elections_district_2018, elections_communes_2018))\n\nglimpse(elections_2018)\n## Observations: 15,544\n## Variables: 6\n## $ Party     &lt;chr&gt; \"PIRATEN\", \"PIRATEN\", \"PIRATEN\", \"PIRATEN\", \"PIRATEN\",…\n## $ Year      &lt;dbl&gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, …\n## $ Variables &lt;chr&gt; \"Pourcentage\", \"Suffrage total\", \"Suffrages de liste\",…\n## $ Values    &lt;dbl&gt; 6.446204e-02, 2.275490e+05, 1.815600e+05, 4.598900e+04…\n## $ locality  &lt;chr&gt; \"Grand-Duchy of Luxembourg\", \"Grand-Duchy of Luxembour…\n## $ division  &lt;chr&gt; \"National\", \"National\", \"National\", \"National\", \"Natio…\n\nThis blog post is already quite long, so I will analyze the data now that R can easily ingest it in a future blog post."
  },
  {
    "objectID": "posts/2018-10-05-ggplot2_purrr_officer.html",
    "href": "posts/2018-10-05-ggplot2_purrr_officer.html",
    "title": "Exporting editable plots from R to Powerpoint: making ggplot2 purrr with officer",
    "section": "",
    "text": "A kind reader let me know that the function create_pptx() is now outdated, and proposed an update which you can find here: here. Thank you @Jeremy!\n\n\nI was recently confronted to the following problem: creating hundreds of plots that could still be edited by our client. What this meant was that I needed to export the graphs in Excel or Powerpoint or some other such tool that was familiar to the client, and not export the plots directly to pdf or png as I would normally do. I still wanted to use R to do it though, because I could do what I always do to when I need to perform repetitive tasks such as producing hundreds of plots; map over a list of, say, countries, and make one plot per country. This is something I discussed in a previous blog post, Make ggplot2 purrr.\n\n\nSo, after some online seaching, I found the {officer} package. This package allows you to put objects into Microsoft documents. For example, editable plots in a Powerpoint document. This is what I will show in this blog post.\n\n\nLet’s start by loading the required packages:\n\nlibrary(\"tidyverse\")\nlibrary(\"officer\")\nlibrary(\"rvg\")\n\nThen, I will use the data from the time use survey, which I discussed in a previous blog post Going from a human readable Excel file to a machine-readable csv with {tidyxl}.\n\n\nYou can download the data here.\n\n\nLet’s import and prepare it:\n\ntime_use &lt;- rio::import(\"clean_data.csv\")\n\n\ntime_use &lt;- time_use %&gt;%\n    filter(population %in% c(\"Male\", \"Female\")) %&gt;%\n    filter(activities %in% c(\"Personal care\", \"Sleep\", \"Eating\", \n                             \"Employment\", \"Household and family care\")) %&gt;%\n    group_by(day) %&gt;%\n    nest()\n\nI only kept two categories, “Male” and “Female” and 5 activities. Then I grouped by day and nested the data. This is how it looks like:\n\ntime_use\n## # A tibble: 3 x 2\n##   day                         data             \n##   &lt;chr&gt;                       &lt;list&gt;           \n## 1 Year 2014_Monday til Friday &lt;tibble [10 × 4]&gt;\n## 2 Year 2014_Saturday          &lt;tibble [10 × 4]&gt;\n## 3 Year 2014_Sunday            &lt;tibble [10 × 4]&gt;\n\nAs shown, time_use is a tibble with 2 columns, the first day contains the days, and the second data, is of type list, and each element of these lists are tibbles themselves. Let’s take a look inside one:\n\ntime_use$data[1]\n## [[1]]\n## # A tibble: 10 x 4\n##    population activities                time  time_in_minutes\n##    &lt;chr&gt;      &lt;chr&gt;                     &lt;chr&gt;           &lt;int&gt;\n##  1 Male       Personal care             11:00             660\n##  2 Male       Sleep                     08:24             504\n##  3 Male       Eating                    01:46             106\n##  4 Male       Employment                08:11             491\n##  5 Male       Household and family care 01:59             119\n##  6 Female     Personal care             11:15             675\n##  7 Female     Sleep                     08:27             507\n##  8 Female     Eating                    01:48             108\n##  9 Female     Employment                06:54             414\n## 10 Female     Household and family care 03:49             229\n\nI can now create plots for each of the days with the following code:\n\nmy_plots &lt;- time_use %&gt;%\n    mutate(plots = map2(.y = day, .x = data, ~ggplot(data = .x) + theme_minimal() +\n                       geom_col(aes(y = time_in_minutes, x = activities, fill = population), \n                                position = \"dodge\") +\n                       ggtitle(.y) +\n                       ylab(\"Time in minutes\") +\n                       xlab(\"Activities\")))\n\nThese steps are all detailled in my blog post Make ggplot2 purrr. Let’s take a look at my_plots:\n\nmy_plots\n## # A tibble: 3 x 3\n##   day                         data              plots \n##   &lt;chr&gt;                       &lt;list&gt;            &lt;list&gt;\n## 1 Year 2014_Monday til Friday &lt;tibble [10 × 4]&gt; &lt;gg&gt;  \n## 2 Year 2014_Saturday          &lt;tibble [10 × 4]&gt; &lt;gg&gt;  \n## 3 Year 2014_Sunday            &lt;tibble [10 × 4]&gt; &lt;gg&gt;\n\nThe last column, called plots is a list where each element is a plot! We can take a look at one:\n\nmy_plots$plots[1]\n## [[1]]\n\n\n\n\nNow, this is where I could export these plots as pdfs or pngs. But this is not what I need. I need to export these plots as editable charts for Powerpoint. To do this for one image, I would do the following (as per {officer}’s documentation):\n\nread_pptx() %&gt;%\n    add_slide(layout = \"Title and Content\", master = \"Office Theme\") %&gt;%\n    ph_with_vg(code = print(one_plot), type = \"body\") %&gt;% \n    print(target = path)\n\nTo map this over a list of arguments, I wrote a wrapper:\n\ncreate_pptx &lt;- function(plot, path){\n    if(!file.exists(path)) {\n        out &lt;- read_pptx()\n    } else {\n        out &lt;- read_pptx(path)\n    }\n    \n    out %&gt;%\n        add_slide(layout = \"Title and Content\", master = \"Office Theme\") %&gt;%\n        ph_with_vg(code = print(plot), type = \"body\") %&gt;% \n        print(target = path)\n}\n\nThis function takes two arguments, plot and path. plot must be an plot object such as the ones contained inside the plots column of my_plots tibble. path is the path of where I want to save the pptx.\n\n\nThe first lines check if the file exists, if yes, the slides get added to the existing file, if not a new pptx gets created. The rest of the code is very similar to the one from the documentation. Now, to create my pptx I simple need to map over the plots column and provide a path:\n\nmap(my_plots$plots, create_pptx, path = \"test.pptx\")\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n\n## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,\n## options = options): Failed to parse QName 'xsi:xmlns:' [202]\n## [[1]]\n## [1] \"/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx\"\n## \n## [[2]]\n## [1] \"/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx\"\n## \n## [[3]]\n## [1] \"/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx\"\n\nHere is the end result:\n\n\n\n\n\nInside Powerpoint (or in this case Libreoffice), the plots are geometric shapes that can now be edited!"
  },
  {
    "objectID": "posts/2024-11-09-haha.html",
    "href": "posts/2024-11-09-haha.html",
    "title": "November blog post",
    "section": "",
    "text": "Undesired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\n\n1 + 1"
  },
  {
    "objectID": "posts/2018-07-01-tidy_ive.html",
    "href": "posts/2018-07-01-tidy_ive.html",
    "title": "Missing data imputation and instrumental variables regression: the tidy approach",
    "section": "",
    "text": "In this blog post I will discuss missing data imputation and instrumental variables regression. This is based on a short presentation I will give at my job. You can find the data used here on this website: http://eclr.humanities.manchester.ac.uk/index.php/IV_in_R\n\n\nThe data is used is from Wooldridge’s book, Econometrics: A modern Approach. You can download the data by clicking here.\n\n\nThis is the variable description:\n\n 1. inlf                     =1 if in labor force, 1975\n 2. hours                    hours worked, 1975\n 3. kidslt6                  # kids &lt; 6 years\n 4. kidsge6                  # kids 6-18\n 5. age                      woman's age in yrs\n 6. educ                     years of schooling\n 7. wage                     estimated wage from earns., hours\n 8. repwage                  reported wage at interview in 1976\n 9. hushrs                   hours worked by husband, 1975\n10. husage                   husband's age\n11. huseduc                  husband's years of schooling\n12. huswage                  husband's hourly wage, 1975\n13. faminc                   family income, 1975\n14. mtr                      fed. marginal tax rate facing woman\n15. motheduc                 mother's years of schooling\n16. fatheduc                 father's years of schooling\n17. unem                     unem. rate in county of resid.\n18. city                     =1 if live in SMSA\n19. exper                    actual labor mkt exper\n20. nwifeinc                 (faminc - wage*hours)/1000\n21. lwage                    log(wage)\n22. expersq                  exper^2\n\nThe goal is to first impute missing data in the data set, and then determine the impact of one added year of education on wages. If one simply ignores missing values, bias can be introduced depending on the missingness mechanism. The second problem here is that education is likely to be endogeneous (and thus be correlated to the error term), as it is not randomly assigned. This causes biased estimates and may lead to seriously wrong conclusions. So missingness and endogeneity should be dealt with, but dealing with both issues is more of a programming challenge than an econometrics challenge. Thankfully, the packages contained in the {tidyverse} as well as {mice} will save the day!\n\n\nIf you inspect the data, you will see that there are no missing values. So I will use the {mice} package to first ampute the data (which means adding missing values). This, of course, is done for education purposes. If you’re lucky enough to not have missing values in your data, you shouldn’t add them!\n\n\nLet’s load all the packages needed:\n\nlibrary(tidyverse)\nlibrary(AER)\nlibrary(naniar)\nlibrary(mice)\n\nSo first, let’s read in the data, and ampute it:\n\nwages_data &lt;- read_csv(\"http://eclr.humanities.manchester.ac.uk/images/5/5f/Mroz.csv\")\n## Parsed with column specification:\n## cols(\n##   .default = col_integer(),\n##   wage = col_character(),\n##   repwage = col_double(),\n##   huswage = col_double(),\n##   mtr = col_double(),\n##   unem = col_double(),\n##   nwifeinc = col_double(),\n##   lwage = col_character()\n## )\n## See spec(...) for full column specifications.\n\nFirst, I only select the variables I want to use and convert them to the correct class:\n\nwages_data &lt;- wages_data %&gt;% \n    select(wage, educ, fatheduc, motheduc, inlf, hours, \n               kidslt6, kidsge6, age, huswage, \n               mtr, unem, city, exper) %&gt;% \n    mutate_at(vars(kidslt6, kidsge6, hours, educ, age, wage, huswage, mtr,\n                    motheduc, fatheduc, unem, exper), as.numeric) %&gt;% \n    mutate_at(vars(city, inlf), as.character)\n## Warning in evalq(as.numeric(wage), &lt;environment&gt;): NAs introduced by\n## coercion\n\nIn the data, some women are not in the labour force, and thus do not have any wages; meaning they should have a 0 there. Instead, this is represented with the following symbol: “.”. So I convert these dots to 0. One could argue that the wages should not be 0, but that they’re truly missing. This is true, and there are ways to deal with such questions (Heckman’s selection model for instance), but this is not the point here.\n\nwages_data &lt;- wages_data %&gt;% \n    mutate(wage = ifelse(is.na(wage), 0, wage))\n\nLet’s double check if there are any missing values in the data, using naniar::vis_miss():\n\nvis_miss(wages_data)\n\n\n\n\nNope! Let’s ampute it:\n\nwages_mis &lt;- ampute(wages_data)$amp\n## Warning: Data is made numeric because the calculation of weights requires\n## numeric data\n\nampute() returns an object where the amp element is the amputed data. This is what I save into the new variable wages_mis.\n\n\nLet’s take a look:\n\nvis_miss(wages_mis)\n\n\n\n\nOk, so now we have missing values. Let’s use the recently added mice::parlmice() function to impute the dataset, in parallel:\n\nimp_wages &lt;- parlmice(data = wages_mis, m = 10, maxit = 20, cl.type = \"FORK\")\n\nFor reproducibility, I save these objects to disk:\n\nwrite_csv(wages_mis, \"wages_mis.csv\")\n\nsaveRDS(imp_wages, \"imp_wages.rds\")\n\nAs a sanity check, let’s look at the missingness pattern for the first completed dataset:\n\nvis_miss(complete(imp_wages))\n\n\n\n\nmice::parlmice() was able to impute the dataset. I imputed it 10 times, so now I have 10 imputed datasets. If I want to estimate a model using this data, I will need to do so 10 times. This is where the tidyverse comes into play. First, let’s combine all the 10 imputed datasets into one long dataset, with an index to differentiate them. This is done easily with mice::complete():\n\nimp_wages_df &lt;- mice::complete(imp_wages, \"long\")\n\nLet’s take a look at the data:\n\nhead(imp_wages_df)\n##   .imp .id   wage educ fatheduc motheduc inlf hours kidslt6 kidsge6 age\n## 1    1   1 3.3540   12        7       12    1  1610       1       0  32\n## 2    1   2 1.3889   12        7        7    1  1656       0       2  30\n## 3    1   3 4.5455   12        7       12    1  1980       0       3  35\n## 4    1   4 1.0965   12        7        7    1   456       0       3  34\n## 5    1   5 4.5918   14       14       12    1  1568       1       2  31\n## 6    1   6 4.7421   12        7       14    1  2032       0       0  54\n##   huswage    mtr unem city exper\n## 1  4.0288 0.7215  5.0    0    14\n## 2  8.4416 0.6615 11.0    1     5\n## 3  3.5807 0.6915  5.0    0    15\n## 4  3.5417 0.7815  5.0    0     6\n## 5 10.0000 0.6215  9.5    1    14\n## 6  4.7364 0.6915  7.5    1    33\n\nAs you can see, there are two new columns, .id and .imp. .imp equals i means that it is the ith imputed dataset.\n\n\nBecause I have 0’s in my dependent variable, I will not log the wages but instead use the Inverse Hyperbolic Sine transformation. Marc F. Bellemare wrote a nice post about it here.\n\nihs &lt;- function(x){\n    log(x + sqrt(x**2 + 1))\n}\n\nI can now apply this function, but first I have to group by .imp. Remember, these are 10 separated datasets. I also create the experience squared:\n\nimp_wages_df &lt;- imp_wages_df %&gt;% \n    group_by(.imp) %&gt;% \n    mutate(ihs_wage = ihs(wage),\n           exper2 = exper**2)\n\nNow comes some tidyverse magic. I will create a new dataset by using the nest() function from tidyr.\n\n(imp_wages &lt;- imp_wages_df %&gt;% \n    group_by(.imp) %&gt;% \n    nest())\n## # A tibble: 10 x 2\n##     .imp data               \n##    &lt;int&gt; &lt;list&gt;             \n##  1     1 &lt;tibble [753 × 17]&gt;\n##  2     2 &lt;tibble [753 × 17]&gt;\n##  3     3 &lt;tibble [753 × 17]&gt;\n##  4     4 &lt;tibble [753 × 17]&gt;\n##  5     5 &lt;tibble [753 × 17]&gt;\n##  6     6 &lt;tibble [753 × 17]&gt;\n##  7     7 &lt;tibble [753 × 17]&gt;\n##  8     8 &lt;tibble [753 × 17]&gt;\n##  9     9 &lt;tibble [753 × 17]&gt;\n## 10    10 &lt;tibble [753 × 17]&gt;\n\nAs you can see, imp_wages is now a dataset with two columns: .imp, indexing the imputed datasets, and a column called data, where each element is itself a tibble! data is a so-called list-column. You can read more about it on the purrr tutorial written by Jenny Bryan.\n\n\nEstimating a model now is easy, if you’re familiar with purrr. This is how you do it:\n\nimp_wages_reg = imp_wages %&gt;% \n    mutate(lin_reg = map(data, \n                         ~lm(ihs_wage ~ educ + inlf + hours + \n                                 kidslt6 + kidsge6 + age + huswage + \n                                 mtr + unem + city + exper + exper2, \n                             data = .)))\n\nOk, so what happened here? imp_wages is a data frame, so it’s possible to add a column to it with mutate(). I call that column lin_reg and use map() on the column called data (remember, this column is actually a list of data frame objects, and map() takes a list as an argument, and then a function or formula) with the following formula:\n\n~lm(ihs_wage ~ educ + inlf + hours + \n        kidslt6 + kidsge6 + age + huswage + \n        mtr + unem + city + exper + exper2, \n    data = .)\n\nThis formula is nothing more that a good old linear regression. The last line data = . means that the data to be used inside lm() should be coming from the list called data, which is the second column of imp_wages. As I’m writing these lines, I realize it is confusing as hell. But I promise you that learning to use purrr is a bit like learning how to use a bicycle. Very difficult to explain, but once you know how to do it, it feels super natural. Take some time to play with the lines above to really understand what happened.\n\n\nNow, let’s take a look at the result:\n\nimp_wages_reg\n## # A tibble: 10 x 3\n##     .imp data                lin_reg\n##    &lt;int&gt; &lt;list&gt;              &lt;list&gt; \n##  1     1 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  2     2 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  3     3 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  4     4 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  5     5 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  6     6 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  7     7 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  8     8 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n##  9     9 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   \n## 10    10 &lt;tibble [753 × 17]&gt; &lt;lm&gt;\n\nimp_wages_reg now has a third column called lin_reg where each element is a linear model, estimated on the data from the data column! We can now pool the results of these 10 regressions using mice::pool():\n\npool_lin_reg &lt;- pool(imp_wages_reg$lin_reg)\n\nsummary(pool_lin_reg)\n##                  estimate    std.error  statistic       df      p.value\n## (Intercept)  1.2868701172 3.214473e-01  4.0033628 737.9337 6.876133e-05\n## educ         0.0385310276 8.231906e-03  4.6806931 737.9337 3.401935e-06\n## inlf         1.8845418354 5.078235e-02 37.1101707 737.9337 0.000000e+00\n## hours       -0.0001164143 3.011378e-05 -3.8658143 737.9337 1.204773e-04\n## kidslt6     -0.0438925013 3.793152e-02 -1.1571510 737.9337 2.475851e-01\n## kidsge6     -0.0117978229 1.405226e-02 -0.8395678 737.9337 4.014227e-01\n## age         -0.0030084595 2.666614e-03 -1.1281946 737.9337 2.596044e-01\n## huswage     -0.0231736955 5.607364e-03 -4.1327255 737.9337 3.995866e-05\n## mtr         -2.2109176781 3.188827e-01 -6.9333267 737.9337 8.982592e-12\n## unem         0.0028775444 5.462973e-03  0.5267360 737.9337 5.985352e-01\n## city         0.0157414671 3.633755e-02  0.4332011 737.9337 6.649953e-01\n## exper        0.0164364027 6.118875e-03  2.6861806 737.9337 7.389936e-03\n## exper2      -0.0002022602 1.916146e-04 -1.0555575 737.9337 2.915159e-01\n\nThis function averages the results from the 10 regressions and computes correct standard errors. This is based on Rubin’s rules (Rubin, 1987, p. 76). As you can see, the linear regression indicates that one year of added education has a positive, significant effect of log wages (they’re not log wages, I used the IHS transformation, but log wages just sounds better than inverted hyperbolic sined wages). This effect is almost 4%.\n\n\nBut education is not randomly assigned, and as such might be endogenous. This is where instrumental variables come into play. An instrument is a variables that impacts the dependent variable only through the endogenous variable (here, education). For example, the education of the parents do not have a direct impact over one’s wage, but having college-educated parents means that you are likely college-educated yourself, and thus have a higher wage that if you only have a high school diploma.\n\n\nI am thus going to instrument education with both parents’ education:\n\nimp_wages_reg = imp_wages_reg %&gt;% \n    mutate(iv_reg = map(data, \n                         ~ivreg(ihs_wage ~ educ + inlf + hours + \n                                 kidslt6 + kidsge6 + age + huswage + \n                                 mtr + unem + city + exper + exper2 |.-educ + fatheduc + motheduc, \n                             data = .)))\n\nThe only difference from before is the formula:\n\n~ivreg(ihs_wage ~ educ + inlf + hours + \n           kidslt6 + kidsge6 + age + huswage + \n           mtr + unem + city + exper + exper2 |.-educ + fatheduc + motheduc, \n       data = .)\n## ~ivreg(ihs_wage ~ educ + inlf + hours + kidslt6 + kidsge6 + age + \n##     huswage + mtr + unem + city + exper + exper2 | . - educ + \n##     fatheduc + motheduc, data = .)\n\nInstead of lm() I use AER::ivreg() and the formula has a second part, after the | symbol. This is where I specify that I instrument education with the parents’ education.\n\n\nimp_wages_reg now looks like this:\n\nimp_wages_reg\n## # A tibble: 10 x 4\n##     .imp data                lin_reg iv_reg \n##    &lt;int&gt; &lt;list&gt;              &lt;list&gt;  &lt;list&gt; \n##  1     1 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  2     2 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  3     3 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  4     4 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  5     5 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  6     6 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  7     7 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  8     8 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n##  9     9 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n## 10    10 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;\n\nLet’s take a look at the results:\n\npool_iv_reg &lt;- pool(imp_wages_reg$iv_reg)\n\nsummary(pool_iv_reg)\n##                  estimate    std.error  statistic       df      p.value\n## (Intercept)  2.0091904157 5.146812e-01  3.9037568 737.9337 1.033832e-04\n## educ         0.0038859137 2.086592e-02  0.1862326 737.9337 8.523136e-01\n## inlf         1.9200207113 5.499457e-02 34.9129122 737.9337 0.000000e+00\n## hours       -0.0001313866 3.157375e-05 -4.1612608 737.9337 3.537881e-05\n## kidslt6     -0.0234593391 4.000689e-02 -0.5863824 737.9337 5.577979e-01\n## kidsge6     -0.0123239220 1.422241e-02 -0.8665145 737.9337 3.864897e-01\n## age         -0.0040874625 2.763340e-03 -1.4791748 737.9337 1.395203e-01\n## huswage     -0.0242737100 5.706497e-03 -4.2536970 737.9337 2.373189e-05\n## mtr         -2.6385172445 3.998419e-01 -6.5989008 737.9337 7.907430e-11\n## unem         0.0047331976 5.622137e-03  0.8418859 737.9337 4.001246e-01\n## city         0.0255647706 3.716783e-02  0.6878197 737.9337 4.917824e-01\n## exper        0.0180917073 6.258779e-03  2.8906127 737.9337 3.957817e-03\n## exper2      -0.0002291007 1.944599e-04 -1.1781381 737.9337 2.391213e-01\n\nAs you can see, education is not statistically significant anymore! This is why it is quite important to think about endogeneity issues. However, it is not always very easy to find suitable instruments. A series of tests exist to determine if you have relevant and strong instruments, but this blog post is already long enough. I will leave this for a future blog post."
  },
  {
    "objectID": "posts/2017-03-29-make-ggplot2-purrr.html",
    "href": "posts/2017-03-29-make-ggplot2-purrr.html",
    "title": "Make ggplot2 purrr",
    "section": "",
    "text": "Update: I’ve included another way of saving a separate plot by group in this article, as pointed out by @monitus. Actually, this is the preferred solution; using dplyr::do() is deprecated, according to Hadley Wickham himself.\n\n\nI’ll be honest: the title is a bit misleading. I will not use purrr that much in this blog post. Actually, I will use one single purrr function, at the very end. I use dplyr much more. However Make ggplot2 purrr sounds better than Make ggplot dplyr or whatever the verb for dplyr would be.\n\n\nAlso, this blog post was inspired by a stackoverflow question and in particular one of the answers. So I don’t bring anything new to the table, but I found this stackoverflow answer so useful and so underrated (only 16 upvotes as I’m writing this!) that I wanted to write something about it.\n\n\nBasically the idea of this blog post is to show how to create graphs using ggplot2, but by grouping by a factor variable beforehand. To illustrate this idea, let’s use the data from the Penn World Tables 9.0. The easiest way to get this data is to install the package called pwt9 with:\n\ninstall.packages(\"pwt9\")\n\nand then load the data with:\n\ndata(\"pwt9.0\")\n\nNow, let’s load the needed packages. I am also using ggthemes which makes themeing your ggplots very easy. I’ll be making Tufte-style plots.\n\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(pwt9)\n\nFirst let’s select a list of countries:\n\ncountry_list &lt;- c(\"France\", \"Germany\", \"United States of America\", \"Luxembourg\", \"Switzerland\", \"Greece\")\n\nsmall_pwt &lt;- pwt9.0 %&gt;%\n  filter(country %in% country_list)\n\nLet’s us also order the countries in the data frame as I have written them in country_list:\n\nsmall_pwt &lt;- small_pwt %&gt;%\n  mutate(country = factor(country, levels = country_list, ordered = TRUE))\n\nYou might be wondering why this is important. At the end of the article, we are going to save the plots to disk. If we do not re-order the countries inside the data frame as in country_list, the name of the files will not correspond to the correct plots!\n\n\nUpdate: While this can still be interesting to know, especially if you want to order the bars of a barplot made with ggplot2, I included a suggestion by @expersso that does not require your data to be ordered!\n\n\nNow when you want to plot the same variable by countries, say avh (Average annual hours worked by persons engaged), the usual way to do this is with one of facet_wrap() or facet_grid():\n\nggplot(data = small_pwt) + theme_tufte() +\n  geom_line(aes(y = avh, x = year)) +\n  facet_wrap(~country)\n\n\n\nggplot(data = small_pwt) + theme_tufte() +\n  geom_line(aes(y = avh, x = year)) +\n  facet_grid(country~.)\n\n\n\n\nAs you can see, for this particular example, facet_grid() is not very useful, but do notice its argument, country~., which is different from facet_wrap()’s argument. This way, I get the graphs stacked horizontally. If I had used facet_grid(~country) the graphs would be side by side and completely unreadable.\n\n\nNow, let’s go to the meat of this post: what if you would like to have one single graph for each country? You’d probably think of using dplyr::group_by() to form the groups and then the graphs. This is the way to go, but you also have to use dplyr::do(). This is because as far as I understand, ggplot2 is not dplyr-aware, and using an arbitrary function with groups is only possible with dplyr::do().\n\n\nUpdate: As explained in the intro above, I also added the solution that uses tidyr::nest():\n\n# Ancient, deprecated way of doing this\nplots &lt;- small_pwt %&gt;%\n  group_by(country) %&gt;%\n  do(plot = ggplot(data = .) + theme_tufte() +\n       geom_line(aes(y = avh, x = year)) +\n       ggtitle(unique(.$country)) +\n       ylab(\"Year\") +\n       xlab(\"Average annual hours worked by persons engaged\"))\n\nAnd this is the approach that uses tidyr::nest():\n\n# Preferred approach\nplots &lt;- small_pwt %&gt;%\n  group_by(country) %&gt;%\n  nest() %&gt;%\n  mutate(plot = map2(data, country, ~ggplot(data = .x) + theme_tufte() +\n       geom_line(aes(y = avh, x = year)) +\n       ggtitle(.y) +\n       ylab(\"Year\") +\n       xlab(\"Average annual hours worked by persons engaged\")))\n\nIf you know dplyr at least a little bit, the above lines should be easy for you to understand. But notice how we get the title of the graphs, with ggtitle(unique(.$country)), which was actually the point of the stackoverflow question.\n\n\nUpdate: The modern version uses tidyr::nest(). Its documentation tells us:\n\n\nThere are many possible ways one could choose to nest columns inside a data frame. nest() creates a list of data frames containing all the nested variables: this seems to be the most useful form in practice. Let’s take a closer look at what it does exactly:\n\nsmall_pwt %&gt;%\n  group_by(country) %&gt;%\n  nest() %&gt;%\n  head()\n## # A tibble: 6 x 2\n##   country                  data              \n##   &lt;ord&gt;                    &lt;list&gt;            \n## 1 Switzerland              &lt;tibble [65 × 46]&gt;\n## 2 Germany                  &lt;tibble [65 × 46]&gt;\n## 3 France                   &lt;tibble [65 × 46]&gt;\n## 4 Greece                   &lt;tibble [65 × 46]&gt;\n## 5 Luxembourg               &lt;tibble [65 × 46]&gt;\n## 6 United States of America &lt;tibble [65 × 46]&gt;\n\nThis is why I love lists in R; we get a tibble where each element of the column data is itself a tibble. We can now apply any function that we know works on lists.\n\n\nWhat might be surprising though, is the object that is created by this code. Let’s take a look at plots:\n\nprint(plots)\n## # A tibble: 6 x 3\n##   country                  data               plot    \n##   &lt;ord&gt;                    &lt;list&gt;             &lt;list&gt;  \n## 1 Switzerland              &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 2 Germany                  &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 3 France                   &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 4 Greece                   &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 5 Luxembourg               &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n## 6 United States of America &lt;tibble [65 × 46]&gt; &lt;S3: gg&gt;\n\nAs dplyr::do()’s documentation tells us, the return values get stored inside a list. And this is exactly what we get back; a list of plots! Lists are a very flexible and useful class, and you cannot spell list without purrr (at least not when you’re a neRd).\n\n\nHere are the final lines that use purrr::map2() to save all these plots at once inside your working directory:\n\n\nUpdate: I have changed the code below which does not require your data frame to be ordered according to the variable country_list.\n\n# file_names &lt;- paste0(country_list, \".pdf\")\n\nmap2(paste0(plots$country, \".pdf\"), plots$plot, ggsave)\n\nAs I said before, if you do not re-order the countries inside the data frame, the names of the files and the plots will not match. Try running all the code without re-ordering, you’ll see!\n\n\nI hope you found this post useful. You can follow me on twitter for blog updates.\n\n\nUpdate: Many thanks to the readers of this article and for their useful suggestions. I love the R community; everyday I learn something new and useful!"
  },
  {
    "objectID": "posts/2017-12-17-teaching_tidyverse.html",
    "href": "posts/2017-12-17-teaching_tidyverse.html",
    "title": "Teaching the tidyverse to beginners",
    "section": "",
    "text": "End October I tweeted this:\n\n\n\nwill teach #rstats soon again but this time following @drob 's suggestion of the tidyverse first as laid out here: https://t.co/js8SsUs8Nv\n\n— Bruno Rodrigues (@brodriguesco) October 24, 2017\n\n\n\nand it generated some discussion. Some people believe that this is the right approach, and some others think that one should first present base R and then show how the tidyverse complements it. This year, I taught three classes; a 12-hour class to colleagues that work with me, a 15-hour class to master’s students and 3 hours again to some of my colleagues. Each time, I decided to focus on the tidyverse(almost) entirely, and must say that I am not disappointed with the results!\n\n\nThe 12 hour class was divided in two 6 hours days. It was a bit intense, especially the last 3 hours that took place Friday afternoon. The crowd was composed of some economists that had experience with STATA, some others that were mostly using Excel and finally some colleagues from the IT department that sometimes need to dig into some data themselves. Apart from 2 people, all the other never had any experience with R.\n\n\nWe went from 0 to being able to do the plot below after the end of the first day (so 6 hours in). Keep in mind that practically none of them even had opened RStudio before. I show the code so you can see the progress made in just a few hours:\n\nlibrary(Ecdat)\nlibrary(tidyverse)\nlibrary(ggthemes)\ndata(Bwages)\nbwages = Bwages %&gt;%\n  mutate(educ_level = case_when(educ == 1 ~ \"Primary School\",\n                                educ == 2 ~ \"High School\",\n                                educ == 3 ~ \"Some university\",\n                                educ == 4 ~ \"Master's degree\",\n                                educ == 5 ~ \"Doctoral degree\"))\n\nggplot(bwages) +\n  geom_smooth(aes(exper, wage, colour = educ_level)) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n## `geom_smooth()` using method = 'loess'\n\n\n\n\nOf course some of them needed some help here and there, and I also gave them hints (for example I told them about case_when() and try to use it inside mutate() instead of nested ifs) but it was mostly due to lack of experience and because they hadn’t had the time to fully digest R’s syntax which was for most people involved completely new.\n\n\nOn the second day I showed purrr::map() and purrr::reduce() and overall it went quite well too. I even showed list-columns, and this is where I started losing some of them; I did not insist too much on it though, only wanted to show them the flexibility of data.frame objects. Some of them were quite impressed by list-columns! Then I started showing (for and while) loops and writing functions. I even showed them tidyeval and again, it went relatively well. Once they had the opportunity to play a bit around with it, I think it clicked (plus they have lots of code examples to go back too).\n\n\nAt the end, people seemed to have enjoyed the course, but told me that Friday was heavy; indeed it was, but I feel that it was mostly because 12 hours spread on 2 days is not the best format for this type of material, but we all had time constraints.\n\n\nThe 15 hour Master’s course was spread over 4 days, and covered basically the same. I just used the last 3 hours to show the students some basic functions for model estimation (linear, count, logit/probit and survival models). Again, the students were quite impressed by how easily they could get descriptive statistics by first grouping by some variables. Through their questions, I even got to show them scoped versions of dplyr verbs, such as select_if() and summarise_at(). I was expecting to lose them there, but actually most of them got these scoped versions quite fast. These students already had some experience with R though, but none with the tidyverse.\n\n\nFinally the 3 hour course was perhaps the most interesting; I only had 100% total beginners. Some just knew R by name and had never heard/seen/opened RStudio (with the exception of one person)! I did not show them any loops, function definitions and no plots. I only showed them how RStudio looked and worked, what were (and how to install) packages (as well as the CRAN Task Views) and then how to import data with rio and do descriptive statistics only with dplyr. They were really interested and quite impressed by rio (“what do you mean I can use the same code for importing any dataset, in any format?”) but also by the simplicity of dplyr.\n\n\nIn all the courses, I did show the $ primitive to refer to columns inside a data.frame. First I showed them lists which is where I introduced $. Then it was easy to explain to them why it was the same for a column inside a data.frame; a data.frame is simply a list! This is also the distinction I made from the previous years; I simply mentioned (and showed really quickly) matrices and focused almost entirely on lists. Most participants, if not all, had learned to program statistics by thinking about linear algebra and matrices. Nothing wrong with that, but I feel that R really shines when you focus on lists and on how to work with them.\n\n\nOverall as the teacher, I think that focusing on the tidyverse might be a very good strategy. I might have to do some adjustments here and there for the future courses, but my hunch is that the difficulties that some participants had were not necessarily due to the tidyverse but simply to lack of time to digest what was shown, as well as a total lack of experience with R. I do not think that these participants would have better understood a more traditional, base, matrix-oriented course."
  },
  {
    "objectID": "posts/2015-05-03-update-introduction-r-programming.html",
    "href": "posts/2015-05-03-update-introduction-r-programming.html",
    "title": "Update to Introduction to programming econometrics with R",
    "section": "",
    "text": "This semester I taught a course on applied econometrics with the R programming language. For this, I created a document that I gave to my students and shared online. This is the kind of document I would have liked to read when I first started using R. I already had some programming experience in C and Pascal but this is not necessarily the case for everyone that is confronted to R when they start learning about econometrics.\nThis is why the beginning of the document focuses more on general programming knowledge and techniques, and then only on econometrics. People online seemed to like the document, as I’ve received some positive comments by David Smith from Revolution R (read his blog post about the document here) and Dave Giles which links to David’s blog post here. People on twitter have also retweeted David’s and Dave’s tweets to their blog posts and I’ve received some requests by people to send them the PDF by email (because they live in places where Dropbox is not accessible unfortunately).\nThe document is still a work in progress (and will probably remain so for a long time), but I’ve added some new sections about reproducible research and thought that this update could warrant a new blog post.\nFor now, only linear models are reviewed, but I think I’ll start adding some chapters about non-linear models soonish. The goal for these notes, however, is not to re-invent the wheel: there are lots of good books about econometrics with R out there and packages that estimate a very wide range of models. What I want for these notes, is to focus more on the programming knowledge an econometrician needs, in a very broad and general sense. I want my students to understand that R is a true programming language, and that they need to use every feature offered by such a language, and not think of R as a black box into which you only type pre-programmed commands, but also be able to program their own routines.\nAlso, I’ve made it possible to create the PDF using a Makefile. This may be useful for people that do not have access to Dropbox, but are familiar with git.\nYou can compile the book in two ways: first download the whole repository:\ngit clone git@bitbucket.org:b-rodrigues/programmingeconometrics.git\nand then, with Rstudio, open the file appliedEconometrics.Rnw and knit it. Another solution is to use the Makefile. Just type:\nmake\ninside a terminal (should work on GNU+Linux and OSX systems) and it should compile the document. You may get some message about “additional information” for some R packages. When these come up, just press Q on your keyboard to continue the compilation process.\nGet the notes here.\nAs always, if you have questions or suggestions, do not hesitate to send me an email and I sure hope you’ll find these notes useful!"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html",
    "href": "posts/2017-03-24-lesser_known_purrr.html",
    "title": "Lesser known purrr tricks",
    "section": "",
    "text": "purrr is a package that extends R’s functional programming capabilities. It brings a lot of new stuff to the table and in this post I show you some of the most useful (at least to me) functions included in purrr."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#getting-rid-of-loops-with-map",
    "href": "posts/2017-03-24-lesser_known_purrr.html#getting-rid-of-loops-with-map",
    "title": "Lesser known purrr tricks",
    "section": "\nGetting rid of loops with map()\n",
    "text": "Getting rid of loops with map()\n\nlibrary(purrr)\n\nnumbers &lt;- list(11, 12, 13, 14)\n\nmap_dbl(numbers, sqrt)\n## [1] 3.316625 3.464102 3.605551 3.741657\n\nYou might wonder why this might be preferred to a for loop? It’s a lot less verbose, and you do not need to initialise any kind of structure to hold the result. If you google “create empty list in R” you will see that this is very common. However, with the map() family of functions, there is no need for an initial structure. map_dbl() returns an atomic list of real numbers, but if you use map() you will get a list back. Try them all out!"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#map-conditionally",
    "href": "posts/2017-03-24-lesser_known_purrr.html#map-conditionally",
    "title": "Lesser known purrr tricks",
    "section": "\nMap conditionally\n",
    "text": "Map conditionally\n\n\n\nmap_if()\n\n# Create a helper function that returns TRUE if a number is even\nis_even &lt;- function(x){\n  !as.logical(x %% 2)\n}\n\nmap_if(numbers, is_even, sqrt)\n## [[1]]\n## [1] 11\n## \n## [[2]]\n## [1] 3.464102\n## \n## [[3]]\n## [1] 13\n## \n## [[4]]\n## [1] 3.741657\n\n\n\nmap_at()\n\nmap_at(numbers, c(1,3), sqrt)\n## [[1]]\n## [1] 3.316625\n## \n## [[2]]\n## [1] 12\n## \n## [[3]]\n## [1] 3.605551\n## \n## [[4]]\n## [1] 14\n\nmap_if() and map_at() have a further argument than map(); in the case of map_if(), a predicate function ( a function that returns TRUE or FALSE) and a vector of positions for map_at(). This allows you to map your function only when certain conditions are met, which is also something that a lot of people google for."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#map-a-function-with-multiple-arguments",
    "href": "posts/2017-03-24-lesser_known_purrr.html#map-a-function-with-multiple-arguments",
    "title": "Lesser known purrr tricks",
    "section": "\nMap a function with multiple arguments\n",
    "text": "Map a function with multiple arguments\n\nnumbers2 &lt;- list(1, 2, 3, 4)\n\nmap2(numbers, numbers2, `+`)\n## [[1]]\n## [1] 12\n## \n## [[2]]\n## [1] 14\n## \n## [[3]]\n## [1] 16\n## \n## [[4]]\n## [1] 18\n\nYou can map two lists to a function which takes two arguments using map_2(). You can even map an arbitrary number of lists to any function using pmap().\n\n\nBy the way, try this in: +(1,3) and see what happens."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#dont-stop-execution-of-your-function-if-something-goes-wrong",
    "href": "posts/2017-03-24-lesser_known_purrr.html#dont-stop-execution-of-your-function-if-something-goes-wrong",
    "title": "Lesser known purrr tricks",
    "section": "\nDon’t stop execution of your function if something goes wrong\n",
    "text": "Don’t stop execution of your function if something goes wrong\n\npossible_sqrt &lt;- possibly(sqrt, otherwise = NA_real_)\n\nnumbers_with_error &lt;- list(1, 2, 3, \"spam\", 4)\n\nmap(numbers_with_error, possible_sqrt)\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 1.414214\n## \n## [[3]]\n## [1] 1.732051\n## \n## [[4]]\n## [1] NA\n## \n## [[5]]\n## [1] 2\n\nAnother very common issue is to keep running your loop even when something goes wrong. In most cases the loop simply stops at the error, but you would like it to continue and see where it failed. Try to google “skip error in a loop” or some variation of it and you’ll see that a lot of people really just want that. This is possible by combining map() and possibly(). Most solutions involve the use of tryCatch() which I personally do not find very easy to use."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#dont-stop-execution-of-your-function-if-something-goes-wrong-and-capture-the-error",
    "href": "posts/2017-03-24-lesser_known_purrr.html#dont-stop-execution-of-your-function-if-something-goes-wrong-and-capture-the-error",
    "title": "Lesser known purrr tricks",
    "section": "\nDon’t stop execution of your function if something goes wrong and capture the error\n",
    "text": "Don’t stop execution of your function if something goes wrong and capture the error\n\nsafe_sqrt &lt;- safely(sqrt, otherwise = NA_real_)\n\nmap(numbers_with_error, safe_sqrt)\n## [[1]]\n## [[1]]$result\n## [1] 1\n## \n## [[1]]$error\n## NULL\n## \n## \n## [[2]]\n## [[2]]$result\n## [1] 1.414214\n## \n## [[2]]$error\n## NULL\n## \n## \n## [[3]]\n## [[3]]$result\n## [1] 1.732051\n## \n## [[3]]$error\n## NULL\n## \n## \n## [[4]]\n## [[4]]$result\n## [1] NA\n## \n## [[4]]$error\n## \n\nsafely() is very similar to possibly() but it returns a list of lists. An element is thus a list of the result and the accompagnying error message. If there is no error, the error component is NULL if there is an error, it returns the error message."
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#transpose-a-list",
    "href": "posts/2017-03-24-lesser_known_purrr.html#transpose-a-list",
    "title": "Lesser known purrr tricks",
    "section": "\nTranspose a list\n",
    "text": "Transpose a list\n\nsafe_result_list &lt;- map(numbers_with_error, safe_sqrt)\n\ntranspose(safe_result_list)\n## $result\n## $result[[1]]\n## [1] 1\n## \n## $result[[2]]\n## [1] 1.414214\n## \n## $result[[3]]\n## [1] 1.732051\n## \n## $result[[4]]\n## [1] NA\n## \n## $result[[5]]\n## [1] 2\n## \n## \n## $error\n## $error[[1]]\n## NULL\n## \n## $error[[2]]\n## NULL\n## \n## $error[[3]]\n## NULL\n## \n## $error[[4]]\n## \n\nHere we transposed the above list. This means that we still have a list of lists, but where the first list holds all the results (which you can then access with safe_result_list$result) and the second list holds all the errors (which you can access with safe_result_list$error). This can be quite useful!"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#apply-a-function-to-a-lower-depth-of-a-list",
    "href": "posts/2017-03-24-lesser_known_purrr.html#apply-a-function-to-a-lower-depth-of-a-list",
    "title": "Lesser known purrr tricks",
    "section": "\nApply a function to a lower depth of a list\n",
    "text": "Apply a function to a lower depth of a list\n\ntransposed_list &lt;- transpose(safe_result_list)\n\ntransposed_list %&gt;%\n    at_depth(2, is_null)\n## Warning: at_depth() is deprecated, please use `modify_depth()` instead\n## $result\n## $result[[1]]\n## [1] FALSE\n## \n## $result[[2]]\n## [1] FALSE\n## \n## $result[[3]]\n## [1] FALSE\n## \n## $result[[4]]\n## [1] FALSE\n## \n## $result[[5]]\n## [1] FALSE\n## \n## \n## $error\n## $error[[1]]\n## [1] TRUE\n## \n## $error[[2]]\n## [1] TRUE\n## \n## $error[[3]]\n## [1] TRUE\n## \n## $error[[4]]\n## [1] FALSE\n## \n## $error[[5]]\n## [1] TRUE\n\nSometimes working with lists of lists can be tricky, especially when we want to apply a function to the sub-lists. This is easily done with at_depth()!"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#set-names-of-list-elements",
    "href": "posts/2017-03-24-lesser_known_purrr.html#set-names-of-list-elements",
    "title": "Lesser known purrr tricks",
    "section": "\nSet names of list elements\n",
    "text": "Set names of list elements\n\nname_element &lt;- c(\"sqrt()\", \"ok?\")\n\nset_names(transposed_list, name_element)\n## $`sqrt()`\n## $`sqrt()`[[1]]\n## [1] 1\n## \n## $`sqrt()`[[2]]\n## [1] 1.414214\n## \n## $`sqrt()`[[3]]\n## [1] 1.732051\n## \n## $`sqrt()`[[4]]\n## [1] NA\n## \n## $`sqrt()`[[5]]\n## [1] 2\n## \n## \n## $`ok?`\n## $`ok?`[[1]]\n## NULL\n## \n## $`ok?`[[2]]\n## NULL\n## \n## $`ok?`[[3]]\n## NULL\n## \n## $`ok?`[[4]]\n##"
  },
  {
    "objectID": "posts/2017-03-24-lesser_known_purrr.html#reduce-a-list-to-a-single-value",
    "href": "posts/2017-03-24-lesser_known_purrr.html#reduce-a-list-to-a-single-value",
    "title": "Lesser known purrr tricks",
    "section": "\nReduce a list to a single value\n",
    "text": "Reduce a list to a single value\n\nreduce(numbers, `*`)\n## [1] 24024\n\nreduce() applies the function * iteratively to the list of numbers. There’s also accumulate():\n\naccumulate(numbers, `*`)\n## [1]    11   132  1716 24024\n\nwhich keeps the intermediary results.\n\n\nThis function is very general, and you can reduce anything:\n\n\nMatrices:\n\nmat1 &lt;- matrix(rnorm(10), nrow = 2)\nmat2 &lt;- matrix(rnorm(10), nrow = 2)\nmat3 &lt;- matrix(rnorm(10), nrow = 2)\nlist_mat &lt;- list(mat1, mat2, mat3)\n\nreduce(list_mat, `+`)\n##             [,1]       [,2]       [,3]     [,4]      [,5]\n## [1,] -2.48530177  1.0110049  0.4450388 1.280802 1.3413979\n## [2,]  0.07596679 -0.6872268 -0.6579242 1.615237 0.8231933\n\neven data frames:\n\ndf1 &lt;- as.data.frame(mat1)\ndf2 &lt;- as.data.frame(mat2)\ndf3 &lt;- as.data.frame(mat3)\n\nlist_df &lt;- list(df1, df2, df3)\n\nreduce(list_df, dplyr::full_join)\n## Joining, by = c(\"V1\", \"V2\", \"V3\", \"V4\", \"V5\")\n## Joining, by = c(\"V1\", \"V2\", \"V3\", \"V4\", \"V5\")\n##           V1         V2          V3          V4         V5\n## 1 -0.6264538 -0.8356286  0.32950777  0.48742905  0.5757814\n## 2  0.1836433  1.5952808 -0.82046838  0.73832471 -0.3053884\n## 3 -0.8969145  1.5878453 -0.08025176  0.70795473  1.9844739\n## 4  0.1848492 -1.1303757  0.13242028 -0.23969802 -0.1387870\n## 5 -0.9619334  0.2587882  0.19578283  0.08541773 -1.2188574\n## 6 -0.2925257 -1.1521319  0.03012394  1.11661021  1.2673687\n\nHope you enjoyed this list of useful functions! If you enjoy the content of my blog, you can follow me on twitter."
  },
  {
    "objectID": "posts/2017-07-02-dplyr-0-70-tutorial.html",
    "href": "posts/2017-07-02-dplyr-0-70-tutorial.html",
    "title": "Lesser known dplyr 0.7* tricks",
    "section": "",
    "text": "This blog post is an update to an older one I wrote in March. In the post from March, dplyr was at version 0.50, but since then a major update introduced some changes that make some of the tips in that post obsolete. So here I revisit the blog post from March by using dplyr 0.70."
  },
  {
    "objectID": "posts/2017-07-02-dplyr-0-70-tutorial.html#create-new-columns-with-mutate-and-case_when",
    "href": "posts/2017-07-02-dplyr-0-70-tutorial.html#create-new-columns-with-mutate-and-case_when",
    "title": "Lesser known dplyr 0.7* tricks",
    "section": "\nCreate new columns with mutate() and case_when()\n",
    "text": "Create new columns with mutate() and case_when()\n\n\nThe basic things such as selecting columns, renaming them, filtering, etc did not change with this new version. What did change however is creating new columns using case_when(). First, load dplyr and the mtcars dataset:\n\nlibrary(\"dplyr\")\ndata(mtcars)\n\nThis was how it was done in version 0.50 (notice the ’.\\(’ symbol before the variable ‘carb’):&lt;/p&gt;\n&lt;pre class=\"r\"&gt;&lt;code&gt;mtcars %&gt;%\n    mutate(carb_new = case_when(.\\)carb == 1 ~ \"one\", .\\(carb == 2 ~ &quot;two&quot;,\n                                .\\)carb == 4 ~ \"four\", TRUE ~ \"other\")) %&gt;% head(5)\n\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb carb_new\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4     four\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4     four\n## 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1      one\n## 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1      one\n## 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2      two\n\nThis has been simplified to:\n\nmtcars %&gt;%\n    mutate(carb_new = case_when(carb == 1 ~ \"one\",\n                                carb == 2 ~ \"two\",\n                                carb == 4 ~ \"four\",\n                                TRUE ~ \"other\")) %&gt;%\n    head(5)\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb carb_new\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4     four\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4     four\n## 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1      one\n## 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1      one\n## 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2      two\n\nNo need for .$ anymore."
  },
  {
    "objectID": "posts/2017-07-02-dplyr-0-70-tutorial.html#apply-a-function-to-certain-columns-only-by-rows-with-purrrlyr",
    "href": "posts/2017-07-02-dplyr-0-70-tutorial.html#apply-a-function-to-certain-columns-only-by-rows-with-purrrlyr",
    "title": "Lesser known dplyr 0.7* tricks",
    "section": "\nApply a function to certain columns only, by rows, with purrrlyr\n",
    "text": "Apply a function to certain columns only, by rows, with purrrlyr\n\n\ndplyr wasn’t the only package to get an overhaul, purrr also got the same treatment.\n\n\nIn the past, I applied a function to certains columns like this:\n\nmtcars %&gt;%\n    select(am, gear, carb) %&gt;%\n    purrr::by_row(sum, .collate = \"cols\", .to = \"sum_am_gear_carb\") -&gt; mtcars2\nhead(mtcars2)\n\nNow, by_row() does not exist in purrr anymore, but instead a new package called purrrlyr was introduced with functions that don’t really fit inside purrr nor dplyr:\n\nmtcars %&gt;%\n    select(am, gear, carb) %&gt;%\n    purrrlyr::by_row(sum, .collate = \"cols\", .to = \"sum_am_gear_carb\") -&gt; mtcars2\nhead(mtcars2)\n## # A tibble: 6 x 4\n##      am  gear  carb sum_am_gear_carb\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;\n## 1     1     4     4                9\n## 2     1     4     4                9\n## 3     1     4     1                6\n## 4     0     3     1                4\n## 5     0     3     2                5\n## 6     0     3     1                4\n\nThink of purrrlyr as purrrs and dplyrs love child."
  },
  {
    "objectID": "posts/2017-07-02-dplyr-0-70-tutorial.html#using-dplyr-functions-inside-your-own-functions-or-what-is-tidyeval",
    "href": "posts/2017-07-02-dplyr-0-70-tutorial.html#using-dplyr-functions-inside-your-own-functions-or-what-is-tidyeval",
    "title": "Lesser known dplyr 0.7* tricks",
    "section": "\nUsing dplyr functions inside your own functions, or what is tidyeval\n",
    "text": "Using dplyr functions inside your own functions, or what is tidyeval\n\n\nProgramming with dplyr has been simplified a lot. Before version 0.70, one needed to use dplyr in conjuction with lazyeval to use dplyr functions inside one’s own fuctions. It was not always very easy, especially if you mixed columns and values inside your functions. Here’s the example from the March blog post:\n\nextract_vars &lt;- function(data, some_string){\n\n  data %&gt;%\n    select_(lazyeval::interp(~contains(some_string))) -&gt; data\n\n  return(data)\n}\n\nextract_vars(mtcars, \"spam\")\n\nMore examples are available in this other blog post.\n\n\nI will revisit them now with dplyr’s new tidyeval syntax. I’d recommend you read the Tidy evaluation vignette here. This vignette is part of the rlang package, which gets used under the hood by dplyr for all your programming needs. Here is the function I called simpleFunction(), written with the old dplyr syntax:\n\nsimpleFunction &lt;- function(dataset, col_name){\n  dataset %&gt;%\n    group_by_(col_name) %&gt;%\n    summarise(mean_mpg = mean(mpg)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, \"cyl\")\n## # A tibble: 3 x 2\n##     cyl mean_mpg\n##   &lt;dbl&gt;    &lt;dbl&gt;\n## 1     4     26.7\n## 2     6     19.7\n## 3     8     15.1\n\nWith the new synax, it must be rewritten a little bit:\n\nsimpleFunction &lt;- function(dataset, col_name){\n  col_name &lt;- enquo(col_name)\n  dataset %&gt;%\n    group_by(!!col_name) %&gt;%\n    summarise(mean_mpg = mean(mpg)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, cyl)\n## # A tibble: 3 x 2\n##     cyl mean_mpg\n##   &lt;dbl&gt;    &lt;dbl&gt;\n## 1     4     26.7\n## 2     6     19.7\n## 3     8     15.1\n\nWhat has changed? Forget the underscore versions of the usual functions such as select_(), group_by_(), etc. Now, you must quote the column name using enquo() (or just quo() if working interactively, outside a function), which returns a quosure. This quosure can then be evaluated using !! in front of the quosure and inside the usual dplyr functions.\n\n\nLet’s look at another example:\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  filter_criteria &lt;- lazyeval::interp(~y == x, .values=list(y = as.name(col_name), x = value))\n  dataset %&gt;%\n    filter_(filter_criteria) %&gt;%\n    summarise(mean_cyl = mean(cyl)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, \"am\", 1)\n##   mean_cyl\n## 1 5.076923\n\nAs you can see, it’s a bit more complicated, as you needed to use lazyeval::interp() to make it work. With the improved dplyr, here’s how it’s done:\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  col_name &lt;- enquo(col_name)\n  dataset %&gt;%\n    filter((!!col_name) == value) %&gt;%\n    summarise(mean_cyl = mean(cyl)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, am, 1)\n##   mean_cyl\n## 1 5.076923\n\nMuch, much easier! There is something that you must pay attention to though. Notice that I’ve written:\n\nfilter((!!col_name) == value)\n\nand not:\n\nfilter(!!col_name == value)\n\nI have enclosed !!col_name inside parentheses. I struggled with this, but thanks to help from @dmi3k and @_lionelhenry I was able to understand what was happening (isn’t the #rstats community on twitter great?).\n\n\nOne last thing: let’s make this function a bit more general. I hard-coded the variable cyl inside the body of the function, but maybe you’d like the mean of another variable? Easy:\n\nsimpleFunction &lt;- function(dataset, group_col, mean_col, value){\n  group_col &lt;- enquo(group_col)\n  mean_col &lt;- enquo(mean_col)\n  dataset %&gt;%\n    filter((!!group_col) == value) %&gt;%\n    summarise(mean((!!mean_col))) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, am, cyl, 1)\n##   mean(cyl)\n## 1  5.076923\n\n«That’s very nice Bruno, but mean((cyl)) in the output looks ugly as sin» you might think, and you’d be right. It is possible to set the name of the column in the output using := instead of =:\n\nsimpleFunction &lt;- function(dataset, group_col, mean_col, value){\n  group_col &lt;- enquo(group_col)\n  mean_col &lt;- enquo(mean_col)\n  mean_name &lt;- paste0(\"mean_\", mean_col)[2]\n  dataset %&gt;%\n    filter((!!group_col) == value) %&gt;%\n    summarise(!!mean_name := mean((!!mean_col))) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(mtcars, am, cyl, 1)\n##   mean_cyl\n## 1 5.076923\n\nTo get the name of the column I added this line:\n\nmean_name &lt;- paste0(\"mean_\", mean_col)[2]\n\nTo see what it does, try the following inside an R interpreter (remember to us quo() instead of enquo() outside functions!):\n\npaste0(\"mean_\", quo(cyl))\n## [1] \"mean_~\"   \"mean_cyl\"\n\nenquo() quotes the input, and with paste0() it gets converted to a string that can be used as a column name. However, the ~ is in the way and the output of paste0() is a vector of two strings: the correct name is contained in the second element, hence the [2]. There might be a more elegant way of doing that, but for now this has been working well for me.\n\n\nThat was it folks! I do recommend you read the Programming with dplyr vignette here as well as other blog posts, such as the one recommended to me by @dmi3k here.\n\n\nHave fun with dplyr 0.70!"
  },
  {
    "objectID": "posts/2018-06-10-scraping_pdfs.html",
    "href": "posts/2018-06-10-scraping_pdfs.html",
    "title": "Getting data from pdfs using the pdftools package",
    "section": "",
    "text": "It is often the case that data is trapped inside pdfs, but thankfully there are ways to extract it from the pdfs. A very nice package for this task is pdftools (Github link) and this blog post will describe some basic functionality from that package.\n\n\nFirst, let’s find some pdfs that contain interesting data. For this post, I’m using the diabetes country profiles from the World Health Organization. You can find them here. If you open one of these pdfs, you are going to see this:\n\n\n\n\n\nhttp://www.who.int/diabetes/country-profiles/lux_en.pdf?ua=1\n\n\n\n\nI’m interested in this table here in the middle:\n\n\n\n\n\nhttp://www.who.int/diabetes/country-profiles/lux_en.pdf?ua=1\n\n\n\n\nI want to get the data from different countries, put it all into a nice data frame and make a simple plot.\n\n\nLet’s first start by loading the needed packages:\n\nlibrary(\"pdftools\")\nlibrary(\"glue\")\nlibrary(\"tidyverse\")\n## ── Attaching packages ────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──\n## ✔ ggplot2 2.2.1     ✔ purrr   0.2.5\n## ✔ tibble  1.4.2     ✔ dplyr   0.7.5\n## ✔ tidyr   0.8.1     ✔ stringr 1.3.1\n## ✔ readr   1.1.1     ✔ forcats 0.3.0\n## ── Conflicts ───────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::collapse() masks glue::collapse()\n## ✖ dplyr::filter()   masks stats::filter()\n## ✖ dplyr::lag()      masks stats::lag()\nlibrary(\"ggthemes\")\n\ncountry &lt;- c(\"lux\", \"fra\", \"deu\", \"usa\", \"prt\", \"gbr\")\n\nurl &lt;- \"http://www.who.int/diabetes/country-profiles/{country}_en.pdf?ua=1\"\n\nThe first 4 lines load the needed packages for this exercise: pdftools is the package that I described in the beginning of the post, glue is optional but offers a nice alternative to the paste() and paste0() functions. Take a closer look at the url: you’ll see that I wrote {country}. This is not in the original links; the original links look like this (for example for the USA):\n\n\"http://www.who.int/diabetes/country-profiles/usa_en.pdf?ua=1\"\n\nSo because I’m interested in several countries, I created a vector with the country codes of the countries I’m interested in. Now, using the glue() function, something magical happens:\n\n(urls &lt;- glue(url))\n## http://www.who.int/diabetes/country-profiles/lux_en.pdf?ua=1\n## http://www.who.int/diabetes/country-profiles/fra_en.pdf?ua=1\n## http://www.who.int/diabetes/country-profiles/deu_en.pdf?ua=1\n## http://www.who.int/diabetes/country-profiles/usa_en.pdf?ua=1\n## http://www.who.int/diabetes/country-profiles/prt_en.pdf?ua=1\n## http://www.who.int/diabetes/country-profiles/gbr_en.pdf?ua=1\n\nThis created a vector with all the links where {country} is replaced by each of the codes contained in the variable country.\n\n\nI use the same trick to create the names of the pdfs that I will download:\n\npdf_names &lt;- glue(\"report_{country}.pdf\")\n\nAnd now I can download them:\n\nwalk2(urls, pdf_names, download.file, mode = \"wb\")\n\nwalk2() is a function from the purrr package that is similar to map2(). You could use map2() for this, but walk2() is cleaner here, because dowload.file() is a function with a so-called side effect; it downloads files. map2() is used for functions without side effects.\n\n\nNow, I can finally use the pdf_text() function from the pdftools function to get the text from the pdfs:\n\nraw_text &lt;- map(pdf_names, pdf_text)\n\nraw_text is a list of where each element is the text from one of the pdfs. Let’s take a look:\n\nstr(raw_text)\n## List of 6\n##  $ : chr \"Luxembourg                                                                                                     \"| __truncated__\n##  $ : chr \"France                                                                                                         \"| __truncated__\n##  $ : chr \"Germany                                                                                                        \"| __truncated__\n##  $ : chr \"United States Of America                                                                                       \"| __truncated__\n##  $ : chr \"Portugal                                                                                                       \"| __truncated__\n##  $ : chr \"United Kingdom                                                                                                 \"| __truncated__\n\nLet’s take a look at one of these elements, which is nothing but a very long character:\n\nraw_text[[1]]\n## [1] \"Luxembourg                                                                                                                                          Total population: 567 000\\n                                                                                                                                                         Income group: High\\nMortality\\nNumber of diabetes deaths                                                                     Number of deaths attributable to high blood glucose\\n                                                                     males         females                                                            males       females\\nages 30–69                                                           &lt;100            &lt;100     ages 30–69                                              &lt;100          &lt;100\\nages 70+                                                             &lt;100            &lt;100     ages 70+                                                &lt;100          &lt;100\\nProportional mortality (% of total deaths, all ages)                                          Trends in age-standardized prevalence of diabetes\\n                    Communicable,\\n                   maternal, perinatal              Injuries                                                    35%\\n                    and nutritional                   6%                     Cardiovascular\\n                      conditions                                               diseases\\n                          6%                                                      33%\\n                                                                                                                30%\\n                                                                                                                25%\\n                                                                                              % of population\\n               Other NCDs\\n                  16%                                                                                           20%\\n                                     No data available                                                          15%           No data available\\n              Diabetes                                                                                          10%\\n                 2%\\n                                                                                                                5%\\n                   Respiratory\\n                    diseases\\n                       6%                                                                                       0%\\n                                                           Cancers\\n                                                            31%\\n                                                                                                                                  males     females\\nPrevalence of diabetes and related risk factors\\n                                                                                                                      males               females               total\\nDiabetes                                                                                                              8.3%                 5.3%                 6.8%\\nOverweight                                                                                                            70.7%               51.5%                61.0%\\nObesity                                                                                                               28.3%               21.3%                24.8%\\nPhysical inactivity                                                                                                   28.2%               31.7%                30.0%\\nNational response to diabetes\\nPolicies, guidelines and monitoring\\nOperational policy/strategy/action plan for diabetes                                                                                                ND\\nOperational policy/strategy/action plan to reduce overweight and obesity                                                                            ND\\nOperational policy/strategy/action plan to reduce physical inactivity                                                                               ND\\nEvidence-based national diabetes guidelines/protocols/standards                                                                                     ND\\nStandard criteria for referral of patients from primary care to higher level of care                                                                ND\\nDiabetes registry                                                                                                                                   ND\\nRecent national risk factor survey in which blood glucose was measured                                                                              ND\\nAvailability of medicines, basic technologies and procedures in the public health sector\\nMedicines in primary care facilities                                                          Basic technologies in primary care facilities\\nInsulin                                                                               ND      Blood glucose measurement                                             ND\\nMetformin                                                                             ND      Oral glucose tolerance test                                           ND\\nSulphonylurea                                                                         ND      HbA1c test                                                            ND\\nProcedures                                                                                    Dilated fundus examination                                            ND\\nRetinal photocoagulation                                                              ND      Foot vibration perception by tuning fork                              ND\\nRenal replacement therapy by dialysis                                                 ND      Foot vascular status by Doppler                                       ND\\nRenal replacement therapy by transplantation                                          ND      Urine strips for glucose and ketone measurement                       ND\\nND = country did not respond to country capacity survey\\n〇 = not generally available   ● = generally available\\nWorld Health Organization – Diabetes country profiles, 2016.\\n\"\n\nAs you can see, this is a very long character string with some line breaks (the \"\" character). So first, we need to split this string into a character vector by the \"\" character. Also, it might be difficult to see, but the table starts at the line with the following string: \"Prevalence of diabetes\" and ends with \"National response to diabetes\". Also, we need to get the name of the country from the text and add it as a column. As you can see, a whole lot of operations are needed, so what I do is put all these operations into a function that I will apply to each element of raw_text:\n\nclean_table &lt;- function(table){\n    table &lt;- str_split(table, \"\\n\", simplify = TRUE)\n    country_name &lt;- table[1, 1] %&gt;% \n        stringr::str_squish() %&gt;% \n        stringr::str_extract(\".+?(?=\\\\sTotal)\")\n    table_start &lt;- stringr::str_which(table, \"Prevalence of diabetes\")\n    table_end &lt;- stringr::str_which(table, \"National response to diabetes\")\n    table &lt;- table[1, (table_start +1 ):(table_end - 1)]\n    table &lt;- str_replace_all(table, \"\\\\s{2,}\", \"|\")\n    text_con &lt;- textConnection(table)\n    data_table &lt;- read.csv(text_con, sep = \"|\")\n    colnames(data_table) &lt;- c(\"Condition\", \"Males\", \"Females\", \"Total\")\n    dplyr::mutate(data_table, Country = country_name)\n}\n\nI advise you to go through all these operations and understand what each does. However, I will describe some of the lines, such as this one:\n\nstringr::str_extract(\".+?(?=\\\\sTotal)\")\n\nThis uses a very bizarre looking regular expression: \".+?(?=\\sTotal)\". This extracts everything before a space, followed by the string \"Total\". This is because the first line, the one that contains the name of the country looks like this: \"Luxembourg Total population: 567 000\". So everything before a space followed by the word \"Total\" is the country name. Then there’s these lines:\n\ntable &lt;- str_replace_all(table, \"\\\\s{2,}\", \"|\")\ntext_con &lt;- textConnection(table)\ndata_table &lt;- read.csv(text_con, sep = \"|\")\n\nThe first lines replaces 2 spaces or more (“\\s{2,}”) with \"|\". The reason I do this is because then I can read the table back into R as a data frame by specifying the separator as the “|” character. On the second line, I define table as a text connection, that I can then read back into R using read.csv(). On the second to the last line I change the column names and then I add a column called \"Country\" to the data frame.\n\n\nNow, I can map this useful function to the list of raw text extracted from the pdfs:\n\ndiabetes &lt;- map_df(raw_text, clean_table) %&gt;% \n    gather(Sex, Share, Males, Females, Total) %&gt;% \n    mutate(Share = as.numeric(str_extract(Share, \"\\\\d{1,}\\\\.\\\\d{1,}\")))\n\nI reshape the data with the gather() function (see what the data looks like before and after reshaping). I then convert the \"Share\" column into a numeric (it goes from something that looks like \"12.3 %\" into 12.3) and then I can create a nice plot. But first let’s take a look at the data:\n\ndiabetes\n##              Condition                  Country     Sex Share\n## 1             Diabetes               Luxembourg   Males   8.3\n## 2           Overweight               Luxembourg   Males  70.7\n## 3              Obesity               Luxembourg   Males  28.3\n## 4  Physical inactivity               Luxembourg   Males  28.2\n## 5             Diabetes                   France   Males   9.5\n## 6           Overweight                   France   Males  69.9\n## 7              Obesity                   France   Males  25.3\n## 8  Physical inactivity                   France   Males  21.2\n## 9             Diabetes                  Germany   Males   8.4\n## 10          Overweight                  Germany   Males  67.0\n## 11             Obesity                  Germany   Males  24.1\n## 12 Physical inactivity                  Germany   Males  20.1\n## 13            Diabetes United States Of America   Males   9.8\n## 14          Overweight United States Of America   Males  74.1\n## 15             Obesity United States Of America   Males  33.7\n## 16 Physical inactivity United States Of America   Males  27.6\n## 17            Diabetes                 Portugal   Males  10.7\n## 18          Overweight                 Portugal   Males  65.0\n## 19             Obesity                 Portugal   Males  21.4\n## 20 Physical inactivity                 Portugal   Males  33.5\n## 21            Diabetes           United Kingdom   Males   8.4\n## 22          Overweight           United Kingdom   Males  71.1\n## 23             Obesity           United Kingdom   Males  28.5\n## 24 Physical inactivity           United Kingdom   Males  35.4\n## 25            Diabetes               Luxembourg Females   5.3\n## 26          Overweight               Luxembourg Females  51.5\n## 27             Obesity               Luxembourg Females  21.3\n## 28 Physical inactivity               Luxembourg Females  31.7\n## 29            Diabetes                   France Females   6.6\n## 30          Overweight                   France Females  58.6\n## 31             Obesity                   France Females  26.1\n## 32 Physical inactivity                   France Females  31.2\n## 33            Diabetes                  Germany Females   6.4\n## 34          Overweight                  Germany Females  52.7\n## 35             Obesity                  Germany Females  21.4\n## 36 Physical inactivity                  Germany Females  26.5\n## 37            Diabetes United States Of America Females   8.3\n## 38          Overweight United States Of America Females  65.3\n## 39             Obesity United States Of America Females  36.3\n## 40 Physical inactivity United States Of America Females  42.1\n## 41            Diabetes                 Portugal Females   7.8\n## 42          Overweight                 Portugal Females  55.0\n## 43             Obesity                 Portugal Females  22.8\n## 44 Physical inactivity                 Portugal Females  40.8\n## 45            Diabetes           United Kingdom Females   6.9\n## 46          Overweight           United Kingdom Females  62.4\n## 47             Obesity           United Kingdom Females  31.1\n## 48 Physical inactivity           United Kingdom Females  44.3\n## 49            Diabetes               Luxembourg   Total   6.8\n## 50          Overweight               Luxembourg   Total  61.0\n## 51             Obesity               Luxembourg   Total  24.8\n## 52 Physical inactivity               Luxembourg   Total  30.0\n## 53            Diabetes                   France   Total   8.0\n## 54          Overweight                   France   Total  64.1\n## 55             Obesity                   France   Total  25.7\n## 56 Physical inactivity                   France   Total  26.4\n## 57            Diabetes                  Germany   Total   7.4\n## 58          Overweight                  Germany   Total  59.7\n## 59             Obesity                  Germany   Total  22.7\n## 60 Physical inactivity                  Germany   Total  23.4\n## 61            Diabetes United States Of America   Total   9.1\n## 62          Overweight United States Of America   Total  69.6\n## 63             Obesity United States Of America   Total  35.0\n## 64 Physical inactivity United States Of America   Total  35.0\n## 65            Diabetes                 Portugal   Total   9.2\n## 66          Overweight                 Portugal   Total  59.8\n## 67             Obesity                 Portugal   Total  22.1\n## 68 Physical inactivity                 Portugal   Total  37.3\n## 69            Diabetes           United Kingdom   Total   7.7\n## 70          Overweight           United Kingdom   Total  66.7\n## 71             Obesity           United Kingdom   Total  29.8\n## 72 Physical inactivity           United Kingdom   Total  40.0\n\nNow let’s go for the plot:\n\nggplot(diabetes) + theme_fivethirtyeight() + scale_fill_hc() +\n    geom_bar(aes(y = Share, x = Sex, fill = Country), \n             stat = \"identity\", position = \"dodge\") +\n    facet_wrap(~Condition)\n\n\n\n\nThat was a whole lot of work for such a simple plot!\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2015-01-12-introduction-to-programming-econometrics-with-r.html",
    "href": "posts/2015-01-12-introduction-to-programming-econometrics-with-r.html",
    "title": "Introduction to programming econometrics with R",
    "section": "",
    "text": "This semester, I’ll be teaching an introduction to applied econometrics with R, so I’ve decided to write a very small book called “Introduction to programming Econometrics with R”. This is primarily intended for bachelor students and the focus is not much on econometric theory, but more on how to implement econometric theory into computer code, using the R programming language. It’s very basic and doesn’t cover any advanced topics in econometrics and is intended for people with 0 previous programming knowledge. It is still very rough around the edges, and it’s missing the last chapter about reproducible research, and the references, but I think it’s time to put it out there; someone else than my students may find it useful. The book’s probably full of typos and mistakes, so don’t hesitate to drop me an e-mail if you find something fishy: contact@brodrigues.co\nAlso there might be some sections at the beginning that only concern my students. Just ignore that.\nGet it here: download\n\nUpdate (2017-01-22)\nYou might find the book useful as it is now, but I never had a chance to finish it. I might get back to it once I’ll have more time, and port it to bookdown."
  },
  {
    "objectID": "posts/2018-02-16-importing_30gb_of_data.html",
    "href": "posts/2018-02-16-importing_30gb_of_data.html",
    "title": "Importing 30GB of data into R with sparklyr",
    "section": "",
    "text": "Disclaimer: the first part of this blog post draws heavily from Working with CSVs on the Command Line, which is a beautiful resource that lists very nice tips and tricks to work with CSV files before having to load them into R, or any other statistical software. I highly recommend it! Also, if you find this interesting, read also Data Science at the Command Line another great resource!\n\n\nIn this blog post I am going to show you how to analyze 30GB of data. 30GB of data does not qualify as big data, but it’s large enough that you cannot simply import it into R and start working on it, unless you have a machine with a lot of RAM.\n\n\nLet’s start by downloading some data. I am going to import and analyze (very briefly) the airline dataset that you can download from Microsoft here. I downloaded the file AirOnTimeCSV.zip from AirOnTime87to12. Once you decompress it, you’ll end up with 303 csv files, each around 80MB. Before importing them into R, I will use command line tools to bind the rows together. But first, let’s make sure that the datasets all have the same columns. I am using Linux, and if you are too, or if you are using macOS, you can follow along. Windows users that installed the Linux Subsystem can also use the commands I am going to show! First, I’ll use the head command in bash. If you’re familiar with head() from R, the head command in bash works exactly the same:\n\n[18-02-15 21:12] brodriguesco in /Documents/AirOnTimeCSV ➤ head -5 airOT198710.csv\n\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\",\"FL_DATE\",\"UNIQUE_CARRIER\",\"TAIL_NUM\",\"FL_NUM\",\n1987,10,1,4,1987-10-01,\"AA\",\"\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0901\",1.00,\n1987,10,2,5,1987-10-02,\"AA\",\"\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0901\",1.00\n1987,10,3,6,1987-10-03,\"AA\",\"\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0859\",-1.00\n1987,10,4,7,1987-10-04,\"AA\",\"\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0900\",0.00,\n\nlet’s also check the 5 first lines of the last file:\n\n[18-02-15 21:13] cbrunos in brodriguesco in /Documents/AirOnTimeCSV ➤ head -5 airOT201212.csv\n\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\",\"FL_DATE\",\"UNIQUE_CARRIER\",\"TAIL_NUM\",\"FL_NUM\",\n2012,12,1,6,2012-12-01,\"AA\",\"N322AA\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0852\",\n2012,12,2,7,2012-12-02,\"AA\",\"N327AA\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0853\",\n2012,12,3,1,2012-12-03,\"AA\",\"N319AA\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"0856\"\n2012,12,4,2,2012-12-04,\"AA\",\"N329AA\",\"1\",12478,\"JFK\",\"NY\",12892,\"LAX\",\"CA\",\"0900\",\"1006\"\n\nWhy do that in bash instead of R? This way, I don’t need to import the data into R before checking its contents!\n\n\nIt does look like the structure did not change. Before importing the data into R, I am going to bind the rows of the datasets using other command line tools. Again, the reason I don’t import all the files into R is because I would need around 30GB of RAM to do so. So it’s easier to do it with bash:\n\nhead -1 airOT198710.csv &gt; combined.csv\nfor file in $(ls airOT*); do cat $file | sed \"1 d\" &gt;&gt; combined.csv; done\n\nOn the first line I use head again to only copy the column names (the first line of the first file) into a new file called combined.csv.\n\n\nThis &gt; operator looks like the now well known pipe operator in R, %&gt;%, but in bash, %&gt;% is actually |, not &gt;. &gt; redirects the output of the left hand side to a file on the right hand side, not to another command. On the second line, I loop over the files. I list the files with ls, and because I want only to loop over those that are named airOTxxxxx I use a regular expression, airOT* to only list those. The second part is do cat $file. do is self-explanatory, and cat stands for catenate. Think of it as head, but on all rows instead of just 5; it prints $file to the terminal. $file one element of the list of files I am looping over. But because I don’t want to see the contents of $file on my terminal, I redirect the output with the pipe, | to another command, sed. sed has an option, \"1 d\", and what this does is filtering out the first line, containing the header, from $file before appending it with &gt;&gt; to combined.csv. If you found this interesting, read more about it here.\n\n\nThis creates a 30GB CSV file that you can then import. But how? There seems to be different ways to import and work with larger than memory data in R using your personal computer. I chose to use {sparklyr}, an R package that allows you to work with Apache Spark from R. Apache Spark is a fast and general engine for large-scale data processing, and {sparklyr} not only offers bindings to it, but also provides a complete {dplyr} backend. Let’s start:\n\nlibrary(sparklyr)\nlibrary(tidyverse)\n\nspark_dir = \"/my_2_to_disk/spark/\"\n\nI first load {sparklyr} and the {tidyverse} and also define a spark_dir. This is because Spark creates a lot of temporary files that I want to save there instead of my root partition, which is on my SSD. My root partition only has around 20GO of space left, so whenever I tried to import the data I would get the following error:\n\njava.io.IOException: No space left on device\n\nIn order to avoid this error, I define this directory on my 2TO hard disk. I then define the temporary directory using the two lines below:\n\nconfig = spark_config()\n\nconfig$`sparklyr.shell.driver-java-options` &lt;-  paste0(\"-Djava.io.tmpdir=\", spark_dir)\n\nThis is not sufficient however; when I tried to read in the data, I got another error:\n\njava.lang.OutOfMemoryError: Java heap space\n\nThe solution for this one is to add the following lines to your config():\n\nconfig$`sparklyr.shell.driver-memory` &lt;- \"4G\"\nconfig$`sparklyr.shell.executor-memory` &lt;- \"4G\"\nconfig$`spark.yarn.executor.memoryOverhead` &lt;- \"512\"\n\nFinally, I can load the data. Because I am working on my machine, I connect to a \"local\" Spark instance. Then, using spark_read_csv(), I specify the Spark connection, sc, I give a name to the data that will be inside the database and the path to it:\n\nsc = spark_connect(master = \"local\", config = config)\n\nair = spark_read_csv(sc, name = \"air\", path = \"combined.csv\")\n\nOn my machine, this took around 25 minutes, and RAM usage was around 6GO.\n\n\nIt is possible to use standard {dplyr} verbs with {sparklyr} objects, so if I want the mean delay at departure per day, I can simply write:\n\ntic = Sys.time()\nmean_dep_delay = air %&gt;%\n  group_by(YEAR, MONTH, DAY_OF_MONTH) %&gt;%\n  summarise(mean_delay = mean(DEP_DELAY))\n(toc = Sys.time() - tic)\nTime difference of 0.05634999 secs\n\nThat’s amazing, only 0.06 seconds to compute these means! Wait a minute, that’s weird… I mean my computer is brand new and quite powerful but still… Let’s take a look at mean_dep_delay:\n\nhead(mean_dep_delay)\n# Source:   lazy query [?? x 4]\n# Database: spark_connection\n# Groups:   YEAR, MONTH\n   YEAR MONTH DAY_OF_MONTH mean_delay\n  &lt;int&gt; &lt;int&gt;        &lt;int&gt;      &lt;dbl&gt;\n1  1987    10            9       6.71\n2  1987    10           10       3.72\n3  1987    10           12       4.95\n4  1987    10           14       4.53\n5  1987    10           23       6.48\n6  1987    10           29       5.77\nWarning messages:\n1: Missing values are always removed in SQL.\nUse `AVG(x, na.rm = TRUE)` to silence this warning\n2: Missing values are always removed in SQL.\nUse `AVG(x, na.rm = TRUE)` to silence this warning\n\nSurprisingly, this takes around 5 minutes to print? Why? Look at the class of mean_dep_delay: it’s a lazy query that only gets evaluated once I need it. Look at the first line; lazy query [?? x 4]. This means that I don’t even know how many rows are in mean_dep_delay! The contents of mean_dep_delay only get computed once I explicitly ask for them. I do so with the collect() function, which transfers the Spark object into R’s memory:\n\ntic = Sys.time()\nr_mean_dep_delay = collect(mean_dep_delay)\n(toc = Sys.time() - tic)\nTime difference of 5.2399 mins\n\nAlso, because it took such a long time to compute: I save it to disk:\n\nsaveRDS(r_mean_dep_delay, \"mean_dep_delay.rds\")\n\nSo now that I transferred this sparklyr table to a standard tibble in R, I can create a nice plot of departure delays:\n\nlibrary(lubridate)\n\ndep_delay =  r_mean_dep_delay %&gt;%\n  arrange(YEAR, MONTH, DAY_OF_MONTH) %&gt;%\n  mutate(date = ymd(paste(YEAR, MONTH, DAY_OF_MONTH, sep = \"-\")))\n\nggplot(dep_delay, aes(date, mean_delay)) + geom_smooth()\n## `geom_smooth()` using method = 'gam'\n\n\n\n\nThat’s it for now, but in a future blog post I will continue to explore this data!\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2016-07-26-read-a-lot-of-datasets-at-once-with-r.html",
    "href": "posts/2016-07-26-read-a-lot-of-datasets-at-once-with-r.html",
    "title": "Read a lot of datasets at once with R",
    "section": "",
    "text": "I often have to read a lot of datasets at once using R. So I’ve wrote the following function to solve this issue:\n\nread_list &lt;- function(list_of_datasets, read_func){\n\n        read_and_assign &lt;- function(dataset, read_func){\n                dataset_name &lt;- as.name(dataset)\n                dataset_name &lt;- read_func(dataset)\n        }\n\n        # invisible is used to suppress the unneeded output\n        output &lt;- invisible(\n                sapply(list_of_datasets,\n                           read_and_assign, read_func = read_func, simplify = FALSE, USE.NAMES = TRUE))\n\n        # Remove the extension at the end of the data set names\n        names_of_datasets &lt;- c(unlist(strsplit(list_of_datasets, \"[.]\"))[c(T, F)])\n        names(output) &lt;- names_of_datasets\n        return(output)\n}\n\nYou need to supply a list of datasets as well as the function to read the datasets to read_list. So for example to read in .csv files, you could use read.csv() (or read_csv() from the readr package, which I prefer to use), or read_dta() from the package haven for STATA files, and so on.\n\n\nNow imagine you have some data in your working directory. First start by saving the name of the datasets in a variable:\n\ndata_files &lt;- list.files(pattern = \".csv\")\n\nprint(data_files)\n## [1] \"data_1.csv\" \"data_2.csv\" \"data_3.csv\"\n\nNow you can read all the data sets and save them in a list with read_list():\n\nlibrary(\"readr\")\nlibrary(\"tibble\")\n\nlist_of_data_sets &lt;- read_list(data_files, read_csv)\n\n\nglimpse(list_of_data_sets)\n## List of 3\n##  $ data_1:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,018930679\" \"0,8748013128\" \"0,1025635934\" \"0,6246140983\" ...\n##   ..$ col2: chr [1:19] \"0,0377725807\" \"0,5959457638\" \"0,4429121533\" \"0,558387159\" ...\n##   ..$ col3: chr [1:19] \"0,6241767189\" \"0,031324594\" \"0,2238059868\" \"0,2773350732\" ...\n##  $ data_2:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9098418493\" \"0,1127788509\" \"0,5818891392\" \"0,1011773532\" ...\n##   ..$ col2: chr [1:19] \"0,7455905887\" \"0,4015039612\" \"0,6625796605\" \"0,029955339\" ...\n##   ..$ col3: chr [1:19] \"0,327232932\" \"0,2784035673\" \"0,8092386735\" \"0,1216045306\" ...\n##  $ data_3:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9236124896\" \"0,6303271761\" \"0,6413583054\" \"0,5573887416\" ...\n##   ..$ col2: chr [1:19] \"0,2114708388\" \"0,6984538266\" \"0,0469865249\" \"0,9271510226\" ...\n##   ..$ col3: chr [1:19] \"0,4941919971\" \"0,7391538511\" \"0,3876723797\" \"0,2815014394\" ...\n\nIf you prefer not to have the datasets in a list, but rather import them into the global environment, you can change the above function like so:\n\nread_list &lt;- function(list_of_datasets, read_func){\n\n        read_and_assign &lt;- function(dataset, read_func){\n                assign(dataset, read_func(dataset), envir = .GlobalEnv)\n        }\n\n        # invisible is used to suppress the unneeded output\n        output &lt;- invisible(\n                sapply(list_of_datasets,\n                           read_and_assign, read_func = read_func, simplify = FALSE, USE.NAMES = TRUE))\n\n}\n\nBut I personnally don’t like this second option, but I put it here for completeness."
  },
  {
    "objectID": "posts/2018-03-03-sparklyr_h2o_rsparkling.html",
    "href": "posts/2018-03-03-sparklyr_h2o_rsparkling.html",
    "title": "Getting {sparklyr}, {h2o}, {rsparkling} to work together and some fun with bash",
    "section": "",
    "text": "This is going to be the type of blog posts that would perhaps be better as a gist, but it is easier for me to use my blog as my own personal collection of gists. Plus, someone else might find this useful, so here it is! In this blog post I am going to show a little trick to randomly sample rows from a text file using bash, and then train a model using the {h2o} package. I will also use the {rsparkling} package. From {rsparkling}’s documentation: {rsparkling} is a package that provides an R interface to the H2O Sparkling Water machine learning library. and will be needed to transfer the data from Spark to H2O.\n\n\nIn a previous blog post I used the {sparklyr} package to load a 30GB csv file into R. I created the file by combining around 300 csv files, each around 80MB big. Here, I would like to use the machine learning functions included in the {h2o} packages to train a random forest on this data. However, I only want to have a simple prototype that simply runs, and check if all the packages work well together. If everything is ok, I’ll keep iterating to make the model better (in a possible subsequent post).\n\n\nFor fast prototyping, using 30GB of data is not a good idea, so I am going to sample 500000 from this file using the linux command line (works on macOS too and also on Windows if you installed the linux subsystem). Why not use R to sample 500000 rows? Because on my machine, loading the 30GB file takes 25 minutes. Sampling half a million lines from it would take quite long too. So here are some bash lines that do that directly on the file, without needing to load it into R beforehand:\n\n[18-03-03 21:50] brodriguesco in /Documents/AirOnTimeCSV ➤ get_seeded_random()\n{\n  seed=\"$1\"\n  openssl enc -aes-256-ctr -pass pass:\"$seed\" -nosalt \\\n  &lt;/dev/zero 2&gt;/dev/null\n}\n\n[18-03-03 21:50] brodriguesco in /Documents/AirOnTimeCSV ➤ sed \"1 d\" combined.csv | shuf --random-source=&lt;(get_seeded_random 42) -n 500000 &gt; small_combined_temp.csv\n\n[18-03-03 21:56] brodriguesco in /Documents/AirOnTimeCSV ➤ head -1 combined.csv &gt; colnames.csv\n\n[18-03-03 21:56] brodriguesco in /Documents/AirOnTimeCSV ➤ cat colnames.csv small_combined_temp.csv &gt; small_combined.csv\n\nThe first function I took from the gnu coreutils manual which allows me to fix the random seed to reproduce the same sampling of the file. Then I use \"sed 1 d\" cobmined.csv to remove the first line of combined.csv which is the header of the file. Then, I pipe the result of sed using | to shuf which does the shuffling. The option –random-source=&lt;(get_seeded_random 42) fixes the seed, and -n 500000 only shuffles 500000 and not the whole file. The final bit of the line, &gt; small_combined_temp.csv, saves the result to small_cobmined_temp.csv. Because I need to add back the header, I use head -1 to extract the first line of combined.csv and save it into colnames.csv. Finally, I bind the rows of both files using cat colnames.csv small_combined_temp.csv and save the result into small_combined.cvs. Taken together, all these steps took about 5 minutes (without counting the googling around for finding how to pass a fixed seed to shuf).\n\n\nNow that I have this small dataset, I can write a small prototype:\n\n\nFirst, you need to install {sparklyr}, {rsparkling} and {h2o}. Refer to this to know how to install the packages. I had a mismatch between the version of H2O that was automatically installed when I installed the {h2o} package, and the version of Spark that {sparklyr} installed but thankfully the {h2o} package returns a very helpful error message with the following lines:\n\ndetach(\"package:rsparkling\", unload = TRUE)\n                       if (\"package:h2o\" %in% search()) { detach(\"package:h2o\", unload = TRUE) }\n                       if (isNamespaceLoaded(\"h2o\")){ unloadNamespace(\"h2o\") }\n                       remove.packages(\"h2o\")\n                       install.packages(\"h2o\", type = \"source\", repos = \"https://h2o-release.s3.amazonaws.com/h2o/rel-weierstrass/2/R\")\n\nwhich tells you which version to install.\n\n\nSo now, let’s load everything:\n\nlibrary(sparklyr)\nlibrary(rsparkling)\nlibrary(h2o)\n## \n## ----------------------------------------------------------------------\n## \n## Your next step is to start H2O:\n##     &gt; h2o.init()\n## \n## For H2O package documentation, ask for help:\n##     &gt; ??h2o\n## \n## After starting H2O, you can use the Web UI at http://localhost:54321\n## For more information visit http://docs.h2o.ai\n## \n## ----------------------------------------------------------------------\n## \n## Attaching package: 'h2o'\n## The following objects are masked from 'package:stats':\n## \n##     cor, sd, var\n## The following objects are masked from 'package:base':\n## \n##     &&, %*%, %in%, ||, apply, as.factor, as.numeric, colnames,\n##     colnames&lt;-, ifelse, is.character, is.factor, is.numeric, log,\n##     log10, log1p, log2, round, signif, trunc\nh2o.init()\n## \n## H2O is not running yet, starting it now...\n## \n## Note:  In case of errors look at the following log files:\n##     /tmp/Rtmph48vf9/h2o_cbrunos_started_from_r.out\n##     /tmp/Rtmph48vf9/h2o_cbrunos_started_from_r.err\n## \n## \n## Starting H2O JVM and connecting: .. Connection successful!\n## \n## R is connected to the H2O cluster: \n##     H2O cluster uptime:         1 seconds 944 milliseconds \n##     H2O cluster version:        3.16.0.2 \n##     H2O cluster version age:    4 months and 15 days !!! \n##     H2O cluster name:           H2O_started_from_R_cbrunos_bpn152 \n##     H2O cluster total nodes:    1 \n##     H2O cluster total memory:   6.98 GB \n##     H2O cluster total cores:    12 \n##     H2O cluster allowed cores:  12 \n##     H2O cluster healthy:        TRUE \n##     H2O Connection ip:          localhost \n##     H2O Connection port:        54321 \n##     H2O Connection proxy:       NA \n##     H2O Internal Security:      FALSE \n##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 \n##     R Version:                  R version 3.4.4 (2018-03-15)\n## Warning in h2o.clusterInfo(): \n## Your H2O cluster version is too old (4 months and 15 days)!\n## Please download and install the latest version from http://h2o.ai/download/\n\nI left all the startup messages because they’re quite helpful. Especially that bit telling you to start H2O with h2o.init(). If something’s wrong, h2o.init() will give you helpful information.\n\n\nNow that all this is loaded, I can start working on the data (the steps below are explained in detail in my previous blog post):\n\nspark_dir = \"/my_2_to_disk/spark/\"\n\nconfig = spark_config()\n\nconfig$`sparklyr.shell.driver-memory` &lt;- \"4G\"\nconfig$`sparklyr.shell.executor-memory` &lt;- \"4G\"\nconfig$`spark.yarn.executor.memoryOverhead` &lt;- \"512\"\nconfig$`sparklyr.shell.driver-java-options` = paste0(\"-Djava.io.tmpdir=\", spark_dir)\n\nsc = spark_connect(master = \"local\", config = config)\n\nAnother useful function that allows you to check if everything is alright is h2o_context():\n\nh2o_context(sc)\n&lt;jobj[12]&gt;\n  org.apache.spark.h2o.H2OContext\n\nSparkling Water Context:\n * H2O name: sparkling-water-cbrunos_local-1520111879840\n * cluster size: 1\n * list of used nodes:\n  (executorId, host, port)\n  ------------------------\n  (driver,127.0.0.1,54323)\n  ------------------------\n\n  Open H2O Flow in browser: http://127.0.0.1:54323 (CMD + click in Mac OSX)\n\n\nNow, let’s load the data into R with {sparklyr}:\n\nair = spark_read_csv(sc, name = \"air\", path = \"small_combined.csv\")\n\nOf course, here, using Spark is overkill, because small_combined.csv is only around 100MB big, so no need for {sparklyr} but as stated in the beginning this is only to have a quick and dirty prototype. Once all the pieces are working together, I can iterate on the real data, for which {sparklyr} will be needed. Now, if I needed to use {dplyr} I could use it on air, but I don’t want to do anything on it, so I convert it to a h2o data frame. h2o data frames are needed as arguments for the machine learning algorithms included in the {h2o} package. as_h2o_frame() is a function included in {rsparkling}:\n\nair_hf = as_h2o_frame(sc, air)\n\nThen, I convert the columns I need to factors (I am only using factors here):\n\nair_hf$ORIGIN = as.factor(air_hf$ORIGIN)\nair_hf$UNIQUE_CARRIER = as.factor(air_hf$UNIQUE_CARRIER)\nair_hf$DEST = as.factor(air_hf$DEST)\n\n{h2o} functions need the names of the predictors and of the target columns, so let’s define that:\n\ntarget = \"ARR_DELAY\"\npredictors = c(\"UNIQUE_CARRIER\", \"ORIGIN\", \"DEST\")\n\nNow, let’s train a random Forest, without any hyper parameter tweaking:\n\nmodel = h2o.randomForest(predictors, target, training_frame = air_hf)\n\nNow that this runs, I will in the future split the data into training, validation and test set, and train a model with better hyper parameters. For now, let’s take a look at the summary of model:\n\nsummary(model)\nModel Details:\n==============\n\nH2ORegressionModel: drf\nModel Key:  DRF_model_R_1520111880605_1\nModel Summary:\n  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n1              50                       50            11055998        20\n  max_depth mean_depth min_leaves max_leaves mean_leaves\n1        20   20.00000       1856       6129  4763.42000\n\nH2ORegressionMetrics: drf\n** Reported on training data. **\n** Metrics reported on Out-Of-Bag training samples **\n\nMSE:  964.9246\nRMSE:  31.06324\nMAE:  17.65517\nRMSLE:  NaN\nMean Residual Deviance :  964.9246\n\n\n\n\n\nScoring History:\n             timestamp   duration number_of_trees training_rmse training_mae\n1  2018-03-03 22:52:24  0.035 sec               0\n2  2018-03-03 22:52:25  1.275 sec               1      30.93581     17.78216\n3  2018-03-03 22:52:25  1.927 sec               2      31.36998     17.78867\n4  2018-03-03 22:52:26  2.272 sec               3      31.36880     17.80359\n5  2018-03-03 22:52:26  2.564 sec               4      31.29683     17.79467\n6  2018-03-03 22:52:26  2.854 sec               5      31.31226     17.79467\n7  2018-03-03 22:52:27  3.121 sec               6      31.26214     17.78542\n8  2018-03-03 22:52:27  3.395 sec               7      31.20749     17.75703\n9  2018-03-03 22:52:27  3.666 sec               8      31.19706     17.74753\n10 2018-03-03 22:52:27  3.935 sec               9      31.16108     17.73547\n11 2018-03-03 22:52:28  4.198 sec              10      31.13725     17.72493\n12 2018-03-03 22:52:32  8.252 sec              27      31.07608     17.66648\n13 2018-03-03 22:52:36 12.462 sec              44      31.06325     17.65474\n14 2018-03-03 22:52:38 14.035 sec              50      31.06324     17.65517\n   training_deviance\n1\n2          957.02450\n3          984.07580\n4          984.00150\n5          979.49147\n6          980.45794\n7          977.32166\n8          973.90720\n9          973.25655\n10         971.01272\n11         969.52856\n12         965.72249\n13         964.92530\n14         964.92462\n\nVariable Importances: (Extract with `h2o.varimp`)\n=================================================\n\nVariable Importances:\n        variable relative_importance scaled_importance percentage\n1         ORIGIN    291883392.000000          1.000000   0.432470\n2           DEST    266749168.000000          0.913890   0.395230\n3 UNIQUE_CARRIER    116289536.000000          0.398411   0.172301\n&gt;\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2017-07-27-spread_rename_at.html",
    "href": "posts/2017-07-27-spread_rename_at.html",
    "title": "tidyr::spread() and dplyr::rename_at() in action",
    "section": "",
    "text": "I was recently confronted to a situation that required going from a long dataset to a wide dataset, but with a small twist: there were two datasets, which I had to merge into one. You might wonder what kinda crappy twist that is, right? Well, let’s take a look at the data:\n\ndata1; data2\n## # A tibble: 20 x 4\n##    country date       variable_1       value\n##    &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;            &lt;int&gt;\n##  1 lu      01/01/2005 maybe               22\n##  2 lu      01/07/2005 maybe               13\n##  3 lu      01/01/2006 maybe               40\n##  4 lu      01/07/2006 maybe               25\n##  5 lu      01/01/2005 totally_agree       42\n##  6 lu      01/07/2005 totally_agree       17\n##  7 lu      01/01/2006 totally_agree       25\n##  8 lu      01/07/2006 totally_agree       16\n##  9 lu      01/01/2005 totally_disagree    39\n## 10 lu      01/07/2005 totally_disagree    17\n## 11 lu      01/01/2006 totally_disagree    23\n## 12 lu      01/07/2006 totally_disagree    21\n## 13 lu      01/01/2005 kinda_disagree      69\n## 14 lu      01/07/2005 kinda_disagree      12\n## 15 lu      01/01/2006 kinda_disagree      10\n## 16 lu      01/07/2006 kinda_disagree       9\n## 17 lu      01/01/2005 kinda_agree         38\n## 18 lu      01/07/2005 kinda_agree         31\n## 19 lu      01/01/2006 kinda_agree         19\n## 20 lu      01/07/2006 kinda_agree         12\n## # A tibble: 20 x 4\n##    country date       variable_2       value\n##    &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;            &lt;int&gt;\n##  1 lu      01/01/2005 kinda_agree         22\n##  2 lu      01/07/2005 kinda_agree         13\n##  3 lu      01/01/2006 kinda_agree         40\n##  4 lu      01/07/2006 kinda_agree         25\n##  5 lu      01/01/2005 totally_agree       42\n##  6 lu      01/07/2005 totally_agree       17\n##  7 lu      01/01/2006 totally_agree       25\n##  8 lu      01/07/2006 totally_agree       16\n##  9 lu      01/01/2005 totally_disagree    39\n## 10 lu      01/07/2005 totally_disagree    17\n## 11 lu      01/01/2006 totally_disagree    23\n## 12 lu      01/07/2006 totally_disagree    21\n## 13 lu      01/01/2005 maybe               69\n## 14 lu      01/07/2005 maybe               12\n## 15 lu      01/01/2006 maybe               10\n## 16 lu      01/07/2006 maybe                9\n## 17 lu      01/01/2005 kinda_disagree      38\n## 18 lu      01/07/2005 kinda_disagree      31\n## 19 lu      01/01/2006 kinda_disagree      19\n## 20 lu      01/07/2006 kinda_disagree      12\n\nAs explained in Hadley (2014), this is how you should keep your data… But for a particular purpose, I had to transform these datasets. What I was asked to do was to merge these into a single wide data frame. Doing this for one dataset is easy:\n\ndata1 %&gt;%\n  spread(variable_1, value)\n## # A tibble: 4 x 7\n##   country date       kinda_agree kinda_disagree maybe totally_agree\n##   &lt;chr&gt;   &lt;chr&gt;            &lt;int&gt;          &lt;int&gt; &lt;int&gt;         &lt;int&gt;\n## 1 lu      01/01/2005          38             69    22            42\n## 2 lu      01/01/2006          19             10    40            25\n## 3 lu      01/07/2005          31             12    13            17\n## 4 lu      01/07/2006          12              9    25            16\n## # ... with 1 more variable: totally_disagree &lt;int&gt;\n\nBut because data1 and data2 have the same levels for variable_1 and variable_2, this would not work. So the solution I found online, in this SO thread was to use tidyr::spread() with dplyr::rename_at() like this:\n\ndata1 &lt;- data1 %&gt;%\n  spread(variable_1, value) %&gt;%\n  rename_at(vars(-country, -date), funs(paste0(\"variable1:\", .)))\n\nglimpse(data1)\n## Observations: 4\n## Variables: 7\n## $ country                      &lt;chr&gt; \"lu\", \"lu\", \"lu\", \"lu\"\n## $ date                         &lt;chr&gt; \"01/01/2005\", \"01/01/2006\", \"01/0...\n## $ `variable1:kinda_agree`      &lt;int&gt; 38, 19, 31, 12\n## $ `variable1:kinda_disagree`   &lt;int&gt; 69, 10, 12, 9\n## $ `variable1:maybe`            &lt;int&gt; 22, 40, 13, 25\n## $ `variable1:totally_agree`    &lt;int&gt; 42, 25, 17, 16\n## $ `variable1:totally_disagree` &lt;int&gt; 39, 23, 17, 21\ndata2 &lt;- data2 %&gt;%\n  spread(variable_2, value) %&gt;%\n  rename_at(vars(-country, -date), funs(paste0(\"variable2:\", .)))\n\nglimpse(data2)\n## Observations: 4\n## Variables: 7\n## $ country                      &lt;chr&gt; \"lu\", \"lu\", \"lu\", \"lu\"\n## $ date                         &lt;chr&gt; \"01/01/2005\", \"01/01/2006\", \"01/0...\n## $ `variable2:kinda_agree`      &lt;int&gt; 22, 40, 13, 25\n## $ `variable2:kinda_disagree`   &lt;int&gt; 38, 19, 31, 12\n## $ `variable2:maybe`            &lt;int&gt; 69, 10, 12, 9\n## $ `variable2:totally_agree`    &lt;int&gt; 42, 25, 17, 16\n## $ `variable2:totally_disagree` &lt;int&gt; 39, 23, 17, 21\n\nrename_at() needs variables which you pass to vars(), a helper function to select variables, and a function that will do the renaming, passed to funs(). The function I use is simply paste0(), which pastes a string, for example “variable1:” with the name of the columns, given by the single ‘.’, a dummy argument. Now these datasets can be merged:\n\ndata1 %&gt;%\n  full_join(data2) %&gt;%\n  glimpse()\n## Joining, by = c(\"country\", \"date\")\n## Observations: 4\n## Variables: 12\n## $ country                      &lt;chr&gt; \"lu\", \"lu\", \"lu\", \"lu\"\n## $ date                         &lt;chr&gt; \"01/01/2005\", \"01/01/2006\", \"01/0...\n## $ `variable1:kinda_agree`      &lt;int&gt; 38, 19, 31, 12\n## $ `variable1:kinda_disagree`   &lt;int&gt; 69, 10, 12, 9\n## $ `variable1:maybe`            &lt;int&gt; 22, 40, 13, 25\n## $ `variable1:totally_agree`    &lt;int&gt; 42, 25, 17, 16\n## $ `variable1:totally_disagree` &lt;int&gt; 39, 23, 17, 21\n## $ `variable2:kinda_agree`      &lt;int&gt; 22, 40, 13, 25\n## $ `variable2:kinda_disagree`   &lt;int&gt; 38, 19, 31, 12\n## $ `variable2:maybe`            &lt;int&gt; 69, 10, 12, 9\n## $ `variable2:totally_agree`    &lt;int&gt; 42, 25, 17, 16\n## $ `variable2:totally_disagree` &lt;int&gt; 39, 23, 17, 21\n\nHope this post helps you understand the difference between long and wide datasets better, as well as dplyr::rename_at()!"
  },
  {
    "objectID": "posts/2016-11-04-ive-started-writing-a-book-functional-programming-and-unit-testing-for-data-munging-with-r.html",
    "href": "posts/2016-11-04-ive-started-writing-a-book-functional-programming-and-unit-testing-for-data-munging-with-r.html",
    "title": "I’ve started writing a ‘book’: Functional programming and unit testing for data munging with R",
    "section": "",
    "text": "I have started writing a ‘book’ using the awesome bookdown package. In the book I explain and show why using functional programming and putting your functions in your own packages is the way to go when you want to clean, prepare and transform large data sets. It makes testing and documenting your code easier. You don’t need to think about managing paths either. The book is far from complete, but I plan on working on it steadily. For now, you can read an intro to functional programming, unit testing and creating your own packages that will hold your code. I also show you can write documentation for your functions. I am also looking for feedback; so if you have any suggestions, do not hesitate to shoot me an email or a tweet! You can read the book by clicking here."
  },
  {
    "objectID": "posts/2015-11-11-bootstrapping-did-with-r.html",
    "href": "posts/2015-11-11-bootstrapping-did-with-r.html",
    "title": "Bootstrapping standard errors for difference-in-differences estimation with R",
    "section": "",
    "text": "I’m currently working on a paper (with my colleague Vincent Vergnat who is also a Phd candidate at BETA) where I want to estimate the causal impact of the birth of a child on hourly and daily wages as well as yearly worked hours. For this we are using non-parametric difference-in-differences (henceforth DiD) and thus have to bootstrap the standard errors. In this post, I show how this is possible using the function boot.\n\n\nFor this we are going to replicate the example from Wooldridge’s Econometric Analysis of Cross Section and Panel Data and more specifically the example on page 415. You can download the data for R here. The question we are going to try to answer is how much does the price of housing decrease due to the presence of an incinerator in the neighborhood?\n\n\nFirst put the data in a folder and set the correct working directory and load the boot library.\n\nlibrary(boot)\nsetwd(\"/home/path/to/data/kiel data/\")\nload(\"kielmc.RData\")\n\nNow you need to write a function that takes the data as an argument, as well as an indices argument. This argument is used by the boot function to select samples. This function should return the statistic you’re interested in, in our case, the DiD estimate.\n\nrun_DiD &lt;- function(my_data, indices){\n    d &lt;- my_data[indices,]\n    return(\n        mean(d$rprice[d$year==1981 & d$nearinc==1]) - \n        mean(d$rprice[d$year==1981 & d$nearinc==0]) - \n        (mean(d$rprice[d$year==1978 & d$nearinc==1]) - \n        mean(d$rprice[d$year==1978 & d$nearinc==0]))\n    )\n}\n\nYou’re almost done! To bootstrap your DiD estimate you just need to use the boot function. If you have cpu with multiple cores (which you should, single core machines are quite outdated by now) you can even parallelize the bootstrapping.\n\nboot_est &lt;- boot(data, run_DiD, R=1000, parallel=\"multicore\", ncpus = 2)\n\nNow you should just take a look at your estimates:\n\nboot_est\n \nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = data, statistic = run_DiD, R = 1000, parallel = \"multicore\", \n ncpus = 2)\n\n\nBootstrap Statistics :\n original    bias    std. error\nt1* -11863.9 -553.3393    8580.435\n\nThese results are very similar to the ones in the book, only the standard error is higher.\n\n\nYou can get confidence intervals like this:\n\nquantile(boot_est$t, c(0.025, 0.975))\n##       2.5%      97.5% \n## -30186.397   3456.133\n\nor a t-statistic:\n\nboot_est$t0/sd(boot_est$t)\n## [1] -1.382669\n\nOr the density of the replications:\n\nplot(density(boot_est$t))\n\n&lt;img src=\"/img/density_did.png\" width=\"670\" height=\"450\" /&gt;&lt;/a&gt;\n\n\nJust as in the book, we find that the DiD estimate is not significant to the 5% level."
  },
  {
    "objectID": "posts/2025-01-09-github_pages_setup_with_quarto.html",
    "href": "posts/2025-01-09-github_pages_setup_with_quarto.html",
    "title": "github pages setup for this website",
    "section": "",
    "text": "Desired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\n\n1 + 1"
  },
  {
    "objectID": "posts/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions.html",
    "href": "posts/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions.html",
    "title": "Data frame columns as arguments to dplyr functions",
    "section": "",
    "text": "Suppose that you would like to create a function which does a series of computations on a data frame. You would like to pass a column as this function’s argument. Something like:\n\ndata(cars)\nconvertToKmh &lt;- function(dataset, col_name){\n  dataset$col_name &lt;- dataset$speed * 1.609344\n  return(dataset)\n}\n\nThis example is obviously not very interesting (you don’t need a function for this), but it will illustrate the point. You would like to append a column called speed_in_kmh with the speed in kilometers per hour to this dataset, but this is what happens:\n\nhead(convertToKmh(cars, \"speed_in_kmh\"))\n##   speed dist  col_name\n1     4    2  6.437376\n2     4   10  6.437376\n3     7    4 11.265408\n4     7   22 11.265408\n5     8   16 12.874752\n6     9   10 14.484096\n\nYour column is not called speed_in_kmh but col_name! It turns out that there is a very simple solution:\n\nconvertToKmh &lt;- function(dataset, col\\_name){\n  dataset[col_name] &lt;- dataset$speed * 1.609344\n  return(dataset)\n}\n\nhead(convertToKmh(cars, \"speed\\_in\\_kmh\"))\n##   speed dist speed\\_in\\_kmh\n1     4    2     6.437376\n2     4   10     6.437376\n3     7    4    11.265408\n4     7   22    11.265408\n5     8   16    12.874752\n6     9   10    14.484096\n\nYou can access columns with [] instead of $.\n\n\nBut sometimes you want to do more complex things and for example have a function that groups by a variable and then computes new variables, filters by another and so on. You would like to avoid having to hard code these variables in your function, because then why write a function and of course you would like to use dplyr to do it.\n\n\nI often use dplyr functions in my functions. For illustration purposes, consider this very simple function:\n\nsimpleFunction &lt;- function(dataset, col_name){\n  require(\"dplyr\")\n  dataset %&gt;% \n    group\\_by(col\\_name) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\nsimpleFunction(cars, \"dist\")\n\nThis function takes a dataset as an argument, as well as a column name. However, this does not work. You get this error:\n\nError: unknown variable to group by : col_name \n\nThe variable col_name is passed to simpleFunction() as a string, but group_by() requires a variable name. So why not try to convert col_name to a name?\n\nsimpleFunction &lt;- function(dataset, col_name){\n  require(\"dplyr\")\n  col\\_name &lt;- as.name(col_name)\n  dataset %&gt;% \n    group\\_by(col\\_name) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\nsimpleFunction(cars, \"dist\")\n\nYou get the same error as before:\n\nError: unknown variable to group by : col_name \n\nSo how can you pass a column name to group_by()? Well, there is another version of group_by() called group_by_() that uses standard evaluation. You can learn more about it here. Let’s take a look at what happens when we use group_by_():\n\nsimpleFunction &lt;- function(dataset, col_name){\n  require(\"dplyr\")\n  dataset %&gt;% \n    group\\_by\\_(col_name) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\nsimpleFunction(cars, \"dist\")\nA tibble: 35 x 2\n dist mean\\_speed\n&lt;dbl&gt;      &lt;dbl&gt;\n1      2        4.0\n2      4        7.0\n3     10        6.5\n4     14       12.0\n5     16        8.0\n6     17       11.0\n7     18       10.0\n8     20       13.5\n9     22        7.0\n10    24       12.0\n ... with 25 more rows\n\nWe can even use a formula instead of a string:\n\nsimpleFunction(cars, ~dist)\n A tibble: 35 x 2\n    dist mean_speed\n   &lt;dbl&gt;      &lt;dbl&gt;\n1      2        4.0\n2      4        7.0\n3     10        6.5\n4     14       12.0\n5     16        8.0\n6     17       11.0\n7     18       10.0\n8     20       13.5\n9     22        7.0\n10    24       12.0\n... with 25 more rows\n\nWhat if you want to pass column names and constants, for example to filter without hardcoding anything?\n\n\nTrying to do it naively will only yield pain and despair:\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  require(\"dplyr\")\n  dataset %&gt;% \n    filter\\_(col\\_name == value) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n&gt; simpleFunction(cars, \"dist\", 10)\n\n  mean_speed\n1        NaN\n\n&gt; simpleFunction(cars, dist, 10)\n\n Error in col_name == value : \n  comparison (1) is possible only for atomic and list types \n  \n&gt; simpleFunction(cars, ~dist, 10)\n\n  mean_speed\n1        NaN\n\n\nTo solve this issue, we need to know a little bit about two concepts, lazy evaluation and non-standard evaluation. I recommend you read the following document from Hadley Wickham’s book Advanced R as well as the part on lazy evaluation here.\n\n\nA nice package called lazyeval can help us out. We would like to make R understand that the column name is not col_name but the string inside it \"dist\", and now we would like to use filter() for dist equal to 10.\n\n\nIn the lazyeval package, you’ll find the function interp(). interp() allows you to\n\n\n\nbuild an expression up from a mixture of constants and variables.\n\n\n\nTake a look at this example:\n\nlibrary(lazyeval)\ninterp(~x+y, x = 2)\n## ~2 + y\n\nWhat you get back is this nice formula that you can then use within functions. To see why this is useful, let’s look at the above example again, and make it work using interp():\n\nsimpleFunction &lt;- function(dataset, col_name, value){\n  require(\"dplyr\")\n  require(\"lazyeval\")\n  filter\\_criteria &lt;- interp(~y == x, .values=list(y = as.name(col_name), x = value))\n  dataset %&gt;% \n    filter\\_(filter_criteria) %&gt;%\n    summarise(mean_speed = mean(speed)) -&gt; dataset\n  return(dataset)\n}\n\n\nsimpleFunction(cars, \"dist\", 10)\n  mean\\_speed\n1        6.5\n\nAnd now it works! For some reason, you have to pass the column name as a string though.\n\n\nSources: apart from the documents above, the following stackoverflow threads helped me out quite a lot: In R: pass column name as argument and use it in function with dplyr::mutate() and lazyeval::interp() and Non-standard evaluation (NSE) in dplyr’s filter_ & pulling data from MySQL."
  },
  {
    "objectID": "posts/2018-04-15-announcing_pmice.html",
    "href": "posts/2018-04-15-announcing_pmice.html",
    "title": "{pmice}, an experimental package for missing data imputation in parallel using {mice} and {furrr}",
    "section": "",
    "text": "Yesterday I wrote this blog post which showed how one could use {furrr} and {mice} to impute missing data in parallel, thus speeding up the process tremendously.\n\n\nTo make using this snippet of code easier, I quickly cobbled together an experimental package called {pmice} that you can install from Github:\n\ndevtools::install_github(\"b-rodrigues/pmice\")\n\nFor now, it returns a list of mids objects and not a mids object like mice::mice() does, but I’ll be working on it. Contributions welcome!\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2016-07-30-merge-a-list-of-datasets-together.html",
    "href": "posts/2016-07-30-merge-a-list-of-datasets-together.html",
    "title": "Merge a list of datasets together",
    "section": "",
    "text": "Last week I showed how to read a lot of datasets at once with R, and this week I’ll continue from there and show a very simple function that uses this list of read datasets and merges them all together.\n\n\nFirst we’ll use read_list() to read all the datasets at once (for more details read last week’s post):\n\nlibrary(\"readr\")\nlibrary(\"tibble\")\n\ndata_files &lt;- list.files(pattern = \".csv\")\n\nprint(data_files)\n## [1] \"data_1.csv\" \"data_2.csv\" \"data_3.csv\"\nlist_of_data_sets &lt;- read_list(data_files, read_csv)\n\nglimpse(list_of_data_sets)\n## List of 3\n##  $ data_1:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,018930679\" \"0,8748013128\" \"0,1025635934\" \"0,6246140983\" ...\n##   ..$ col2: chr [1:19] \"0,0377725807\" \"0,5959457638\" \"0,4429121533\" \"0,558387159\" ...\n##   ..$ col3: chr [1:19] \"0,6241767189\" \"0,031324594\" \"0,2238059868\" \"0,2773350732\" ...\n##  $ data_2:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9098418493\" \"0,1127788509\" \"0,5818891392\" \"0,1011773532\" ...\n##   ..$ col2: chr [1:19] \"0,7455905887\" \"0,4015039612\" \"0,6625796605\" \"0,029955339\" ...\n##   ..$ col3: chr [1:19] \"0,327232932\" \"0,2784035673\" \"0,8092386735\" \"0,1216045306\" ...\n##  $ data_3:Classes 'tbl_df', 'tbl' and 'data.frame':  19 obs. of  3 variables:\n##   ..$ col1: chr [1:19] \"0,9236124896\" \"0,6303271761\" \"0,6413583054\" \"0,5573887416\" ...\n##   ..$ col2: chr [1:19] \"0,2114708388\" \"0,6984538266\" \"0,0469865249\" \"0,9271510226\" ...\n##   ..$ col3: chr [1:19] \"0,4941919971\" \"0,7391538511\" \"0,3876723797\" \"0,2815014394\" ...\n\nYou see that all these datasets have the same column names. We can now merge them using this simple function:\n\nmulti_join &lt;- function(list_of_loaded_data, join_func, ...){\n\n    require(\"dplyr\")\n\n    output &lt;- Reduce(function(x, y) {join_func(x, y, ...)}, list_of_loaded_data)\n\n    return(output)\n}\n\nThis function uses Reduce(). Reduce() is a very important function that can be found in all functional programming languages. What does Reduce() do? Let’s take a look at the following example:\n\nReduce(`+`, c(1, 2, 3, 4, 5))\n## [1] 15\n\nReduce() has several arguments, but you need to specify at least two: a function, here + and a list, here c(1, 2, 3, 4, 5). The next code block shows what Reduce() basically does:\n\n0 + c(1, 2, 3, 4, 5)\n0 + 1 + c(2, 3, 4, 5)\n0 + 1 + 2 + c(3, 4, 5)\n0 + 1 + 2 + 3 + c(4, 5)\n0 + 1 + 2 + 3 + 4 + c(5)\n0 + 1 + 2 + 3 + 4 + 5\n\n0 had to be added as in “init”. You can also specify this “init” to Reduce():\n\nReduce(`+`, c(1, 2, 3, 4, 5), init = 20)\n## [1] 35\n\nSo what multi_join() does, is the same operation as in the example above, but where the function is a user supplied join or merge function, and the list of datasets is the one read with read_list().\n\n\nLet’s see what happens when we use multi_join() on our list:\n\nmerged_data &lt;- multi_join(list_of_data_sets, full_join)\nclass(merged_data)\n## [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\nglimpse(merged_data)\n## Observations: 57\n## Variables: 3\n## $ col1 &lt;chr&gt; \"0,018930679\", \"0,8748013128\", \"0,1025635934\", \"0,6246140...\n## $ col2 &lt;chr&gt; \"0,0377725807\", \"0,5959457638\", \"0,4429121533\", \"0,558387...\n## $ col3 &lt;chr&gt; \"0,6241767189\", \"0,031324594\", \"0,2238059868\", \"0,2773350...\n\nYou should make sure that all the data frames have the same column names but you can also join data frames with different column names if you give the argument by to the join function. This is possible thanks to … that allows you to pass further argument to join_func().\n\n\nThis function was inspired by the one found on the blog Coffee and Econometrics in the Morning."
  },
  {
    "objectID": "posts/2018-01-03-lists_all_the_way.html",
    "href": "posts/2018-01-03-lists_all_the_way.html",
    "title": "It’s lists all the way down",
    "section": "",
    "text": "There’s a part 2 to this post: read it here.\n\n\nToday, I had the opportunity to help someone over at the R for Data Science Slack group (read more about this group here) and I thought that the question asked could make for an interesting blog post, so here it is!\n\n\nDisclaimer: the way I’m doing things here is totally not optimal, but I want to illustrate how to map functions over nested lists. But I show the optimal way at the end, so for the people that are familiar with purrr don’t get mad at me.\n\n\nSuppose you have to do certain data transformation tasks on a data frame, and you write a nice function that does that for you:\n\nlibrary(tidyverse)\ndata(mtcars)\n\nnice_function = function(df, param1, param2){\n  df = df %&gt;%\n    filter(cyl == param1, am == param2) %&gt;%\n    mutate(result = mpg * param1 * (2 - param2))\n\n  return(df)\n}\n\nnice_function(mtcars, 4, 0)\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\n\nThis might seem like a silly function and not a nice function, but it will illustrate the point I want to make (and the question that was asked) very well. This function is completely useless, but bear with me. Now, suppose that you want to do these operations for each value of cyl and am (of course you can do that without using nice_function()…). First, you might want to fix the value of am to 0, and then loop over the values of cyl. But as I have explained in this other blog post I prefer using the map() functions included in purrr. For example:\n\nvalues_cyl = c(4, 6, 8)\n\n(result = map(values_cyl, nice_function, df = mtcars, param2 = 0))\n## [[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\n## \n## [[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## \n## [[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n\nWhat you get here is a list for each value in values_cyl; so one list for 4, one for 6 and one for 8. Suppose now that you are feeling adventurous, and want to loop over the values of am too:\n\nvalues_am = c(0, 1)\n\nSo first, we need to map a function to each element of values_am. But which function? Well, for given value of am, our problem is the same as before; we need to map nice_function() to each value of cyl. So, that’s what we’re going to do:\n\n(result = map(values_am, ~map(values_cyl, nice_function, df = mtcars, param2 = .)))\n## [[1]]\n## [[1]][[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\n## \n## [[1]][[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## \n## [[1]][[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## \n## \n## [[2]]\n## [[2]][[1]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## \n## [[2]][[2]]\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2\n## \n## [[2]][[3]]\n##    mpg cyl disp  hp drat   wt qsec vs am gear carb result\n## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4\n## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0\n\nWe now have a list of size 2 (for each value of am) where each element is itself a list of size 3 (for each value of cyl) where each element is a data frame. Are you still with me? Also, notice that the second map is given as a formula (notice the ~ in front of the second map). This creates an anonymous function, where the parameter is given by the . (think of the . as being the x in f(x)). So the . is the stand-in for the values contained inside values_am.\n\n\nThe people that are familiar with the map() functions must be fuming right now; there is a way to avoid this nested hell. I will talk about it soon, but first I want to play around with this list of lists.\n\n\nIf you have a list of data frames, you can bind their rows together with reduce(list_of_dfs, rbind). You would like to this here, but because your lists of data frames are contained inside another list… you guessed it, you have to map over it!\n\n(result2 = map(result, ~reduce(., rbind)))\n## [[1]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## \n## [[2]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 2  32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 3  30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 4  33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 5  27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 6  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 7  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 8  21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 9  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 10 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 11 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 12 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 13 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nHere again, I pass reduce() as a formula to map() to create an anonymous function. Again, the . is used as the stand-in for each element contained in result; a list of data frames, where reduce(., rbind) knows what to do. Now that we have this we can use reduce() with rbind() again to get a single data frame:\n\n(result3 = reduce(result2, rbind))\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## 20 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 21 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 22 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 23 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 24 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 25 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 26 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 27 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 28 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 29 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 31 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 32 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nOf course, since reduce(list_of_dfs, rbind) is such a common operation, you could have simply used dplyr::bind_rows, which does exactly this:\n\n(result2 = map(result, bind_rows))\n## [[1]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## \n## [[2]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 2  32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 3  30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 4  33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 5  27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 6  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 7  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 8  21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 9  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 10 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 11 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 12 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 13 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nand then:\n\n(result3 = bind_rows(result2))\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## 20 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 21 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 22 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 23 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 24 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 25 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 26 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 27 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 28 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 29 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 31 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 32 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nOf course, things are even simpler: you can avoid this deeply nested monstrosity by using map_df() instead of map()! map_df() works just like map() but return a data frame (hence the _df in the name) instead of a list:\n\n(result_df = map_df(values_am, ~map_df(values_cyl, nice_function, df = mtcars, param2 = .)))\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## 20 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 21 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 22 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 23 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 24 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 25 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 26 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 27 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## 28 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 29 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2\n## 31 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4\n## 32 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0\n\nIf you look at the source code of map_df() you see that dplyr::bind_rows gets called at the end:\n\nmap_df\n## function (.x, .f, ..., .id = NULL) \n## {\n##     if (!is_installed(\"dplyr\")) {\n##         abort(\"`map_df()` requires dplyr\")\n##     }\n##     .f &lt;- as_mapper(.f, ...)\n##     res &lt;- map(.x, .f, ...)\n##     dplyr::bind_rows(res, .id = .id)\n## }\n## &lt;bytecode: 0x55dad486e6a0&gt;\n## &lt;environment: namespace:purrr&gt;\n\nSo moral of the story? There are a lot of variants of the common purrr::map() functions (as well as of dplyr verbs, such as filter_at, select_if, etc…) and learning about them can save you from a lot of pain! However, if you need to apply a function to nested lists this is still possible; you just have to think about the structure of the nested list for a bit. There is also another function that you might want to study, modify_depth() which solves related issues but I will end the blog post here. I might talk about it in a future blog post.\n\n\nAlso, if you want to learn more about R and the tidyverse, do read the link I posted in the introduction of the post and join the R4ds slack group! There are a lot of very nice people there that want to help you get better with your R-fu. Also, this is where I got the inspiration to write this blog post and I am thankful to the people there for the discussions; I feel comfortable with R, but I still learn new tips and tricks every day!\n\n\nIf you enjoy these blog posts, you can follow me on twitter. And happy new yeaR!"
  },
  {
    "objectID": "posts/2018-07-08-rob_stderr.html",
    "href": "posts/2018-07-08-rob_stderr.html",
    "title": "Dealing with heteroskedasticity; regression with robust standard errors using R",
    "section": "",
    "text": "First of all, is it heteroskedasticity or heteroscedasticity? According to McCulloch (1985), heteroskedasticity is the proper spelling, because when transliterating Greek words, scientists use the Latin letter k in place of the Greek letter κ (kappa). κ sometimes is transliterated as the Latin letter c, but only when these words entered the English language through French, such as scepter.\n\n\nNow that this is out of the way, we can get to the meat of this blogpost (foreshadowing pun). A random variable is said to be heteroskedastic, if its variance is not constant. For example, the variability of expenditures may increase with income. Richer families may spend a similar amount on groceries as poorer people, but some rich families will sometimes buy expensive items such as lobster. The variability of expenditures for rich families is thus quite large. However, the expenditures on food of poorer families, who cannot afford lobster, will not vary much. Heteroskedasticity can also appear when data is clustered; for example, variability of expenditures on food may vary from city to city, but is quite constant within a city.\n\n\nTo illustrate this, let’s first load all the packages needed for this blog post:\n\nlibrary(robustbase)\nlibrary(tidyverse)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(modelr)\nlibrary(broom)\n\nFirst, let’s load and prepare the data:\n\ndata(\"education\")\n\neducation &lt;- education %&gt;% \n    rename(residents = X1,\n           per_capita_income = X2,\n           young_residents = X3,\n           per_capita_exp = Y,\n           state = State) %&gt;% \n    mutate(region = case_when(\n        Region == 1 ~ \"northeast\",\n        Region == 2 ~ \"northcenter\",\n        Region == 3 ~ \"south\",\n        Region == 4 ~ \"west\"\n    )) %&gt;% \n    select(-Region)\n\nI will be using the education data set from the {robustbase} package. I renamed some columns and changed the values of the Region column. Now, let’s do a scatterplot of per capita expenditures on per capita income:\n\nggplot(education, aes(per_capita_income, per_capita_exp)) + \n    geom_point() +\n    theme_dark()\n\n\n\n\nIt would seem that, as income increases, variability of expenditures increases too. Let’s look at the same plot by region:\n\nggplot(education, aes(per_capita_income, per_capita_exp)) + \n    geom_point() + \n    facet_wrap(~region) + \n    theme_dark()\n\n\n\n\nI don’t think this shows much; it would seem that observations might be clustered, but there are not enough observations to draw any conclusion from this plot (in any case, drawing conclusions from only plots is dangerous).\n\n\nLet’s first run a good ol’ linear regression:\n\nlmfit &lt;- lm(per_capita_exp ~ region + residents + young_residents + per_capita_income, data = education)\n\nsummary(lmfit)\n## \n## Call:\n## lm(formula = per_capita_exp ~ region + residents + young_residents + \n##     per_capita_income, data = education)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -77.963 -25.499  -2.214  17.618  89.106 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)       -467.40283  142.57669  -3.278 0.002073 ** \n## regionnortheast     15.72741   18.16260   0.866 0.391338    \n## regionsouth          7.08742   17.29950   0.410 0.684068    \n## regionwest          34.32416   17.49460   1.962 0.056258 .  \n## residents           -0.03456    0.05319  -0.650 0.519325    \n## young_residents      1.30146    0.35717   3.644 0.000719 ***\n## per_capita_income    0.07204    0.01305   5.520 1.82e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 39.88 on 43 degrees of freedom\n## Multiple R-squared:  0.6292, Adjusted R-squared:  0.5774 \n## F-statistic: 12.16 on 6 and 43 DF,  p-value: 6.025e-08\n\nLet’s test for heteroskedasticity using the Breusch-Pagan test that you can find in the {lmtest} package:\n\nbptest(lmfit)\n## \n##  studentized Breusch-Pagan test\n## \n## data:  lmfit\n## BP = 17.921, df = 6, p-value = 0.006432\n\nThis test shows that we can reject the null that the variance of the residuals is constant, thus heteroskedacity is present. To get the correct standard errors, we can use the vcovHC() function from the {sandwich} package (hence the choice for the header picture of this post):\n\nlmfit %&gt;% \n    vcovHC() %&gt;% \n    diag() %&gt;% \n    sqrt()\n##       (Intercept)   regionnortheast       regionsouth        regionwest \n##      311.31088691       25.30778221       23.56106307       24.12258706 \n##         residents   young_residents per_capita_income \n##        0.09184368        0.68829667        0.02999882\n\nBy default vcovHC() estimates a heteroskedasticity consistent (HC) variance covariance matrix for the parameters. There are several ways to estimate such a HC matrix, and by default vcovHC() estimates the “HC3” one. You can refer to Zeileis (2004) for more details.\n\n\nWe see that the standard errors are much larger than before! The intercept and regionwest variables are not statistically significant anymore.\n\n\nYou can achieve the same in one single step:\n\ncoeftest(lmfit, vcov = vcovHC(lmfit))\n## \n## t test of coefficients:\n## \n##                      Estimate  Std. Error t value Pr(&gt;|t|)  \n## (Intercept)       -467.402827  311.310887 -1.5014  0.14056  \n## regionnortheast     15.727405   25.307782  0.6214  0.53759  \n## regionsouth          7.087424   23.561063  0.3008  0.76501  \n## regionwest          34.324157   24.122587  1.4229  0.16198  \n## residents           -0.034558    0.091844 -0.3763  0.70857  \n## young_residents      1.301458    0.688297  1.8908  0.06540 .\n## per_capita_income    0.072036    0.029999  2.4013  0.02073 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIt’s is also easy to change the estimation method for the variance-covariance matrix:\n\ncoeftest(lmfit, vcov = vcovHC(lmfit, type = \"HC0\"))\n## \n## t test of coefficients:\n## \n##                      Estimate  Std. Error t value  Pr(&gt;|t|)    \n## (Intercept)       -467.402827  172.577569 -2.7084  0.009666 ** \n## regionnortheast     15.727405   20.488148  0.7676  0.446899    \n## regionsouth          7.087424   17.755889  0.3992  0.691752    \n## regionwest          34.324157   19.308578  1.7777  0.082532 .  \n## residents           -0.034558    0.054145 -0.6382  0.526703    \n## young_residents      1.301458    0.387743  3.3565  0.001659 ** \n## per_capita_income    0.072036    0.016638  4.3296 8.773e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAs I wrote above, by default, the type argument is equal to “HC3”.\n\n\nAnother way of dealing with heteroskedasticity is to use the lmrob() function from the {robustbase} package. This package is quite interesting, and offers quite a lot of functions for robust linear, and nonlinear, regression models. Running a robust linear regression is just the same as with lm():\n\nlmrobfit &lt;- lmrob(per_capita_exp ~ region + residents + young_residents + per_capita_income, \n                  data = education)\n\nsummary(lmrobfit)\n## \n## Call:\n## lmrob(formula = per_capita_exp ~ region + residents + young_residents + per_capita_income, \n##     data = education)\n##  \\--&gt; method = \"MM\"\n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -57.074 -14.803  -0.853  24.154 174.279 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)       -156.37169  132.73828  -1.178  0.24526   \n## regionnortheast     20.64576   26.45378   0.780  0.43940   \n## regionsouth         10.79695   29.42746   0.367  0.71549   \n## regionwest          45.22589   33.07950   1.367  0.17867   \n## residents            0.03406    0.04412   0.772  0.44435   \n## young_residents      0.57896    0.25512   2.269  0.02832 * \n## per_capita_income    0.04328    0.01442   3.000  0.00447 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Robust residual standard error: 26.4 \n## Multiple R-squared:  0.6235, Adjusted R-squared:  0.571 \n## Convergence in 24 IRWLS iterations\n## \n## Robustness weights: \n##  observation 50 is an outlier with |weight| = 0 ( &lt; 0.002); \n##  7 weights are ~= 1. The remaining 42 ones are summarized as\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n## 0.05827 0.85200 0.93870 0.85250 0.98700 0.99790 \n## Algorithmic parameters: \n##        tuning.chi                bb        tuning.psi        refine.tol \n##         1.548e+00         5.000e-01         4.685e+00         1.000e-07 \n##           rel.tol         scale.tol         solve.tol       eps.outlier \n##         1.000e-07         1.000e-10         1.000e-07         2.000e-03 \n##             eps.x warn.limit.reject warn.limit.meanrw \n##         1.071e-08         5.000e-01         5.000e-01 \n##      nResample         max.it       best.r.s       k.fast.s          k.max \n##            500             50              2              1            200 \n##    maxit.scale      trace.lev            mts     compute.rd fast.s.large.n \n##            200              0           1000              0           2000 \n##                   psi           subsampling                   cov \n##            \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \n## compute.outlier.stats \n##                  \"SM\" \n## seed : int(0)\n\nThis however, gives you different estimates than when fitting a linear regression model. The estimates should be the same, only the standard errors should be different. This is because the estimation method is different, and is also robust to outliers (at least that’s my understanding, I haven’t read the theoretical papers behind the package yet).\n\n\nFinally, it is also possible to bootstrap the standard errors. For this I will use the bootstrap() function from the {modelr} package:\n\nresamples &lt;- 100\n\nboot_education &lt;- education %&gt;% \n modelr::bootstrap(resamples)\n\nLet’s take a look at the boot_education object:\n\nboot_education\n## # A tibble: 100 x 2\n##    strap      .id  \n##    &lt;list&gt;     &lt;chr&gt;\n##  1 &lt;resample&gt; 001  \n##  2 &lt;resample&gt; 002  \n##  3 &lt;resample&gt; 003  \n##  4 &lt;resample&gt; 004  \n##  5 &lt;resample&gt; 005  \n##  6 &lt;resample&gt; 006  \n##  7 &lt;resample&gt; 007  \n##  8 &lt;resample&gt; 008  \n##  9 &lt;resample&gt; 009  \n## 10 &lt;resample&gt; 010  \n## # … with 90 more rows\n\nThe column strap contains resamples of the original data. I will run my linear regression from before on each of the resamples:\n\n(\n    boot_lin_reg &lt;- boot_education %&gt;% \n        mutate(regressions = \n                   map(strap, \n                       ~lm(per_capita_exp ~ region + residents + \n                               young_residents + per_capita_income, \n                           data = .))) \n)\n## # A tibble: 100 x 3\n##    strap      .id   regressions\n##    &lt;list&gt;     &lt;chr&gt; &lt;list&gt;     \n##  1 &lt;resample&gt; 001   &lt;lm&gt;       \n##  2 &lt;resample&gt; 002   &lt;lm&gt;       \n##  3 &lt;resample&gt; 003   &lt;lm&gt;       \n##  4 &lt;resample&gt; 004   &lt;lm&gt;       \n##  5 &lt;resample&gt; 005   &lt;lm&gt;       \n##  6 &lt;resample&gt; 006   &lt;lm&gt;       \n##  7 &lt;resample&gt; 007   &lt;lm&gt;       \n##  8 &lt;resample&gt; 008   &lt;lm&gt;       \n##  9 &lt;resample&gt; 009   &lt;lm&gt;       \n## 10 &lt;resample&gt; 010   &lt;lm&gt;       \n## # … with 90 more rows\n\nI have added a new column called regressions which contains the linear regressions on each bootstrapped sample. Now, I will create a list of tidied regression results:\n\n(\n    tidied &lt;- boot_lin_reg %&gt;% \n        mutate(tidy_lm = \n                   map(regressions, broom::tidy))\n)\n## # A tibble: 100 x 4\n##    strap      .id   regressions tidy_lm         \n##    &lt;list&gt;     &lt;chr&gt; &lt;list&gt;      &lt;list&gt;          \n##  1 &lt;resample&gt; 001   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  2 &lt;resample&gt; 002   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  3 &lt;resample&gt; 003   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  4 &lt;resample&gt; 004   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  5 &lt;resample&gt; 005   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  6 &lt;resample&gt; 006   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  7 &lt;resample&gt; 007   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  8 &lt;resample&gt; 008   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n##  9 &lt;resample&gt; 009   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n## 10 &lt;resample&gt; 010   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;\n## # … with 90 more rows\n\nbroom::tidy() creates a data frame of the regression results. Let’s look at one of these:\n\ntidied$tidy_lm[[1]]\n## # A tibble: 7 x 5\n##   term              estimate std.error statistic  p.value\n##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)       -571.     109.        -5.22  4.92e- 6\n## 2 regionnortheast    -48.0     17.2       -2.80  7.71e- 3\n## 3 regionsouth        -21.3     15.1       -1.41  1.66e- 1\n## 4 regionwest           1.88    13.9        0.135 8.93e- 1\n## 5 residents           -0.134    0.0608    -2.21  3.28e- 2\n## 6 young_residents      1.50     0.308      4.89  1.47e- 5\n## 7 per_capita_income    0.100    0.0125     8.06  3.85e-10\n\nThis format is easier to handle than the standard lm() output:\n\ntidied$regressions[[1]]\n## \n## Call:\n## lm(formula = per_capita_exp ~ region + residents + young_residents + \n##     per_capita_income, data = .)\n## \n## Coefficients:\n##       (Intercept)    regionnortheast        regionsouth  \n##         -571.0568           -48.0018           -21.3019  \n##        regionwest          residents    young_residents  \n##            1.8808            -0.1341             1.5042  \n## per_capita_income  \n##            0.1005\n\nNow that I have all these regression results, I can compute any statistic I need. But first, let’s transform the data even further:\n\nlist_mods &lt;- tidied %&gt;% \n    pull(tidy_lm)\n\nlist_mods is a list of the tidy_lm data frames. I now add an index and bind the rows together (by using map2_df() instead of map2()):\n\nmods_df &lt;- map2_df(list_mods, \n                   seq(1, resamples), \n                   ~mutate(.x, resample = .y))\n\nLet’s take a look at the final object:\n\nhead(mods_df, 25)\n## # A tibble: 25 x 6\n##    term              estimate std.error statistic  p.value resample\n##    &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;\n##  1 (Intercept)       -571.     109.        -5.22  4.92e- 6        1\n##  2 regionnortheast    -48.0     17.2       -2.80  7.71e- 3        1\n##  3 regionsouth        -21.3     15.1       -1.41  1.66e- 1        1\n##  4 regionwest           1.88    13.9        0.135 8.93e- 1        1\n##  5 residents           -0.134    0.0608    -2.21  3.28e- 2        1\n##  6 young_residents      1.50     0.308      4.89  1.47e- 5        1\n##  7 per_capita_income    0.100    0.0125     8.06  3.85e-10        1\n##  8 (Intercept)        -97.2    145.        -0.672 5.05e- 1        2\n##  9 regionnortheast     -1.48    10.8       -0.136 8.92e- 1        2\n## 10 regionsouth         12.5     11.4        1.09  2.82e- 1        2\n## # … with 15 more rows\n\nNow this is a very useful format, because I now can group by the term column and compute any statistics I need, in the present case the standard deviation:\n\n(\n    r.std.error &lt;- mods_df %&gt;% \n        group_by(term) %&gt;% \n        summarise(r.std.error = sd(estimate))\n)\n## # A tibble: 7 x 2\n##   term              r.std.error\n##   &lt;chr&gt;                   &lt;dbl&gt;\n## 1 (Intercept)          220.    \n## 2 per_capita_income      0.0197\n## 3 regionnortheast       24.5   \n## 4 regionsouth           21.1   \n## 5 regionwest            22.7   \n## 6 residents              0.0607\n## 7 young_residents        0.498\n\nWe can append this column to the linear regression model result:\n\nlmfit %&gt;% \n    broom::tidy() %&gt;% \n    full_join(r.std.error) %&gt;% \n    select(term, estimate, std.error, r.std.error)\n## Joining, by = \"term\"\n## # A tibble: 7 x 4\n##   term               estimate std.error r.std.error\n##   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n## 1 (Intercept)       -467.      143.        220.    \n## 2 regionnortheast     15.7      18.2        24.5   \n## 3 regionsouth          7.09     17.3        21.1   \n## 4 regionwest          34.3      17.5        22.7   \n## 5 residents           -0.0346    0.0532      0.0607\n## 6 young_residents      1.30      0.357       0.498 \n## 7 per_capita_income    0.0720    0.0131      0.0197\n\nAs you see, using the whole bootstrapping procedure is longer than simply using either one of the first two methods. However, this procedure is very flexible and can thus be adapted to a very large range of situations. Either way, in the case of heteroskedasticity, you can see that results vary a lot depending on the procedure you use, so I would advise to use them all as robustness tests and discuss the differences."
  },
  {
    "objectID": "posts/2018-01-19-mapping_functions_with_any_cols.html",
    "href": "posts/2018-01-19-mapping_functions_with_any_cols.html",
    "title": "Mapping a list of functions to a list of datasets with a list of columns as arguments",
    "section": "",
    "text": "This week I had the opportunity to teach R at my workplace, again. This course was the “advanced R” course, and unlike the one I taught at the end of last year, I had one more day (so 3 days in total) where I could show my colleagues the joys of the tidyverse and R.\n\n\nTo finish the section on programming with R, which was the very last section of the whole 3 day course I wanted to blow their minds; I had already shown them packages from the tidyverse in the previous days, such as dplyr, purrr and stringr, among others. I taught them how to use ggplot2, broom and modelr. They also liked janitor and rio very much. I noticed that it took them a bit more time and effort for them to digest purrr::map() and purrr::reduce(), but they all seemed to see how powerful these functions were. To finish on a very high note, I showed them the ultimate purrr::map() use case.\n\n\nConsider the following; imagine you have a situation where you are working on a list of datasets. These datasets might be the same, but for different years, or for different countries, or they might be completely different datasets entirely. If you used rio::import_list() to read them into R, you will have them in a nice list. Let’s consider the following list as an example:\n\nlibrary(tidyverse)\ndata(mtcars)\ndata(iris)\n\ndata_list = list(mtcars, iris)\n\nI made the choice to have completely different datasets. Now, I would like to map some functions to the columns of these datasets. If I only worked on one, for example on mtcars, I would do something like:\n\nmy_summarise_f = function(dataset, cols, funcs){\n  dataset %&gt;%\n    summarise_at(vars(!!!cols), funs(!!!funcs))\n}\n\nAnd then I would use my function like so:\n\nmtcars %&gt;%\n  my_summarise_f(quos(mpg, drat, hp), quos(mean, sd, max))\n##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max\n## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93\n##   hp_max\n## 1    335\n\nmy_summarise_f() takes a dataset, a list of columns and a list of functions as arguments and uses tidy evaluation to apply mean(), sd(), and max() to the columns mpg, drat and hp of mtcars. That’s pretty useful, but not useful enough! Now I want to apply this to the list of datasets I defined above. For this, let’s define the list of columns I want to work on:\n\ncols_mtcars = quos(mpg, drat, hp)\ncols_iris = quos(Sepal.Length, Sepal.Width)\n\ncols_list = list(cols_mtcars, cols_iris)\n\nNow, let’s use some purrr magic to apply the functions I want to the columns I have defined in list_cols:\n\nmap2(data_list,\n     cols_list,\n     my_summarise_f, funcs = quos(mean, sd, max))\n## [[1]]\n##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max\n## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93\n##   hp_max\n## 1    335\n## \n## [[2]]\n##   Sepal.Length_mean Sepal.Width_mean Sepal.Length_sd Sepal.Width_sd\n## 1          5.843333         3.057333       0.8280661      0.4358663\n##   Sepal.Length_max Sepal.Width_max\n## 1              7.9             4.4\n\nThat’s pretty useful, but not useful enough! I want to also use different functions to different datasets!\n\n\nWell, let’s define a list of functions then:\n\nfuncs_mtcars = quos(mean, sd, max)\nfuncs_iris = quos(median, min)\n\nfuncs_list = list(funcs_mtcars, funcs_iris)\n\nBecause there is no map3(), we need to use pmap():\n\npmap(\n  list(\n    dataset = data_list,\n    cols = cols_list,\n    funcs = funcs_list\n  ),\n  my_summarise_f)\n## [[1]]\n##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max\n## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93\n##   hp_max\n## 1    335\n## \n## [[2]]\n##   Sepal.Length_median Sepal.Width_median Sepal.Length_min Sepal.Width_min\n## 1                 5.8                  3              4.3               2\n\nNow I’m satisfied! Let me tell you, this blew their minds 😄!\n\n\nTo be able to use things like that, I told them to always solve a problem for a single example, and from there, try to generalize their solution using functional programming tools found in purrr.\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Bruno Rodrigues and hold a PhD in Economics from the University of Strasbourg.\n\n\n\nI’m currently employed as a statistician for the Ministry of Research and Higher Education in Luxembourg. Before that I was senior data scientist and then manager in the data science team at PwC Luxembourg, and before that I was a research assistant at STATEC Research.\nMy hobbies are boxing, lifting weights, cycling, cooking and reading or listening to audiobooks, which is more compatible with the life of a young father. I started this blog to share my enthusiasm for statistics. My blog posts are reshared on R-bloggers and RWeekly. I also enjoy learning about the R programming language and sharing my knowledge. That’s why I made this blog and write ebooks. I also have a youtube channel, where I show some tips and tricks with R, or rant about stuff."
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "Build reproducible analytical pipelines that will output consistent, high-quality analyses using the R programming language, Git, Github and Docker. Learn about functional and literate programming to keep your code concise, easier to test, easier to share and easily understandable by others.\n\n\n\n\nRead it for free: https://raps-with-r.dev/\n\n\nBuy the DRM free Epub or PDF: https://leanpub.com/raps-with-r/\n\n\nBuy a physical copy on Amazon: https://www.amazon.com/Building-reproducible-analytical-pipelines-R/dp/B0C87H6MGF\n\n\n\n\n\n\nThis book focuses on teaching you how to use R in a modern way. Why modern? Because you will learn about the tidyverse collection of packages, which enable you to write efficient, legible code. You will also learn about functional programming, some statistical methods, writing your own packages… By the end of the book, you will have all the knowledge and tools on hand to use R for your day-to-day data science tasks!\n\n\n\n\nRead it for free: https://modern-rstats.eu/\n\n\n\nBuy the DRM free Epub or PDF (very out of date, update coming Q4-2025): https://leanpub.com/modern_tidyverse/\n\n\nBuy a physical copy (coming one day)"
  },
  {
    "objectID": "books.html#building-reproducible-analytical-pipelines-with-r",
    "href": "books.html#building-reproducible-analytical-pipelines-with-r",
    "title": "Books",
    "section": "",
    "text": "Build reproducible analytical pipelines that will output consistent, high-quality analyses using the R programming language, Git, Github and Docker. Learn about functional and literate programming to keep your code concise, easier to test, easier to share and easily understandable by others.\n\n\n\n\nRead it for free: https://raps-with-r.dev/\n\n\nBuy the DRM free Epub or PDF: https://leanpub.com/raps-with-r/\n\n\nBuy a physical copy on Amazon: https://www.amazon.com/Building-reproducible-analytical-pipelines-R/dp/B0C87H6MGF\n\n\n\n\n\n\nThis book focuses on teaching you how to use R in a modern way. Why modern? Because you will learn about the tidyverse collection of packages, which enable you to write efficient, legible code. You will also learn about functional programming, some statistical methods, writing your own packages… By the end of the book, you will have all the knowledge and tools on hand to use R for your day-to-day data science tasks!\n\n\n\n\nRead it for free: https://modern-rstats.eu/\n\n\n\nBuy the DRM free Epub or PDF (very out of date, update coming Q4-2025): https://leanpub.com/modern_tidyverse/\n\n\nBuy a physical copy (coming one day)"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks, presentations, workshops…",
    "section": "",
    "text": "BUILDING REPRODUCIBLE ANALYTICAL PIPELINES WITH R, DOCKER AND NIX: online talk for RLadies Rome, on the 29th of June 2024. Repository, Slides and video.\nRix: reproducible environments with Nix: talk I gave for the online part of the 2024 edition of useR! in Salzburg. Video\nRIX: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk and tutorial I gave for the 2024 edition of the Rencontres R in Vannes, Brittany, France. Slides and tutorial. No video available.\nRix: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk I gave on the 7th of June, 2024 for the INED (Institute Nationale d’Études Démographiques, National institute for demographic studies). Repository, Slides and Video.\nReproducible research with Nix and rix, a talk I gave at the DIPF, Leibniz Institute for Research and Information in Education, in LOS! (Leibniz Open Science!), the event series of the Leibniz Strategy Forum on Open Science. The workshop was held online, on May 16, 2024. Repository, Slides and video of the talk\n“rix: An R package for reproducible dev environments with Nix (FOSDEM 2024)”, a talk I gave at FOSDEM 2024 on the 4th of February 2024 in the Nix and NixOS dev room. Video here.\n“Building reproducible analytical pipelines”, Workshop I gave for the “R User Group Tunis” on December 9th, 2023. Video here. Slides and code here.\n“Porquê usar monads (programação funcional)? Uma ilustração com o pacote {chronicler}” (Why use monads (functional programming)? An illustration with the {chronicler} package), Talk I gave (online) at the INE (Instituto Nacional de Estatística), on the 7th of June 2023. Slides in Portuguese.\n“La reproductibilité avec R, ou pourquoi celle-ci est située sur un continuum” (Reproducibility with R, or why it lies on a continuum). Talk I gave at the Rencontres R 2023 in Avignon, France, on the 26th of June 2023. Slides in French, Video with English subtitles.\n“Building reproducible analytical pipelines”, Workshop I gave for “Workshops for Ukraine” on June 29th, 2023. You can pull the Docker image with the slides and code here.\n“Building reproducible analytical pipelines”, Talk I gave for ReproducibiliTea UCL on the 19th of July, 2023. Slides and Video. You can pull the Docker image with the slides and code here."
  },
  {
    "objectID": "talks.html#talks",
    "href": "talks.html#talks",
    "title": "Talks, presentations, workshops…",
    "section": "",
    "text": "BUILDING REPRODUCIBLE ANALYTICAL PIPELINES WITH R, DOCKER AND NIX: online talk for RLadies Rome, on the 29th of June 2024. Repository, Slides and video.\nRix: reproducible environments with Nix: talk I gave for the online part of the 2024 edition of useR! in Salzburg. Video\nRIX: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk and tutorial I gave for the 2024 edition of the Rencontres R in Vannes, Brittany, France. Slides and tutorial. No video available.\nRix: ENVIRONNEMENTS DE DÉVELOPPEMENT REPRODUCTIBLES POUR DÉVELOPPEURS R, in French, talk I gave on the 7th of June, 2024 for the INED (Institute Nationale d’Études Démographiques, National institute for demographic studies). Repository, Slides and Video.\nReproducible research with Nix and rix, a talk I gave at the DIPF, Leibniz Institute for Research and Information in Education, in LOS! (Leibniz Open Science!), the event series of the Leibniz Strategy Forum on Open Science. The workshop was held online, on May 16, 2024. Repository, Slides and video of the talk\n“rix: An R package for reproducible dev environments with Nix (FOSDEM 2024)”, a talk I gave at FOSDEM 2024 on the 4th of February 2024 in the Nix and NixOS dev room. Video here.\n“Building reproducible analytical pipelines”, Workshop I gave for the “R User Group Tunis” on December 9th, 2023. Video here. Slides and code here.\n“Porquê usar monads (programação funcional)? Uma ilustração com o pacote {chronicler}” (Why use monads (functional programming)? An illustration with the {chronicler} package), Talk I gave (online) at the INE (Instituto Nacional de Estatística), on the 7th of June 2023. Slides in Portuguese.\n“La reproductibilité avec R, ou pourquoi celle-ci est située sur un continuum” (Reproducibility with R, or why it lies on a continuum). Talk I gave at the Rencontres R 2023 in Avignon, France, on the 26th of June 2023. Slides in French, Video with English subtitles.\n“Building reproducible analytical pipelines”, Workshop I gave for “Workshops for Ukraine” on June 29th, 2023. You can pull the Docker image with the slides and code here.\n“Building reproducible analytical pipelines”, Talk I gave for ReproducibiliTea UCL on the 19th of July, 2023. Slides and Video. You can pull the Docker image with the slides and code here."
  },
  {
    "objectID": "talks.html#interviews-podcasts",
    "href": "talks.html#interviews-podcasts",
    "title": "Talks, presentations, workshops…",
    "section": "Interviews, podcasts…",
    "text": "Interviews, podcasts…\n\nFrench Data Workers Podcast #1 Large Scale Testing : Contenir la covid-19 avec des dépistages ciblés\nFrench Bruno Rodrigues défend une approche basée sur la reproductibilité de la data science au Luxembourg\nEnglish Leanpub Frontmatter Podcast #263"
  },
  {
    "objectID": "posts/2018-01-05-lists_all_the_way2.html",
    "href": "posts/2018-01-05-lists_all_the_way2.html",
    "title": "It’s lists all the way down, part 2: We need to go deeper",
    "section": "",
    "text": "Shortly after my previous blog post, I saw this tweet on my timeline:\n\n\n\nThe purrr resolution for 2018 - learn at least one purrr function per week - is officially launched with encouragement and inspiration from @statwonk and @hadleywickham. We start with modify_depth: https://t.co/dCMnSHP7Pl. Please join to learn and share. #rstats\n\n— Isabella R. Ghement (@IsabellaGhement) January 3, 2018\n\n\n\nThis is a great initiative, and a big coincidence, as I just had blogged about nested lists and how to map over them. I also said this in my previous blog post:\n\n\n\nThere is also another function that you might want to study, modify_depth() which solves related issues but I will end the blog post here. I might talk about it in a future blog post.\n\n\n\nAnd so after I got this reply from @IsabellaGhement:\n\n\n\nBruno, I would love it if you would chime in with an explicit contrast between nested map calls (which I personally find a bit clunky) and alternatives. In other words, present solutions side-by-side and highlight pros and cons. That would be very useful! 🤗\n\n— Isabella R. Ghement (@IsabellaGhement) January 4, 2018\n\n\n\nWhat else was I supposed to do than blog about purrr::modify_depth()?\n\n\nBear in mind that I was not really familiar with this function before writing my last blog post; and even then, I decided to keep it for another blog post, which is this one. Which came much faster than what I had originally planned. So I might have missed some functionality; if that’s the case don’t hesitate to tweet me an example or send me an email! (bruno at brodrigues dot co)\n\n\nSo what is this blog post about? It’s about lists, nested lists, and some things that you can do with them. Let’s use the same example as in my last post:\n\nlibrary(tidyverse)\ndata(mtcars)\n\nnice_function = function(df, param1, param2){\n  df = df %&gt;%\n    filter(cyl == param1, am == param2) %&gt;%\n    mutate(result = mpg * param1 * (2 - param2))\n\n  return(df)\n}\n\nnice_function(mtcars, 4, 0)\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\nvalues_cyl = c(4, 6, 8)\n\nvalues_am = c(0, 1)\n\nNow that we’re here, we would like to apply nice_function() to each element of values_cyl and values_am. In essence, loop over these values. But because loops are not really easy to manipulate, (as explained, in part, here) I use the map* family of functions included in purrr (When I teach R, I only show loops in the advanced topics chapter of my notes). So let’s “loop” over values_cyl and values_am with map() (and not map_df(); there is a reason for this, bear with me):\n\n(result = map(values_am, ~map(values_cyl, nice_function, df = mtcars, param2 = .)))\n## [[1]]\n## [[1]][[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0\n## \n## [[1]][[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6\n## \n## [[1]][[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2\n## \n## \n## [[2]]\n## [[2]][[1]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result\n## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2\n## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6\n## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6\n## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6\n## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2\n## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0\n## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6\n## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6\n## \n## [[2]][[2]]\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0\n## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2\n## \n## [[2]][[3]]\n##    mpg cyl disp  hp drat   wt qsec vs am gear carb result\n## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4\n## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0\n\nUntil now, nothing new compared to my previous post (so if you have a hard time to follow what I’m doing here, go read it here).\n\n\nAs far as I know, there is no way, in this example, to avoid this nested map call. However, suppose now that you want to apply a function to each single data frame contained in the list result. Of course, here, you could simply use bind_rows() to have a single data frame and then apply your function to it. But suppose that you want to keep this list structure; at the end, I will give an example of why you might want that, using another purrr function, walk() and Thomas’ J. Leeper brilliant rio package.\n\n\nSo suppose you want to use this function here:\n\ndouble_col = function(dataset, col){\n  col = enquo(col)\n  col_name = paste0(\"double_\", quo_name(col))\n  dataset %&gt;%\n    mutate(!!col_name := 2*(!!col))\n}\n\nto double the values of a column of a dataset. It uses tidyeval’s enquo(), quo_name() and !!() functions to make it work with tidyverse functions such as mutate(). You can use it like this:\n\ndouble_col(mtcars, hp)\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb double_hp\n## 1  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4       220\n## 2  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4       220\n## 3  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1       186\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1       220\n## 5  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2       350\n## 6  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1       210\n## 7  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4       490\n## 8  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2       124\n## 9  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2       190\n## 10 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4       246\n## 11 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4       246\n## 12 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3       360\n## 13 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3       360\n## 14 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3       360\n## 15 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4       410\n## 16 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4       430\n## 17 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4       460\n## 18 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1       132\n## 19 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2       104\n## 20 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1       130\n## 21 21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1       194\n## 22 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2       300\n## 23 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2       300\n## 24 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4       490\n## 25 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2       350\n## 26 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1       132\n## 27 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2       182\n## 28 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2       226\n## 29 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4       528\n## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6       350\n## 31 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8       670\n## 32 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2       218\n\nNice, but you want to use this function on all of the data frames contained in your result list. You can use a nested map() as before:\n\nmap(result, ~map(., .f = double_col, col = disp))\n## [[1]]\n## [[1]][[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2\n## \n## [[1]][[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8       516.0\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2       450.0\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4       335.2\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6       335.2\n## \n## [[1]][[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2       720.0\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8       720.0\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4       551.6\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8       551.6\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2       551.6\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4       944.0\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4       920.0\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2       880.0\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0       636.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2       608.0\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8       700.0\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2       800.0\n## \n## \n## [[2]]\n## [[2]][[1]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2       216.0\n## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6       157.4\n## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6       151.4\n## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6       142.2\n## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2       158.0\n## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0       240.6\n## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6       190.2\n## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6       242.0\n## \n## [[2]][[2]]\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0         320\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0         320\n## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2         290\n## \n## [[2]][[3]]\n##    mpg cyl disp  hp drat   wt qsec vs am gear carb result double_disp\n## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4         702\n## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0         602\n\nbut there’s an easier solution, which is using modify_depth():\n\n(result = modify_depth(result, .depth = 2, double_col, col = disp))\n## [[1]]\n## [[1]][[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2\n## \n## [[1]][[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8       516.0\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2       450.0\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4       335.2\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6       335.2\n## \n## [[1]][[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2       720.0\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8       720.0\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4       551.6\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8       551.6\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2       551.6\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4       944.0\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4       920.0\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2       880.0\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0       636.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2       608.0\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8       700.0\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2       800.0\n## \n## \n## [[2]]\n## [[2]][[1]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2       216.0\n## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6       157.4\n## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6       151.4\n## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6       142.2\n## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2       158.0\n## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0       240.6\n## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6       190.2\n## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6       242.0\n## \n## [[2]][[2]]\n##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0         320\n## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0         320\n## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2         290\n## \n## [[2]][[3]]\n##    mpg cyl disp  hp drat   wt qsec vs am gear carb result double_disp\n## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4         702\n## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0         602\n\nSo how does it work? modify_depth() needs a list and a .depth argument, which corresponds to where you you want to apply your function. The following lines of code might help you understand:\n\n# Depth of 1:\n\nresult[[1]]\n## [[1]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2\n## \n## [[2]]\n##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8       516.0\n## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2       450.0\n## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4       335.2\n## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6       335.2\n## \n## [[3]]\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp\n## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2       720.0\n## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8       720.0\n## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4       551.6\n## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8       551.6\n## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2       551.6\n## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4       944.0\n## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4       920.0\n## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2       880.0\n## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0       636.0\n## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2       608.0\n## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8       700.0\n## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2       800.0\n\nIn this example, a depth of 1 corresponds to a list of three data frame. Can you use your function double_col() on a list of three data frames? No, because the domain of double_col() is the set of data frames, not the set of lists of data frames. So you need to go deeper:\n\n# Depth of 2:\n\nresult[[1]][[1]] # or try result[[1]][[2]] or result[[1]][[3]]\n##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp\n## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4\n## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6\n## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2\n\nAt the depth of 2, you’re dealing with data frames! So you can use your function double_col(). With a depth of 2, one might not see the added value of modify_depth() over nested map calls, but if you have to go even deeper, nested map calls are very confusing and verbose.\n\n\nNow for the last part; why doing all this, and not simply bind all the rows, apply double_col() and call it a day? Well, suppose that there is a reason you have these data frames inside lists; for example, the first element, i.e., result[[1]] might be data for, say, Portugal, for 3 different years. result[[2]] however, is data for France, for the same years. Suppose also that you have to give this data, after having worked on it, to a colleague (or to another institution) in the Excel format; one Excel workbook per country, one sheet per year. This example might seem contrived, but I have been confronted to this exact situation very often. Well, if you bind all the rows together, how are you going to save the data in the workbooks like you are required to?\n\n\nWell, thanks to rio, one line of code is enough:\n\nlibrary(rio)\n\nwalk2(result, list(\"portugal.xlsx\", \"france.xlsx\"), export)\n\nI know what you’re thinking; Bruno, that’s two lines of code!. Yes, but I had to load rio. Also, walk() (and walk2()) are basically the same as map(), but you use walk() over map() when you are only interested in the side effect of the function you are applying over your list; here, export() which is rio’s function to write data to disk. The side effect of this function is… writing data to disk! You could have used map2() just the same, but I wanted to show you walk2() (however, you cannot replace map() by walk() in most cases; try it and see what happens).\n\n\nHere’s what it looks like:\n\n\n\n\n\nI have two Excel workbooks, (one per list), where each sheet is a data frame!\n\n\nIf you enjoy these blog posts, you can follow me on twitter."
  },
  {
    "objectID": "posts/2016-03-31-unit-testing-with-r.html",
    "href": "posts/2016-03-31-unit-testing-with-r.html",
    "title": "Unit testing with R",
    "section": "",
    "text": "I've been introduced to unit testing while working with colleagues on quite a big project for which we use Python.\n\n\nAt first I was a bit skeptical about the need of writing unit tests, but now I must admit that I am seduced by the idea and by the huge time savings it allows. Naturally, I was wondering if the same could be achieved with R, and was quite happy to find out that it also possible to write unit tests in R using a package called testthat.\n\n\nUnit tests (Not to be confused with unit root tests for time series) are small functions that test your code and help you make sure everything is alright. I'm going to show how the testthat packages works with a very trivial example, that might not do justice to the idea of unit testing. But you'll hopefully see why writing unit tests is not a waste of your time, especially if your project gets very complex (if you're writing a package for example).\n\n\nFirst, you'll need to download and install testthat. Some dependencies will also be installed.\n\n\nNow, you'll need a function to test. Let's suppose you've written a function that returns the nth Fibonacci number:\n\nFibonacci &lt;- function(n){\n    a &lt;- 0\n    b &lt;- 1\n    for (i in 1:n){\n        temp &lt;- b\n        b &lt;- a\n        a &lt;- a + temp\n    }\n    return(a)\n}\n\n\nYou then save this function in a file, let's call it fibo.R. What you'll probably do once you've written this function, is to try it out:\n\nFibonacci(5)\n\n## [1] 5\n\n\nYou'll see that the function returns the right result and continue programming. The idea behind unit testing is write a bunch of functions that you can run after you make changes to your code, just to check that everything is still running as it should.\n\n\nLet's create a script called test_fibo.R and write the following code in it:\n\ntest_that(\"Test Fibo(15)\",{\n  phi &lt;- (1 + sqrt(5))/2\n  psi &lt;- (1 - sqrt(5))/2\n  expect_equal(Fibonacci(15), (phi**15 - psi**15)/sqrt(5))\n})\n\n\nThe code above uses Binet's formula, a closed form formula that gives the nth Fibonacci number and compares it our implementation of the algorithm. If you didn't know about Binet's formula, you could simply compute some numbers by hand and compare them to what your function returns, for example. The function expect_equal is a function from the package testthat and does exactly what it tells. We expect the result of our implementation to be equal to the result of Binet's Formula. The file test_fibo.R can contain as many tests as you need. Also, the file that contains the tests must start with the string test, so that testthat knows with files it has to run.\n\n\nNow, we're almost done, create yet another script, let's call it run_tests.R and write the following code in it:\n\nlibrary(testthat) \n\nsource(\"path/to/fibo.R\")\n\ntest_results &lt;- test_dir(\"path/to/tests\", reporter=\"summary\")\n\n\nAfter running these lines, and if everything goes well, you should see a message like this:\n\n&gt; library(testthat)\n&gt; source(\"path/to/fibo.R\")\n&gt; test_results &lt;- test_dir(\"path/to/tests\", reporter=\"summary\")\n\n.\nYour tests are dandy! \n\n\nNotice the small . over the message? This means that one test was run successfully. You'll get one dot per successful test. If you take a look at test_results you'll see this:\n\n&gt; test_results\n         file context          test nb failed skipped error  user system  real\n1 test_fibo.R         Test Fibo(15)  1      0   FALSE FALSE 0.004      0 0.006\n\n\nYou'll see each file and each function inside the files that were tested, and also whether the test was skipped, failed etc. This may seem overkill for such a simple function, but imagine that you write dozens of functions that get more and more complex over time. You might have to change a lot of lines because as time goes by you add new functionality, but don't want to break what was working. Running your unit tests each time you make changes can help you pinpoint regressions in your code. Unit tests can also help you start with your code. It can happen that sometimes you don't know exactly how to start; well you could start by writing a unit test that returns the result you want to have and then try to write the code to make that unit test pass. This is called test-driven development.\n\n\nI hope that this post motivated you to write unit tests and make you a better R programmer!"
  },
  {
    "objectID": "posts/2018-02-11-census-random_forest.html",
    "href": "posts/2018-02-11-census-random_forest.html",
    "title": "Predicting job search by training a random forest on an unbalanced dataset",
    "section": "",
    "text": "Update 2022: there some literature advising against using techniques to artificially balance a dataset, for example one. Use at your own risks!\nIn this blog post, I am going to train a random forest on census data from the US to predict the probability that someone is looking for a job. To this end, I downloaded the US 1990 census data from the UCI Machine Learning Repository. Having a background in economics, I am always quite interested by such datasets. I downloaded the raw data which is around 820mb uncompressed. You can download it from this folder here.\nBefore training a random forest on it, some preprocessing is needed. First problem: the columns in the data do not have names. Actually, training a random forest on unamed variables is possible, but I like my columns to have names. The names are on a separate file, called USCensus1990raw.attributes.txt. This is how this file looks like:\nThe variable names are always written in upper case and sometimes end with some numbers. Regular expressions will help extract these column names:\nUsing readLines I load this text file into R. Then with stringr::str_extract_all, I can extract the variable names from this text file. The regular expression, 1+(\\d{1,}|[A-Z])\\s+ can seem complicated, but by breaking it up, it’ll be clear:\nThis regular expression matches only the variable names. By using ^ I only limit myself to the uppercase letters at the start of the line, which already removes a lot of unneeded lines from the text. Then, by matching numbers or letters, followed by spaces, I avoid matching strings such as VAR:. There’s probably a shorter way to write this regular expression, but since this one works, I stopped looking for another solution.\nNow that I have a vector called column_names, I can baptize the columns in my dataset:\nI also add a column called caseid to the dataset, but it’s actually not really needed. But it made me look for and find rownames_to_column(), which can be useful:\nNow I select the variables I need. I use dplyr::select() to select the columns I need (actually, I will remove some of these later for the purposes of the blog post, but will continue exploring them. Maybe write a part 2?):\nNow, I convert factor variables to factors and only relevel the race variable:\nSo the variable I want to predict is looking which has 2 levels (I removed the level 0, which stands for NA). I convert all the variables that are supposed to be factors into factors using mutate_at() and then reselect a subsample of the columns. census is now a tibble with 39 columns and 2458285 rows. I will train the forest on a subsample only, because with cross validation it would take forever on the whole dataset.\nI run the training on another script, that I will then run using the Rscript command instead of running it from Spacemacs (yes, I don’t use RStudio at home but Spacemacs + ESS). Here’s the script:\n90% of the individuals in the sample are not looking for a new job. For training purposes, I will only use 50000 observations instead of the whole sample. I’m already thinking about writing another blog post where I show how to use the whole data. But 50000 observations should be more than enough to have a pretty nice model. However, having 90% of observations belonging to a single class can cause problems with the model; the model might predict that everyone should belong to class 2 and in doing so, the model would be 90% accurate! Let’s ignore this for now, but later I am going to tackle this issue with a procedure calleds SMOTE.\nNow, using caret::trainIndex(), I partition the data into a training sample and a testing sample:\nI also save the testing data to disk, because when the training is done I’ll lose my R session (remember, I’ll run the training using Rscript):\nBefore training the model, I’ll change some options; I’ll do 5-fold cross validation that I repeat 5 times. This will further split the training set into training/testing sets which will increase my confidence in the metrics that I get from the training. This will ensure that the best model really is the best, and not a fluke resulting from the splitting of the data that I did beforehand. Then, I will test the best model on the testing data from above:\nA very nice feature from the caret package is the possibility to make the training in parallel. For this, load the doParallel package (which I did above), and then register the number of cores you want to use for training with makeCluster(). You can replace detectCores() by the number of cores you want to use:\nFinally, we can train the model:\nBecause it takes around 1 and a half hours to train, I save the model to disk using saveRDS():\nThe picture below shows all the cores from my computer running and RAM usage being around 20gb during the training process:\nAnd this the results of training the random forest on the unbalanced data:\nIf someone really is looking for a job, the model is able to predict it correctly 92% of the times and 98% of the times if that person is not looking for a job. It’s slightly better than simply saying than no one is looking for a job, which would be right 90% of the times, but not great either.\nTo train to make the model more accurate in predicting class 1, I will resample the training set, but by downsampling class 2 and upsampling class 1. This can be done with the function SMOTE() from the {DMwR} package. However, the testing set should have the same distribution as the population, so I should not apply SMOTE() to the testing set. I will resplit the data, but this time with a 95/5 % percent split; this way I have 5% of the original dataset used for testing, I can use SMOTE() on the 95% remaining training set. Because SMOTEing takes some time, I save the SMOTEd training set using readRDS() for later use:\nThe testing set has 34780 observations and below you can see the distribution of the target variable, looking:\nHere are the results:\nThe balanced accuracy is higher, but unlike what I expected (and hoped), this model is worse in predicting class 1! I will be trying one last thing; since I have a lot of data at my disposal, I will simply sample 25000 observations where the target variable looking equals 1, and then sample another 25000 observations where the target variable equals 2 (without using SMOTE()). Then I’ll simply bind the rows and train the model on that:\nAnd here are the results:\nLooks like it’s not much better than using SMOTE()!\nThere are several ways I could achieve better predictions; tuning the model is one possibility, or perhaps going with another type of model altogether. I will certainly come back to this dataset in future blog posts!\nUsing the best model, let’s take a look at which variables are the most important for predicting job search:\nIt’s also possible to have a plot of the above:\nTo make sense of this, we have to read the description of the features here.\nrlabor3 is the most important variable, and means that the individual is unemployed. rlabor6 means not in the labour force. Then the age of the individual as well as the individual’s income play a role. tmpabsnt is a variable that equals 1 if the individual is temporary absent from work, due to a layoff. All these variables having an influence on the probability of looking for a job make sense, but looks like a very simple model focusing on just a couple of variables would make as good a job as the random forest.\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2018-02-11-census-random_forest.html#footnotes",
    "href": "posts/2018-02-11-census-random_forest.html#footnotes",
    "title": "Predicting job search by training a random forest on an unbalanced dataset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA-Z↩︎\nA-Z↩︎"
  },
  {
    "objectID": "posts/2016-06-21-careful-with-trycatch.html",
    "href": "posts/2016-06-21-careful-with-trycatch.html",
    "title": "Careful with tryCatch",
    "section": "",
    "text": "tryCatch is one of the functions that allows the users to handle errors in a simple way. With it, you can do things like: if(error), then(do this).\n\n\nTake the following example:\n\nsqrt(\"a\")\nError in sqrt(\"a\") : non-numeric argument to mathematical function\n\nNow maybe you’d want something to happen when such an error happens. You can achieve that with tryCatch:\n\ntryCatch(sqrt(\"a\"), error=function(e) print(\"You can't take the square root of a character, silly!\"))\n## [1] \"You can't take the square root of a character, silly!\"\n\nWhy am I interested in tryCatch?\n\n\nI am currently working with dates, specifically birthdays of people in my data sets. For a given mother, the birthday of her child is given in three distinct columns: a column for the child’s birth year, birth month and birth day respectively. I’ve wanted to put everything in a single column and convert the birthday to unix time (I have a very good reason to do that, but I won’t bore you with the details).\n\n\nLet’s create some data:\n\nmother &lt;- as.data.frame(list(month=12, day=1, year=1988))\n\nIn my data, there’s a lot more columns of course, such as the mother’s wage, education level, etc, but for illustration purposes, this is all that’s needed.\n\n\nNow, to create this birthday column:\n\nmother$birth1 &lt;- as.POSIXct(paste0(as.character(mother$year), \n                                   \"-\", as.character(mother$month), \n                                   \"-\", as.character(mother$day)), \n                            origin=\"1970-01-01\")\n\nand to convert it to unix time:\n\nmother$birth1 &lt;- as.numeric(as.POSIXct(paste0(as.character(mother$year), \n                                              \"-\", as.character(mother$month), \n                                              \"-\", as.character(mother$day)),\n                                       origin=\"1970-01-01\"))\n\nprint(mother)\n##   month day year    birth1\n## 1    12   1 1988 596934000\n\nNow let’s see what happens in this other example here:\n\nmother2 &lt;- as.data.frame(list(month=2, day=30, year=1988))\n\nmother2$birth1 &lt;- as.POSIXct(paste0(as.character(mother2$year), \n                                    \"-\", as.character(mother2$month), \n                                    \"-\", as.character(mother2$day)), \n                             origin=\"1970-01-01\")\n\nThis is what happens:\n\nError in as.POSIXlt.character(x, tz, ...) : \n  character string is not in a standard unambiguous format\n\nThis error is to be expected; there is no 30th of February! It turns out that in some rare cases, weird dates like this exist in my data. Probably some encoding errors. Not a problem I thought, I could use tryCatch and return NA in the case of an error.\n\nmother2 &lt;- as.data.frame(list(month=2, day=30, year=1988))\n\nmother2$birth1 &lt;- tryCatch(as.POSIXct(paste0(as.character(mother2$year), \n                                    \"-\", as.character(mother2$month), \n                                    \"-\", as.character(mother2$day)), \n                             origin=\"1970-01-01\"), error=function(e) NA)\n\nprint(mother2)\n##   month day year birth1\n## 1     2  30 1988     NA\n\nPretty great, right? Well, no. Take a look at what happens in this case:\n\nmother &lt;- as.data.frame(list(month=c(12, 2), day=c(1, 30), year=c(1988, 1987)))\nprint(mother)\n##   month day year\n## 1    12   1 1988\n## 2     2  30 1987\n\nWe’d expect to have a correct date for the first mother and an NA for the second. However, this is what happens\n\nmother$birth1 &lt;- tryCatch(as.POSIXct(paste0(as.character(mother$year), \n                                    \"-\", as.character(mother$month), \n                                    \"-\", as.character(mother$day)), \n                             origin=\"1970-01-01\"), error=function(e) NA)\n\nprint(mother)\n##   month day year birth1\n## 1    12   1 1988     NA\n## 2     2  30 1987     NA\n\nAs you can see, we now have an NA for both mothers! That’s actually to be expected. Indeed, this little example illustrates it well:\n\nsqrt(c(4, 9, \"haha\"))\nError in sqrt(c(4, 9, \"haha\")) : \n  non-numeric argument to mathematical function\n\nBut you’d like to have this:\n\n[1]  2  3 NA\n\nSo you could make the same mistake as myself and use tryCatch:\n\ntryCatch(sqrt(c(4, 9, \"haha\")), error=function(e) NA)\n## [1] NA\n\nBut you only get NA in return. That’s actually completely normal, but it took me off-guard and I spent quite some time to figure out what was happening. Especially because I had written unit tests to test my function create_birthdays() that was doing the above computations and all tests were passing! The problem was that in my tests, I only had a single individual, so for a wrong date, having NA for this individual was expected behaviour. But in a panel, only some individuals have a weird date like the 30th of February, but because of those, the whole column was filled with NA’s! What I’m doing now is trying to either remove these weird birthdays (there are mothers whose children were born on the 99-99-9999. Documentation is lacking, but this probably means missing value), or tyring to figure out how to only get NA’s for the “weird” dates. I guess that the answer lies with dplyr’s group_by() and mutate() to compute this birthdays for each individual separately."
  },
  {
    "objectID": "posts/2014-04-23-r-s4-rootfinding.html",
    "href": "posts/2014-04-23-r-s4-rootfinding.html",
    "title": "Object Oriented Programming with R: An example with a Cournot duopoly",
    "section": "",
    "text": "I started reading Applied Computational Economics & Finance by Mario J. Miranda and Paul L. Fackler. It is a very interesting book that I recommend to every one of my colleagues. The only issue I have with this book, is that the programming language they use is Matlab, which is proprietary. While there is a free as in freedom implementation of the Matlab language, namely Octave, I still prefer using R. In this post, I will illustrate one example the authors present in the book with R, using the package rootSolve. rootSolve implements Newtonian algorithms to find roots of functions; to specify the functions for which I want the roots, I use R's Object-Oriented Programming (OOP) capabilities to build a model that returns two functions. This is optional, but I found that it was a good example to illustrate OOP, even though simpler solutions exist, one of which was proposed by reddit user TheDrownedKraken (whom I thank) and will be presented at the end of the article.\n\n\nTheoretical background\n\n\nThe example is taken from Miranda's and Fackler's book, on page 35. The authors present a Cournot duopoly model. In a Cournot duopoly model, two firms compete against each other by quantities. Both produce a certain quantity of an homogenous good, and take the quantity produce by their rival as given.\n\n\nThe inverse demand of the good is :\n\n\\[P(q) = q^{-\\dfrac{1}{\\eta}}\\]\n\nthe cost function for firm i is:\n\n\\[C_i(q_i) = P(q_1+q_2)*q_i - C_i(q_i)\\]\n\nand the profit for firm i:\n\n\\[\\pi_i(q1,q2) = P(q_1+q_2)q_i - C_i(q_i)\\]\n\nThe optimality condition for firm i is thus:\n\n\\[\\dfrac{\\partial \\pi_i}{\\partial q_i} = (q1+q2)^{-\\dfrac{1}{\\eta}} - \\dfrac{1}{\\eta} (q_1+q_2)^{\\dfrac{-1}{\\eta-1}}q_i - c_iq_i=0.\\]\n\nImplementation in R\n\n\nIf we want to find the optimal quantities (q_1) and (q_2) we need to program the optimality condition and we could also use the jacobian of the optimality condition. The jacobian is generally useful to speed up the root finding routines. This is were OOP is useful. First let's create a new class, called Model:\n\n\nsetClass(Class = \"Model\", slots = list(OptimCond = \"function\", JacobiOptimCond = \"function\"))\n\n\nThis new class has two slots, which here are functions (in general slots are properties of your class); we need the model to return the optimality condition and the jacobian of the optimality condition.\n\n\nNow we can create a function which will return these two functions for certain values of the parameters, c and  of the model:\n\n\nmy_mod &lt;- function(eta, c) {\n    e = -1/eta\n\n    OptimCond &lt;- function(q) {\n        return(sum(q)^e + e * sum(q)^(e - 1) * q - diag(c) %*% q)\n    }\n\n    JacobiOptimCond &lt;- function(q) {\n      return(\n        (e * sum(q)^e) * array(1, c(2, 2)) +\n        (e * sum(q)^(e - 1)) * diag(1, 2) +\n        (e - 1) * e * sum(q)^(e - 2) * q * c(1, 1) - diag(c)\n      )\n    }\n\n    return(new(\"Model\", OptimCond = OptimCond, JacobiOptimCond = JacobiOptimCond))\n\n}\n\n\nThe function my_mod takes two parameters, eta and c and returns two functions, the optimality condition and the jacobian of the optimality condition. Both are now accessible via my_mod(eta=1.6,c = c(0.6,0.8))@OptimCond and my_mod(eta=1.6,c = c(0.6,0.8))@JacobiOptimCond respectively (and by specifying values for eta and c).\n\n\nNow, we can use the rootSolve package to get the optimal values (q_1) and (q_2)\n\n\nlibrary(\"rootSolve\")\n\nmultiroot(f = my_mod(eta = 1.6, c = c(0.6, 0.8))@OptimCond,\n          start = c(1, 1),\n          maxiter = 100,\n          jacfunc = my_mod(eta = 1.6, c = c(0.6, 0.8))@JacobiOptimCond)\n\n## $root\n## [1] 0.8396 0.6888\n## \n## $f.root\n##            [,1]\n## [1,] -2.220e-09\n## [2,]  9.928e-09\n## \n## $iter\n## [1] 4\n## \n## $estim.precis\n## [1] 6.074e-09\n\n\nAfter 4 iterations, we get that  and  are equal to 0.84 and 0.69 respectively, which are the same values as in the book!\n\n\nSuggestion by Reddit user, TheDrownedKraken\n\n\nI posted this blog post on the rstats subbreddit on www.reddit.com. I got a very useful comment by reddit member TheDrownedKraken which suggested the following approach, which doesn't need a new class to be build. I thank him for this. Here is his suggestion:\n\n\ngenerator &lt;- function(eta, a) {\n    e = -1/eta\n\n    OptimCond &lt;- function(q) {\n        return(sum(q)^e + e * sum(q)^(e - 1) * q - diag(a) %*% q)\n    }\n\n    JacobiOptimCond &lt;- function(q) {\n      return(\n        (e * sum(q)^e) * array(1, c(2, 2)) +\n        (e * sum(q)^(e - 1)) * diag(1, 2) +\n        (e - 1) * e * sum(q)^(e - 2) * q * c(1, 1) - diag(a)\n      )\n    }\n\n    return(list(OptimCond = OptimCond, JacobiOptimCond = JacobiOptimCond))\n\n}\n\nf.s &lt;- generator(eta = 1.6, a = c(0.6, 0.8))\n\nmultiroot(f = f.s$OptimCond, start = c(1, 1), maxiter = 100, jacfunc = f.s$JacobiOptimCond)\n\n## $root\n## [1] 0.8396 0.6888\n## \n## $f.root\n##            [,1]\n## [1,] -2.220e-09\n## [2,]  9.928e-09\n## \n## $iter\n## [1] 4\n## \n## $estim.precis\n## [1] 6.074e-09"
  },
  {
    "objectID": "posts/2014-11-11-benchmarks-r-blas-atlas-rro.html",
    "href": "posts/2014-11-11-benchmarks-r-blas-atlas-rro.html",
    "title": "R, R with Atlas, R with OpenBLAS and Revolution R Open: which is fastest?",
    "section": "",
    "text": "In this short post, I benchmark different “versions” of R. I compare the execution speeds of R, R linked against OpenBLAS, R linked against ATLAS and Revolution R Open. Revolution R Open is a new open source version of R made by Revolution Analytics. It is linked against MKL and should offer huge speed improvements over vanilla R. Also, it uses every cores of your computer by default, without any change whatsoever to your code.\n\n\nTL;DR: Revolution R Open is the fastest of all the benchmarked versions (with R linked against OpenBLAS and ATLAS just behind), and easier to setup.\n\n\nSetup\n\n\nI benchmarked these different versions of R using R-benchmark-25.R that you can download here. This benchmark file was created by Simon Urbanek.\n\n\nI ran the benchmarks on my OpenSUSE 13.2 computer with a Pentium Dual-Core CPU E6500@2.93GHz with 4GB of Ram. It's outdated, but it's still quite fast for most of my numerical computation needs. I installed “vanilla” R from the official OpenSUSE repositories which is currently at version 3.1.2.\n\n\nThen, I downloaded OpenBLAS and ATLAS also from the official OpenSUSE repositories and made R use these libraries instead of its own implementation of BLAS. The way I did that is a bit hacky, but works: first, go to /usr/lib64/R/lib and backup libRblas.so (rename it to libRblas.soBackup for instance). Then link /usr/lib64/libopenblas.so.0 to /usr/lib64/R/lib/libRblas, and that's it, R will use OpenBLAS. For ATLAS, you can do it in the same fashion, but you'll find the library in /usr/lib64/atlas/. These paths should be the same for any GNU/Linux distribution. For other operating systems, I'm sure you can find where these libraries are with Google.\n\n\nThe last version I benchmarked was Revolution R Open. This is a new version of R released by Revolution Analytics. Revolution Analytics had their own version of R, called Revolution R, for quite some time now. They decided to release a completely free as in freedom and free as in free beer version of this product which they now renamed Revolution R Open. You can download Revolution R Open here. You can have both “vanilla” R and Revolution R Open installed on your system.\n\n\nResults\n\n\nI ran the R-benchmark-25.R 6 times for every version but will only discuss the 4 best runs.\n\n\n\n\n\nR version\n\n\nFastest run\n\n\nSlowest run\n\n\nMean Run\n\n\n\n\nVanilla R\n\n\n63.65\n\n\n66.21\n\n\n64.61\n\n\n\n\nOpenBLAS R\n\n\n15.63\n\n\n18.96\n\n\n16.94\n\n\n\n\nATLAS R\n\n\n16.92\n\n\n21.57\n\n\n18.24\n\n\n\n\nRRO\n\n\n14.96\n\n\n16.08\n\n\n15.49\n\n\n\n\nAs you can read from the table above, Revolution R Open was the fastest of the four versions, but not significantly faster than BLAS or ATLAS R. However, RRO uses all the available cores by default, so if your code relies on a lot matrix algebra, RRO might be actually a lot more faster than OpenBLAS and ATLAS R. Another advantage of RRO is that it is very easy to install, and also works with Rstudio and is compatible with every R package to existence. “Vanilla” R is much slower than the other three versions, more than 3 times as slow!\n\n\nConclusion\n\n\nWith other benchmarks, you could get other results, but I don’t think that “vanilla” R could beat any of the other three versions. Whatever your choice, I recommend not using plain, “vanilla” R. The other options are much faster than standard R, and don't require much work to set up. I'd personally recommend Revolution R Open, as it is free software and compatible with CRAN packages and Rstudio."
  },
  {
    "objectID": "posts/2017-12-27-build_formulae.html",
    "href": "posts/2017-12-27-build_formulae.html",
    "title": "Building formulae",
    "section": "",
    "text": "This Stackoverflow question made me think about how to build formulae. For example, you might want to programmatically build linear model formulae and then map these models on data. For example, suppose the following (output suppressed):\n\ndata(mtcars)\n\nlm(mpg ~ hp, data = mtcars)\nlm(mpg ~I(hp^2), data = mtcars)\nlm(mpg ~I(hp^3), data = mtcars)\nlm(mpg ~I(hp^4), data = mtcars)\nlm(mpg ~I(hp^5), data = mtcars)\nlm(mpg ~I(hp^6), data = mtcars)\n\nTo avoid doing this, one can write a function that builds the formulae:\n\ncreate_form = function(power){\n  rhs = substitute(I(hp^pow), list(pow=power))\n  rlang::new_formula(quote(mpg), rhs)\n}\n\nIf you are not familiar with substitute(), try the following to understand what it does:\n\nsubstitute(y ~ x, list(x = 1))\n## y ~ 1\n\nThen using rlang::new_formula() I build a formula by providing the left hand side, which is quote(mpg) here, and the right hand side, which I built using substitute(). Now I can create a list of formulae:\n\nlibrary(tidyverse)\n\nlist_formulae = map(seq(1, 6), create_form)\n\nstr(list_formulae)\n## List of 6\n##  $ :Class 'formula'  language mpg ~ I(hp^1L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605f897ca0&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^2L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605f891418&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^3L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605da76098&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^4L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605da6a600&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^5L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605da68980&gt; \n##  $ :Class 'formula'  language mpg ~ I(hp^6L)\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55605da66d38&gt;\n\nAs you can see, power got replaced by 1, 2, 3,… and each element of the list is a nice formula. Exactly what lm() needs. So now it’s easy to map lm() to this list of formulae:\n\ndata(mtcars)\n\nmap(list_formulae, lm, data = mtcars)\n## [[1]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^1)  \n##    30.09886     -0.06823  \n## \n## \n## [[2]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^2)  \n##  24.3887252   -0.0001649  \n## \n## \n## [[3]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^3)  \n##   2.242e+01   -4.312e-07  \n## \n## \n## [[4]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^4)  \n##   2.147e+01   -1.106e-09  \n## \n## \n## [[5]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^5)  \n##   2.098e+01   -2.801e-12  \n## \n## \n## [[6]]\n## \n## Call:\n## .f(formula = .x[[i]], data = ..1)\n## \n## Coefficients:\n## (Intercept)      I(hp^6)  \n##   2.070e+01   -7.139e-15\n\nThis is still a new topic for me there might be more elegant ways to do that, using tidyeval to remove the hardcoding of the columns in create_form(). I might continue exploring this."
  },
  {
    "objectID": "posts/2017-01-07-my-free-book-has-a-cover.html",
    "href": "posts/2017-01-07-my-free-book-has-a-cover.html",
    "title": "My free book has a cover!",
    "section": "",
    "text": "I’m currently writing a book as a hobby. It’s titled Functional programming and unit testing for data munging with R and you can get it for free here. You can also read it online for free on my webpage What’s the book about?\nHere’s the teaser text:\n\nLearn the basics of functional programming, unit testing and package development for the R programming language in order to make your data tidy!\n\nThe book now has a beautiful cover thanks to @putosaure. Putosaure is a Paris based graphic designer who also reviews video games. He is also a very good friend of mine and I am very happy he made this beautiful cover for my book:\n\n\n\nIn it, we see a guy holding a shield with the Greek letter lambda, which also happens to be the letter to designate functional programming. I’ve added the title with the Komika Title font.\nConsider this cover in beta, it’ll probably evolve some more. But I couldn’t wait to use it!\nI love it. Hope you’ll love it too!"
  },
  {
    "objectID": "posts/2018-09-11-human_to_machine.html",
    "href": "posts/2018-09-11-human_to_machine.html",
    "title": "Going from a human readable Excel file to a machine-readable csv with {tidyxl}",
    "section": "",
    "text": "I won’t write a very long introduction; we all know that Excel is ubiquitous in business, and that it has a lot of very nice features, especially for business practitioners that do not know any programming. However, when people use Excel for purposes it was not designed for, it can be a hassle. Often, people use Excel as a reporting tool, which it is not; they create very elaborated and complicated spreadsheets that are human readable, but impossible to import within any other tool.\n\n\nIn this blog post (which will probably be part of a series), I show you how you can go from this:\n\n\n\n\n\nto this:\n\n\n\n\n\nYou can find the data I will use here. Click on the “Time use” folder and you can download the workbook.\n\n\nThe Excel workbook contains several sheets (in French and English) of the amount of time Luxembourguish citizens spend from Monday to Sunday. For example, on average, people that are in employment spend almost 8 hours sleeping during the week days, and 8:45 hours on Saturday.\n\n\nAs you can see from the screenshot, each sheet contains several tables that have lots of headers and these tables are next to one another. Trying to import these sheets with good ol’ readxl::read_excel() produces a monster.\n\n\nThis is where {tidyxl} comes into play. Let’s import the workbook with {tidyxl}:\n\nlibrary(tidyverse)\nlibrary(tidyxl)\n\ntime_use_xl &lt;- xlsx_cells(\"time-use.xlsx\")\n\nLet’s see what happened:\n\nhead(time_use_xl)\n## # A tibble: 6 x 21\n##   sheet address   row   col is_blank data_type error logical numeric\n##   &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;lgl&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;     &lt;dbl&gt;\n## 1 Index A1          1     1 FALSE    character &lt;NA&gt;  NA           NA\n## 2 Index B1          1     2 TRUE     blank     &lt;NA&gt;  NA           NA\n## 3 Index C1          1     3 TRUE     blank     &lt;NA&gt;  NA           NA\n## 4 Index D1          1     4 TRUE     blank     &lt;NA&gt;  NA           NA\n## 5 Index E1          1     5 TRUE     blank     &lt;NA&gt;  NA           NA\n## 6 Index F1          1     6 TRUE     blank     &lt;NA&gt;  NA           NA\n## # … with 12 more variables: date &lt;dttm&gt;, character &lt;chr&gt;,\n## #   character_formatted &lt;list&gt;, formula &lt;chr&gt;, is_array &lt;lgl&gt;,\n## #   formula_ref &lt;chr&gt;, formula_group &lt;int&gt;, comment &lt;chr&gt;, height &lt;dbl&gt;,\n## #   width &lt;dbl&gt;, style_format &lt;chr&gt;, local_format_id &lt;int&gt;\n\nAs you can see, the sheet was imported, but the result might be unexpected. Actually, time_use_xl is a tibble object, where each row is one cell of the Excel sheet. This might seem very complicated to handle, but you will see that it actually makes things way easier.\n\n\nI only want to work on the English sheets so I use the following code to ignore the French ones:\n\nsheets &lt;- xlsx_sheet_names(\"time-use.xlsx\") %&gt;%\n    keep(grepl(pattern = \".*day$\", .))\n\nAlso, there’s a sheet that aggregates the results for week days and weekends, which I also ignore.\n\n\nNow, to extract the tables from each sheet I wrote the following function:\n\nextract_data &lt;- function(sheet){\n    activities &lt;- sheet %&gt;%\n        filter(col == 2) %&gt;%\n        select(row, character) %&gt;%\n        filter(row %in% seq(6,58)) %&gt;%\n        rename(activities = character) %&gt;%\n        select(-row)\n    \n    cols_to_extract &lt;- sheet %&gt;% \n        filter(grepl(\"Population who completed.*\", character)) %&gt;% \n        pull(col)\n    \n    headers_pos &lt;- cols_to_extract - 1\n    \n    headers &lt;- sheet %&gt;%\n        filter(col %in% headers_pos, row == 3) %&gt;%\n        pull(character)\n    \n    cols_to_extract %&gt;% \n        map(~filter(sheet, col %in% .)) %&gt;%\n        map(~select(., sheet, address, row, col, character)) %&gt;%\n        map(~filter(., row %in% seq(6,58))) %&gt;%\n        map(~select(., character)) %&gt;%\n        map2(.x = ., .y = headers, ~mutate(.x, \"population\" = .y)) %&gt;%\n        map(., ~bind_cols(activities, .)) %&gt;%\n        bind_rows()\n}\n\nLet’s study it step by step and see how it works. First, there’s the argument, sheet. This function will be mapped to each sheet of the workbook. Then, the first block I wrote, extracts the activities:\n\n    activities &lt;- sheet %&gt;%\n        filter(col == 2) %&gt;%\n        select(row, character) %&gt;%\n        filter(row %in% seq(6,58)) %&gt;%\n        rename(activities = character) %&gt;%\n        select(-row)\n\nI only keep the second column (filter(col == 2)); col is a column of the tibble and if you look inside the workbook, you will notice that the activities are on the second column, or the B column. Then, I select two columns, the row and the character column. row is self-explanatory and character actually contains whatever is written inside the cells. Then, I only keep rows 6 to 58, because that is what interests me; the rest is either empty cells, or unneeded. Finally, I rename the character column to activities and remove the row column.\n\n\nThe second block:\n\n    cols_to_extract &lt;- sheet %&gt;% \n        filter(grepl(\"Population who completed.*\", character)) %&gt;% \n        pull(col)\n\nreturns the index of the columns I want to extract. I am only interested in the people that have completed the activities, so using grepl() inside filter(), I located these columns, and use pull()… to pull them out of the data frame! cols_to_extract is thus a nice atomic vector of columns that I want to keep.\n\n\nIn the third block, I extract the headers:\n\n    headers_pos &lt;- cols_to_extract - 1\n\nWhy - 1? This is because if you look in the Excel, you will see that the headers are one column before the column labeled “People who completed the activity”. For example on column G, I have “People who completed the activity” and on column F I have the header, in this case “Male”.\n\n\nNow I actually extract the headers:\n\n    headers &lt;- sheet %&gt;%\n        filter(col %in% headers_pos, row == 3) %&gt;%\n        pull(character)\n\nHeaders are always on the third row, but on different columns, hence the col %in% headers_pos. I then pull out the values inside the cells with pull(character). So my headers object will be an atomic vector with “All”, “Male”, “Female”, “10 - 19 years”, etc… everything on row 3.\n\n\nFinally, the last block, actually extracts the data:\n\n    cols_to_extract %&gt;% \n        map(~filter(sheet, col %in% .)) %&gt;%\n        map(~select(., sheet, address, row, col, character)) %&gt;%\n        map(~filter(., row %in% seq(6,58))) %&gt;%\n        map(~select(., character)) %&gt;%\n        map2(.x = ., .y = headers, ~mutate(.x, \"population\" = .y)) %&gt;%\n        map(., ~bind_cols(activities, .)) %&gt;%\n        bind_rows()\n\ncols_to_extract is a vector with the positions of the columns that interest me. So for example “4”, “7”, “10” and so on. I map this vector to the sheet, which returns me a list of extracted data frames. I pass this down to a select() (which is inside map()… why? Because the input parameter is a list of data frames). So for each data frame inside the list, I select the columns sheet, address, row, col and character. Then, for each data frame inside the list, I use filter() to only keep the rows from position 6 to 58. Then, I only select the character column, which actually contains the text inside the cell. Then, using map2(), I add the values inside the headers object as a new column, called population. Then, I bind the activities column to the data frame and bind all the rows together.\n\n\nTime to use this function! Let’s see:\n\nclean_data &lt;- sheets %&gt;%\n    map(~filter(time_use_xl, sheet %in% .)) %&gt;%\n    set_names(sheets) %&gt;%\n    map(extract_data) %&gt;%\n    map2(.x = ., .y = sheets, ~mutate(.x, \"day\" = .y)) %&gt;%\n    bind_rows() %&gt;%\n    select(day, population, activities, time = character)\n\nglimpse(clean_data)\n## Observations: 2,968\n## Variables: 4\n## $ day        &lt;chr&gt; \"Year 2014_Monday til Friday\", \"Year 2014_Monday til …\n## $ population &lt;chr&gt; \"All\", \"All\", \"All\", \"All\", \"All\", \"All\", \"All\", \"All…\n## $ activities &lt;chr&gt; \"Personal care\", \"Sleep\", \"Eating\", \"Other personal c…\n## $ time       &lt;chr&gt; \"11:07\", \"08:26\", \"01:47\", \"00:56\", \"07:37\", \"07:47\",…\n\nSo I map my list of sheets to the tibble I imported with readxl, use set_names to name the elements of my list (which is superfluous, but I wanted to show this; might interest you!) and then map this result to my little function. I could stop here, but I then add a new column to each data frame that contains the day on which the data was measured, bind the rows together and reorder the columns. Done!\n\n\nNow, how did I come up with this function? I did not start with a function. I started by writing some code that did what I wanted for one table only, inside one sheet only. Only when I got something that worked, did I start to generalize to several tables and then to several sheets. Most of the time spent was actually in trying to find patterns in the Excel sheet that I could use to write my function (for example noticing that the headers I wanted where always one column before the column I was interested in). This is my advice when working with function programming; always solve the issue for one element, wrap this code inside a function, and then simply map this function to a list of elements!"
  },
  {
    "objectID": "posts/2018-09-15-time_use.html",
    "href": "posts/2018-09-15-time_use.html",
    "title": "How Luxembourguish residents spend their time: a small {flexdashboard} demo using the Time use survey data",
    "section": "",
    "text": "In a previous blog post I have showed how you could use the {tidyxl} package to go from a human readable Excel Workbook to a tidy data set (or flat file, as they are also called). Some people then contributed their solutions, which is always something I really enjoy when it happens. This way, I also get to learn things!\n\n\n@expersso proposed a solution without {tidyxl}:\n\n\n\nInteresting data wrangling exercise in #rstats. My solution (without using {tidyxl}): https://t.co/VjuOoM82yX https://t.co/VsXFyowigu\n\n— Eric (@expersso) September 12, 2018\n\n\n\nBen Stenhaug also proposed a solution on his github which is simpler than my code in a lot of ways!\n\n\nUpdate: @nacnudus also contributed his own version using {unpivotr}:\n\n\n\nHere’s a version using unpivotr https://t.co/l2hy6zCuKj\n\n— Duncan Garmonsway (@nacnudus) September 15, 2018\n\n\n\nNow, it would be too bad not to further analyze this data. I’ve been wanting to play around with the {flexdashboard} package for some time now, but never really got the opportunity to do so. The opportunity has now arrived. Using the cleaned data from the last post, I will further tweak it a little bit, and then produce a very simple dashboard using {flexdashboard}.\n\n\nIf you want to skip the rest of the blog post and go directly to the dashboard, just click here.\n\n\nTo make the data useful, I need to convert the strings that represent the amount of time spent doing a task (for example “1:23”) to minutes. For this I use the {chron} package:\n\nclean_data &lt;- clean_data %&gt;%\n    mutate(time_in_minutes = paste0(time, \":00\")) %&gt;% # I need to add \":00\" for the seconds else it won't work\n    mutate(time_in_minutes = \n               chron::hours(chron::times(time_in_minutes)) * 60 + \n               chron::minutes(chron::times(time_in_minutes)))\n\nrio::export(clean_data, \"clean_data.csv\")\n\nNow we’re ready to go! Below is the code to build the dashboard; if you want to try, you should copy and paste the code inside a Rmd document:\n\n---\ntitle: \"Time Use Survey of Luxembourguish residents\"\noutput: \n  flexdashboard::flex_dashboard:\n    orientation: columns\n    vertical_layout: fill\nruntime: shiny\n\n---\n\n`` `{r setup, include=FALSE}\nlibrary(flexdashboard)\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(ggthemes)\n\nmain_categories &lt;- c(\"Personal care\",\n                     \"Employment\",\n                     \"Study\",\n                     \"Household and family care\",\n                     \"Voluntary work and meetings\",\n                     \"Social life and entertainment\",\n                     \"Sports and outdoor activities\",\n                     \"Hobbies and games\",\n                     \"Media\",\n                     \"Travel\")\n\ndf &lt;- read.csv(\"clean_data.csv\") %&gt;%\n    rename(Population = population) %&gt;%\n    rename(Activities = activities)\n`` `\n\nInputs {.sidebar}\n-----------------------------------------------------------------------\n\n`` `{r}\n\nselectInput(inputId = \"activitiesName\", \n            label = \"Choose an activity\", \n            choices = unique(df$Activities))\n\nselectInput(inputId = \"dayName\", \n            label = \"Choose a day\", \n            choices = unique(df$day), \n            selected = \"Year 2014_Monday til Friday\")\n\nselectInput(inputId = \"populationName\", \n            label = \"Choose a population\", \n            choices = unique(df$Population), \n            multiple = TRUE, selected = c(\"Male\", \"Female\"))\n\n`` `\n\nThe Time Use Survey (TUS) aims to measure accurately how people allocate their time across different day-to-day activities. To this end, people are asked to keep records of all their activities in a time diary. For each activity, additional information is collected about whether or not the person was alone doing it or together with other persons, where did the activity take place, etc. The main studies on time use have been conducted to calculate indicators making possible comparative analysis of quality of life within the same population or between countries. International studies care more about specific activities such as work (unpaid or not), free time, leisure, personal care (including sleep), etc.\nSource: http://statistiques.public.lu/en/surveys/espace-households/time-use/index.html\n\nLayout based on https://jjallaire.shinyapps.io/shiny-biclust/\n\nRow\n-----------------------------------------------------------------------\n\n### Minutes spent per day on certain activities\n    \n`` `{r}\ndfInput &lt;- reactive({\n        df %&gt;% filter(Activities == input$activitiesName,\n                      Population %in% input$populationName,\n                      day %in% input$dayName)\n    })\n\n    dfInput2 &lt;- reactive({\n        df %&gt;% filter(Activities %in% main_categories,\n                      Population %in% input$populationName,\n                      day %in% input$dayName)\n    })\n    \n  renderPlotly({\n\n        df1 &lt;- dfInput()\n\n        p1 &lt;- ggplot(df1, \n                     aes(x = Activities, y = time_in_minutes, fill = Population)) +\n            geom_col(position = \"dodge\") + \n            theme_minimal() + \n            xlab(\"Activities\") + \n            ylab(\"Time in minutes\") +\n            scale_fill_gdocs()\n\n        ggplotly(p1)})\n`` `\n\nRow \n-----------------------------------------------------------------------\n\n### Proportion of the day spent on main activities\n    \n`` `{r}\nrenderPlotly({\n    \n       df2 &lt;- dfInput2()\n       \n       p2 &lt;- ggplot(df2, \n                   aes(x = Population, y = time_in_minutes, fill = Activities)) +\n           geom_bar(stat=\"identity\", position=\"fill\") + \n            xlab(\"Proportion\") + \n            ylab(\"Proportion\") +\n           theme_minimal() +\n           scale_fill_gdocs()\n       \n       ggplotly(p2)\n   })\n`` `\n\nYou will see that I have defined the following atomic vector:\n\nmain_categories &lt;- c(\"Personal care\",\n                     \"Employment\",\n                     \"Study\",\n                     \"Household and family care\",\n                     \"Voluntary work and meetings\",\n                     \"Social life and entertainment\",\n                     \"Sports and outdoor activities\",\n                     \"Hobbies and games\",\n                     \"Media\",\n                     \"Travel\")\n\nIf you go back to the raw Excel file, you will see that these main categories are then split into secondary activities. The first bar plot of the dashboard does not distinguish between the main and secondary activities, whereas the second barplot only considers the main activities. I could have added another column to the data that helped distinguish whether an activity was a main or secondary one, but I was lazy. The source code of the dashboard is very simple as it uses R Markdown. To have interactivity, I’ve used Shiny to dynamically filter the data, and built the plots with {ggplot2}. Finally, I’ve passed the plots to the ggplotly() function from the {plotly} package for some quick and easy javascript goodness!"
  },
  {
    "objectID": "posts/2016-12-21-work-on-lists-of-datasets-instead-of-individual-datasets-by-using-functional-programming.html",
    "href": "posts/2016-12-21-work-on-lists-of-datasets-instead-of-individual-datasets-by-using-functional-programming.html",
    "title": "Work on lists of datasets instead of individual datasets by using functional programming",
    "section": "",
    "text": "Analyzing a lot of datasets can be tedious. In my work, I often have to compute descriptive statistics, or plot some graphs for some variables for a lot of datasets. The variables in question have the same name accross the datasets but are measured for different years. As an example, imagine you have this situation:\n\n\ndata2000 &lt;- mtcars\ndata2001 &lt;- mtcars\n\n\nFor the sake of argument, imagine that data2000 is data from a survey conducted in the year 2000 and data2001 is the same survey but conducted in the year 2001. For illustration purposes, I use the mtcars dataset, but I could have used any other example. In these sort of situations, the variables are named the same in both datasets. Now if I want to check the summary statistics of a variable, I might do it by running:\n\n\nsummary(data2000$cyl)\nsummary(data2001$cyl)\n\n\nbut this can get quite tedious, especially if instead of only having two years of data, you have 20 years. Another possibility is to merge both datasets and then check the summary statistics of the variable of interest. But this might require a lot of preprocessing, and sometimes you really just want to do a quick check, or some dirty graphs. So you might be tempted to write a loop, which would require to put these two datasets in some kind of structure, such as a list:\n\n\nlist_data &lt;- list(\"data2000\" = data2000, \"data2001\" = data2001)\n\nfor (i in 1:2){\n    print(summary(list_data[[i]]$cyl))\n}\n\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000 \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000\n\nBut this also might get tedious, especially if you want to do this for a lot of different variables, and want to use different functions than summary().\n\n\nAnother, simpler way of doing this, is to use purrr::map() or lapply(). But there is a catch though: how do we specify the column we want to work on? Let’s try some things out:\n\n\nlibrary(purrr)\n\nmap(list_data, summary(cyl))\n\nError in summary(cyl) : object 'cyl' not found\n\nMaybe this will work:\n\n\nmap(list_data, summary, cyl)\n\n## $data2000\n     mpg             cyl             disp             hp       \nMin.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \nMedian :19.20   Median :6.000   Median :196.3   Median :123.0  \nMean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \nMax.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n     drat             wt             qsec             vs        \nMin.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \nMedian :3.695   Median :3.325   Median :17.71   Median :0.0000  \nMean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \nMax.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n      am              gear            carb      \nMin.   :0.0000   Min.   :3.000   Min.   :1.000  \n1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \nMedian :0.0000   Median :4.000   Median :2.000  \nMean   :0.4062   Mean   :3.688   Mean   :2.812  \n3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \nMax.   :1.0000   Max.   :5.000   Max.   :8.000  \n\ndata2001\n     mpg             cyl             disp             hp       \nMin.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \nMedian :19.20   Median :6.000   Median :196.3   Median :123.0  \nMean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \nMax.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n     drat             wt             qsec             vs        \nMin.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \nMedian :3.695   Median :3.325   Median :17.71   Median :0.0000  \nMean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \nMax.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n      am              gear            carb      \nMin.   :0.0000   Min.   :3.000   Min.   :1.000  \n1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \nMedian :0.0000   Median :4.000   Median :2.000  \nMean   :0.4062   Mean   :3.688   Mean   :2.812  \n3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \nMax.   :1.0000   Max.   :5.000   Max.   :8.000\n\nNot quite! You get the summary statistics of every variable, cyl simply gets ignored. This might be ok in our small toy example, but if you have dozens of datasets with hundreds of variables, the output becomes unreadable. The solution is to use an anonymous functions:\n\n\nmap(list_data, (function(x) summary(x$cyl)))\n\n## $data2000\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000 \n\n$data2001\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   4.000   6.000   6.188   8.000   8.000\n\nThis is, in my opinion, much more readable than a loop, and the output of this is another list, so it’s easy to save it:\n\n\nsummary_cyl &lt;- map(list_data, (function(x) summary(x$cyl)))\nstr(summary_cyl)\n\n## List of 2\n$ data2000:Classes 'summaryDefault', 'table'  Named num [1:6] 4 4 6 6.19 8 ...\n .. ..- attr(*, \"names\")= chr [1:6] \"Min.\" \"1st Qu.\" \"Median\" \"Mean\" ...\n$ data2001:Classes 'summaryDefault', 'table'  Named num [1:6] 4 4 6 6.19 8 ...\n .. ..- attr(*, \"names\")= chr [1:6] \"Min.\" \"1st Qu.\" \"Median\" \"Mean\" ...\n\nWith the loop, you would need to “allocate” an empty list that you would fill at each iteration.\n\n\nSo this is already nice, but wouldn’t it be nicer to simply have to type:\n\n\nsummary(list_data$cyl)\n\n\nand have the summary of variable cyl for each dataset in the list? Well it is possible with the following function I wrote to make my life easier:\n\n\nto_map &lt;- function(func){\n  function(list, column, ...){\n    if(missing(column)){\n        res &lt;- purrr::map(list, (function(x) func(x, ...)))\n      } else {\n        res &lt;- purrr::map(list, (function(x) func(x[column], ...)))\n             }\n    res\n  }\n}\n\n\nBy following this chapter of Hadley Wickham’s book, Advanced R, I was able to write this function. What does it do? It basically generalizes a function to work on a list of datasets instead of just on a dataset. So for example, in the case of summary():\n\n\nsummarymap &lt;- to_map(summary)\n\nsummarymap(list_data, \"cyl\")\n\n$data2000\n     cyl       \nMin.   :4.000  \n1st Qu.:4.000  \nMedian :6.000  \nMean   :6.188  \n3rd Qu.:8.000  \nMax.   :8.000  \n\n$data2001\n     cyl       \nMin.   :4.000  \n1st Qu.:4.000  \nMedian :6.000  \nMean   :6.188  \n3rd Qu.:8.000  \nMax.   :8.000\n\nSo now everytime I want to have summary statistics for a variable, I just need to use summarymap():\n\n\nsummarymap(list_data, \"mpg\")\n\n## $data2000\n      mpg       \n Min.   :10.40  \n 1st Qu.:15.43  \n Median :19.20  \n Mean   :20.09  \n 3rd Qu.:22.80  \n Max.   :33.90  \n\n$data2001\n      mpg       \n Min.   :10.40  \n 1st Qu.:15.43  \n Median :19.20  \n Mean   :20.09  \n 3rd Qu.:22.80  \n Max.   :33.90\n\nIf I want the summary statistics for every variable, I simply omit the column name:\n\n\nsummarymap(list_data)\n\n$data2000\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n$data2001\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000\n\nI can use any function:\n\n\ntablemap &lt;- to_map(table)\n\ntablemap(list_data, \"cyl\")\n\n## $data2000\n\n 4  6  8 \n11  7 14 \n\n$data2001\n\n 4  6  8 \n11  7 14\n\ntablemap(list_data, \"mpg\")\n\n## $data2000\n\n10.4 13.3 14.3 14.7   15 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7 19.2 19.7 \n   2    1    1    1    1    2    1    1    1    1    1    1    1    2    1 \n  21 21.4 21.5 22.8 24.4   26 27.3 30.4 32.4 33.9 \n   2    2    1    2    1    1    1    2    1    1 \n\n$data2001\n\n10.4 13.3 14.3 14.7   15 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7 19.2 19.7 \n   2    1    1    1    1    2    1    1    1    1    1    1    1    2    1 \n  21 21.4 21.5 22.8 24.4   26 27.3 30.4 32.4 33.9 \n   2    2    1    2    1    1    1    2    1    1\n\nI hope you will find this little function useful, and as usual, for any comments just drop me an email by clicking the red enveloppe in the top right corner or tweet me."
  },
  {
    "objectID": "posts/2013-11-07-gmm-with-rmd.html",
    "href": "posts/2013-11-07-gmm-with-rmd.html",
    "title": "Nonlinear Gmm with R - Example with a logistic regression",
    "section": "",
    "text": "In this post, I will explain how you can use the R gmm package to estimate a non-linear model, and more specifically a logit model. For my research, I have to estimate Euler equations using the Generalized Method of Moments. I contacted Pierre Chaussé, the creator of the gmm library for help, since I was having some difficulties. I am very grateful for his help (without him, I'd still probably be trying to estimate my model!).\n\n\nTheoretical background, motivation and data set\n\n\nI will not dwell in the theory too much, because you can find everything you need here. I think it’s more interesting to try to understand why someone would use the Generalized Method of Moments instead of maximization of the log-likelihood. Well, in some cases, getting the log-likelihood can be quite complicated, as can be the case for arbitrary, non-linear models (for example if you want to estimate the parameters of a very non-linear utility function). Also, moment conditions can sometimes be readily available, so using GMM instead of MLE is trivial. And finally, GMM is… well, a very general method: every popular estimator can be obtained as a special case of the GMM estimator, which makes it quite useful.\n\n\nAnother question that I think is important to answer is: why this post? Well, because that’s exactly the kind of post I would have loved to have found 2 months ago, when I was beginning to work with the GMM. Most posts I found presented the gmm package with very simple and trivial examples, which weren’t very helpful. The example presented below is not very complicated per se, but much more closer to a real-world problem than most stuff that is out there. At least, I hope you will find it useful!\n\n\nFor illustration purposes, I'll use data from Marno Verbeek's A guide to modern Econometrics, used in the illustration on page 197. You can download the data from the book's companion page here under the section Data sets or from the Ecdat package in R, which I’ll be using.\n\n\nImplementation in R\n\n\nI don't estimate the exact same model, but only use a subset of the variables available in the data set. Keep in mind that this post is just for illustration purposes.\n\n\nFirst load the gmm package and load the data set:\n\n\nlibrary(gmm)\nlibrary(Ecdat)\ndata(\"Benefits\")\n\nBenefits &lt;- transform(\n  Benefits,\n  age2 = age**2,\n  rr2 = rr**2\n  )\n\n\nWe can then estimate a logit model with the glm() function:\n\n\nnative &lt;- glm(ui ~ age + age2 + dkids + dykids + head + male + married + rr + rr2,\n              data = Benefits,\n              family = binomial(link = \"logit\"),\n              na.action = na.pass)\n\nsummary(native)\n\n## \n## Call:\n## glm(formula = y ~ age + age2 + dkids + dykids + head + male + \n##     married + rr + rr2, family = binomial(link = \"logit\"), na.action = na.pass)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -1.889  -1.379   0.788   0.896   1.237  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)   \n## (Intercept) -1.00534    0.56330   -1.78   0.0743 . \n## age          0.04909    0.02300    2.13   0.0328 * \n## age2        -0.00308    0.00293   -1.05   0.2924   \n## dkids       -0.10922    0.08374   -1.30   0.1921   \n## dykids       0.20355    0.09490    2.14   0.0320 * \n## head        -0.21534    0.07941   -2.71   0.0067 **\n## male        -0.05988    0.08456   -0.71   0.4788   \n## married      0.23354    0.07656    3.05   0.0023 **\n## rr           3.48590    1.81789    1.92   0.0552 . \n## rr2         -5.00129    2.27591   -2.20   0.0280 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 6086.1  on 4876  degrees of freedom\n## Residual deviance: 5983.9  on 4867  degrees of freedom\n## AIC: 6004\n## \n## Number of Fisher Scoring iterations: 4\n\n\nNow comes the interesting part: how can you estimate such a non-linear model with the gmm() function from the gmm package?\n\n\nFor every estimation with the Generalized Method of Moments, you will need valid moment conditions. It turns out that in the case of the logit model, this moment condition is quite simple:\n\n\\[E[X' * (Y-\\Lambda(X'\\theta))] = 0\\]\n\nwhere ( () ) is the logistic function. Let's translate this condition into code. First, we need the logistic function:\n\n\nlogistic &lt;- function(theta, data) {\n    return(1/(1 + exp(-data %*% theta)))\n}\n\n\nand let's also define a new data frame, to make our life easier with the moment conditions (don’t forget to add a column of ones to the matrix, hence the 1 after y):\n\n\ndat &lt;- data.matrix(with(Benefits,\n                        cbind(ui, 1, age, age2, dkids,\n                              dykids, head, sex,\n                              married, rr, rr2)))\n\n\nand now the moment condition itself:\n\n\nmoments &lt;- function(theta, data) {\n  y &lt;- as.numeric(data[, 1])\n  x &lt;- data.matrix(data[, 2:11])\n  m &lt;- x * as.vector((y - logistic(theta, x)))\n  return(cbind(m))\n}\n\n\nThe moment condition(s) are given by a function which returns a matrix with as many columns as moment conditions (same number of columns as parameters for just-identified models).\n\n\nTo use the gmm() function to estimate our model, we need to specify some initial values to get the maximization routine going. One neat trick is simply to use the coefficients of a linear regression; I found it to work well in a lot of situations:\n\n\ninit &lt;- (lm(ui ~ age + age2 + dkids + dykids + head + sex + married + rr + rr2,\n            data = Benefits))$coefficients\n\n\nAnd finally, we have everything to use gmm():\n\n\nmy_gmm &lt;- gmm(moments, x = dat, t0 = init, type = \"iterative\", crit = 1e-25, wmatrix = \"optimal\", method = \"Nelder-Mead\", control = list(reltol = 1e-25, maxit = 20000))\n\nsummary(my_gmm)\n\n\nPlease, notice the options crit=1e-25,method=“Nelder-Mead”,control=list(reltol=1e-25,maxit=20000): these options mean that the Nelder-Mead algorithm is used, and to specify further options to the Nelder-Mead algorithm, the control option is used. This is very important, as Pierre Chaussé explained to me: non-linear optimization is an art, and most of the time the default options won't cut it and will give you false results. To add insult to injury, the Generalized Method of Moments itself is very capricious and you will also have to play around with different initial values to get good results. As you can see, the Convergence code equals 10, which is a code specific to the Nelder-Mead method which indicates «degeneracy of the Nelder–Mead simplex.» . I’m not sure if this is a bad thing though, but other methods can give you better results. I’d suggest you try always different maximization routines with different starting values to see if your estimations are robust. Here, the results are very similar to what we obtained with the built-in function glm() so we can stop here.\n\n\nShould you notice any error whatsoever, do not hesitate to tell me."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html",
    "title": "How to use jailbreakr",
    "section": "",
    "text": "The jailbreakr package is probably one of the most interesting packages I came across recently. This package makes it possible to extract messy data from spreadsheets. What is meant by messy? I am sure you already had to deal with spreadsheets that contained little tables inside a single sheet for example. As far as I know, there is no simple way of extracting these tables without having to fiddle around a lot. This is now over with jailbreakr. Well not entirely, because jailbreakr is still in development, but it works well already. If you want to know more about the planned features, you can watch the following video by Jenny Bryan, one of the package’s authors."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html#what-is-jailbreakr",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html#what-is-jailbreakr",
    "title": "How to use jailbreakr",
    "section": "",
    "text": "The jailbreakr package is probably one of the most interesting packages I came across recently. This package makes it possible to extract messy data from spreadsheets. What is meant by messy? I am sure you already had to deal with spreadsheets that contained little tables inside a single sheet for example. As far as I know, there is no simple way of extracting these tables without having to fiddle around a lot. This is now over with jailbreakr. Well not entirely, because jailbreakr is still in development, but it works well already. If you want to know more about the planned features, you can watch the following video by Jenny Bryan, one of the package’s authors."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html#installation-and-data",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html#installation-and-data",
    "title": "How to use jailbreakr",
    "section": "\nInstallation and data\n",
    "text": "Installation and data\n\n\nYou will have to install the package from Github, as it is not on CRAN yet. Here is the Github link. To install the package, just run the following commands in an R console:\n\ndevtools::install_github(c(\"hadley/xml2\",\n                           \"rsheets/linen\",\n                           \"rsheets/cellranger\",\n                           \"rsheets/rexcel\",\n                           \"rsheets/jailbreakr\"))\n\nIf you get the following error:\n\ndevtools::install_github(\"hadley/xml2\")\nDownloading GitHub repo hadley/xml2@master\nfrom URL https://api.github.com/repos/hadley/xml2/zipball/master\nError in system(full, intern = quiet, ignore.stderr = quiet, ...) :\n    error in running command\n\nand if you’re on a GNU+Linux distribution try to run the following command:\n\noptions(unzip = \"internal\")\n\nand then run github_install() again.\n\n\nAs you can see, you need some other packages to make it work. Now we are going to get some data. We are going to download some time series from the European Commission, data I had to deal with recently. Download the data by clicking here and look for the spreadsheet titled Investment_total_factors_nace2.xlsx. The data we are interested in is on the second sheet, named TOT. You cannot import this sheet easily into R because there are four tables on the same sheet. Let us use jailbreakr to get these tables out of the sheet and into nice, tidy, data frames."
  },
  {
    "objectID": "posts/2017-02-17-how_to_use_jailbreakr.html#jailbreakr-to-the-rescue",
    "href": "posts/2017-02-17-how_to_use_jailbreakr.html#jailbreakr-to-the-rescue",
    "title": "How to use jailbreakr",
    "section": "\njailbreakr to the rescue\n",
    "text": "jailbreakr to the rescue\n\n\nThe first step is to read the data in. For this, we are going to use the rexcel package, which is also part of the rsheets organization on Github that was set up by Jenny Brian and Rich Fitzjohn, the authors of these packages. rexcel imports the sheet you want but not in a way that is immediately useful to you. It just gets the sheet into R, which makes it then possible to use jailbreakr’s magic on it. First, let’s import the packages we need:\n\nlibrary(\"rexcel\")\nlibrary(\"jailbreakr\")\n\nWe need to check which sheet to import. There are two sheets, and we want to import the one called TOT, the second one. But is it really the second one? I have noticed that sometimes, there are hidden sheets which makes importing the one you want impossible. So first, let use use another package, readxl and its function excel_sheets() to make sure we are extracting the sheet we really need:\n\nsheets &lt;- readxl::excel_sheets(path_to_data)\n\ntot_sheet &lt;- which(sheets == \"TOT\")\n\nprint(tot_sheet)\n## [1] 3\n\nAs you can see, the sheet we want is not the second, but the third! Let us import this sheet into R now (this might take more time than you think; on my computer it takes around 10 seconds):\n\nmy_sheet &lt;- rexcel_read(path_to_data, sheet = tot_sheet)\n\nNow we can start using jailbreakr. The function split_sheet() is the one that splits the sheet into little tables:\n\ntables &lt;- split_sheet(my_sheet)\nstr(tables)\n## List of 4\n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 34 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list \n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 33 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list \n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 32 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list \n##  $ :Classes 'worksheet_view', 'R6' &lt;worksheet_view&gt;\n##   Public:\n##     cells: active binding\n##     clone: function (deep = FALSE) \n##     data: NULL\n##     dim: 33 28\n##     header: NULL\n##     idx: list\n##     initialize: function (sheet, xr, filter, header, data) \n##     lookup: active binding\n##     lookup2: active binding\n##     merged: active binding\n##     sheet: worksheet, R6\n##     table: function (col_names = TRUE, ...) \n##     values: function () \n##     xr: cell_limits, list\n\ntables is actually a list containing worksheet_view objects. Take a look at the dim attribute: you see the dimensions of the tables there. When I started using jailbreakr I was stuck here. I was looking for the function that would extract the data frames and could not find it. Then I watched the video and I understood what I had to do: a worksheet_view object has a values() method that does the extraction for you. This is a bit unusual in R (it made me feel like I was using Python); maybe in future versions this values() method will become a separate function of its own in the package. What happens when we use values()?\n\nlibrary(\"purrr\")\nlist_of_data &lt;-  map(tables, (function(x)(x$values())))\nmap(list_of_data, head)\n## [[1]]\n##      [,1]     [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]\n## [1,] \"TOT\"    NA      NA    NA    NA    NA    NA    NA    NA    NA   \n## [2,] \"DEMAND\" 33603   33969 34334 34699 35064 35430 35795 36160 36525\n## [3,] \"FDEMT\"  \"FDEMN\" NA    NA    NA    NA    NA    NA    NA    NA   \n## [4,] \"EU\"     \":\"     16.9  -1.4  20.2  34.5  31.4  37.5  39    37.3 \n## [5,] \"EA\"     \":\"     15.5  -13.1 14.8  30.9  25.1  35.2  39.2  37.1 \n## [6,] \"BE\"     \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   42.3  43.1 \n##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n## [1,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [2,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543\n## [3,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [4,] 39.2  27.5  20.6  21.4  29.8  26.4  32.5  47.1  19    -1.3  23.5 \n## [5,] 39.5  25.3  18.2  18.9  27.4  23    28.2  46.1  12.3  -9.3  19.3 \n## [6,] 45.8  42.2  42.9  43.8  45.8  47.4  49.1  50.9  48.2  46.9  46.3 \n##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] \n## [1,] NA    NA    NA    NA    NA    NA    NA    \n## [2,] 40908 41274 41639 42004 42369 42735 43100 \n## [3,] NA    NA    NA    NA    NA    NA    NA    \n## [4,] 29    22    21.1  25.6  31.8  22.9  \"30.7\"\n## [5,] 26.2  18.6  15.7  21.7  28.8  17.3  26.6  \n## [6,] 46.8  47.1  48.2  50.1  49.2  34.5  34.4  \n## \n## [[2]]\n##      [,1]        [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]\n## [1,] \"FINANCIAL\" 33603   33969 34334 34699 35064 35430 35795 36160 36525\n## [2,] \"FFINT\"     \"FFINN\" NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] \"EU\"        \":\"     -5.1  -6.2  2.7   6.7   9     14.4  13.9  14   \n## [4,] \"EA\"        \":\"     -8.8  -13.5 -3.4  2.6   5.7   12.5  13.2  13.1 \n## [5,] \"BE\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   21.5  22.4 \n## [6,] \"BG\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"  \n##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n## [1,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543\n## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] 16.4  9.4   7.4   8.1   12.4  8.4   13.6  23.4  4.1   -4    10.9 \n## [4,] 16.5  8     6.8   5.1   9.9   4.8   8.4   24.3  -2.8  -10.5 9.3  \n## [5,] 20.9  22.3  32.2  33.5  33.8  34.8  35    34.5  37.2  33.5  32.7 \n## [6,] \":\"   \":\"   20.8  24    27.1  28.3  33.4  37.5  37.7  26.6  30.4 \n##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] \n## [1,] 40908 41274 41639 42004 42369 42735 43100 \n## [2,] NA    NA    NA    NA    NA    NA    NA    \n## [3,] 12.4  10.2  8.8   13.4  17.4  6.2   \"12.3\"\n## [4,] 9     7.2   5     11    13.1  -1    6.5   \n## [5,] 31.5  32.3  33    31.7  32.2  19.9  20.5  \n## [6,] 33.8  35.6  36    41.5  41.6  44.2  43.8  \n## \n## [[3]]\n##      [,1]        [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]\n## [1,] \"TECHNICAL\" 33603   33969 34334 34699 35064 35430 35795 36160 36525\n## [2,] \"FTECT\"     \"FTECN\" NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] \"EU\"        \":\"     39.2  37.6  38.3  40    40.7  42.8  43.5  43.8 \n## [4,] \"EA\"        \":\"     39.7  36.2  37.5  41.2  40    44    44.8  44.9 \n## [5,] \"BE\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   58.8  58.5 \n## [6,] \"BG\"        \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"  \n##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n## [1,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543\n## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] 37    31.1  27.2  30.9  30.4  30.3  27.4  40.5  25.8  23.1  27.4 \n## [4,] 37    30.3  27.4  31    29.9  29.7  24.8  41    23.4  19.5  26.4 \n## [5,] 58.3  58.4  57.7  59.2  59.6  59.4  60.2  59.5  60.5  57.9  56.3 \n## [6,] \":\"   \":\"   17.3  17.5  21.1  21.5  25.3  28.2  26.1  21    25.3 \n##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] \n## [1,] 40908 41274 41639 42004 42369 42735 43100 \n## [2,] NA    NA    NA    NA    NA    NA    NA    \n## [3,] 28.9  26.3  31.3  32.1  32.1  30.2  \"34.6\"\n## [4,] 28.5  25.9  32.1  32.4  33.1  30.2  36    \n## [5,] 56.7  57.7  57.9  58.6  59.1  13.1  13.1  \n## [6,] 24.6  26.8  30.4  31.9  34.1  34.8  33.7  \n## \n## [[4]]\n##      [,1]    [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10] [,11]\n## [1,] \"OTHER\" 33603   33969 34334 34699 35064 35430 35795 36160 36525 36891\n## [2,] \"FOTHT\" \"FOTHN\" NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] \"EU\"    \":\"     2.9   -0.5  3.9   3.9   1     4.1   4.7   7     7.2  \n## [4,] \"EA\"    \":\"     2.3   -4.9  1.4   1.3   -2.4  1.1   3.2   5.8   7    \n## [5,] \"BE\"    \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   14    14.9  15.9 \n## [6,] \"BG\"    \":\"     \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"   \":\"  \n##      [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]\n## [1,] 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543 40908\n## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n## [3,] -1.5  6.2   8.1   7.6   1.4   2.4   13.7  -1.9  -3.2  1.1   1.1  \n## [4,] -3.7  5.5   7.1   7.2   -2.2  0.4   15.5  -4.6  -8.4  0.3   -3.3 \n## [5,] 16.3  22.8  23.1  22.4  24.5  25.3  25.5  26.6  26.6  24.7  24.6 \n## [6,] \":\"   -2.3  -0.8  2.4   2.9   3.5   4.8   5.5   2.2   3.3   3.2  \n##      [,23] [,24] [,25] [,26] [,27] [,28]\n## [1,] 41274 41639 42004 42369 42735 43100\n## [2,] NA    NA    NA    NA    NA    NA   \n## [3,] -1.6  0.9   2.7   1.9   -3.3  \"2.1\"\n## [4,] -2.3  0.6   2.5   2.1   -5.4  1.7  \n## [5,] 26.4  25.9  25    25.3  4.7   5.2  \n## [6,] 5.9   7     8.2   9.6   9.4   9.1\n\nWe are getting really close to something useful! Now we can get the first table and do some basic cleaning to have a tidy dataset:\n\ndataset1 &lt;- list_of_data[[1]]\n\ndataset1 &lt;- dataset1[-c(1:3), ]\ndataset1[dataset1 == \":\"] &lt;- NA\ncolnames(dataset1) &lt;- c(\"country\", seq(from = 1991, to = 2017))\n\nhead(dataset1)\n##      country 1991 1992 1993  1994 1995 1996 1997 1998 1999 2000 2001 2002\n## [1,] \"EU\"    NA   16.9 -1.4  20.2 34.5 31.4 37.5 39   37.3 39.2 27.5 20.6\n## [2,] \"EA\"    NA   15.5 -13.1 14.8 30.9 25.1 35.2 39.2 37.1 39.5 25.3 18.2\n## [3,] \"BE\"    NA   NA   NA    NA   NA   NA   NA   42.3 43.1 45.8 42.2 42.9\n## [4,] \"BG\"    NA   NA   NA    NA   NA   NA   NA   NA   NA   NA   NA   39.6\n## [5,] \"CZ\"    NA   NA   NA    NA   NA   NA   NA   NA   NA   NA   NA   54.9\n## [6,] \"DK\"    49.5 45   50    59.5 62.5 55.5 60.5 57.5 56   61.5 57.5 59.5\n##      2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016\n## [1,] 21.4 29.8 26.4 32.5 47.1 19   -1.3 23.5 29   22   21.1 25.6 31.8 22.9\n## [2,] 18.9 27.4 23   28.2 46.1 12.3 -9.3 19.3 26.2 18.6 15.7 21.7 28.8 17.3\n## [3,] 43.8 45.8 47.4 49.1 50.9 48.2 46.9 46.3 46.8 47.1 48.2 50.1 49.2 34.5\n## [4,] 43   42.8 45.5 49.1 52.6 50.7 39.5 45.5 47.4 45.6 50.5 51.4 49.9 53.2\n## [5,] 37   48.5 67.9 66.4 66.8 69.3 64.7 61   56   47.5 53   53.5 67.5 58  \n## [6,] 53.5 50   59   64   63   56   33.5 57   47   48   52   45.5 40.5 36.5\n##      2017  \n## [1,] \"30.7\"\n## [2,] 26.6  \n## [3,] 34.4  \n## [4,] 52.8  \n## [5,] 59.5  \n## [6,] 37.5\n\nEt voilà! We went from a messy spreadsheet to a tidy dataset in a matter of minutes. Even though this package is still in early development and not all the features that are planned are available, the basics are there and can save you a lot of pain!"
  },
  {
    "objectID": "posts/2017-10-26-margins_r.html",
    "href": "posts/2017-10-26-margins_r.html",
    "title": "Easy peasy STATA-like marginal effects with R",
    "section": "",
    "text": "Model interpretation is essential in the social sciences. If one wants to know the effect of variable x on the dependent variable y, marginal effects are an easy way to get the answer. STATA includes a margins command that has been ported to R by Thomas J. Leeper of the London School of Economics and Political Science. You can find the source code of the package on github. In this short blog post, I demo some of the functionality of margins.\n\n\nFirst, let’s load some packages:\n\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(broom)\nlibrary(margins)\nlibrary(Ecdat)\n\nAs an example, we are going to use the Participation data from the Ecdat package:\n\ndata(Participation)\n?Participation\nLabor Force Participation\n\nDescription\n\na cross-section\n\nnumber of observations : 872\n\nobservation : individuals\n\ncountry : Switzerland\n\nUsage\n\ndata(Participation)\nFormat\n\nA dataframe containing :\n\nlfp\nlabour force participation ?\n\nlnnlinc\nthe log of nonlabour income\n\nage\nage in years divided by 10\n\neduc\nyears of formal education\n\nnyc\nthe number of young children (younger than 7)\n\nnoc\nnumber of older children\n\nforeign\nforeigner ?\n\nSource\n\nGerfin, Michael (1996) “Parametric and semiparametric estimation of the binary response”, Journal of Applied Econometrics, 11(3), 321-340.\n\nReferences\n\nDavidson, R. and James G. MacKinnon (2004) Econometric Theory and Methods, New York, Oxford University Press, http://www.econ.queensu.ca/ETM/, chapter 11.\n\nJournal of Applied Econometrics data archive : http://qed.econ.queensu.ca/jae/.\n\nThe variable of interest is lfp: whether the individual participates in the labour force or not. To know which variables are relevant in the decision to participate in the labour force, one could estimate a logit model, using glm().\n\nlogit_participation = glm(lfp ~ ., data = Participation, family = \"binomial\")\n\nNow that we ran the regression, we can take a look at the results. I like to use broom::tidy() to look at the results of regressions, as tidy() returns a nice data.frame, but you could use summary() if you’re only interested in reading the output:\n\ntidy(logit_participation)\n##          term    estimate  std.error  statistic      p.value\n## 1 (Intercept) 10.37434616 2.16685216  4.7877499 1.686617e-06\n## 2     lnnlinc -0.81504064 0.20550116 -3.9661122 7.305449e-05\n## 3         age -0.51032975 0.09051783 -5.6378920 1.721444e-08\n## 4        educ  0.03172803 0.02903580  1.0927211 2.745163e-01\n## 5         nyc -1.33072362 0.18017027 -7.3859224 1.514000e-13\n## 6         noc -0.02198573 0.07376636 -0.2980454 7.656685e-01\n## 7  foreignyes  1.31040497 0.19975784  6.5599678 5.381941e-11\n\nFrom the results above, one can only interpret the sign of the coefficients. To know how much a variable influences the labour force participation, one has to use margins():\n\neffects_logit_participation = margins(logit_participation) \n\nprint(effects_logit_participation)\n## Average marginal effects\n## glm(formula = lfp ~ ., family = \"binomial\", data = Participation)\n##  lnnlinc     age     educ     nyc       noc foreignyes\n##  -0.1699 -0.1064 0.006616 -0.2775 -0.004584     0.2834\n\nUsing summary() on the object returned by margins() provides more details:\n\nsummary(effects_logit_participation)\n##      factor     AME     SE       z      p   lower   upper\n##         age -0.1064 0.0176 -6.0494 0.0000 -0.1409 -0.0719\n##        educ  0.0066 0.0060  1.0955 0.2733 -0.0052  0.0185\n##  foreignyes  0.2834 0.0399  7.1102 0.0000  0.2053  0.3615\n##     lnnlinc -0.1699 0.0415 -4.0994 0.0000 -0.2512 -0.0887\n##         noc -0.0046 0.0154 -0.2981 0.7656 -0.0347  0.0256\n##         nyc -0.2775 0.0333 -8.3433 0.0000 -0.3426 -0.2123\n\nAnd it is also possible to plot the effects with base graphics:\n\nplot(effects_logit_participation)\n\n\n\n\nThis uses the basic R plotting capabilities, which is useful because it is a simple call to the function plot() but if you’ve been using ggplot2 and want this graph to have the same look as the others made with ggplot2 you first need to save the summary in a variable. Let’s overwrite this effects_logit_participation variable with its summary:\n\neffects_logit_participation = summary(effects_logit_participation)\n\nAnd now it is possible to use ggplot2 to create the same plot:\n\nggplot(data = effects_logit_participation) +\n  geom_point(aes(factor, AME)) +\n  geom_errorbar(aes(x = factor, ymin = lower, ymax = upper)) +\n  geom_hline(yintercept = 0) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45))\n\n\n\n\nSo an infinitesimal increase, in say, non-labour income (lnnlinc) of 0.001 is associated with a decrease of the probability of labour force participation by 0.001*17 percentage points.\n\n\nYou can also extract the marginal effects of a single variable, with dydx():\n\nhead(dydx(Participation, logit_participation, \"lnnlinc\"))\n##   dydx_lnnlinc\n## 1  -0.15667764\n## 2  -0.20014487\n## 3  -0.18495109\n## 4  -0.05377262\n## 5  -0.18710476\n## 6  -0.19586986\n\nWhich makes it possible to extract the effects for a list of individuals that you can create yourself:\n\nmy_subjects = tribble(\n    ~lfp,  ~lnnlinc, ~age, ~educ, ~nyc, ~noc, ~foreign,\n    \"yes\",   10.780,  7.0,     4,    1,    1,    \"yes\",\n     \"no\",     1.30,  9.0,     1,    4,    1,    \"yes\"\n)\n\ndydx(my_subjects, logit_participation, \"lnnlinc\")\n##   dydx_lnnlinc\n## 1  -0.09228119\n## 2  -0.17953451\n\nI used the tribble() function from the tibble package to create this test data set, row by row. Then, using dydx(), I get the marginal effect of variable lnnlinc for these two individuals. No doubt that this package will be a huge help convincing more social scientists to try out R and make a potential transition from STATA easier."
  },
  {
    "objectID": "posts/2017-11-14-peace_r.html",
    "href": "posts/2017-11-14-peace_r.html",
    "title": "Peace of mind with purrr",
    "section": "",
    "text": "I think what I enjoy the most about functional programming is the peace of mind that comes with it. With functional programming, there’s a lot of stuff you don’t need to think about. You can write functions that are general enough so that they solve a variety of problems. For example, imagine for a second that R does not have the sum() function anymore. If you want to compute the sum of, say, the first 100 integers, you could write a loop that would do that for you:\n\nnumbers = 0\n\nfor (i in 1:100){\n  numbers = numbers + i\n}\n\nprint(numbers)\n\n[1] 5050\n\n\nThe problem with this approach, is that you cannot reuse any of the code there, even if you put it inside a function. For instance, what if you want to merge 4 datasets together? You would need something like this:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata(mtcars)\n\nmtcars1 = mtcars %&gt;%\n  mutate(id = \"1\")\n\nmtcars2 = mtcars %&gt;%\n  mutate(id = \"2\")\n\nmtcars3 = mtcars %&gt;%\n  mutate(id = \"3\")\n\nmtcars4 = mtcars %&gt;%\n  mutate(id = \"4\")\n\ndatasets = list(mtcars1, mtcars2, mtcars3, mtcars4)\n\ntemp = datasets[[1]]\n\nfor(i in 1:3){\n  temp = full_join(temp, datasets[[i+1]])\n}\n\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\n\n\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\n\nglimpse(temp)\n\nRows: 128\nColumns: 12\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n$ id   &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n\n\nOf course, the logic is very similar as before, but you need to think carefully about the structure holding your elements (which can be numbers, datasets, characters, etc…) as well as be careful about indexing correctly… and depending on the type of objects you are working on, you might need to tweak the code further.\nHow would a functional programming approach make this easier? Of course, you could use purrr::reduce() to solve these problems. However, since I assumed that sum() does not exist, I will also assume that purrr::reduce() does not exist either and write my own, clumsy implementation. Here’s the code:\n\nmy_reduce = function(a_list, a_func, init = NULL, ...){\n\n  if(is.null(init)){\n    init = `[[`(a_list, 1)\n    a_list = tail(a_list, -1)\n  }\n\n  car = `[[`(a_list, 1)\n  cdr = tail(a_list, -1)\n  init = a_func(init, car, ...)\n\n  if(length(cdr) != 0){\n    my_reduce(cdr, a_func, init, ...)\n  }\n  else {\n    init\n  }\n}\n\nThis can look much more complicated than before, but the idea is quite simple; if you know about recursive functions (recursive functions are functions that call themselves). I won’t explain how the function works, because it is not the main point of the article (but if you’re curious, I encourage you to play around with it). The point is that now, I can do the following:\n\nmy_reduce(list(1,2,3,4,5), `+`)\n\n[1] 15\n\nmy_reduce(datasets, full_join) %&gt;% glimpse\n\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb, id)`\n\n\nRows: 128\nColumns: 12\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n$ id   &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n\n\nBut since my_reduce() is very general, I can even do this:\n\nmy_reduce(list(1, 2, 3, 4, \"5\"), paste)\n\n[1] \"1 2 3 4 5\"\n\n\nOf course, paste() is vectorized, so you could just as well do paste(1, 2, 3, 4, 5), but again, I want to insist on the fact that writing functions, even if they look a bit complicated, can save you a huge amount of time in the long run.\nBecause I know that my function is quite general, I can be confident that it will work in a lot of different situations; as long as the a_func argument is a binary operator that combines the elements inside a_list, it’s going to work. And I don’t need to think about indexing, about having temporary variables or thinking about the structure that will hold my results."
  },
  {
    "objectID": "posts/2013-01-16-simulated-maximum-likelihood-with-r.html",
    "href": "posts/2013-01-16-simulated-maximum-likelihood-with-r.html",
    "title": "Simulated Maximum Likelihood with R",
    "section": "",
    "text": "This document details section 12.4.5. Unobserved Heterogeneity Example from Cameron and Trivedi’s book - MICROECONOMETRICS: Methods and Applications. The original source code giving the results from table 12.2 are available from the authors' site here and written for Stata. This is an attempt to translate the code to R. I’d like to thank Reddit user anonemouse2010 for his advice which helped me write the function.\n\n\nConsult the original source code if you want to read the authors' comments. If you want the R source code without all the commentaries, grab it here. This is not guaranteed to work, nor to be correct. It could set your pet on fire and/or eat your first born. Use at your own risk. I may, or may not, expand this example. Corrections, constructive criticism are welcome.\n\n\nThe model is \\(y=\\theta+u+\\varepsilon\\) where \\(\\theta\\) is a scalar parameter equal to 1. \\(u\\) is extreme value type 1 (Gumbel distribution), \\(\\varepsilon \\leadsto \\mathbb{N}(0,1)\\). For more details, consult the book.\n\n\nImport the data\n\n\nYou can consult the original source code to see how the authors simulated the data. To get the same results, and verify that I didn't make mistakes I prefer importing their data directly from their website.\n\n\ndata &lt;- read.table(\"http://cameron.econ.ucdavis.edu/mmabook/mma12p2mslmsm.asc\")\nu &lt;- data[, 1]\ne &lt;- data[, 2]\ny &lt;- data[, 3]\nnumobs &lt;- length(u)\nsimreps &lt;- 10000\n\n\nSimulation\n\n\nIn the code below, the following likelihood function:\n\\[\\log{\\hat{L}_N(\\theta)} = \\dfrac{1}{N} \\sum_{i=1}^N\\log{\\big( \\dfrac{1}{S}\\sum_{s=1}^S \\dfrac{1}{\\sqrt{2\\pi}} \\exp \\{ -(-y_i-\\theta-u_i^s)^2/2 \\}\\big)}\\]\nwhich can be found on page 397 is programmed using the function sapply.\n\n\ndenssim &lt;- function(theta) {\n    loglik &lt;- mean(sapply(y, function(y) log(mean((1/sqrt(2 * pi)) * exp(-(y - theta + log(-log(runif(simreps))))^2/2)))))\n    return(-loglik)\n}\n\n\nThis likelihood is then maximized:\n\n\nsystem.time(res &lt;- optim(0.1, denssim, method = \"BFGS\", control = list(maxit = simreps)))\n\n   user  system elapsed \n  1.460   0.197   1.657 \n\n\n\nConvergence is achieved pretty rapidly, to\n\n\nres\n\n$par\n[1] 1.193006\n\n$value\n[1] 1.921685\n\n$counts\nfunction gradient \n      58       10 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\nwhich is close to the true value of the parameter 1 (which was used to generate the data).\n\n\nLet's try again with another parameter value, for example ( ). We have to generate y again:\n\n\ny2 &lt;- 2.5 + u + e\n\n\nand slightly modify the likelihood:\n\n\ndenssim2 &lt;- function(theta) {\n  loglik &lt;- mean(\n    sapply(\n      y2,\n      function(y2) log(mean((1/sqrt(2 * pi)) * exp(-(y2 - theta + log(-log(runif(simreps))))^2/2)))))\n  return(-loglik)\n}\n\n\nwhich can then be maximized:\n\n\nsystem.time(res2 &lt;- optim(0.1, denssim2, method = \"BFGS\", control = list(maxit = simreps)))\n\n   user  system elapsed \n  1.321   0.000   1.322 \n\n\n\nThe value that maximizes the likelihood is:\n\n\nres2\n\n$par\n[1] 2.73753\n\n$value\n[1] 1.92257\n\n$counts\nfunction gradient \n      49       10 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\nwhich is close to the true value of the parameter 2.5 (which was used to generate the data)."
  },
  {
    "objectID": "posts/2018-06-24-fun_ts.html",
    "href": "posts/2018-06-24-fun_ts.html",
    "title": "Forecasting my weight with R",
    "section": "",
    "text": "I’ve been measuring my weight almost daily for almost 2 years now; I actually started earlier, but not as consistently. The goal of this blog post is to get re-acquaiented with time series; I haven’t had the opportunity to work with time series for a long time now and I have seen that quite a few packages that deal with time series have been released on CRAN. In this blog post, I will explore my weight measurements using some functions from the {tsibble} and {tibbletime} packages, and then do some predictions with the {forecast} package.\n\n\nFirst, let’s load the needed packages, read in the data and convert it to a tsibble:\n\nlibrary(\"tidyverse\")\nlibrary(\"readr\")\nlibrary(\"forecast\")\nlibrary(\"tsibble\")\nlibrary(\"tibbletime\")\nlibrary(\"mice\")\nweight &lt;- read_csv(\"https://gist.githubusercontent.com/b-rodrigues/ea60679135f8dbed448ccf66a216811f/raw/18b469f3b0720f76ce5ee2715d0f9574b615f170/gistfile1.txt\") %&gt;% \n    as_tsibble()\n## Parsed with column specification:\n## cols(\n##   Date = col_date(format = \"\"),\n##   Poids = col_double()\n## )\n## The `index` is `Date`.\n\nYou can read more about {tsibble} here. Here, I use {tsibble} mostly for the next step, which is using the function fill_na() on the tsibble. fill_na() turns implicit missing values into explicit missing values. These are implicit missing values:\n\n          Date Poids\n1   2013-01-01 84.10\n2   2013-01-04 85.60\n\nand this is the same view, but with explicit missing values:\n\n          Date Poids\n1   2013-01-01 84.10\n2   2013-01-02 NA\n3   2013-01-03 NA\n4   2013-01-04 85.60\n\nThis is useful to do, because I want to impute the missing values using the {mice} package. Let’s do this:\n\nweight &lt;- weight %&gt;% \n    fill_na()\n\nimp_weight &lt;- mice(data = weight) %&gt;% \n    mice::complete(\"long\")\n## \n##  iter imp variable\n##   1   1  Poids\n##   1   2  Poids\n##   1   3  Poids\n##   1   4  Poids\n##   1   5  Poids\n##   2   1  Poids\n##   2   2  Poids\n##   2   3  Poids\n##   2   4  Poids\n##   2   5  Poids\n##   3   1  Poids\n##   3   2  Poids\n##   3   3  Poids\n##   3   4  Poids\n##   3   5  Poids\n##   4   1  Poids\n##   4   2  Poids\n##   4   3  Poids\n##   4   4  Poids\n##   4   5  Poids\n##   5   1  Poids\n##   5   2  Poids\n##   5   3  Poids\n##   5   4  Poids\n##   5   5  Poids\n\nLet’s take a look at imp_weight:\n\nhead(imp_weight)\n##   .imp .id       Date Poids\n## 1    1   1 2013-10-28  84.1\n## 2    1   2 2013-10-29  84.4\n## 3    1   3 2013-10-30  83.5\n## 4    1   4 2013-10-31  84.1\n## 5    1   5 2013-11-01  85.6\n## 6    1   6 2013-11-02  85.2\n\nLet’s select the relevant data. I filter from the 11th of July 2016, which is where I started weighing myself almost every day, to the 31st of May 2018. I want to predict my weight for the month of June (you might think of the month of June 2018 as the test data, and the rest as training data):\n\nimp_weight_train &lt;- imp_weight %&gt;% \n    filter(Date &gt;= \"2016-07-11\", Date &lt;= \"2018-05-31\")\n\nIn the next lines, I create a column called imputation which is simply the same as the column .imp but of character class, remove unneeded columns and rename some other columns (“Poids” is French for weight):\n\nimp_weight_train &lt;- imp_weight_train %&gt;% \n    mutate(imputation = as.character(.imp)) %&gt;% \n    select(-.id, -.imp) %&gt;% \n    rename(date = Date) %&gt;% \n    rename(weight = Poids)\n\nLet’s take a look at the data:\n\nggplot(imp_weight_train, aes(date, weight, colour = imputation)) +\n    geom_line() + \n    theme(legend.position = \"bottom\")\n\n\n\n\nThis plots gives some info, but it might be better to smooth the lines. This is possible by computing a rolling mean. For this I will use the rollify() function of the {tibbletime} package:\n\nmean_roll_5 &lt;- rollify(mean, window = 5)\nmean_roll_10 &lt;- rollify(mean, window = 10)\n\nrollify() can be seen as an adverb, pretty much like purrr::safely(); rollify() is a higher order function that literally rollifies a function, in this case mean() which means that rollifying the mean creates a function that returns the rolling mean. The window argument lets you decide how smooth you want the curve to be: the higher the smoother. However, you will lose some observations. Let’s use this functions to add the rolling means to the data frame:\n\nimp_weight_train &lt;- imp_weight_train %&gt;% \n    group_by(imputation) %&gt;% \n    mutate(roll_5 = mean_roll_5(weight),\n           roll_10 = mean_roll_10(weight))\n\nNow, let’s plot these new curves:\n\nggplot(imp_weight_train, aes(date, roll_5, colour = imputation)) +\n    geom_line() + \n    theme(legend.position = \"bottom\")\n## Warning: Removed 20 rows containing missing values (geom_path).\n\n\n\nggplot(imp_weight_train, aes(date, roll_10, colour = imputation)) +\n    geom_line() + \n    theme(legend.position = \"bottom\")\n## Warning: Removed 45 rows containing missing values (geom_path).\n\n\n\n\nThat’s easier to read, isn’t it?\n\n\nNow, I will use the auto.arima() function to train a model on the data to forecast my weight for the month of June. However, my data, imp_weight_train is a list of datasets. auto.arima() does not take a data frame as an argument, much less so a list of datasets. I’ll create a wrapper around auto.arima() that works on a dataset, and then map it to the list of datasets:\n\nauto.arima.df &lt;- function(data, y, ...){\n\n    y &lt;- enquo(y)\n\n    yts &lt;- data %&gt;% \n        pull(!!y) %&gt;% \n        as.ts()\n\n    auto.arima(yts, ...)\n}\n\nauto.arima.df() takes a data frame as argument, and then y, which is the column that contains the univariate time series. This column then gets pulled out of the data frame, converted to a time series object with as.ts(), and then passed down to auto.arima(). I can now use this function on my list of data sets. The first step is to nest the data:\n\nnested_data &lt;- imp_weight_train %&gt;% \n    group_by(imputation) %&gt;% \n    nest() \n\nLet’s take a look at nested_data:\n\nnested_data\n## # A tibble: 5 x 2\n##   imputation data              \n##   &lt;chr&gt;      &lt;list&gt;            \n## 1 1          &lt;tibble [690 × 4]&gt;\n## 2 2          &lt;tibble [690 × 4]&gt;\n## 3 3          &lt;tibble [690 × 4]&gt;\n## 4 4          &lt;tibble [690 × 4]&gt;\n## 5 5          &lt;tibble [690 × 4]&gt;\n\nnested_data is a tibble with a column called data, which is a so-called list-column. Each element of data is itself a tibble. This is a useful structure, because now I can map auto.arima.df() to the data frame:\n\nmodels &lt;- nested_data %&gt;% \n    mutate(model = map(data, auto.arima.df, y = weight))\n\nThis trick can be a bit difficult to follow the first time you see it. The idea is the following: nested_data is a tibble. Thus, I can add a column to it using mutate(). So far so good. Now that I am “inside” the mutate call, I can use purrr::map(). Why? purrr::map() takes a list and then a function as arguments. Remember that data is a list column; you can see it above, the type of the column data is list. So data is a list, and thus can be used inside purrr::map(). Great. Now, what is inside data? tibbles, where inside each of them is a column called weight. This is the column that contains my univariate time series I want to model. Let’s take a look at models:\n\nmodels\n## # A tibble: 5 x 3\n##   imputation data               model      \n##   &lt;chr&gt;      &lt;list&gt;             &lt;list&gt;     \n## 1 1          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;\n## 2 2          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;\n## 3 3          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;\n## 4 4          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;\n## 5 5          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;\n\nmodels is a tibble with a column called model, where each element is a model of type ARIMA.\n\n\nAdding forecasts is based on the same trick as above, and we use the forecast() function:\n\nforecasts &lt;- models %&gt;% \n    mutate(predictions = map(model, forecast, h = 24)) %&gt;% \n    mutate(predictions = map(predictions, as_tibble)) %&gt;% \n    pull(predictions) \n\nI forecast 24 days (I am writing this on the 24th of June), and convert the predictions to tibbles, and then pull only the predictions tibble:\n\nforecasts\n## [[1]]\n## # A tibble: 24 x 5\n##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`\n##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1             71.5    70.7    72.3    70.2    72.8\n##  2             71.5    70.7    72.4    70.3    72.8\n##  3             71.5    70.6    72.3    70.1    72.8\n##  4             71.5    70.6    72.4    70.1    72.9\n##  5             71.4    70.5    72.4    70.0    72.9\n##  6             71.5    70.5    72.4    70.0    72.9\n##  7             71.4    70.5    72.4    69.9    72.9\n##  8             71.4    70.4    72.4    69.9    72.9\n##  9             71.4    70.4    72.4    69.9    72.9\n## 10             71.4    70.4    72.4    69.8    73.0\n## # ... with 14 more rows\n## \n## [[2]]\n## # A tibble: 24 x 5\n##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`\n##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1             71.6    70.8    72.3    70.3    72.8\n##  2             71.6    70.8    72.5    70.3    72.9\n##  3             71.5    70.6    72.4    70.2    72.9\n##  4             71.5    70.6    72.5    70.1    72.9\n##  5             71.5    70.5    72.5    70.0    73.0\n##  6             71.5    70.5    72.5    70.0    73.0\n##  7             71.5    70.5    72.5    69.9    73.0\n##  8             71.5    70.4    72.5    69.9    73.1\n##  9             71.5    70.4    72.5    69.8    73.1\n## 10             71.4    70.3    72.6    69.7    73.1\n## # ... with 14 more rows\n## \n## [[3]]\n## # A tibble: 24 x 5\n##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`\n##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1             71.6    70.8    72.4    70.4    72.8\n##  2             71.5    70.7    72.4    70.2    72.8\n##  3             71.5    70.6    72.4    70.2    72.9\n##  4             71.5    70.6    72.4    70.1    72.9\n##  5             71.5    70.5    72.4    70.0    72.9\n##  6             71.5    70.5    72.4    70.0    73.0\n##  7             71.5    70.5    72.5    69.9    73.0\n##  8             71.4    70.4    72.5    69.9    73.0\n##  9             71.4    70.4    72.5    69.8    73.0\n## 10             71.4    70.4    72.5    69.8    73.1\n## # ... with 14 more rows\n## \n## [[4]]\n## # A tibble: 24 x 5\n##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`\n##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1             71.5    70.8    72.3    70.3    72.8\n##  2             71.5    70.7    72.4    70.3    72.8\n##  3             71.5    70.7    72.4    70.2    72.8\n##  4             71.5    70.6    72.4    70.1    72.9\n##  5             71.5    70.6    72.4    70.1    72.9\n##  6             71.5    70.5    72.5    70.0    73.0\n##  7             71.5    70.5    72.5    69.9    73.0\n##  8             71.5    70.4    72.5    69.9    73.0\n##  9             71.4    70.4    72.5    69.8    73.1\n## 10             71.4    70.3    72.5    69.8    73.1\n## # ... with 14 more rows\n## \n## [[5]]\n## # A tibble: 24 x 5\n##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`\n##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1             71.5    70.8    72.3    70.3    72.8\n##  2             71.5    70.7    72.4    70.3    72.8\n##  3             71.5    70.7    72.4    70.2    72.8\n##  4             71.5    70.6    72.4    70.1    72.9\n##  5             71.5    70.6    72.4    70.1    72.9\n##  6             71.5    70.5    72.4    70.0    73.0\n##  7             71.5    70.5    72.5    69.9    73.0\n##  8             71.5    70.4    72.5    69.9    73.0\n##  9             71.4    70.4    72.5    69.8    73.1\n## 10             71.4    70.3    72.5    69.8    73.1\n## # ... with 14 more rows\n\nSo forecasts is a list of tibble, each containing a forecast. Remember that I have 5 tibbles, because I imputed the data 5 times. I will merge this list of data sets together into one, but before I need to add a column that indices the forecasts:\n\nforecasts &lt;- map2(.x = forecasts, .y = as.character(seq(1, 5)), \n     ~mutate(.x, id = .y)) %&gt;% \n    bind_rows() %&gt;% \n    select(-c(`Lo 80`, `Hi 80`))\n\ncolnames(forecasts) &lt;- c(\"point_forecast\", \"low_95\", \"hi_95\", \"id\")\n\nLet’s take a look again at forecasts:\n\nforecasts\n## # A tibble: 120 x 4\n##    point_forecast low_95 hi_95 id   \n##             &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n##  1           71.5   70.2  72.8 1    \n##  2           71.5   70.3  72.8 1    \n##  3           71.5   70.1  72.8 1    \n##  4           71.5   70.1  72.9 1    \n##  5           71.4   70.0  72.9 1    \n##  6           71.5   70.0  72.9 1    \n##  7           71.4   69.9  72.9 1    \n##  8           71.4   69.9  72.9 1    \n##  9           71.4   69.9  72.9 1    \n## 10           71.4   69.8  73.0 1    \n## # ... with 110 more rows\n\nI now select the true values for the month of June. I also imputed this data, but here I will simply keep the average of the imputations:\n\nweight_june &lt;- imp_weight %&gt;% \n    filter(Date &gt;= \"2018-06-01\") %&gt;% \n    select(-.id) %&gt;% \n    group_by(Date) %&gt;% \n    summarise(true_weight = mean(Poids)) %&gt;% \n    rename(date = Date)\n\nLet’s take a look at weight_june:\n\nweight_june\n## # A tibble: 24 x 2\n##    date       true_weight\n##    &lt;date&gt;           &lt;dbl&gt;\n##  1 2018-06-01        71.8\n##  2 2018-06-02        70.8\n##  3 2018-06-03        71.2\n##  4 2018-06-04        71.4\n##  5 2018-06-05        70.9\n##  6 2018-06-06        70.8\n##  7 2018-06-07        70.5\n##  8 2018-06-08        70.1\n##  9 2018-06-09        70.3\n## 10 2018-06-10        71.0\n## # ... with 14 more rows\n\nLet’s repeat weight_june 5 times, and add the index 1 to 5. Why? Because I want to merge the true data with the forecasts, and having the data in this form makes things easier:\n\nweight_june &lt;- modify(list_along(1:5), ~`&lt;-`(., weight_june)) %&gt;% \n    map2(.y = as.character(seq(1, 5)), \n         ~mutate(.x, id = .y)) %&gt;% \n    bind_rows()\n\nThe first line:\n\nmodify(list_along(1:5), ~`&lt;-`(., weight_june)) \n\nlooks quite complicated, but you will see that it is not, once we break it apart. modify() modifies a list. The list to modify is list_along(1:5), which create a list of NULLs:\n\nlist_along(1:5)\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n\nThe second argument of modify() is either a function or a formula. I created the following formula:\n\n~`&lt;-`(., weight_june)\n\nWe all know the function &lt;-(), but are not used to see it that way. But consider the following:\n\na &lt;- 3\n`&lt;-`(a, 3)\n\nThese two formulations are equivalent. So these lines fill the empty element of the list of NULLs with the data frame weight_june. Then I add the id column and then bind the rows together: bind_rows().\n\n\nLet’s bind the columns of weight_june and forecasts and take a look at it:\n\nforecasts &lt;- bind_cols(weight_june, forecasts) %&gt;% \n    select(-id1)\n\nforecasts\n## # A tibble: 120 x 6\n##    date       true_weight id    point_forecast low_95 hi_95\n##    &lt;date&gt;           &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n##  1 2018-06-01        71.8 1               71.5   70.2  72.8\n##  2 2018-06-02        70.8 1               71.5   70.3  72.8\n##  3 2018-06-03        71.2 1               71.5   70.1  72.8\n##  4 2018-06-04        71.4 1               71.5   70.1  72.9\n##  5 2018-06-05        70.9 1               71.4   70.0  72.9\n##  6 2018-06-06        70.8 1               71.5   70.0  72.9\n##  7 2018-06-07        70.5 1               71.4   69.9  72.9\n##  8 2018-06-08        70.1 1               71.4   69.9  72.9\n##  9 2018-06-09        70.3 1               71.4   69.9  72.9\n## 10 2018-06-10        71.0 1               71.4   69.8  73.0\n## # ... with 110 more rows\n\nNow, for the last plot:\n\nggplot(forecasts, aes(x = date, colour = id)) +\n    geom_line(aes(y = true_weight), size = 2) + \n    geom_line(aes(y = hi_95)) + \n    geom_line(aes(y = low_95)) + \n    theme(legend.position = \"bottom\")\n\n\n\n\nThe true data fall within all the confidence intervals, but I am a bit surprised by the intervals, especially the upper confidence intervals; they all are way above 72kg, however my true weight has been fluctuating around 71kg for quite some months now. I think I have to refresh my memory on time series, because I am certainly missing something!\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2018-04-10-brotools_describe.html",
    "href": "posts/2018-04-10-brotools_describe.html",
    "title": "Get basic summary statistics for all the variables in a data frame",
    "section": "",
    "text": "I have added a new function to my {brotools} package, called describe(), which takes a data frame as an argument, and returns another data frame with descriptive statistics. It is very much inspired by the {skmir} package but also by assist::describe() (click on the packages to be redirected to the respective Github repos) but I wanted to write my own for two reasons: first, as an exercice, and second I really only needed the function skim_to_wide() from {skimr}. So instead of installing a whole package for a single function, I decided to write my own (since I use {brotools} daily).\n\n\nBelow you can see it in action:\n\nlibrary(dplyr)\ndata(starwars)\nbrotools::describe(starwars)\n## # A tibble: 10 x 13\n##    variable  type   nobs  mean    sd mode     min   max   q25 median   q75\n##    &lt;chr&gt;     &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n##  1 birth_ye… Nume…    87  87.6 155.  19         8   896  35       52  72  \n##  2 height    Nume…    87 174.   34.8 172       66   264 167      180 191  \n##  3 mass      Nume…    87  97.3 169.  77        15  1358  55.6     79  84.5\n##  4 eye_color Char…    87  NA    NA   blue      NA    NA  NA       NA  NA  \n##  5 gender    Char…    87  NA    NA   male      NA    NA  NA       NA  NA  \n##  6 hair_col… Char…    87  NA    NA   blond     NA    NA  NA       NA  NA  \n##  7 homeworld Char…    87  NA    NA   Tatoo…    NA    NA  NA       NA  NA  \n##  8 name      Char…    87  NA    NA   Luke …    NA    NA  NA       NA  NA  \n##  9 skin_col… Char…    87  NA    NA   fair      NA    NA  NA       NA  NA  \n## 10 species   Char…    87  NA    NA   Human     NA    NA  NA       NA  NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n\nAs you can see, the object that is returned by describe() is a tibble.\n\n\nFor now, this function does not handle dates, but it’s in the pipeline.\n\n\nYou can also only describe certain columns:\n\nbrotools::describe(starwars, height, mass, name)\n## # A tibble: 3 x 13\n##   variable type    nobs  mean    sd mode      min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 height   Numer…    87 174.   34.8 172        66   264 167      180 191  \n## 2 mass     Numer…    87  97.3 169.  77         15  1358  55.6     79  84.5\n## 3 name     Chara…    87  NA    NA   Luke S…    NA    NA  NA       NA  NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n\nIf you want to try it out, you can install {brotools} from Github:\n\ndevtools::install_github(\"b-rodrigues/brotools\")\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2024-12-09-huhu.html",
    "href": "posts/2024-12-09-huhu.html",
    "title": "huhu",
    "section": "",
    "text": "Undesired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\n\n1 + 1"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html",
    "href": "posts/2017-03-08-lesser_known_tricks.html",
    "title": "Lesser known dplyr tricks",
    "section": "",
    "text": "In this blog post I share some lesser-known (at least I believe they are) tricks that use mainly functions from dplyr."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#removing-unneeded-columns",
    "href": "posts/2017-03-08-lesser_known_tricks.html#removing-unneeded-columns",
    "title": "Lesser known dplyr tricks",
    "section": "\nRemoving unneeded columns\n",
    "text": "Removing unneeded columns\n\n\nDid you know that you can use - in front of a column name to remove it from a data frame?\n\nmtcars %&gt;% \n    select(-disp) %&gt;% \n    head()\n##                    mpg cyl  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#re-ordering-columns",
    "href": "posts/2017-03-08-lesser_known_tricks.html#re-ordering-columns",
    "title": "Lesser known dplyr tricks",
    "section": "\nRe-ordering columns\n",
    "text": "Re-ordering columns\n\n\nStill using select(), it is easy te re-order columns in your data frame:\n\nmtcars %&gt;% \n    select(cyl, disp, hp, everything()) %&gt;% \n    head()\n##                   cyl disp  hp  mpg drat    wt  qsec vs am gear carb\n## Mazda RX4           6  160 110 21.0 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag       6  160 110 21.0 3.90 2.875 17.02  0  1    4    4\n## Datsun 710          4  108  93 22.8 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive      6  258 110 21.4 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout   8  360 175 18.7 3.15 3.440 17.02  0  0    3    2\n## Valiant             6  225 105 18.1 2.76 3.460 20.22  1  0    3    1\n\nAs its name implies everything() simply means all the other columns."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#renaming-columns-with-rename",
    "href": "posts/2017-03-08-lesser_known_tricks.html#renaming-columns-with-rename",
    "title": "Lesser known dplyr tricks",
    "section": "\nRenaming columns with rename()\n",
    "text": "Renaming columns with rename()\n\nmtcars &lt;- rename(mtcars, spam_mpg = mpg)\nmtcars &lt;- rename(mtcars, spam_disp = disp)\nmtcars &lt;- rename(mtcars, spam_hp = hp)\n\nhead(mtcars)\n##                   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am\n## Mazda RX4             21.0   6       160     110 3.90 2.620 16.46  0  1\n## Mazda RX4 Wag         21.0   6       160     110 3.90 2.875 17.02  0  1\n## Datsun 710            22.8   4       108      93 3.85 2.320 18.61  1  1\n## Hornet 4 Drive        21.4   6       258     110 3.08 3.215 19.44  1  0\n## Hornet Sportabout     18.7   8       360     175 3.15 3.440 17.02  0  0\n## Valiant               18.1   6       225     105 2.76 3.460 20.22  1  0\n##                   gear carb\n## Mazda RX4            4    4\n## Mazda RX4 Wag        4    4\n## Datsun 710           4    1\n## Hornet 4 Drive       3    1\n## Hornet Sportabout    3    2\n## Valiant              3    1"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#selecting-columns-with-a-regexp",
    "href": "posts/2017-03-08-lesser_known_tricks.html#selecting-columns-with-a-regexp",
    "title": "Lesser known dplyr tricks",
    "section": "\nSelecting columns with a regexp\n",
    "text": "Selecting columns with a regexp\n\n\nIt is easy to select the columns that start with “spam” with some helper functions:\n\nmtcars %&gt;% \n    select(contains(\"spam\")) %&gt;% \n    head()\n##                   spam_mpg spam_disp spam_hp\n## Mazda RX4             21.0       160     110\n## Mazda RX4 Wag         21.0       160     110\n## Datsun 710            22.8       108      93\n## Hornet 4 Drive        21.4       258     110\n## Hornet Sportabout     18.7       360     175\n## Valiant               18.1       225     105\n\ntake also a look at starts_with(), ends_with(), contains(), matches(), num_range(), one_of() and everything()."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#create-new-columns-with-mutate-and-if_else",
    "href": "posts/2017-03-08-lesser_known_tricks.html#create-new-columns-with-mutate-and-if_else",
    "title": "Lesser known dplyr tricks",
    "section": "\nCreate new columns with mutate() and if_else()\n",
    "text": "Create new columns with mutate() and if_else()\n\nmtcars %&gt;% \n    mutate(vs_new = if_else(\n        vs == 1, \n        \"one\", \n        \"zero\", \n        NA_character_)) %&gt;% \n    head()\n##   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am gear carb vs_new\n## 1     21.0   6       160     110 3.90 2.620 16.46  0  1    4    4   zero\n## 2     21.0   6       160     110 3.90 2.875 17.02  0  1    4    4   zero\n## 3     22.8   4       108      93 3.85 2.320 18.61  1  1    4    1    one\n## 4     21.4   6       258     110 3.08 3.215 19.44  1  0    3    1    one\n## 5     18.7   8       360     175 3.15 3.440 17.02  0  0    3    2   zero\n## 6     18.1   6       225     105 2.76 3.460 20.22  1  0    3    1    one\n\nYou might want to create a new variable conditionally on several values of another column:\n\nmtcars %&gt;% \n    mutate(carb_new = case_when(.$carb == 1 ~ \"one\",\n                                .$carb == 2 ~ \"two\",\n                                .$carb == 4 ~ \"four\",\n                                 TRUE ~ \"other\")) %&gt;% \n    head(15)\n##    spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am gear carb\n## 1      21.0   6     160.0     110 3.90 2.620 16.46  0  1    4    4\n## 2      21.0   6     160.0     110 3.90 2.875 17.02  0  1    4    4\n## 3      22.8   4     108.0      93 3.85 2.320 18.61  1  1    4    1\n## 4      21.4   6     258.0     110 3.08 3.215 19.44  1  0    3    1\n## 5      18.7   8     360.0     175 3.15 3.440 17.02  0  0    3    2\n## 6      18.1   6     225.0     105 2.76 3.460 20.22  1  0    3    1\n## 7      14.3   8     360.0     245 3.21 3.570 15.84  0  0    3    4\n## 8      24.4   4     146.7      62 3.69 3.190 20.00  1  0    4    2\n## 9      22.8   4     140.8      95 3.92 3.150 22.90  1  0    4    2\n## 10     19.2   6     167.6     123 3.92 3.440 18.30  1  0    4    4\n## 11     17.8   6     167.6     123 3.92 3.440 18.90  1  0    4    4\n## 12     16.4   8     275.8     180 3.07 4.070 17.40  0  0    3    3\n## 13     17.3   8     275.8     180 3.07 3.730 17.60  0  0    3    3\n## 14     15.2   8     275.8     180 3.07 3.780 18.00  0  0    3    3\n## 15     10.4   8     472.0     205 2.93 5.250 17.98  0  0    3    4\n##    carb_new\n## 1      four\n## 2      four\n## 3       one\n## 4       one\n## 5       two\n## 6       one\n## 7      four\n## 8       two\n## 9       two\n## 10     four\n## 11     four\n## 12    other\n## 13    other\n## 14    other\n## 15     four\n\nMind the .$ before the variable carb. There is a github issue about this, and it is already fixed in the development version of dplyr, which means that in the next version of dplyr, case_when() will work as any other specialized dplyr function inside mutate()."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#apply-a-function-to-certain-columns-only-by-rows",
    "href": "posts/2017-03-08-lesser_known_tricks.html#apply-a-function-to-certain-columns-only-by-rows",
    "title": "Lesser known dplyr tricks",
    "section": "\nApply a function to certain columns only, by rows\n",
    "text": "Apply a function to certain columns only, by rows\n\nmtcars %&gt;%\n    select(am, gear, carb) %&gt;%\n    purrr::by_row(sum, .collate = \"cols\", .to = \"sum_am_gear_carb\") -&gt; mtcars2\nhead(mtcars2)\n\nFor this, I had to use purrr’s by_row() function. You can then add this column to your original data frame:\n\nmtcars &lt;- cbind(mtcars, \"sum_am_gear_carb\" = mtcars2$sum_am_gear_carb)\nhead(mtcars)\n##                   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am\n## Mazda RX4             21.0   6       160     110 3.90 2.620 16.46  0  1\n## Mazda RX4 Wag         21.0   6       160     110 3.90 2.875 17.02  0  1\n## Datsun 710            22.8   4       108      93 3.85 2.320 18.61  1  1\n## Hornet 4 Drive        21.4   6       258     110 3.08 3.215 19.44  1  0\n## Hornet Sportabout     18.7   8       360     175 3.15 3.440 17.02  0  0\n## Valiant               18.1   6       225     105 2.76 3.460 20.22  1  0\n##                   gear carb sum_am_gear_carb\n## Mazda RX4            4    4                9\n## Mazda RX4 Wag        4    4                9\n## Datsun 710           4    1                6\n## Hornet 4 Drive       3    1                4\n## Hornet Sportabout    3    2                5\n## Valiant              3    1                4"
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#use-do-to-do-any-arbitrary-operation",
    "href": "posts/2017-03-08-lesser_known_tricks.html#use-do-to-do-any-arbitrary-operation",
    "title": "Lesser known dplyr tricks",
    "section": "\nUse do() to do any arbitrary operation\n",
    "text": "Use do() to do any arbitrary operation\n\nmtcars %&gt;% \n    group_by(cyl) %&gt;% \n    do(models = lm(spam_mpg ~ drat + wt, data = .)) %&gt;% \n    broom::tidy(models)\n## # A tibble: 9 x 6\n## # Groups:   cyl [3]\n##     cyl term        estimate std.error statistic p.value\n##   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1     4 (Intercept)   33.2      17.1       1.94  0.0877 \n## 2     4 drat           1.32      3.45      0.384 0.711  \n## 3     4 wt            -5.24      2.22     -2.37  0.0456 \n## 4     6 (Intercept)   30.7       7.51      4.08  0.0151 \n## 5     6 drat          -0.444     1.17     -0.378 0.725  \n## 6     6 wt            -2.99      1.57     -1.91  0.129  \n## 7     8 (Intercept)   29.7       7.09      4.18  0.00153\n## 8     8 drat          -1.47      1.63     -0.903 0.386  \n## 9     8 wt            -2.45      0.799    -3.07  0.0107\n\ndo() is useful when you want to use any R function (user defined functions work too!) with dplyr functions. First I grouped the observations by cyl and then ran a linear model for each group. Then I converted the output to a tidy data frame using broom::tidy()."
  },
  {
    "objectID": "posts/2017-03-08-lesser_known_tricks.html#using-dplyr-functions-inside-your-own-functions",
    "href": "posts/2017-03-08-lesser_known_tricks.html#using-dplyr-functions-inside-your-own-functions",
    "title": "Lesser known dplyr tricks",
    "section": "\nUsing dplyr functions inside your own functions\n",
    "text": "Using dplyr functions inside your own functions\n\nextract_vars &lt;- function(data, some_string){\n    \n  data %&gt;%\n    select_(lazyeval::interp(~contains(some_string))) -&gt; data\n    \n  return(data)\n}\n\nextract_vars(mtcars, \"spam\")\n##                     spam_mpg spam_disp spam_hp\n## Mazda RX4               21.0     160.0     110\n## Mazda RX4 Wag           21.0     160.0     110\n## Datsun 710              22.8     108.0      93\n## Hornet 4 Drive          21.4     258.0     110\n## Hornet Sportabout       18.7     360.0     175\n## Valiant                 18.1     225.0     105\n## Duster 360              14.3     360.0     245\n## Merc 240D               24.4     146.7      62\n## Merc 230                22.8     140.8      95\n## Merc 280                19.2     167.6     123\n## Merc 280C               17.8     167.6     123\n## Merc 450SE              16.4     275.8     180\n## Merc 450SL              17.3     275.8     180\n## Merc 450SLC             15.2     275.8     180\n## Cadillac Fleetwood      10.4     472.0     205\n## Lincoln Continental     10.4     460.0     215\n## Chrysler Imperial       14.7     440.0     230\n## Fiat 128                32.4      78.7      66\n## Honda Civic             30.4      75.7      52\n## Toyota Corolla          33.9      71.1      65\n## Toyota Corona           21.5     120.1      97\n## Dodge Challenger        15.5     318.0     150\n## AMC Javelin             15.2     304.0     150\n## Camaro Z28              13.3     350.0     245\n## Pontiac Firebird        19.2     400.0     175\n## Fiat X1-9               27.3      79.0      66\n## Porsche 914-2           26.0     120.3      91\n## Lotus Europa            30.4      95.1     113\n## Ford Pantera L          15.8     351.0     264\n## Ferrari Dino            19.7     145.0     175\n## Maserati Bora           15.0     301.0     335\n## Volvo 142E              21.4     121.0     109\n\nAbout this last point, you can read more about it here.\n\n\nHope you liked this small list of tricks!"
  },
  {
    "objectID": "posts/2013-01-29-method-of-simulated-moments-with-r.html",
    "href": "posts/2013-01-29-method-of-simulated-moments-with-r.html",
    "title": "Method of Simulated Moments with R",
    "section": "",
    "text": "This document details section 12.5.6. Unobserved Heterogeneity Example. The original source code giving the results from table 12.3 are available from the authors' site here and written for Stata. This is an attempt to translate the code to R.\n\n\nConsult the original source code if you want to read the authors' comments. If you want the R source code without all the commentaries, grab it here. This is not guaranteed to work, nor to be correct. It could set your pet on fire and/or eat your first born. Use at your own risk. I may, or may not, expand this example. Corrections, constructive criticism are welcome.\n\n\nThe model is the same as the one described here, so I won't go into details. The moment condition used is \\(E[(y_i-\\theta-u_i)]=0\\), so we can replace the expectation operator by the empirical mean:\n\n\n\\[\\dfrac{1}{N} \\sum_{i=1}^N(y_i - \\theta - E[u_i])=0\\]\n\n\nSupposing that \\(E[\\overline{u}]\\) is unknown, we can instead use the method of simulated moments for \\(\\theta\\) defined by:\n\n\n\\[\\dfrac{1}{N} \\sum_{i=1}^N(y_i - \\theta - \\dfrac{1}{S} \\sum_{s=1}^S u_i^s)=0\\]\n\n\nImport the data\n\n\nYou can consult the original source code to see how the authors simulated the data. To get the same results, and verify that I didn't make mistakes I prefer importing their data directly from their website.\n\n\ndata &lt;- read.table(\"http://cameron.econ.ucdavis.edu/mmabook/mma12p2mslmsm.asc\")\nu &lt;- data[, 1]\ne &lt;- data[, 2]\ny &lt;- data[, 3]\nnumobs &lt;- length(u)\nsimreps &lt;- 10000\n\n\nSimulation\n\n\nIn the code below, we simulate the equation defined above:\n\n\nusim &lt;- -log(-log(runif(simreps)))\nesim &lt;- rnorm(simreps, 0, 1)\n\nisim &lt;- 0\nwhile (isim &lt; simreps) {\n\n    usim = usim - log(-log(runif(simreps)))\n    esim = esim + rnorm(simreps, 0, 1)\n\n    isim = isim + 1\n\n}\n\nusimbar = usim/simreps\nesimbar = esim/simreps\n\ntheta = y - usimbar - esimbar\n\ntheta_msm &lt;- mean(theta)\napprox_sterror &lt;- sd(theta)/sqrt(simreps)\n\n\nThese steps yield the following results:\n\n\ntheta_msm\n\n[1] 1.187978\n\n\nand\n\napprox_sterror\n\n[1] 0.01676286"
  },
  {
    "objectID": "posts/2017-03-27-introducing_brotools.html",
    "href": "posts/2017-03-27-introducing_brotools.html",
    "title": "Introducing brotools",
    "section": "",
    "text": "I’m happy to announce my first R package, called brotools. This is a package that contains functions that are specific to my needs but that you might find also useful. I blogged about some of these functions, so if you follow my blog you might already be familiar with some of them. It is not on CRAN and might very well never be. The code is hosted on bitbucket and you can install the package with\n\ndevtools::install_bitbucket(\"b-rodrigues/brotools\")\n\nHope you’ll find the brotools useful!"
  },
  {
    "objectID": "posts/2017-08-27-why_tidyeval.html",
    "href": "posts/2017-08-27-why_tidyeval.html",
    "title": "Why I find tidyeval useful",
    "section": "",
    "text": "First thing’s first: maybe you shouldn’t care about tidyeval. Maybe you don’t need it. If you exclusively work interactively, I don’t think that learning about tidyeval is important. I can only speak for me, and explain to you why I personally find tidyeval useful.\n\n\nI wanted to write this blog post after reading this twitter thread and specifically this question.\n\n\nMara Averick then wrote this blogpost linking to 6 other blog posts that give some tidyeval examples. Reading them, plus the Programming with dplyr vignette should help you get started with tidyeval.\n\n\nBut maybe now you know how to use it, but not why and when you should use it… Basically, whenever you want to write a function that looks something like this:\n\nmy_function(my_data, one_column_inside_data)\n\nis when you want to use the power of tidyeval.\n\n\nI work at STATEC, Luxembourg’s national institute of statistics. I work on all kinds of different projects, and when data gets updated (for example because a new round of data collection for some survey finished), I run my own scripts on the fresh data to make the data nice and tidy for analysis. Because surveys get updated, sometimes column names change a little bit, and this can cause some issues.\n\n\nVery recently, a dataset I work with got updated. Data collection was finished, so I just loaded my hombrewed package written for this project, changed the path from last year’s script to this year’s fresh data path, ran the code, and watched as the folders got populated with new ggplot2 graphs and LaTeX tables with descriptive statistics and regression results. This is then used to generate this year’s report. However, by looking at the graphs, I noticed something weird; some graphs were showing some very strange patterns. It turns out that one column got its name changed, and also one of its values got changed too.\n\n\nLast year, this column, let’s call it spam, had values 1 for good and 0 for bad. This year the column is called Spam and the values are 1 and 2. When I found out that this was the source of the problem, I just had to change the arguments of my functions from\n\ngenerate_spam_plot(dataset = data2016, column = spam, value = 1)\ngenerate_spam_plot(dataset = data2016, column = spam, value = 0)\n\nto\n\ngenerate_spam_plot(dataset = data2017, column = Spam, value = 1)\ngenerate_spam_plot(dataset = data2017, column = Spam, value = 2)\n\nwithout needing to change anything else. This is why I use tidyeval; without it, writing a function such as genereta_spam_plot would not be easy. It would be possible, but not easy.\n\n\nIf you want to know more about tidyeval and working programmatically with R, I shamelessly invite you to read a book I’ve been working on: https://b-rodrigues.github.io/fput/ It’s still a WIP, but maybe you’ll find it useful. I plan on finishing it by the end of the year, but there’s already some content to keep you busy!"
  },
  {
    "objectID": "posts/2015-02-22-export-r-output-to-file.html",
    "href": "posts/2015-02-22-export-r-output-to-file.html",
    "title": "Export R output to a file",
    "section": "",
    "text": "Sometimes it is useful to export the output of a long-running R command. For example, you might want to run a time consuming regression just before leaving work on Friday night, but would like to get the output saved inside your Dropbox folder to take a look at the results before going back to work on Monday.\n\n\nThis can be achieved very easily using capture.output() and cat() like so:\n\nout &lt;- capture.output(summary(my_very_time_consuming_regression))\n\ncat(\"My title\", out, file=\"summary_of_my_very_time_consuming_regression.txt\", sep=\"\\n\", append=TRUE)\n\nmy_very_time_consuming_regression is an object of class lm for example. I save the output of summary(my_very_time_consuming_regression) as text using capture.output and save it in a variable called out. Finally, I save out to a file called summary_of_my_very_time_consuming_regression.txt with the first sentence being My title (you can put anything there). The file summary_of_my_very_time_consuming_regression.txt doesn’t have to already exist in your working directory. The option sep=\"\" is important or else the whole output will be written in a single line. Finally, append=TRUE makes sure your file won’t be overwritten; additional output will be appended to the file, which can be nice if you want to compare different versions of your model."
  },
  {
    "objectID": "posts/2018-03-12-keep_trying.html",
    "href": "posts/2018-03-12-keep_trying.html",
    "title": "Keep trying that api call with purrr::possibly()",
    "section": "",
    "text": "Sometimes you need to call an api to get some result from a web service, but sometimes this call might fail. You might get an error 500 for example, or maybe you’re making too many calls too fast. Regarding this last point, I really encourage you to read Ethics in Web Scraping.\n\n\nIn this blog post I will show you how you can keep trying to make this api call using purrr::possibly().\n\n\nFor this, let’s use this function that will simulate an api call:\n\nget_data = function(){\n  number = rbinom(1, 1, 0.9)\n  ifelse(number == 0, \"OK\", stop(\"Error: too many calls!\"))\n}\n\nThis function simply returns a random draw from a binomial distribution. If this number equals 0 with probability 0.1, the function returns “OK”, if not, it throws an error. Because the probability of success is only 10%, your api call might be unsuccessful:\n\nget_data()\nError in ifelse(number == 0, \"OK\", stop(\"Error: too many calls!\")) :\n  Error: too many calls!\n\nHow to keep trying until it works? For this, we’re going to use purrr::possibly(); this function takes another function as argument and either returns the result, or another output in case of error, that the user can define:\n\npossibly_get_data = purrr::possibly(get_data, otherwise = NULL)\n\nLet’s try it:\n\nset.seed(12)\npossibly_get_data()\n## NULL\n\nWith set.seed(12), the function returns a number different from 0, and thus throws an error: but because we’re wrapping the function around purrr::possibly(), the function now returns NULL. The first step is done; now we can use this to our advantage:\n\ndefinitely_get_data = function(func, n_tries, sleep, ...){\n\n  possibly_func = purrr::possibly(func, otherwise = NULL)\n\n  result = NULL\n  try_number = 1\n\n  while(is.null(result) && try_number &lt;= n_tries){\n    print(paste(\"Try number: \", try_number))\n    try_number = try_number + 1\n    result = possibly_func(...)\n    Sys.sleep(sleep)\n  }\n\n  return(result)\n}\n\ndefinitely_get_data() is a function that takes any function as argument, as well as a user provided number of tries (as well as … to pass further arguments to func()). Remember, if func() fails, it will return NULL; the while loop ensures that while the result is NULL, and the number of tries is below what you provided, the function will keep getting called. I didn’t talk about sleep; this argument is provided to Sys.sleep() which introduces a break between calls that is equal to sleep seconds. This ensures you don’t make too many calls too fast. Let’s try it out:\n\nset.seed(123)\ndefinitely_get_data(get_data, 10, 1)\n## [1] \"Try number:  1\"\n## [1] \"Try number:  2\"\n## [1] \"Try number:  3\"\n## [1] \"Try number:  4\"\n## [1] \"Try number:  5\"\n## [1] \"OK\"\n\nIt took 5 tries to get the result! However, if after 10 tries get_data() fails to return what you need it will stop (but you can increase the number of tries…).\n\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  },
  {
    "objectID": "posts/2013-12-31-r-cas.html",
    "href": "posts/2013-12-31-r-cas.html",
    "title": "Using R as a Computer Algebra System with Ryacas",
    "section": "",
    "text": "R is used to perform statistical analysis and doesn't focus on symbolic maths. But it is sometimes useful to let the computer derive a function for you (and have the analytic expression of said derivative), but maybe you don't want to leave your comfy R shell. It is possible to turn R into a full-fledged computer algebra system. CASs are tools that perform symbolic operations, such as getting the expression of the derivative of a user-defined (and thus completely arbitrary) function. Popular CASs include the proprietary Mathematica and Maple. There exists a lot of CASs under a Free Software license, Maxima (based on the very old Macsyma), Yacas, Xcas… In this post I will focus on Yacas and the Ryacas libarary. There is also the possibility to use the rSympy library that uses the Sympy Python library, which has a lot more features than Yacas. However, depending on your operating system installation can be tricky as it also requires rJava as a dependency.\n\n\nEven though Ryacas is quite nice to have, there are some issues though. For example, let's say you want the first derivative of a certain function f. If you use Ryacas to get it, the returned object won't be a function. There is a way to “extract” the text from the returned object and make a function out of it. But there are still other issues; I'll discuss them later.\n\n\nInstallation\n\n\nInstallation should be rather painless. On Linux you need to install Yacas first, which should be available in the major distros' repositories. Then you can install Ryacas from within the R shell. On Windows, you need to run these three commands (don't bother installing Yacas first):\n\n\ninstall.packages(Ryacas)\nlibrary(Ryacas)\nyacasInstall()\n\n\nYou can find more information on the project's page.\n\n\nExample session\n\n\nFirst, you must load Ryacas and define symbols that you will use in your functions.\n\n\nlibrary(Ryacas)\n\n## Loading required package: Ryacas Loading required package: XML\n\n\nx &lt;- Sym(\"x\")\n\n\nYou can then define your fonctions:\n\n\nmy_func &lt;- function(x) {\n  return(x/(x^2 + 3))\n}\n\n\nAnd you can get the derivative for instance:\n\n\nmy_deriv &lt;- yacas(deriv(my_func(x), x))\n\n## [1] \"Starting Yacas!\"\n\n\nIf you check the class of my_deriv, you'll see that it is of class yacas, which is not very useful. Let's «convert» it to a function:\n\n\nmy_deriv2 &lt;- function(x) {\n  eval(parse(text = my_deriv$YacasForm))\n}\n\n\nWe can then evaluate it. A lot of different operations are possible. But there are some problems.\n\n\nIssues with Ryacas\n\n\nYou can't use elements of a vector as parameters of your function, i.e.:\n\n\ntheta &lt;- Sym(\"theta\")\nfunc &lt;- function(x) {\n  return(theta[1] * x + theta[2])\n}\n\n\nLet's integrate this\nFunc &lt;- yacas(Integrate(func(x), x)) \n\n\nreturns (x^2theta)/2+NAx; which is not quite what we want…there is a workaround however. Define your functions like this:\n\n\na &lt;- Sym(\"a\")\nb &lt;- Sym(\"b\")\nfunc2 &lt;- function(x) {\n  return(a * x + b)\n}\n\n# Let&#39;s integrate this\nFunc2 &lt;- yacas(Integrate(func2(x), x))\n\n\nwe get the expected result: (x^2a)/2+bx;. Now replace a and b by the thetas:\n\n\nFunc2 &lt;- gsub(\"a\", \"theta[1]\", Func2$YacasForm)\nFunc2 &lt;- gsub(\"b\", \"theta[2]\", Func2)\n\n\nNow we have what we want:\n\n\nFunc2\n\n## [1] \"(x^2*theta[1])/2+theta[2]*x;\"\n\n\nYou can then copy-paste this result into a function.\n\n\nAnother problem is if you use built-in functions that are different between R and Yacas. For example:\n\n\nmy_log &lt;- function(x) {\n    return(sin(log(2 + x)))\n}\n\n\nNow try to differentiate it:\n\n\ndmy_log &lt;- yacas(deriv(my_log(x), x))\n\n\nyou get: Cos(Ln(x+2))/(x+2);. The problem with this, is that R doesn't recognize Cos as the cosine (which is cos in R) and the same goes for Ln. These are valid Yacas functions, but that is not the case in R. So you'll have to use gsub to replace these functions and then copy paste the end result into a function.\n\n\nConclusion\n\n\nWhile it has some flaws, Ryacas can be quite useful if you need to derive or integrate complicated expressions that you then want to use in R. Using some of the tricks I showed here, you should be able to overcome some of its shortcomings. If installation of rJava and thus rSympy becomes easier, I'll probably also do a short blog-post about it, as it has more features than Ryacas."
  },
  {
    "objectID": "posts/2018-04-14-playing_with_furrr.html",
    "href": "posts/2018-04-14-playing_with_furrr.html",
    "title": "Imputing missing values in parallel using {furrr}",
    "section": "",
    "text": "Today I saw this tweet on my timeline:\n\n\n\nFor those of us that just can't wait until RStudio officially supports parallel purrr in #rstats, boy have I got something for you. Introducing furrr, parallel purrr through the use of futures. Go ahead, break things, you know you want to:https://t.co/l9z1UC2Tew\n\n— Davis Vaughan (@dvaughan32) April 13, 2018\n\n\n\nand as a heavy {purrr} user, as well as the happy owner of a 6-core AMD Ryzen 5 1600X cpu, I was very excited to try out {furrr}. For those unfamiliar with {purrr}, you can read some of my previous blog posts on it here, here or here.\n\n\nTo summarize very quickly: {purrr} contains so-called higher order functions, which are functions that take other functions as argument. One such function is map(). Consider the following simple example:\n\nnumbers &lt;- seq(1, 10)\n\nIf you want the square root of this numbers, you can of course simply use the sqrt() function, because it is vectorized:\n\nsqrt(numbers)\n##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751\n##  [8] 2.828427 3.000000 3.162278\n\nBut in a lot of situations, the solution is not so simple. Sometimes you have to loop over the values. This is what we would need to do if sqrt() was not vectorized:\n\nsqrt_numbers &lt;- rep(0, 10)\n\nfor(i in length(numbers)){\n  sqrt_numbers[i] &lt;- sqrt(numbers[i])\n}\n\nFirst, you need to initialize a container, and then you have to populate the sqrt_numbers list with the results. Using, {purrr} is way easier:\n\nlibrary(tidyverse)\nmap(numbers, sqrt)\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 1.414214\n## \n## [[3]]\n## [1] 1.732051\n## \n## [[4]]\n## [1] 2\n## \n## [[5]]\n## [1] 2.236068\n## \n## [[6]]\n## [1] 2.44949\n## \n## [[7]]\n## [1] 2.645751\n## \n## [[8]]\n## [1] 2.828427\n## \n## [[9]]\n## [1] 3\n## \n## [[10]]\n## [1] 3.162278\n\nmap() is only one of the nice functions that are bundled inside {purrr}. Mastering {purrr} can really make you a much more efficient R programmer. Anyways, recently, I have been playing around with imputation and the {mice} package. {mice} comes with an example dataset called boys, let’s take a look at it:\n\nlibrary(mice)\n\ndata(boys)\n\nbrotools::describe(boys) %&gt;%\n  select(variable, type, n_missing, everything())\n## # A tibble: 9 x 13\n##   variable type    n_missing  nobs   mean    sd mode     min   max   q25\n##   &lt;chr&gt;    &lt;chr&gt;       &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numeric         0   748   9.16  6.89 0.035  0.035  21.2  1.58\n## 2 bmi      Numeric        21   748  18.1   3.05 14.54 11.8    31.7 15.9 \n## 3 hc       Numeric        46   748  51.5   5.91 33.7  33.7    65   48.1 \n## 4 hgt      Numeric        20   748 132.   46.5  50.1  50     198   84.9 \n## 5 tv       Numeric       522   748  11.9   7.99 &lt;NA&gt;   1      25    4   \n## 6 wgt      Numeric         4   748  37.2  26.0  3.65   3.14  117.  11.7 \n## 7 gen      Factor        503   748  NA    NA    &lt;NA&gt;  NA      NA   NA   \n## 8 phb      Factor        503   748  NA    NA    &lt;NA&gt;  NA      NA   NA   \n## 9 reg      Factor          3   748  NA    NA    south NA      NA   NA   \n## # ... with 3 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, n_unique &lt;int&gt;\n\nIn the code above I use the describe() function from my personal package to get some summary statistics of the boys dataset (you can read more about this function here). I am especially interested in the number of missing values, which is why I re-order the columns. If I did not re-order the columns, it would not appear in the output on my blog.\n\n\nWe see that some columns have a lot of missing values. Using the mice function, it is very easy to impute them:\n\nstart &lt;- Sys.time()\nimp_boys &lt;- mice(boys, m = 10, maxit = 100, printFlag = FALSE)\nend &lt;- Sys.time() - start\n\nprint(end)\n## Time difference of 3.290611 mins\n\nImputation on a single core took around 3 minutes on my computer. This might seem ok, but if you have a larger data set with more variables, 3 minutes can become 3 hours. And if you increase maxit, which helps convergence, or the number of imputations, 3 hours can become 30 hours. With a 6-core CPU this could potentially be brought down to 5 hours (in theory). Let’s see if we can go faster, but first let’s take a look at the imputed data.\n\n\nThe mice() function returns a mids object. If you want to look at the data, you have to use the complete() function (careful, there is also a complete() function in the {tidyr} package, so to avoid problems, I suggest you explicitely call mice::complete()):\n\nimp_boys &lt;- mice::complete(imp_boys, \"long\")\n\nbrotools::describe(imp_boys) %&gt;%\n  select(variable, type, n_missing, everything())\n## # A tibble: 11 x 13\n##    variable type   n_missing  nobs   mean     sd mode     min   max    q25\n##    &lt;chr&gt;    &lt;chr&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n##  1 .id      Numer…         0  7480 374.   216.   1      1     748   188.  \n##  2 .imp     Numer…         0  7480   5.5    2.87 1      1      10     3   \n##  3 age      Numer…         0  7480   9.16   6.89 0.035  0.035  21.2   1.58\n##  4 bmi      Numer…         0  7480  18.0    3.03 14.54 11.8    31.7  15.9 \n##  5 hc       Numer…         0  7480  51.6    5.89 33.7  33.7    65    48.3 \n##  6 hgt      Numer…         0  7480 131.    46.5  50.1  50     198    83   \n##  7 tv       Numer…         0  7480   8.39   8.09 2      1      25     2   \n##  8 wgt      Numer…         0  7480  37.1   26.0  3.65   3.14  117.   11.7 \n##  9 gen      Factor         0  7480  NA     NA    G1    NA      NA    NA   \n## 10 phb      Factor         0  7480  NA     NA    P1    NA      NA    NA   \n## 11 reg      Factor         0  7480  NA     NA    south NA      NA    NA   \n## # ... with 3 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, n_unique &lt;int&gt;\n\nAs expected, no more missing values. The “long” argument inside mice::complete() is needed if you want the complete() function to return a long dataset. Doing the above “manually” using {purrr} is possible with the following code:\n\nstart &lt;- Sys.time()\nimp_boys_purrr &lt;- map(rep(1, 10), ~mice(data = boys, m = ., maxit = 100, printFlag = FALSE))\nend &lt;- Sys.time() - start\n\nprint(end)\n## Time difference of 3.393966 mins\n\nWhat this does is map the function ~mice(data = boys, m = ., maxit = 100, printFlag = FALSE) to a list of 1s, and creates 10 imputed data sets. m = . means that m will be equal to whatever is inside the list we are mapping our function over, so 1, then 1 then another 1 etc…. It took around the same amount of time as using mice() directly.\n\n\nimp_boys_purrr is now a list of 10 mids objects. We thus need to map mice::complete() to imp_boys_purrr to get the data:\n\nimp_boys_purrr_complete &lt;- map(imp_boys_purrr, mice::complete)\n\nNow, imp_boys_purrr_complete is a list of 10 datasets. Let’s map brotools::describe() to it:\n\nmap(imp_boys_purrr_complete, brotools::describe)\n## [[1]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.03 14.54 11.8    31.7 15.9    17.4  19.5\n## 3 hc       Numer…   748  51.7   5.90 33.7  33.7    65   48.3    53.1  56  \n## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   84     146.  175. \n## 5 tv       Numer…   748   8.35  8.00 3      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.2  26.0  3.65   3.14  117.  11.7    34.7  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[2]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.03 14.54 11.8    31.7 15.9    17.5  19.5\n## 3 hc       Numer…   748  51.6   5.88 33.7  33.7    65   48.3    53.2  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.5   145.  175  \n## 5 tv       Numer…   748   8.37  8.02 1      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.1  26.0  3.65   3.14  117.  11.9    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P2    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[3]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.1   3.04 14.54 11.8    31.7 15.9    17.5  19.5\n## 3 hc       Numer…   748  51.6   5.87 33.7  33.7    65   48.5    53.3  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.0   145.  175  \n## 5 tv       Numer…   748   8.46  8.14 2      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.2  26.1  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[4]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.1   3.02 14.54 11.8    31.7 15.9    17.5  19.4\n## 3 hc       Numer…   748  51.7   5.93 33.7  33.7    65   48.5    53.4  56  \n## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   82.9   145.  175  \n## 5 tv       Numer…   748   8.45  8.11 2      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.2  26.0  3.65   3.14  117.  11.7    34.7  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[5]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.03 14.54 11.8    31.7 15.9    17.5  19.5\n## 3 hc       Numer…   748  51.6   5.91 33.7  33.7    65   48.3    53.2  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.0   146.  175. \n## 5 tv       Numer…   748   8.21  8.02 3      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.1  26.0  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[6]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.05 14.54 11.8    31.7 15.9    17.4  19.5\n## 3 hc       Numer…   748  51.7   5.89 33.7  33.7    65   48.3    53.2  56  \n## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   83.0   146.  175  \n## 5 tv       Numer…   748   8.44  8.24 3      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.1  26.0  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[7]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.1   3.04 14.54 11.8    31.7 15.9    17.4  19.5\n## 3 hc       Numer…   748  51.6   5.88 33.7  33.7    65   48.2    53.2  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.5   146.  175  \n## 5 tv       Numer…   748   8.47  8.15 2      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.2  26.1  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[8]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.04 14.54 11.8    31.7 15.9    17.4  19.4\n## 3 hc       Numer…   748  51.6   5.85 33.7  33.7    65   48.2    53.3  56  \n## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   83.0   146.  175  \n## 5 tv       Numer…   748   8.36  8.06 2      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.2  26.1  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[9]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.05 14.54 11.8    31.7 15.9    17.4  19.5\n## 3 hc       Numer…   748  51.6   5.90 33.7  33.7    65   48.3    53.2  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.9   146.  175  \n## 5 tv       Numer…   748   8.57  8.25 1      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.1  26.1  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n## \n## [[10]]\n## # A tibble: 9 x 13\n##   variable type    nobs   mean    sd mode     min   max   q25 median   q75\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3\n## 2 bmi      Numer…   748  18.0   3.04 14.54 11.8    31.7 15.9    17.4  19.5\n## 3 hc       Numer…   748  51.6   5.89 33.7  33.7    65   48.3    53.1  56  \n## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.0   146.  175  \n## 5 tv       Numer…   748   8.49  8.18 2      1      25    2       3    15  \n## 6 wgt      Numer…   748  37.1  26.1  3.65   3.14  117.  11.7    34.6  59.6\n## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  \n## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  \n## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  \n## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;\n\nBefore merging this 10 datasets together into one, it would be nice to have a column with the id of the datasets. This can easily be done with a variant of purrr::map(), called map2():\n\nimp_boys_purrr &lt;- map2(.x = seq(1,10), .y = imp_boys_purrr_complete, ~mutate(.y, imp_id = as.character(.x)))\n\nmap2() applies a function, say f(), to 2 lists sequentially: f(x_1, y_1), then f(x_2, y_2), etc… So here I map mutate() to create a new column, imp_id in each dataset. Now let’s bind the rows and take a look at the data:\n\nimp_boys_purrr &lt;- bind_rows(imp_boys_purrr)\n\nimp_boys_purrr %&gt;%\n  brotools::describe() %&gt;%\n  select(variable, type, n_missing, everything())\n## # A tibble: 10 x 13\n##    variable type     n_missing  nobs   mean    sd mode     min   max   q25\n##    &lt;chr&gt;    &lt;chr&gt;        &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 age      Numeric          0  7480   9.16  6.89 0.035  0.035  21.2  1.58\n##  2 bmi      Numeric          0  7480  18.0   3.04 14.54 11.8    31.7 15.9 \n##  3 hc       Numeric          0  7480  51.6   5.89 33.7  33.7    65   48.3 \n##  4 hgt      Numeric          0  7480 131.   46.5  50.1  50     198   83   \n##  5 tv       Numeric          0  7480   8.42  8.11 3      1      25    2   \n##  6 wgt      Numeric          0  7480  37.1  26.0  3.65   3.14  117.  11.7 \n##  7 imp_id   Charact…         0  7480  NA    NA    1     NA      NA   NA   \n##  8 gen      Factor           0  7480  NA    NA    G1    NA      NA   NA   \n##  9 phb      Factor           0  7480  NA    NA    P1    NA      NA   NA   \n## 10 reg      Factor           0  7480  NA    NA    south NA      NA   NA   \n## # ... with 3 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, n_unique &lt;int&gt;\n\nYou may ask yourself why I am bothering with all this. This will become apparent now. We can now use the code we wrote to get our 10 imputed datasets using purrr::map() and simply use furrr::future_map() to parallelize the imputation process:\n\nlibrary(furrr)\n## Loading required package: future\nplan(multiprocess)\n\nstart &lt;- Sys.time()\nimp_boys_future &lt;- future_map(rep(1, 10), ~mice(data = boys, m = ., maxit = 100, printFlag = FALSE))\nend &lt;- Sys.time() - start\n\nprint(end)\n## Time difference of 33.73772 secs\n\nBoooom! Much faster! And simply by loading {furrr}, then using plan(multiprocess) to run the code in parallel (if you forget that, the code will run on a single core) and using future_map() instead of map().\n\n\nLet’s take a look at the data:\n\nimp_boys_future_complete &lt;- map(imp_boys_future, mice::complete)\n\nimp_boys_future &lt;- map2(.x = seq(1,10), .y = imp_boys_future_complete, ~mutate(.y, imp_id = as.character(.x)))\n\nimp_boys_future &lt;- bind_rows(imp_boys_future)\n\nimp_boys_future %&gt;%\n  brotools::describe() %&gt;%\n  select(variable, type, n_missing, everything())\n## # A tibble: 10 x 13\n##    variable type     n_missing  nobs   mean    sd mode     min   max   q25\n##    &lt;chr&gt;    &lt;chr&gt;        &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 age      Numeric          0  7480   9.16  6.89 0.035  0.035  21.2  1.58\n##  2 bmi      Numeric          0  7480  18.0   3.04 14.54 11.8    31.7 15.9 \n##  3 hc       Numeric          0  7480  51.6   5.89 33.7  33.7    65   48.4 \n##  4 hgt      Numeric          0  7480 131.   46.5  50.1  50     198   83   \n##  5 tv       Numeric          0  7480   8.35  8.09 3      1      25    2   \n##  6 wgt      Numeric          0  7480  37.1  26.0  3.65   3.14  117.  11.7 \n##  7 imp_id   Charact…         0  7480  NA    NA    1     NA      NA   NA   \n##  8 gen      Factor           0  7480  NA    NA    G1    NA      NA   NA   \n##  9 phb      Factor           0  7480  NA    NA    P1    NA      NA   NA   \n## 10 reg      Factor           0  7480  NA    NA    south NA      NA   NA   \n## # ... with 3 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, n_unique &lt;int&gt;\n\nSo imputation went from 3.4 minutes (around 200 seconds) to 30 seconds. How cool is that? If you want to play around with {furrr} you must install it from Github, as it is not yet available on CRAN:\n\ndevtools::install_github(\"DavisVaughan/furrr\")\n\nIf you are not comfortable with map() (and thus future_map()) but still want to impute in parallel, there is this very nice script here to do just that. I created a package around this script, called parlMICE (the same name as the script), to make installation and usage easier. You can install it like so:\n\ndevtools::install_github(\"b-rodrigues/parlMICE\")\n\nIf you found this blog post useful, you might want to follow me on twitter for blog post updates."
  }
]