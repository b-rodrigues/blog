<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Econometrics and Free Software</title>
<link>https://b-rodrigues.github.io/</link>
<atom:link href="https://b-rodrigues.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.6.37</generator>
<lastBuildDate>Thu, 31 Jan 2019 00:00:00 GMT</lastBuildDate>
<item>
  <title>Using Data Science to read 10 years of Luxembourguish newspapers from the 19th century</title>
  <link>https://b-rodrigues.github.io/posts/2019-01-31-newspapers_shiny_app.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://brodriguesco.shinyapps.io/newspapers_app/"> <img src="https://b-rodrigues.github.io/assets/img/tf_idf.png" title="Click here to go the app"></a>
</p>
</div>
<p>
I have been playing around with historical newspaper data (see <a href="../posts/2019-01-04-newspapers.html">here</a> and <a href="../posts/2019-01-13-newspapers_mets_alto.html">here</a>). I have extracted the data from the largest archive available, as described in the previous blog post, and now created a shiny dashboard where it is possible to visualize the most common words per article, as well as read a summary of each article. The summary was made using a method called <em>textrank</em>, using the <code>{textrank}</code> package, which extracts relevant sentences using the Pagerank (developed by Google) algorithm. You can read the scientific paper <a href="https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf">here</a> for more info.
</p>
<p>
You can play around with the app by clicking <a href="https://brodriguesco.shinyapps.io/newspapers_app/">here</a>. In the next blog post, I will explain how I created the app, step by step. It‚Äôs going to be a long blog post!
</p>
<p>
Using the app, I noticed that some war happened around November 1860 in China, which turned out to be the <a href="https://en.wikipedia.org/wiki/Second_Opium_War">Second Opium War</a>. The war actually ended in October 1860, but apparently the news took several months to travel to Europe.
</p>
<p>
I also learned that already in the 1861, there was public transportation between some Luxembourguish villages, and French villages that were by the border (see the publication from the 17th of December 1861).
</p>
<p>
Let me know if you find about historical events using my app!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-01-31-newspapers_shiny_app.html</guid>
  <pubDate>Thu, 31 Jan 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Making sense of the METS and ALTO XML standards</title>
  <link>https://b-rodrigues.github.io/posts/2019-01-13-newspapers_mets_alto.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=V1qpvpH26fo"> <img src="https://b-rodrigues.github.io/assets/img/union.png" title="The 19th century was a tough place"></a>
</p>
</div>
<p>
Last week I wrote a <a href="https://www.brodrigues.co/blog/2019-01-04-newspapers/">blog post</a> where I analyzed one year of newspapers ads from 19th century newspapers. The data is made available by the <a href="https://data.bnl.lu/data/historical-newspapers/">national library of Luxembourg</a>. In this blog post, which is part 1 of a 2 part series, I extract data from the 257gb archive, which contains 10 years of publications of the <em>L‚ÄôUnion</em>, another 19th century Luxembourguish newspaper written in French. As I explained in the previous post, to make life easier to data scientists, the national library also included ALTO and METS files (which are a XML files used to describe the layout and contents of physical text sources, such as pages of a book or newspaper) which can be easily parsed by R.
</p>
<p>
This is how a ALTO file looks like:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/alto.png"><!-- -->
</p>
<p>
Each page of the newspaper of a given day has one ALTO file. This is how a METS file looks like:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/mets.png"><!-- -->
</p>
<p>
For each daily issue of the newspaper, there is a METS file. So 1 METS file for 4 ALTO files.
</p>
<p>
In my last blog post, I only extracted the words from the ALTO file (red rectangles of the first screenshot) and did not touch the METS file. The problem of doing this is that I get all the words for each page, without knowing which come from the same article. If I want to know which words come from the same article, I need to use the info from the METS file. From the METS file I have the ID of the article, and some other metadata, such as the title of the article and the type of the article (which can be <em>article</em>, <em>advertisement</em>, etc). The information highlighted with the green rectangles in the METS file can be linked to the green rectangles from the ALTO files. My goal is to get the following data frame from the METS file:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/mets_df.png"><!-- -->
</p>
<p>
and this data frame from the ALTO files:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/alto_df.png"><!-- -->
</p>
<p>
As you can see, by combining both data frames I can know which words come from the same article, which will be helpful for further analysis. <a href="https://en.wikipedia.org/wiki/1860s">A lot of things happened in the 1860s.</a> I am really curious to see if and how these events where reported in a Luxembourguish newspaper. I am particularly curious about how long it took to report certain news from far away, such as the assassination of Abraham Lincoln. But before that I need to extract the data!
</p>
<p>
I will only focus on the METS file. The logic for the ALTO file is the same. All the source code will be in the appendix of this blog post.
</p>
<p>
First, let‚Äôs take a look at a METS file:
</p>
<pre class="r"><code>library(tidyverse)
mets &lt;- read_file("1533660_newspaper_lunion_1860-11-14/1533660_newspaper_lunion_1860-11-14-mets.xml")</code></pre>
<p>
This is how it looks like:
</p>
<pre><code>"&lt;?xml version='1.0' encoding='utf-8'?&gt;\r\n&lt;mets xmlns=\"http://www.loc.gov/METS/\" xmlns:mix=\"http://www.loc.gov/mix/v20\" xmlns:mods=\"http://www.loc.gov/mods/v3\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" LABEL=\"L'UNION. 1860-11-14_01\" OBJID=\"https://persist.lu/ark:/70795/m62fcm\" TYPE=\"Newspaper\" xsi:schemaLocation=\"http://www.loc.gov/METS/ http://www.loc.gov/standards/mets/mets.xsd http://www.loc.gov/mods/v3 http://www.loc.gov/standards/mods/v3/mods-3-6.xsd http://www.loc.gov/mix/v20 http://www.loc.gov/standards/mix/mix.xsd\"&gt;\r\n  &lt;metsHdr CREATEDATE=\"2010-12-03T20:35:05\" LASTMODDATE=\"2018-05-09T05:35:51Z\"&gt;\r\n    &lt;agent OTHERTYPE=\"SOFTWARE\" ROLE=\"CREATOR\" TYPE=\"OTHER\"&gt;\r\n      &lt;name&gt;CCS docWORKS/METAe Version 6.4-3&lt;/name&gt;\r\n      &lt;note&gt;docWORKS-ID: 101636&lt;/note&gt;\r\n    &lt;/agent&gt;\r\n  &lt;/metsHdr&gt;\r\n  &lt;dmdSec ID=\"MODSMD_COLLECTION\"&gt;\r\n    &lt;mdWrap LABEL=\"Bibliographic meta-data of the collection\" MDTYPE=\"MODS\" MIMETYPE=\"text/xml\"&gt;\r\n      &lt;xmlData&gt;\r\n        &lt;mods:mods&gt;\r\n          &lt;mods:identifier type=\"local\"&gt;lunion&lt;/mods:identifier&gt;\r\n          &lt;mods:titleInfo ID=\"MODSMD_COLLECTION_TI1\" xml:lang=\"fr\"&gt;\r\n            &lt;mods:title&gt;L'UNION.&lt;/mods:title&gt;\r\n          &lt;/mods:titleInfo&gt;\r\n        &lt;/mods:mods&gt;\r\n      &lt;/xmlData&gt;\r\n    &lt;/mdWrap&gt;\r\n  &lt;/dmdSec&gt;\r\n  &lt;dmdSec ID=\"MODSMD_SECTION1\"&gt;\r\n    &lt;mdWrap MDTYPE=\"MODS\" MIMETYPE=\"text/xml\"&gt;\r\n      &lt;xmlData&gt;\r\n        &lt;mods:mods&gt;\r\n          &lt;mods:titleInfo ID=\"MODSMD_SECTION1_TI1\" xml:lang=\"fr\"&gt;\r\n            &lt;mods:title&gt;Chemins de fer. ‚Äî Service d'hiver.&lt;/mods:title&gt;\r\n          &lt;/mods:titleInfo&gt;\r\n          &lt;mods:language&gt;\r\n            &lt;mods:languageTerm authority=\"rfc3066\" type=\"code\"&gt;fr&lt;/mods:languageTerm&gt;\r\n ...."</code></pre>
<p>
As usual when you import text files like this, it‚Äôs always a good idea to split the file. I will split at the <code>‚ÄúDMDID‚Äù</code> character. Take a look back at the second screenshot. The very first tag, first row, first word after <code>div</code> is <code>‚ÄúDMDID‚Äù</code>. By splitting at this level, I will get back a list, where each element is the content of this <code>div DMDID</code> block. This is exactly what I need, since this block contains the information from the green rectangles. So let‚Äôs split the <code>mets</code> variable at this level:
</p>
<pre class="r"><code>mets_articles &lt;- mets %&gt;%
    str_split("DMDID") %&gt;%
    flatten_chr()</code></pre>
<p>
Let‚Äôs take a look at <code>mets_articles</code>:
</p>
<pre class="r"><code>str(mets_articles)</code></pre>
<pre><code> chr [1:25] "&lt;?xml version='1.0' encoding='utf-8'?&gt;\r\n&lt;mets xmlns=\"http://www.loc.gov/METS/\" xmlns:mix=\"http://www.loc.g"| __truncated__ ...</code></pre>
<p>
Doesn‚Äôt seem to be very helpful, but actually it is. We can see that <code>mets_articles</code> is a now a list of 25 elements.
</p>
<p>
This means that for each element of <code>mets_articles</code>, I need to get the identifier, the label, the type (the red rectangles from the screenshot), but also the information from the <code>‚ÄúBEGIN‚Äù</code> element (the green rectangle).
</p>
<p>
To do this, I‚Äôll be using regular expressions. In general, I start by experimenting in the console, and then when things start looking good, I write a function. Here is this function:
</p>
<pre class="r"><code>extractor &lt;- function(string, regex, all = FALSE){
    if(all) {
        string %&gt;%
            str_extract_all(regex) %&gt;%
            flatten_chr() %&gt;%
            str_extract_all("[:alnum:]+", simplify = FALSE) %&gt;%
            map(paste, collapse = "_") %&gt;%
            flatten_chr()
    } else {
        string %&gt;%
            str_extract(regex) %&gt;%
            str_extract_all("[:alnum:]+", simplify = TRUE) %&gt;%
            paste(collapse = " ") %&gt;%
            tolower()
    }
}</code></pre>
<p>
This function may seem complicated, but it simply encapsulates some pretty standard steps to get the data I need. I had to consider two cases. The first case is when I need to extract all the elements with <code>str_extract_all()</code>, or only the first occurrence, with <code>str_extract()</code>. Let‚Äôs test it on the first article of the <code>mets_articles</code> list:
</p>
<pre class="r"><code>mets_articles_1 &lt;- mets_articles[1]</code></pre>
<pre class="r"><code>extractor(mets_articles_1, "ID", all = FALSE)</code></pre>
<pre><code>## [1] "id"</code></pre>
<p>
Let‚Äôs see what happens with <code>all = TRUE</code>:
</p>
<pre class="r"><code>extractor(mets_articles_1, "ID", all = TRUE)</code></pre>
<pre><code>##   [1] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [15] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [29] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [43] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [57] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [71] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [85] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [99] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
## [113] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
## [127] "ID" "ID" "ID" "ID" "ID"</code></pre>
<p>
This seems to work as intended. Since I need to call this function several times, I‚Äôll be writing another function that extracts all I need:
</p>
<pre class="r"><code>extract_mets &lt;- function(article){

    id &lt;- article %&gt;%
        extractor("(?&lt;=ID)(.*?)(?=LABEL)")

    label &lt;- article %&gt;%
        extractor("(?&lt;=LABEL)(.*?)(?=TYPE)")

    type &lt;- article %&gt;%
        extractor("(?&lt;=TYPE)(.*?)(?=&gt;)")

    begins &lt;- article %&gt;%
        extractor("(?&lt;=BEGIN)(.*?)(?=BETYPE)", all = TRUE)

    tibble::tribble(~label, ~type, ~begins, ~id,
                    label, type, begins, id) %&gt;%
        unnest()
}</code></pre>
<p>
This function uses complex regular expressions to extract the strings I need, and then puts the result into a data frame, with the <code>tibble()</code> function. I then use <code>unnest()</code>, because <code>label</code>, <code>type</code>, <code>begins</code> and <code>id</code> are not the same length. <code>label</code>, <code>type</code> and <code>id</code> are of length 1, while <code>begins</code> is longer. This means that when I put them into a data frame it looks like this:
</p>
<pre class="r"><code>tribble(~a, ~b,
"a", rep("b", 4))</code></pre>
<pre><code>## # A tibble: 1 x 2
##   a     b        
##   &lt;chr&gt; &lt;list&gt;   
## 1 a     &lt;chr [4]&gt;</code></pre>
<p>
With <code>unnest()</code>, I get a nice data frame:
</p>
<pre class="r"><code>tribble(~a, ~b,
"a", rep("b", 4)) %&gt;% 
  unnest()</code></pre>
<pre><code>## # A tibble: 4 x 2
##   a     b    
##   &lt;chr&gt; &lt;chr&gt;
## 1 a     b    
## 2 a     b    
## 3 a     b    
## 4 a     b</code></pre>
<p>
Now, I simply need to map this function to all the files and that‚Äôs it! For this, I will write yet another helper function:
</p>
<pre class="r"><code>mets_csv &lt;- function(page_path){
    
    page &lt;- read_file(page_path)
    
    doc_name &lt;- str_extract(page_path, "(?&lt;=/).*")
    
    mets_articles &lt;- page %&gt;%
        str_split("DMDID") %&gt;%
        flatten_chr()
    
    mets_df &lt;- map_df(mets_articles, extract_mets)
    
    mets_df &lt;- mets_df %&gt;%
        mutate(document = doc_name)
    
    write_csv(mets_df, paste0(page_path, ".csv"))
}</code></pre>
<p>
This function takes the path to a METS file as input, and processes it using the steps I explained above. The only difference is that I add a column containing the name of the file that was processed, and write the resulting data frame directly to disk as a data frame. Finally, I can map this function to all the METS files:
</p>
<pre class="r"><code># Extract content from METS files

pages_mets &lt;- str_match(list.files(path = "./", all.files = TRUE, recursive = TRUE), ".*mets.xml") %&gt;%
    discard(is.na)

library(furrr)

plan(multiprocess, workers = 8)

tic &lt;- Sys.time()
future_map(pages_mets, mets_csv)
toc &lt;- Sys.time()

toc - tic</code></pre>
<p>
I use <code>{furrr}</code> to extract the data from all the files in parallel, by putting 8 cores of my CPU to work. This took around 3 minutes and 20 seconds to finish.
</p>
<p>
That‚Äôs it for now, stay tuned for part 2 where I will analyze this fresh data!
</p>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">
Appendix
</h2>
<pre class="r"><code>extract_alto &lt;- function(article){
    begins &lt;- article[1] %&gt;%
        extractor("(?&lt;=^ID)(.*?)(?=HPOS)", all = TRUE)

    content &lt;- article %&gt;%
        extractor("(?&lt;=CONTENT)(.*?)(?=WC)", all = TRUE)

    tibble::tribble(~begins, ~content,
                    begins, content) %&gt;%
        unnest()
}

alto_csv &lt;- function(page_path){

    page &lt;- read_file(page_path)

    doc_name &lt;- str_extract(page_path, "(?&lt;=/text/).*")

    alto_articles &lt;- page %&gt;%
        str_split("TextBlock ") %&gt;%
        flatten_chr()

    alto_df &lt;- map_df(alto_articles, extract_alto)

    alto_df &lt;- alto_df %&gt;%
        mutate(document = doc_name)

    write_csv(alto_df, paste0(page_path, ".csv"))
}


alto &lt;- read_file("1533660_newspaper_lunion_1860-11-14/text/1860-11-14_01-00001.xml")


# Extract content from alto files

pages_alto &lt;- str_match(list.files(path = "./", all.files = TRUE, recursive = TRUE), ".*/text/.*.xml") %&gt;%
    discard(is.na)


library(furrr)

plan(multiprocess, workers = 8)

tic &lt;- Sys.time()
future_map(pages_alto, alto_csv)
toc &lt;- Sys.time()

toc - tic

#Time difference of 18.64776 mins</code></pre>
</section>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-01-13-newspapers_mets_alto.html</guid>
  <pubDate>Sun, 13 Jan 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Looking into 19th century ads from a Luxembourguish newspaper with R</title>
  <link>https://b-rodrigues.github.io/posts/2019-01-04-newspapers.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=0xzN6FM5x_E"> <img src="https://b-rodrigues.github.io/assets/img/Wales.jpg" title="Sometimes ads are better than this. Especially if it's Flex Tape ¬Æ ads."></a>
</p>
</div>
<p>
The <a href="https://data.bnl.lu/data/historical-newspapers/">national library of Luxembourg</a> published some very interesting data sets; scans of historical newspapers! There are several data sets that you can download, from 250mb up to 257gb. I decided to take a look at the 32gb ‚ÄúML Starter Pack‚Äù. It contains high quality scans of one year of the <em>L‚Äôind√©pendence Luxembourgeoise</em> (Luxembourguish independence) from the year 1877. To make life easier to data scientists, the national library also included ALTO and METS files (which is a XML schema that is used to describe the layout and contents of physical text sources, such as pages of a book or newspaper) which can be easily parsed by R.
</p>
<p>
<em>L‚Äôind√©pendence Luxembourgeoise</em> is quite interesting in that it is a Luxembourguish newspaper written in French. Luxembourg always had 3 languages that were used in different situations, French, German and Luxembourguish. Luxembourguish is the language people used (and still use) for day to day life and to speak to their baker. Historically however, it was not used for the press or in politics. Instead it was German that was used for the press (or so I thought) and French in politics (only in <a href="http://legilux.public.lu/eli/etat/leg/loi/1984/02/24/n1/jo">1984</a> was Luxembourguish made an official Language of Luxembourg). It turns out however that <em>L‚Äôind√©pendence Luxembourgeoise</em>, a daily newspaper that does not exist anymore, was in French. This piqued my interest, and it also made analysis easier, for 2 reasons: I first started with the <em>Luxemburger Wort</em> (Luxembourg‚Äôs Word I guess would be a translation), which still exists today, but which is in German. And at that time, German was written using the Fraktur font, which makes it barely readable. Look at the alphabet in Fraktur:
</p>
<pre><code>ùï¨ ùï≠ ùïÆ ùïØ ùï∞ ùï± ùï≤ ùï≥ ùï¥ ùïµ ùï∂ ùï∑ ùï∏ ùïπ ùï∫ ùïª ùïº ùïΩ ùïæ ùïø ùñÄ ùñÅ ùñÇ ùñÉ ùñÑ ùñÖ
ùñÜ ùñá ùñà ùñâ ùñä ùñã ùñå ùñç ùñé ùñè ùñê ùñë ùñí ùñì ùñî ùñï ùññ ùñó ùñò ùñô ùñö ùñõ ùñú ùñù ùñû ùñü</code></pre>
<p>
It‚Äôs not like German is already hard enough, they had to invent the least readable font ever to write German in, to make extra sure it would be hell to decipher.
</p>
<p>
So basically I couldn‚Äôt be bothered to try to read a German newspaper in Fraktur. That‚Äôs when I noticed the <em>L‚Äôind√©pendence Luxembourgeoise</em>‚Ä¶ A Luxembourguish newspaper? Written in French? Sounds interesting.
</p>
<p>
And oh boy. Interesting it was.
</p>
<p>
19th century newspapers articles were something else. There‚Äôs this article for instance:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/pray for senators.png"><!-- -->
</p>
<p>
For those of you that do not read French, this article relates that in France, the ministry of justice required priests to include prayers on the Sunday that follows the start of the new season of parliamentary discussions, in order for God to provide senators his help.
</p>
<p>
There this gem too:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/tallest_soldier.jpg"><!-- -->
</p>
<p>
This article presents the tallest soldier of the German army, called Emhke, and nominated by the German Emperor himself to accompany him during his visit to Palestine. Emhke was 2.08 meters tall and weighted 236 pounds (apparently at the time Luxembourg was not fully sold on the metric system).
</p>
<p>
Anyway, I decided to take a look at ads. The last paper of this 4 page newspaper always contained ads and other announcements. For example, there‚Äôs this ad for a pharmacy:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/pharmacy.png"><!-- -->
</p>
<p>
that sells tea, and mineral water. Yes, tea and mineral water. In a pharmacy. Or this one:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/upside_down.png"><!-- -->
</p>
<p>
which is literally upside down in the newspaper (the one from the 10th of April 1877). I don‚Äôt know if it‚Äôs a mistake or if it‚Äôs a marketing ploy, but it did catch my attention, 140 years later, so <em>bravo</em>. This is an announcement made by a shop owner that wants to sell all his merchandise for cheap, perhaps to make space for new stuff coming in?
</p>
<p>
So I decided brush up on my natural language processing skills with R and do topic modeling on these ads. The challenge here is that a single document, the 4th page of the newspaper, contains a lot of ads. So it will probably be difficult to clearly isolate topics. But let‚Äôs try nonetheless. First of all, let‚Äôs load all the <code>.xml</code> files that contain the data. These files look like this:
</p>
<pre><code>&lt;TextLine ID="LINE6" STYLEREFS="TS11" HEIGHT="42" WIDTH="449" HPOS="165" VPOS="493"&gt;
                                    &lt;String ID="S16" CONTENT="l‚Äôapr√®s-midi," WC="0.638" CC="0803367024653" HEIGHT="42" WIDTH="208" HPOS="165" VPOS="493"/&gt;
                                    &lt;SP ID="SP11" WIDTH="24" HPOS="373" VPOS="493"/&gt;
                                    &lt;String ID="S17" CONTENT="le" WC="0.8" CC="40" HEIGHT="30" WIDTH="29" HPOS="397" VPOS="497"/&gt;
                                    &lt;SP ID="SP12" WIDTH="14" HPOS="426" VPOS="497"/&gt;
                                    &lt;String ID="S18" CONTENT="Gouverne" WC="0.638" CC="72370460" HEIGHT="31" WIDTH="161" HPOS="440" VPOS="496" SUBS_TYPE="HypPart1" SUBS_CONTENT="Gouvernement"/&gt;
                                    &lt;HYP CONTENT="-" WIDTH="11" HPOS="603" VPOS="514"/&gt;
                                  &lt;/TextLine&gt;
                        &lt;TextLine ID="LINE7" STYLEREFS="TS11" HEIGHT="41" WIDTH="449" HPOS="166" VPOS="541"&gt;
                                    &lt;String ID="S19" CONTENT="ment" WC="0.725" CC="0074" HEIGHT="26" WIDTH="81" HPOS="166" VPOS="545" SUBS_TYPE="HypPart2" SUBS_CONTENT="Gouvernement"/&gt;
                                    &lt;SP ID="SP13" WIDTH="24" HPOS="247" VPOS="545"/&gt;
                                    &lt;String ID="S20" CONTENT="Royal" WC="0.62" CC="74503" HEIGHT="41" WIDTH="100" HPOS="271" VPOS="541"/&gt;
                                    &lt;SP ID="SP14" WIDTH="26" HPOS="371" VPOS="541"/&gt;
                                    &lt;String ID="S21" CONTENT="Grand-Ducal" WC="0.682" CC="75260334005" HEIGHT="32" WIDTH="218" HPOS="397" VPOS="541"/&gt;
                                  &lt;/TextLine&gt;</code></pre>
<p>
I‚Äôm interested in the ‚ÄúCONTENT‚Äù tag, which contains the words. Let‚Äôs first get that into R.
</p>
<p>
Load the packages, and the files:
</p>
<pre class="r"><code>library(tidyverse)
library(tidytext)
library(topicmodels)
library(brotools)

ad_pages &lt;- str_match(list.files(path = "./", all.files = TRUE, recursive = TRUE), ".*4-alto.xml") %&gt;%
    discard(is.na)</code></pre>
<p>
I save the path of all the pages at once into the <code>ad_pages</code> variables. To understand how and why this works, you must take a look at the hierarchy of the folder:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/layout.png"><!-- -->
</p>
<p>
Inside each of these folder, there is a <code>text</code> folder, and inside this folder there are the <code>.xml</code> files. Because this structure is bit complex, I use the <code>list.files()</code> function with the <code>all.files</code> and <code>recursive</code> argument set to <code>TRUE</code> which allow me to dig deep into the folder structure and list every single file. I am only interested into the 4th page though, so that‚Äôs why I use <code>str_match()</code> to only keep the 4th page using the <code>‚Äú.*4-alto.xml‚Äù</code> regular expression. This is the right regular expression, because the files are named like so:
</p>
<pre><code>1877-12-29_01-00004-alto.xml</code></pre>
<p>
So in the end, <code>ad_pages</code> is a list of all the paths to these files. I then write a function to extract the contents of the ‚ÄúCONTENT‚Äù tag. Here is the function.
</p>
<pre class="r"><code>get_words &lt;- function(page_path){
    
    page &lt;- read_file(page_path)
    
    page_name &lt;- str_extract(page_path, "1.*(?=-0000)") 
    
    page %&gt;%  
        str_split("\n", simplify = TRUE) %&gt;% 
        keep(str_detect(., "CONTENT")) %&gt;% 
        str_extract("(?&lt;=CONTENT)(.*?)(?=WC)") %&gt;% 
        discard(is.na) %&gt;% 
        str_extract("[:alpha:]+") %&gt;% 
        tolower %&gt;% 
        as_tibble %&gt;% 
        rename(tokens = value) %&gt;% 
        mutate(page = page_name)
}</code></pre>
<p>
This function takes the path to a page as argument, and returns a tibble with the two columns: one containing the words, which I called <code>tokens</code> and the second the name of the document this word was found. I uploaded on <code>.xml</code> file <a href="https://gist.github.com/b-rodrigues/a22d2aa63dff01d88acc2916c003489d">here</a> so that you can try the function yourself. The difficult part is <code>str_extract(‚Äú(?&lt;=CONTENT)(.*?)(?=WC)‚Äú)</code> which is were the words inside the ‚ÄúCONTENT‚Äù tag get extracted.
</p>
<p>
I then map this function to all the pages, and get a nice tibble with all the words:
</p>
<pre class="r"><code>ad_words &lt;- map_dfr(ad_pages, get_words)</code></pre>
<pre class="r"><code>ad_words</code></pre>
<pre><code>## # A tibble: 1,114,662 x 2
##    tokens     page                            
##    &lt;chr&gt;      &lt;chr&gt;                           
##  1 afin       1877-01-05_01/text/1877-01-05_01
##  2 de         1877-01-05_01/text/1877-01-05_01
##  3 mettre     1877-01-05_01/text/1877-01-05_01
##  4 mes        1877-01-05_01/text/1877-01-05_01
##  5 honorables 1877-01-05_01/text/1877-01-05_01
##  6 clients    1877-01-05_01/text/1877-01-05_01
##  7 √†          1877-01-05_01/text/1877-01-05_01
##  8 m√™me       1877-01-05_01/text/1877-01-05_01
##  9 d          1877-01-05_01/text/1877-01-05_01
## 10 avantages  1877-01-05_01/text/1877-01-05_01
## # ‚Ä¶ with 1,114,652 more rows</code></pre>
<p>
I then do some further cleaning, removing stop words (French and German, because there are some ads in German) and a bunch of garbage characters and words, which are probably when the OCR failed. I also remove some German words from the few German ads that are in the paper, because they have a very high tf-idf (I‚Äôll explain below what that is). I also remove very common words in ads that were just like stopwords. Every ad of a shop mentioned their clients with <em>honorable client√®le</em>, or used the word <em>vente</em>, and so on. This is what you see below in the very long calls to <code>str_remove_all</code>. I also compute the <code>tf_idf</code> and I am grateful to ThinkR blog post on that, which you can read <a href="https://thinkr.fr/text-mining-et-topic-modeling-avec-r/">here</a>. It‚Äôs in French though, but the idea of the blog post is to present topic modeling with Wikipedia articles. You can also read the section on tf-idf from the Text Mining with R ebook, <a href="https://www.tidytextmining.com/tfidf.html">here</a>. tf-idf gives a measure of how common words are. Very common words, like stopwords, have a tf-idf of 0. So I use this to further remove very common words, by only keeping words with a tf-idf greater than 0.01. This is why I manually remove garbage words and German words below, because they are so uncommon that they have a very high tf-idf and mess up the rest of the analysis. To find these words I had to go back and forth between the tibble of cleaned words and my code, and manually add all these exceptions. It took some time, but definitely made the results of the next steps better.<br> I then use <code>cast_dtm</code> to cast the tibble into a DocumentTermMatrix object, which is needed for the <code>LDA()</code> function that does the topic modeling:
</p>
<pre class="r"><code>stopwords_fr &lt;- read_csv("https://raw.githubusercontent.com/stopwords-iso/stopwords-fr/master/stopwords-fr.txt",
                         col_names = FALSE)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   X1 = col_character()
## )</code></pre>
<pre class="r"><code>stopwords_de &lt;- read_csv("https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/stopwords-de.txt",
                         col_names = FALSE)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   X1 = col_character()
## )</code></pre>
<pre><code>## Warning: 1 parsing failure.
## row col  expected    actual                                                                                   file
## 157  -- 1 columns 2 columns 'https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/stopwords-de.txt'</code></pre>
<pre class="r"><code>ad_words2 &lt;- ad_words %&gt;% 
    filter(!is.na(tokens)) %&gt;% 
    mutate(tokens = str_remove_all(tokens, 
                                   '[|\\|!|"|#|$|%|&amp;|\\*|+|,|-|.|/|:|;|&lt;|=|&gt;|?|@|^|_|`|‚Äô|\'|‚Äò|(|)|\\||~|=|]|¬∞|&lt;|&gt;|¬´|¬ª|\\d{1,100}|¬©|¬Æ|‚Ä¢|‚Äî|‚Äû|‚Äú|-|¬¶\\\\|‚Äù')) %&gt;%
    mutate(tokens = str_remove_all(tokens,
                                   "j'|j‚Äô|m‚Äô|m'|n‚Äô|n'|c‚Äô|c'|qu‚Äô|qu'|s‚Äô|s'|t‚Äô|t'|l‚Äô|l'|d‚Äô|d'|luxembourg|honneur|rue|prix|maison|frs|ber|adresser|unb|mois|vente|informer|sann|neben|rbudj|artringen|salz|eingetragen|ort|ftofjenb|groifdjen|ort|boch|chem|jahrgang|uoa|genannt|neuwahl|wechsel|sittroe|yerlorenkost|beichsmark|tttr|slpril|ofto|rbudj|felben|acferft√ºcf|etr|eft|sbege|incl|estce|bes|franzosengrund|qne|nne|mme|qni|faire|id|kil")) %&gt;%
    anti_join(stopwords_de, by = c("tokens" = "X1")) %&gt;% 
    filter(!str_detect(tokens, "¬ß")) %&gt;% 
    mutate(tokens = ifelse(tokens == "in√©dite", "in√©dit", tokens)) %&gt;% 
    filter(tokens != "") %&gt;% 
    anti_join(stopwords_fr, by = c("tokens" = "X1")) %&gt;% 
    count(page, tokens) %&gt;% 
    bind_tf_idf(tokens, page, n) %&gt;% 
    arrange(desc(tf_idf))

dtm_long &lt;- ad_words2 %&gt;% 
    filter(tf_idf &gt; 0.01) %&gt;% 
    cast_dtm(page, tokens, n)</code></pre>
<p>
To read more details on this, I suggest you take a look at the following section of the Text Mining with R ebook: <a href="https://www.tidytextmining.com/topicmodeling.html#latent-dirichlet-allocation">Latent Dirichlet Allocation</a>.
</p>
<p>
I choose to model 10 topics (<code>k = 10</code>), and set the <code>alpha</code> parameter to 5. This hyperparamater controls how many topics are present in one document. Since my ads are all in one page (one document), I increased it. Let‚Äôs fit the model, and plot the results:
</p>
<pre class="r"><code>lda_model_long &lt;- LDA(dtm_long, k = 10, control = list(alpha = 5))</code></pre>
<p>
I plot the per-topic-per-word probabilities, the ‚Äúbeta‚Äù from the model and plot the 5 words that contribute the most to each topic:
</p>
<pre class="r"><code>result &lt;- tidy(lda_model_long, "beta")

result %&gt;%
    group_by(topic) %&gt;%
    top_n(5, beta) %&gt;%
    ungroup() %&gt;%
    arrange(topic, -beta) %&gt;% 
    mutate(term = reorder(term, beta)) %&gt;%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    coord_flip() +
    theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/newspapers-13-1.png" width="672">
</p>
<p>
So some topics seem clear to me, other not at all. For example topic 4 seems to be about shoes made out of leather. The word <code>semelle</code>, sole, also appears. Then there‚Äôs a lot of topics that reference either music, bals, or instruments. I guess these are ads for local music festivals, or similar events. There‚Äôs also an ad for what seems to be bundles of sticks, topic 3: <code>ch√™ne</code> is oak, <code>copeaux</code> is shavings and you know what <code>fagots</code> is. The first word <code>st√®re</code> which I did not know is a unit of volume equal to one cubic meter (see <a href="https://en.wikipedia.org/wiki/Stere">Wikipedia</a>). So they were likely selling bundle of oak sticks by the cubic meter. For the other topics, I either lack context or perhaps I just need to adjust <code>k</code>, the number of topics to model, and <code>alpha</code> to get better results. In the meantime, topic 1 is about shoes (<code>chaussures</code>), theatre, fuel (<code>combustible</code>) and farts (<code>pet</code>). Really wonder what they were selling in that shop.
</p>
<p>
In any case, this was quite an interesting project. I learned a lot about topic modeling and historical newspapers of my country! I do not know if I will continue exploring it myself, but I am really curious to see what others will do with it!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-01-04-newspapers.html</guid>
  <pubDate>Fri, 04 Jan 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>R or Python? Why not both? Using Anaconda Python within R with {reticulate}</title>
  <link>https://b-rodrigues.github.io/posts/2018-12-30-reticulate.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://youtu.be/I8vaCrVIR-Q?t=1h2m26s"> <img src="https://b-rodrigues.github.io/assets/img/why not both.png" title="This literally starts playing when you run both R and Python in the same session"></a>
</p>
</div>
<p>
This short blog post illustrates how easy it is to use R and Python in the same R Notebook thanks to the <code>{reticulate}</code> package. For this to work, you might need to upgrade RStudio to the <a href="https://www.rstudio.com/products/rstudio/download/preview/">current preview version</a>. Let‚Äôs start by importing <code>{reticulate}</code>:
</p>
<pre class="r"><code>library(reticulate)</code></pre>
<p>
<code>{reticulate}</code> is an RStudio package that provides ‚Äú<em>a comprehensive set of tools for interoperability between Python and R</em>‚Äù. With it, it is possible to call Python and use Python libraries within an R session, or define Python chunks in R markdown. I think that using R Notebooks is the best way to work with Python and R; when you want to use Python, you simply use a Python chunk:
</p>
<pre><code>```{python}
your python code here
```</code></pre>
<p>
There‚Äôs even autocompletion for Python object methods:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/autocompletion.png"><!-- -->
</p>
<p>
Fantastic!
</p>
<p>
However, if you wish to use Python interactively within your R session, you must start the Python REPL with the <code>repl_python()</code> function, which starts a Python REPL. You can then do whatever you want, even access objects from your R session, and then when you exit the REPL, any object you created in Python remains accessible in R. I think that using Python this way is a bit more involved and would advise using R Notebooks if you need to use both languages.
</p>
<p>
I installed the Anaconda Python distribution to have Python on my system. To use it with <code>{reticulate}</code> I must first use the <code>use_python()</code> function that allows me to set which version of Python I want to use:
</p>
<pre class="r"><code># This is an R chunk
use_python("~/miniconda3/bin/python")</code></pre>
<p>
I can now load a dataset, still using R:
</p>
<pre class="r"><code># This is an R chunk
data(mtcars)
head(mtcars)</code></pre>
<pre><code>##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1</code></pre>
<p>
and now, to access the <code>mtcars</code> data frame, I simply use the <code>r</code> object:
</p>
<pre class="python"><code># This is a Python chunk
print(r.mtcars.describe())</code></pre>
<pre><code>##              mpg        cyl        disp   ...            am       gear     carb
## count  32.000000  32.000000   32.000000   ...     32.000000  32.000000  32.0000
## mean   20.090625   6.187500  230.721875   ...      0.406250   3.687500   2.8125
## std     6.026948   1.785922  123.938694   ...      0.498991   0.737804   1.6152
## min    10.400000   4.000000   71.100000   ...      0.000000   3.000000   1.0000
## 25%    15.425000   4.000000  120.825000   ...      0.000000   3.000000   2.0000
## 50%    19.200000   6.000000  196.300000   ...      0.000000   4.000000   2.0000
## 75%    22.800000   8.000000  326.000000   ...      1.000000   4.000000   4.0000
## max    33.900000   8.000000  472.000000   ...      1.000000   5.000000   8.0000
## 
## [8 rows x 11 columns]</code></pre>
<p>
<code>.describe()</code> is a Python Pandas DataFrame method to get summary statistics of our data. This means that <code>mtcars</code> was automatically converted from a <code>tibble</code> object to a Pandas DataFrame! Let‚Äôs check its type:
</p>
<pre class="python"><code># This is a Python chunk
print(type(r.mtcars))</code></pre>
<pre><code>## &lt;class 'pandas.core.frame.DataFrame'&gt;</code></pre>
<p>
Let‚Äôs save the summary statistics in a variable:
</p>
<pre class="python"><code># This is a Python chunk
summary_mtcars = r.mtcars.describe()</code></pre>
<p>
Let‚Äôs access this from R, by using the <code>py</code> object:
</p>
<pre class="r"><code># This is an R chunk
class(py$summary_mtcars)</code></pre>
<pre><code>## [1] "data.frame"</code></pre>
<p>
Let‚Äôs try something more complex. Let‚Äôs first fit a linear model in Python, and see how R sees it:
</p>
<pre class="python"><code># This is a Python chunk
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
model = smf.ols('mpg ~ hp', data = r.mtcars).fit()
print(model.summary())</code></pre>
<pre><code>##                             OLS Regression Results                            
## ==============================================================================
## Dep. Variable:                    mpg   R-squared:                       0.602
## Model:                            OLS   Adj. R-squared:                  0.589
## Method:                 Least Squares   F-statistic:                     45.46
## Date:                Sun, 10 Feb 2019   Prob (F-statistic):           1.79e-07
## Time:                        00:25:51   Log-Likelihood:                -87.619
## No. Observations:                  32   AIC:                             179.2
## Df Residuals:                      30   BIC:                             182.2
## Df Model:                           1                                         
## Covariance Type:            nonrobust                                         
## ==============================================================================
##                  coef    std err          t      P&gt;|t|      [0.025      0.975]
## ------------------------------------------------------------------------------
## Intercept     30.0989      1.634     18.421      0.000      26.762      33.436
## hp            -0.0682      0.010     -6.742      0.000      -0.089      -0.048
## ==============================================================================
## Omnibus:                        3.692   Durbin-Watson:                   1.134
## Prob(Omnibus):                  0.158   Jarque-Bera (JB):                2.984
## Skew:                           0.747   Prob(JB):                        0.225
## Kurtosis:                       2.935   Cond. No.                         386.
## ==============================================================================
## 
## Warnings:
## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
<p>
Just for fun, I ran the linear regression with the Scikit-learn library too:
</p>
<pre class="python"><code># This is a Python chunk
import numpy as np
from sklearn.linear_model import LinearRegression  
regressor = LinearRegression()  
x = r.mtcars[["hp"]]
y = r.mtcars[["mpg"]]
model_scikit = regressor.fit(x, y)
print(model_scikit.intercept_)</code></pre>
<pre><code>## [30.09886054]</code></pre>
<pre class="python"><code>print(model_scikit.coef_)</code></pre>
<pre><code>## [[-0.06822828]]</code></pre>
<p>
Let‚Äôs access the <code>model</code> variable in R and see what type of object it is in R:
</p>
<pre class="r"><code># This is an R chunk
model_r &lt;- py$model
class(model_r)</code></pre>
<pre><code>## [1] "statsmodels.regression.linear_model.RegressionResultsWrapper"
## [2] "statsmodels.base.wrapper.ResultsWrapper"                     
## [3] "python.builtin.object"</code></pre>
<p>
So because this is a custom Python object, it does not get converted into the equivalent R object. This is described <a href="https://rstudio.github.io/reticulate/index.html">here</a>. However, you can still use Python methods from within an R chunk!
</p>
<pre class="r"><code># This is an R chunk
model_r$aic</code></pre>
<pre><code>## [1] 179.2386</code></pre>
<pre class="r"><code>model_r$params</code></pre>
<pre><code>##   Intercept          hp 
## 30.09886054 -0.06822828</code></pre>
<p>
I must say that I am very impressed with the <code>{reticulate}</code> package. I think that even if you are primarily a Python user, this is still very interesting to know in case you need a specific function from an R package. Just write all your script inside a Python Markdown chunk and then use the R function you need from an R chunk! Of course there is also a way to use R from Python, a Python library called <code>rpy2</code> but I am not very familiar with it. From what I read, it seems to be also quite simple to use.
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-12-30-reticulate.html</guid>
  <pubDate>Sun, 30 Dec 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Some fun with {gganimate}</title>
  <link>https://b-rodrigues.github.io/posts/2018-12-27-fun_gganimate.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/wiid_gganimate.webm" type="video/webm">
Your browser does not support the video tag. </video>
</div>
<p>
In this short blog post I show you how you can use the <code>{gganimate}</code> package to create animations from <code>{ggplot2}</code> graphs with data from UNU-WIDER.
</p>
<section id="wiid-data" class="level2">
<h2 class="anchored" data-anchor-id="wiid-data">
WIID data
</h2>
<p>
Just before Christmas, UNU-WIDER released a new edition of their World Income Inequality Database:
</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
<em>NEW <a href="https://twitter.com/hashtag/DATA?src=hash&amp;ref_src=twsrc%5Etfw">#DATA</a></em><br>We‚Äôve just released a new version of the World Income Inequality Database.<br>WIID4 includes <a href="https://twitter.com/hashtag/data?src=hash&amp;ref_src=twsrc%5Etfw">#data</a> from 7 new countries, now totalling 189, and reaches the year 2017. All data is freely available for download on our website: <a href="https://t.co/XFxuLvyKTC">https://t.co/XFxuLvyKTC</a> <a href="https://t.co/rCf9eXN8D5">pic.twitter.com/rCf9eXN8D5</a>
</p>
‚Äî UNU-WIDER (<span class="citation" data-cites="UNUWIDER">@UNUWIDER</span>) <a href="https://twitter.com/UNUWIDER/status/1076001879556005888?ref_src=twsrc%5Etfw">December 21, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>
The data is available in Excel and STATA formats, and I thought it was a great opportunity to release it as an R package. You can install it with:
</p>
<pre class="r"><code>devtools::install_github("b-rodrigues/wiid4")</code></pre>
<p>
Here a short description of the data, taken from UNU-WIDER‚Äôs website:
</p>
<p>
<em>‚ÄúThe World Income Inequality Database (WIID) presents information on income inequality for developed, developing, and transition countries. It provides the most comprehensive set of income inequality statistics available and can be downloaded for free.</em>
</p>
<p>
<em>WIID4, released in December 2018, covers 189 countries (including historical entities), with over 11,000 data points in total. With the current version, the latest observations now reach the year 2017.‚Äù</em>
</p>
<p>
It was also a good opportunity to play around with the <code>{gganimate}</code> package. This package makes it possible to create animations and is an extension to <code>{ggplot2}</code>. Read more about it <a href="https://github.com/thomasp85/gganimate">here</a>.
</p>
</section>
<section id="preparing-the-data" class="level2">
<h2 class="anchored" data-anchor-id="preparing-the-data">
Preparing the data
</h2>
<p>
To create a smooth animation, I need to have a cylindrical panel data set; meaning that for each country in the data set, there are no missing years. I also chose to focus on certain variables only; net income, all the population of the country (instead of just focusing on the economically active for instance) as well as all the country itself (and not just the rural areas). On <a href="https://www.wider.unu.edu/sites/default/files/WIID/PDF/WIID4%20User%20Guide.pdf">this link</a> you can find a codebook (pdf warning), so you can understand the filters I defined below better.
</p>
<p>
Let‚Äôs first load the packages, data and perform the necessary transformations:
</p>
<pre class="r"><code>library(wiid4)
library(tidyverse)
library(ggrepel)
library(gganimate)
library(brotools)

small_wiid4 &lt;- wiid4 %&gt;%
    mutate(eu = as.character(eu)) %&gt;%
    mutate(eu = case_when(eu == "1" ~ "EU member state",
                          eu == "0" ~ "Non-EU member state")) %&gt;%
    filter(resource == 1, popcovr == 1, areacovr == 1, scale == 2) %&gt;%
    group_by(country) %&gt;%
    group_by(country, year) %&gt;%
    filter(quality_score == max(quality_score)) %&gt;%
    filter(source == min(source)) %&gt;%
    filter(!is.na(bottom5)) %&gt;%
    group_by(country) %&gt;%
    mutate(flag = ifelse(all(seq(2004, 2016) %in% year), 1, 0)) %&gt;%
    filter(flag == 1, year &gt; 2003) %&gt;%
    mutate(year = lubridate::ymd(paste0(year, "-01-01")))</code></pre>
<p>
For some country and some years, there are several sources of data with varying quality. I only keep the highest quality sources with:
</p>
<pre class="r"><code>    group_by(country, year) %&gt;%
    filter(quality_score == max(quality_score)) %&gt;%</code></pre>
<p>
If there are different sources of equal quality, I give priority to the sources that are the most comparable across country (Luxembourg Income Study, LIS data) to less comparable sources with (at least that‚Äôs my understanding of the <code>source</code> variable):
</p>
<pre class="r"><code>    filter(source == min(source)) %&gt;%</code></pre>
<p>
I then remove missing data with:
</p>
<pre class="r"><code>    filter(!is.na(bottom5)) %&gt;%</code></pre>
<p>
<code>bottom5</code> and <code>top5</code> give the share of income that is controlled by the bottom 5% and top 5% respectively. These are the variables that I want to plot.
</p>
<p>
Finally I keep the years 2004 to 2016, without any interruption with the following line:
</p>
<pre class="r"><code>    mutate(flag = ifelse(all(seq(2004, 2016) %in% year), 1, 0)) %&gt;%
    filter(flag == 1, year &gt; 2003) %&gt;%</code></pre>
<p>
<code>ifelse(all(seq(2004, 2016) %in% year), 1, 0))</code> creates a flag that equals <code>1</code> only if the years 2004 to 2016 are present in the data without any interruption. Then I only keep the data from 2004 on and only where the flag variable equals 1.
</p>
<p>
In the end, I ended up only with European countries. It would have been interesting to have countries from other continents, but apparently only European countries provide data in an annual basis.
</p>
</section>
<section id="creating-the-animation" class="level2">
<h2 class="anchored" data-anchor-id="creating-the-animation">
Creating the animation
</h2>
<p>
To create the animation I first started by creating a static ggplot showing what I wanted; a scatter plot of the income by bottom and top 5%. The size of the bubbles should be proportional to the GDP of the country (another variable provided in the data). Once the plot looked how I wanted I added the lines that are specific to <code>{gganimate}</code>:
</p>
<pre class="r"><code>    labs(title = 'Year: {frame_time}', x = 'Top 5', y = 'Bottom 5') +
    transition_time(year) +
    ease_aes('linear')</code></pre>
<p>
I took this from <code>{gganimate}</code>‚Äôs README.
</p>
<pre class="r"><code>animation &lt;- ggplot(small_wiid4) +
    geom_point(aes(y = bottom5, x = top5, colour = eu, size = log(gdp_ppp_pc_usd2011))) +
    xlim(c(10, 20)) +
    geom_label_repel(aes(y = bottom5, x = top5, label = country), hjust = 1, nudge_x = 20) +
    theme(legend.position = "bottom") +
    theme_blog() +
    scale_color_blog() +
    labs(title = 'Year: {frame_time}', x = 'Top 5', y = 'Bottom 5') +
    transition_time(year) +
    ease_aes('linear')</code></pre>
<p>
I use <code>geom_label_repel</code> to place the countries‚Äô labels on the right of the plot. If I don‚Äôt do this, the labels of the countries would be floating around and the animation would be unreadable.
</p>
<p>
I then spent some time trying to render a nice webm instead of a gif. It took some trial and error and I am still not entirely satisfied with the result, but here is the code to render the animation:
</p>
<pre class="r"><code>animate(animation, renderer = ffmpeg_renderer(options = list(s = "864x480", 
                                                             vcodec = "libvpx-vp9",
                                                             crf = "15",
                                                             b = "1600k", 
                                                             vf = "setpts=5*PTS")))</code></pre>
<p>
The option <code>vf = ‚Äúsetpts=5*PTS‚Äù</code> is important because it slows the video down, so we can actually see something. <code>crf = ‚Äú15‚Äù</code> is the quality of the video (lower is better), <code>b = ‚Äú1600k‚Äù</code> is the bitrate, and <code>vcodec = ‚Äúlibvpx-vp9‚Äù</code> is the codec I use. The video you saw at the top of this post is the result. You can also find the video <a href="https://raw.githubusercontent.com/rbind/b-rodrigues.github.com/master/static/img/wiid_gganimate.webm">here</a>, and here‚Äôs a gif if all else fails:
</p>
<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=3zXx0ReqOOI"> <img src="https://b-rodrigues.github.io/assets/img/wiid_gganimate_gif.gif" title="Click to listen to OST of this gif"></a>
</p>
</div>
<p>
I would have preferred if the video was smoother, which should be possible by creating more frames. I did not find such an option in <code>{gganimate}</code>, and perhaps there is none, at least for now.
</p>
<p>
In any case <code>{gganimate}</code> is pretty nice to play with, and I‚Äôll definitely use it more!
</p>
</section>
<section id="update" class="level3">
<h3 class="anchored" data-anchor-id="update">
Update
</h3>
<p>
Silly me! It turns out thate the <code>animate()</code> function has arguments that can control the number of frames and the duration, without needing to pass options to the renderer. I was looking at options for the renderer only, without having read the documentation of the <code>animate()</code> function. It turns out that you can pass several arguments to the <code>animate()</code> function; for example, here is how you can make a GIF that lasts for 20 seconds running and 20 frames per second, pausing for 5 frames at the end and then restarting:
</p>
<pre class="r"><code>animate(animation, nframes = 400, duration = 20, fps = 20, end_pause = 5, rewind = TRUE)</code></pre>
<p>
I guess that you should only pass options to the renderer if you really need fine-grained control.
</p>
<p>
This took around 2 minutes to finish. You can use the same options with the ffmpeg renderer too. Here is what the gif looks like:
</p>
<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=3zXx0ReqOOI"> <img src="https://b-rodrigues.github.io/assets/img/wiid_gganimate_gif_smooth.gif" title="Click to listen to OST of this gif"></a>
</p>
</div>
<p>
Much, much smoother!
</p>
</section>


 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-12-27-fun_gganimate.html</guid>
  <pubDate>Thu, 27 Dec 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Objects types and some useful R functions for beginners</title>
  <link>https://b-rodrigues.github.io/posts/2018-12-24-modern_objects.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=M-1nTwiHxic"> <img width="400" src="https://b-rodrigues.github.io/assets/img/santa_sanders.jpg" title="The frydiest time of the year"></a>
</p>
</div>
<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<p>
This blog post is an excerpt of my ebook <em>Modern R with the tidyverse</em> that you can read for free <a href="https://b-rodrigues.github.io/modern_R/">here</a>. This is taken from Chapter 2, which explains the different R objects you can manipulate as well as some functions to get you started.
</p>
<section id="objects-types-and-useful-r-functions-to-get-started" class="level2">
<h2 class="anchored" data-anchor-id="objects-types-and-useful-r-functions-to-get-started">
Objects, types and useful R functions to get started
</h2>
<p>
All objects in R have a given <em>type</em>. You already know most of them, as these types are also used in mathematics. Integers, floating point numbers, or floats, matrices, etc, are all objects you are already familiar with. But R has other, maybe lesser known data types (that you can find in a lot of other programming languages) that you need to become familiar with. But first, we need to learn how to assign a value to a variable. This can be done in two ways:
</p>
<pre class="r"><code>a &lt;- 3</code></pre>
<p>
or
</p>
<pre class="r"><code>a = 3</code></pre>
<p>
in very practical terms, there is no difference between the two. I prefer using <code>&lt;-</code> for assigning values to variables and reserve <code>=</code> for passing arguments to functions, for example:
</p>
<pre class="r"><code>spam &lt;- mean(x = c(1,2,3))</code></pre>
<p>
I think this is less confusing than:
</p>
<pre class="r"><code>spam = mean(x = c(1,2,3))</code></pre>
<p>
but as I explained above you can use whatever you feel most comfortable with.
</p>
<section id="the-numeric-class" class="level3">
<h3 class="anchored" data-anchor-id="the-numeric-class">
The <code>numeric</code> class
</h3>
<p>
To define single numbers, you can do the following:
</p>
<pre class="r"><code>a &lt;- 3</code></pre>
<p>
The <code>class()</code> function allows you to check the class of an object:
</p>
<pre class="r"><code>class(a)</code></pre>
<pre><code>## [1] "numeric"</code></pre>
<p>
Decimals are defined with the character <code>.</code>:
</p>
<pre class="r"><code>a &lt;- 3.14</code></pre>
<p>
R also supports integers. If you find yourself in a situation where you explicitly need an integer and not a floating point number, you can use the following:
</p>
<pre class="r"><code>a  &lt;- as.integer(3)
class(a)</code></pre>
<pre><code>## [1] "integer"</code></pre>
<p>
The <code>as.integer()</code> function is very useful, because it converts its argument into an integer. There is a whole family of <code>as.*()</code> functions. To convert <code>a</code> into a floating point number again:
</p>
<pre class="r"><code>class(as.numeric(a))</code></pre>
<pre><code>## [1] "numeric"</code></pre>
<p>
There is also <code>is.numeric()</code> which tests whether a number is of the <code>numeric</code> class:
</p>
<pre class="r"><code>is.numeric(a)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>
These functions are very useful, there is one for any of the supported types in R. Later, we are going to learn about the <code>{purrr}</code> package, which is a very powerful package for functional programming. This package includes further such functions.
</p>
</section>
<section id="the-character-class" class="level3">
<h3 class="anchored" data-anchor-id="the-character-class">
The <code>character</code> class
</h3>
<p>
Use <code>‚Äù ‚Äú</code> to define characters (called strings in other programming languages):
</p>
<pre class="r"><code>a &lt;- "this is a string"</code></pre>
<pre class="r"><code>class(a)</code></pre>
<pre><code>## [1] "character"</code></pre>
<p>
To convert something to a character you can use the <code>as.character()</code> function:
</p>
<pre class="r"><code>a &lt;- 4.392

class(a)</code></pre>
<pre><code>## [1] "numeric"</code></pre>
<pre class="r"><code>class(as.character(a))</code></pre>
<pre><code>## [1] "character"</code></pre>
<p>
It is also possible to convert a character to a numeric:
</p>
<pre class="r"><code>a &lt;- "4.392"

class(a)</code></pre>
<pre><code>## [1] "character"</code></pre>
<pre class="r"><code>class(as.numeric(a))</code></pre>
<pre><code>## [1] "numeric"</code></pre>
<p>
But this only works if it makes sense:
</p>
<pre class="r"><code>a &lt;- "this won't work, chief"

class(a)</code></pre>
<pre><code>## [1] "character"</code></pre>
<pre class="r"><code>as.numeric(a)</code></pre>
<pre><code>## Warning: NAs introduced by coercion</code></pre>
<pre><code>## [1] NA</code></pre>
<p>
A very nice package to work with characters is <code>{stringr}</code>, which is also part of the <code>{tidyverse}</code>.
</p>
</section>
<section id="the-factor-class" class="level3">
<h3 class="anchored" data-anchor-id="the-factor-class">
The <code>factor</code> class
</h3>
<p>
Factors look like characters, but are very different. They are the representation of categorical variables. A <code>{tidyverse}</code> package to work with factors is <code>{forcats}</code>. You would rarely use factor variables outside of datasets, so for now, it is enough to know that this class exists. We are going to learn more about factor variables in Chapter 4, by using the <code>{forcats}</code> package.
</p>
</section>
<section id="the-date-class" class="level3">
<h3 class="anchored" data-anchor-id="the-date-class">
The <code>Date</code> class
</h3>
<p>
Dates also look like characters, but are very different too:
</p>
<pre class="r"><code>as.Date("2019/03/19")</code></pre>
<pre><code>## [1] "2019-03-19"</code></pre>
<pre class="r"><code>class(as.Date("2019/03/19"))</code></pre>
<pre><code>## [1] "Date"</code></pre>
<p>
Manipulating dates and time can be tricky, but thankfully there‚Äôs a <code>{tidyverse}</code> package for that, called <code>{lubridate}</code>. We are going to go over this package in Chapter 4.
</p>
</section>
<section id="the-logical-class" class="level3">
<h3 class="anchored" data-anchor-id="the-logical-class">
The <code>logical</code> class
</h3>
<p>
This class is the result of logical comparisons, for example, if you type:
</p>
<pre class="r"><code>4 &gt; 3</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>
R returns <code>TRUE</code>, which is an object of class <code>logical</code>:
</p>
<pre class="r"><code>k &lt;- 4 &gt; 3
class(k)</code></pre>
<pre><code>## [1] "logical"</code></pre>
<p>
In other programming languages, <code>logical</code>s are often called <code>bool</code>s. A <code>logical</code> variable can only have two values, either <code>TRUE</code> or <code>FALSE</code>. You can test the truthiness of a variable with <code>isTRUE()</code>:
</p>
<pre class="r"><code>k &lt;- 4 &gt; 3
isTRUE(k)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>
How can you test if a variable is false? There is not a <code>isFALSE()</code> function (at least not without having to load a package containing this function), but there is way to do it:
</p>
<pre class="r"><code>k &lt;- 4 &gt; 3
!isTRUE(k)</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>
The <code>!</code> operator indicates negation, so the above expression could be translated as <em>is k not TRUE?</em>. There are other such operators, namely <code>&amp;, &amp;&amp;, |, ||</code>. <code>&amp;</code> means <em>and</em> and <code>|</code> stands for <em>or</em>. You might be wondering what the difference between <code>&amp;</code> and <code>&amp;&amp;</code> is? Or between <code>|</code> and <code>||</code>? <code>&amp;</code> and <code>|</code> work on vectors, doing pairwise comparisons:
</p>
<pre class="r"><code>one &lt;- c(TRUE, FALSE, TRUE, FALSE)
two &lt;- c(FALSE, TRUE, TRUE, TRUE)
one &amp; two</code></pre>
<pre><code>## [1] FALSE FALSE  TRUE FALSE</code></pre>
<p>
Compare this to the <code>&amp;&amp;</code> operator:
</p>
<pre class="r"><code>one &lt;- c(TRUE, FALSE, TRUE, FALSE)
two &lt;- c(FALSE, TRUE, TRUE, TRUE)
one &amp;&amp; two</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>
The <code>&amp;&amp;</code> and <code>||</code> operators only compare the first element of the vectors and stop as soon as a the return value can be safely determined. This is called short-circuiting. Consider the following:
</p>
<pre class="r"><code>one &lt;- c(TRUE, FALSE, TRUE, FALSE)
two &lt;- c(FALSE, TRUE, TRUE, TRUE)
three &lt;- c(TRUE, TRUE, FALSE, FALSE)
one &amp;&amp; two &amp;&amp; three</code></pre>
<pre><code>## [1] FALSE</code></pre>
<pre class="r"><code>one || two || three</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>
The <code>||</code> operator stops as soon it evaluates to <code>TRUE</code> whereas the <code>&amp;&amp;</code> stops as soon as it evaluates to <code>FALSE</code>. Personally, I rarely use <code>||</code> or <code>&amp;&amp;</code> because I get confused. I find using <code>|</code> or <code>&amp;</code> in combination with the <code>all()</code> or <code>any()</code> functions much more useful:
</p>
<pre class="r"><code>one &lt;- c(TRUE, FALSE, TRUE, FALSE)
two &lt;- c(FALSE, TRUE, TRUE, TRUE)
any(one &amp; two)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>all(one &amp; two)</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>
<code>any()</code> checks whether any of the vector‚Äôs elements are <code>TRUE</code> and <code>all()</code> checks if all elements of the vector are <code>TRUE</code>.
</p>
<p>
As a final note, you should know that is possible to use <code>T</code> for <code>TRUE</code> and <code>F</code> for <code>FALSE</code> but I would advise against doing this, because it is not very explicit.
</p>
</section>
<section id="vectors-and-matrices" class="level3">
<h3 class="anchored" data-anchor-id="vectors-and-matrices">
Vectors and matrices
</h3>
<p>
You can create a vector in different ways. But first of all, it is important to understand that a vector in most programming languages is nothing more than a list of things. These things can be numbers (either integers or floats), strings, or even other vectors. A vector in R can only contain elements of one single type. This is not the case for a list, which is much more flexible. We will talk about lists shortly, but let‚Äôs first focus on vectors and matrices.
</p>
<section id="the-c-function" class="level4">
<h4 class="anchored" data-anchor-id="the-c-function">
The <code>c()</code> function
</h4>
<p>
A very important function that allows you to build a vector is <code>c()</code>:
</p>
<pre class="r"><code>a &lt;- c(1,2,3,4,5)</code></pre>
<p>
This creates a vector with elements 1, 2, 3, 4, 5. If you check its class:
</p>
<pre class="r"><code>class(a)</code></pre>
<pre><code>## [1] "numeric"</code></pre>
<p>
This can be confusing: you where probably expecting a to be of class <em>vector</em> or something similar. This is not the case if you use <code>c()</code> to create the vector, because <code>c()</code> doesn‚Äôt build a vector in the mathematical sense, but a so-called atomic vector. Checking its dimension:
</p>
<pre class="r"><code>dim(a)</code></pre>
<pre><code>## NULL</code></pre>
<p>
returns <code>NULL</code> because an atomic vector doesn‚Äôt have a dimension. If you want to create a true vector, you need to use <code>cbind()</code> or <code>rbind()</code>.
</p>
<p>
But before continuing, be aware that atomic vectors can only contain elements of the same type:
</p>
<pre class="r"><code>c(1, 2, "3")</code></pre>
<pre><code>## [1] "1" "2" "3"</code></pre>
<p>
because ‚Äú3‚Äù is a character, all the other values get implicitly converted to characters. You have to be very careful about this, and if you use atomic vectors in your programming, you have to make absolutely sure that no characters or logicals or whatever else are going to convert your atomic vector to something you were not expecting.
</p>
</section>
<section id="cbind-and-rbind" class="level4">
<h4 class="anchored" data-anchor-id="cbind-and-rbind">
<code>cbind()</code> and <code>rbind()</code>
</h4>
<p>
You can create a <em>true</em> vector with <code>cbind()</code>:
</p>
<pre class="r"><code>a &lt;- cbind(1, 2, 3, 4, 5)</code></pre>
<p>
Check its class now:
</p>
<pre class="r"><code>class(a)</code></pre>
<pre><code>## [1] "matrix"</code></pre>
<p>
This is exactly what we expected. Let‚Äôs check its dimension:
</p>
<pre class="r"><code>dim(a)</code></pre>
<pre><code>## [1] 1 5</code></pre>
<p>
This returns the dimension of <code>a</code> using the LICO notation (number of LInes first, the number of COlumns).
</p>
<p>
It is also possible to bind vectors together to create a matrix.
</p>
<pre class="r"><code>b &lt;- cbind(6,7,8,9,10)</code></pre>
<p>
Now let‚Äôs put vector <code>a</code> and <code>b</code> into a matrix called <code>matrix_c</code> using <code>rbind()</code>. <code>rbind()</code> functions the same way as <code>cbind()</code> but glues the vectors together by rows and not by columns.
</p>
<pre class="r"><code>matrix_c &lt;- rbind(a,b)
print(matrix_c)</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    2    3    4    5
## [2,]    6    7    8    9   10</code></pre>
</section>
<section id="the-matrix-class" class="level4">
<h4 class="anchored" data-anchor-id="the-matrix-class">
The <code>matrix</code> class
</h4>
<p>
R also has support for matrices. For example, you can create a matrix of dimension (5,5) filled with 0‚Äôs with the <code>matrix()</code> function:
</p>
<pre class="r"><code>matrix_a &lt;- matrix(0, nrow = 5, ncol = 5)</code></pre>
<p>
If you want to create the following matrix:
</p>
<p>
<img src="https://latex.codecogs.com/png.latex?%5B%20B%20=%20(%0A%5C%5B%5Cbegin%7Barray%7D%7Bccc%7D%0A2%20&amp;amp;%204%20&amp;amp;%203%20%5C%5C%0A1%20&amp;amp;%205%20&amp;amp;%207%0A%5Cend%7Barray%7D%5C%5D%0A)%20%5D">
</p>
<p>
you would do it like this:
</p>
<pre class="r"><code>B &lt;- matrix(c(2, 4, 3, 1, 5, 7), nrow = 2, byrow = TRUE)</code></pre>
<p>
The option <code>byrow = TRUE</code> means that the rows of the matrix will be filled first.
</p>
<p>
You can access individual elements of <code>matrix_a</code> like so:
</p>
<pre class="r"><code>matrix_a[2, 3]</code></pre>
<pre><code>## [1] 0</code></pre>
<p>
and R returns its value, 0. We can assign a new value to this element if we want. Try:
</p>
<pre class="r"><code>matrix_a[2, 3] &lt;- 7</code></pre>
<p>
and now take a look at <code>matrix_a</code> again.
</p>
<pre class="r"><code>print(matrix_a)</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4] [,5]
## [1,]    0    0    0    0    0
## [2,]    0    0    7    0    0
## [3,]    0    0    0    0    0
## [4,]    0    0    0    0    0
## [5,]    0    0    0    0    0</code></pre>
<p>
Recall our vector <code>b</code>:
</p>
<pre class="r"><code>b &lt;- cbind(6,7,8,9,10)</code></pre>
<p>
To access its third element, you can simply write:
</p>
<pre class="r"><code>b[3]</code></pre>
<pre><code>## [1] 8</code></pre>
<p>
I have heard many people praising R for being a matrix based language. Matrices are indeed useful, and statisticians are used to working with them. However, I very rarely use matrices in my day to day work, and prefer an approach based on data frames (which will be discussed below). This is because working with data frames makes it easier to use R‚Äôs advanced functional programming language capabilities, and this is where R really shines in my opinion. Working with matrices almost automatically implies using loops and all the iterative programming techniques, <em>√† la Fortran</em>, which I personally believe are ill-suited for interactive statistical programming (as discussed in the introduction).
</p>
</section>
</section>
<section id="the-list-class" class="level3">
<h3 class="anchored" data-anchor-id="the-list-class">
The <code>list</code> class
</h3>
<p>
The <code>list</code> class is a very flexible class, and thus, very useful. You can put anything inside a list, such as numbers:
</p>
<pre class="r"><code>list1 &lt;- list(3, 2)</code></pre>
<p>
or other lists constructed with <code>c()</code>:
</p>
<pre class="r"><code>list2 &lt;- list(c(1, 2), c(3, 4))</code></pre>
<p>
you can also put objects of different classes in the same list:
</p>
<pre class="r"><code>list3 &lt;- list(3, c(1, 2), "lists are amazing!")</code></pre>
<p>
and of course create list of lists:
</p>
<pre class="r"><code>my_lists &lt;- list(list1, list2, list3)</code></pre>
<p>
To check the contents of a list, you can use the structure function <code>str()</code>:
</p>
<pre class="r"><code>str(my_lists)</code></pre>
<pre><code>## List of 3
##  $ :List of 2
##   ..$ : num 3
##   ..$ : num 2
##  $ :List of 2
##   ..$ : num [1:2] 1 2
##   ..$ : num [1:2] 3 4
##  $ :List of 3
##   ..$ : num 3
##   ..$ : num [1:2] 1 2
##   ..$ : chr "lists are amazing!"</code></pre>
<p>
or you can use RStudio‚Äôs <em>Environment</em> pane:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/rstudio_environment_list.gif"><!-- -->
</p>
<p>
You can also create named lists:
</p>
<pre class="r"><code>list4 &lt;- list("a" = 2, "b" = 8, "c" = "this is a named list")</code></pre>
<p>
and you can access the elements in two ways:
</p>
<pre class="r"><code>list4[[1]]</code></pre>
<pre><code>## [1] 2</code></pre>
<p>
or, for named lists:
</p>
<pre class="r"><code>list4$c</code></pre>
<pre><code>## [1] "this is a named list"</code></pre>
<p>
Lists are used extensively because they are so flexible. You can build lists of datasets and apply functions to all the datasets at once, build lists of models, lists of plots, etc‚Ä¶ In the later chapters we are going to learn all about them. Lists are central objects in a functional programming workflow for interactive statistical analysis.
</p>
</section>
<section id="the-data.frame-and-tibble-classes" class="level3">
<h3 class="anchored" data-anchor-id="the-data.frame-and-tibble-classes">
The <code>data.frame</code> and <code>tibble</code> classes
</h3>
<p>
In the next chapter we are going to learn how to import datasets into R. Once you import data, the resulting object is either a <code>data.frame</code> or a <code>tibble</code> depending on which package you used to import the data. <code>tibble</code>s extend <code>data.frame</code>s so if you know about <code>data.frame</code> objects already, working with <code>tibble</code>s will be very easy. <code>tibble</code>s have a better <code>print()</code> method, and some other niceties.
</p>
<p>
However, I want to stress that these objects are central to R and are thus very important; they are actually special cases of lists, discussed above. There are different ways to print a <code>data.frame</code> or a <code>tibble</code> if you wish to inspect it. You can use <code>View(my_data)</code> to show the <code>my_data</code> <code>data.frame</code> in the <em>View</em> pane of RStudio:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/rstudio_view_data.gif"><!-- -->
</p>
<p>
You can also use the <code>str()</code> function:
</p>
<pre class="r"><code>str(my_data)</code></pre>
<p>
And if you need to access an individual column, you can use the <code>$</code> sign, same as for a list:
</p>
<pre class="r"><code>my_data$col1</code></pre>
</section>
<section id="formulas" class="level3">
<h3 class="anchored" data-anchor-id="formulas">
Formulas
</h3>
<p>
We will learn more about formulas later, but because it is an important object, it is useful if you already know about them early on. A formula is defined in the following way:
</p>
<pre class="r"><code>my_formula &lt;- ~x

class(my_formula)</code></pre>
<pre><code>## [1] "formula"</code></pre>
<p>
Formula objects are defined using the <code>~</code> symbol. Formulas are useful to define statistical models, for example for a linear regression:
</p>
<pre class="r"><code>lm(y ~ x)</code></pre>
<p>
or also to define anonymous functions, but more on this later.
</p>
</section>
</section>
<section id="models" class="level2">
<h2 class="anchored" data-anchor-id="models">
Models
</h2>
<p>
A statistical model is an object like any other in R:
</p>
<pre class="r"><code>data(mtcars)

my_model &lt;- lm(mpg ~ hp, mtcars)

class(my_model)</code></pre>
<pre><code>## [1] "lm"</code></pre>
<p>
<code>my_model</code> is an object of class <code>lm</code>. You can apply different functions to a model object:
</p>
<pre class="r"><code>summary(my_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = mpg ~ hp, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.7121 -2.1122 -0.8854  1.5819  8.2360 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***
## hp          -0.06823    0.01012  -6.742 1.79e-07 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.863 on 30 degrees of freedom
## Multiple R-squared:  0.6024, Adjusted R-squared:  0.5892 
## F-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07</code></pre>
<p>
This class will be explored in later chapters.
</p>
<section id="null-na-and-nan" class="level3">
<h3 class="anchored" data-anchor-id="null-na-and-nan">
NULL, NA and NaN
</h3>
<p>
The <code>NULL</code>, <code>NA</code> and <code>NaN</code> classes are pretty special. <code>NULL</code> is returned when the result of function is undetermined. For example, consider <code>list4</code>:
</p>
<pre class="r"><code>list4</code></pre>
<pre><code>## $a
## [1] 2
## 
## $b
## [1] 8
## 
## $c
## [1] "this is a named list"</code></pre>
<p>
if you try to access an element that does not exist, such as <code>d</code>, you will get <code>NULL</code> back:
</p>
<pre class="r"><code>list4$d</code></pre>
<pre><code>## NULL</code></pre>
<p>
<code>NaN</code> means ‚ÄúNot a Number‚Äù and is returned when a function return something that is not a number:
</p>
<pre class="r"><code>sqrt(-1)</code></pre>
<pre><code>## Warning in sqrt(-1): NaNs produced</code></pre>
<pre><code>## [1] NaN</code></pre>
<p>
or:
</p>
<pre class="r"><code>0/0</code></pre>
<pre><code>## [1] NaN</code></pre>
<p>
Basically, numbers that cannot be represented as floating point numbers are <code>NaN</code>.
</p>
<p>
Finally, there‚Äôs <code>NA</code> which is closely related to <code>NaN</code> but is used for missing values. <code>NA</code> stands for <code>Not Available</code>. There are several types of <code>NA</code>s:
</p>
<ul>
<li>
<code>NA_integer_</code>
</li>
<li>
<code>NA_real_</code>
</li>
<li>
<code>NA_complex_</code>
</li>
<li>
<code>NA_character_</code>
</li>
</ul>
<p>
but these are in principle only used when you need to program your own functions and need to explicitly test for the missingness of, say, a character value.
</p>
<p>
To test whether a value is <code>NA</code>, use the <code>is.na()</code> function.
</p>
</section>
<section id="useful-functions-to-get-you-started" class="level3">
<h3 class="anchored" data-anchor-id="useful-functions-to-get-you-started">
Useful functions to get you started
</h3>
<p>
This section will list several basic R functions that are very useful and should be part of your toolbox.
</p>
<section id="sequences" class="level4">
<h4 class="anchored" data-anchor-id="sequences">
Sequences
</h4>
<p>
There are several functions that create sequences, <code>seq()</code>, <code>seq_along()</code> and <code>rep()</code>. <code>rep()</code> is easy enough:
</p>
<pre class="r"><code>rep(1, 10)</code></pre>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1</code></pre>
<p>
This simply repeats <code>1</code> 10 times. You can repeat other objects too:
</p>
<pre class="r"><code>rep("HAHA", 10)</code></pre>
<pre><code>##  [1] "HAHA" "HAHA" "HAHA" "HAHA" "HAHA" "HAHA" "HAHA" "HAHA" "HAHA" "HAHA"</code></pre>
<p>
To create a sequence, things are not as straightforward. There is <code>seq()</code>:
</p>
<pre class="r"><code>seq(1, 10)</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10</code></pre>
<pre class="r"><code>seq(70, 80)</code></pre>
<pre><code>##  [1] 70 71 72 73 74 75 76 77 78 79 80</code></pre>
<p>
It is also possible to provide a <code>by</code> argument:
</p>
<pre class="r"><code>seq(1, 10, by = 2)</code></pre>
<pre><code>## [1] 1 3 5 7 9</code></pre>
<p>
<code>seq_along()</code> behaves similarly, but returns the length of the object passed to it. So if you pass <code>list4</code> to <code>seq_along()</code>, it will return a sequence from 1 to 3:
</p>
<pre class="r"><code>seq_along(list4)</code></pre>
<pre><code>## [1] 1 2 3</code></pre>
<p>
which is also true for <code>seq()</code> actually:
</p>
<pre class="r"><code>seq(list4)</code></pre>
<pre><code>## [1] 1 2 3</code></pre>
<p>
but these two functions behave differently for arguments of length equal to 1:
</p>
<pre class="r"><code>seq(10)</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10</code></pre>
<pre class="r"><code>seq_along(10)</code></pre>
<pre><code>## [1] 1</code></pre>
<p>
So be quite careful about that. I would advise you do not use <code>seq()</code>, but only <code>seq_along()</code> and <code>seq_len()</code>. <code>seq_len()</code> only takes arguments of length 1:
</p>
<pre class="r"><code>seq_len(10)</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10</code></pre>
<pre class="r"><code>seq_along(10)</code></pre>
<pre><code>## [1] 1</code></pre>
<p>
The problem with <code>seq()</code> is that it is unpredictable; depending on its input, the output will either be an integer or a sequence. When programming, it is better to have function that are stricter and fail when confronted to special cases, instead of returning some result. This is a bit of a recurrent issue with R, and the functions from the <code>{tidyverse}</code> mitigate this issue by being stricter than their base R counterparts. For example, consider the <code>ifelse()</code> function from base R:
</p>
<pre class="r"><code>ifelse(3 &gt; 5, 1, "this is false")</code></pre>
<pre><code>## [1] "this is false"</code></pre>
<p>
and compare it to <code>{dplyr}</code>‚Äôs implementation, <code>if_else()</code>:
</p>
<pre class="r"><code>if_else(3 &gt; 5, 1, "this is false")
Error: `false` must be type double, not character
Call `rlang::last_error()` to see a backtrace</code></pre>
<p>
<code>if_else()</code> fails because the return value when <code>FALSE</code> is not a double (a real number) but a character. This might seem unnecessarily strict, but at least it is predictable. This makes debugging easier when used inside functions. In Chapter 8 we are going to learn how to write our own functions, and being strict makes programming easier.
</p>
</section>
<section id="basic-string-manipulation" class="level4">
<h4 class="anchored" data-anchor-id="basic-string-manipulation">
Basic string manipulation
</h4>
<p>
For now, we have not closely studied <code>character</code> objects, we only learned how to define them. Later, in Chapter 5 we will learn about the <code>{stringr}</code> package which provides useful function to work with strings. However, there are several base R functions that are very useful that you might want to know nonetheless, such as <code>paste()</code> and <code>paste0()</code>:
</p>
<pre class="r"><code>paste("Hello", "amigo")</code></pre>
<pre><code>## [1] "Hello amigo"</code></pre>
<p>
but you can also change the separator if needed:
</p>
<pre class="r"><code>paste("Hello", "amigo", sep = "--")</code></pre>
<pre><code>## [1] "Hello--amigo"</code></pre>
<p>
<code>paste0()</code> is the same as <code>paste()</code> but does not have any <code>sep</code> argument:
</p>
<pre class="r"><code>paste0("Hello", "amigo")</code></pre>
<pre><code>## [1] "Helloamigo"</code></pre>
<p>
If you provide a vector of characters, you can also use the <code>collapse</code> argument, which places whatever you provide for <code>collapse</code> between the characters of the vector:
</p>
<pre class="r"><code>paste0(c("Joseph", "Mary", "Jesus"), collapse = ", and ")</code></pre>
<pre><code>## [1] "Joseph, and Mary, and Jesus"</code></pre>
<p>
To change the case of characters, you can use <code>toupper()</code> and <code>tolower()</code>:
</p>
<pre class="r"><code>tolower("HAHAHAHAH")</code></pre>
<pre><code>## [1] "hahahahah"</code></pre>
<pre class="r"><code>toupper("hueuehuehuheuhe")</code></pre>
<pre><code>## [1] "HUEUEHUEHUHEUHE"</code></pre>
</section>
<section id="mathematical-functions" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-functions">
Mathematical functions
</h4>
<p>
Finally, there are the classical mathematical functions that you know and love:
</p>
<ul>
<li>
<code>sqrt()</code>
</li>
<li>
<code>exp()</code>
</li>
<li>
<code>log()</code>
</li>
<li>
<code>abs()</code>
</li>
<li>
<code>sin()</code>, <code>cos()</code>, <code>tan()</code>, and others
</li>
<li>
<code>sum()</code>, <code>cumsum()</code>, <code>prod()</code>, <code>cumprod()</code>
</li>
<li>
<code>max()</code>, <code>min()</code>
</li>
</ul>
<p>
and many others‚Ä¶
</p>
</section>
</section>
</section>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-12-24-modern_objects.html</guid>
  <pubDate>Mon, 24 Dec 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Using the tidyverse for more than data manipulation: estimating pi with Monte Carlo methods</title>
  <link>https://b-rodrigues.github.io/posts/2018-12-21-tidyverse_pi.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=kZJY15dyMig"> <img width="400" src="https://b-rodrigues.github.io/assets/img/casino.jpg" title="Audentes Fortuna Iuvat"></a>
</p>
</div>
<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<p>
This blog post is an excerpt of my ebook <em>Modern R with the tidyverse</em> that you can read for free <a href="https://b-rodrigues.github.io/modern_R/">here</a>. This is taken from Chapter 5, which presents the <code>{tidyverse}</code> packages and how to use them to compute descriptive statistics and manipulate data. In the text below, I show how you can use the <code>{tidyverse}</code> functions and principles for the estimation of <img src="https://latex.codecogs.com/png.latex?()"> using Monte Carlo simulation.
</p>
<section id="going-beyond-descriptive-statistics-and-data-manipulation" class="level2">
<h2 class="anchored" data-anchor-id="going-beyond-descriptive-statistics-and-data-manipulation">
Going beyond descriptive statistics and data manipulation
</h2>
<p>
The <code>{tidyverse}</code> collection of packages can do much more than simply data manipulation and descriptive statisics. You can use the principles we have covered and the functions you now know to do much more. For instance, you can use a few <code>{tidyverse}</code> functions to do Monte Carlo simulations, for example to estimate <img src="https://latex.codecogs.com/png.latex?()">.
</p>
<p>
Draw the unit circle inside the unit square, the ratio of the area of the circle to the area of the square will be <img src="https://latex.codecogs.com/png.latex?(/4)">. Then shot K arrows at the square; roughly <img src="https://latex.codecogs.com/png.latex?(K*/4)"> should have fallen inside the circle. So if now you shoot N arrows at the square, and M fall inside the circle, you have the following relationship <img src="https://latex.codecogs.com/png.latex?(M%20=%20N/4)"><em>. You can thus compute <img src="https://latex.codecogs.com/png.latex?()"> like so: <img src="https://latex.codecogs.com/png.latex?(=%204"></em>M/N).
</p>
<p>
The more arrows N you throw at the square, the better approximation of <img src="https://latex.codecogs.com/png.latex?()"> you‚Äôll have. Let‚Äôs try to do this with a tidy Monte Carlo simulation. First, let‚Äôs randomly pick some points inside the unit square:
</p>
<pre class="r"><code>library(tidyverse)
library(brotools)</code></pre>
<pre class="r"><code>n &lt;- 5000

set.seed(2019)
points &lt;- tibble("x" = runif(n), "y" = runif(n))</code></pre>
<p>
Now, to know if a point is inside the unit circle, we need to check wether <img src="https://latex.codecogs.com/png.latex?(x%5E2%20+%20y%5E2%20%3C%201)">. Let‚Äôs add a new column to the <code>points</code> tibble, called <code>inside</code> equal to 1 if the point is inside the unit circle and 0 if not:
</p>
<pre class="r"><code>points &lt;- points %&gt;% 
    mutate(inside = map2_dbl(.x = x, .y = y, ~ifelse(.x**2 + .y**2 &lt; 1, 1, 0))) %&gt;% 
    rowid_to_column("N")</code></pre>
<p>
Let‚Äôs take a look at <code>points</code>:
</p>
<pre class="r"><code>points</code></pre>
<pre><code>## # A tibble: 5,000 x 4
##        N       x      y inside
##    &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1     1 0.770   0.984       0
##  2     2 0.713   0.0107      1
##  3     3 0.303   0.133       1
##  4     4 0.618   0.0378      1
##  5     5 0.0505  0.677       1
##  6     6 0.0432  0.0846      1
##  7     7 0.820   0.727       0
##  8     8 0.00961 0.0758      1
##  9     9 0.102   0.373       1
## 10    10 0.609   0.676       1
## # ‚Ä¶ with 4,990 more rows</code></pre>
<p>
The <code>rowid_to_column()</code> function, from the <code>{tibble}</code> package, adds a new column to the data frame with an id, going from 1 to the number of rows in the data frame. Now, I can compute the estimation of <img src="https://latex.codecogs.com/png.latex?()"> at each row, by computing the cumulative sum of the 1‚Äôs in the <code>inside</code> column and dividing that by the current value of <code>N</code> column:
</p>
<pre class="r"><code>points &lt;- points %&gt;% 
    mutate(estimate = 4*cumsum(inside)/N)</code></pre>
<p>
<code>cumsum(inside)</code> is the <code>M</code> from the formula. Now, we can finish by plotting the result:
</p>
<pre class="r"><code>ggplot(points) + 
    geom_line(aes(y = estimate, x = N), colour = "#82518c") + 
    geom_hline(yintercept = pi) +
    theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/tidyverse_pi-7-1.png" width="672">
</p>
<p>
In Chapter 6, we are going to learn all about <code>{ggplot2}</code>.
</p>
<p>
As the number of tries grows, the estimation of <img src="https://latex.codecogs.com/png.latex?()"> gets better.
</p>
<p>
Using a data frame as a structure to hold our simulated points and the results makes it very easy to avoid loops, and thus write code that is more concise and easier to follow. If you studied a quantitative field in u8niversity, you might have done a similar exercise at the time, very likely by defining a matrix to hold your points, and an empty vector to hold whether a particular point was inside the unit circle. Then you wrote a loop to compute whether a point was inside the unit circle, save this result in the before-defined empty vector and then compute the estimation of <img src="https://latex.codecogs.com/png.latex?()">. Again, I take this opportunity here to stress that there is nothing wrong with this approach per se, but R, with the <code>{tidyverse}</code> is better suited for a workflow where lists or data frames are the central objects and where the analyst operates over them with functional programming techniques.
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-12-21-tidyverse_pi.html</guid>
  <pubDate>Fri, 21 Dec 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Manipulate dates easily with {lubridate}</title>
  <link>https://b-rodrigues.github.io/posts/2018-12-15-lubridate_africa.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=FTQbiNvZqaY"> <img width="400" src="https://b-rodrigues.github.io/assets/img/africa.jpg" title="One of my favourite songs"></a>
</p>
</div>
<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<p>
This blog post is an excerpt of my ebook <em>Modern R with the tidyverse</em> that you can read for free <a href="https://b-rodrigues.github.io/modern_R/">here</a>. This is taken from Chapter 5, which presents the <code>{tidyverse}</code> packages and how to use them to compute descriptive statistics and manipulate data. In the text below, I scrape a table from Wikipedia, which shows when African countries gained independence from other countries. Then, using <code>{lubridate}</code> functions I show you how you can answers questions such as <em>Which countries gained independence before 1960?</em>.
</p>
<section id="set-up-scraping-some-data-from-wikipedia" class="level2">
<h2 class="anchored" data-anchor-id="set-up-scraping-some-data-from-wikipedia">
Set-up: scraping some data from Wikipedia
</h2>
<p>
<code>{lubridate}</code> is yet another tidyverse package, that makes dealing with dates or duration data (and intervals) as painless as possible. I do not use every function contained in the package daily, and as such will only focus on some of the functions. However, if you have to deal with dates often, you might want to explore the package thoroughly.
</p>
<p>
Let‚Äôs get some data from a Wikipedia table:
</p>
<pre class="r"><code>library(tidyverse)
library(rvest)</code></pre>
<pre class="r"><code>page &lt;- read_html("https://en.wikipedia.org/wiki/Decolonisation_of_Africa")

independence &lt;- page %&gt;%
    html_node(".wikitable") %&gt;%
    html_table(fill = TRUE)

independence &lt;- independence %&gt;%
    select(-Rank) %&gt;%
    map_df(~str_remove_all(., "\\[.*\\]")) %&gt;%
    rename(country = `Country[a]`,
           colonial_name = `Colonial name`,
           colonial_power = `Colonial power[b]`,
           independence_date = `Independence date[c]`,
           first_head_of_state = `First head of state[d]`,
           independence_won_through = `Independence won through`)</code></pre>
<p>
This dataset was scraped from the following Wikipedia <a href="https://en.wikipedia.org/wiki/Decolonisation_of_Africa#Timeline">table</a>. It shows when African countries gained independence from which colonial powers. In Chapter 11, I will show you how to scrape Wikipedia pages using R. For now, let‚Äôs take a look at the contents of the dataset:
</p>
<pre class="r"><code>independence</code></pre>
<pre><code>## # A tibble: 54 x 6
##    country colonial_name colonial_power independence_da‚Ä¶ first_head_of_s‚Ä¶
##    &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;            &lt;chr&gt;           
##  1 Liberia Liberia       United States  26 July 1847     Joseph Jenkins ‚Ä¶
##  2 South ‚Ä¶ Cape Colony ‚Ä¶ United Kingdom 31 May 1910      Louis Botha     
##  3 Egypt   Sultanate of‚Ä¶ United Kingdom 28 February 1922 Fuad I          
##  4 Eritrea Italian Erit‚Ä¶ Italy          10 February 1947 Haile Selassie  
##  5 Libya   British Mili‚Ä¶ United Kingdo‚Ä¶ 24 December 1951 Idris           
##  6 Sudan   Anglo-Egypti‚Ä¶ United Kingdo‚Ä¶ 1 January 1956   Ismail al-Azhari
##  7 Tunisia French Prote‚Ä¶ France         20 March 1956    Muhammad VIII a‚Ä¶
##  8 Morocco French Prote‚Ä¶ France&nbsp;Spain   2 March 19567 A‚Ä¶ Mohammed V      
##  9 Ghana   Gold Coast    United Kingdom 6 March 1957     Kwame Nkrumah   
## 10 Guinea  French West ‚Ä¶ France         2 October 1958   Ahmed S√©kou Tou‚Ä¶
## # ‚Ä¶ with 44 more rows, and 1 more variable: independence_won_through &lt;chr&gt;</code></pre>
<p>
as you can see, the date of independence is in a format that might make it difficult to answer questions such as <em>Which African countries gained independence before 1960 ?</em> for two reasons. First of all, the date uses the name of the month instead of the number of the month (well, this is not such a big deal, but still), and second of all the type of the independence day column is <em>character</em> and not ‚Äúdate‚Äù. So our first task is to correctly define the column as being of type date, while making sure that R understands that <em>January</em> is supposed to be ‚Äú01‚Äù, and so on.
</p>
</section>
<section id="using-lubridate" class="level2">
<h2 class="anchored" data-anchor-id="using-lubridate">
Using <code>{lubridate}</code>
</h2>
<p>
There are several helpful functions included in <code>{lubridate}</code> to convert columns to dates. For instance if the column you want to convert is of the form ‚Äú2012-11-21‚Äù, then you would use the function <code>ymd()</code>, for ‚Äúyear-month-day‚Äù. If, however the column is ‚Äú2012-21-11‚Äù, then you would use <code>ydm()</code>. There‚Äôs a few of these helper functions, and they can handle a lot of different formats for dates. In our case, having the name of the month instead of the number might seem quite problematic, but it turns out that this is a case that <code>{lubridate}</code> handles painfully:
</p>
<pre class="r"><code>library(lubridate)</code></pre>
<pre><code>## 
## Attaching package: 'lubridate'</code></pre>
<pre><code>## The following object is masked from 'package:base':
## 
##     date</code></pre>
<pre class="r"><code>independence &lt;- independence %&gt;%
  mutate(independence_date = dmy(independence_date))</code></pre>
<pre><code>## Warning: 5 failed to parse.</code></pre>
<p>
Some dates failed to parse, for instance for Morocco. This is because these countries have several independence dates; this means that the string to convert looks like:
</p>
<pre><code>"2 March 1956
7 April 1956
10 April 1958
4 January 1969"</code></pre>
<p>
which obviously cannot be converted by <code>{lubridate}</code> without further manipulation. I ignore these cases for simplicity‚Äôs sake.
</p>
<p>
Let‚Äôs take a look at the data now:
</p>
<pre class="r"><code>independence</code></pre>
<pre><code>## # A tibble: 54 x 6
##    country colonial_name colonial_power independence_da‚Ä¶ first_head_of_s‚Ä¶
##    &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;          &lt;date&gt;           &lt;chr&gt;           
##  1 Liberia Liberia       United States  1847-07-26       Joseph Jenkins ‚Ä¶
##  2 South ‚Ä¶ Cape Colony ‚Ä¶ United Kingdom 1910-05-31       Louis Botha     
##  3 Egypt   Sultanate of‚Ä¶ United Kingdom 1922-02-28       Fuad I          
##  4 Eritrea Italian Erit‚Ä¶ Italy          1947-02-10       Haile Selassie  
##  5 Libya   British Mili‚Ä¶ United Kingdo‚Ä¶ 1951-12-24       Idris           
##  6 Sudan   Anglo-Egypti‚Ä¶ United Kingdo‚Ä¶ 1956-01-01       Ismail al-Azhari
##  7 Tunisia French Prote‚Ä¶ France         1956-03-20       Muhammad VIII a‚Ä¶
##  8 Morocco French Prote‚Ä¶ France&nbsp;Spain   NA               Mohammed V      
##  9 Ghana   Gold Coast    United Kingdom 1957-03-06       Kwame Nkrumah   
## 10 Guinea  French West ‚Ä¶ France         1958-10-02       Ahmed S√©kou Tou‚Ä¶
## # ‚Ä¶ with 44 more rows, and 1 more variable: independence_won_through &lt;chr&gt;</code></pre>
<p>
As you can see, we now have a date column in the right format. We can now answer questions such as <em>Which countries gained independence before 1960?</em> quite easily, by using the functions <code>year()</code>, <code>month()</code> and <code>day()</code>. Let‚Äôs see which countries gained independence before 1960:
</p>
<pre class="r"><code>independence %&gt;%
  filter(year(independence_date) &lt;= 1960) %&gt;%
  pull(country)</code></pre>
<pre><code>##  [1] "Liberia"                          "South Africa"                    
##  [3] "Egypt"                            "Eritrea"                         
##  [5] "Libya"                            "Sudan"                           
##  [7] "Tunisia"                          "Ghana"                           
##  [9] "Guinea"                           "Cameroon"                        
## [11] "Togo"                             "Mali"                            
## [13] "Madagascar"                       "Democratic Republic of the Congo"
## [15] "Benin"                            "Niger"                           
## [17] "Burkina Faso"                     "Ivory Coast"                     
## [19] "Chad"                             "Central African Republic"        
## [21] "Republic of the Congo"            "Gabon"                           
## [23] "Mauritania"</code></pre>
<p>
You guessed it, <code>year()</code> extracts the year of the date column and converts it as a <em>numeric</em> so that we can work on it. This is the same for <code>month()</code> or <code>day()</code>. Let‚Äôs try to see if countries gained their independence on Christmas Eve:
</p>
<pre class="r"><code>independence %&gt;%
  filter(month(independence_date) == 12,
         day(independence_date) == 24) %&gt;%
  pull(country)</code></pre>
<pre><code>## [1] "Libya"</code></pre>
<p>
Seems like Libya was the only one! You can also operate on dates. For instance, let‚Äôs compute the difference between two dates, using the <code>interval()</code> column:
</p>
<pre class="r"><code>independence %&gt;%
  mutate(today = lubridate::today()) %&gt;%
  mutate(independent_since = interval(independence_date, today)) %&gt;%
  select(country, independent_since)</code></pre>
<pre><code>## # A tibble: 54 x 2
##    country      independent_since             
##    &lt;chr&gt;        &lt;S4: Interval&gt;                
##  1 Liberia      1847-07-26 UTC--2019-02-10 UTC
##  2 South Africa 1910-05-31 UTC--2019-02-10 UTC
##  3 Egypt        1922-02-28 UTC--2019-02-10 UTC
##  4 Eritrea      1947-02-10 UTC--2019-02-10 UTC
##  5 Libya        1951-12-24 UTC--2019-02-10 UTC
##  6 Sudan        1956-01-01 UTC--2019-02-10 UTC
##  7 Tunisia      1956-03-20 UTC--2019-02-10 UTC
##  8 Morocco      NA--NA                        
##  9 Ghana        1957-03-06 UTC--2019-02-10 UTC
## 10 Guinea       1958-10-02 UTC--2019-02-10 UTC
## # ‚Ä¶ with 44 more rows</code></pre>
<p>
The <code>independent_since</code> column now contains an <em>interval</em> object that we can convert to years:
</p>
<pre class="r"><code>independence %&gt;%
  mutate(today = lubridate::today()) %&gt;%
  mutate(independent_since = interval(independence_date, today)) %&gt;%
  select(country, independent_since) %&gt;%
  mutate(years_independent = as.numeric(independent_since, "years"))</code></pre>
<pre><code>## # A tibble: 54 x 3
##    country      independent_since              years_independent
##    &lt;chr&gt;        &lt;S4: Interval&gt;                             &lt;dbl&gt;
##  1 Liberia      1847-07-26 UTC--2019-02-10 UTC             172. 
##  2 South Africa 1910-05-31 UTC--2019-02-10 UTC             109. 
##  3 Egypt        1922-02-28 UTC--2019-02-10 UTC              97.0
##  4 Eritrea      1947-02-10 UTC--2019-02-10 UTC              72  
##  5 Libya        1951-12-24 UTC--2019-02-10 UTC              67.1
##  6 Sudan        1956-01-01 UTC--2019-02-10 UTC              63.1
##  7 Tunisia      1956-03-20 UTC--2019-02-10 UTC              62.9
##  8 Morocco      NA--NA                                      NA  
##  9 Ghana        1957-03-06 UTC--2019-02-10 UTC              61.9
## 10 Guinea       1958-10-02 UTC--2019-02-10 UTC              60.4
## # ‚Ä¶ with 44 more rows</code></pre>
<p>
We can now see for how long the last country to gain independence has been independent. Because the data is not tidy (in some cases, an African country was colonized by two powers, see Libya), I will only focus on 4 European colonial powers: Belgium, France, Portugal and the United Kingdom:
</p>
<pre class="r"><code>independence %&gt;%
  filter(colonial_power %in% c("Belgium", "France", "Portugal", "United Kingdom")) %&gt;%
  mutate(today = lubridate::today()) %&gt;%
  mutate(independent_since = interval(independence_date, today)) %&gt;%
  mutate(years_independent = as.numeric(independent_since, "years")) %&gt;%
  group_by(colonial_power) %&gt;%
  summarise(last_colony_independent_for = min(years_independent, na.rm = TRUE))</code></pre>
<pre><code>## # A tibble: 4 x 2
##   colonial_power last_colony_independent_for
##   &lt;chr&gt;                                &lt;dbl&gt;
## 1 Belgium                               56.6
## 2 France                                41.6
## 3 Portugal                              43.2
## 4 United Kingdom                        42.6</code></pre>
<p>
<code>{lubridate}</code> contains many more functions. If you often work with dates, duration or interval data, <code>{lubridate}</code> is a package that you have to master.
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-12-15-lubridate_africa.html</guid>
  <pubDate>Sat, 15 Dec 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>What hyper-parameters are, and what to do with them; an illustration with ridge regression</title>
  <link>https://b-rodrigues.github.io/posts/2018-12-02-hyper-parameters.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=13Gd5kpLzsw"> <img width="400" src="https://b-rodrigues.github.io/assets/img/ridge.jpg" title="Gameboy ridge"></a>
</p>
</div>
<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<p>
This blog post is an excerpt of my ebook <em>Modern R with the tidyverse</em> that you can read for free <a href="https://b-rodrigues.github.io/modern_R/">here</a>. This is taken from Chapter 7, which deals with statistical models. In the text below, I explain what hyper-parameters are, and as an example I run a ridge regression using the <code>{glmnet}</code> package. The book is still being written, so comments are more than welcome!
</p>
<section id="hyper-parameters" class="level2">
<h2 class="anchored" data-anchor-id="hyper-parameters">
Hyper-parameters
</h2>
<p>
Hyper-parameters are parameters of the model that cannot be directly learned from the data. A linear regression does not have any hyper-parameters, but a random forest for instance has several. You might have heard of ridge regression, lasso and elasticnet. These are extensions to linear models that avoid over-fitting by penalizing <em>large</em> models. These extensions of the linear regression have hyper-parameters that the practitioner has to tune. There are several ways one can tune these parameters, for example, by doing a grid-search, or a random search over the grid or using more elaborate methods. To introduce hyper-parameters, let‚Äôs get to know ridge regression, also called Tikhonov regularization.
</p>
<section id="ridge-regression" class="level3">
<h3 class="anchored" data-anchor-id="ridge-regression">
Ridge regression
</h3>
<p>
Ridge regression is used when the data you are working with has a lot of explanatory variables, or when there is a risk that a simple linear regression might overfit to the training data, because, for example, your explanatory variables are collinear. If you are training a linear model and then you notice that it generalizes very badly to new, unseen data, it is very likely that the linear model you trained overfits the data. In this case, ridge regression might prove useful. The way ridge regression works might seem counter-intuititive; it boils down to fitting a <em>worse</em> model to the training data, but in return, this worse model will generalize better to new data.
</p>
<p>
The closed form solution of the ordinary least squares estimator is defined as:
</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cbeta%7D%20=%20(X'X)%5E%7B-1%7DX'Y"></p>
<p>
where <img src="https://latex.codecogs.com/png.latex?X"> is the design matrix (the matrix made up of the explanatory variables) and <img src="https://latex.codecogs.com/png.latex?Y"> is the dependent variable. For ridge regression, this closed form solution changes a little bit:
</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cbeta%7D%20=%20(X'X%20+%20%5Clambda%20I_p)%5E%7B-1%7DX'Y"></p>
<p>
where <img src="https://latex.codecogs.com/png.latex?lambda%20%5Cin%20%5Cmathbb%7BR%7D"> is an hyper-parameter and <img src="https://latex.codecogs.com/png.latex?I_p"> is the identity matrix of dimension <img src="https://latex.codecogs.com/png.latex?p"> (<img src="https://latex.codecogs.com/png.latex?p"> is the number of explanatory variables). This formula above is the closed form solution to the following optimisation program:
</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Csum_%7Bi=1%7D%5En%20%5Cleft(y_i%20-%20%5Csum_%7Bj=1%7D%5Epx_%7Bij%7D%5Cbeta_j%5Cright)%5E2%20"></p>
<p>
such that:
</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Csum_%7Bj=1%7D%5Ep(%5Cbeta_j)%5E2%20%3C%20c"></p>
<p>
for any strictly positive <img src="https://latex.codecogs.com/png.latex?(c)">.
</p>
<p>
The <code>glmnet()</code> function from the <code>{glmnet}</code> package can be used for ridge regression, by setting the <code>alpha</code> argument to 0 (setting it to 1 would do LASSO, and setting it to a number between 0 and 1 would do elasticnet). But in order to compare linear regression and ridge regression, let me first divide the data into a training set and a testing set. I will be using the <code>Housing</code> data from the <code>{Ecdat}</code> package:
</p>
<pre class="r"><code>library(tidyverse)
library(Ecdat)
library(glmnet)</code></pre>
<pre class="r"><code>index &lt;- 1:nrow(Housing)

set.seed(12345)
train_index &lt;- sample(index, round(0.90*nrow(Housing)), replace = FALSE)

test_index &lt;- setdiff(index, train_index)

train_x &lt;- Housing[train_index, ] %&gt;% 
    select(-price)

train_y &lt;- Housing[train_index, ] %&gt;% 
    pull(price)

test_x &lt;- Housing[test_index, ] %&gt;% 
    select(-price)

test_y &lt;- Housing[test_index, ] %&gt;% 
    pull(price)</code></pre>
<p>
I do the train/test split this way, because <code>glmnet()</code> requires a design matrix as input, and not a formula. Design matrices can be created using the <code>model.matrix()</code> function:
</p>
<pre class="r"><code>train_matrix &lt;- model.matrix(train_y ~ ., data = train_x)

test_matrix &lt;- model.matrix(test_y ~ ., data = test_x)</code></pre>
<p>
To run an unpenalized linear regression, we can set the penalty to 0:
</p>
<pre class="r"><code>model_lm_ridge &lt;- glmnet(y = train_y, x = train_matrix, alpha = 0, lambda = 0)</code></pre>
<p>
The model above provides the same result as a linear regression. Let‚Äôs compare the coefficients between the two:
</p>
<pre class="r"><code>coef(model_lm_ridge)</code></pre>
<pre><code>## 13 x 1 sparse Matrix of class "dgCMatrix"
##                       s0
## (Intercept) -3247.030393
## (Intercept)     .       
## lotsize         3.520283
## bedrooms     1745.211187
## bathrms     14337.551325
## stories      6736.679470
## drivewayyes  5687.132236
## recroomyes   5701.831289
## fullbaseyes  5708.978557
## gashwyes    12508.524241
## aircoyes    12592.435621
## garagepl     4438.918373
## prefareayes  9085.172469</code></pre>
<p>
and now the coefficients of the linear regression (because I provide a design matrix, I have to use <code>lm.fit()</code> instead of <code>lm()</code> which requires a formula, not a matrix.)
</p>
<pre class="r"><code>coef(lm.fit(x = train_matrix, y = train_y))</code></pre>
<pre><code>##  (Intercept)      lotsize     bedrooms      bathrms      stories 
## -3245.146665     3.520357  1744.983863 14336.336858  6737.000410 
##  drivewayyes   recroomyes  fullbaseyes     gashwyes     aircoyes 
##  5686.394123  5700.210775  5709.493884 12509.005265 12592.367268 
##     garagepl  prefareayes 
##  4439.029607  9085.409155</code></pre>
<p>
as you can see, the coefficients are the same. Let‚Äôs compute the RMSE for the unpenalized linear regression:
</p>
<pre class="r"><code>preds_lm &lt;- predict(model_lm_ridge, test_matrix)

rmse_lm &lt;- sqrt(mean((preds_lm - test_y)^2))</code></pre>
<p>
The RMSE for the linear unpenalized regression is equal to 14463.08.
</p>
<p>
Let‚Äôs now run a ridge regression, with <code>lambda</code> equal to 100, and see if the RMSE is smaller:
</p>
<pre class="r"><code>model_ridge &lt;- glmnet(y = train_y, x = train_matrix, alpha = 0, lambda = 100)</code></pre>
<p>
and let‚Äôs compute the RMSE again:
</p>
<pre class="r"><code>preds &lt;- predict(model_ridge, test_matrix)

rmse &lt;- sqrt(mean((preds - test_y)^2))</code></pre>
<p>
The RMSE for the linear penalized regression is equal to 14460.71, which is smaller than before. But which value of <code>lambda</code> gives smallest RMSE? To find out, one must run model over a grid of <code>lambda</code> values and pick the model with lowest RMSE. This procedure is available in the <code>cv.glmnet()</code> function, which picks the best value for <code>lambda</code>:
</p>
<pre class="r"><code>best_model &lt;- cv.glmnet(train_matrix, train_y)
# lambda that minimises the MSE
best_model$lambda.min</code></pre>
<pre><code>## [1] 66.07936</code></pre>
<p>
According to <code>cv.glmnet()</code> the best value for <code>lambda</code> is 66.0793576. In the next section, we will implement cross validation ourselves, in order to find the hyper-parameters of a random forest.
</p>


</section>
</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-12-02-hyper-parameters.html</guid>
  <pubDate>Sun, 02 Dec 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>A tutorial on tidy cross-validation with R</title>
  <link>https://b-rodrigues.github.io/posts/2018-11-25-tidy_cv.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=7T6pgZdFLP0"> <img width="400" src="https://b-rodrigues.github.io/assets/img/cross_validation.gif" title="Visual representation of cross‚Åªvalidation inside your computer *click for virtual weed*"></a>
</p>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">
Introduction
</h2>
<p>
This blog posts will use several packages from the <a href="https://github.com/tidymodels"><code>{tidymodels}</code></a> collection of packages, namely <a href="https://tidymodels.github.io/recipes/"><code>{recipes}</code></a>, <a href="https://tidymodels.github.io/rsample/"><code>{rsample}</code></a> and <a href="https://tidymodels.github.io/parsnip/"><code>{parsnip}</code></a> to train a random forest the tidy way. I will also use <a href="http://mlrmbo.mlr-org.com/"><code>{mlrMBO}</code></a> to tune the hyper-parameters of the random forest.
</p>
</section>
<section id="set-up" class="level2">
<h2 class="anchored" data-anchor-id="set-up">
Set up
</h2>
<p>
Let‚Äôs load the needed packages:
</p>
<pre class="r"><code>library("tidyverse")
library("tidymodels")
library("parsnip")
library("brotools")
library("mlbench")</code></pre>
<p>
Load the data, included in the <code>{mlrbench}</code> package:
</p>
<pre class="r"><code>data("BostonHousing2")</code></pre>
<p>
I will train a random forest to predict the housing price, which is the <code>cmedv</code> column:
</p>
<pre class="r"><code>head(BostonHousing2)</code></pre>
<pre><code>##         town tract      lon     lat medv cmedv    crim zn indus chas   nox
## 1     Nahant  2011 -70.9550 42.2550 24.0  24.0 0.00632 18  2.31    0 0.538
## 2 Swampscott  2021 -70.9500 42.2875 21.6  21.6 0.02731  0  7.07    0 0.469
## 3 Swampscott  2022 -70.9360 42.2830 34.7  34.7 0.02729  0  7.07    0 0.469
## 4 Marblehead  2031 -70.9280 42.2930 33.4  33.4 0.03237  0  2.18    0 0.458
## 5 Marblehead  2032 -70.9220 42.2980 36.2  36.2 0.06905  0  2.18    0 0.458
## 6 Marblehead  2033 -70.9165 42.3040 28.7  28.7 0.02985  0  2.18    0 0.458
##      rm  age    dis rad tax ptratio      b lstat
## 1 6.575 65.2 4.0900   1 296    15.3 396.90  4.98
## 2 6.421 78.9 4.9671   2 242    17.8 396.90  9.14
## 3 7.185 61.1 4.9671   2 242    17.8 392.83  4.03
## 4 6.998 45.8 6.0622   3 222    18.7 394.63  2.94
## 5 7.147 54.2 6.0622   3 222    18.7 396.90  5.33
## 6 6.430 58.7 6.0622   3 222    18.7 394.12  5.21</code></pre>
<p>
Only keep relevant columns:
</p>
<pre class="r"><code>boston &lt;- BostonHousing2 %&gt;% 
    select(-medv, -town, -lon, -lat) %&gt;% 
    rename(price = cmedv)</code></pre>
<p>
I remove <code>town</code>, <code>lat</code> and <code>lon</code> because the information contained in the column <code>tract</code> is enough.
</p>
<p>
To train and evaluate the model‚Äôs performance, I split the data in two. One data set, which I call the training set, will be further split into two down below. I won‚Äôt touch the second data set, the test set, until the very end.
</p>
<pre class="r"><code>train_test_split &lt;- initial_split(boston, prop = 0.9)

housing_train &lt;- training(train_test_split)

housing_test &lt;- testing(train_test_split)</code></pre>
<p>
I want to train a random forest to predict price of houses, but random forests have so-called hyperparameters, which are parameters that cannot be estimated, or learned, from the data. Instead, these parameters have to be chosen by the analyst. In order to choose them, you can use values from the literature that seemed to have worked well (like is done in Macro-econometrics) or you can further split the train set into two, create a grid of hyperparameter, train the model on one part of the data for all values of the grid, and compare the predictions of the models on the second part of the data. You then stick with the model that performed the best, for example, the model with lowest RMSE. The thing is, you can‚Äôt estimate the true value of the RMSE with only one value. It‚Äôs like if you wanted to estimate the height of the population by drawing one single observation from the population. You need a bit more observations. To approach the true value of the RMSE for a give set of hyperparameters, instead of doing one split, I‚Äôll do 30. I then compute the average RMSE, which implies training 30 models for each combination of the values of the hyperparameters I am interested in.
</p>
<p>
First, let‚Äôs split the training data again, using the <code>mc_cv()</code> function from <code>{rsample}</code> package. This function implements Monte Carlo cross-validation:
</p>
<pre class="r"><code>validation_data &lt;- mc_cv(housing_train, prop = 0.9, times = 30)</code></pre>
<p>
What does <code>validation_data</code> look like?
</p>
<pre class="r"><code>validation_data</code></pre>
<pre><code>## # # Monte Carlo cross-validation (0.9/0.1) with 30 resamples  
## # A tibble: 30 x 2
##    splits           id        
##    &lt;list&gt;           &lt;chr&gt;     
##  1 &lt;split [411/45]&gt; Resample01
##  2 &lt;split [411/45]&gt; Resample02
##  3 &lt;split [411/45]&gt; Resample03
##  4 &lt;split [411/45]&gt; Resample04
##  5 &lt;split [411/45]&gt; Resample05
##  6 &lt;split [411/45]&gt; Resample06
##  7 &lt;split [411/45]&gt; Resample07
##  8 &lt;split [411/45]&gt; Resample08
##  9 &lt;split [411/45]&gt; Resample09
## 10 &lt;split [411/45]&gt; Resample10
## # ‚Ä¶ with 20 more rows</code></pre>
<p>
Let‚Äôs look further down:
</p>
<pre class="r"><code>validation_data$splits[[1]]</code></pre>
<pre><code>## &lt;411/45/456&gt;</code></pre>
<p>
The first value is the number of rows of the first set, the second value of the second, and the third was the original amount of values in the training data, before splitting again.
</p>
<p>
How should we call these two new data sets? The author of <code>{rsample}</code>, Max Kuhn, talks about the <em>analysis</em> and the <em>assessment</em> sets:
</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
rsample convention for now but I intend on using it everywhere. Reusing training and testing is insane.
</p>
‚Äî Max Kuhn (<span class="citation" data-cites="topepos">@topepos</span>) <a href="https://twitter.com/topepos/status/1066131042615140353?ref_src=twsrc%5Etfw">November 24, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>
Now, in order to continue I need pre-process the data. I will do this in three steps. The first and the second step are used to center and scale the numeric variables and the third step converts character and factor variables to dummy variables. This is needed because I will train a random forest, which cannot handle factor variables directly. Let‚Äôs define a recipe to do that, and start by pre-processing the testing set. I write a wrapper function around the recipe, because I will need to apply this recipe to various data sets:
</p>
<pre class="r"><code>simple_recipe &lt;- function(dataset){
    recipe(price ~ ., data = dataset) %&gt;%
        step_center(all_numeric()) %&gt;%
        step_scale(all_numeric()) %&gt;%
        step_dummy(all_nominal())
}</code></pre>
<p>
Once the recipe is defined, I can use the <code>prep()</code> function, which estimates the parameters from the data which are needed to process the data. For example, for centering, <code>prep()</code> estimates the mean which will then be subtracted from the variables. With <code>bake()</code> the estimates are then applied on the data:
</p>
<pre class="r"><code>testing_rec &lt;- prep(simple_recipe(housing_test), testing = housing_test)

test_data &lt;- bake(testing_rec, newdata = housing_test)</code></pre>
<pre><code>## Warning: Please use `new_data` instead of `newdata` with `bake`. 
## In recipes versions &gt;= 0.1.4, this will cause an error.</code></pre>
<p>
It is important to split the data before using <code>prep()</code> and <code>bake()</code>, because if not, you will use observations from the test set in the <code>prep()</code> step, and thus introduce knowledge from the test set into the training data. This is called data leakage, and must be avoided. This is why it is necessary to first split the training data into an analysis and an assessment set, and then also pre-process these sets separately. However, the <code>validation_data</code> object cannot now be used with <code>recipe()</code>, because it is not a dataframe. No worries, I simply need to write a function that extracts the analysis and assessment sets from the <code>validation_data</code> object, applies the pre-processing, trains the model, and returns the RMSE. This will be a big function, at the center of the analysis.
</p>
<p>
But before that, let‚Äôs run a simple linear regression, as a benchmark. For the linear regression, I will not use any CV, so let‚Äôs pre-process the training set:
</p>
<pre class="r"><code>trainlm_rec &lt;- prep(simple_recipe(housing_train), testing = housing_train)

trainlm_data &lt;- bake(trainlm_rec, newdata = housing_train)</code></pre>
<pre><code>## Warning: Please use `new_data` instead of `newdata` with `bake`. 
## In recipes versions &gt;= 0.1.4, this will cause an error.</code></pre>
<pre class="r"><code>linreg_model &lt;- lm(price ~ ., data = trainlm_data)

broom::augment(linreg_model, newdata = test_data) %&gt;% 
    rmse(price, .fitted)</code></pre>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard       0.438</code></pre>
<p>
<code>broom::augment()</code> adds the predictions to the <code>test_data</code> in a new column, <code>.fitted</code>. I won‚Äôt use this trick with the random forest, because there is no <code>augment()</code> method for random forests from the <code>{ranger}</code> which I‚Äôll use. I‚Äôll add the predictions to the data myself.
</p>
<p>
Ok, now let‚Äôs go back to the random forest and write the big function:
</p>
<pre class="r"><code>my_rf &lt;- function(mtry, trees, split, id){
    
    analysis_set &lt;- analysis(split)
    
    analysis_prep &lt;- prep(simple_recipe(analysis_set), training = analysis_set)
    
    analysis_processed &lt;- bake(analysis_prep, newdata = analysis_set)
    
    model &lt;- rand_forest(mtry = mtry, trees = trees) %&gt;%
        set_engine("ranger", importance = 'impurity') %&gt;%
        fit(price ~ ., data = analysis_processed)

    assessment_set &lt;- assessment(split)
    
    assessment_prep &lt;- prep(simple_recipe(assessment_set), testing = assessment_set)
    
    assessment_processed &lt;- bake(assessment_prep, newdata = assessment_set)

    tibble::tibble("id" = id,
        "truth" = assessment_processed$price,
        "prediction" = unlist(predict(model, new_data = assessment_processed)))
}</code></pre>
<p>
The <code>rand_forest()</code> function is available from the <code>{parsnip}</code> package. This package provides an unified interface to a lot of other machine learning packages. This means that instead of having to learn the syntax of <code>range()</code> and <code>randomForest()</code> and, and‚Ä¶ you can simply use the <code>rand_forest()</code> function and change the <code>engine</code> argument to the one you want (<code>ranger</code>, <code>randomForest</code>, etc).
</p>
<p>
Let‚Äôs try this function:
</p>
<pre class="r"><code>results_example &lt;- map2_df(.x = validation_data$splits,
                           .y = validation_data$id,
                           ~my_rf(mtry = 3, trees = 200, split = .x, id = .y))</code></pre>
<pre class="r"><code>head(results_example)</code></pre>
<pre><code>## # A tibble: 6 x 3
##   id           truth prediction
##   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;
## 1 Resample01  0.0235    -0.104 
## 2 Resample01 -0.135     -0.0906
## 3 Resample01 -0.378     -0.158 
## 4 Resample01 -0.232      0.0623
## 5 Resample01 -0.0859     0.0173
## 6 Resample01  0.169      0.303</code></pre>
<p>
I can now compute the RMSE when <code>mtry</code> = 3 and <code>trees</code> = 200:
</p>
<pre class="r"><code>results_example %&gt;%
    group_by(id) %&gt;%
    rmse(truth, prediction) %&gt;%
    summarise(mean_rmse = mean(.estimate)) %&gt;%
    pull</code></pre>
<pre><code>## [1] 0.4319164</code></pre>
<p>
The random forest has already lower RMSE than the linear regression. The goal now is to lower this RMSE by tuning the <code>mtry</code> and <code>trees</code> hyperparameters. For this, I will use Bayesian Optimization methods implemented in the <code>{mlrMBO}</code> package.
</p>
</section>
<section id="bayesian-hyperparameter-optimization" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-hyperparameter-optimization">
Bayesian hyperparameter optimization
</h2>
<p>
I will re-use the code from above, and define a function that does everything from pre-processing to returning the metric I want to minimize by tuning the hyperparameters, the RMSE:
</p>
<pre class="r"><code>tuning &lt;- function(param, validation_data){

    mtry &lt;- param[1]
    trees &lt;- param[2]

    results &lt;- purrr::map2_df(.x = validation_data$splits,
                       .y = validation_data$id,
                       ~my_rf(mtry = mtry, trees = trees, split = .x, id = .y))

    results %&gt;%
        group_by(id) %&gt;%
        rmse(truth, prediction) %&gt;%
        summarise(mean_rmse = mean(.estimate)) %&gt;%
        pull
}</code></pre>
<p>
This is exactly the code from before, but it now returns the RMSE. Let‚Äôs try the function with the values from before:
</p>
<pre class="r"><code>tuning(c(3, 200), validation_data)</code></pre>
<pre><code>## [1] 0.4330951</code></pre>
<p>
Let‚Äôs also plot the value of RMSE for <code>mtry = 3</code> and <code>trees</code> from 200 to 300. This takes some time, because I need to evaluate this costly function 100 times. If evaluating the function was cheap, I could have made a 3D plot by varying values of <code>mtry</code> too, but then again if evaluating the function was cheap, I would run an exhaustive grid search to find the hyperparameters instead of using Bayesian optimization.
</p>
<pre class="r"><code>plot_points &lt;- crossing("mtry" = 3, "trees" = seq(200, 300))

plot_data &lt;- plot_points %&gt;% 
    mutate(value = map_dbl(seq(200, 300), ~tuning(c(3, .), validation_data)))</code></pre>
<pre class="r"><code>plot_data %&gt;% 
    ggplot(aes(y = value, x = trees)) + 
    geom_line(colour = "#82518c") + 
    theme_blog() +
    ggtitle("RMSE for mtry = 3")</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/tidy_cv-21-1.png" width="672">
</p>
<p>
For <code>mtry = 3</code> the minimum seems to lie around 255. The function to minimize is not smooth at all.
</p>
<p>
I now follow the code that can be found in the <a href="https://arxiv.org/abs/1703.03373">arxiv</a> paper to run the optimization. I think I got the gist of the paper, but I did not understand everything yet. For now, I am still experimenting with the library at the moment, but from what I understand, a simpler model, called the surrogate model, is used to look for promising points and to evaluate the value of the function at these points. This seems somewhat similar (in spirit) to the <em>Indirect Inference</em> method as described in <a href="https://www.jstor.org/stable/2285076">Gourieroux, Monfort, Renault</a>.
</p>
<p>
Let‚Äôs first load the package and create the function to optimize:
</p>
<pre class="r"><code>library("mlrMBO")</code></pre>
<pre class="r"><code>fn &lt;- makeSingleObjectiveFunction(name = "tuning",
                                 fn = tuning,
                                 par.set = makeParamSet(makeIntegerParam("x1", lower = 3, upper = 8),
                                                        makeIntegerParam("x2", lower = 50, upper = 500)))</code></pre>
<p>
This function is based on the function I defined before. The parameters to optimize are also defined as are their bounds. I will look for <code>mtry</code> between the values of 3 and 8, and <code>trees</code> between 50 and 500.
</p>
<p>
Now comes the part I didn‚Äôt quite get.
</p>
<pre class="r"><code># Create initial random Latin Hypercube Design of 10 points
library(lhs)# for randomLHS
des &lt;- generateDesign(n = 5L * 2L, getParamSet(fn), fun = randomLHS)</code></pre>
<p>
I think this means that these 10 points are the points used to start the whole process. I did not understand why they have to be sampled from a hypercube, but ok. Then I choose the surrogate model, a random forest too, and predict the standard error. Here also, I&nbsp;did not quite get why the standard error can be an option.
</p>
<pre class="r"><code># Specify kriging model with standard error estimation
surrogate &lt;- makeLearner("regr.ranger", predict.type = "se", keep.inbag = TRUE)</code></pre>
<p>
Here I define some options:
</p>
<pre class="r"><code># Set general controls
ctrl &lt;- makeMBOControl()
ctrl &lt;- setMBOControlTermination(ctrl, iters = 10L)
ctrl &lt;- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())</code></pre>
<p>
And this is the optimization part:
</p>
<pre class="r"><code># Start optimization
result &lt;- mbo(fn, des, surrogate, ctrl, more.args = list("validation_data" = validation_data))</code></pre>
<pre class="r"><code>result</code></pre>
<pre><code>## Recommended parameters:
## x1=6; x2=381
## Objective: y = 0.393
## 
## Optimization path
## 10 + 10 entries in total, displaying last 10 (or less):
##    x1  x2         y dob eol error.message exec.time            ei
## 11  6 370 0.3943479   1  NA          &lt;NA&gt;     8.913 -3.134568e-05
## 12  6 362 0.3950402   2  NA          &lt;NA&gt;     8.844 -2.987934e-05
## 13  6 373 0.3939587   3  NA          &lt;NA&gt;     8.939 -2.259674e-05
## 14  6 394 0.3962875   4  NA          &lt;NA&gt;     9.342 -7.427682e-06
## 15  6 368 0.3944954   5  NA          &lt;NA&gt;     8.760 -4.121337e-06
## 16  6 378 0.3938796   6  NA          &lt;NA&gt;     8.949 -4.503591e-07
## 17  6 381 0.3934176   7  NA          &lt;NA&gt;     9.109 -1.141853e-06
## 18  6 380 0.3948077   8  NA          &lt;NA&gt;     9.026 -4.718394e-08
## 19  6 381 0.3932636   9  NA          &lt;NA&gt;     9.022 -9.801395e-08
## 20  6 383 0.3953004  10  NA          &lt;NA&gt;     9.184 -1.579619e-09
##    error.model train.time prop.type propose.time           se      mean
## 11        &lt;NA&gt;      0.014 infill_ei        0.449 0.0010924600 0.3955131
## 12        &lt;NA&gt;      0.012 infill_ei        0.458 0.0007415920 0.3948705
## 13        &lt;NA&gt;      0.012 infill_ei        0.460 0.0006116756 0.3947185
## 14        &lt;NA&gt;      0.012 infill_ei        0.729 0.0003104694 0.3943572
## 15        &lt;NA&gt;      0.023 infill_ei        0.444 0.0003446061 0.3945085
## 16        &lt;NA&gt;      0.013 infill_ei        0.458 0.0002381887 0.3944642
## 17        &lt;NA&gt;      0.013 infill_ei        0.492 0.0002106454 0.3943200
## 18        &lt;NA&gt;      0.013 infill_ei        0.516 0.0002093524 0.3940764
## 19        &lt;NA&gt;      0.014 infill_ei        0.756 0.0002481260 0.3941597
## 20        &lt;NA&gt;      0.013 infill_ei        0.483 0.0001687982 0.3939285</code></pre>
<p>
So the recommended parameters are 6 for <code>mtry</code> and 381 for <code>trees</code>. The value of the RMSE is lower than before, and equals 0.393. Let‚Äôs now train the random forest on the training data with this values. First, I pre-process the training data:
</p>
<pre class="r"><code>training_rec &lt;- prep(simple_recipe(housing_train), testing = housing_train)

train_data &lt;- bake(training_rec, newdata = housing_train)</code></pre>
<pre><code>## Warning: Please use `new_data` instead of `newdata` with `bake`. 
## In recipes versions &gt;= 0.1.4, this will cause an error.</code></pre>
<p>
Let‚Äôs now train our final model and predict the prices:
</p>
<pre class="r"><code>final_model &lt;- rand_forest(mtry = 6, trees = 381) %&gt;%
        set_engine("ranger", importance = 'impurity') %&gt;%
        fit(price ~ ., data = train_data)

price_predict &lt;- predict(final_model, new_data = select(test_data, -price))</code></pre>
<p>
Let‚Äôs transform the data back and compare the predicted prices to the true ones visually:
</p>
<pre class="r"><code>cbind(price_predict * sd(housing_train$price) + mean(housing_train$price), 
      housing_test$price)</code></pre>
<pre><code>##        .pred housing_test$price
## 1  34.811111               34.7
## 2  20.591304               22.9
## 3  19.463920               18.9
## 4  20.321990               21.7
## 5  19.063132               17.5
## 6  15.969125               14.5
## 7  18.203023               15.6
## 8  17.139943               13.9
## 9  21.393329               24.2
## 10 27.508482               25.0
## 11 24.030162               24.1
## 12 21.222857               21.2
## 13 23.052677               22.2
## 14 20.303233               19.3
## 15 21.134554               21.7
## 16 22.913097               18.5
## 17 20.029506               18.8
## 18 18.045923               16.2
## 19 17.321006               13.3
## 20 18.201785               13.4
## 21 29.928316               32.5
## 22 24.339983               26.4
## 23 45.518316               42.3
## 24 29.551251               26.7
## 25 26.513473               30.1
## 26 42.984738               46.7
## 27 43.513001               48.3
## 28 25.436146               23.3
## 29 21.766247               24.3
## 30 36.328740               36.0
## 31 32.830061               31.0
## 32 38.736098               35.2
## 33 31.573311               32.0
## 34 19.847848               19.4
## 35 23.401032               23.1
## 36 22.000914               19.4
## 37 20.155696               18.7
## 38 21.342003               22.6
## 39 20.846330               19.9
## 40 13.752108               13.8
## 41 12.499064               13.1
## 42 15.019987               16.3
## 43  8.489851                7.2
## 44  7.803981               10.4
## 45 18.629488               20.8
## 46 14.645669               14.3
## 47 15.094423               15.2
## 48 20.470057               17.7
## 49 15.147170               13.3
## 50 15.880035               13.6</code></pre>
<p>
Let‚Äôs now compute the RMSE:
</p>
<pre class="r"><code>tibble::tibble("truth" = test_data$price,
        "prediction" = unlist(price_predict)) %&gt;% 
    rmse(truth, prediction)</code></pre>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard       0.327</code></pre>
<p>
Very nice.
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-11-25-tidy_cv.html</guid>
  <pubDate>Sun, 25 Nov 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The best way to visit Luxembourguish castles is doing data science + combinatorial optimization</title>
  <link>https://b-rodrigues.github.io/posts/2018-11-21-lux_castle.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=XQDm6I3mbMU"> <img width="400" src="https://b-rodrigues.github.io/assets/img/harold_kumar.jpg" title="Only 00's kids will get the reference"></a>
</p>
</div>
<p>
Inspired by David Schoch‚Äôs blog post, <a href="http://blog.schochastics.net/post/traveling-beerdrinker-problem/">Traveling Beerdrinker Problem</a>. Check out his blog, he has some amazing posts!
</p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">
Introduction
</h2>
<p>
Luxembourg, as any proper European country, is full of castles. According to Wikipedia,
</p>
<p>
‚ÄúBy some optimistic estimates, there are as many as 130 castles in Luxembourg but more realistically there are probably just over a hundred, although many of these could be considered large residences or manor houses rather than castles‚Äù.
</p>
<p>
I see the editors are probably German or French, calling our castles <em>manor houses</em>! They only say that because Luxembourg is small, so our castles must be small too, right?
</p>
<p>
Banter aside, with that many castles, what is the best way to visit them all? And by best way I mean shortest way. This is a classical <strong>Travelling salesman problem</strong>. To solve this, I need the following elements:
</p>
<ul>
<li>
A list of castles to visit, with their coordinates
</li>
<li>
The distances between these castles to each other
</li>
<li>
A program to solve the TSP
</li>
</ul>
<p>
Let‚Äôs start by loading some packages:
</p>
<pre class="r"><code>library(tidyverse)
library(magrittr)
library(rvest)
library(curl)
library(brotools)
library(RJSONIO)
library(TSP)
library(ggimage)</code></pre>
<p>
First step; scrape the data.
</p>
</section>
<section id="scraping-the-data-thats-the-data-science-part" class="level2">
<h2 class="anchored" data-anchor-id="scraping-the-data-thats-the-data-science-part">
Scraping the data (that‚Äôs the data science part)
</h2>
<p>
Let‚Äôs start by having a list of castles. For this, I&nbsp;go to the French Wikipedia page of <a href="https://fr.wikipedia.org/wiki/Liste_de_ch%C3%A2teaux_luxembourgeois">Luxembourguish castles</a>.
</p>
<p>
The Luxembourguish page is more <a href="https://lb.wikipedia.org/wiki/L%C3%ABscht_vun_de_L%C3%ABtzebuerger_Buergen_a_Schl%C3%A4sser">exhaustive</a>, but the names are in Luxembourguish, and I&nbsp;doubt that OpenStreetMap, which I‚Äôll use to get the coordinates, understands Luxembourguish.
</p>
<p>
This list has around 50 castles, a reasonable amount of castles. Scraping the table is quite easy:
</p>
<pre class="r"><code>page &lt;- read_html("https://fr.wikipedia.org/wiki/Liste_de_ch%C3%A2teaux_luxembourgeois")

castles &lt;- page %&gt;%
    html_node(".wikitable") %&gt;%
    html_table(fill = TRUE) %&gt;%
    select(Nom, Localit√©) %&gt;%
    mutate(query = paste0(Nom, ", ", Localit√©))</code></pre>
<p>
I also add a <code>query</code> column which concatenates the name of the castle (‚ÄúNom‚Äù) to where it is found (‚ÄúLocalit√©‚Äù). The query should be a better choice that simply the castle name to get the coordinates.
</p>
<p>
Now, I need to add the coordinates to this data frame. For this, I use a function I&nbsp;found online that gets the coordinates from OpenStreetMap:
</p>
<pre class="r"><code>## geocoding function using OSM Nominatim API
## details: http://wiki.openstreetmap.org/wiki/Nominatim
## made by: D.Kisler

#https://datascienceplus.com/osm-nominatim-with-r-getting-locations-geo-coordinates-by-its-address/

nominatim_osm &lt;- function(address = NULL){
    if(suppressWarnings(is.null(address)))
        return(data.frame())
    tryCatch(
        d &lt;- jsonlite::fromJSON(
            gsub('\\@addr\\@', gsub('\\s+', '\\%20', address),
                 'http://nominatim.openstreetmap.org/search/@addr@?format=json&amp;addressdetails=0&amp;limit=1')
        ), error = function(c) return(data.frame())
    )
    if(length(d) == 0) return(data.frame())
    return(data.frame(lon = as.numeric(d$lon), lat = as.numeric(d$lat)))
}</code></pre>
<p>
I can now easily add the coordinates by mapping the <code>nominatim_osm()</code> function to the <code>query</code> column I built before:
</p>
<pre class="r"><code>castles_osm &lt;- castles %&gt;%
    mutate(geolocation = map(query, nominatim_osm))</code></pre>
<p>
Let‚Äôs take a look at <code>castles_osm</code>:
</p>
<pre class="r"><code>head(castles_osm)</code></pre>
<pre><code>##                            Nom    Localit√©
## 1         Ch√¢teau d'Ansembourg  Ansembourg
## 2 Nouveau Ch√¢teau d'Ansembourg  Ansembourg
## 3             Ch√¢teau d'Aspelt      Aspelt
## 4          Ch√¢teau de Beaufort    Beaufort
## 5            Ch√¢teau de Beggen Dommeldange
## 6       Ch√¢teau de Colmar-Berg Colmar-Berg
##                                      query         geolocation
## 1         Ch√¢teau d'Ansembourg, Ansembourg 6.046748, 49.700693
## 2 Nouveau Ch√¢teau d'Ansembourg, Ansembourg   6.04760, 49.70085
## 3                 Ch√¢teau d'Aspelt, Aspelt 6.222653, 49.524822
## 4            Ch√¢teau de Beaufort, Beaufort 2.757293, 43.297466
## 5           Ch√¢teau de Beggen, Dommeldange 6.137765, 49.643383
## 6      Ch√¢teau de Colmar-Berg, Colmar-Berg 6.087944, 49.814687</code></pre>
<p>
I now clean the data. There were several mistakes or castles that were not found, which I&nbsp;added manually. I did not notice these mistakes immediately, but when I computed the distances matrix I notices several inconsistencies; 0‚Äôs in positions other than the diagonal, as well as NAs. So I went back to the raw data and corrected what was wrong, this time by looking at Google Maps. Thankfully there were not that many mistakes. Below the whole workflow:
</p>
<pre class="r"><code># Little helper function to clean the lon and lat columns
extract_numbers &lt;- function(string){
    str_extract_all(string, "\\d+", simplify = TRUE) %&gt;%
        paste0(collapse = ".")
}

castles &lt;- castles_osm %&gt;%
    mutate(geolocation = 
               ifelse(Nom == "Ch√¢teau de Wintrange", "6.3517223, 49.5021975", geolocation)) %&gt;%
    mutate(geolocation = 
               ifelse(Nom == "Septfontaines, Rollingergrund", "6.1028634, 49.6257147", geolocation)) %&gt;%
    mutate(geolocation = 
               ifelse(Nom == "Ch√¢teau de Septfontaines", "5.9617443, 49.7006292", geolocation)) %&gt;%
    mutate(geolocation = 
               ifelse(Nom == "Ch√¢teau de Senningen", "6.2342581, 49.6464632", geolocation)) %&gt;%
    mutate(geolocation = 
               ifelse(Nom == "Ch√¢teau de Schauwenburg", "6.0478341, 49.6110245", geolocation)) %&gt;%
    mutate(geolocation = 
               ifelse(Nom == "Ch√¢teau de Schuttbourg", "5.8980951, 49.7878706", geolocation)) %&gt;%
    mutate(geolocation = 
               ifelse(Nom == "Ch√¢teau de Meysembourg", "6.1864882, 49.7704348", geolocation)) %&gt;%
    mutate(geolocation = 
               ifelse(Nom == "Ch√¢teau de Mamer", "6.0232432, 49.6262397", geolocation)) %&gt;%
    mutate(geolocation = 
               ifelse(Nom == "Ch√¢teau de Born", "6.5125214, 49.7611168", geolocation)) %&gt;%
    # Found chateau de Betzdorf in Germany, not Luxembourg:
    mutate(geolocation = ifelse(Nom == "Ch√¢teau Betzdorf", "6.330278, 49.694167", geolocation)) %&gt;%
    # Found ch√¢teau de Clemency in France, not Luxembourg:
    mutate(geolocation = ifelse(Nom == "Ch√¢teau de Clemency", "5.874167, 49.598056", geolocation)) %&gt;%
    separate(geolocation, into = c("lon", "lat"), sep = ",") %&gt;%
    filter(!is.na(lat)) %&gt;%
    mutate(lon = map(lon, extract_numbers)) %&gt;%
    mutate(lat = map(lat, extract_numbers)) %&gt;%
    # Ch√¢teau de Beaufort found is in southern France, not the one in lux
    # Ch√¢teau de Dudelange is wrong (same as Bettembourg)
    # Ch√¢teau de P√©tange is wrong (same as Differdange)
    # Ch√¢teau d'Urspelt is wrong (same as Clervaux)
    # Ch√¢teau d'Hesperange is wrong (same as Palais Grand-Ducal)
    mutate(lon = ifelse(Nom == "Ch√¢teau de Beaufort", "6.2865176", lon),
           lat = ifelse(Nom == "Ch√¢teau de Beaufort", "49.8335306", lat)) %&gt;%
    mutate(lon = ifelse(Nom == "Ch√¢teau Dudelange", "6.0578438", lon),
           lat = ifelse(Nom == "Ch√¢teau Dudelange", "49.4905049", lat)) %&gt;%
    mutate(lon = ifelse(Nom == "Ch√¢teau de P√©tange", "6.105703", lon),
           lat = ifelse(Nom == "Ch√¢teau de P√©tange", "49.7704746", lat)) %&gt;%
    mutate(lon = ifelse(Nom == "Ch√¢teau d' Urspelt", "6.043375", lon),
           lat = ifelse(Nom == "Ch√¢teau d' Urspelt", "50.075342", lat)) %&gt;%
    mutate(lon = ifelse(Nom == "Ch√¢teau d'Hesperange", "6.1524302", lon),
           lat = ifelse(Nom == "Ch√¢teau d'Hesperange", "49.573071", lat)) %&gt;%
    mutate(latlon = paste0(lat, ",", lon)) %&gt;%
    mutate(lon = as.numeric(lon), lat = as.numeric(lat))</code></pre>
<p>
In the end, I have 48 castles, 2 of them were not found neither by OpenStreetMap nor Google Maps.
</p>
<p>
Now I can get the distances matrix. For this, I opened an account at <a href="https://www.graphhopper.com/">Graphhopper</a> and used their <a href="https://graphhopper.com/api/1/docs/matrix/#matrix-api">Matrix API</a>. When you open a free account, you get a standard account for free for two weeks, which was perfect for this little exercise.
</p>
<p>
To use the Matrix API you can make a call with curl from your terminal, like this:
</p>
<pre><code>curl "https://graphhopper.com/api/1/matrix?point=49.932707,11.588051&amp;point=50.241935,10.747375&amp;point=50.118817,11.983337&amp;type=json&amp;vehicle=car&amp;debug=true&amp;out_array=weights&amp;out_array=times&amp;out_array=distances&amp;key=[YOUR_KEY]"</code></pre>
<p>
To use this from R, I use the <code>{curl}</code> package and the <code>curl_download()</code> function to download and write the output to disk.
</p>
<p>
I built the url like this. First, the ‚Äúpoints‚Äù part:
</p>
<pre class="r"><code>points &lt;- paste(castles$latlon, collapse = "&amp;point=")</code></pre>
<details>
<p>
</p><summary>
Click if you want to see the ‚Äúpoints‚Äù string
</summary>
<p></p>
<pre class="r"><code>points</code></pre>
<pre><code>## [1] "49.70069265,6.04674779400653&amp;point=49.7008533,6.04759957386294&amp;point=49.5248216,6.2226525964455&amp;point=49.8335306,6.2865176&amp;point=49.64338295,6.1377647435619&amp;point=49.8146867,6.08794389490417&amp;point=49.5749356,5.9841033&amp;point=49.5173197,6.09641390513718&amp;point=49.8760687,6.22027097982788&amp;point=49.694167,6.330278&amp;point=49.7611168,6.5125214&amp;point=49.70256665,6.21740997690437&amp;point=49.905581,6.07950107769784&amp;point=49.9127745,6.13764166375989&amp;point=49.598056,5.874167&amp;point=50.0544533,6.03028463135369&amp;point=49.75943095,5.82586812555896&amp;point=49.52132545,5.88917535225117&amp;point=49.6345518,6.1386377&amp;point=49.4905049,6.0578438&amp;point=49.8600716,6.11163732377525&amp;point=49.9110418,5.93440053120085&amp;point=49.7475976,6.18681116161273&amp;point=49.61092115,6.13288873913352&amp;point=49.573071,6.1524302&amp;point=49.71207855,6.05156617599082&amp;point=49.6694157,5.9496767&amp;point=49.7704143,6.18888954785334&amp;point=49.6262397,6.0232432&amp;point=49.7478579,6.10315847283333&amp;point=49.7704348,6.1864882&amp;point=49.6328906,6.25941956000154&amp;point=49.7704746,6.105703&amp;point=49.54325715,5.9262570638974&amp;point=49.470114,6.3658507&amp;point=49.719675,6.09334070925783&amp;point=49.7878706,5.8980951&amp;point=49.6110245,6.0478341&amp;point=49.6464632,6.2342581&amp;point=49.7006292,5.9617443&amp;point=49.6257147,6.1028634&amp;point=49.556964,6.380786&amp;point=50.075342,6.043375&amp;point=49.7682266,5.9803414&amp;point=49.9348908,6.20279648757301&amp;point=49.6604088,6.1337864&amp;point=49.9664662,5.93854270968922&amp;point=49.5021975,6.3517223"</code></pre>
</details>
<p>
Then, I added my key, and pasted these elements together to form the correct url:
</p>
<pre class="r"><code>my_key &lt;- "my_key_was_here"

url &lt;- paste0("https://graphhopper.com/api/1/matrix?point=", points, "&amp;type=json&amp;vehicle=car&amp;debug=true&amp;out_array=weights&amp;out_array=times&amp;out_array=distances&amp;key=", my_key)</code></pre>
<p>
Then, I get the matrix like this:
</p>
<pre class="r"><code>castles_dist &lt;- "distances_graphhopper.json"
curl_download(url, castles_dist)</code></pre>
<p>
Let‚Äôs take a look at the object:
</p>
<pre class="r"><code>distances &lt;- castles_dist$distances</code></pre>
<details>
<p>
</p><summary>
Click if you want to see the distance object
</summary>
<p></p>
<pre class="r"><code>distances</code></pre>
<pre><code>## [[1]]
##  [1]     0    48 46364 38416 16619 20387 19617 31990 31423 46587 60894
## [12] 19171 36961 30701 25734 52929 22843 42618 18138 40015 24860 39395
## [23] 17163 18938 28107  2570 10882 16888 12302  9350 16599 32025 14369
## [34] 40780 56004  6069 17602 16112 31552  8180 14523 49431 53199 13354
## [45] 43769 15868 46237 53617
## 
## [[2]]
##  [1]    48     0 46412 38464 16667 20435 19665 32038 31471 46635 60942
## [12] 19219 37009 30749 25781 52977 22890 42665 18186 40063 24908 39443
## [23] 17211 18986 28155  2618 10930 16936 12350  9398 16647 32073 14417
## [34] 40828 56052  6116 17650 16160 31599  8228 14571 49478 53247 13402
## [45] 43817 15916 46285 53665
## 
## [[3]]
##  [1] 46900 46947     0 48698 30281 45548 30424 17056 56584 31187 52215
## [12] 28799 62122 55862 39130 78090 66375 33585 23961 21009 50021 64556
## [23] 35740 25853 10852 49283 43218 43052 37317 36283 42763 17250 39530
## [34] 31748 14513 33919 60798 31885 22872 44629 37023 14605 78360 44602
## [45] 68930 26320 71398 12126
## 
## [[4]]
##  [1] 38214 38261 48754     0 34949 23582 55661 48848 11274 29579 26880
## [12] 25631 32853 20633 67818 46577 47882 65319 33355 58499 26540 40460
## [23] 24359 38061 43577 37674 61661 21704 55760 29822 25030 36698 27956
## [34] 63482 72862 32945 42596 50328 34302 42495 38740 46782 46847 35141
## [45] 20752 33520 47302 56789
## 
## [[5]]
##  [1] 16494 16541 25311 35192     0 25375 19477 16687 36411 20939 42124
## [12] 13107 41949 35689 27116 57917 44116 30670  1432 26337 29848 44383
## [23] 18673  5767 11224 18877 20958 23922 15058 16110 23633 13255 19357
## [34] 28833 40700 15310 30392 12024 12782 20050  7178 30661 58187 24429
## [45] 48757  2511 51225 33346
## 
## [[6]]
##  [1] 18468 18516 43459 23632 29633     0 33496 43553 15352 45965 60272
## [12] 23583 20890 14630 39612 36858 27623 60024 28268 53203  8789 21614
## [23] 17217 32765 38282 17929 29164 13853 27119  8892 15798 31403  7026
## [34] 58187 67567 13199 22337 27919 30929 22750 26330 48809 37128 14882
## [45] 27698 21128 28456 51494
## 
## [[7]]
##  [1] 19645 19693 30022 55860 21434 35353     0 17740 46389 45070 59377
## [12] 35962 51927 45668 10941 67895 36650 11975 16038 16675 39826 49570
## [23] 42902 13747 19018 22029 13093 32857  7837 24321 32568 30508 29335
## [34]  8659 39662 20998 33544  9053 30034 17958 19337 40306 68165 29296
## [45] 58736 20683 59099 37275
## 
## [[8]]
##  [1] 33194 33242 17113 48244 16576 45094 16196     0 56130 37454 51761
## [12] 28345 61668 55408 26667 77635 52670 21122 15199  8546 49567 64102
## [23] 35286 12148 11167 35578 29512 42597 23612 35829 42308 22891 39076
## [34] 19285 26753 30159 47093 12671 22418 30923 23317 27397 77906 44148
## [45] 68476 25866 70944 24366
## 
## [[9]]
##  [1] 34049 34097 59040 11039 45215 18025 49077 59134     0 35131 34289
## [12] 25588 18148  8956 55193 34944 47717 75605 43849 68785 11835 33390
## [23] 19644 48347 53863 33510 44745 17010 42701 26274 22029 46984 22791
## [34] 73768 83148 28780 42477 43501 46511 38331 41912 64390 35215 29033
## [45] 11504 36710 37265 67075
## 
## [[10]]
##  [1] 40768 40815 31200 29561 26887 47108 44877 38064 35204     0 24550
## [12] 11501 46805 40546 57034 62773 59493 54535 25521 47714 51581 66116
## [23] 18215 27276 32792 40228 50876 23464 44975 37844 23175 16279 41090
## [34] 52698 33741 35479 54253 39544 10472 52287 30906 22876 63043 46162
## [45] 44908 19378 72959 30101
## 
## [[11]]
##  [1] 54812 54860 52014 26837 40931 61152 58921 52108 34149 24114     0
## [12] 30135 56417 42752 71078 69452 73537 68579 39565 61758 65626 80160
## [23] 33264 41320 46836 54272 64920 34961 59020 51888 38287 30323 55134
## [34] 66742 46672 49523 68297 53588 33981 66331 44951 37998 69722 60206
## [45] 39488 41924 87003 44227
## 
## [[12]]
##  [1] 19189 19237 28715 23758 13122 25545 35622 28809 25568 11495 27547
## [12]     0 42119 35859 47779 58087 37930 45280 12217 38459 30018 44553
## [23]  8427 18021 23537 18649 41621 13676 35721 16280 13387 16659 19527
## [34] 43443 52822 13901 32690 30289 10679 28544 17602 34064 58357 24599
## [45] 34791 11692 51395 36749
## 
## [[13]]
##  [1] 36813 36860 61804 32740 47978 20788 51841 61898 18594 46628 54032
## [12] 41927     0 11355 57957 25913 31393 78369 46612 71548 10254 18035
## [23] 33039 51110 56626 36273 47508 29674 45464 29037 31619 49747 26873
## [34] 76531 85911 31544 28499 46264 49274 41094 44675 67153 26183 28043
## [45] 22973 39473 21910 69838
## 
## [[14]]
##  [1] 30553 30601 55544 20685 41718 14528 45581 55638  9008 40368 42945
## [12] 35668 11355     0 51697 26249 44221 72109 40353 65288  8339 26597
## [23] 26779 44850 50367 30014 41249 23415 39204 22778 25360 43488 20613
## [34] 70272 79652 25284 38981 40004 43014 34835 38415 60894 26519 25537
## [45] 13328 33213 30473 63579
## 
## [[15]]
##  [1] 25606 25654 38728 67712 27136 41314 10938 26445 52350 56922 71229
## [12] 37346 57888 51628     0 73856 27918 10658 28655 25381 45787 55480
## [23] 42912 31851 30870 27990 14521 38818 16023 30282 38529 42360 35296
## [34]  9692 48367 26996 35203 18968 41886 19386 25040 49012 74126 25555
## [45] 64696 26385 63610 45980
## 
## [[16]]
##  [1]  52597  52644  77588  46234  63762  36572  67625  77681  34889  62412
## [11]  69484  57711  25729  26142  73741      0  51707  94153  62396  87332
## [21]  30382  29845  48823  66894  72410  52057  63292  45458  61248  44821
## [31]  47403  65531  42657  92315 101695  47328  48813  62048  65058  56878
## [41]  60459  82937   3927  48357  26809  55257  16900  85622
## 
## [[17]]
##  [1] 22742 22790 66383 47823 44393 27506 36726 52009 45012 59978 74285
## [12] 37595 31384 44290 27751 49817     0 38346 45912 60034 38449 23624
## [23] 32592 49107 48126 21474 20735 31486 33280 24718 31198 45415 24781
## [34] 37380 76023 27212  9559 36224 44942 16608 42297 71343 51639 13101
## [45] 57358 43642 33153 73636
## 
## [[18]]
##  [1] 43466 43513 32742 64592 35685 61442 12026 20459 72478 53802 68109
## [12] 44693 78016 71756 10711 93983 38600     0 31781 19395 65915 80450
## [23] 51634 28730 27749 45849 20641 58945 19674 52177 58656 39239 55424
## [34]  6452 42381 40430 45885 20422 38766 25506 33588 43026 94253 53116
## [45] 84824 34934 87292 39994
## 
## [[19]]
##  [1] 17092 17140 23308 33262  1432 30429 15982 15299 41465 25814 40121
## [12] 12039 47003 40743 27715 62970 44714 29283     0 24950 34902 49437
## [23] 17605  4379  9837 19476 21557 22854 15656 21164 22565 11252 24411
## [34] 27446 39313 18800 37574 12622 10778 20648  5791 28657 63241 29483
## [45] 53811  3497 56279 31342
## 
## [[20]]
##  [1] 40369 40417 21028 58519 26851 55368 16853  5415 66404 47729 62035
## [12] 38620 71942 65683 25560 87910 59845 20015 25473     0 59842 74376
## [23] 45561 22423 21442 42753 36687 52872 30786 46104 52583 33166 49351
## [34] 18178 30668 37333 54268 25355 32693 38098 30492 31312 88180 50019
## [45] 78751 31837 81219 28281
## 
## [[21]]
##  [1] 25435 25483 50426 23657 36601  9410 40463 50520  9511 52933 67240
## [12] 30550 10254  8470 46579 30697 31384 66991 35235 60171     0 22368
## [23] 19572 39732 45249 24896 36131 16208 34087 17660 18153 38370 15495
## [34] 65154 74534 20166 26099 34887 37897 29717 33298 55776 30967 18643
## [45] 21538 28096 29210 58461
## 
## [[22]]
##  [1] 39601 39649 64592 40786 50767 21730 50009 64686 34013 67099 81406
## [12] 44716 18356 26775 55533 27246 27163 81157 49401 74337 22735     0
## [23] 35431 53898 59415 39062 38903 32067 42694 31826 34012 52536 26191
## [34] 79320 88700 34332 24269 49053 52063 31939 47464 69942 29069 20615
## [45] 35072 42262 10583 72627
## 
## [[23]]
##  [1] 17968 18016 35386 24320 18687 19877 42293 35480 19624 18209 33230
## [12]  8427 33133 26873 42921 49101 32654 51951 17782 45130 21032 35141
## [23]     0 21756 30208 17139 28664  5714 26043  8833  5426 23330 10461
## [34] 50114 59493 12123 27413 26843 20737 23268 23168 40735 49371 19323
## [45] 29874 17258 41983 43420
## 
## [[24]]
##  [1] 17512 17560 15422 36059  4428 33226 13943 13260 44262 27280 41587
## [12] 14836 49800 43540 24703 65768 41703 27243  3050 22910 37699 52234
## [23] 20402     0  7797 19895 18546 25651 12645 23961 25362 12718 27208
## [34] 25406 37273 16329 31410  8506 12244 19956  6213 26147 66038 32280
## [45] 56608  6317 59076 34886
## 
## [[25]]
##  [1] 27872 27920 10877 44424 10352 41273 18191 11042 52309 33634 47940
## [12] 24525 57847 51588 30348 73815 47348 27849  8975 20692 45747 60281
## [23] 31466  5924     0 30256 24190 38777 18290 32009 38488 19071 35255
## [34] 26012 35056 24837 41771 12858 18598 25601 17995 21917 74085 40327
## [45] 64656 19340 67124 21432
## 
## [[26]]
##  [1]  2570  2618 48748 37876 19003 19847 22001 34374 30884 46048 60355
## [12] 18632 36421 30162 28117 52389 21574 45001 28350 42399 24321 32360
## [23] 17165 21322 30491     0 13266 16059 14686  8521 15770 31485 13830
## [34] 43164 58387  5529 16334 18496 31012 10564 16906 48891 52659 12086
## [45] 43230 18252 45698 56000
## 
## [[27]]
##  [1] 10882 10930 43104 61689 21113 31083 13102 28730 42119 50899 65206
## [12] 31324 47657 41397 14188 63624 20735 20742 22632 36755 35556 38436
## [23] 27859 25828 24847 13266     0 27584 10000 20046 27295 36337 25065
## [34] 19286 52743 16764 22410 12945 35863  6736 19017 48063 63894 18161
## [45] 54465 20362 47965 50356
## 
## [[28]]
##  [1] 16863 16910 42857 21661 23936 13872 32894 42951 16990 23458 34927
## [12] 13676 29769 23509 39010 45736 31548 59422 23031 52601 17668 31777
## [23]  5714 27005 37680 16033 27558     0 26517  7728   640 30801  9331
## [34] 57585 66965 12597 26308 27318 25985 22162 25728 48207 46006 18217
## [45] 26509 22507 38619 50892
## 
## [[29]]
##  [1] 12254 12302 36928 55514 14937 28982  7761 22554 40018 44724 59031
## [12] 25148 45556 39296 16297 61524 33297 19546 16457 30579 33455 42179
## [23] 25758 19652 18671 14638 10140 26486     0 17950 26197 30161 22964
## [34] 16230 46568 14086 26153  3631 29688 11548 12841 41888 61794 21904
## [45] 52364 14186 51708 44180
## 
## [[30]]
##  [1]  9350  9398 36159 28322 22333 12073 24364 36253 23110 38665 52972
## [12] 16282 28647 22388 30480 44615 24850 52724 20968 45903 16547 31081
## [23]  8859 25465 30981  8521 20046  7753 17987     0  7465 24103  5802
## [34] 50887 60266  4067 19610 18787 23629 17344 17198 41508 44885 11519
## [45] 35456 13828 37924 44193
## 
## [[31]]
##  [1] 16574 16622 42568 24986 23647 18483 32605 42662 21985 23169 38253
## [12] 13387 31714 25454 38721 47681 31259 59133 22742 52312 19613 33722
## [23]  5426 26716 37391 15744 27269   640 26229  7439     0 30512  9042
## [34] 57296 66676 12308 26019 27029 25696 21873 25440 47918 47951 17928
## [45] 28454 22218 40564 50603
## 
## [[32]]
##  [1] 26503 26551 16931 35994 12622 32844 30612 23800 43880 15938 30245
## [12] 16095 49418 43158 42770 65386 45229 40271 11257 33450 37317 51852
## [23] 23036 13012 18528 25964 36612 30348 30711 23579 30059     0 26826
## [34] 38433 28491 21215 39989 25280  5038 38023 16642 16486 65656 31898
## [45] 56226 13616 58694 22401
## 
## [[33]]
##  [1] 14182 14230 39173 27991 25347  7318 29210 39267 22746 41679 55986
## [12] 19297 27681 21422 35326 43649 24862 55738 23982 48917 15581 26003
## [23] 10461 28479 33996 13643 24878  9331 22833  5790  9042 27117     0
## [34] 53901 63281  8913 19576 23633 26643 18464 22044 44523 43919 12121
## [45] 29215 16842 35531 47208
## 
## [[34]]
##  [1] 28166 28214 31292 63142 34235 59992  8710 19009 71028 52352 66659
## [12] 43243 76566 70306  9382 92533 36885  6378 30331 17945 64465 64447
## [23] 50184 27280 26299 30550 18965 57495 16357 50727 57206 37789 53974
## [34]     0 40931 38980 44170 17105 37316 23829 32138 41576 92803 34921
## [45] 83374 33484 85842 38544
## 
## [[35]]
##  [1]  56656  56703  14836  71705  40037  68555  40180  26812  79591  33969
## [11]  46710  51806  85129  78869  48886 101097  76131  43341  38660  30765
## [21]  73028  87563  58747  35609  34628  59039  52974  66059  47073  59291
## [31]  65770  28534  62537  41504      0  53620  70554  41641  45879  54384
## [41]  46778  10804 101367  67609  91937  49327  94405   4470
## 
## [[36]]
##  [1]  6069  6116 38793 33128 15430 15099 20926 29765 26135 41299 55606
## [12] 13883 31673 25413 27213 47641 27484 40392 23602 37789 19572 34107
## [23] 11875 28099 33615  5529 16764 12602 14123  4067 12314 26737  9081
## [34] 38555 53778     0 22244 14923 26263 14062 13334 44142 47911 14153
## [45] 38481 11296 40949 46827
## 
## [[37]]
##  [1] 17599 17647 60259 42618 30514 22302 33512 45885 39869 54835 69142
## [12] 32452 28491 39147 35036 46923  9559 56513 37137 53910 33306 20731
## [23] 27449 41635 42002 16331 22406 26343 26197 19575 26054 40272 19576
## [34] 54675 69899 22068     0 30007 39799 15442 28418 57678 48746  7897
## [45] 52215 29998 30260 67512
## 
## [[38]]
##  [1] 16015 16063 31108 49694 12129 29728  8933 12719 40764 38904 53211
## [12] 29795 46302 40042 19294 62270 36294 27361 13648 24759 34201 48736
## [23] 26504  8314 12851 18398 13137 27231  3723 18696 26943 24341 23710
## [34] 17000 40748 14832 29913     0 23868 14545 10032 36068 62540 25665
## [45] 53110 11378 55578 38360
## 
## [[39]]
##  [1] 25665 25713 22868 35156 11784 32006 29774 22961 43042 10466 34023
## [12] 15257 48580 42320 41932 64548 44391 39433 10419 32612 36479 51014
## [23] 22198 12174 17690 25126 35774 29510 29873 22741 29221  5081 25988
## [34] 37595 46975 20377 39151 24442     0 37185 15804 21298 64818 31060
## [45] 55388 12778 57856 30902
## 
## [[40]]
##  [1]  8180  8228 44691 42719 20175 24690 17967 30317 35726 52486 66793
## [12] 28309 41264 35005 19052 57232 16608 25606 21695 38341 29164 31472
## [23] 23306 27415 26433 10564  6736 22201 11607 17344 21912 37924 18672
## [34] 24151 54330 14062 15446 14551 37450     0 18079 49650 57502 11198
## [45] 48073 19424 41001 51943
## 
## [[41]]
##  [1] 14500 14548 37450 38853  8571 28213 17483 23076 39249 31405 45712
## [12] 17630 44787 38527 25122 60755 42122 33703  5844 31101 32686 47221
## [23] 23196  6938 19193 16883 18965 25716 13064 17181 25428 16842 22195
## [34] 31866 47090 13317 28398 10030 16369 18056     0 34248 61025 24150
## [45] 51595  7820 54063 44703
## 
## [[42]]
##  [1] 44607 44655 14655 46763 30726 50948 40494 27126 61984 22848 38359
## [12] 34199 67522 61262 49200 83489 76445 43655 29360 31079 55421 69956
## [23] 41140 24844 22014 44067 53288 48451 47387 41683 48162 16486 44930
## [34] 41818 10865 39319 58092 41955 21254 54699 47093     0 83759 50002
## [45] 74330 31720 76798  7225
## 
## [[43]]
##  [1]  53049  53096  78040  46686  64214  37024  68076  78133  35341  62864
## [11]  69936  58163  26181  26593  74193   3927  51887  94604  62848  87784
## [21]  30834  37251  49275  67346  72862  52509  63744  45910  61700  45273
## [31]  47855  65983  43109  92767 102147  47780  48993  62500  65510  57330
## [41]  60911  83389      0  48032  27261  55709  18722  86074
## 
## [[44]]
##  [1] 13481 13529 44241 35163 30415 14847 29395 44335 31781 46747 61054
## [12] 24364 28035 31060 30271 46467 13182 52395 29050 49792 25219 20275
## [23] 19361 33547 39063 12213 18288 18256 22079 11488 17967 32185 12121
## [34] 50557 68348 13981  7897 25889 31711 11325 24300 49590 53557     0
## [45] 44128 21910 29803 52275
## 
## [[45]]
##  [1] 40446 40494 65437 20517 51611 24421 55474 65531 11556 44883 39488
## [12] 34762 24423 13328 61590 26917 54113 82002 50246 75181 18231 34751
## [23] 29845 54743 60259 39906 51142 26480 49097 32670 28426 53381 29188
## [34] 80164 89544 35177 48873 49897 52907 44727 48308 70786 27187 35430
## [45]     0 43106 33848 73471
## 
## [[46]]
##  [1] 15720 15767 25996 33807  2532 23092 18703 24296 34128 19554 42809
## [12] 11722 39666 33406 26342 55633 43342 34923  4080 32321 27565 42100
## [23] 17288  7632 20818 18103 20185 22537 14284 13827 22248 13940 17074
## [34] 33086 48309 11317 30237 11250 11347 19276  7865 31345 55903 22146
## [45] 46474     0 48942 34030
## 
## [[47]]
##  [1] 48326 48373 73317 53314 59491 30454 61401 73410 39547 75823 76564
## [12] 53440 23890 33221 63609 16972 35356 74204 58125 83061 30123 12418
## [23] 44155 62623 68139 47786 50295 40791 54086 40550 42736 61260 37582
## [34] 73238 97424 43057 32463 57777 60787 43331 56188 78666 18794 32007
## [45] 33889 50986     0 81351
## 
## [[48]]
##  [1] 53987 54035 12168 56733 37369 53583 37512 24144 64619 31546 44287
## [12] 36834 70157 63897 46218 86125 73463 40673 35992 28097 58056 72591
## [23] 43775 32941 31960 56371 50305 51087 44405 44319 50798 22413 47565
## [34] 38836  4459 50952 67886 38973 30907 51716 44110  7225 86395 52637
## [45] 76965 34355 79433     0</code></pre>
</details>
<p>
<code>distances</code> is a list where the first element is the distances from the first castle to all the others. Let‚Äôs make it a matrix:
</p>
<pre class="r"><code>distances_matrix &lt;- distances %&gt;%
    reduce(rbind)</code></pre>
<details>
<p>
</p><summary>
Click if you want to see the distance matrix
</summary>
<p></p>
<pre class="r"><code>distances_matrix</code></pre>
<pre><code>##      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11]
## out     0    48 46364 38416 16619 20387 19617 31990 31423 46587 60894
##        48     0 46412 38464 16667 20435 19665 32038 31471 46635 60942
##     46900 46947     0 48698 30281 45548 30424 17056 56584 31187 52215
##     38214 38261 48754     0 34949 23582 55661 48848 11274 29579 26880
##     16494 16541 25311 35192     0 25375 19477 16687 36411 20939 42124
##     18468 18516 43459 23632 29633     0 33496 43553 15352 45965 60272
##     19645 19693 30022 55860 21434 35353     0 17740 46389 45070 59377
##     33194 33242 17113 48244 16576 45094 16196     0 56130 37454 51761
##     34049 34097 59040 11039 45215 18025 49077 59134     0 35131 34289
##     40768 40815 31200 29561 26887 47108 44877 38064 35204     0 24550
##     54812 54860 52014 26837 40931 61152 58921 52108 34149 24114     0
##     19189 19237 28715 23758 13122 25545 35622 28809 25568 11495 27547
##     36813 36860 61804 32740 47978 20788 51841 61898 18594 46628 54032
##     30553 30601 55544 20685 41718 14528 45581 55638  9008 40368 42945
##     25606 25654 38728 67712 27136 41314 10938 26445 52350 56922 71229
##     52597 52644 77588 46234 63762 36572 67625 77681 34889 62412 69484
##     22742 22790 66383 47823 44393 27506 36726 52009 45012 59978 74285
##     43466 43513 32742 64592 35685 61442 12026 20459 72478 53802 68109
##     17092 17140 23308 33262  1432 30429 15982 15299 41465 25814 40121
##     40369 40417 21028 58519 26851 55368 16853  5415 66404 47729 62035
##     25435 25483 50426 23657 36601  9410 40463 50520  9511 52933 67240
##     39601 39649 64592 40786 50767 21730 50009 64686 34013 67099 81406
##     17968 18016 35386 24320 18687 19877 42293 35480 19624 18209 33230
##     17512 17560 15422 36059  4428 33226 13943 13260 44262 27280 41587
##     27872 27920 10877 44424 10352 41273 18191 11042 52309 33634 47940
##      2570  2618 48748 37876 19003 19847 22001 34374 30884 46048 60355
##     10882 10930 43104 61689 21113 31083 13102 28730 42119 50899 65206
##     16863 16910 42857 21661 23936 13872 32894 42951 16990 23458 34927
##     12254 12302 36928 55514 14937 28982  7761 22554 40018 44724 59031
##      9350  9398 36159 28322 22333 12073 24364 36253 23110 38665 52972
##     16574 16622 42568 24986 23647 18483 32605 42662 21985 23169 38253
##     26503 26551 16931 35994 12622 32844 30612 23800 43880 15938 30245
##     14182 14230 39173 27991 25347  7318 29210 39267 22746 41679 55986
##     28166 28214 31292 63142 34235 59992  8710 19009 71028 52352 66659
##     56656 56703 14836 71705 40037 68555 40180 26812 79591 33969 46710
##      6069  6116 38793 33128 15430 15099 20926 29765 26135 41299 55606
##     17599 17647 60259 42618 30514 22302 33512 45885 39869 54835 69142
##     16015 16063 31108 49694 12129 29728  8933 12719 40764 38904 53211
##     25665 25713 22868 35156 11784 32006 29774 22961 43042 10466 34023
##      8180  8228 44691 42719 20175 24690 17967 30317 35726 52486 66793
##     14500 14548 37450 38853  8571 28213 17483 23076 39249 31405 45712
##     44607 44655 14655 46763 30726 50948 40494 27126 61984 22848 38359
##     53049 53096 78040 46686 64214 37024 68076 78133 35341 62864 69936
##     13481 13529 44241 35163 30415 14847 29395 44335 31781 46747 61054
##     40446 40494 65437 20517 51611 24421 55474 65531 11556 44883 39488
##     15720 15767 25996 33807  2532 23092 18703 24296 34128 19554 42809
##     48326 48373 73317 53314 59491 30454 61401 73410 39547 75823 76564
##     53987 54035 12168 56733 37369 53583 37512 24144 64619 31546 44287
##     [,12] [,13] [,14] [,15]  [,16] [,17] [,18] [,19] [,20] [,21] [,22]
## out 19171 36961 30701 25734  52929 22843 42618 18138 40015 24860 39395
##     19219 37009 30749 25781  52977 22890 42665 18186 40063 24908 39443
##     28799 62122 55862 39130  78090 66375 33585 23961 21009 50021 64556
##     25631 32853 20633 67818  46577 47882 65319 33355 58499 26540 40460
##     13107 41949 35689 27116  57917 44116 30670  1432 26337 29848 44383
##     23583 20890 14630 39612  36858 27623 60024 28268 53203  8789 21614
##     35962 51927 45668 10941  67895 36650 11975 16038 16675 39826 49570
##     28345 61668 55408 26667  77635 52670 21122 15199  8546 49567 64102
##     25588 18148  8956 55193  34944 47717 75605 43849 68785 11835 33390
##     11501 46805 40546 57034  62773 59493 54535 25521 47714 51581 66116
##     30135 56417 42752 71078  69452 73537 68579 39565 61758 65626 80160
##         0 42119 35859 47779  58087 37930 45280 12217 38459 30018 44553
##     41927     0 11355 57957  25913 31393 78369 46612 71548 10254 18035
##     35668 11355     0 51697  26249 44221 72109 40353 65288  8339 26597
##     37346 57888 51628     0  73856 27918 10658 28655 25381 45787 55480
##     57711 25729 26142 73741      0 51707 94153 62396 87332 30382 29845
##     37595 31384 44290 27751  49817     0 38346 45912 60034 38449 23624
##     44693 78016 71756 10711  93983 38600     0 31781 19395 65915 80450
##     12039 47003 40743 27715  62970 44714 29283     0 24950 34902 49437
##     38620 71942 65683 25560  87910 59845 20015 25473     0 59842 74376
##     30550 10254  8470 46579  30697 31384 66991 35235 60171     0 22368
##     44716 18356 26775 55533  27246 27163 81157 49401 74337 22735     0
##      8427 33133 26873 42921  49101 32654 51951 17782 45130 21032 35141
##     14836 49800 43540 24703  65768 41703 27243  3050 22910 37699 52234
##     24525 57847 51588 30348  73815 47348 27849  8975 20692 45747 60281
##     18632 36421 30162 28117  52389 21574 45001 28350 42399 24321 32360
##     31324 47657 41397 14188  63624 20735 20742 22632 36755 35556 38436
##     13676 29769 23509 39010  45736 31548 59422 23031 52601 17668 31777
##     25148 45556 39296 16297  61524 33297 19546 16457 30579 33455 42179
##     16282 28647 22388 30480  44615 24850 52724 20968 45903 16547 31081
##     13387 31714 25454 38721  47681 31259 59133 22742 52312 19613 33722
##     16095 49418 43158 42770  65386 45229 40271 11257 33450 37317 51852
##     19297 27681 21422 35326  43649 24862 55738 23982 48917 15581 26003
##     43243 76566 70306  9382  92533 36885  6378 30331 17945 64465 64447
##     51806 85129 78869 48886 101097 76131 43341 38660 30765 73028 87563
##     13883 31673 25413 27213  47641 27484 40392 23602 37789 19572 34107
##     32452 28491 39147 35036  46923  9559 56513 37137 53910 33306 20731
##     29795 46302 40042 19294  62270 36294 27361 13648 24759 34201 48736
##     15257 48580 42320 41932  64548 44391 39433 10419 32612 36479 51014
##     28309 41264 35005 19052  57232 16608 25606 21695 38341 29164 31472
##     17630 44787 38527 25122  60755 42122 33703  5844 31101 32686 47221
##     34199 67522 61262 49200  83489 76445 43655 29360 31079 55421 69956
##     58163 26181 26593 74193   3927 51887 94604 62848 87784 30834 37251
##     24364 28035 31060 30271  46467 13182 52395 29050 49792 25219 20275
##     34762 24423 13328 61590  26917 54113 82002 50246 75181 18231 34751
##     11722 39666 33406 26342  55633 43342 34923  4080 32321 27565 42100
##     53440 23890 33221 63609  16972 35356 74204 58125 83061 30123 12418
##     36834 70157 63897 46218  86125 73463 40673 35992 28097 58056 72591
##     [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33]
## out 17163 18938 28107  2570 10882 16888 12302  9350 16599 32025 14369
##     17211 18986 28155  2618 10930 16936 12350  9398 16647 32073 14417
##     35740 25853 10852 49283 43218 43052 37317 36283 42763 17250 39530
##     24359 38061 43577 37674 61661 21704 55760 29822 25030 36698 27956
##     18673  5767 11224 18877 20958 23922 15058 16110 23633 13255 19357
##     17217 32765 38282 17929 29164 13853 27119  8892 15798 31403  7026
##     42902 13747 19018 22029 13093 32857  7837 24321 32568 30508 29335
##     35286 12148 11167 35578 29512 42597 23612 35829 42308 22891 39076
##     19644 48347 53863 33510 44745 17010 42701 26274 22029 46984 22791
##     18215 27276 32792 40228 50876 23464 44975 37844 23175 16279 41090
##     33264 41320 46836 54272 64920 34961 59020 51888 38287 30323 55134
##      8427 18021 23537 18649 41621 13676 35721 16280 13387 16659 19527
##     33039 51110 56626 36273 47508 29674 45464 29037 31619 49747 26873
##     26779 44850 50367 30014 41249 23415 39204 22778 25360 43488 20613
##     42912 31851 30870 27990 14521 38818 16023 30282 38529 42360 35296
##     48823 66894 72410 52057 63292 45458 61248 44821 47403 65531 42657
##     32592 49107 48126 21474 20735 31486 33280 24718 31198 45415 24781
##     51634 28730 27749 45849 20641 58945 19674 52177 58656 39239 55424
##     17605  4379  9837 19476 21557 22854 15656 21164 22565 11252 24411
##     45561 22423 21442 42753 36687 52872 30786 46104 52583 33166 49351
##     19572 39732 45249 24896 36131 16208 34087 17660 18153 38370 15495
##     35431 53898 59415 39062 38903 32067 42694 31826 34012 52536 26191
##         0 21756 30208 17139 28664  5714 26043  8833  5426 23330 10461
##     20402     0  7797 19895 18546 25651 12645 23961 25362 12718 27208
##     31466  5924     0 30256 24190 38777 18290 32009 38488 19071 35255
##     17165 21322 30491     0 13266 16059 14686  8521 15770 31485 13830
##     27859 25828 24847 13266     0 27584 10000 20046 27295 36337 25065
##      5714 27005 37680 16033 27558     0 26517  7728   640 30801  9331
##     25758 19652 18671 14638 10140 26486     0 17950 26197 30161 22964
##      8859 25465 30981  8521 20046  7753 17987     0  7465 24103  5802
##      5426 26716 37391 15744 27269   640 26229  7439     0 30512  9042
##     23036 13012 18528 25964 36612 30348 30711 23579 30059     0 26826
##     10461 28479 33996 13643 24878  9331 22833  5790  9042 27117     0
##     50184 27280 26299 30550 18965 57495 16357 50727 57206 37789 53974
##     58747 35609 34628 59039 52974 66059 47073 59291 65770 28534 62537
##     11875 28099 33615  5529 16764 12602 14123  4067 12314 26737  9081
##     27449 41635 42002 16331 22406 26343 26197 19575 26054 40272 19576
##     26504  8314 12851 18398 13137 27231  3723 18696 26943 24341 23710
##     22198 12174 17690 25126 35774 29510 29873 22741 29221  5081 25988
##     23306 27415 26433 10564  6736 22201 11607 17344 21912 37924 18672
##     23196  6938 19193 16883 18965 25716 13064 17181 25428 16842 22195
##     41140 24844 22014 44067 53288 48451 47387 41683 48162 16486 44930
##     49275 67346 72862 52509 63744 45910 61700 45273 47855 65983 43109
##     19361 33547 39063 12213 18288 18256 22079 11488 17967 32185 12121
##     29845 54743 60259 39906 51142 26480 49097 32670 28426 53381 29188
##     17288  7632 20818 18103 20185 22537 14284 13827 22248 13940 17074
##     44155 62623 68139 47786 50295 40791 54086 40550 42736 61260 37582
##     43775 32941 31960 56371 50305 51087 44405 44319 50798 22413 47565
##     [,34]  [,35] [,36] [,37] [,38] [,39] [,40] [,41] [,42]  [,43] [,44]
## out 40780  56004  6069 17602 16112 31552  8180 14523 49431  53199 13354
##     40828  56052  6116 17650 16160 31599  8228 14571 49478  53247 13402
##     31748  14513 33919 60798 31885 22872 44629 37023 14605  78360 44602
##     63482  72862 32945 42596 50328 34302 42495 38740 46782  46847 35141
##     28833  40700 15310 30392 12024 12782 20050  7178 30661  58187 24429
##     58187  67567 13199 22337 27919 30929 22750 26330 48809  37128 14882
##      8659  39662 20998 33544  9053 30034 17958 19337 40306  68165 29296
##     19285  26753 30159 47093 12671 22418 30923 23317 27397  77906 44148
##     73768  83148 28780 42477 43501 46511 38331 41912 64390  35215 29033
##     52698  33741 35479 54253 39544 10472 52287 30906 22876  63043 46162
##     66742  46672 49523 68297 53588 33981 66331 44951 37998  69722 60206
##     43443  52822 13901 32690 30289 10679 28544 17602 34064  58357 24599
##     76531  85911 31544 28499 46264 49274 41094 44675 67153  26183 28043
##     70272  79652 25284 38981 40004 43014 34835 38415 60894  26519 25537
##      9692  48367 26996 35203 18968 41886 19386 25040 49012  74126 25555
##     92315 101695 47328 48813 62048 65058 56878 60459 82937   3927 48357
##     37380  76023 27212  9559 36224 44942 16608 42297 71343  51639 13101
##      6452  42381 40430 45885 20422 38766 25506 33588 43026  94253 53116
##     27446  39313 18800 37574 12622 10778 20648  5791 28657  63241 29483
##     18178  30668 37333 54268 25355 32693 38098 30492 31312  88180 50019
##     65154  74534 20166 26099 34887 37897 29717 33298 55776  30967 18643
##     79320  88700 34332 24269 49053 52063 31939 47464 69942  29069 20615
##     50114  59493 12123 27413 26843 20737 23268 23168 40735  49371 19323
##     25406  37273 16329 31410  8506 12244 19956  6213 26147  66038 32280
##     26012  35056 24837 41771 12858 18598 25601 17995 21917  74085 40327
##     43164  58387  5529 16334 18496 31012 10564 16906 48891  52659 12086
##     19286  52743 16764 22410 12945 35863  6736 19017 48063  63894 18161
##     57585  66965 12597 26308 27318 25985 22162 25728 48207  46006 18217
##     16230  46568 14086 26153  3631 29688 11548 12841 41888  61794 21904
##     50887  60266  4067 19610 18787 23629 17344 17198 41508  44885 11519
##     57296  66676 12308 26019 27029 25696 21873 25440 47918  47951 17928
##     38433  28491 21215 39989 25280  5038 38023 16642 16486  65656 31898
##     53901  63281  8913 19576 23633 26643 18464 22044 44523  43919 12121
##         0  40931 38980 44170 17105 37316 23829 32138 41576  92803 34921
##     41504      0 53620 70554 41641 45879 54384 46778 10804 101367 67609
##     38555  53778     0 22244 14923 26263 14062 13334 44142  47911 14153
##     54675  69899 22068     0 30007 39799 15442 28418 57678  48746  7897
##     17000  40748 14832 29913     0 23868 14545 10032 36068  62540 25665
##     37595  46975 20377 39151 24442     0 37185 15804 21298  64818 31060
##     24151  54330 14062 15446 14551 37450     0 18079 49650  57502 11198
##     31866  47090 13317 28398 10030 16369 18056     0 34248  61025 24150
##     41818  10865 39319 58092 41955 21254 54699 47093     0  83759 50002
##     92767 102147 47780 48993 62500 65510 57330 60911 83389      0 48032
##     50557  68348 13981  7897 25889 31711 11325 24300 49590  53557     0
##     80164  89544 35177 48873 49897 52907 44727 48308 70786  27187 35430
##     33086  48309 11317 30237 11250 11347 19276  7865 31345  55903 22146
##     73238  97424 43057 32463 57777 60787 43331 56188 78666  18794 32007
##     38836   4459 50952 67886 38973 30907 51716 44110  7225  86395 52637
##     [,45] [,46] [,47] [,48]
## out 43769 15868 46237 53617
##     43817 15916 46285 53665
##     68930 26320 71398 12126
##     20752 33520 47302 56789
##     48757  2511 51225 33346
##     27698 21128 28456 51494
##     58736 20683 59099 37275
##     68476 25866 70944 24366
##     11504 36710 37265 67075
##     44908 19378 72959 30101
##     39488 41924 87003 44227
##     34791 11692 51395 36749
##     22973 39473 21910 69838
##     13328 33213 30473 63579
##     64696 26385 63610 45980
##     26809 55257 16900 85622
##     57358 43642 33153 73636
##     84824 34934 87292 39994
##     53811  3497 56279 31342
##     78751 31837 81219 28281
##     21538 28096 29210 58461
##     35072 42262 10583 72627
##     29874 17258 41983 43420
##     56608  6317 59076 34886
##     64656 19340 67124 21432
##     43230 18252 45698 56000
##     54465 20362 47965 50356
##     26509 22507 38619 50892
##     52364 14186 51708 44180
##     35456 13828 37924 44193
##     28454 22218 40564 50603
##     56226 13616 58694 22401
##     29215 16842 35531 47208
##     83374 33484 85842 38544
##     91937 49327 94405  4470
##     38481 11296 40949 46827
##     52215 29998 30260 67512
##     53110 11378 55578 38360
##     55388 12778 57856 30902
##     48073 19424 41001 51943
##     51595  7820 54063 44703
##     74330 31720 76798  7225
##     27261 55709 18722 86074
##     44128 21910 29803 52275
##         0 43106 33848 73471
##     46474     0 48942 34030
##     33889 50986     0 81351
##     76965 34355 79433     0</code></pre>
</details>
<p>
Let‚Äôs baptize the rows and columns:
</p>
<pre class="r"><code>colnames(distances_matrix) &lt;- castles$Nom

rownames(distances_matrix) &lt;- castles$Nom</code></pre>
<p>
Now that we have the data, we can solve the TSP.
</p>
</section>
<section id="solving-the-travelling-salesman-problem-thats-the-combinatorial-optimization-part" class="level2">
<h2 class="anchored" data-anchor-id="solving-the-travelling-salesman-problem-thats-the-combinatorial-optimization-part">
Solving the Travelling salesman problem (that‚Äôs the combinatorial optimization part)
</h2>
<p>
Let‚Äôs first coerce the <code>distances_matrix</code> to an <code>ATSP</code> object, which is needed for the solver. <code>ATSP</code> stands for asymmetrical TSP. Asymmetrical because the <code>distances_matrix</code> is not symmetric, meaning that going from Castle A to Castle B is longer than going from Castle B to Castle A (for example).
</p>
<pre class="r"><code>atsp_castles &lt;- ATSP(distances_matrix)</code></pre>
<p>
I then define a list of all the available methods:
</p>
<pre class="r"><code>methods &lt;- c("identity", "random", "nearest_insertion",
             "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
             "nn", "repetitive_nn", "two_opt")</code></pre>
<p>
And solve the problem with all the methods:
</p>
<pre class="r"><code>solutions &lt;- map(methods, ~solve_TSP(x = atsp_castles, method = ., two_opt = TRUE, rep = 10,  two_opt_repetitions = 10)) %&gt;%
    set_names(methods)</code></pre>
<pre><code>## Warning: executing %dopar% sequentially: no parallel backend registered</code></pre>
<p>
I do this because the results vary depending on the methods, and I want to be exhaustive (solving this problem is quite fast, so there‚Äôs no reason not to do it):
</p>
<pre class="r"><code>solutions_df &lt;- solutions %&gt;%
    map_df(as.numeric)</code></pre>
<p>
<code>solutions_df</code> is a data frame with the order of the castles to visit in rows and the method used in columns.
</p>
<details>
<p>
</p><summary>
Click if you want to see the solutions
</summary>
<p></p>
<pre class="r"><code>solutions_df</code></pre>
<pre><code>## # A tibble: 48 x 9
##    identity random nearest_inserti‚Ä¶ cheapest_insert‚Ä¶ farthest_insert‚Ä¶
##       &lt;dbl&gt;  &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;
##  1        1     10               37               44               15
##  2        2     11               17               37               27
##  3       36      4               22               17               29
##  4       33      9               47               40               38
##  5        6     45               16               27               41
##  6       44     43               43               15               19
##  7       37     16               13               18                5
##  8       17     47               21               34               46
##  9       22     22               14                7               12
## 10       47     13               45               20               23
## # ‚Ä¶ with 38 more rows, and 4 more variables: arbitrary_insertion &lt;dbl&gt;,
## #   nn &lt;dbl&gt;, repetitive_nn &lt;dbl&gt;, two_opt &lt;dbl&gt;</code></pre>
</details>
<p>
Now, let‚Äôs extract the tour lengths, see which one is the minimum, then plot it.
</p>
<pre class="r"><code>tour_lengths &lt;- solutions %&gt;%
    map_dbl(tour_length)

which.min(tour_lengths)</code></pre>
<pre><code>## arbitrary_insertion 
##                   6</code></pre>
<p>
The total length of the tour is 474 kilometers (that‚Äôs 295 miles). Before plotting the data, let‚Äôs re-order it according to the solution:
</p>
<pre class="r"><code>castles_to_visit &lt;- castles[pull(solutions_df, names(which.min(tour_lengths))), ]</code></pre>
</section>
<section id="plot-the-solution" class="level2">
<h2 class="anchored" data-anchor-id="plot-the-solution">
Plot the solution
</h2>
<p>
To plot the solution, I first use a data frame I created with the longitude and latitude of Luxembourguish communes, from the <code>geojson</code> file available on the <a href="https://data.public.lu/en/datasets/limites-administratives-du-grand-duche-de-luxembourg/#resource-39af91a6-9ce4-4c18-8271-313b3ad7c7f5">OpenData Portal</a>. I converted it to a data frame because it is easier to manipulate this way. The code to do that is in the appendix of this blog post:
</p>
<pre class="r"><code>communes_df &lt;- read_csv("communes_df.csv")</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   lon = col_double(),
##   lat = col_double(),
##   commune = col_character()
## )</code></pre>
<p>
Now I can use <code>{ggplot2}</code> to create the map with the tour. I use <code>geom_polygon()</code> to build the map, <code>geom_point()</code> to add the castles, <code>geom_path()</code> to connect the points according to the solution I found and <code>geom_point()</code> again to highlight the starting castle:
</p>
<pre class="r"><code>ggplot() +
    geom_polygon(data = communes_df, aes(x = lon, y = lat, group = commune), colour = "grey", fill = NA) +
    geom_point(data = castles, aes(x = lon, y = lat), colour = "#82518c", size = 3) +
    geom_path(data = castles_to_visit, aes(x = lon, y = lat), colour = "#647e0e") +
    geom_point(data = (slice(castles_to_visit, 1)), aes(x = lon, y = lat), colour = "white", size = 5) +
    theme_void() +
    ggtitle("The shortest tour to visit 48 Luxembourguish castles") +
    theme(legend.position = "bottom",
          legend.title = element_blank(),
          legend.text = element_text(colour = "white"),
          plot.background = element_rect("#272b30"),
          plot.title = element_text(colour = "white")) </code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/lux_castle-28-1.png" width="768">
</p>
<p>
The white point is the starting point of the tour. As a bonus, let‚Äôs do the same plot without points, but castles emojis instead (using the <code>{ggimage}</code> package):
</p>
<pre class="r"><code>ggplot() +
    geom_polygon(data = communes_df, aes(x = lon, y = lat, group = commune), colour = "grey", fill = NA) +
    geom_emoji(data = castles, aes(x = lon, y = lat, image = "1f3f0")) + # &lt;- this is the hex code for the "european castle" emoji
    geom_path(data = castles_to_visit, aes(x = lon, y = lat), colour = "#647e0e") +
    theme_void() +
    ggtitle("The shortest tour to visit 48 Luxembourguish castles") +
    theme(legend.position = "bottom",
          legend.title = element_blank(),
          legend.text = element_text(colour = "white"),
          plot.background = element_rect("#272b30"),
          plot.title = element_text(colour = "white"))</code></pre>
<pre><code>## Warning: Ignoring unknown parameters: image_colour</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/lux_castle-29-1.png" width="768">
</p>
<p>
It‚Äôs horrible.
</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">
Appendix
</h2>
<p>
The code below converts the <code>geojson</code> that can be downloaded from the <a href="https://data.public.lu/en/datasets/limites-administratives-du-grand-duche-de-luxembourg/#resource-39af91a6-9ce4-4c18-8271-313b3ad7c7f5">OpenData Portal</a> to <code>csv</code>. A <code>csv</code> file is easier to handle. I only focus on the communes.
</p>
<pre class="r"><code>limadmin &lt;- RJSONIO::fromJSON("limadmin.geojson")

communes &lt;- limadmin$communes

extract_communes &lt;- function(features){

    res &lt;- features$geometry$coordinates %&gt;%
        map(lift(rbind)) %&gt;%
        as.data.frame() %&gt;%
        rename(lon = X1,
               lat = X2)

    res %&gt;%
        mutate(commune = features$properties[1])
}

communes_df &lt;- map(limadmin$communes$features, extract_communes)

## Steinfort and Waldbredimus special treatment:

steinfort &lt;- limadmin$communes$features[[5]]$geometry$coordinates[[1]] %&gt;%
    map(lift(rbind)) %&gt;%
    as.data.frame() %&gt;%
    rename(lon = X1,
           lat = X2) %&gt;%
    mutate(commune = "Steinfort")

waldbredimus &lt;- limadmin$communes$features[[44]]$geometry$coordinates[[1]] %&gt;%
    map(lift(rbind)) %&gt;%
    as.data.frame() %&gt;%
    rename(lon = X1,
           lat = X2) %&gt;%
    mutate(commune = "Waldbredimus")

communes_df[[5]] &lt;- NULL
communes_df[[43]] &lt;- NULL


communes_df &lt;- bind_rows(communes_df, list(steinfort, waldbredimus))</code></pre>
</section>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-11-21-lux_castle.html</guid>
  <pubDate>Wed, 21 Nov 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Using a genetic algorithm for the hyperparameter optimization of a SARIMA model</title>
  <link>https://b-rodrigues.github.io/posts/2018-11-16-rgenoud_arima.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://keiwan.itch.io/evolution"> <img width="400" src="https://b-rodrigues.github.io/assets/img/tap-walker.gif" title="Nietzsche's √úbermensch"></a>
</p>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">
Introduction
</h2>
<p>
In this blog post, I‚Äôll use the data that I cleaned in a previous <a href="../posts/2018-11-14-luxairport.html">blog post</a>, which you can download <a href="https://github.com/b-rodrigues/avia_par_lu/tree/master">here</a>. If you want to follow along, download the monthly data. In my <a href="../posts/2018-11-15-tidy_gridsearch.html">last blog post</a> I showed how to perform a grid search the ‚Äútidy‚Äù way. As an example, I looked for the right hyperparameters of a SARIMA model. However, the goal of the post was not hyperparameter optimization per se, so I did not bother with tuning the hyperparameters on a validation set, and used the test set for both validation of the hyperparameters and testing the forecast. Of course, this is not great because doing this might lead to overfitting the hyperparameters to the test set. So in this blog post I split my data into trainig, validation and testing sets and use a genetic algorithm to look for the hyperparameters. Again, this is not the most optimal way to go about this problem, since the <code>{forecast}</code> package contains the very useful <code>auto.arima()</code> function. I just wanted to see what kind of solution a genetic algorithm would return, and also try different cost functions. If you‚Äôre interested, read on!
</p>
</section>
<section id="setup" class="level2">
<h2 class="anchored" data-anchor-id="setup">
Setup
</h2>
<p>
Let‚Äôs first load some libraries and define some helper functions (the helper functions were explained in the previous blog posts):
</p>
<pre class="r"><code>library(tidyverse)
library(forecast)
library(rgenoud)
library(parallel)
library(lubridate)
library(furrr)
library(tsibble)
library(brotools)

ihs &lt;- function(x){
    log(x + sqrt(x**2 + 1))
}

to_tibble &lt;- function(forecast_object){
    point_estimate &lt;- forecast_object$mean %&gt;%
        as_tsibble() %&gt;%
        rename(point_estimate = value,
               date = index)

    upper &lt;- forecast_object$upper %&gt;%
        as_tsibble() %&gt;%
        spread(key, value) %&gt;%
        rename(date = index,
               upper80 = `80%`,
               upper95 = `95%`)

    lower &lt;- forecast_object$lower %&gt;%
        as_tsibble() %&gt;%
        spread(key, value) %&gt;%
        rename(date = index,
               lower80 = `80%`,
               lower95 = `95%`)

    reduce(list(point_estimate, upper, lower), full_join)
}</code></pre>
<p>
Now, let‚Äôs load the data:
</p>
<pre class="r"><code>avia_clean_monthly &lt;- read_csv("https://raw.githubusercontent.com/b-rodrigues/avia_par_lu/master/avia_clean_monthy.csv")</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   destination = col_character(),
##   date = col_date(format = ""),
##   passengers = col_double()
## )</code></pre>
<p>
Let‚Äôs split the data into a train set, a validation set and a test set:
</p>
<pre class="r"><code>avia_clean_train &lt;- avia_clean_monthly %&gt;%
    select(date, passengers) %&gt;%
    filter(year(date) &lt; 2013) %&gt;%
    group_by(date) %&gt;%
    summarise(total_passengers = sum(passengers)) %&gt;%
    pull(total_passengers) %&gt;%
    ts(., frequency = 12, start = c(2005, 1))

avia_clean_validation &lt;- avia_clean_monthly %&gt;%
    select(date, passengers) %&gt;%
    filter(between(year(date), 2013, 2016)) %&gt;%
    group_by(date) %&gt;%
    summarise(total_passengers = sum(passengers)) %&gt;%
    pull(total_passengers) %&gt;%
    ts(., frequency = 12, start = c(2013, 1))

avia_clean_test &lt;- avia_clean_monthly %&gt;%
    select(date, passengers) %&gt;%
    filter(year(date) &gt;= 2016) %&gt;%
    group_by(date) %&gt;%
    summarise(total_passengers = sum(passengers)) %&gt;%
    pull(total_passengers) %&gt;%
    ts(., frequency = 12, start = c(2016, 1))

logged_test_data &lt;- ihs(avia_clean_test)

logged_validation_data &lt;- ihs(avia_clean_validation)

logged_train_data &lt;- ihs(avia_clean_train)</code></pre>
<p>
I will train the models on data from 2005 to 2012, look for the hyperparameters on data from 2013 to 2016 and test the accuracy on data from 2016 to March 2018. For this kind of exercise, the ideal situation would be to perform cross-validation. Doing this with time-series data is not obvious because of the autocorrelation between observations, which would be broken by sampling independently which is required by CV. Also, if for example you do leave-one-out CV, you would end up trying to predict a point in, say, 2017, with data from 2018, which does not make sense. So you should be careful about that. <code>{forecast}</code> is able to perform <a href="https://robjhyndman.com/hyndsight/tscv/">CV for time series</a> and <code>scikit-learn</code>, the Python package, is able to perform <a href="https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split">cross-validation of time series data</a> too. I will not do it in this blog post and simply focus on the genetic algorithm part.
</p>
<p>
Let‚Äôs start by defining the cost function to minimize. I‚Äôll try several, in the first one I&nbsp;will minimize the RMSE:
</p>
<pre class="r"><code>cost_function_rmse &lt;- function(param, train_data, validation_data, forecast_periods){
    order &lt;- param[1:3]
    season &lt;- c(param[4:6], 12)
    model &lt;- purrr::possibly(arima, otherwise = NULL)(x = train_data, order = order, 
                                                      seasonal = season,
                                                      method = "ML")
    if(is.null(model)){
        return(9999999)
    } else {
      forecast_model &lt;- forecast::forecast(model, h = forecast_periods)
      point_forecast &lt;- forecast_model$mean
      sqrt(mean(point_forecast - validation_data) ** 2)
    }
}</code></pre>
<p>
If <code>arima()</code> is not able to estimate a model for the given parameters, I force it to return <code>NULL</code>, and in that case force the cost function to return a very high cost. If a model was successfully estimated, then I compute the RMSE.
</p>
<p>
Let‚Äôs also take a look at what <code>auto.arima()</code> says:
</p>
<pre class="r"><code>starting_model &lt;- auto.arima(logged_train_data)
summary(starting_model)</code></pre>
<pre><code>## Series: logged_train_data 
## ARIMA(3,0,0)(0,1,1)[12] with drift 
## 
## Coefficients:
##          ar1     ar2     ar3     sma1   drift
##       0.2318  0.2292  0.3661  -0.8498  0.0029
## s.e.  0.1016  0.1026  0.1031   0.2101  0.0010
## 
## sigma^2 estimated as 0.004009:  log likelihood=107.98
## AIC=-203.97   AICc=-202.88   BIC=-189.38
## 
## Training set error measures:
##                        ME       RMSE        MAE         MPE      MAPE
## Training set 0.0009924108 0.05743719 0.03577996 0.006323241 0.3080978
##                   MASE        ACF1
## Training set 0.4078581 -0.02707016</code></pre>
<p>
Let‚Äôs compute the cost at this vector of parameters:
</p>
<pre class="r"><code>cost_function_rmse(c(1, 0, 2, 2, 1, 0),
              train_data = logged_train_data,
              validation_data = logged_validation_data,
              forecast_periods = 65)</code></pre>
<pre><code>## [1] 0.1731473</code></pre>
<p>
Ok, now let‚Äôs start with optimizing the hyperparameters. Let‚Äôs help the genetic algorithm a little bit by defining where it should perform the search:
</p>
<pre class="r"><code>domains &lt;- matrix(c(0, 3, 0, 2, 0, 3, 0, 3, 0, 2, 0, 3), byrow = TRUE, ncol = 2)</code></pre>
<p>
This matrix constraints the first parameter to lie between 0 and 3, the second one between 0 and 2, and so on.
</p>
<p>
Let‚Äôs call the <code>genoud()</code> function from the <code>{rgenoud}</code> package, and use 8 cores:
</p>
<pre class="r"><code>cl &lt;- makePSOCKcluster(8)
clusterExport(cl, c('logged_train_data', 'logged_validation_data'))

tic &lt;- Sys.time()

auto_arima_rmse &lt;- genoud(cost_function_rmse,
                     nvars = 6,
                     data.type.int = TRUE,
                     starting.values = c(1, 0, 2, 2, 1, 0), # &lt;- from auto.arima
                     Domains = domains,
                     cluster = cl,
                     train_data = logged_train_data,
                     validation_data = logged_validation_data,
                     forecast_periods = length(logged_validation_data),
                     hard.generation.limit = TRUE)
toc_rmse &lt;- Sys.time() - tic</code></pre>
<p>
<code>makePSOCKcluster()</code> is a function from the <code>{parallel}</code> package. I must also <em>export</em> the global variables <code>logged_train_data</code> or <code>logged_validation_data</code>. If I don‚Äôt do that, the workers called by <code>genoud()</code> will not <em>know</em> about these variables and an error will be returned. The option <code>data.type.int = TRUE</code> force the algorithm to look only for integers, and <code>hard.generation.limit =&nbsp;TRUE</code> forces the algorithm to stop after 100 generations.
</p>
<p>
The process took 7 minutes, which is faster than doing the grid search. What was the solution found?
</p>
<pre class="r"><code>auto_arima_rmse</code></pre>
<pre><code>## $value
## [1] 0.0001863039
## 
## $par
## [1] 3 2 1 1 2 1
## 
## $gradients
## [1] NA NA NA NA NA NA
## 
## $generations
## [1] 11
## 
## $peakgeneration
## [1] 1
## 
## $popsize
## [1] 1000
## 
## $operators
## [1] 122 125 125 125 125 126 125 126   0</code></pre>
<p>
Let‚Äôs train the model using the <code>arima()</code> function at these parameters:
</p>
<pre class="r"><code>best_model_rmse &lt;- arima(logged_train_data, order = auto_arima_rmse$par[1:3], 
                         season = list(order = auto_arima_rmse$par[4:6], period = 12),
                         method = "ML")

summary(best_model_rmse)</code></pre>
<pre><code>## 
## Call:
## arima(x = logged_train_data, order = auto_arima_rmse$par[1:3], seasonal = list(order = auto_arima_rmse$par[4:6], 
##     period = 12), method = "ML")
## 
## Coefficients:
##           ar1      ar2      ar3      ma1     sar1     sma1
##       -0.6999  -0.4541  -0.0476  -0.9454  -0.4996  -0.9846
## s.e.   0.1421   0.1612   0.1405   0.1554   0.1140   0.2193
## 
## sigma^2 estimated as 0.006247:  log likelihood = 57.34,  aic = -100.67
## 
## Training set error measures:
##                         ME       RMSE        MAE          MPE      MAPE
## Training set -0.0006142355 0.06759545 0.04198561 -0.005408262 0.3600483
##                   MASE         ACF1
## Training set 0.4386693 -0.008298546</code></pre>
<p>
Let‚Äôs extract the forecasts:
</p>
<pre class="r"><code>best_model_rmse_forecast &lt;- forecast::forecast(best_model_rmse, h = 65)

best_model_rmse_forecast &lt;- to_tibble(best_model_rmse_forecast)</code></pre>
<pre><code>## Joining, by = "date"
## Joining, by = "date"</code></pre>
<pre class="r"><code>starting_model_forecast &lt;- forecast(starting_model, h = 65)

starting_model_forecast &lt;- to_tibble(starting_model_forecast)</code></pre>
<pre><code>## Joining, by = "date"
## Joining, by = "date"</code></pre>
<p>
and plot the forecast to see how it looks:
</p>
<pre class="r"><code>avia_clean_monthly %&gt;%
    group_by(date) %&gt;%
    summarise(total = sum(passengers)) %&gt;%
    mutate(total_ihs = ihs(total)) %&gt;%
    ggplot() +
    ggtitle("Minimization of RMSE") +
    geom_line(aes(y = total_ihs, x = date), colour = "#82518c") +
    scale_x_date(date_breaks = "1 year", date_labels = "%m-%Y") +
    geom_ribbon(data = best_model_rmse_forecast, aes(x = date, ymin = lower95, ymax = upper95),
                fill = "#666018", alpha = 0.2) +
    geom_line(data = best_model_rmse_forecast, aes(x = date, y = point_estimate), 
              linetype = 2, colour = "#8e9d98") +
    geom_ribbon(data = starting_model_forecast, aes(x = date, ymin = lower95, ymax = upper95),
                fill = "#98431e", alpha = 0.2) +
    geom_line(data = starting_model_forecast, aes(x = date, y = point_estimate), 
              linetype = 2, colour = "#a53031") +
    theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/rgenoud_arima-14-1.png" width="672">
</p>
<p>
The yellowish line and confidence intervals come from minimizing the genetic algorithm, and the redish from <code>auto.arima()</code>. Interesting; the point estimate is very precise, but the confidence intervals are very wide. Low bias, high variance.
</p>
<p>
Now, let‚Äôs try with another cost function, where I minimize the BIC, similar to the <code>auto.arima()</code> function:
</p>
<pre class="r"><code>cost_function_bic &lt;- function(param, train_data, validation_data, forecast_periods){
    order &lt;- param[1:3]
    season &lt;- c(param[4:6], 12)
    model &lt;- purrr::possibly(arima, otherwise = NULL)(x = train_data, order = order, 
                                                      seasonal = season,
                                                      method = "ML")
    if(is.null(model)){
        return(9999999)
    } else {
        BIC(model)
    }
}</code></pre>
<p>
Let‚Äôs take a look at the cost at the parameter values returned by <code>auto.arima()</code>:
</p>
<pre class="r"><code>cost_function_bic(c(1, 0, 2, 2, 1, 0),
              train_data = logged_train_data,
              validation_data = logged_validation_data,
              forecast_periods = 65)</code></pre>
<pre><code>## [1] -184.6397</code></pre>
<p>
Let the genetic algorithm run again:
</p>
<pre class="r"><code>cl &lt;- makePSOCKcluster(8)
clusterExport(cl, c('logged_train_data', 'logged_validation_data'))

tic &lt;- Sys.time()

auto_arima_bic &lt;- genoud(cost_function_bic,
                     nvars = 6,
                     data.type.int = TRUE,
                     starting.values = c(1, 0, 2, 2, 1, 0), # &lt;- from auto.arima
                     Domains = domains,
                     cluster = cl,
                     train_data = logged_train_data,
                     validation_data = logged_validation_data,
                     forecast_periods = length(logged_validation_data),
                     hard.generation.limit = TRUE)
toc_bic &lt;- Sys.time() - tic</code></pre>
<p>
This time, it took 6 minutes, a bit slower than before. Let‚Äôs take a look at the solution:
</p>
<pre class="r"><code>auto_arima_bic</code></pre>
<pre><code>## $value
## [1] -201.0656
## 
## $par
## [1] 0 1 1 1 0 1
## 
## $gradients
## [1] NA NA NA NA NA NA
## 
## $generations
## [1] 12
## 
## $peakgeneration
## [1] 1
## 
## $popsize
## [1] 1000
## 
## $operators
## [1] 122 125 125 125 125 126 125 126   0</code></pre>
<p>
Let‚Äôs train the model at these parameters:
</p>
<pre class="r"><code>best_model_bic &lt;- arima(logged_train_data, order = auto_arima_bic$par[1:3], 
                        season = list(order = auto_arima_bic$par[4:6], period = 12),
                        method = "ML")

summary(best_model_bic)</code></pre>
<pre><code>## 
## Call:
## arima(x = logged_train_data, order = auto_arima_bic$par[1:3], seasonal = list(order = auto_arima_bic$par[4:6], 
##     period = 12), method = "ML")
## 
## Coefficients:
##           ma1    sar1    sma1
##       -0.6225  0.9968  -0.832
## s.e.   0.0835  0.0075   0.187
## 
## sigma^2 estimated as 0.004145:  log likelihood = 109.64,  aic = -211.28
## 
## Training set error measures:
##                       ME       RMSE        MAE        MPE      MAPE
## Training set 0.003710982 0.06405303 0.04358164 0.02873561 0.3753513
##                   MASE        ACF1
## Training set 0.4553447 -0.03450603</code></pre>
<p>
And let‚Äôs plot the results:
</p>
<pre class="r"><code>best_model_bic_forecast &lt;- forecast::forecast(best_model_bic, h = 65)

best_model_bic_forecast &lt;- to_tibble(best_model_bic_forecast)</code></pre>
<pre><code>## Joining, by = "date"
## Joining, by = "date"</code></pre>
<pre class="r"><code>avia_clean_monthly %&gt;%
    group_by(date) %&gt;%
    summarise(total = sum(passengers)) %&gt;%
    mutate(total_ihs = ihs(total)) %&gt;%
    ggplot() +
    ggtitle("Minimization of BIC") +
    geom_line(aes(y = total_ihs, x = date), colour = "#82518c") +
    scale_x_date(date_breaks = "1 year", date_labels = "%m-%Y") +
    geom_ribbon(data = best_model_bic_forecast, aes(x = date, ymin = lower95, ymax = upper95),
                fill = "#5160a0", alpha = 0.2) +
    geom_line(data = best_model_bic_forecast, aes(x = date, y = point_estimate), 
              linetype = 2, colour = "#208480") +
    geom_ribbon(data = starting_model_forecast, aes(x = date, ymin = lower95, ymax = upper95),
                fill = "#98431e", alpha = 0.2) +
    geom_line(data = starting_model_forecast, aes(x = date, y = point_estimate), 
              linetype = 2, colour = "#a53031") +
    theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/rgenoud_arima-21-1.png" width="672">
</p>
<p>
The solutions are very close, both in terms of point estimates and confidence intervals. Bias increased, but variance lowered‚Ä¶ This gives me an idea! What if I minimize the RMSE, while keeping the number of parameters low, as a kind of regularization? This is somewhat what minimising BIC does, but let‚Äôs try to do it a more ‚Äúnaive‚Äù approach:
</p>
<pre class="r"><code>cost_function_rmse_low_k &lt;- function(param, train_data, validation_data, forecast_periods, max.order){
    order &lt;- param[1:3]
    season &lt;- c(param[4:6], 12)
    if(param[1] + param[3] + param[4] + param[6] &gt; max.order){
        return(9999999)
    } else {
        model &lt;- purrr::possibly(arima, otherwise = NULL)(x = train_data, 
                                                          order = order, 
                                                          seasonal = season,
                                                          method = "ML")
    }
    if(is.null(model)){
        return(9999999)
    } else {
        forecast_model &lt;- forecast::forecast(model, h = forecast_periods)
        point_forecast &lt;- forecast_model$mean
        sqrt(mean(point_forecast - validation_data) ** 2)
    }
}</code></pre>
<p>
This is also similar to what <code>auto.arima()</code> does; by default, the <code>max.order</code> argument in <code>auto.arima()</code> is set to 5, and is the sum of <code>p + q + P + Q</code>. So I‚Äôll try something similar.
</p>
<p>
Let‚Äôs take a look at the cost at the parameter values returned by <code>auto.arima()</code>:
</p>
<pre class="r"><code>cost_function_rmse_low_k(c(1, 0, 2, 2, 1, 0),
              train_data = logged_train_data,
              validation_data = logged_validation_data,
              forecast_periods = 65,
              max.order = 5)</code></pre>
<pre><code>## [1] 0.1731473</code></pre>
<p>
Let‚Äôs see what will happen:
</p>
<pre class="r"><code>cl &lt;- makePSOCKcluster(8)
clusterExport(cl, c('logged_train_data', 'logged_validation_data'))

tic &lt;- Sys.time()

auto_arima_rmse_low_k &lt;- genoud(cost_function_rmse_low_k,
                         nvars = 6,
                         data.type.int = TRUE,
                         starting.values = c(1, 0, 2, 2, 1, 0), # &lt;- from auto.arima
                         max.order = 5,
                         Domains = domains,
                         cluster = cl,
                         train_data = logged_train_data,
                         validation_data = logged_validation_data,
                         forecast_periods = length(logged_validation_data),
                         hard.generation.limit = TRUE)
toc_rmse_low_k &lt;- Sys.time() - tic</code></pre>
<p>
It took 1 minute to train this one, quite fast! Let‚Äôs take a look:
</p>
<pre class="r"><code>auto_arima_rmse_low_k</code></pre>
<pre><code>## $value
## [1] 0.002503478
## 
## $par
## [1] 1 2 0 3 1 0
## 
## $gradients
## [1] NA NA NA NA NA NA
## 
## $generations
## [1] 11
## 
## $peakgeneration
## [1] 1
## 
## $popsize
## [1] 1000
## 
## $operators
## [1] 122 125 125 125 125 126 125 126   0</code></pre>
<p>
And let‚Äôs plot it:
</p>
<pre class="r"><code>best_model_rmse_low_k &lt;- arima(logged_train_data, order = auto_arima_rmse_low_k$par[1:3], 
                               season = list(order = auto_arima_rmse_low_k$par[4:6], period = 12),
                               method = "ML")

summary(best_model_rmse_low_k)</code></pre>
<pre><code>## 
## Call:
## arima(x = logged_train_data, order = auto_arima_rmse_low_k$par[1:3], seasonal = list(order = auto_arima_rmse_low_k$par[4:6], 
##     period = 12), method = "ML")
## 
## Coefficients:
##           ar1     sar1     sar2     sar3
##       -0.6468  -0.7478  -0.5263  -0.1143
## s.e.   0.0846   0.1171   0.1473   0.1446
## 
## sigma^2 estimated as 0.01186:  log likelihood = 57.88,  aic = -105.76
## 
## Training set error measures:
##                        ME      RMSE        MAE         MPE      MAPE
## Training set 0.0005953302 0.1006917 0.06165919 0.003720452 0.5291736
##                   MASE       ACF1
## Training set 0.6442205 -0.3706693</code></pre>
<pre class="r"><code>best_model_rmse_low_k_forecast &lt;- forecast::forecast(best_model_rmse_low_k, h = 65)

best_model_rmse_low_k_forecast &lt;- to_tibble(best_model_rmse_low_k_forecast)</code></pre>
<pre><code>## Joining, by = "date"
## Joining, by = "date"</code></pre>
<pre class="r"><code>avia_clean_monthly %&gt;%
    group_by(date) %&gt;%
    summarise(total = sum(passengers)) %&gt;%
    mutate(total_ihs = ihs(total)) %&gt;%
    ggplot() +
    ggtitle("Minimization of RMSE + low k") +
    geom_line(aes(y = total_ihs, x = date), colour = "#82518c") +
    scale_x_date(date_breaks = "1 year", date_labels = "%m-%Y") +
    geom_ribbon(data = best_model_rmse_low_k_forecast, aes(x = date, ymin = lower95, ymax = upper95),
                fill = "#5160a0", alpha = 0.2) +
    geom_line(data = best_model_rmse_low_k_forecast, aes(x = date, y = point_estimate), 
              linetype = 2, colour = "#208480") +
    geom_ribbon(data = starting_model_forecast, aes(x = date, ymin = lower95, ymax = upper95),
                fill = "#98431e", alpha = 0.2) +
    geom_line(data = starting_model_forecast, aes(x = date, y = point_estimate), 
              linetype = 2, colour = "#a53031") +
    theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/rgenoud_arima-28-1.png" width="672">
</p>
<p>
Looks like this was not the right strategy. There might be a better cost function than what I have tried, but looks like minimizing the BIC is the way to go.
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>econometrics</category>
  <guid>https://b-rodrigues.github.io/posts/2018-11-16-rgenoud_arima.html</guid>
  <pubDate>Fri, 16 Nov 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Searching for the optimal hyper-parameters of an ARIMA model in parallel: the tidy gridsearch approach</title>
  <link>https://b-rodrigues.github.io/posts/2018-11-15-tidy_gridsearch.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://youtu.be/3NxM-AL18lU?t=33s"> <img width="400" src="https://b-rodrigues.github.io/assets/img/dank_memes.jpg" title="What a time to be alive"></a>
</p>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">
Introduction
</h2>
<p>
In this blog post, I‚Äôll use the data that I cleaned in a previous <a href="../posts/2018-11-14-luxairport.html">blog post</a>, which you can download <a href="https://github.com/b-rodrigues/avia_par_lu/tree/master">here</a>. If you want to follow along, download the monthly data.
</p>
<p>
In the previous blog post, I used the <code>auto.arima()</code> function to very quickly get a ‚Äúgood-enough‚Äù model to predict future monthly total passengers flying from LuxAirport. ‚ÄúGood-enough‚Äù models can be all you need in a lot of situations, but perhaps you‚Äôd like to have a better model. I will show here how you can get a better model by searching through a grid of hyper-parameters.
</p>
<p>
This blog post was partially inspired by: <a href="https://drsimonj.svbtle.com/grid-search-in-the-tidyverse" class="uri">https://drsimonj.svbtle.com/grid-search-in-the-tidyverse</a>
</p>
</section>
<section id="the-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-problem">
The problem
</h2>
<p>
SARIMA models have a lot of hyper-parameters, 7 in total! Three trend hyper-parameters, <em>p, d, q</em>, same as for an ARIMA model, and four seasonal hyper-parameters, <em>P, D, Q, S</em>. The traditional way t o search for these hyper-parameters is the so-called Box-Jenkins method. You can read about it <a href="https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc44a.htm">here</a>. This method was described in a 1970 book, <em>Time series analysis: Forecasting and control</em> by Box and Jenkins. The method requires that you first prepare the data by logging it and differencing it, in order to make the time series stationary. You then need to analyze ACF and PACF plots, in order to determine the right amount of lags‚Ä¶ It take some time, but this method made sense in a time were computing power was very expensive. Today, we can simply let our computer search through thousands of models, check memes on the internet, and come back to the best fit. This blog post is for you, the busy data scientist meme connoisseurs who cannot waste time with theory and other such useless time drains, when there are literally thousands of new memes being created and shared every day. Every second counts. To determine what model is best, I will do pseudo out-of-sample forecasting and compute the RMSE for each model. I will then choose the model that has the lowest RMSE.
</p>
</section>
<section id="setup" class="level2">
<h2 class="anchored" data-anchor-id="setup">
Setup
</h2>
<p>
Let‚Äôs first load some libraries:
</p>
<pre class="r"><code>library(tidyverse)
library(forecast)
library(lubridate)
library(furrr)
library(tsibble)
library(brotools)

ihs &lt;- function(x){
    log(x + sqrt(x**2 + 1))
}</code></pre>
<p>
Now, let‚Äôs load the data:
</p>
<pre class="r"><code>avia_clean_monthly &lt;- read_csv("https://raw.githubusercontent.com/b-rodrigues/avia_par_lu/master/avia_clean_monthy.csv")</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   destination = col_character(),
##   date = col_date(format = ""),
##   passengers = col_double()
## )</code></pre>
<p>
Let‚Äôs split the data into a training set and into a testing set:
</p>
<pre class="r"><code>avia_clean_train &lt;- avia_clean_monthly %&gt;%
    select(date, passengers) %&gt;%
    filter(year(date) &lt; 2015) %&gt;%
    group_by(date) %&gt;%
    summarise(total_passengers = sum(passengers)) %&gt;%
    pull(total_passengers) %&gt;%
    ts(., frequency = 12, start = c(2005, 1))

avia_clean_test &lt;- avia_clean_monthly %&gt;%
    select(date, passengers) %&gt;%
    filter(year(date) &gt;= 2015) %&gt;%
    group_by(date) %&gt;%
    summarise(total_passengers = sum(passengers)) %&gt;%
    pull(total_passengers) %&gt;%
    ts(., frequency = 12, start = c(2015, 1))

logged_train_data &lt;- ihs(avia_clean_train)

logged_test_data &lt;- ihs(avia_clean_test)</code></pre>
<p>
I also define a helper function:
</p>
<pre class="r"><code>to_tibble &lt;- function(forecast_object){
    point_estimate &lt;- forecast_object$mean %&gt;%
        as_tsibble() %&gt;%
        rename(point_estimate = value,
               date = index)

    upper &lt;- forecast_object$upper %&gt;%
        as_tsibble() %&gt;%
        spread(key, value) %&gt;%
        rename(date = index,
               upper80 = `80%`,
               upper95 = `95%`)

    lower &lt;- forecast_object$lower %&gt;%
        as_tsibble() %&gt;%
        spread(key, value) %&gt;%
        rename(date = index,
               lower80 = `80%`,
               lower95 = `95%`)

    reduce(list(point_estimate, upper, lower), full_join)
}</code></pre>
<p>
This function takes a <code>forecast</code> object as argument, and returns a nice tibble. This will be useful later, and is based on the code I already used in my previous <a href="https://www.brodrigues.co/blog/2018-11-14-luxairport/">blog post</a>.
</p>
<p>
Now, let‚Äôs take a closer look at the <code>arima()</code> function:
</p>
<pre><code>ARIMA Modelling of Time Series

Description

Fit an ARIMA model to a univariate time series.

Usage

arima(x, order = c(0L, 0L, 0L),
      seasonal = list(order = c(0L, 0L, 0L), period = NA),
      xreg = NULL, include.mean = TRUE,
      transform.pars = TRUE,
      fixed = NULL, init = NULL,
      method = c("CSS-ML", "ML", "CSS"), n.cond,
      SSinit = c("Gardner1980", "Rossignol2011"),
      optim.method = "BFGS",
      optim.control = list(), kappa = 1e6)</code></pre>
<p>
The user is supposed to enter the hyper-parameters as two lists, one called <code>order</code> for <em>p, d, q</em> and one called <code>seasonal</code> for <em>P, D, Q, S</em>. So what we need is to define these lists:
</p>
<pre class="r"><code>order_list &lt;- list("p" = seq(0, 3),
                   "d" = seq(0, 2),
                   "q" = seq(0, 3)) %&gt;%
    cross() %&gt;%
    map(lift(c))</code></pre>
<p>
I first start with <code>order_list</code>. This list has 3 elements, ‚Äúp‚Äù, ‚Äúd‚Äù and ‚Äúq‚Äù. Each element is a sequence from 0 to 3 (2 in the case of ‚Äúd‚Äù). When I pass this list to <code>purrr::cross()</code> I get the product set of the starting list, so in this case a list of 4<em>3</em>4 = 48 elements. However, this list looks pretty bad:
</p>
<pre class="r"><code>list("p" = seq(0, 3),
     "d" = seq(0, 2),
     "q" = seq(0, 3)) %&gt;%
    cross() %&gt;%
    head(3)</code></pre>
<pre><code>## [[1]]
## [[1]]$p
## [1] 0
## 
## [[1]]$d
## [1] 0
## 
## [[1]]$q
## [1] 0
## 
## 
## [[2]]
## [[2]]$p
## [1] 1
## 
## [[2]]$d
## [1] 0
## 
## [[2]]$q
## [1] 0
## 
## 
## [[3]]
## [[3]]$p
## [1] 2
## 
## [[3]]$d
## [1] 0
## 
## [[3]]$q
## [1] 0</code></pre>
<p>
I would like to have something like this instead:
</p>
<pre><code>[[1]]
p d q 
0 0 0 

[[2]]
p d q 
1 0 0 

[[3]]
p d q 
2 0 0 </code></pre>
<p>
This is possible with the last line, <code>map(lift(c))</code>. There‚Äôs a lot going on in this very small line of code. First of all, there‚Äôs <code>map()</code>. <code>map()</code> iterates over lists, and applies a function, in this case <code>lift(c)</code>. <code>purrr::lift()</code> is a very interesting function that lifts the domain of definition of a function from one type of input to another. The function whose input I am lifting is <code>c()</code>. So now, <code>c()</code> can take a list instead of a vector. Compare the following:
</p>
<pre class="r"><code># The usual

c("a", "b")</code></pre>
<pre><code>## [1] "a" "b"</code></pre>
<pre class="r"><code># Nothing happens
c(list("a", "b"))</code></pre>
<pre><code>## [[1]]
## [1] "a"
## 
## [[2]]
## [1] "b"</code></pre>
<pre class="r"><code># Magic happens
lift(c)(list("a", "b"))</code></pre>
<pre><code>## [1] "a" "b"</code></pre>
<p>
So <code>order_list</code> is exactly what I wanted:
</p>
<pre class="r"><code>head(order_list)</code></pre>
<pre><code>## [[1]]
## p d q 
## 0 0 0 
## 
## [[2]]
## p d q 
## 1 0 0 
## 
## [[3]]
## p d q 
## 2 0 0 
## 
## [[4]]
## p d q 
## 3 0 0 
## 
## [[5]]
## p d q 
## 0 1 0 
## 
## [[6]]
## p d q 
## 1 1 0</code></pre>
<p>
I do the same for <code>season_list</code>:
</p>
<pre class="r"><code>season_list &lt;- list("P" = seq(0, 3),
                    "D" = seq(0, 2),
                    "Q" = seq(0, 3),
                    "period" = 12)  %&gt;%
    cross() %&gt;%
    map(lift(c))</code></pre>
<p>
I now coerce these two lists of vectors to tibbles:
</p>
<pre class="r"><code>orderdf &lt;- tibble("order" = order_list)

seasondf &lt;- tibble("season" = season_list)</code></pre>
<p>
And I can now finally create the grid of hyper-parameters:
</p>
<pre class="r"><code>hyper_parameters_df &lt;- crossing(orderdf, seasondf)

nrows &lt;- nrow(hyper_parameters_df)

head(hyper_parameters_df)</code></pre>
<pre><code>## # A tibble: 6 x 2
##   order     season   
##   &lt;list&gt;    &lt;list&gt;   
## 1 &lt;int [3]&gt; &lt;dbl [4]&gt;
## 2 &lt;int [3]&gt; &lt;dbl [4]&gt;
## 3 &lt;int [3]&gt; &lt;dbl [4]&gt;
## 4 &lt;int [3]&gt; &lt;dbl [4]&gt;
## 5 &lt;int [3]&gt; &lt;dbl [4]&gt;
## 6 &lt;int [3]&gt; &lt;dbl [4]&gt;</code></pre>
<p>
The <code>hyper_parameters_df</code> data frame has 2304 rows, meaning, I will now estimate 2304 models, and will do so in parallel. Let‚Äôs just take a quick look at the internals of <code>hyper_parameters_df</code>:
</p>
<pre class="r"><code>glimpse(hyper_parameters_df)</code></pre>
<pre><code>## Observations: 2,304
## Variables: 2
## $ order  &lt;list&gt; [&lt;0, 0, 0&gt;, &lt;0, 0, 0&gt;, &lt;0, 0, 0&gt;, &lt;0, 0, 0&gt;, &lt;0, 0, 0&gt;, ‚Ä¶
## $ season &lt;list&gt; [&lt;0, 0, 0, 12&gt;, &lt;1, 0, 0, 12&gt;, &lt;2, 0, 0, 12&gt;, &lt;3, 0, 0, ‚Ä¶</code></pre>
<p>
So in the <code>order</code> column, the vector <code>0, 0, 0</code> is repeated as many times as there are combinations of <em>P, D, Q, S</em> for <code>season</code>. Same for all the other vectors of the <code>order</code> column.
</p>
</section>
<section id="training-the-models" class="level2">
<h2 class="anchored" data-anchor-id="training-the-models">
Training the models
</h2>
<p>
Because training these models might take some time, I will use the fantastic <code>{furrr}</code> package by <a href="https://twitter.com/dvaughan32">Davis Vaughan</a> to train the <code>arima()</code> function in parallel. For this, I first define 8 workers:
</p>
<pre class="r"><code>plan(multiprocess, workers = 8)</code></pre>
<p>
And then I run the code:
</p>
<pre class="r"><code>tic &lt;- Sys.time()
models_df &lt;- hyper_parameters_df %&gt;%
    mutate(models = future_map2(.x = order,
                         .y = season,
                         ~possibly(arima, otherwise = NULL)(x = logged_train_data,
                                                                           order = .x, seasonal = .y)))
running_time &lt;- Sys.time() - tic</code></pre>
<p>
I use <code>future_map2()</code>, which is just like <code>map2()</code> but running in parallel. I add a new column to the data called <code>models</code>, which will contain the models trained over all the different combinations of <code>order</code> and <code>season</code>. The models are trained on the <code>logged_train_data</code>.
</p>
<p>
Training the 2304 models took 18 minutes, which is plenty of time to browse the latest memes, but still quick enough that it justifies the whole approach. Let‚Äôs take a look at the <code>models_df</code> object:
</p>
<pre class="r"><code>head(models_df)</code></pre>
<pre><code>## # A tibble: 6 x 3
##   order     season    models 
##   &lt;list&gt;    &lt;list&gt;    &lt;list&gt; 
## 1 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;
## 2 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;
## 3 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;
## 4 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;
## 5 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;
## 6 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt;</code></pre>
<p>
As you can see, the <code>models</code> column contains all the trained models. The model on the first row, was trained with the hyperparameters of row 1, and so on. But, our work is not over! We now need to find the best model. First, I add a new column to the tibble, which contains the forecast. From the forecast, I extract the point estimate:
</p>
<pre class="r"><code>models_df %&gt;%
    mutate(forecast = map(models, ~possibly(forecast, otherwise = NULL)(., h = 39))) %&gt;%
    mutate(point_forecast = map(forecast, ~.$`mean`)) %&gt;%
    ....</code></pre>
<p>
You have to be familiar with a <code>forecast</code> object to understand the last line: a <code>forecast</code> object is a list with certain elements, the point estimates, the confidence intervals, and so on. To get the point estimates, I have to extract the ‚Äúmean‚Äù element from the list. Hence the weird <code>~.$mean</code>. Then I need to add a new list-column, where each element is the vector of true values, meaning the data from 2015 to 2018. Because I have to add it as a list of size 2304, I do that with <code>purrr::rerun()</code>:
</p>
<pre class="r"><code>rerun(5, c("a", "b", "c"))</code></pre>
<pre><code>## [[1]]
## [1] "a" "b" "c"
## 
## [[2]]
## [1] "a" "b" "c"
## 
## [[3]]
## [1] "a" "b" "c"
## 
## [[4]]
## [1] "a" "b" "c"
## 
## [[5]]
## [1] "a" "b" "c"</code></pre>
<p>
It is then easy to compute the RMSE, which I add as a column to the original data:
</p>
<pre class="r"><code>... %&gt;%
    mutate(true_value = rerun(nrows, logged_test_data)) %&gt;%
    mutate(rmse = map2_dbl(point_forecast, true_value,
                           ~sqrt(mean((.x - .y) ** 2))))</code></pre>
<p>
The whole workflow is here:
</p>
<pre class="r"><code>models_df &lt;- models_df %&gt;%
    mutate(forecast = map(models, ~possibly(forecast, otherwise = NULL)(., h = 39))) %&gt;%
    mutate(point_forecast = map(forecast, ~.$`mean`)) %&gt;%
    mutate(true_value = rerun(nrows, logged_test_data)) %&gt;%
    mutate(rmse = map2_dbl(point_forecast, true_value,
                           ~sqrt(mean((.x - .y) ** 2))))</code></pre>
<p>
This is how <code>models_df</code> looks now:
</p>
<pre class="r"><code>head(models_df)</code></pre>
<pre><code>## # A tibble: 6 x 7
##   order     season    models  forecast   point_forecast true_value  rmse
##   &lt;list&gt;    &lt;list&gt;    &lt;list&gt;  &lt;list&gt;     &lt;list&gt;         &lt;list&gt;     &lt;dbl&gt;
## 1 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.525
## 2 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.236
## 3 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.235
## 4 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.217
## 5 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.190
## 6 &lt;int [3]&gt; &lt;dbl [4]&gt; &lt;Arima&gt; &lt;forecast&gt; &lt;ts&gt;           &lt;ts&gt;       0.174</code></pre>
<p>
Now, I can finally select the best performing model. I select the model with minimum RMSE:
</p>
<pre class="r"><code>best_model &lt;- models_df %&gt;%
    filter(rmse == min(rmse, na.rm = TRUE))</code></pre>
<p>
And save the forecast into a new variable, as a <code>tibble</code>, using my <code>to_tibble()</code> function:
</p>
<pre class="r"><code>(best_model_forecast &lt;- to_tibble(best_model$forecast[[1]]))</code></pre>
<pre><code>## Joining, by = "date"
## Joining, by = "date"</code></pre>
<pre><code>## # A tsibble: 39 x 6 [1M]
##        date point_estimate upper80 upper95 lower80 lower95
##       &lt;mth&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1 2015 Jan           11.9    12.1    12.1    11.8    11.7
##  2 2015 Feb           11.9    12.0    12.1    11.7    11.6
##  3 2015 Mar           12.1    12.3    12.3    11.9    11.9
##  4 2015 Apr           12.2    12.3    12.4    12.0    11.9
##  5 2015 May           12.2    12.4    12.5    12.1    12.0
##  6 2015 Jun           12.3    12.4    12.5    12.1    12.0
##  7 2015 Jul           12.2    12.3    12.4    12.0    11.9
##  8 2015 Aug           12.3    12.5    12.6    12.2    12.1
##  9 2015 Sep           12.3    12.5    12.6    12.2    12.1
## 10 2015 Oct           12.2    12.4    12.5    12.1    12.0
## # ‚Ä¶ with 29 more rows</code></pre>
<p>
And now, I can plot it:
</p>
<pre class="r"><code>avia_clean_monthly %&gt;%
    group_by(date) %&gt;%
    summarise(total = sum(passengers)) %&gt;%
    mutate(total_ihs = ihs(total)) %&gt;%
    ggplot() +
    ggtitle("Logged data") +
    geom_line(aes(y = total_ihs, x = date), colour = "#82518c") +
    scale_x_date(date_breaks = "1 year", date_labels = "%m-%Y") +
    geom_ribbon(data = best_model_forecast, aes(x = date, ymin = lower95, ymax = upper95), 
                fill = "#666018", alpha = 0.2) +
    geom_line(data = best_model_forecast, aes(x = date, y = point_estimate), linetype = 2, colour = "#8e9d98") +
    theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/tidy_gridsearch-28-1.png" width="672">
</p>
<p>
Compared to the previous <a href="https://www.brodrigues.co/blog/2018-11-14-luxairport/">blog post</a>, the dotted line now seems to follow the true line even better! However, this is not suprising, as I am using the test set as a validation set, which might lead to overfitting the hyperparameters to the test set. Also, I am not saying that you should always do a gridsearch whenever you have a problem like this one. In the case of univariate time series, I am still doubtful that a gridsearch like this is really necessary. The goal of this blog post was not to teach you how to look for hyperparameters per se, but more to show you how to do a grid search the tidy way. I‚Äôll be writing about <em>proper</em> hyperparameter optimization in a future blog post. Also, the other thing I wanted to show was the power of <code>{furrr}</code>.
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>econometrics</category>
  <guid>https://b-rodrigues.github.io/posts/2018-11-15-tidy_gridsearch.html</guid>
  <pubDate>Thu, 15 Nov 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Easy time-series prediction with R: a tutorial with air traffic data from Lux Airport</title>
  <link>https://b-rodrigues.github.io/posts/2018-11-14-luxairport.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=GIQn8pab8Vc"> <img src="https://b-rodrigues.github.io/assets/img/lx_aie.jpg" title="Luxembourg's largest aircraft landing"></a>
</p>
</div>
<p>
In this blog post, I will show you how you can quickly and easily forecast a univariate time series. I am going to use data from the EU Open Data Portal on air passenger transport. You can find the data <a href="https://data.europa.eu/euodp/en/data/dataset/2EwfWXj5d94BUOzfoABKSQ">here</a>. I downloaded the data in the TSV format for Luxembourg Airport, but you could repeat the analysis for any airport.
</p>
<p>
Once you have the data, load some of the package we are going to need:
</p>
<pre class="r"><code>library(tidyverse)
library(lubridate)
library(forecast)
library(tsibble)
library(brotools)</code></pre>
<p>
and define the following function:
</p>
<pre class="r"><code>ihs &lt;- function(x){
    log(x + sqrt(x**2 + 1))
}</code></pre>
<p>
This function, the inverse hyperbolic sine, is useful to transform data in a manner that is very close to logging it, but that allows for 0‚Äôs. The data from Eurostat is not complete for some reason, so there are some 0 sometimes. To avoid having to log 0, which in R yields <code>-Inf</code>, I use this transformation.
</p>
<p>
Now, let‚Äôs load the data:
</p>
<pre class="r"><code>avia &lt;- read_tsv("avia_par_lu.tsv")</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   .default = col_character()
## )</code></pre>
<pre><code>## See spec(...) for full column specifications.</code></pre>
<p>
Let‚Äôs take a look at the data:
</p>
<pre class="r"><code>head(avia)</code></pre>
<pre><code>## # A tibble: 6 x 238
##   `unit,tra_meas,‚Ä¶ `2018Q1` `2018M03` `2018M02` `2018M01` `2017Q4` `2017Q3`
##   &lt;chr&gt;            &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;   
## 1 FLIGHT,CAF_PAS,‚Ä¶ 511      172       161       178       502      475     
## 2 FLIGHT,CAF_PAS,‚Ä¶ :        :         :         :         :        :       
## 3 FLIGHT,CAF_PAS,‚Ä¶ :        :         :         :         399      306     
## 4 FLIGHT,CAF_PAS,‚Ä¶ 485      167       151       167       493      497     
## 5 FLIGHT,CAF_PAS,‚Ä¶ 834      293       267       274       790      728     
## 6 FLIGHT,CAF_PAS,‚Ä¶ :        :         :         :         :        :       
## # ‚Ä¶ with 231 more variables: `2017Q2` &lt;chr&gt;, `2017Q1` &lt;chr&gt;,
## #   `2017M12` &lt;chr&gt;, `2017M11` &lt;chr&gt;, `2017M10` &lt;chr&gt;, `2017M09` &lt;chr&gt;,
## #   `2017M08` &lt;chr&gt;, `2017M07` &lt;chr&gt;, `2017M06` &lt;chr&gt;, `2017M05` &lt;chr&gt;,
## #   `2017M04` &lt;chr&gt;, `2017M03` &lt;chr&gt;, `2017M02` &lt;chr&gt;, `2017M01` &lt;chr&gt;,
## #   `2017` &lt;chr&gt;, `2016Q4` &lt;chr&gt;, `2016Q3` &lt;chr&gt;, `2016Q2` &lt;chr&gt;,
## #   `2016Q1` &lt;chr&gt;, `2016M12` &lt;chr&gt;, `2016M11` &lt;chr&gt;, `2016M10` &lt;chr&gt;,
## #   `2016M09` &lt;chr&gt;, `2016M08` &lt;chr&gt;, `2016M07` &lt;chr&gt;, `2016M06` &lt;chr&gt;,
## #   `2016M05` &lt;chr&gt;, `2016M04` &lt;chr&gt;, `2016M03` &lt;chr&gt;, `2016M02` &lt;chr&gt;,
## #   `2016M01` &lt;chr&gt;, `2016` &lt;chr&gt;, `2015Q4` &lt;chr&gt;, `2015Q3` &lt;chr&gt;,
## #   `2015Q2` &lt;chr&gt;, `2015Q1` &lt;chr&gt;, `2015M12` &lt;chr&gt;, `2015M11` &lt;chr&gt;,
## #   `2015M10` &lt;chr&gt;, `2015M09` &lt;chr&gt;, `2015M08` &lt;chr&gt;, `2015M07` &lt;chr&gt;,
## #   `2015M06` &lt;chr&gt;, `2015M05` &lt;chr&gt;, `2015M04` &lt;chr&gt;, `2015M03` &lt;chr&gt;,
## #   `2015M02` &lt;chr&gt;, `2015M01` &lt;chr&gt;, `2015` &lt;chr&gt;, `2014Q4` &lt;chr&gt;,
## #   `2014Q3` &lt;chr&gt;, `2014Q2` &lt;chr&gt;, `2014Q1` &lt;chr&gt;, `2014M12` &lt;chr&gt;,
## #   `2014M11` &lt;chr&gt;, `2014M10` &lt;chr&gt;, `2014M09` &lt;chr&gt;, `2014M08` &lt;chr&gt;,
## #   `2014M07` &lt;chr&gt;, `2014M06` &lt;chr&gt;, `2014M05` &lt;chr&gt;, `2014M04` &lt;chr&gt;,
## #   `2014M03` &lt;chr&gt;, `2014M02` &lt;chr&gt;, `2014M01` &lt;chr&gt;, `2014` &lt;chr&gt;,
## #   `2013Q4` &lt;chr&gt;, `2013Q3` &lt;chr&gt;, `2013Q2` &lt;chr&gt;, `2013Q1` &lt;chr&gt;,
## #   `2013M12` &lt;chr&gt;, `2013M11` &lt;chr&gt;, `2013M10` &lt;chr&gt;, `2013M09` &lt;chr&gt;,
## #   `2013M08` &lt;chr&gt;, `2013M07` &lt;chr&gt;, `2013M06` &lt;chr&gt;, `2013M05` &lt;chr&gt;,
## #   `2013M04` &lt;chr&gt;, `2013M03` &lt;chr&gt;, `2013M02` &lt;chr&gt;, `2013M01` &lt;chr&gt;,
## #   `2013` &lt;chr&gt;, `2012Q4` &lt;chr&gt;, `2012Q3` &lt;chr&gt;, `2012Q2` &lt;chr&gt;,
## #   `2012Q1` &lt;chr&gt;, `2012M12` &lt;chr&gt;, `2012M11` &lt;chr&gt;, `2012M10` &lt;chr&gt;,
## #   `2012M09` &lt;chr&gt;, `2012M08` &lt;chr&gt;, `2012M07` &lt;chr&gt;, `2012M06` &lt;chr&gt;,
## #   `2012M05` &lt;chr&gt;, `2012M04` &lt;chr&gt;, `2012M03` &lt;chr&gt;, `2012M02` &lt;chr&gt;,
## #   `2012M01` &lt;chr&gt;, `2012` &lt;chr&gt;, ‚Ä¶</code></pre>
<p>
So yeah, useless in that state. The first column actually is composed of 3 columns, merged together, and instead of having one column with the date, and another with the value, we have one column per date. Some cleaning is necessary before using this data.
</p>
<p>
Let‚Äôs start with going from a wide to a long data set:
</p>
<pre class="r"><code>avia %&gt;%
    select("unit,tra_meas,airp_pr\\time", contains("20")) %&gt;%
    gather(date, passengers, -`unit,tra_meas,airp_pr\\time`)</code></pre>
<p>
The first line makes it possible to only select the columns that contain the string ‚Äú20‚Äù, so selecting columns from 2000 onward. Then, using gather, I go from long to wide. The data looks like this now:
</p>
<pre><code>## # A tibble: 117,070 x 3
##    `unit,tra_meas,airp_pr\\time`  date   passengers
##    &lt;chr&gt;                          &lt;chr&gt;  &lt;chr&gt;     
##  1 FLIGHT,CAF_PAS,LU_ELLX_AT_LOWW 2018Q1 511       
##  2 FLIGHT,CAF_PAS,LU_ELLX_BE_EBBR 2018Q1 :         
##  3 FLIGHT,CAF_PAS,LU_ELLX_CH_LSGG 2018Q1 :         
##  4 FLIGHT,CAF_PAS,LU_ELLX_CH_LSZH 2018Q1 485       
##  5 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDF 2018Q1 834       
##  6 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDI 2018Q1 :         
##  7 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDM 2018Q1 1095      
##  8 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDR 2018Q1 :         
##  9 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDT 2018Q1 :         
## 10 FLIGHT,CAF_PAS,LU_ELLX_DK_EKCH 2018Q1 :         
## # ‚Ä¶ with 117,060 more rows</code></pre>
<p>
Now, let‚Äôs separate the first column into 3 columns:
</p>
<pre class="r"><code>avia %&gt;%
    select("unit,tra_meas,airp_pr\\time", contains("20")) %&gt;%
    gather(date, passengers, -`unit,tra_meas,airp_pr\\time`) %&gt;%
     separate(col = `unit,tra_meas,airp_pr\\time`, into = c("unit", "tra_meas", "air_pr\\time"), sep = ",")</code></pre>
<p>
This separates the first column into 3 new columns, ‚Äúunit‚Äù, ‚Äútra_meas‚Äù and ‚Äúair_pr‚Äù. This step is not necessary for the rest of the analysis, but might as well do it. The data looks like this now:
</p>
<pre><code>## # A tibble: 117,070 x 5
##    unit   tra_meas `air_pr\\time`  date   passengers
##    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt;     
##  1 FLIGHT CAF_PAS  LU_ELLX_AT_LOWW 2018Q1 511       
##  2 FLIGHT CAF_PAS  LU_ELLX_BE_EBBR 2018Q1 :         
##  3 FLIGHT CAF_PAS  LU_ELLX_CH_LSGG 2018Q1 :         
##  4 FLIGHT CAF_PAS  LU_ELLX_CH_LSZH 2018Q1 485       
##  5 FLIGHT CAF_PAS  LU_ELLX_DE_EDDF 2018Q1 834       
##  6 FLIGHT CAF_PAS  LU_ELLX_DE_EDDI 2018Q1 :         
##  7 FLIGHT CAF_PAS  LU_ELLX_DE_EDDM 2018Q1 1095      
##  8 FLIGHT CAF_PAS  LU_ELLX_DE_EDDR 2018Q1 :         
##  9 FLIGHT CAF_PAS  LU_ELLX_DE_EDDT 2018Q1 :         
## 10 FLIGHT CAF_PAS  LU_ELLX_DK_EKCH 2018Q1 :         
## # ‚Ä¶ with 117,060 more rows</code></pre>
<p>
The next steps are simple renamings. I have copy-pasted the information from the Eurostat page where you can <a href="http://appsso.eurostat.ec.europa.eu/nui/show.do?dataset=avia_par_lu&amp;lang=en">view the data online</a>. If you click here:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/eurostat_click_here.png"><!-- -->
</p>
<p>
you will be able to select the variables you want displayed in the table, as well as the dictionary of the variables. I simply copy pasted it and recoded the variables. You can take a look at the whole cleaning workflow by clicking ‚ÄúClick to expand‚Äù below:
</p>
<details>
<p>
</p><summary>
Click here to take a look at the whole cleaning workflow
</summary>
<p></p>
<pre class="r"><code>avia_clean &lt;- avia %&gt;%
    select("unit,tra_meas,airp_pr\\time", contains("20")) %&gt;%
    gather(date, passengers, -`unit,tra_meas,airp_pr\\time`) %&gt;%
    separate(col = `unit,tra_meas,airp_pr\\time`, into = c("unit", "tra_meas", "air_pr\\time"), sep = ",") %&gt;%
    mutate(tra_meas = fct_recode(tra_meas,
         `Passengers on board` = "PAS_BRD",
         `Passengers on board (arrivals)` = "PAS_BRD_ARR",
         `Passengers on board (departures)` = "PAS_BRD_DEP",
         `Passengers carried` = "PAS_CRD",
         `Passengers carried (arrival)` = "PAS_CRD_ARR",
         `Passengers carried (departures)` = "PAS_CRD_DEP",
         `Passengers seats available` = "ST_PAS",
         `Passengers seats available (arrivals)` = "ST_PAS_ARR",
         `Passengers seats available (departures)` = "ST_PAS_DEP",
         `Commercial passenger air flights` = "CAF_PAS",
         `Commercial passenger air flights (arrivals)` = "CAF_PAS_ARR",
         `Commercial passenger air flights (departures)` = "CAF_PAS_DEP")) %&gt;%
    mutate(unit = fct_recode(unit,
                             Passenger = "PAS",
                             Flight = "FLIGHT",
                             `Seats and berths` = "SEAT")) %&gt;%
    mutate(destination = fct_recode(`air_pr\\time`,
                                     `WIEN-SCHWECHAT` = "LU_ELLX_AT_LOWW",
                                     `BRUSSELS` = "LU_ELLX_BE_EBBR",
                                     `GENEVA` = "LU_ELLX_CH_LSGG",
                                     `ZURICH` = "LU_ELLX_CH_LSZH",
                                     `FRANKFURT/MAIN` = "LU_ELLX_DE_EDDF",
                                     `HAMBURG` = "LU_ELLX_DE_EDDH",
                                     `BERLIN-TEMPELHOF` = "LU_ELLX_DE_EDDI",
                                     `MUENCHEN` = "LU_ELLX_DE_EDDM",
                                     `SAARBRUECKEN` = "LU_ELLX_DE_EDDR",
                                     `BERLIN-TEGEL` = "LU_ELLX_DE_EDDT",
                                     `KOBENHAVN/KASTRUP` = "LU_ELLX_DK_EKCH",
                                     `HURGHADA / INTL` = "LU_ELLX_EG_HEGN",
                                     `IRAKLION/NIKOS KAZANTZAKIS` = "LU_ELLX_EL_LGIR",
                                     `FUERTEVENTURA` = "LU_ELLX_ES_GCFV",
                                     `GRAN CANARIA` = "LU_ELLX_ES_GCLP",
                                     `LANZAROTE` = "LU_ELLX_ES_GCRR",
                                     `TENERIFE SUR/REINA SOFIA` = "LU_ELLX_ES_GCTS",
                                     `BARCELONA/EL PRAT` = "LU_ELLX_ES_LEBL",
                                     `ADOLFO SUAREZ MADRID-BARAJAS` = "LU_ELLX_ES_LEMD",
                                     `MALAGA/COSTA DEL SOL` = "LU_ELLX_ES_LEMG",
                                     `PALMA DE MALLORCA` = "LU_ELLX_ES_LEPA",
                                     `SYSTEM - PARIS` = "LU_ELLX_FR_LF90",
                                     `NICE-COTE D'AZUR` = "LU_ELLX_FR_LFMN",
                                     `PARIS-CHARLES DE GAULLE` = "LU_ELLX_FR_LFPG",
                                     `STRASBOURG-ENTZHEIM` = "LU_ELLX_FR_LFST",
                                     `KEFLAVIK` = "LU_ELLX_IS_BIKF",
                                     `MILANO/MALPENSA` = "LU_ELLX_IT_LIMC",
                                     `BERGAMO/ORIO AL SERIO` = "LU_ELLX_IT_LIME",
                                     `ROMA/FIUMICINO` = "LU_ELLX_IT_LIRF",
                                     `AGADIR/AL MASSIRA` = "LU_ELLX_MA_GMAD",
                                     `AMSTERDAM/SCHIPHOL` = "LU_ELLX_NL_EHAM",
                                     `WARSZAWA/CHOPINA` = "LU_ELLX_PL_EPWA",
                                     `PORTO` = "LU_ELLX_PT_LPPR",
                                     `LISBOA` = "LU_ELLX_PT_LPPT",
                                     `STOCKHOLM/ARLANDA` = "LU_ELLX_SE_ESSA",
                                     `MONASTIR/HABIB BOURGUIBA` = "LU_ELLX_TN_DTMB",
                                     `ENFIDHA-HAMMAMET INTERNATIONAL` = "LU_ELLX_TN_DTNH",
                                     `ENFIDHA ZINE EL ABIDINE BEN ALI` = "LU_ELLX_TN_DTNZ",
                                     `DJERBA/ZARZIS` = "LU_ELLX_TN_DTTJ",
                                     `ANTALYA (MIL-CIV)` = "LU_ELLX_TR_LTAI",
                                     `ISTANBUL/ATATURK` = "LU_ELLX_TR_LTBA",
                                     `SYSTEM - LONDON` = "LU_ELLX_UK_EG90",
                                     `MANCHESTER` = "LU_ELLX_UK_EGCC",
                                     `LONDON GATWICK` = "LU_ELLX_UK_EGKK",
                                     `LONDON/CITY` = "LU_ELLX_UK_EGLC",
                                     `LONDON HEATHROW` = "LU_ELLX_UK_EGLL",
                                     `LONDON STANSTED` = "LU_ELLX_UK_EGSS",
                                     `NEWARK LIBERTY INTERNATIONAL, NJ.` = "LU_ELLX_US_KEWR",
                                     `O.R TAMBO INTERNATIONAL` = "LU_ELLX_ZA_FAJS")) %&gt;%
    mutate(passengers = as.numeric(passengers)) %&gt;%
    select(unit, tra_meas, destination, date, passengers)</code></pre>
<pre><code>## Warning: NAs introduced by coercion</code></pre>
</details>
<p>
There is quarterly data and monthly data. Let‚Äôs separate the two:
</p>
<pre class="r"><code>avia_clean_quarterly &lt;- avia_clean %&gt;%
    filter(tra_meas == "Passengers on board (arrivals)",
           !is.na(passengers)) %&gt;%
    filter(str_detect(date, "Q")) %&gt;%
    mutate(date = yq(date))</code></pre>
<p>
In the ‚Äúdate‚Äù column, I detect the observations with ‚ÄúQ‚Äù in their name, indicating that it is quarterly data. I do the same for monthly data, but I have to add the string ‚Äú01‚Äù to the dates. This transforms a date that looks like this ‚Äú2018M1‚Äù to this ‚Äú2018M101‚Äù. ‚Äú2018M101‚Äù can then be converted into a date by using the <code>ymd()</code> function from lubridate. <code>yq()</code> was used for the quarterly data.
</p>
<pre class="r"><code>avia_clean_monthly &lt;- avia_clean %&gt;%
    filter(tra_meas == "Passengers on board (arrivals)",
           !is.na(passengers)) %&gt;%
    filter(str_detect(date, "M")) %&gt;%
    mutate(date = paste0(date, "01")) %&gt;%
    mutate(date = ymd(date)) %&gt;%
    select(destination, date, passengers)</code></pre>
<p>
Time for some plots. Let‚Äôs start with the raw data:
</p>
<pre class="r"><code>avia_clean_monthly %&gt;%
    group_by(date) %&gt;%
    summarise(total = sum(passengers)) %&gt;%
    ggplot() +
    ggtitle("Raw data") +
    geom_line(aes(y = total, x = date), colour = "#82518c") +
    scale_x_date(date_breaks = "1 year", date_labels = "%m-%Y") + 
    theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/luxairport-16-1.png" width="672">
</p>
<p>
And now with the logged data (or rather, the data transformed using the inverted hyperbolic sine transformation):
</p>
<pre class="r"><code>avia_clean_monthly %&gt;%
    group_by(date) %&gt;%
    summarise(total = sum(passengers)) %&gt;%
    mutate(total_ihs = ihs(total)) %&gt;%
    ggplot() +
    ggtitle("Logged data") +
    geom_line(aes(y = total_ihs, x = date), colour = "#82518c") +
    scale_x_date(date_breaks = "1 year", date_labels = "%m-%Y") + 
    theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/luxairport-17-1.png" width="672">
</p>
<p>
We clearly see a seasonal pattern in the data. There is also an upward trend. We will have to deal with these two problems if we want to do some forecasting. For this, let‚Äôs limit ourselves to data from before 2015, and convert the ‚Äúpassengers‚Äù column from the data to a time series object, using the <code>ts()</code> function:
</p>
<pre class="r"><code>avia_clean_train &lt;- avia_clean_monthly %&gt;%
    select(date, passengers) %&gt;%
    filter(year(date) &lt; 2015) %&gt;%
    group_by(date) %&gt;%
    summarise(total_passengers = sum(passengers)) %&gt;%
    pull(total_passengers) %&gt;%
    ts(., frequency = 12, start = c(2005, 1))</code></pre>
<p>
We will try to <em>pseudo</em>-forecast the data from 2015 to the last point available, March 2018. First, let‚Äôs tranform the data:
</p>
<pre class="r"><code>logged_data &lt;- ihs(avia_clean_train)</code></pre>
<p>
Taking the log, or ihs of the data deals with stabilizing the variance of the time series.
</p>
<p>
There might also be a need to difference the data. Computing the differences between consecutive observations makes the time-series stationary. This will be taken care of by the <code>auto.arima()</code> function, if needed. The <code>auto.arima()</code> function returns the best ARIMA model according to different statistical criterions, such as the AIC, AICc or BIC.
</p>
<pre class="r"><code>(model_fit &lt;- auto.arima(logged_data))</code></pre>
<pre><code>## Series: logged_data 
## ARIMA(2,1,1)(2,1,0)[12] 
## 
## Coefficients:
##           ar1      ar2      ma1     sar1     sar2
##       -0.4061  -0.2431  -0.3562  -0.5590  -0.3282
## s.e.   0.2003   0.1432   0.1994   0.0911   0.0871
## 
## sigma^2 estimated as 0.004503:  log likelihood=137.11
## AIC=-262.21   AICc=-261.37   BIC=-246.17</code></pre>
<p>
<code>auto.arima()</code> found that the best model would be an <img src="https://latex.codecogs.com/png.latex?(ARIMA(2,%201,%201)(2,%201,%200)_%7B12%7D)">. This is an seasonal autoregressive model, with p = 2, d = 1, q = 1, P = 2 and D = 1.
</p>
<pre class="r"><code>model_forecast &lt;- forecast(model_fit, h = 39)</code></pre>
<p>
I can now forecast the model for the next 39 months (which correspond to the data available).
</p>
<p>
To plot the forecast, one could do a simple call to the plot function. But the resulting plot is not very aesthetic. To plot my own, I have to grab the data that was forecast, and do some munging again:
</p>
<pre class="r"><code>point_estimate &lt;- model_forecast$mean %&gt;%
    as_tsibble() %&gt;%
    rename(point_estimate = value,
           date = index)

upper &lt;- model_forecast$upper %&gt;%
    as_tsibble() %&gt;%
    spread(key, value) %&gt;%
    rename(date = index,
           upper80 = `80%`,
           upper95 = `95%`)

lower &lt;- model_forecast$lower %&gt;%
    as_tsibble() %&gt;%
    spread(key, value) %&gt;%
    rename(date = index,
           lower80 = `80%`,
           lower95 = `95%`)

estimated_data &lt;- reduce(list(point_estimate, upper, lower), full_join, by = "date")</code></pre>
<p>
<code>as_tsibble()</code> is a function from the <code>{tsibble}</code> package that converts objects that are <em>time-series aware</em> to <em>time-aware</em> tibbles. If you are not familiar with <code>ts_tibble()</code>, I urge you to run the above lines one by one, and especially to compare <code>as_tsibble()</code> with the standard <code>as_tibble()</code> from the <code>{tibble}</code> package.
</p>
<p>
This is how <code>estimated_data</code> looks:
</p>
<pre class="r"><code>head(estimated_data)</code></pre>
<pre><code>## # A tsibble: 6 x 6 [1M]
##       date point_estimate upper80 upper95 lower80 lower95
##      &lt;mth&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1 2015 Jan           11.9    12.0    12.1    11.8    11.8
## 2 2015 Feb           11.9    12.0    12.0    11.8    11.7
## 3 2015 Mar           12.1    12.2    12.3    12.0    12.0
## 4 2015 Apr           12.2    12.3    12.4    12.1    12.1
## 5 2015 May           12.3    12.4    12.4    12.2    12.1
## 6 2015 Jun           12.3    12.4    12.5    12.2    12.1</code></pre>
<p>
We can now plot the data, with the forecast, and with the 95% confidence interval:
</p>
<pre class="r"><code>avia_clean_monthly %&gt;%
    group_by(date) %&gt;%
    summarise(total = sum(passengers)) %&gt;%
    mutate(total_ihs = ihs(total)) %&gt;%
    ggplot() +
    ggtitle("Logged data") +
    geom_line(aes(y = total_ihs, x = date), colour = "#82518c") +
    scale_x_date(date_breaks = "1 year", date_labels = "%m-%Y") +
    geom_ribbon(data = estimated_data, aes(x = date, ymin = lower95, ymax = upper95), fill = "#666018", alpha = 0.2) +
    geom_line(data = estimated_data, aes(x = date, y = point_estimate), linetype = 2, colour = "#8e9d98") +
    theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/luxaiport-24-1.png" width="672">
</p>
<p>
The pseudo-forecast (the dashed line) is not very far from the truth, only overestimating the seasonal peaks, but the true line is within the 95% confidence interval, which is good!
</p>



 ]]></description>
  <category>R</category>
  <category>econometrics</category>
  <guid>https://b-rodrigues.github.io/posts/2018-11-14-luxairport.html</guid>
  <pubDate>Wed, 14 Nov 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Analyzing NetHack data, part 2: What players kill the most</title>
  <link>https://b-rodrigues.github.io/posts/2018-11-10-nethack_analysis_part2.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=VnW2g6qbbrA"> <img src="https://b-rodrigues.github.io/assets/img/monsters.png" title="Wizard of Yendor battle music"></a>
</p>
</div>
<p>
Link to <a href="../posts/2018-11-01-nethack.html">webscraping the data</a>
</p>
<p>
Link to <a href="../posts/2018-11-03-nethack_analysis.html">Analysis, part 1</a>
</p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">
Introduction
</h2>
<p>
This is the third blog post that deals with data from the game NetHack, and oh boy, did a lot of things happen since the last blog post! Here‚Äôs a short timeline of the events:
</p>
<ul>
<li>
I scraped data from <a href="https://alt.org/nethack/">alt.org/nethack</a> and made a package with the data available on Github (that package was too big for CRAN)
</li>
<li>
Then, I analyzed the data, focusing on what monsters kill the players the most, and also where players die the most
</li>
<li>
<span class="citation"><span class="citation" data-cites="GridSageGames">@GridSageGames</span></span>, developer of the roguelike Cogmind and moderator of the roguelike subreddit, posted the blog post on reddit
</li>
<li>
I noticed that actually, by scraping the data like I did, I only got a sample of 100 daily games
</li>
<li>
This point was also discussed on Reddit, and bhhak, an UnNetHack developer (UnNetHack is a fork of NetHack) suggested I used the xlogfiles instead
</li>
<li>
xlogfiles are log files generated by NetHack, and are also available on alt.org/nethack
</li>
<li>
I started scraping them, and getting a lot more data
</li>
<li>
I got contacted on twitter by <span class="citation"><span class="citation" data-cites="paxed">@paxed</span></span>, an admin of alt.org/nethack:
</li>
</ul>
{{% tweet ‚Äú1059333642592366593‚Äù %}}
<ul>
<li>
He gave me access to ALL THE DATA on alt.org/nethack!
</li>
<li>
The admins of <a href="https://alt.org/nethack/">alt.org/nethack</a> will release all the data to the public!
</li>
</ul>
<p>
So, I will now continue with the blog post I wanted to do in the first place; focusing now on what roles players choose to play the most, and also which monsters they kill the most. BUT! Since all the data will be released to the public, my <code>{nethack}</code> package that contains data that I&nbsp;scraped is not that useful anymore. So I changed the nature of the package. Now the package contains some functions: a function to parse and prepare the xlogfiles from NetHack that you can download from <a href="https://alt.org/nethack/">alt.org/nethack</a> (or from any other public server), a function to download dumplogs such as this <a href="http://archive.is/7awsb">one</a>. These dumplogs contain a lot of info that I will extract in this blog post, using another function included in the <code>{nethack}</code> package. The package also contains a sample of 6000 runs from NetHack version 3.6.1.
</p>
<p>
You can install the package with the following command line:
</p>
<pre class="r"><code>devtools::install_github("b-rodrigues/nethack")</code></pre>
</section>
<section id="the-nethack-package" class="level2">
<h2 class="anchored" data-anchor-id="the-nethack-package">
The <code>{nethack}</code> package
</h2>
<p>
In <a href="https://www.brodrigues.co/blog/2018-11-03-nethack_analysis/">part 1</a> I showed what killed players the most. Here, I will focus on what monsters players kill the most. Let‚Äôs start by loading some packages:
</p>
<pre class="r"><code>library(tidyverse)
library(lubridate)
library(magrittr)
library(ggridges)
library(brotools)
library(rvest)
library(nethack)</code></pre>
<p>
Let‚Äôs first describe the data:
</p>
<pre class="r"><code>brotools::describe(nethack) %&gt;% 
  print(n = Inf)</code></pre>
<pre><code>## # A tibble: 23 x 17
##    variable type    nobs     mean       sd mode       min     max      q05
##    &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;
##  1 deathdn‚Ä¶ Nume‚Ä¶ 6.00e3  8.45e-1  1.30e+0 2       0.      7.00e0   0.    
##  2 deathlev Nume‚Ä¶ 6.00e3  4.32e+0  3.69e+0 10     -5.00e0  4.50e1   1.00e0
##  3 deaths   Nume‚Ä¶ 6.00e3  8.88e-1  3.54e-1 1       0.      5.00e0   0.    
##  4 endtime  Nume‚Ä¶ 6.00e3  1.53e+9  4.72e+6 1534‚Ä¶   1.52e9  1.54e9   1.53e9
##  5 hp       Nume‚Ä¶ 6.00e3  6.64e+0  4.96e+1 -1     -9.40e1  1.79e3  -8.00e0
##  6 maxhp    Nume‚Ä¶ 6.00e3  3.82e+1  5.29e+1 57      2.00e0  1.80e3   1.10e1
##  7 maxlvl   Nume‚Ä¶ 6.00e3  5.52e+0  6.36e+0 10      1.00e0  5.30e1   1.00e0
##  8 points   Nume‚Ä¶ 6.00e3  4.69e+4  4.18e+5 10523   0.      9.92e6   1.40e1
##  9 realtime Nume‚Ä¶ 6.00e3  4.42e+3  1.60e+4 4575    0.      3.23e5   6.90e1
## 10 startti‚Ä¶ Nume‚Ä¶ 6.00e3  1.53e+9  4.72e+6 1534‚Ä¶   1.52e9  1.54e9   1.53e9
## 11 turns    Nume‚Ä¶ 6.00e3  3.60e+3  9.12e+3 6797    3.10e1  1.97e5   9.49e1
## 12 align    Char‚Ä¶ 6.00e3 NA       NA       Cha    NA      NA       NA     
## 13 align0   Char‚Ä¶ 6.00e3 NA       NA       Cha    NA      NA       NA     
## 14 death    Char‚Ä¶ 6.00e3 NA       NA       kill‚Ä¶  NA      NA       NA     
## 15 gender   Char‚Ä¶ 6.00e3 NA       NA       Fem    NA      NA       NA     
## 16 gender0  Char‚Ä¶ 6.00e3 NA       NA       Fem    NA      NA       NA     
## 17 killed_‚Ä¶ Char‚Ä¶ 6.00e3 NA       NA       fain‚Ä¶  NA      NA       NA     
## 18 name     Char‚Ä¶ 6.00e3 NA       NA       drud‚Ä¶  NA      NA       NA     
## 19 race     Char‚Ä¶ 6.00e3 NA       NA       Elf    NA      NA       NA     
## 20 role     Char‚Ä¶ 6.00e3 NA       NA       Wiz    NA      NA       NA     
## 21 dumplog  List  1.33e6 NA       NA       &lt;NA&gt;   NA      NA       NA     
## 22 birthda‚Ä¶ Date  6.00e3 NA       NA       &lt;NA&gt;   NA      NA       NA     
## 23 deathda‚Ä¶ Date  6.00e3 NA       NA       &lt;NA&gt;   NA      NA       NA     
## # ... with 8 more variables: q25 &lt;dbl&gt;, median &lt;dbl&gt;, q75 &lt;dbl&gt;,
## #   q95 &lt;dbl&gt;, n_missing &lt;int&gt;, n_unique &lt;int&gt;, starting_date &lt;date&gt;,
## #   ending_date &lt;date&gt;</code></pre>
<p>
All these columns are included in xlogfiles. The data was prepared using two functions, included in <code>{nethack}</code>:
</p>
<pre class="r"><code>xlog &lt;- read_delim("~/path/to/nethack361_xlog.csv", "\t", escape_double = FALSE, 
                   col_names = FALSE, trim_ws = TRUE)

xlog_df &lt;- clean_xlog(xlog)</code></pre>
<p>
<code>nethack361_xlog.csv</code> is the raw xlogfiles that you can get from NetHack public servers. <code>clean_xlog()</code> is a function that parses an xlogfile and returns a clean data frame. <code>xlog_df</code> will be a data frame that will look just as the one included in <code>{nethack}</code>. It is then possible to get the dumplog from each run included in <code>xlog_df</code> using <code>get_dumplog()</code>:
</p>
<pre class="r"><code>xlog_df &lt;- get_dumplog(xlog_df)</code></pre>
<p>
This function adds a column called <code>dumplog</code> with the dumplog of that run. I will now analyze the dumplog file, by focusing on monsters vanquished, genocided or extinct. In a future blogpost I will focus on other achievements.
</p>
</section>
<section id="roles-played-and-other-starting-stats" class="level2">
<h2 class="anchored" data-anchor-id="roles-played-and-other-starting-stats">
Roles played (and other starting stats)
</h2>
<p>
I will take a look at the races, roles, gender and alignment players start with the most. I will do pie charts to visualize these variable, so first, let‚Äôs start by writing a general function that allows me to do just that:
</p>
<pre class="r"><code>create_pie &lt;- function(dataset, variable, repel = FALSE){

  if(repel){
    geom_label &lt;- function(...){
      ggrepel::geom_label_repel(...)
    }
  }

  variable &lt;- enquo(variable)

  dataset %&gt;%
    count((!!variable)) %&gt;%
    mutate(total = sum(n),
           freq = n/total,
           labels = scales::percent(freq)) %&gt;% 
    arrange(desc(freq)) %&gt;%
    ggplot(aes(x = "", y = freq, fill = (!!variable))) + 
    geom_col() + 
    geom_label(aes(label = labels), position = position_stack(vjust = 0.25), show.legend = FALSE) + 
    coord_polar("y") + 
    theme_blog() + 
    scale_fill_blog() + 
    theme(legend.title = element_blank(),
          panel.grid = element_blank(),
          axis.text = element_blank(),
          axis.title = element_blank())
}</code></pre>
<p>
Now I can easily plot the share of races chosen:
</p>
<pre class="r"><code>create_pie(nethack, race)</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis_part2-9-1.png" width="672">
</p>
<p>
or the share of alignment:
</p>
<pre class="r"><code>create_pie(nethack, align0)</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis_part2-10-1.png" width="672">
</p>
<p>
Same for the share of gender:
</p>
<pre class="r"><code>create_pie(nethack, gender0)</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis_part2-11-1.png" width="672">
</p>
<p>
and finally for the share of roles:
</p>
<pre class="r"><code>create_pie(nethack, role, repel = TRUE) </code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis_part2-12-1.png" width="672">
</p>
<p>
<code>create_pie()</code> is possible thanks to <em>tidy evaluation</em> in <a href="https://www.tidyverse.org/articles/2018/07/ggplot2-3-0-0/"><code>{ggplot2}</code></a>, which makes it possible to write a function that passes data frame columns down to <code>aes()</code>. Before version 3.0 of <code>{ggplot2}</code> this was not possible, and writing such a function would have been a bit more complicated. Now, it‚Äôs as easy as pie, if I dare say.
</p>
<p>
Something else I want to look at, is the distribution of turns by role:
</p>
<pre class="r"><code>nethack %&gt;%
  filter(turns &lt; quantile(turns, 0.98)) %&gt;%
  ggplot(aes(x = turns, y = role, group = role, fill = role)) +
    geom_density_ridges(scale = 6, size = 0.25, rel_min_height = 0.01) + 
    theme_blog() + 
    scale_fill_blog() + 
    theme(axis.text.y = element_blank(),
          axis.title.y = element_blank())</code></pre>
<pre><code>## Picking joint bandwidth of 486</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis_part2-13-1.png" width="672">
</p>
<p>
I use the very cool <code>{ggridges}</code> package for that. The distribution seems to mostly be the same (of course, one should do a statistical test to be sure), but the one for the role ‚ÄúValkyrie‚Äù seems to be quite different from the others. It is known that it is easier to win the game playing as a Valkyrie, but a question remains: is it really easier as a Valkyrie, or do good players tend to play as Valkyries more often?
</p>
</section>
<section id="creatures-vanquished-genocided-or-extinct" class="level2">
<h2 class="anchored" data-anchor-id="creatures-vanquished-genocided-or-extinct">
Creatures vanquished, genocided or extinct
</h2>
<p>
The dumplog lists which, and how many of which, creatures were vanquished during the run, as well as creatures that were genocided and extinct. The player can genocide an entire species by reading a <em>scroll of genocide</em> (or by sitting on a throne). A species gets extinct if the player manages to kill every monster from that species (there‚Äôs other ways too, but for the sake of simplicity, let‚Äôs just say that when the players kills every monster from a species, the species is extinct). The following lines are an extract of a dumplog:
</p>
<pre><code>"Vanquished creatures:"
"    Baalzebub"
"    Orcus"
"    Juiblex"
"the Wizard of Yendor (4 times)"
"    Pestilence (thrice)"
"    Famine"
"    Vlad the Impaler"
"  4 arch-liches"
"  an arch-lich"
"  a high priest"
"..."
"..."
"..."
"2873 creatures vanquished." </code></pre>
<p>
If I want to analyze this, I have to first solve some problems:
</p>
<ul>
<li>
Replace ‚Äúa‚Äù and ‚Äúan‚Äù by ‚Äú1‚Äù
</li>
<li>
Put the digit in the string ‚Äú(4 times)‚Äù in front of the name of the monster (going from ‚Äúthe Wizard of Yendor (4 times)‚Äù to ‚Äú4 the Wizard of Yendor‚Äù)
</li>
<li>
Do something similar for ‚Äútwice‚Äù and ‚Äúthrice‚Äù
</li>
<li>
Put everything into singular (for example, arch-liches into arch-lich)
</li>
<li>
Trim whitespace
</li>
<li>
Extract the genocided or extinct status from the dumplog too
</li>
<li>
Finally, return a data frame with all the needed info
</li>
</ul>
<p>
I wrote a function called <code>extracted_defeated_monsters()</code> and included it in the <code>{nethack}</code> package. I discuss this function in appendix, but what it does is extracting information from dumplog files about vanquished, genocided or extinct monsters and returns a tidy dataframe with that info. This function has a lot of things going on inside it, so if you‚Äôre interested in learning more about regular expression and other <code>{tidyverse}</code> tricks, I really encourage you to read its source code.
</p>
<p>
I can now easily add this info to my data:
</p>
<pre class="r"><code>nethack %&lt;&gt;%
  mutate(monsters_destroyed = map(dumplog, ~possibly(extract_defeated_monsters, otherwise = NA)(.)))</code></pre>
<p>
Let‚Äôs take a look at one of them:
</p>
<pre class="r"><code>nethack$monsters_destroyed[[117]]</code></pre>
<pre><code>## # A tibble: 285 x 3
##    value monster              status
##    &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; 
##  1     1 baalzebub            &lt;NA&gt;  
##  2     1 orcu                 &lt;NA&gt;  
##  3     1 juiblex              &lt;NA&gt;  
##  4     4 the wizard of yendor &lt;NA&gt;  
##  5     3 pestilence           &lt;NA&gt;  
##  6     1 famine               &lt;NA&gt;  
##  7     1 vlad the impaler     &lt;NA&gt;  
##  8     4 arch-lich            &lt;NA&gt;  
##  9     1 high priest          &lt;NA&gt;  
## 10     1 medusa               &lt;NA&gt;  
## # ... with 275 more rows</code></pre>
<pre class="r"><code>nethack$monsters_destroyed[[117]] %&gt;% 
  count(status)</code></pre>
<pre><code>## # A tibble: 3 x 2
##   status        n
##   &lt;chr&gt;     &lt;int&gt;
## 1 extinct       2
## 2 genocided     7
## 3 &lt;NA&gt;        276</code></pre>
<p>
The status variable tells us if that monster was genocided or extinct during that run. <code>status</code> equal to ‚ÄúNA‚Äù means vanquished.
</p>
<p>
It is now possible to look at, say, the top 15 vanquished monsters (normalized):
</p>
<pre class="r"><code>nethack %&gt;%
  filter(!is.na(monsters_destroyed)) %&gt;%
  pull(monsters_destroyed) %&gt;%
  bind_rows %&gt;%
  group_by(monster) %&gt;%
  summarise(total = sum(value)) %&gt;%
  top_n(15) %&gt;%
  ungroup() %&gt;%
  mutate(norm_total = (total - min(total))/(max(total) - min(total))) %&gt;%
  mutate(monster = fct_reorder(monster, norm_total, .desc = FALSE)) %&gt;%
  ggplot() + 
  geom_col(aes(y = norm_total, x = monster)) + 
  coord_flip() + 
  theme_blog() + 
  scale_fill_blog() + 
  ylab("Ranking") +
  xlab("Monster")</code></pre>
<pre><code>## Selecting by total</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis_part2-16-1.png" width="672">
</p>
<p>
In this type of graph, the most vanquished monster, ‚Äúgnome‚Äù has a value of 1, and the least vanquished one, 0. This normalization step is also used in the pre-processing step of machine learning algorithms. This helps convergence of the gradient descent algorithm for instance.
</p>
<p>
Monsters can also get genocided or extinct. Let‚Äôs make a pie chart of the proportion of genocided and extinct monsters (I lump monsters that are genocided or extinct less than 5% of the times into a category called other). Because I want two pie charts, I nest the data after having grouped it by the status variable. This is a trick I discussed in this blog <a href="https://www.brodrigues.co/blog/2017-03-29-make-ggplot2-purrr/">post</a> and that I use very often:
</p>
<pre class="r"><code>nethack %&gt;%
  filter(!is.na(monsters_destroyed)) %&gt;%
  pull(monsters_destroyed) %&gt;%
  bind_rows %&gt;%
  filter(!is.na(status)) %&gt;%
  group_by(status) %&gt;% 
  count(monster) %&gt;% 
  mutate(monster = fct_lump(monster, prop = 0.05, w = n)) %&gt;% 
  group_by(status, monster) %&gt;% 
  summarise(total_count = sum(n)) %&gt;%
  mutate(freq = total_count/sum(total_count),
         labels = scales::percent(freq)) %&gt;%
  arrange(desc(freq)) %&gt;%
  group_by(status) %&gt;%
  nest() %&gt;%
  mutate(pie_chart = map2(.x = status,
                          .y = data,
                          ~ggplot(data = .y,
                                  aes(x = "", y = freq, fill = (monster))) + 
    geom_col() + 
    ggrepel::geom_label_repel(aes(label = labels), position = position_stack(vjust = 0.25), show.legend = FALSE) + 
    coord_polar("y") + 
    theme_blog() + 
    scale_fill_blog() + 
      ggtitle(.x) +
    theme(legend.title = element_blank(),
          panel.grid = element_blank(),
          axis.text = element_blank(),
          axis.title = element_blank())
  )) %&gt;%
  pull(pie_chart)</code></pre>
<pre><code>## Warning in mutate_impl(.data, dots): Unequal factor levels: coercing to
## character</code></pre>
<pre><code>## Warning in mutate_impl(.data, dots): binding character and factor vector,
## coercing into character vector

## Warning in mutate_impl(.data, dots): binding character and factor vector,
## coercing into character vector</code></pre>
<pre><code>## [[1]]</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis_part2-17-1.png" width="672">
</p>
<pre><code>## 
## [[2]]</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis_part2-17-2.png" width="672">
</p>
<p>
That was it for this one, the graphs are not that super sexy, but the amount of work that went into making them was quite consequent. The main reason was that parsing xlogfiles was a bit tricky, but the main challenge was extracting information from dumplog files. This proved to be a bit more complicated than expected (just take a look at the source code of <code>extract_defeated_monsters()</code> to get an idea‚Ä¶).
</p>
</section>
<section id="bonus-plot" class="level2">
<h2 class="anchored" data-anchor-id="bonus-plot">
Bonus plot
</h2>
<section id="correct-number-of-daily-games" class="level3">
<h3 class="anchored" data-anchor-id="correct-number-of-daily-games">
Correct number of daily games
</h3>
<p>
The daily number of games are available <a href="https://alt.org/nethack/dailygames_ct.html">here</a>. Let‚Äôs extract this info and remake the plot that shows the number of runs per day:
</p>
<pre class="r"><code>games &lt;- read_html("https://alt.org/nethack/dailygames_ct.html") %&gt;%
        html_nodes(xpath = '//table') %&gt;%
        html_table(fill = TRUE) </code></pre>
<p>
This extracts all the tables and puts them into a list. Let‚Äôs take a look at one:
</p>
<pre class="r"><code>head(games[[1]])</code></pre>
<pre><code>##   2018  2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018
## 1         NA    1    2    3    4    5    6    7    8    9   10   11   12
## 2  Jan 11639  275  370  394  363  392  276  288  324  297  411  413  430
## 3  Feb 10819  375  384  359  376  440  345  498  457  416  376  421  416
## 4  Mar 12148  411  403  421  392  447  391  451  298  350  309  309  369
## 5  Apr 13957  456  513  482  516  475  490  397  431  436  438  541  493
## 6  May 13361  595  509  576  620  420  443  407  539  440  446  404  282
##   2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018
## 1   13   14   15   16   17   18   19   20   21   22   23   24   25   26
## 2  331  341  318  483  408  424  464  412  371  430  348  315  359  375
## 3  385  367  443  324  283  341  385  398  361  379  399  276  455  460
## 4  390  358  362  345  388  360  411  382  371  400  410  417  328  431
## 5  593  537  396  578  403  435  526  448  339  377  476  492  528  393
## 6  265  358  419  564  483  429  423  299  424  404  450  408  355  409
##   2018 2018 2018 2018 2018
## 1   27   28   29   30   31
## 2  432  371  385  440  399
## 3  353  347   NA   NA   NA
## 4  386  484  493  486  395
## 5  407  421  463  477   NA
## 6  417  433  360  391  389</code></pre>
<p>
Let‚Äôs clean this up.
</p>
<pre class="r"><code>clean_table &lt;- function(df){
  # Promotes first row to header
  colnames(df) &lt;- df[1, ]
  df &lt;- df[-1, ]
  
  # Remove column with total from the month
  df &lt;- df[, -2]
  
  # Name the first column "month"
  
  colnames(df)[1] &lt;- "month"
  
  # Now put it in a tidy format
  df %&gt;%
    gather(day, games_played, -month)
}</code></pre>
<p>
Now I can clean up all the tables. I apply this function to each element of the list <code>games</code>. I also add a year column:
</p>
<pre class="r"><code>games &lt;- map(games, clean_table) %&gt;%
  map2_dfr(.x = ., 
       .y = seq(2018, 2001),
       ~mutate(.x, year = .y))</code></pre>
<p>
Now I can easily create the plot I wanted
</p>
<pre class="r"><code>games %&lt;&gt;%
  mutate(date = lubridate::ymd(paste(year, month, day, sep = "-")))</code></pre>
<pre><code>## Warning: 122 failed to parse.</code></pre>
<pre class="r"><code>ggplot(games, aes(y = games_played, x = date)) + 
  geom_point(colour = "#0f4150") + 
  geom_smooth(colour = "#82518c") + 
  theme_blog() + 
  ylab("Total games played")</code></pre>
<pre><code>## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'</code></pre>
<pre><code>## Warning: Removed 452 rows containing non-finite values (stat_smooth).</code></pre>
<pre><code>## Warning: Removed 452 rows containing missing values (geom_point).</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis_part2-22-1.png" width="672">
</p>
<p>
There‚Äôs actually a lot more games than 50 per day being played!
</p>
</section>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">
Appendix
</h2>
<section id="fuzzy-matching" class="level3">
<h3 class="anchored" data-anchor-id="fuzzy-matching">
Fuzzy matching
</h3>
<p>
If you take a look at the <code>extract_defeated_monsters()</code> source code, you‚Äôll see that at some point I ‚Äúsingularize‚Äù monster names. I decided to deal with this singular/plural issue, ‚Äúby hand‚Äù, but also explored other possibilities, such as matching the plural nouns with the singular nouns fuzzily. In the end it didn‚Äôt work out so well, but here‚Äôs the code for future reference.
</p>
<pre class="r"><code>monster_list &lt;- read_html("https://nethackwiki.com/wiki/Monsters_(by_difficulty)") %&gt;%
    html_nodes(".prettytable") %&gt;% 
    .[[1]] %&gt;%
    html_table(fill = TRUE)

monster_list %&lt;&gt;%
    select(monster = Name)

head(monster_list)</code></pre>
<pre><code>##      monster
## 1 Demogorgon
## 2   Asmodeus
## 3  Baalzebub
## 4   Dispater
## 5     Geryon
## 6      Orcus</code></pre>
<pre class="r"><code>library(fuzzyjoin)

test_vanquished &lt;- extract_defeated_monsters(nethack$dumplog[[117]])

head(test_vanquished)</code></pre>
<pre><code>## # A tibble: 6 x 3
##   value monster              status
##   &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt; 
## 1     1 baalzebub            &lt;NA&gt;  
## 2     1 orcu                 &lt;NA&gt;  
## 3     1 juiblex              &lt;NA&gt;  
## 4     4 the wizard of yendor &lt;NA&gt;  
## 5     3 pestilence           &lt;NA&gt;  
## 6     1 famine               &lt;NA&gt;</code></pre>
<p>
You can take a look at the result by expanding:
</p>
<details>
<p>
</p><summary>
Click to expand
</summary>
<p></p>
<pre class="r"><code>stringdist_left_join(test_vanquished, monster_list) %&gt;% 
  count(monster.y) %&gt;%
  print(n = Inf)</code></pre>
<pre><code>## Joining by: "monster"</code></pre>
<pre><code>## # A tibble: 297 x 2
##     monster.y                   n
##     &lt;chr&gt;                   &lt;int&gt;
##   1 acid blob                   1
##   2 air elemental               2
##   3 Aleax                       1
##   4 aligned priest              1
##   5 Angel                       1
##   6 ape                         2
##   7 arch-lich                   1
##   8 Baalzebub                   1
##   9 baby black dragon           1
##  10 baby crocodile              1
##  11 baby gray dragon            1
##  12 baby green dragon           1
##  13 baby long worm              1
##  14 baby orange dragon          1
##  15 baby white dragon           1
##  16 baby yellow dragon          1
##  17 balrog                      1
##  18 baluchitherium              1
##  19 barbed devil                1
##  20 barrow wight                1
##  21 bat                         1
##  22 black dragon                1
##  23 black light                 1
##  24 black naga                  1
##  25 black pudding               1
##  26 black unicorn               1
##  27 blue dragon                 1
##  28 blue jelly                  1
##  29 bone devil                  1
##  30 brown mold                  1
##  31 brown pudding               1
##  32 bugbear                     1
##  33 captain                     1
##  34 carnivorous ape             1
##  35 cave spider                 1
##  36 centipede                   1
##  37 chameleon                   1
##  38 chickatrice                 2
##  39 clay golem                  1
##  40 cobra                       1
##  41 cockatrice                  2
##  42 couatl                      1
##  43 coyote                      1
##  44 crocodile                   1
##  45 demilich                    1
##  46 dingo                       1
##  47 disenchanter                1
##  48 dog                         1
##  49 doppelganger                1
##  50 dust vortex                 1
##  51 dwarf                       2
##  52 dwarf king                  1
##  53 dwarf lord                  1
##  54 dwarf mummy                 1
##  55 dwarf zombie                1
##  56 earth elemental             1
##  57 electric eel                1
##  58 elf                         1
##  59 elf mummy                   1
##  60 elf zombie                  1
##  61 elf-lord                    1
##  62 Elvenking                   1
##  63 energy vortex               1
##  64 erinys                      1
##  65 ettin                       1
##  66 ettin mummy                 1
##  67 ettin zombie                1
##  68 Famine                      1
##  69 fire ant                    2
##  70 fire elemental              2
##  71 fire giant                  2
##  72 fire vortex                 2
##  73 flaming sphere              1
##  74 flesh golem                 1
##  75 floating eye                1
##  76 fog cloud                   1
##  77 forest centaur              1
##  78 fox                         1
##  79 freezing sphere             1
##  80 frost giant                 1
##  81 gargoyle                    1
##  82 garter snake                1
##  83 gas spore                   1
##  84 gecko                       1
##  85 gelatinous cube             1
##  86 ghost                       2
##  87 ghoul                       2
##  88 giant ant                   3
##  89 giant bat                   3
##  90 giant beetle                1
##  91 giant eel                   1
##  92 giant mimic                 1
##  93 giant mummy                 1
##  94 giant rat                   3
##  95 giant spider                1
##  96 giant zombie                1
##  97 glass piercer               1
##  98 gnome                       1
##  99 gnome king                  1
## 100 gnome lord                  1
## 101 gnome mummy                 1
## 102 gnome zombie                1
## 103 gnomish wizard              1
## 104 goblin                      1
## 105 gold golem                  2
## 106 golden naga                 1
## 107 golden naga hatchling       1
## 108 gray ooze                   1
## 109 gray unicorn                1
## 110 Green-elf                   1
## 111 gremlin                     1
## 112 Grey-elf                    1
## 113 grid bug                    1
## 114 guardian naga               1
## 115 guardian naga hatchling     1
## 116 hell hound                  1
## 117 hell hound pup              1
## 118 hezrou                      1
## 119 high priest                 1
## 120 hill giant                  1
## 121 hill orc                    1
## 122 hobbit                      1
## 123 hobgoblin                   1
## 124 homunculus                  1
## 125 horned devil                1
## 126 horse                       2
## 127 housecat                    1
## 128 human                       1
## 129 human mummy                 1
## 130 human zombie                1
## 131 ice devil                   1
## 132 ice troll                   1
## 133 ice vortex                  2
## 134 iguana                      1
## 135 imp                         1
## 136 incubus                     1
## 137 iron golem                  1
## 138 iron piercer                1
## 139 jabberwock                  1
## 140 jackal                      1
## 141 jaguar                      1
## 142 jellyfish                   1
## 143 Juiblex                     1
## 144 Keystone Kop                1
## 145 ki-rin                      1
## 146 killer bee                  1
## 147 kitten                      1
## 148 kobold                      1
## 149 kobold lord                 1
## 150 kobold mummy                1
## 151 kobold shaman               1
## 152 kobold zombie               1
## 153 Kop Lieutenant              1
## 154 Kop Sergeant                1
## 155 kraken                      2
## 156 large cat                   1
## 157 large dog                   1
## 158 large kobold                1
## 159 large mimic                 1
## 160 leather golem               1
## 161 leocrotta                   1
## 162 leprechaun                  1
## 163 lich                        2
## 164 lichen                      2
## 165 lieutenant                  1
## 166 little dog                  1
## 167 lizard                      1
## 168 long worm                   1
## 169 Lord Surtur                 1
## 170 lurker above                1
## 171 lynx                        1
## 172 manes                       1
## 173 marilith                    1
## 174 master lich                 1
## 175 master mind flayer          1
## 176 Medusa                      1
## 177 mind flayer                 1
## 178 minotaur                    1
## 179 monk                        2
## 180 monkey                      1
## 181 Mordor orc                  1
## 182 mountain centaur            1
## 183 mountain nymph              1
## 184 mumak                       1
## 185 nalfeshnee                  1
## 186 Nazgul                      1
## 187 newt                        1
## 188 Norn                        1
## 189 nurse                       2
## 190 ochre jelly                 1
## 191 ogre                        1
## 192 ogre king                   1
## 193 ogre lord                   1
## 194 Olog-hai                    1
## 195 orange dragon               1
## 196 orc                         3
## 197 orc mummy                   1
## 198 orc shaman                  1
## 199 orc zombie                  1
## 200 orc-captain                 1
## 201 Orcus                       1
## 202 owlbear                     1
## 203 page                        2
## 204 panther                     1
## 205 paper golem                 1
## 206 Pestilence                  1
## 207 piranha                     1
## 208 pit fiend                   1
## 209 pit viper                   1
## 210 plains centaur              1
## 211 pony                        1
## 212 purple worm                 1
## 213 pyrolisk                    1
## 214 python                      1
## 215 quantum mechanic            1
## 216 quasit                      1
## 217 queen bee                   1
## 218 quivering blob              1
## 219 rabid rat                   1
## 220 ranger                      1
## 221 raven                       2
## 222 red dragon                  1
## 223 red mold                    1
## 224 red naga                    1
## 225 rock mole                   1
## 226 rock piercer                1
## 227 rock troll                  1
## 228 rogue                       2
## 229 rope golem                  1
## 230 roshi                       1
## 231 rothe                       1
## 232 rust monster                1
## 233 salamander                  1
## 234 sandestin                   1
## 235 sasquatch                   1
## 236 scorpion                    1
## 237 sergeant                    1
## 238 sewer rat                   1
## 239 shade                       3
## 240 shark                       2
## 241 shocking sphere             1
## 242 shrieker                    1
## 243 silver dragon               1
## 244 skeleton                    1
## 245 small mimic                 1
## 246 snake                       2
## 247 soldier                     1
## 248 soldier ant                 1
## 249 spotted jelly               1
## 250 stalker                     1
## 251 steam vortex                1
## 252 stone giant                 2
## 253 stone golem                 1
## 254 storm giant                 2
## 255 straw golem                 1
## 256 succubus                    1
## 257 tengu                       1
## 258 tiger                       1
## 259 titanothere                 1
## 260 trapper                     1
## 261 troll                       1
## 262 umber hulk                  1
## 263 Uruk-hai                    1
## 264 vampire                     1
## 265 vampire bat                 1
## 266 vampire lord                1
## 267 violet fungus               1
## 268 Vlad the Impaler            1
## 269 vrock                       1
## 270 warg                        2
## 271 warhorse                    1
## 272 water elemental             1
## 273 water moccasin              1
## 274 water nymph                 1
## 275 werejackal                  2
## 276 wererat                     2
## 277 werewolf                    2
## 278 white dragon                1
## 279 white unicorn               1
## 280 winged gargoyle             1
## 281 winter wolf                 1
## 282 winter wolf cub             1
## 283 wizard                      1
## 284 wolf                        1
## 285 wood golem                  2
## 286 wood nymph                  1
## 287 Woodland-elf                1
## 288 wraith                      1
## 289 wumpus                      1
## 290 xan                         3
## 291 xorn                        2
## 292 yellow dragon               1
## 293 yellow light                1
## 294 yellow mold                 1
## 295 yeti                        1
## 296 zruty                       1
## 297 &lt;NA&gt;                        1</code></pre>
</details>
<p>
As you can see, some matches fail, especially for words that end in ‚Äúy‚Äù in the singular, so ‚Äúies‚Äù in plural, or ‚Äúfire vortices‚Äù that does not get matched to ‚Äúfire vortex‚Äù. I tried all the methods but it‚Äôs either worse, or marginally better.
</p>
</section>
<section id="extracting-info-from-dumplogfiles" class="level3">
<h3 class="anchored" data-anchor-id="extracting-info-from-dumplogfiles">
Extracting info from dumplogfiles
</h3>
<details>
<p>
</p><summary>
Click here to take a look at the source code from extract_defeated_monsters
</summary>
<p></p>
<pre class="r"><code>#' Extract information about defeated monsters from an xlogfile
#' @param xlog A raw xlogfile
#' @return A data frame with information on vanquished, genocided and extincted monsters
#' @importFrom dplyr mutate select filter bind_rows full_join
#' @importFrom tidyr separate
#' @importFrom tibble as_tibble tibble
#' @importFrom magrittr "%&gt;%"
#' @importFrom purrr map2 possibly is_empty modify_if simplify discard
#' @importFrom readr read_lines
#' @importFrom stringr str_which str_replace_all str_replace str_trim str_detect str_to_lower str_extract_all str_extract
#' @export
#' @examples
#' \dontrun{
#' get_dumplog(xlog)
#' }
extract_defeated_monsters &lt;- function(dumplog){

    if(any(str_detect(dumplog, "No creatures were vanquished."))){
        return(NA)
    } else {

        start &lt;- dumplog %&gt;% # &lt;- dectect the start of the list
            str_which("Vanquished creatures")

        end &lt;- dumplog %&gt;% # &lt;- detect the end of the list
            str_which("\\d+ creatures vanquished.")

        if(is_empty(end)){ # This deals with the situation of only one vanquished creature
            end &lt;- start + 2
        }

        list_creatures &lt;- dumplog[(start + 1):(end - 1)] %&gt;% # &lt;- extract the list
            str_replace_all("\\s+an? ", "1 ") %&gt;% # &lt;- replace a or an by 1
            str_trim() # &lt;- trim white space

        # The following function first extracts the digit in the string (123 times)
        # and replaces the 1 with this digit
        # This means that: "1 the Wizard of Yendor (4 times)" becomes "4 the Wizard of Yendor (4 times)"
        str_extract_replace &lt;- function(string){
            times &lt;- str_extract(string, "\\d+(?=\\stimes)")
            str_replace(string, "1", times)
        }

        result &lt;- list_creatures %&gt;%
            # If a string starts with a letter, add a 1
            # This means that: "Baalzebub" becomes "1 Baalzebub"
            modify_if(str_detect(., "^[:alpha:]"), ~paste("1", .)) %&gt;%
            # If the string "(twice)" is detected, replace "1" (that was added the line before) with "2"
            modify_if(str_detect(., "(twice)"), ~str_replace(., "1", "2")) %&gt;%
            # Same for "(thrice)"
            modify_if(str_detect(., "(thrice)"), ~str_replace(., "1", "3")) %&gt;%
            # Exctract the digit in "digit times" and replace the "1" with digit
            modify_if(str_detect(., "(\\d+ times)"), str_extract_replace) %&gt;%
            # Replace "(times)" or "(twice)" etc with ""
            str_replace_all("\\(.*\\)", "") %&gt;%
            str_trim() %&gt;%
            simplify() %&gt;%
            # Convert the resulting list to a tibble. This tibble has one column:
            # value
            # 1 Baalzebub
            # 2 dogs
            #...
            as_tibble() %&gt;%
            # Use tidyr::separate to separate the "value" column into two columns. The extra pieces get merged
            # So for example "1 Vlad the Impaler" becomes "1" "Vlad the Impaler" instead of "1" "Vlad" which
            # would be the case without "extra = "merge""
            separate(value, into = c("value", "monster"), extra = "merge") %&gt;%
            mutate(value = as.numeric(value)) %&gt;%
            mutate(monster = str_to_lower(monster))

        # This function singularizes names:
        singularize_monsters &lt;- function(nethack_data){
            nethack_data %&gt;%
                mutate(monster = str_replace_all(monster, "mummies", "mummy"),
                       monster = str_replace_all(monster, "jellies", "jelly"),
                       monster = str_replace_all(monster, "vortices", "vortex"),
                       monster = str_replace_all(monster, "elves", "elf"),
                       monster = str_replace_all(monster, "wolves", "wolf"),
                       monster = str_replace_all(monster, "dwarves", "dwarf"),
                       monster = str_replace_all(monster, "liches", "lich"),
                       monster = str_replace_all(monster, "baluchiteria", "baluchiterium"),
                       monster = str_replace_all(monster, "homunculi", "homonculus"),
                       monster = str_replace_all(monster, "mumakil", "mumak"),
                       monster = str_replace_all(monster, "sasquatches", "sasquatch"),
                       monster = str_replace_all(monster, "watchmen", "watchman"),
                       monster = str_replace_all(monster, "zruties", "zruty"),
                       monster = str_replace_all(monster, "xes$", "x"),
                       monster = str_replace_all(monster, "s$", ""))
        }

        result &lt;- singularize_monsters(result)
    }
    # If a player did not genocide or extinct any species, return the result:
    if(any(str_detect(dumplog, "No species were genocided or became extinct."))){
        result &lt;- result %&gt;%
            mutate(status = NA_character_)
        return(result)
    } else {

        # If the player genocided or extincted species, add this info:
        start &lt;- dumplog %&gt;% # &lt;- dectect the start of the list
            str_which("Genocided or extinct species:") # &lt;- sometimes this does not appear in the xlogfile

        end &lt;- dumplog %&gt;% # &lt;- detect the end of the list
            str_which("Voluntary challenges")

       if(is_empty(start)){# This deals with the situation start does not exist
           start &lt;- end - 2
       }

        list_creatures &lt;- dumplog[(start + 1):(end - 1)] %&gt;% # &lt;- extract the list
            str_trim() # &lt;- trim white space

        extinct_species &lt;- list_creatures %&gt;%
            str_extract_all("[:alpha:]+\\s(?=\\(extinct\\))", simplify = T) %&gt;%
            str_trim %&gt;%
            discard(`==`(., ""))

        extinct_species_df &lt;- tibble(monster = extinct_species, status = "extinct")

        genocided_species_index &lt;- list_creatures %&gt;%
            str_detect(pattern = "extinct|species") %&gt;%
            `!`

        genocided_species &lt;- list_creatures[genocided_species_index]

        genocided_species_df &lt;- tibble(monster = genocided_species, status = "genocided")

        genocided_or_extinct_df &lt;- singularize_monsters(bind_rows(extinct_species_df, genocided_species_df))

        result &lt;- full_join(result, genocided_or_extinct_df, by = "monster") %&gt;%
            filter(monster != "") # &lt;- this is to remove lines that were added by mistake, for example if start was empty

        return(result)
    }
}</code></pre>
</details>
</section>
</section>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-11-10-nethack_analysis_part2.html</guid>
  <pubDate>Sat, 10 Nov 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Analyzing NetHack data, part 1: What kills the players</title>
  <link>https://b-rodrigues.github.io/posts/2018-11-03-nethack_analysis.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=dpM2o4dRLto"> <img src="https://b-rodrigues.github.io/assets/img/deepfried_loss.png" title="Click here to listen to epic music while reading"></a>
</p>
</div>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">
Abstract
</h2>
<p>
In this post, I will analyse the data I scraped and put into an R package, which I called <code>{nethack}</code>. NetHack is a roguelike game; for more context, read my previous blog <a href="https://www.brodrigues.co/blog/2018-11-01-nethack/">post</a>. You can install the <code>{nethack}</code> package and play around with the data yourself by installing it from github:
</p>
<pre class="r"><code>devtools::install_github("b-rodrigues/nethack")</code></pre>
<p>
And to use it:
</p>
<pre class="r"><code>library(nethack)
data("nethack")</code></pre>
<p>
The data contains information on games played from 2001 to 2018; 322485 rows and 14 columns. I will analyze the data in a future blog post. This post focuses on getting and then sharing the data. By the way, all the content from the public server I scrape is under the CC&nbsp;BY 4.0 license.
</p>
<p>
I built the package by using the very useful <code>{devtools}</code> package.
</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">
Introduction
</h2>
<p>
What I want from this first analysis are several, simple things: how many players manage to ascend (meaning, winning), what monster kills most players, and finally extract data from the <code>dumplog</code> column. The <code>dumplog</code> column is a bit special; each element of the dumplog column is a log file that contains a lot of information from the last turns of a player. I will leave this for a future blog post, though.
</p>
<p>
Let‚Äôs load some packages first:
</p>
<pre class="r"><code>library(nethack)
library(tidyverse)
library(lubridate)
library(magrittr)
library(brotools)</code></pre>
<p>
<code>{brotools}</code> is my own package that contains some functions that I use daily. If you want to install it, run the following line:
</p>
<pre class="r"><code>devtools::install_github("b-rodrigues/brotools")</code></pre>
<p>
The documentation is not up-to-date, I think I‚Äôll do that and release it on CRAN. Some day.
</p>
<p>
Now, let‚Äôs load the ‚Äúnethack‚Äù data, included in the <code>{nethack}</code> package:
</p>
<pre><code>##   rank score     name time turns lev_max hp_max role race gender alignment
## 1    1   360      jkm &lt;NA&gt;    NA     2/2  -2/25  Sam  Hum    Mal       Law
## 2    2   172 yosemite &lt;NA&gt;    NA     1/1  -1/10  Tou  Hum    Fem       Neu
## 3    3  2092    dtype &lt;NA&gt;    NA     6/7  -2/47  Val  Hum    Fem       Neu
## 4    4    32   joorko &lt;NA&gt;    NA     1/1   0/15  Sam  Hum    Mal       Law
## 5    5   118    jorko &lt;NA&gt;    NA     1/1   0/11  Rog  Orc    Fem       Cha
## 6    6  1757   aaronl &lt;NA&gt;    NA     5/5   0/60  Bar  Hum    Mal       Neu
##                                                      death       date
## 1                                   killed by a brown mold 2001-10-24
## 2                                       killed by a jackal 2001-10-24
## 3                                     killed by a fire ant 2001-10-24
## 4                                       killed by a jackal 2001-10-24
## 5                                       killed by a jackal 2001-10-24
## 6 killed by a hallucinogen-distorted ghoul, while helpless 2001-10-24
##   dumplog
## 1      NA
## 2      NA
## 3      NA
## 4      NA
## 5      NA
## 6      NA</code></pre>
<pre class="r"><code>data("nethack")

head(nethack)</code></pre>
<p>
Let‚Äôs create some variables that might be helpful (or perhaps not, we‚Äôll see):
</p>
<pre class="r"><code>nethack %&lt;&gt;% 
  mutate(date = ymd(date),
         year = year(date),
         month = month(date),
         day = day(date))</code></pre>
<p>
This makes it easy to look at the data from, say, June 2017:
</p>
<pre class="r"><code>nethack %&gt;%
  filter(year == 2017, month == 6) %&gt;%
  brotools::describe()</code></pre>
<pre><code>## # A tibble: 17 x 17
##    variable type   nobs    mean      sd mode    min     max   q05   q25
##    &lt;chr&gt;    &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 day      Nume‚Ä¶  1451    17.4  9.00e0 1         1      30     2   10 
##  2 month    Nume‚Ä¶  1451     6    0.     6         6       6     6    6 
##  3 rank     Nume‚Ä¶  1451    47.1  2.95e1 1         1     100     4   20 
##  4 score    Nume‚Ä¶  1451 38156.   3.39e5 488       0 5966425    94  402.
##  5 turns    Nume‚Ä¶  1451  4179.   1.23e4 812       1  291829   204  860.
##  6 year     Nume‚Ä¶  1451  2017    0.     2017   2017    2017  2017 2017 
##  7 alignme‚Ä¶ Char‚Ä¶  1451    NA   NA      Law      NA      NA    NA   NA 
##  8 death    Char‚Ä¶  1451    NA   NA      kill‚Ä¶    NA      NA    NA   NA 
##  9 gender   Char‚Ä¶  1451    NA   NA      Mal      NA      NA    NA   NA 
## 10 hp_max   Char‚Ä¶  1451    NA   NA      -1/16    NA      NA    NA   NA 
## 11 lev_max  Char‚Ä¶  1451    NA   NA      4/4      NA      NA    NA   NA 
## 12 name     Char‚Ä¶  1451    NA   NA      ohno‚Ä¶    NA      NA    NA   NA 
## 13 race     Char‚Ä¶  1451    NA   NA      Hum      NA      NA    NA   NA 
## 14 role     Char‚Ä¶  1451    NA   NA      Kni      NA      NA    NA   NA 
## 15 time     Char‚Ä¶  1451    NA   NA      01:1‚Ä¶    NA      NA    NA   NA 
## 16 dumplog  List   1451    NA   NA      &lt;NA&gt;     NA      NA    NA   NA 
## 17 date     Date   1451    NA   NA      &lt;NA&gt;     NA      NA    NA   NA 
## # ‚Ä¶ with 7 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, q95 &lt;dbl&gt;,
## #   n_missing &lt;int&gt;, n_unique &lt;int&gt;, starting_date &lt;date&gt;,
## #   ending_date &lt;date&gt;</code></pre>
<p>
Let‚Äôs also take a look at a dumplog:
</p>
<details>
<p>
</p><summary>
Click to expand; the dumplog is quite long
</summary>
<p></p>
<pre class="r"><code>nethack %&gt;%
    filter(year == 2018, month == 10) %&gt;%
    slice(1) %&gt;%
    pull(dumplog)</code></pre>
<pre><code>## [[1]]
##   [1] "Unix NetHack Version 3.6.1 - last build Fri Apr 27 19:25:48 2018. (d4ebae12f1a709d1833cf466dd0c553fb97518d2)"
##   [2] ""                                                                                                            
##   [3] "Game began 2018-09-30 22:27:18, ended 2018-10-01 00:01:12."                                                  
##   [4] ""                                                                                                            
##   [5] "brothertrebius, neutral female gnomish Ranger"                                                               
##   [6] ""                                                                                                            
##   [7] "                  -----"                                                                                     
##   [8] "   --------       |....#     -----      --------"                                                            
##   [9] "   |/..%.=|      #...^|######|...|    ##.......|"                                                            
##  [10] "   |/[%..%|      #|...|     #|...|    # |......|"                                                            
##  [11] "   |......|      #-----     #-...-######....&lt;..|"                                                            
##  [12] "   -----|--      ###         -|-.-  #   |......|"                                                            
##  [13] "        ##         ##           #   #   ----f---"                                                            
##  [14] "         ####       #           #  ##       f@Y"                                                             
##  [15] "            #       #           #  #"                                                                        
##  [16] "       -----.-------#           #  #"                                                                        
##  [17] "       |........%..|#           #  #"                                                                        
##  [18] "       |............#           #  #"                                                                        
##  [19] "       |...........|          0##  #"                                                                        
##  [20] "       |...........|         -.--- #"                                                                        
##  [21] "       -------------         |^..|##"                                                                        
##  [22] "                             |...|#"                                                                         
##  [23] "                             |0&gt;..#"                                                                         
##  [24] "                             -----"                                                                          
##  [25] ""                                                                                                            
##  [26] "Brothertre the Trailblazer   St:15 Dx:12 Co:16 In:13 Wi:15 Ch:6  Neutral"                                    
##  [27] "Dlvl:6  $:59 HP:0(54) Pw:40(40) AC:0  Exp:8 T:7398  Satiated Burdened"                                       
##  [28] ""                                                                                                            
##  [29] "Latest messages:"                                                                                            
##  [30] " In what direction? l"                                                                                       
##  [31] " You shoot 2 arrows."                                                                                        
##  [32] " The 1st arrow hits the ape."                                                                                
##  [33] " The 2nd arrow hits the ape!"                                                                                
##  [34] " The ape hits!"                                                                                              
##  [35] " The ape hits!"                                                                                              
##  [36] " The ape bites!"                                                                                             
##  [37] " You ready: q - 9 uncursed arrows."                                                                          
##  [38] " In what direction? l"                                                                                       
##  [39] " The arrow hits the ape."                                                                                    
##  [40] " The ape hits!"                                                                                              
##  [41] " The ape hits!"                                                                                              
##  [42] " The ape bites!"                                                                                             
##  [43] " The ape hits!"                                                                                              
##  [44] " The ape hits!"                                                                                              
##  [45] " The ape misses!"                                                                                            
##  [46] " In what direction? l"                                                                                       
##  [47] " You shoot 2 arrows."                                                                                        
##  [48] " The 1st arrow hits the ape!"                                                                                
##  [49] " The 2nd arrow hits the ape."                                                                                
##  [50] " The ape misses!"                                                                                            
##  [51] " The ape hits!"                                                                                              
##  [52] " The ape misses!"                                                                                            
##  [53] " In what direction? l"                                                                                       
##  [54] " You shoot 2 arrows."                                                                                        
##  [55] " The 1st arrow misses the ape."                                                                              
##  [56] " The 2nd arrow hits the ape."                                                                                
##  [57] " The ape misses!"                                                                                            
##  [58] " The ape hits!"                                                                                              
##  [59] " The ape bites!"                                                                                             
##  [60] " In what direction? l"                                                                                       
##  [61] " The arrow hits the ape!"                                                                                    
##  [62] " The ape hits!"                                                                                              
##  [63] " The ape misses!"                                                                                            
##  [64] " The ape bites!"                                                                                             
##  [65] " You hear someone cursing shoplifters."                                                                      
##  [66] " The ape misses!"                                                                                            
##  [67] " The ape hits!"                                                                                              
##  [68] " The ape bites!"                                                                                             
##  [69] " What do you want to write with? [- amnqsvBJM-OWZ or ?*] -"                                                  
##  [70] " You write in the dust with your fingertip."                                                                 
##  [71] " What do you want to write in the dust here? Elbereth"                                                       
##  [72] " The ape hits!"                                                                                              
##  [73] " The ape hits!"                                                                                              
##  [74] " You die..."                                                                                                 
##  [75] " Do you want your possessions identified? [ynq] (y) y"                                                       
##  [76] " Do you want to see your attributes? [ynq] (y) n"                                                            
##  [77] " Do you want an account of creatures vanquished? [ynaq] (y) n"                                               
##  [78] " Do you want to see your conduct? [ynq] (y) n"                                                               
##  [79] " Do you want to see the dungeon overview? [ynq] (y) q"                                                       
##  [80] ""                                                                                                            
##  [81] "Inventory:"                                                                                                  
##  [82] " Coins"                                                                                                      
##  [83] "  $ - 59 gold pieces"                                                                                        
##  [84] " Weapons"                                                                                                    
##  [85] "  m - 17 blessed +1 arrows"                                                                                  
##  [86] "  n - a blessed +0 arrow"                                                                                    
##  [87] "  q - 3 +0 arrows (in quiver)"                                                                               
##  [88] "  s - a +0 bow (weapon in hand)"                                                                             
##  [89] "  B - 11 +1 darts"                                                                                           
##  [90] "  N - 11 +0 darts"                                                                                           
##  [91] "  a - a +1 dagger (alternate weapon; not wielded)"                                                           
##  [92] " Armor"                                                                                                      
##  [93] "  T - an uncursed +0 dwarvish iron helm (being worn)"                                                        
##  [94] "  z - an uncursed +0 pair of leather gloves (being worn)"                                                    
##  [95] "  U - a cursed -4 pair of iron shoes (being worn)"                                                           
##  [96] "  e - an uncursed +2 cloak of displacement (being worn)"                                                     
##  [97] "  h - a blessed +0 dwarvish mithril-coat (being worn)"                                                       
##  [98] " Comestibles"                                                                                                
##  [99] "  f - 3 uncursed cram rations"                                                                               
## [100] "  j - 2 uncursed food rations"                                                                               
## [101] "  L - an uncursed food ration"                                                                               
## [102] "  P - an uncursed lembas wafer"                                                                              
## [103] "  I - an uncursed lizard corpse"                                                                             
## [104] "  o - an uncursed tin of spinach"                                                                            
## [105] " Scrolls"                                                                                                    
## [106] "  G - 2 uncursed scrolls of blank paper"                                                                     
## [107] "  t - an uncursed scroll of confuse monster"                                                                 
## [108] "  V - an uncursed scroll of identify"                                                                        
## [109] " Potions"                                                                                                    
## [110] "  x - an uncursed potion of gain ability"                                                                    
## [111] "  H - a blessed potion of sleeping"                                                                          
## [112] "  g - 3 uncursed potions of water"                                                                           
## [113] " Rings"                                                                                                      
## [114] "  O - an uncursed ring of slow digestion (on left hand)"                                                     
## [115] "  v - an uncursed ring of stealth (on right hand)"                                                           
## [116] " Tools"                                                                                                      
## [117] "  p - an uncursed magic lamp"                                                                                
## [118] "  k - an uncursed magic whistle"                                                                             
## [119] "  Q - an uncursed mirror"                                                                                    
## [120] "  C - an uncursed saddle"                                                                                    
## [121] "  D - an uncursed stethoscope"                                                                               
## [122] "  y - a +0 unicorn horn"                                                                                     
## [123] "  i - 7 uncursed wax candles"                                                                                
## [124] " Gems/Stones"                                                                                                
## [125] "  W - an uncursed flint stone"                                                                               
## [126] "  M - an uncursed worthless piece of red glass"                                                              
## [127] "  Z - an uncursed worthless piece of violet glass"                                                           
## [128] "  J - an uncursed worthless piece of white glass"                                                            
## [129] ""                                                                                                            
## [130] "Brothertrebius the Ranger's attributes:"                                                                     
## [131] ""                                                                                                            
## [132] "Background:"                                                                                                 
## [133] " You were a Trailblazer, a level 8 female gnomish Ranger."                                                   
## [134] " You were neutral, on a mission for Venus"                                                                   
## [135] " who was opposed by Mercury (lawful) and Mars (chaotic)."                                                    
## [136] ""                                                                                                            
## [137] "Final Characteristics:"                                                                                      
## [138] " You had 0 hit points (max:54)."                                                                             
## [139] " You had 40 magic power (max:40)."                                                                           
## [140] " Your armor class was 0."                                                                                    
## [141] " You had 1552 experience points."                                                                            
## [142] " You entered the dungeon 7398 turns ago."                                                                    
## [143] " Your strength was 15 (limit:18/50)."                                                                        
## [144] " Your dexterity was 12 (limit:18)."                                                                          
## [145] " Your constitution was 16 (limit:18)."                                                                       
## [146] " Your intelligence was 13 (limit:19)."                                                                       
## [147] " Your wisdom was 15 (limit:18)."                                                                             
## [148] " Your charisma was 6 (limit:18)."                                                                            
## [149] ""                                                                                                            
## [150] "Final Status:"                                                                                               
## [151] " You were satiated."                                                                                         
## [152] " You were burdened; movement was slightly slowed."                                                           
## [153] " You were wielding a bow."                                                                                   
## [154] ""                                                                                                            
## [155] "Final Attributes:"                                                                                           
## [156] " You were piously aligned."                                                                                  
## [157] " You were telepathic."                                                                                       
## [158] " You had automatic searching."                                                                               
## [159] " You had infravision."                                                                                       
## [160] " You were displaced."                                                                                        
## [161] " You were stealthy."                                                                                         
## [162] " You had slower digestion."                                                                                  
## [163] " You were guarded."                                                                                          
## [164] " You are dead."                                                                                              
## [165] ""                                                                                                            
## [166] "Vanquished creatures:"                                                                                       
## [167] "  a warhorse"                                                                                                
## [168] "  a tengu"                                                                                                   
## [169] "  a quivering blob"                                                                                          
## [170] " an iron piercer"                                                                                            
## [171] "  2 black lights"                                                                                            
## [172] "  a gold golem"                                                                                              
## [173] "  a werewolf"                                                                                                
## [174] "  3 lizards"                                                                                                 
## [175] "  2 dingoes"                                                                                                 
## [176] "  a housecat"                                                                                                
## [177] "  a white unicorn"                                                                                           
## [178] "  2 dust vortices"                                                                                           
## [179] "  a plains centaur"                                                                                          
## [180] " an ape"                                                                                                     
## [181] "  a Woodland-elf"                                                                                            
## [182] "  2 soldier ants"                                                                                            
## [183] "  a bugbear"                                                                                                 
## [184] " an imp"                                                                                                     
## [185] "  a wood nymph"                                                                                              
## [186] "  a water nymph"                                                                                             
## [187] "  a rock piercer"                                                                                            
## [188] "  a pony"                                                                                                    
## [189] "  3 fog clouds"                                                                                              
## [190] "  a yellow light"                                                                                            
## [191] "  a violet fungus"                                                                                           
## [192] "  2 gnome lords"                                                                                             
## [193] "  2 gnomish wizards"                                                                                         
## [194] "  2 gray oozes"                                                                                              
## [195] "  2 elf zombies"                                                                                             
## [196] "  a straw golem"                                                                                             
## [197] "  a paper golem"                                                                                             
## [198] "  2 giant ants"                                                                                              
## [199] "  2 little dogs"                                                                                             
## [200] "  3 floating eyes"                                                                                           
## [201] "  8 dwarves"                                                                                                 
## [202] "  a homunculus"                                                                                              
## [203] "  3 kobold lords"                                                                                            
## [204] "  3 kobold shamans"                                                                                          
## [205] " 13 hill orcs"                                                                                               
## [206] "  4 rothes"                                                                                                  
## [207] "  2 centipedes"                                                                                              
## [208] "  3 giant bats"                                                                                              
## [209] "  6 dwarf zombies"                                                                                           
## [210] "  a werejackal"                                                                                              
## [211] "  3 iguanas"                                                                                                 
## [212] " 23 killer bees"                                                                                             
## [213] " an acid blob"                                                                                               
## [214] "  a coyote"                                                                                                  
## [215] "  3 gas spores"                                                                                              
## [216] "  5 hobbits"                                                                                                 
## [217] "  7 manes"                                                                                                   
## [218] "  2 large kobolds"                                                                                           
## [219] "  a hobgoblin"                                                                                               
## [220] "  2 giant rats"                                                                                              
## [221] "  2 cave spiders"                                                                                            
## [222] "  a yellow mold"                                                                                             
## [223] "  6 gnomes"                                                                                                  
## [224] "  8 garter snakes"                                                                                           
## [225] "  2 gnome zombies"                                                                                           
## [226] "  8 geckos"                                                                                                  
## [227] " 11 jackals"                                                                                                 
## [228] "  5 foxes"                                                                                                   
## [229] "  2 kobolds"                                                                                                 
## [230] "  2 goblins"                                                                                                 
## [231] "  a sewer rat"                                                                                               
## [232] "  6 grid bugs"                                                                                               
## [233] "  3 lichens"                                                                                                 
## [234] "  2 kobold zombies"                                                                                          
## [235] "  5 newts"                                                                                                   
## [236] "206 creatures vanquished."                                                                                   
## [237] ""                                                                                                            
## [238] "No species were genocided or became extinct."                                                                
## [239] ""                                                                                                            
## [240] "Voluntary challenges:"                                                                                       
## [241] " You never genocided any monsters."                                                                          
## [242] " You never polymorphed an object."                                                                           
## [243] " You never changed form."                                                                                    
## [244] " You used no wishes."                                                                                        
## [245] ""                                                                                                            
## [246] "The Dungeons of Doom: levels 1 to 6"                                                                         
## [247] "   Level 1:"                                                                                                 
## [248] "      A fountain."                                                                                           
## [249] "   Level 2:"                                                                                                 
## [250] "      A sink."                                                                                               
## [251] "   Level 3:"                                                                                                 
## [252] "      A general store, a fountain."                                                                          
## [253] "   Level 4:"                                                                                                 
## [254] "      A general store, a fountain."                                                                          
## [255] "      Stairs down to The Gnomish Mines."                                                                     
## [256] "   Level 5:"                                                                                                 
## [257] "      A fountain."                                                                                           
## [258] "   Level 6: &lt;- You were here."                                                                               
## [259] "      A general store."                                                                                      
## [260] "      Final resting place for"                                                                               
## [261] "         you, killed by an ape."                                                                             
## [262] "The Gnomish Mines: levels 5 to 8"                                                                            
## [263] "   Level 5:"                                                                                                 
## [264] "   Level 6:"                                                                                                 
## [265] "   Level 7:"                                                                                                 
## [266] "      Many shops, a temple, some fountains."                                                                 
## [267] "   Level 8:"                                                                                                 
## [268] ""                                                                                                            
## [269] "Game over:"                                                                                                  
## [270] "                       ----------"                                                                           
## [271] "                      /          \\"                                                                         
## [272] "                     /    REST    \\"                                                                        
## [273] "                    /      IN      \\"                                                                       
## [274] "                   /     PEACE      \\"                                                                      
## [275] "                  /                  \\"                                                                     
## [276] "                  |  brothertrebius  |"                                                                      
## [277] "                  |      59 Au       |"                                                                      
## [278] "                  | killed by an ape |"                                                                      
## [279] "                  |                  |"                                                                      
## [280] "                  |                  |"                                                                      
## [281] "                  |                  |"                                                                      
## [282] "                  |       2018       |"                                                                      
## [283] "                 *|     *  *  *      | *"                                                                    
## [284] "        _________)/\\\\_//(\\/(/\\)/\\//\\/|_)_______"                                                       
## [285] ""                                                                                                            
## [286] "Goodbye brothertrebius the Ranger..."                                                                        
## [287] ""                                                                                                            
## [288] "You died in The Dungeons of Doom on dungeon level 6 with 6652 points,"                                       
## [289] "and 59 pieces of gold, after 7398 moves."                                                                    
## [290] "You were level 8 with a maximum of 54 hit points when you died."                                             
## [291] ""</code></pre>
</details>
<p>
Now, I am curious to see how many games are played per day:
</p>
<pre class="r"><code>runs_per_day &lt;- nethack %&gt;%
  group_by(date) %&gt;%
  count() %&gt;%
  ungroup() 


ggplot(runs_per_day, aes(y = n, x = date)) + 
  geom_point(colour = "#0f4150") + 
  geom_smooth(colour = "#82518c") + 
  theme_blog()</code></pre>
<pre><code>## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis-12-1.png" width="672">
</p>
<p>
The number of games seems to be stable since 2015, around 50. But what is also interesting is not only the number of games played, but also how many of these games resulted in a win.
</p>
<p>
For this, let‚Äôs also add a new column that tells us whether the played <em>ascended</em> (won the game) or not:
</p>
<pre class="r"><code>nethack %&lt;&gt;%
  mutate(Ascended = ifelse(death == "ascended", "Ascended", "Died an horrible death"))</code></pre>
<p>
I‚Äôm curious to see how many players managed to ascend‚Ä¶ NetHack being as hard as diamonds, probably not a lot:
</p>
<pre class="r"><code>ascensions_per_day &lt;- nethack %&gt;%
  group_by(date, Ascended) %&gt;%
  count() %&gt;%
  rename(Total = n)

ggplot(ascensions_per_day) + 
  geom_area(aes(y = Total, x = as.Date(date), fill = Ascended)) +
  theme_blog() +
  labs(y = "Number of runs", x = "Date") +
  scale_fill_blog() +
  theme(legend.title = element_blank())</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis-14-1.png" width="672">
</p>
<p>
Yeah, just as expected. Because there is so much data, it‚Äôs difficult to see clearly, though. Depending on the size of the screen you‚Äôre reading this, it might seem that in some days there are a lot of ascensions. This is only an impression due to the resolution of the picture. Let‚Äôs see the share of ascensions per year (and how many times the quests fail miserably), and this will become more apparent:
</p>
<pre class="r"><code>ascensions_per_day %&gt;%
  mutate(Year = year(as.Date(date))) %&gt;%
  group_by(Year, Ascended) %&gt;%
  summarise(Total = sum(Total, na.rm = TRUE)) %&gt;%
  group_by(Year) %&gt;%
  mutate(denom = sum(Total, na.rm = TRUE)) %&gt;%
  ungroup() %&gt;%
  mutate(Share = Total/denom) %&gt;%
  ggplot() + 
  geom_col(aes(y = Share, x = Year, fill = Ascended)) + 
  theme_blog() + 
  scale_fill_blog() + 
  theme(legend.title = element_blank())</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis-15-1.png" width="672">
</p>
<p>
I will now convert the ‚Äútime‚Äù column to seconds. I am not yet sure that this column is really useful, because NetHack is a turn based game. This means that when the player does not move, neither do the monsters. So the seconds spent playing might not be a good proxy for actual time spent playing. But it makes for a good exercise:
</p>
<pre class="r"><code>convert_to_seconds &lt;- function(time_string){
    time_numeric &lt;- time_string %&gt;%
        str_split(":", simplify = TRUE) %&gt;%
        as.numeric

    time_in_seconds &lt;- sum(time_numeric * c(3600, 60, 1))

    time_in_seconds 
}</code></pre>
<p>
The strings I want to convert are of the form ‚Äú01:34:43‚Äù, so I split at the ‚Äú:‚Äù and then convert the result to numeric. I end up with an atomic vector (<code>c(1, 34, 43)</code>). Then I multiple each element by the right number of seconds, and sum that to get the total. Let‚Äôs apply it to my data:
</p>
<pre class="r"><code>nethack %&lt;&gt;%
  mutate(time_in_seconds = map_dbl(time, convert_to_seconds))</code></pre>
<p>
What is the distribution of ‚Äútime_in_seconds‚Äù?
</p>
<pre class="r"><code>nethack %&gt;%
  describe(time_in_seconds)</code></pre>
<pre><code>## # A tibble: 1 x 15
##   variable type    nobs   mean     sd mode    min    max   q05   q25 median
##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
## 1 time_in‚Ä¶ Nume‚Ä¶ 322485 23529. 2.73e5 &lt;NA&gt;     61 2.72e7   141   622   1689
## # ‚Ä¶ with 4 more variables: q75 &lt;dbl&gt;, q95 &lt;dbl&gt;, n_missing &lt;int&gt;,
## #   n_unique &lt;lgl&gt;</code></pre>
<p>
We see that the minimum of <code>time_in_seconds</code> is 61 whereas the maximum is of the order of 27200000‚Ä¶ This must be a mistake, because that is almost one year!
</p>
<pre class="r"><code>nethack %&gt;%
  filter(time_in_seconds == max(time_in_seconds, na.rm = TRUE))</code></pre>
<pre><code>##   rank      score   name       time   turns lev_max  hp_max role race
## 1   28 3173960108 fisted 7553:41:49 6860357    4/47 362/362  Wiz  Elf
##   gender alignment                      death       date dumplog year
## 1    Mal       Neu drowned in a pool of water 2017-02-02      NA 2017
##   month day               Ascended time_in_seconds
## 1     2   2 Died an horrible death        27193309</code></pre>
<p>
Well‚Ä¶ maybe ‚Äúfisted‚Äù wanted to break the record of the longest NetHack game ever. Congratulations!
</p>
<p>
Let‚Äôs take a look at the density but cut it at 90th percentile:
</p>
<pre class="r"><code>nethack %&gt;%
  filter(!is.na(time_in_seconds),
         time_in_seconds &lt; quantile(time_in_seconds, 0.9, na.rm = TRUE)) %&gt;%
  ggplot() + 
  geom_density(aes(x = time_in_seconds), colour = "#82518c") + 
  theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis-20-1.png" width="672">
</p>
<p>
As expected, the distribution is right skewed. However, as explained above NetHack is a turn based game, meaning that if the player does not move, the monsters won‚Äôt move either. Perhaps it makes more sense to look at the <code>turns</code> column:
</p>
<pre class="r"><code>nethack %&gt;%
  describe(turns)</code></pre>
<pre><code>## # A tibble: 1 x 15
##   variable type    nobs  mean     sd mode    min    max   q05   q25 median
##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
## 1 turns    Nume‚Ä¶ 322485 4495. 19853. &lt;NA&gt;      1 6.86e6   202   871   1818
## # ‚Ä¶ with 4 more variables: q75 &lt;dbl&gt;, q95 &lt;dbl&gt;, n_missing &lt;int&gt;,
## #   n_unique &lt;lgl&gt;</code></pre>
<p>
The maximum is quite large too. Just like before, let‚Äôs focus by cutting the variable at the 90th percentile:
</p>
<pre class="r"><code>nethack %&gt;%
  filter(!is.na(turns),
         turns &lt; quantile(turns, 0.9, na.rm = TRUE)) %&gt;% 
  ggplot() + 
  geom_density(aes(x = turns), colour = "#82518c") + 
  theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis-22-1.png" width="672">
</p>
<p>
I think that using <code>turns</code> makes more sense. In the a future blog post, I will estimate a survival model and see how long players survive, and will use <code>turns</code> instead of <code>time_in_seconds</code>.
</p>
</section>
<section id="analysis" class="level2">
<h2 class="anchored" data-anchor-id="analysis">
Analysis
</h2>
<section id="what-kills-the-players" class="level3">
<h3 class="anchored" data-anchor-id="what-kills-the-players">
What kills the players
</h3>
<p>
To know what kills players so much, some cleaning of the <code>death</code> column is in order. Death can occur from poisoning, starvation, accidents, drowning‚Ä¶ of course monsters can kill the player too. Here are some values of the <code>death</code> variable:
</p>
<pre><code>burned by a tower of flame
choked on a lichen corpse
died of starvation
fell into a pit of iron spikes
killed by a gnome
killed by a gnome called Blabla
killed by a gnome called Blabla while sleeping
slipped while mounting a saddled pony
slipped while mounting a saddled pony called Jolly Jumper
zapped her/himself with a spell</code></pre>
<p>
To know what is the most frequent cause of death, I have to do some cleaning, because if not, ‚Äúkilled by a gnome‚Äù and ‚Äúkilled by a gnome called Blabla‚Äù would be two different causes of death. In the end, what interests me is to know how many times the player got killed by a gnome.
</p>
<p>
The following lines do a cleanup of the <code>death</code> variable:
</p>
<pre class="r"><code>nethack %&lt;&gt;% 
  mutate(death2 = case_when(str_detect(death, "poisoned") ~ "poisoned",
                            str_detect(death, "slipped") ~ "accident",
                            str_detect(death, "petrified") ~ "petrified",
                            str_detect(death, "choked") ~ "accident",
                            str_detect(death, "caught.*self") ~ "accident",
                            str_detect(death, "starvation") ~ "starvation",
                            str_detect(death, "drowned") ~ "drowned",
                            str_detect(death, "fell") ~ "fell",
                            str_detect(death, "zapped") ~ "zapped",
                            str_detect(death, "killed") ~ "killed",
                            TRUE ~ death)) %&gt;%
  mutate(death3 = str_extract(death, "(?&lt;=by|while).*")) %&gt;%
  mutate(death3 = case_when(str_detect(death3, ",|\\bcalled\\b") ~ str_extract(death3, "(.*?),|(.*?)\\bcalled\\b"), 
                            TRUE ~ death3)) %&gt;%
  mutate(death3 = str_remove(death3, ",|called|\\ban?"),
         death3 = str_trim(death3))</code></pre>
<p>
<code>death2</code> is a new variable, in which I broadly categorize causes of death. Using regular expressions I detect causes of death and aggregate some categories, for instance ‚Äúslipped‚Äù and ‚Äúchocked‚Äù into ‚Äúaccident‚Äù. Then, I want to extract everything that comes after the strings ‚Äúby‚Äù or while, and put the result into a new variable called <code>death3</code>. Then I detect the string ‚Äú,‚Äù or ‚Äúcalled‚Äù; if one of these strings is present, I extract everything that comes before ‚Äú,‚Äù or that comes before ‚Äúcalled‚Äù. Finally, I remove ‚Äú,‚Äù, ‚Äúcalled‚Äù or ‚Äúa‚Äù or ‚Äúan‚Äù from the string and trim the whitespaces.
</p>
<p>
Let‚Äôs take a look at these new variables:
</p>
<pre class="r"><code>set.seed(123)
nethack %&gt;%
    select(name, death, death2, death3) %&gt;%
    sample_n(10)</code></pre>
<pre><code>##              name                     death   death2        death3
## 92740   DianaFury     killed by a death ray   killed     death ray
## 254216    Oddabit         killed by a tiger   killed         tiger
## 131889    shachaf      killed by a fire ant   killed      fire ant
## 284758        a43  poisoned by a killer bee poisoned    killer bee
## 303283      goast         killed by a gecko   killed         gecko
## 14692     liberty    killed by a gnome king   killed    gnome king
## 170303     arch18                  ascended ascended          &lt;NA&gt;
## 287786 foolishwtf           killed by a bat   killed           bat
## 177826    Renleve     killed by a giant bat   killed     giant bat
## 147248      TheOV killed by a black unicorn   killed black unicorn</code></pre>
<p>
Now, it is quite easy to know what monsters are the meanest buttholes; let‚Äôs focus on the top 15. Most likely, these are going to be early game monsters. Let‚Äô see:
</p>
<pre class="r"><code>nethack %&gt;%
    filter(!is.na(death3)) %&gt;%
    count(death3) %&gt;%
    top_n(15) %&gt;%
    mutate(death3 = fct_reorder(death3, n, .desc = FALSE)) %&gt;%
    ggplot() + 
    geom_col(aes(y = n, x = death3)) + 
    coord_flip() + 
    theme_blog() + 
    scale_fill_blog() + 
    ylab("Number of deaths caused") +
    xlab("Monster")</code></pre>
<pre><code>## Selecting by n</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis-25-1.png" width="672">
</p>
<p>
Seems like soldier ants are the baddest, followed by jackals and dwarfs. As expected, these are mostly early game monsters. Thus, it would be interesting to look at this distribution, but at different stages in the game. Let‚Äôs create a categorical variable that discretizes <code>turns</code>, and then create one plot per category:
</p>
<details>
<p>
</p><summary>
Click to expand
</summary>
<p></p>
<pre class="r"><code>nethack %&gt;%
    filter(!is.na(death3)) %&gt;%
    filter(!is.na(turns)) %&gt;%
    mutate(turn_flag = case_when(between(turns, 1, 5000) ~ "Less than 5000",
                                 between(turns, 5001, 10000) ~ "Between 5001 and 10000",
                                 between(turns, 10001, 20000) ~ "Between 10001 and 20000",
                                 between(turns, 20001, 40000) ~ "Between 20001 and 40000",
                                 between(turns, 40001, 60000) ~ "Between 40001 and 60000",
                                 turns &gt; 60000 ~ "More than 60000")) %&gt;%
    mutate(turn_flag = factor(turn_flag, levels = c("Less than 5000", 
                                                    "Between 5001 and 10000",
                                                    "Between 10001 and 20000",
                                                    "Between 20001 and 40000",
                                                    "Between 40001 and 60000",
                                                    "More than 60000"), ordered = TRUE)) %&gt;%
    group_by(turn_flag) %&gt;%
    count(death3) %&gt;%
    top_n(15) %&gt;%
    nest() %&gt;%
    mutate(data = map(data, ~mutate(., death3 = fct_reorder(death3, n, .desc = TRUE))))  %&gt;%
    mutate(plots = map2(.x = turn_flag,
                         .y = data,
                         ~ggplot(data = .y) + 
                             geom_col(aes(y = n, x = death3)) + 
                             coord_flip() + 
                             theme_blog() + 
                             scale_fill_blog() + 
                             ylab("Number of deaths caused") +
                             xlab("Monster") + 
                             ggtitle(.x))) %&gt;%
    pull(plots)</code></pre>
<pre><code>## Selecting by n</code></pre>
<pre><code>## [[1]]</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis-26-1.png" width="672">
</p>
<pre><code>## 
## [[2]]</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis-26-2.png" width="672">
</p>
<pre><code>## 
## [[3]]</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis-26-3.png" width="672">
</p>
<pre><code>## 
## [[4]]</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis-26-4.png" width="672">
</p>
<pre><code>## 
## [[5]]</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis-26-5.png" width="672">
</p>
<pre><code>## 
## [[6]]</code></pre>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis-26-6.png" width="672">
</details>
<p>
Finally, for this section, I&nbsp;want to know if there are levels, or floors, where players die more often than others. For this, we can take a look at the <code>lev_max</code> column. Observations in this column are of the form ‚Äú8/10‚Äù. This means that the player died on level 8, but the lowest level that was explored was the 10th. Let‚Äôs do this for the year 2017 first. Before anything, I have to explain the layout of the levels of the game. You can see a diagram <a href="https://nethackwiki.com/wiki/Mazes_of_Menace#Map">here</a>. The player starts on floor 1, and goes down to level 53. Then, the player can ascend, by going on levels -1 to -5. But there are more levels than these ones. -6 and -9 are the sky, and the player can teleport there (but will fall to his death). If the player teleports to level -10, he‚Äôll enter heaven (and die too). Because these levels are special, I do not consider them here. I do not consider level 0 either, which is ‚ÄúNowhere‚Äù. Let‚Äôs get the number of players who died on each floor, but also compute the cumulative death count:
</p>
<pre class="r"><code>died_on_level &lt;- nethack %&gt;%
    filter(Ascended == "Died an horrible death") %&gt;%
    mutate(died_on = str_extract(lev_max, "-?\\d{1,}")) %&gt;%
    mutate(died_on = as.numeric(died_on)) %&gt;%
    group_by(year) %&gt;%
    count(died_on) %&gt;% 
    filter(died_on &gt;= -5, died_on != 0) %&gt;%
    mutate(died_on = case_when(died_on == -1 ~ 54,
                               died_on == -2 ~ 55,
                               died_on == -3 ~ 56,
                               died_on == -4 ~ 57,
                               died_on == -5 ~ 58,
                               TRUE ~ died_on)) %&gt;%
    arrange(desc(died_on)) %&gt;%
    mutate(cumul_deaths = cumsum(n))</code></pre>
<p>
Let‚Äôs take a look:
</p>
<pre class="r"><code>head(died_on_level)</code></pre>
<pre><code>## # A tibble: 6 x 4
## # Groups:   year [6]
##    year died_on     n cumul_deaths
##   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;        &lt;int&gt;
## 1  2002      58     5            5
## 2  2003      58    11           11
## 3  2004      58    19           19
## 4  2005      58    28           28
## 5  2006      58    25           25
## 6  2007      58    22           22</code></pre>
<p>
Now, let‚Äôs compute the number of players who ascended and add this to the cumulative count:
</p>
<pre class="r"><code>ascended_yearly &lt;- nethack %&gt;%
    filter(Ascended == "Ascended") %&gt;%
    group_by(year) %&gt;%
    count(Ascended)</code></pre>
<p>
Let‚Äôs take a look:
</p>
<pre class="r"><code>head(ascended_yearly)</code></pre>
<pre><code>## # A tibble: 6 x 3
## # Groups:   year [6]
##    year Ascended     n
##   &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;
## 1  2001 Ascended     4
## 2  2002 Ascended    38
## 3  2003 Ascended   132
## 4  2004 Ascended   343
## 5  2005 Ascended   329
## 6  2006 Ascended   459</code></pre>
<p>
I will modify the dataset a little bit and merge it with the previous one:
</p>
<pre class="r"><code>ascended_yearly %&lt;&gt;%
  rename(ascended_players = `n`) %&gt;%
  select(-Ascended)</code></pre>
<p>
Let‚Äôs add this to the data frame from before by merging both, and then we can compute the surviving players:
</p>
<pre class="r"><code>died_on_level %&lt;&gt;%
  full_join(ascended_yearly, by = "year") %&gt;%
  mutate(surviving_players = cumul_deaths + ascended_players)</code></pre>
<p>
Now we can compute the share of players who died on each level:
</p>
<pre class="r"><code>died_on_level %&gt;%
    mutate(death_rate = n/surviving_players) %&gt;% 
    ggplot(aes(y = death_rate, x = as.factor(died_on))) + 
    geom_line(aes(group = year, alpha = year), colour = "#82518c") +
    theme_blog() + 
    ylab("Death rate") +
    xlab("Level") + 
    theme(axis.text.x = element_text(angle = 90),
          legend.position = "none") + 
    scale_y_continuous(labels = scales::percent)</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nethack_analysis-33-1.png" width="672">
</p>
<p>
Looks like level 7 is consistently the most dangerous! The death rate there is more than 35%!
</p>
<p>
That‚Äôs it for this blog post, in the next one, I&nbsp;will focus on what players kill!
</p>
</section>
</section>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-11-03-nethack_analysis.html</guid>
  <pubDate>Sat, 03 Nov 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack</title>
  <link>https://b-rodrigues.github.io/posts/2018-11-01-nethack.html</link>
  <description><![CDATA[ 




<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
If someone told me a decade ago (back before I'd ever heard the term "roguelike") what I'd be doing today, I would have trouble believing this‚Ä¶<br><br>Yet here we are. <a href="https://t.co/N6Hh6A4tWl">pic.twitter.com/N6Hh6A4tWl</a>
</p>
‚Äî Josh Ge (<span class="citation" data-cites="GridSageGames">@GridSageGames</span>) <a href="https://twitter.com/GridSageGames/status/1009664438683648001?ref_src=twsrc%5Etfw">June 21, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<section id="update-07-11-2018" class="level2">
<h2 class="anchored" data-anchor-id="update-07-11-2018">
Update 07-11-2018
</h2>
<p>
The <code>{nethack}</code> package currently on Github contains a sample of 6000 NetHack games played on the alt.org/nethack public server between April and November 2018. This data was kindly provided by <a href="https://twitter.com/paxed"><code><span class="citation" data-cites="paxed">@paxed</span></code></a>. The tutorial in this blog post is still useful if you want to learn more about scraping with R and building a data package.
</p>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">
Abstract
</h2>
<p>
In this post, I am going to show you how you can scrape tables from a website, and then create a package with the tidied data to share with the world. The data I am going to scrape comes from a NetHack public server (<a href="https://alt.org/nethack/">link</a>). The data I discuss in this blog post is available in the <code>{nethack}</code> package I created and I will walk you through the process of releasing your package on CRAN. However, <code>{nethack}</code> is too large to be on CRAN (75 mb, while the maximum allowed is 5mb), so you can install it to play around with the data from github:
</p>
<pre class="r"><code>devtools::install_github("b-rodrigues/nethack")</code></pre>
<p>
And to use it:
</p>
<pre class="r"><code>library(nethack)
data("nethack")</code></pre>
<p>
The data contains information on games played from 2001 to 2018; 322485 rows and 14 columns. I will analyze the data in a future blog post. This post focuses on getting and then sharing the data. By the way, all the content from the public server I scrape is under the CC&nbsp;BY 4.0 license.
</p>
<p>
I built the package by using the very useful <code>{devtools}</code> package.
</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">
Introduction
</h2>
<p>
NetHack is a game released in 1987 that is still being played and developed today. NetHack is a roguelike game, meaning that is has procedurally generated dungeons and permadeath. If you die, you have to start over, and because the dungeons are procedurally generated, this means that you cannot learn the layout of the dungeons you explore or know when ennemies are going to attack or even what ennemies are going to attack. Ennemies are not the only thing that you have to be careful about; you can die from a lot of different events, as you will see in this post. Objects that you find, such as a silver ring, might be helpful in a run, but be cursed in the next run.
</p>
<p>
The latest version of the game, 3.6.1, was released on April 27th 2018, and this is how it looks like:
</p>
<p>
<img src="https://upload.wikimedia.org/wikipedia/commons/f/ff/Nethack.png"><!-- -->
</p>
<p>
The graphics are‚Ä¶ bare-bones to say the least. The game runs inside a terminal emulator and is available for any platform. The goal of NetHack is to explore a dungeon and go down every level until you find the Amulet of Yendor. Once you find this Amulet, you have to go all the way back upstairs, enter and fight your way through the Elemental Planes, enter the final Astral Plane, and then finally offer the Amulet of Yendor to your god to finish the game. Needless to say, NetHack is very difficult and players can go years without ever finishing the game.
</p>
<p>
When you start an new game, you have to create a character, which can have several attributes. You have to choose a race (human, elf, orc, etc), a role (tourist, samurai, mage, etc) and an alignment (neutral, law, chaos) and these choices impact your base stats.
</p>
<p>
If you can‚Äôt get past the ASCII graphics, you can play NetHack with tileset:
</p>
<p>
<img src="https://vignette.wikia.nocookie.net/nethack/images/8/80/Vultures_eye.png/revision/latest?cb=20070313215112"><!-- -->
</p>
<p>
You can install NetHack on your computer or you can play online on a public server, such as this <a href="https://alt.org/nethack/">one</a>. There are several advantages when playing on a pubic server; the player does not have to install anyhing, and we data enthusiasts have access to a mine of information! For example, you can view the following <a href="https://alt.org/nethack/gamesday.php?date=20181025">table</a> which contains data on all the games played on October 25th 2018. These tables start in the year 2001, and I am going to scrape the info from these tables, which will allow me to answer several questions. For instance, what is the floor most players die on? What kills most players? What role do players choose more often? I will explore this questions in a future blog post, but for now I will focus on scraping the data and realeasing it as a package to CRAN.
</p>
</section>
<section id="scraping-the-data" class="level2">
<h2 class="anchored" data-anchor-id="scraping-the-data">
Scraping the data
</h2>
<p>
To scrape the data I&nbsp;wrote a big function that does several things:
</p>
<pre class="r"><code>library("tidyverse")
library("rvest")


scrape_one_day &lt;- function(link){

    convert_to_seconds &lt;- function(time_string){
        time_numeric &lt;- time_string %&gt;%
            str_split(":", simplify = TRUE) %&gt;%
            as.numeric
     
     time_in_seconds &lt;- time_numeric * c(3600, 60, 1)
     
     if(is.na(time_in_seconds)){
         time_in_seconds &lt;- 61
     } else {
         time_in_seconds &lt;- sum(time_in_seconds)
     }
     return(time_in_seconds)
    }

    Sys.sleep(1)

    date &lt;- str_extract(link, "\\d{8}")

    read_lines_slow &lt;- function(...){
        Sys.sleep(1)
        read_lines(...)
    }
    
    page &lt;- read_html(link)

        # Get links
    dumplogs &lt;- page %&gt;% 
        html_nodes(xpath = '//*[(@id = "perday")]//td') %&gt;%
        html_children() %&gt;%
        html_attr("href") %&gt;%
        keep(str_detect(., "dumplog"))

    # Get table
    table &lt;- page %&gt;%
        html_node(xpath = '//*[(@id = "perday")]') %&gt;%
        html_table(fill = TRUE)

    if(is_empty(dumplogs)){
        print("dumplogs empty")
        dumplogs &lt;- rep(NA, nrow(table))
    } else {
        dumplogs &lt;- dumplogs
    }
    
    final &lt;- table %&gt;%
        janitor::clean_names() %&gt;%
        mutate(dumplog_links = dumplogs)

    print(paste0("cleaning data of date ", date))
    
    clean_final &lt;- final %&gt;%
        select(-x) %&gt;%
        rename(role = x_2,
               race = x_3,
               gender = x_4,
               alignment = x_5) %&gt;%
        mutate(time_in_seconds = map(time, convert_to_seconds)) %&gt;%
        filter(!(death %in% c("quit", "escaped")), time_in_seconds &gt; 60) %&gt;%
        mutate(dumplog = map(dumplog_links, ~possibly(read_lines_slow, otherwise = NA)(.))) %&gt;%
        mutate(time_in_seconds = ifelse(time_in_seconds == 61, NA, time_in_seconds))

    saveRDS(clean_final, paste0("datasets/data_", date, ".rds"))

}</code></pre>
<p>
Let‚Äôs go through each part. The first part is a function that converts strings like ‚Äú02:21:76‚Äù to seconds:
</p>
<pre class="r"><code>convert_to_seconds &lt;- function(time_string){
    time_numeric &lt;- time_string %&gt;%
        str_split(":", simplify = TRUE) %&gt;%
        as.numeric
 
time_in_seconds &lt;- time_numeric * c(3600, 60, 1)
 
if(is.na(time_in_seconds)){
  time_in_seconds &lt;- 61
  } else {
    time_in_seconds &lt;- sum(time_in_seconds)
    }
return(time_in_seconds)
}</code></pre>
<p>
I will use this function on the column that gives the length of the run. However, before March 2008 this column is always empty, this is why I have the <code>if()‚Ä¶else()</code> statement at the end; if the time in seconds is <code>NA</code>, then I make it 61&nbsp;seconds. I do this because I want to keep runs longer than 60 seconds, something I use <code>filter()</code> for later. But when filtering, if the condition returns <code>NA</code> (which happens when you do <code>NA &gt; 60</code>) then you get an error, and the function fails.
</p>
<p>
The website links I am going to scrape all have the date of the day the runs took place. I am going to keep this date because I will need to name the datasets I am going to write to disk:
</p>
<pre class="r"><code>date &lt;- str_extract(link, "\\d{8}")</code></pre>
<p>
Next, I define this function:
</p>
<pre class="r"><code>read_lines_slow &lt;- function(...){
    Sys.sleep(1)
    read_lines(...)
}</code></pre>
<p>
It is a wrapper around the <code>readr::read_lines()</code> with a call to <code>Sys.sleep(1)</code>. I will be scraping a lot of pages, so letting one second pass between each page will not overload the servers so much.
</p>
<p>
I then read the link with <code>read_html()</code> and start by getting the links of the dumplogs:
</p>
<pre class="r"><code>page &lt;- read_html(link)

# Get links
dumplogs &lt;- page %&gt;% 
    html_nodes(xpath = '//*[(@id = "perday")]//td') %&gt;%
    html_children() %&gt;%
    html_attr("href") %&gt;%
    keep(str_detect(., "dumplog"))</code></pre>
<p>
You might be wondering what are dumplogs. Take a look at this screenshot:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/dumplogs.png"><!-- -->
</p>
<p>
When you click on those <code>d</code>‚Äôs, you land on a page like this <a href="http://archive.is/wljb3">one</a> (I archived it to be sure that this link will not die). These logs contain a lot of info that I want to keep. To find the right <code>xpath</code> to scrape the links, <code>//*[(@id = "perday")]//td</code>, I used the SelectorGadget* extension for Chrome. First I chose the table:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/selectorgadget1.png"><!-- -->
</p>
<p>
and then the links I am interested in:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/selectorgadget2.png"><!-- -->
</p>
<p>
Putting them together, I get the right ‚Äúxpath‚Äù. But just as with the time of the run, dumplogs are only available after a certain date. So in case the <code>dumplogs</code> column is empty, I relpace it with <code>NA</code>.
</p>
<pre class="r"><code>if(is_empty(dumplogs)){
    print("dumplogs empty")
    dumplogs &lt;- rep(NA, nrow(table))
} else {
    dumplogs &lt;- dumplogs
}</code></pre>
<p>
The rest is quite simple:
</p>
<pre class="r"><code># Get table
table &lt;- page %&gt;%
    html_node(xpath = '//*[(@id = "perday")]') %&gt;%
    html_table(fill = TRUE)
               
final &lt;- table %&gt;%
    janitor::clean_names() %&gt;%
    mutate(dumplog_links = dumplogs)
           
print(paste0("cleaning data of date ", date))</code></pre>
<p>
I scrape the table, and then join the dumplog links to the table inside a new column called ‚Äúdumplog_links‚Äù.
</p>
<p>
Because what follows is a long process, I print a message to let me know the progress of the scraping.
</p>
<p>
Now the last part:
</p>
<pre class="r"><code>clean_final &lt;- final %&gt;%
    select(-x) %&gt;%
    rename(role = x_2,
           race = x_3,
           gender = x_4,
           alignment = x_5) %&gt;%
    mutate(time_in_seconds = map(time, convert_to_seconds)) %&gt;%
    filter(!(death %in% c("quit", "escaped")), time_in_seconds &gt; 60) %&gt;%
    mutate(dumplog = map(dumplog_links, ~possibly(read_lines_slow, otherwise = NA)(.))) %&gt;%
    mutate(time_in_seconds = ifelse(time_in_seconds == 61, NA, time_in_seconds))</code></pre>
<p>
I first remove and remane columns. Then I convert the ‚Äútime‚Äù column into seconds and also remove runs that lasted less than 60 seconds or that ended either in ‚Äúquit‚Äù (the player left the game) or ‚Äúescaped‚Äù (the player left the dungeon and the game ended immediately). There are a lot of runs like that and they‚Äôre not interesting. Finally, and this is what takes long, I create a new list-column where each element is the contents of the dumplog for that run. I wrap <code>read_lines_slow()</code> around <code>purrr::possibly()</code> because dumplogs are missing for certains runs and when I try to read them I get an 404 error back. Getting such an error stops the whole process, so with <code>purrr::possibly()</code> I can specify that in that case I want <code>NA</code> back. Basically, a function wrapped inside <code>purrr::possibly()</code> never fails! Finally, if a game lasts for 61 seconds, I convert it back to <code>NA</code> (remember this was used to avoid having problems with the <code>filter()</code> function).
</p>
<p>
Finally, I export what I scraped to disk:
</p>
<pre class="r"><code>saveRDS(clean_final, paste0("datasets/data_", date, ".rds"))</code></pre>
<p>
This is where I use the date; to name the data. This is really important because scraping takes a very long time, so if I don‚Äôt write the progress to disk as it goes, I might lose hours of work if my internet goes down, or if computer freezes or whatever.
</p>
<p>
In the lines below I build the links that I am going to scrape. They‚Äôre all of the form: <code>https://alt.org/nethack/gamesday.php?date=YYYYMMDD</code> so it‚Äôs quite easy to create a list of dates to scrape, for example, for the year 2017:
</p>
<pre class="r"><code>link &lt;- "https://alt.org/nethack/gamesday.php?date="

dates &lt;- seq(as.Date("2017/01/01"), as.Date("2017/12/31"), by = "day") %&gt;%
    str_remove_all("-")

links &lt;- paste0(link, dates)</code></pre>
<p>
Now I can easily scrape the data. To make extra sure that I will not have problems during the scraping process, for example if on a given day no games were played (and thus there is no table to scrape, which would result in an error) , I use the same trick as above by using <code>purrr::possibly()</code>:
</p>
<pre class="r"><code>map(links, ~possibly(scrape_one_day, otherwise = NULL)(.))</code></pre>
<p>
The scraping process took a very long time. I scraped all the data by letting my computer run for three days!
</p>
<p>
After this long process, I import all the <code>.rds</code> files into R:
</p>
<pre class="r"><code>path_to_data &lt;- Sys.glob("datasets/*.rds")
nethack_data &lt;- map(path_to_data, readRDS)</code></pre>
<p>
and take a look at one of them:
</p>
<pre class="r"><code>nethack_data[[5812]] %&gt;% 
  View()</code></pre>
<p>
Let‚Äôs convert the ‚Äúscore‚Äù column to integer. For this, I will need to convert strings that look like ‚Äú12,232‚Äù to integers. I‚Äôll write a short function to do this:
</p>
<pre class="r"><code>to_numeric &lt;- function(string){
  str_remove_all(string, ",") %&gt;%
    as.numeric
}</code></pre>
<pre class="r"><code>nethack_data &lt;- nethack_data %&gt;%
  map(~mutate(., score = to_numeric(score)))</code></pre>
<p>
Let‚Äôs merge the data into a single data frame:
</p>
<pre class="r"><code>nethack_data &lt;- bind_rows(nethack_data)</code></pre>
<p>
Now that I have a nice data frame, I will remove some columns and start the process of making a packages. I remove the columns that I created and that are now useless (such as the <code>dumplog_links</code> column).
</p>
<pre class="r"><code>nethack_data &lt;- nethack_data %&gt;%
  select(rank, score, name, time, turns, lev_max, hp_max, role, race, gender, alignment, death,
         date, dumplog)</code></pre>
<p>
Export this to <code>.rds</code> format, as it will be needed later:
</p>
<pre class="r"><code>saveRDS(nethack_data, "nethack_data.rds")</code></pre>
</section>
<section id="making-a-package-to-share-your-data-with-the-world" class="level2">
<h2 class="anchored" data-anchor-id="making-a-package-to-share-your-data-with-the-world">
Making a package to share your data with the world
</h2>
<p>
As stated in the beginning of this post, I will walk you through the process of creating and releasing your package on CRAN. However, the data I scraped was too large to be made available as a CRAN package. But you can still get the data from Github (link is in the abstract at the beginning of the post).
</p>
<p>
Making a data package is a great way to learn how to make packages, because it is relatively easy to do (for example, you do not need to write unit tests). First, let‚Äôs start a new project in RStudio:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/r_package1.png"><!-- -->
</p>
<p>
Then select ‚ÄúR package‚Äù:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/r_package2.png"><!-- -->
</p>
<p>
Then name your package, create a git repository and then click on ‚ÄúCreate Project‚Äù:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/r_package3.png"><!-- -->
</p>
<p>
RStudio wil open the <code>hello.R</code> script which you can now modify. You got to learn from the best, so I suggest that you modify <code>hello.R</code> by taking inspiration from the <code>babynames</code> package made by Hadley Wickham which you can find <a href="https://github.com/hadley/babynames/blob/master/R/data.R">here</a>. You do not need the first two lines, and can focus on lines 4 to 13. Then, rename the script to <code>data.R</code>. This is how <code>{nethack}‚Äôs</code> looks like:
</p>
<pre class="r"><code>#' NetHack runs data.
#'
#' Data on NetHack runs scraped from https://alt.org/nethack/gamesday.php
#'
#' @format A data frame with 14 variables: \code{rank}, \code{score},
#'   \code{name}, \code{time}, \code{turns}, \code{lev_max}, \code{hp_max}, \code{role}, \code{race},
#'   \code{gender}, \code{alignment}, \code{death}, \code{date} and \code{dumplog}
#' \describe{
#' \item{rank}{The rank of the player on that day}
#' \item{score}{The score the player achieved on that run}
#' \item{name}{The name of the player}
#' \item{time}{The time the player took to finish the game}
#' \item{turns}{The number of turns the player played before finishing the game}
#' \item{lev_max}{First digit: the level the player died on; second digit: the deepest explored level}
#' \item{hp_max}{The maximum character health points the player achieved}
#' \item{role}{The role the player chose to play as}
#' \item{race}{The race the player chose to play as}
#' \item{gender}{The gender the playr chose to play as}
#' \item{alignement}{The alignement the playr chose to play as}
#' \item{death}{The reason of death of the character}
#' \item{date}{The date the game took place}
#' \item{dumplog}{The log of the end game; this is a list column}
#' }
"nethack"</code></pre>
<p>
The comments are special, the ‚Äú#‚Äù is followed by a <code>‚Äô</code>; these are special comments that will be parsed by <code>roxygen2::roxygenise()</code> and converted to documentation files.
</p>
<p>
Next is the <code>DESCRIPTION</code> file. Here is how <code>{nethack}</code>‚Äôs looks like:
</p>
<pre><code>Package: nethack
Type: Package
Title: Data from the Video Game NetHack
Version: 0.1.0
Authors@R: person("Bruno Andr√©", "Rodrigues Coelho", email = "bruno@brodrigues.co",
                  role = c("aut", "cre"))
Description: Data from NetHack runs played between 2001 to 2018 on 
    &lt;https://alt.org/nethack/&gt;, a NetHack public server.
Depends: R (&gt;= 2.10)
License: CC BY 4.0
Encoding: UTF-8
LazyData: true
RoxygenNote: 6.1.0</code></pre>
<p>
Adapt yours accordingly. I chose the license <code>CC BY 4.0</code> because this was the licence under which the original data was published. It is also a good idea to add a <em>Vignette</em>:
</p>
<pre class="r"><code>devtools::use_vignette("the_nethack_package")</code></pre>
<p>
Vignettes are very useful documentation with more details and examples.
</p>
<p>
It is also good practice to add the script that was used to scrape the data. Such scripts go into <code>data-raw/</code>. Create this folder with:
</p>
<pre class="r"><code>devtools::use_data_raw()</code></pre>
<p>
This creates the <code>data-raw/</code> folder where I save the script that scrapes the data. Now is time to put the data in the package. Start by importing the data:
</p>
<pre class="r"><code>nethack &lt;- readRDS("nethack_data.rds")</code></pre>
<p>
To add the data to your package, you can use the following command:
</p>
<pre class="r"><code>devtools::use_data(nethack, compress = "xz")</code></pre>
<p>
This will create the <code>data/</code> folder and put the data in there in the <code>.rda</code> format. I use the ‚Äúcompress‚Äù option to make the data smaller. You can now create the documentation by running:
</p>
<pre class="r"><code>roxygen2::roxygenise()</code></pre>
<p>
Pay attention to the log messages: you might need to remove files (for example the documentation <code>hello.R</code>, under the folder <code>man/</code>).
</p>
<p>
Now you can finaly run <code>R CMD Check</code> by clicking the <code>Check</code> button on the ‚ÄúBuild‚Äù pane:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/r_package_check.png"><!-- -->
</p>
<p>
This will extensively check the package for <code>ERRORS</code>, <code>WARNINGS</code> and <code>NOTES</code>. You need to make sure that the check passes without any <code>ERRORS</code> or <code>WARNINGS</code> and try as much as possible to remove all <code>NOTES</code> too. If you cannot remove a <code>NOTE</code>, for example in my case the following:
</p>
<pre><code>checking installed package size ... NOTE
  installed size is 169.7Mb
  sub-directories of 1Mb or more:
    data  169.6Mb
R CMD check results
0 errors | 0 warnings  | 1 note </code></pre>
<p>
You should document it in a new file called <code>cran-comments.md</code>:
</p>
<pre><code>## Test environments
* local openSUSE Tumbleweed install, R 3.5.1
* win-builder (devel and release)

## R CMD check results
There were no ERRORs or WARNINGs.

There was 1 NOTE:

    *   installed size is 169.7Mb
sub-directories of 1Mb or more:
    data  169.6Mb

The dataset contains 17 years of NetHack games played, hence the size. This package will not be updated often (max once a year).</code></pre>
<p>
Once you have eliminated all errors and warnings, you are almost ready to go.
</p>
<p>
You need now to test the package on different platforms. This depends a bit on the system you run, for me, because I run openSUSE (a GNU+Linux distribution) I have to test on Windows. This can be done with:
</p>
<pre class="r"><code> devtools::build_win(version = "R-release")</code></pre>
<p>
and:
</p>
<pre class="r"><code> devtools::build_win(version = "R-devel")</code></pre>
<p>
Explain that you have tested your package on several platforms in the <code>cran-comments.md</code> file.
</p>
<p>
Finally you can add a <code>README.md</code> and a <code>NEWS.md</code> file and start the process of publishing the package on CRAN:
</p>
<pre class="r"><code>devtools:release()</code></pre>
<p>
If you want many more details than what you can find in this blog post, I urge you to read ‚ÄúR Packages‚Äù by Hadley Wickham, which you can read for free <a href="http://r-pkgs.had.co.nz/">here</a>.
</p>
</section>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-11-01-nethack.html</guid>
  <pubDate>Thu, 01 Nov 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Maps with pie charts on top of each administrative division: an example with Luxembourg‚Äôs elections data</title>
  <link>https://b-rodrigues.github.io/posts/2018-10-27-lux_elections_analysis.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=Bw8g_1VEEL8"> <img src="https://b-rodrigues.github.io/assets/img/europe_map_lux.png"></a>
</p>
</div>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">
Abstract
</h2>
<p>
You can find the data used in this blog post here: <a href="https://github.com/b-rodrigues/elections_lux" class="uri">https://github.com/b-rodrigues/elections_lux</a>
</p>
<p>
This is a follow up to a <a href="https://www.brodrigues.co/blog/2018-10-21-lux_elections/">previous blog post</a> where I extracted data of the 2018 Luxembourguish elections from Excel Workbooks. Now that I have the data, I will create a map of Luxembourg by commune, with pie charts of the results on top of each commune! To do this, I use good ol‚Äô <code>{ggplot2}</code> and another packages called <code>{scatterpie}</code>. As a bonus, I have added the code to extract the data from the 2013 elections from Excel. You‚Äôll find this code in the appendix at the end of the blog post.
</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">
Introduction
</h2>
<p>
Before importing the data for the elections of 2018, let‚Äôs install some packages:
</p>
<pre class="r"><code>install.packages('rgeos', type='source') # Dependency of rgdal
install.packages('rgdal', type='source') # To read in the shapefile</code></pre>
<p>
These packages might be very tricky to install on OSX and Linux, but they‚Äôre needed to import the shapefile of the country, which is needed to draw a map. So to make things easier, I have created an <code>rds</code> object, from the shapefile of Luxembourg, that you can import natively in R without needing these two packages. But if you want to use them, here is how:
</p>
<pre class="r"><code>communes &lt;- readOGR("Limadmin_SHP/LIMADM_COMMUNES.shp")</code></pre>
<p>
By the way, you can download the shapefile for Luxembourg <a href="https://data.public.lu/en/datasets/limites-administratives-du-grand-duche-de-luxembourg/#_">here</a>.
</p>
<p>
I‚Äôll use my shapefile though (that you can download from the same github repo as the data):
</p>
<pre class="r"><code>communes_df &lt;- readRDS("commune_shapefile.rds")</code></pre>
<p>
Here‚Äôs how it looks like:
</p>
<pre class="r"><code>head(communes_df)</code></pre>
<pre><code>##       long      lat order  hole piece      group       id
## 1 91057.65 101536.6     1 FALSE     1 Beaufort.1 Beaufort
## 2 91051.79 101487.3     2 FALSE     1 Beaufort.1 Beaufort
## 3 91043.43 101461.7     3 FALSE     1 Beaufort.1 Beaufort
## 4 91043.37 101449.8     4 FALSE     1 Beaufort.1 Beaufort
## 5 91040.42 101432.1     5 FALSE     1 Beaufort.1 Beaufort
## 6 91035.44 101405.6     6 FALSE     1 Beaufort.1 Beaufort</code></pre>
<p>
Now let‚Äôs load some packages:
</p>
<pre class="r"><code>library("tidyverse")
library("tidyxl")
library("ggplot2")
library("scatterpie")</code></pre>
<p>
Ok, now, let‚Äôs import the elections results data, which is the output of <a href="https://www.brodrigues.co/blog/2018-10-21-lux_elections/">last week‚Äôs blog post</a>:
</p>
<pre class="r"><code>elections &lt;- read_csv("elections_2018.csv")</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   Party = col_character(),
##   Year = col_double(),
##   Variables = col_character(),
##   Values = col_double(),
##   locality = col_character(),
##   division = col_character()
## )</code></pre>
<p>
I will only focus on the data at the commune level, and only use the share of votes for each party:
</p>
<pre class="r"><code>elections_map &lt;- elections %&gt;%
    filter(division == "Commune",
           Variables == "Pourcentage")</code></pre>
<p>
Now I need to make sure that the names of the communes are the same between the elections data and the shapefile. Usual suspects are the ‚ÄúHaute-S√ªre‚Äù and the ‚ÄúRedange-sur-Attert‚Äù communes, but let‚Äôs take a look:
</p>
<pre class="r"><code>locality_elections &lt;- unique(elections_map$locality)
locality_shapefile &lt;- unique(communes_df$id)

setdiff(locality_elections, locality_shapefile)</code></pre>
<pre><code>## [1] "Lac de la Haute-S√ªre" "Redange Attert"</code></pre>
<p>
Yep, exactly as expected. I‚Äôve had problems with the names of these two communes in the past already. Let‚Äôs rename these two communes in the elections data:
</p>
<pre class="r"><code>elections_map &lt;- elections_map %&gt;%
    mutate(commune = case_when(locality == "Lac de la Haute-S√ªre" ~ "Lac de la Haute S√ªre",
                          locality == "Redange Attert" ~ "Redange",
                          TRUE ~ locality))</code></pre>
<p>
Now, I can select the relevant columns from the shapefile:
</p>
<pre class="r"><code>communes_df &lt;- communes_df %&gt;%
    select(long, lat, commune = id)</code></pre>
<p>
and from the elections data:
</p>
<pre class="r"><code>elections_map &lt;- elections_map %&gt;%
    select(commune, Party, Variables, Values)</code></pre>
</section>
<section id="plotting-the-data-on-a-map" class="level2">
<h2 class="anchored" data-anchor-id="plotting-the-data-on-a-map">
Plotting the data on a map
</h2>
<p>
Now, for the type of plot I want to make, using the <code>{scatterpie}</code> package, I need the data to be in the wide format, not long. For this I will use <code>tidyr::spread()</code>:
</p>
<pre class="r"><code>elections_map &lt;- elections_map %&gt;% 
    spread(Party, Values)</code></pre>
<p>
This is how the data looks now:
</p>
<pre class="r"><code>glimpse(elections_map)</code></pre>
<pre><code>## Observations: 102
## Variables: 10
## $ commune     &lt;chr&gt; "Beaufort", "Bech", "Beckerich", "Berdorf", "Bertran‚Ä¶
## $ Variables   &lt;chr&gt; "Pourcentage", "Pourcentage", "Pourcentage", "Pource‚Ä¶
## $ ADR         &lt;dbl&gt; 0.12835106, 0.09848661, 0.08596748, 0.16339234, 0.04‚Ä¶
## $ CSV         &lt;dbl&gt; 0.2426239, 0.2945285, 0.3004751, 0.2604552, 0.290278‚Ä¶
## $ `d√©i gr√©ng` &lt;dbl&gt; 0.15695672, 0.21699651, 0.24072721, 0.15619529, 0.15‚Ä¶
## $ `d√©i L√©nk`  &lt;dbl&gt; 0.04043732, 0.03934808, 0.05435776, 0.02295273, 0.04‚Ä¶
## $ DP          &lt;dbl&gt; 0.15875393, 0.19394645, 0.12899689, 0.15444466, 0.30‚Ä¶
## $ KPL         &lt;dbl&gt; 0.015875393, 0.006519208, 0.004385164, 0.011476366, ‚Ä¶
## $ LSAP        &lt;dbl&gt; 0.11771754, 0.11455180, 0.08852549, 0.16592103, 0.09‚Ä¶
## $ PIRATEN     &lt;dbl&gt; 0.13928411, 0.03562282, 0.09656496, 0.06516242, 0.04‚Ä¶</code></pre>
<p>
For this to work, I need two datasets; one to draw the map (<code>commune_df</code>) and one to draw the pie charts over each commune, with the data to draw the charts, but also the position of where I want the pie charts. For this, I will compute the average of the longitude and latitude, which should be good enough:
</p>
<pre class="r"><code>scatterpie_data &lt;- communes_df %&gt;%
    group_by(commune) %&gt;%
    summarise(long = mean(long),
              lat = mean(lat))</code></pre>
<p>
Now, let‚Äôs join the two datasets:
</p>
<pre class="r"><code>final_data &lt;- left_join(scatterpie_data, elections_map, by = "commune") </code></pre>
<p>
I have all the ingredients to finally plot the data:
</p>
<pre class="r"><code>ggplot() +
    geom_polygon(data = communes_df, aes(x = long, y = lat, group = commune), colour = "grey", fill = NA) +
    geom_scatterpie(data = final_data, aes(x=long, y=lat, group=commune), 
                    cols = c("ADR", "CSV", "d√©i gr√©ng", "d√©i L√©nk", "DP", "KPL", "LSAP", "PIRATEN")) +
    labs(title = "Share of total vote in each commune, 2018 elections") +
    theme_void() +
    theme(legend.position = "bottom",
          legend.title = element_blank(),
          legend.text = element_text(colour = "white"),
          plot.background = element_rect("#272b30"),
          plot.title = element_text(colour = "white")) +
    scale_fill_manual(values = c("ADR" = "#009dd1",
                                 "CSV" = "#ee7d00",
                                 "d√©i gr√©ng" = "#45902c",
                                 "d√©i L√©nk" = "#e94067",
                                 "DP" = "#002a54",
                                 "KPL" = "#ff0000",
                                 "LSAP" = "#ad3648",
                                 "PIRATEN" = "#ad5ea9"))</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/lux_elections_analysis-17-1.png" width="768">
</p>
<p>
Not too bad, but we can‚Äôt really read anything from the pie charts. I will now make their size proportional to the number of voters in each commune. For this, I need to go back to the Excel sheets, and look for the right cell:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/electeurs_inscrits.png"><!-- -->
</p>
<p>
It will be easy to extract this info. It located in cell ‚ÄúE5‚Äù:
</p>
<pre class="r"><code>elections_raw_2018 &lt;- xlsx_cells("leg-2018-10-14-22-58-09-737.xlsx")

electors_commune &lt;- elections_raw_2018 %&gt;%
    filter(!(sheet %in% c("Le Grand-Duch√© de Luxembourg", "Centre", "Est", "Nord", "Sud", "Sommaire"))) %&gt;%
    filter(address == "E5") %&gt;%
    select(sheet, numeric) %&gt;%
    rename(commune = sheet,
           electors = numeric)</code></pre>
<p>
I can now add this to the data:
</p>
<pre class="r"><code>final_data &lt;- final_data %&gt;% 
    full_join(electors_commune) %&gt;%
    mutate(log_electors = log(electors) * 200)</code></pre>
<pre><code>## Joining, by = "commune"</code></pre>
<p>
In the last line, I create a new column called <code>log_electors</code> that I then multiply by 200. This will be useful later.
</p>
<p>
Now I can add the <code>r</code> argument inside the <code>aes()</code> function on the third line, to make the pie chart size proportional to the number of electors in that commune:
</p>
<pre class="r"><code>ggplot() +
  geom_polygon(data = communes_df, aes(x = long, y = lat, group = commune), colour = "grey", fill = NA) +
    geom_scatterpie(data = final_data, aes(x=long, y=lat, group = commune, r = electors), 
                    cols = c("ADR", "CSV", "d√©i gr√©ng", "d√©i L√©nk", "DP", "KPL", "LSAP", "PIRATEN")) +
    labs(title = "Share of total vote in each commune, 2018 elections") +
    theme_void() +
    theme(legend.position = "bottom",
          legend.title = element_blank(),
          legend.text = element_text(colour = "white"),
          plot.background = element_rect("#272b30"),
          plot.title = element_text(colour = "white")) +
    scale_fill_manual(values = c("ADR" = "#009dd1",
                                 "CSV" = "#ee7d00",
                                 "d√©i gr√©ng" = "#45902c",
                                 "d√©i L√©nk" = "#182024",
                                 "DP" = "#002a54",
                                 "KPL" = "#ff0000",
                                 "LSAP" = "#ad3648",
                                 "PIRATEN" = "#ad5ea9"))</code></pre>
<pre><code>## Warning: Removed 32 rows containing non-finite values (stat_pie).</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/lux_elections_analysis-21-1.png" width="768">
</p>
<p>
Ok, that was not a good idea! Perhaps the best option would be to have one map per circonscription. For this, I need the list of communes by circonscription. This is available on Wikipedia. Here are the lists:
</p>
<pre class="r"><code>centre &lt;- c("Bissen", "Colmar-Berg", "Fischbach", "Heffingen", "Larochette",
            "Lintgen", "Lorentzweiler", "Mersch", "Nommern", "Helperknapp", "Bertrange", "Contern", 
            "Hesperange", "Luxembourg", "Niederanven", "Sandweiler", "Schuttrange", "Steinsel", 
            "Strassen", "Walferdange", "Weiler-la-Tour")

est &lt;- c("Beaufort", "Bech", "Berdorf", "Consdorf", "Echternach", "Rosport-Mompach", "Waldbillig",
         "Betzdorf", "Biwer", "Flaxweiler", "Grevenmacher", "Junglinster", "Manternach", "Mertert",
         "Wormeldange","Bous", "Dalheim", "Lenningen", "Mondorf-les-Bains", "Remich", "Schengen",
         "Stadtbredimus", "Waldbredimus")

nord &lt;- c("Clervaux", "Parc Hosingen", "Troisvierges", "Weiswampach", "Wincrange", "Bettendorf", 
          "Bourscheid", "Diekirch", "Erpeldange-sur-S√ªre", "Ettelbruck", "Feulen", "Mertzig", "Reisdorf", 
          "Schieren", "Vall√©e de l'Ernz", "Beckerich", "Ell", "Grosbous", "Pr√©izerdaul", 
          "Rambrouch", "Redange", "Saeul", "Useldange", "Vichten", "Wahl", "Putscheid", "Tandel",
          "Vianden", "Boulaide", "Esch-sur-S√ªre", "Goesdorf", "Kiischpelt", "Lac de la Haute S√ªre",
          "Wiltz", "Winseler")

sud &lt;- c("Dippach", "Garnich", "K√§erjeng", "Kehlen", "Koerich", "Kopstal", "Mamer", 
         "Habscht", "Steinfort", "Bettembourg", "Differdange", "Dudelange", "Esch-sur-Alzette", 
         "Frisange", "Kayl", "Leudelange", "Mondercange", "P√©tange", "Reckange-sur-Mess", "Roeser",
         "Rumelange", "Sanem", "Schifflange")

circonscriptions &lt;- list("centre" = centre, "est" = est,
                         "nord" = nord, "sud" = sud)</code></pre>
<p>
Now, I can make one map per circonscription. First, let‚Äôs split the data sets by circonscription:
</p>
<pre class="r"><code>communes_df_by_circonscription &lt;- circonscriptions %&gt;%
    map(~filter(communes_df, commune %in% .))

final_data_by_circonscription &lt;- circonscriptions %&gt;%
    map(~filter(final_data, commune %in% .))</code></pre>
<p>
By using <code>pmap()</code>, I can reuse the code to generate the plot to each element of the two lists. This is nice because I do not need to copy and paste the code 4 times:
</p>
<pre class="r"><code>pmap(list(x = communes_df_by_circonscription,
          y = final_data_by_circonscription,
          z = names(communes_df_by_circonscription)),
     function(x, y, z){
         ggplot() +
        geom_polygon(data = x, aes(x = long, y = lat, group = commune), 
                     colour = "grey", fill = NA) +
        geom_scatterpie(data = y, aes(x=long, y=lat, group = commune), 
                        cols = c("ADR", "CSV", "d√©i gr√©ng", "d√©i L√©nk", "DP", "KPL", "LSAP", "PIRATEN")) +
        labs(title = paste0("Share of total vote in each commune, 2018 elections for circonscription ", z)) +
        theme_void() +
        theme(legend.position = "bottom",
              legend.title = element_blank(),
              legend.text = element_text(colour = "white"),
              plot.background = element_rect("#272b30"),
              plot.title = element_text(colour = "white")) + 
        scale_fill_manual(values = c("ADR" = "#009dd1",
                                     "CSV" = "#ee7d00",
                                     "d√©i gr√©ng" = "#45902c",
                                     "d√©i L√©nk" = "#182024",
                                     "DP" = "#002a54",
                                     "KPL" = "#ff0000",
                                     "LSAP" = "#ad3648",
                                     "PIRATEN" = "#ad5ea9"))
     }
)</code></pre>
<pre><code>## $centre</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/lux_elections_analysis-24-1.png" width="768">
</p>
<pre><code>## 
## $est</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/lux_elections_analysis-24-2.png" width="768">
</p>
<pre><code>## 
## $nord</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/lux_elections_analysis-24-3.png" width="768">
</p>
<pre><code>## 
## $sud</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/lux_elections_analysis-24-4.png" width="768">
</p>
<p>
I created an anonymous function of three argument, <code>x</code>, <code>y</code> and <code>z</code>. If you are unfamiliar with <code>pmap()</code>, study the above code closely. If you have questions, do not hesitate to reach out!
</p>
<p>
The pie charts are still quite small, but if I try to change the size of the pie charts, I‚Äôll have the same problem as before: inside the same circonscription, some communes have really a lot of electors, and some a very small number. Perhaps I can try with the log of the electors?
</p>
<pre class="r"><code>pmap(list(x = communes_df_by_circonscription,
          y = final_data_by_circonscription,
          z = names(communes_df_by_circonscription)),
     function(x, y, z){
         ggplot() +
        geom_polygon(data = x, aes(x = long, y = lat, group = commune), 
                     colour = "grey", fill = NA) +
        geom_scatterpie(data = y, aes(x=long, y=lat, group = commune, r = log_electors), 
                        cols = c("ADR", "CSV", "d√©i gr√©ng", "d√©i L√©nk", "DP", "KPL", "LSAP", "PIRATEN")) +
        labs(title = paste0("Share of total vote in each commune, 2018 elections for circonscription ", z)) +
        theme_void() +
        theme(legend.position = "bottom",
              legend.title = element_blank(),
              legend.text = element_text(colour = "white"),
              plot.background = element_rect("#272b30"),
              plot.title = element_text(colour = "white")) + 
        scale_fill_manual(values = c("ADR" = "#009dd1",
                                     "CSV" = "#ee7d00",
                                     "d√©i gr√©ng" = "#45902c",
                                     "d√©i L√©nk" = "#182024",
                                     "DP" = "#002a54",
                                     "KPL" = "#ff0000",
                                     "LSAP" = "#ad3648",
                                     "PIRATEN" = "#ad5ea9"))
     }
)</code></pre>
<pre><code>## $centre</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/lux_elections_analysis-25-1.png" width="768">
</p>
<pre><code>## 
## $est</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/lux_elections_analysis-25-2.png" width="768">
</p>
<pre><code>## 
## $nord</code></pre>
<pre><code>## Warning: Removed 16 rows containing non-finite values (stat_pie).</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/lux_elections_analysis-25-3.png" width="768">
</p>
<pre><code>## 
## $sud</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/lux_elections_analysis-25-4.png" width="768">
</p>
<p>
This looks better now!
</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">
Conclusion
</h2>
<p>
Having data in a machine readable format is really important. The amount of code I had to write to go from the Excel Workbooks that contained the data to this plots is quite large, but if the data was in a machine readable format to start with, I could have focused on the plots immediately.
</p>
<p>
The good thing is that I got to practice my skills and discovered <code>{scatterpie}</code>!
</p>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">
Appendix
</h2>
<p>
The following lines of code extract the data (from the 2013 elections) from the Excel Workbooks that can be found in Luxembourguish <a href="https://data.public.lu/fr/datasets/elections-legislatives-2013-donnees-officieuses/#_">Open Data Portal</a>.
</p>
<p>
I will not comment them, as they work in a similar way than in the previous blog post where I extracted the data from the 2018 elections. The only difference, is that the sheet with the national level data was totally different, so I did not extract it. The first reason is because I don‚Äôt need it for this blog post, the second is because I was lazy. For me, that‚Äôs two pretty good reasons not to do something. If you have a question concerning the code below, don‚Äôt hesitate to reach out though!
</p>
<pre class="r"><code>library("tidyverse")
library("tidyxl")
library("brotools")

path &lt;- Sys.glob("content/blog/2013*xlsx")[-5]

elections_raw_2013 &lt;- map(path, xlsx_cells) %&gt;%
    map(~filter(., sheet != "Sommaire"))

elections_sheets_2013 &lt;- map(map(path, xlsx_sheet_names), ~`%-l%`(., "Sommaire"))

list_targets &lt;- list("Centre" = seq(9, 32),
                    "Est" = seq(9, 18),
                    "Nord" = seq(9, 20),
                    "Sud" = seq(9, 34))

position_parties_national &lt;- seq(1, 24, by = 3)

extract_party &lt;- function(dataset, starting_col, target_rows){
    
    almost_clean &lt;- dataset %&gt;%
        filter(row %in% target_rows) %&gt;%
        filter(col %in% c(starting_col, starting_col + 1)) %&gt;%
        select(character, numeric) %&gt;%
        fill(numeric, .direction = "up") %&gt;%
        filter(!is.na(character))
    
    party_name &lt;- almost_clean$character[1]
    
    almost_clean$character[1] &lt;- "Pourcentage"
    
    almost_clean$party &lt;- party_name
    
    colnames(almost_clean) &lt;- c("Variables", "Values", "Party")
    
    almost_clean %&gt;%
        mutate(Year = 2013) %&gt;%
        select(Party, Year, Variables, Values)
    
}


# Treat one district

extract_district &lt;- function(dataset, sheets, target_rows, position_parties_national){

    list_data_districts &lt;- map(sheets, ~filter(.data = dataset, sheet == .)) 

    elections_districts_2013 &lt;- map(.x = list_data_districts,
                                    ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = target_rows))

    map2(.y = elections_districts_2013, .x = sheets,
         ~mutate(.y, locality = .x, division = "Commune", Year = "2013")) %&gt;%
        bind_rows()
}

elections_2013 &lt;- pmap_dfr(list(x = elections_raw_2013, 
          y = elections_sheets_2013,
          z = list_targets), 
     function(x, y, z){
         map_dfr(position_parties_national, 
             ~extract_district(dataset = x, sheets = y, target_rows = z, position_parties_national = .))
     })

# Correct districts
elections_2013 &lt;- elections_2013 %&gt;%
    mutate(division = case_when(locality == "CENTRE" ~ "Electoral district",
                                locality == "EST" ~ "Electoral district",
                                locality == "NORD" ~ "Electoral district",
                                locality == "SUD" ~ "Electoral district",
                                TRUE ~ division))</code></pre>
</section>


</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-10-27-lux_elections_analysis.html</guid>
  <pubDate>Sat, 27 Oct 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Getting the data from the Luxembourguish elections out of Excel</title>
  <link>https://b-rodrigues.github.io/posts/2018-10-21-lux_elections.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=yjzUxDhuXig"> <img src="https://b-rodrigues.github.io/assets/img/gambia.png"></a>
</p>
</div>
<p>
In this blog post, similar to a <a href="https://www.brodrigues.co/blog/2018-09-11-human_to_machine/">previous blog post</a> I am going to show you how we can go from an Excel workbook that contains data to flat file. I will taking advantage of the structure of the tables inside the Excel sheets by writing a function that extracts the tables and then mapping it to each sheet!
</p>
<p>
Last week, October 14th, Luxembourguish nationals went to the polls to elect the Grand Duke! No, actually, the Grand Duke does not get elected. But Luxembourguish citizen did go to the polls to elect the new members of the Chamber of Deputies (a sort of parliament if you will). The way the elections work in Luxembourg is quite interesting; you can vote for a party, or vote for individual candidates from different parties. The candidates that get the most votes will then seat in the parliament. If you vote for a whole party, each of the candidates get a vote. You get as many votes as there are candidates to vote for. So, for example, if you live in the capital city, also called Luxembourg, you get 21 votes to distribute. You could decide to give 10 votes to 10 candidates of party A and 11 to 11 candidates of party B. Why 21 votes? The chamber of Deputies is made up 60 deputies, and the country is divided into four legislative circonscriptions. So each voter in a circonscription gets an amount of votes that is proportional to the population size of that circonscription.
</p>
<p>
Now you certainly wonder why I put the flag of Gambia on top of this post? This is because the government that was formed after the 2013 elections was made up of a coalition of 3 parties; the Luxembourg Socialist Worker‚Äôs Party, the Democratic Party and The Greens. The LSAP managed to get 13 seats in the Chamber, while the DP got 13 and The Greens 6, meaning 32 seats out of 60. So because they made this coalition, they could form the government, and this coalition was named the Gambia coalition because of the colors of these 3 parties: red, blue and green. If you want to take a look at the ballot from 2013 for the southern circonscription, click <a href="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Specimen_Elections_legislatives_Luxembourg_2013.png/1280px-Specimen_Elections_legislatives_Luxembourg_2013.png">here</a>.
</p>
<p>
Now that you have the context, we can go back to some data science. The results of the elections of last week can be found on Luxembourg‚Äôs Open Data portal, right <a href="https://data.public.lu/fr/datasets/elections-legislatives-du-14-octobre-2018-donnees-officieuses/">here</a>. The data is trapped inside Excel sheets; just like I explained in a <a href="https://www.brodrigues.co/blog/2018-09-11-human_to_machine/">previous blog post</a> the data is easily read by human, but not easily digested by any type of data analysis software. So I am going to show you how we are going from this big Excel workbook to a flat file.
</p>
<p>
First of all, if you open the Excel workbook, you will notice that there are a lot of sheets; there is one for the whole country, named ‚ÄúLe Grand-Duch√© de Luxembourg‚Äù, one for the four circonscriptions, ‚ÄúCentre‚Äù, ‚ÄúNord‚Äù, ‚ÄúSud‚Äù, ‚ÄúEst‚Äù and 102 more for each <strong>commune</strong> of the country (a commune is an administrative division). However, the tables are all very similarly shaped, and roughly at the same position.
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/elections_data.png"><!-- -->
</p>
<p>
This is good, because we can write a function to extracts the data and then map it over all the sheets. First, let‚Äôs load some packages and the data for the country:
</p>
<pre class="r"><code>library("tidyverse")
library("tidyxl")
library("brotools")</code></pre>
<pre class="r"><code># National Level 2018
elections_raw_2018 &lt;- xlsx_cells("leg-2018-10-14-22-58-09-737.xlsx",
                        sheets = "Le Grand-Duch√© de Luxembourg")</code></pre>
<p>
<code>{brotools}</code> is my own package. You can install it with:
</p>
<pre class="r"><code>devtools::install_github("b-rodrigues/brotools")</code></pre>
<p>
it contains a function that I will use down below. The function I wrote to extract the tables is not very complex, but requires that you are familiar with how <code>{tidyxl}</code> imports Excel workbooks. So if you are not familiar with it, study the imported data frame for a few minutes. It will make understanding the next function easier:
</p>
<pre class="r"><code>extract_party &lt;- function(dataset, starting_col, target_rows){

    almost_clean &lt;- dataset %&gt;%
        filter(row %in% target_rows) %&gt;%
        filter(col %in% c(starting_col, starting_col + 1)) %&gt;%
        select(character, numeric) %&gt;%
        fill(numeric, .direction = "up") %&gt;%
        filter(!is.na(character))

    party_name &lt;- almost_clean$character[1] %&gt;%
        str_split("-", simplify = TRUE) %&gt;%
        .[2] %&gt;%
        str_trim()

    almost_clean$character[1] &lt;- "Pourcentage"

    almost_clean$party &lt;- party_name

    colnames(almost_clean) &lt;- c("Variables", "Values", "Party")

    almost_clean %&gt;%
        mutate(Year = 2018) %&gt;%
        select(Party, Year, Variables, Values)

}</code></pre>
<p>
This function has three arguments, <code>dataset</code>, <code>starting_col</code> and <code>target_rows</code>. <code>dataset</code> is the data I loaded with <code>xlsx_cells</code> from the <code>{tidyxl}</code> package. I think the following picture illustrates easily what the function does:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/elections_logic.png"><!-- -->
</p>
<p>
So the function first filters only the rows we are interested in, then the cols. I then select the columns I want which are called <code>character</code> and <code>numeric</code> (if the Excel cell contains characters then you will find them in the character column, if it contains numbers you will them in the numeric column), then I fill the empty cells with the values from the <code>numeric</code> column and the I remove the NA‚Äôs. These two last steps might not be so clear; this is how the data looks like up until the <code>select()</code> function:
</p>
<pre class="r"><code>&gt; elections_raw_2018 %&gt;%
+     filter(row %in% seq(11,19)) %&gt;%
+     filter(col %in% c(1, 2)) %&gt;%
+     select(character, numeric)
# A tibble: 18 x 2
   character                       numeric
   &lt;chr&gt;                             &lt;dbl&gt;
 1 1 - PIRATEN - PIRATEN           NA     
 2 NA                               0.0645
 3 Suffrage total                  NA     
 4 NA                          227549     
 5 Suffrages de liste              NA     
 6 NA                          181560     
 7 Suffrage nominatifs             NA     
 8 NA                           45989     
 9 Pourcentage pond√©r√©             NA     
10 NA                               0.0661
11 Suffrage total pond√©r√©          NA     
12 NA                           13394.    
13 Suffrages de liste pond√©r√©      NA     
14 NA                           10308     
15 Suffrage nominatifs pond√©r√©     NA     
16 NA                            3086.    
17 Mandats attribu√©s               NA     
18 NA                               2  </code></pre>
<p>
So by filling the NA‚Äôs in the numeric the data now looks like this:
</p>
<pre class="r"><code>&gt; elections_raw_2018 %&gt;%
+     filter(row %in% seq(11,19)) %&gt;%
+     filter(col %in% c(1, 2)) %&gt;%
+     select(character, numeric) %&gt;%
+     fill(numeric, .direction = "up")
# A tibble: 18 x 2
   character                       numeric
   &lt;chr&gt;                             &lt;dbl&gt;
 1 1 - PIRATEN - PIRATEN            0.0645
 2 NA                               0.0645
 3 Suffrage total              227549     
 4 NA                          227549     
 5 Suffrages de liste          181560     
 6 NA                          181560     
 7 Suffrage nominatifs          45989     
 8 NA                           45989     
 9 Pourcentage pond√©r√©              0.0661
10 NA                               0.0661
11 Suffrage total pond√©r√©       13394.    
12 NA                           13394.    
13 Suffrages de liste pond√©r√©   10308     
14 NA                           10308     
15 Suffrage nominatifs pond√©r√©   3086.    
16 NA                            3086.    
17 Mandats attribu√©s                2     
18 NA                               2 </code></pre>
<p>
And then I filter out the NA‚Äôs from the character column, and that‚Äôs almost it! I simply need to add a new column with the party‚Äôs name and rename the other columns. I also add a ‚ÄúYear‚Äù colmun.
</p>
<p>
Now, each party will have a different starting column. The table with the data for the first party starts on column 1, for the second party it starts on column 4, column 7 for the third party‚Ä¶ So the following vector contains all the starting columns:
</p>
<pre class="r"><code>position_parties_national &lt;- seq(1, 24, by = 3)</code></pre>
<p>
(If you study the Excel workbook closely, you will notice that I do not extract the last two parties. This is because these parties were not present in all of the 4 circonscriptions and are very, very, very small.)
</p>
<p>
The target rows are always the same, from 11 to 19. Now, I simply need to map this function to this list of positions and I get the data for all the parties:
</p>
<pre class="r"><code>elections_national_2018 &lt;- map_df(position_parties_national, extract_party, 
                         dataset = elections_raw_2018, target_rows = seq(11, 19)) %&gt;%
    mutate(locality = "Grand-Duchy of Luxembourg", division = "National")</code></pre>
<p>
I also added the <code>locality</code> and <code>division</code> columns to the data.
</p>
<p>
Let‚Äôs take a look:
</p>
<pre class="r"><code>glimpse(elections_national_2018)</code></pre>
<pre><code>## Observations: 72
## Variables: 6
## $ Party     &lt;chr&gt; "PIRATEN", "PIRATEN", "PIRATEN", "PIRATEN", "PIRATEN",‚Ä¶
## $ Year      &lt;dbl&gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, ‚Ä¶
## $ Variables &lt;chr&gt; "Pourcentage", "Suffrage total", "Suffrages de liste",‚Ä¶
## $ Values    &lt;dbl&gt; 6.446204e-02, 2.275490e+05, 1.815600e+05, 4.598900e+04‚Ä¶
## $ locality  &lt;chr&gt; "Grand-Duchy of Luxembourg", "Grand-Duchy of Luxembour‚Ä¶
## $ division  &lt;chr&gt; "National", "National", "National", "National", "Natio‚Ä¶</code></pre>
<p>
Very nice.
</p>
<p>
Now we need to do the same for the 4 electoral circonscriptions. First, let‚Äôs load the data:
</p>
<pre class="r"><code># Electoral districts 2018
districts &lt;- c("Centre", "Nord", "Sud", "Est")

elections_district_raw_2018 &lt;- xlsx_cells("leg-2018-10-14-22-58-09-737.xlsx",
                                      sheets = districts)</code></pre>
<p>
Now things get trickier. Remember I said that the number of seats is proportional to the population of each circonscription? We simply can‚Äôt use the same target rows as before. For example, for the ‚ÄúCentre‚Äù circonscription, the target rows go from 12 to 37, but for the ‚ÄúEst‚Äù circonscription only from 12 to 23. Ideally, we would need a function that would return the target rows.
</p>
<p>
This is that function:
</p>
<pre class="r"><code># The target rows I need to extract are different from district to district
get_target_rows &lt;- function(dataset, sheet_to_extract, reference_address){

    last_row &lt;- dataset %&gt;%
        filter(sheet == sheet_to_extract) %&gt;%
        filter(address == reference_address) %&gt;%
        pull(numeric)

    seq(12, (11 + 5 + last_row))
}</code></pre>
<p>
This function needs a <code>dataset</code>, a <code>sheet_to_extract</code> and a <code>reference_address</code>. The reference address is a cell that actually contains the number of seats in that circonscription, in our case ‚ÄúB5‚Äù. We can easily get the list of target rows now:
</p>
<pre class="r"><code># Get the target rows
list_targets &lt;- map(districts, get_target_rows, dataset = elections_district_raw_2018, 
                    reference_address = "B5")

list_targets</code></pre>
<pre><code>## [[1]]
##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34
## [24] 35 36 37
## 
## [[2]]
##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25
## 
## [[3]]
##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34
## [24] 35 36 37 38 39
## 
## [[4]]
##  [1] 12 13 14 15 16 17 18 19 20 21 22 23</code></pre>
<p>
Now, let‚Äôs split the data we imported into a list, where each element of the list is a dataframe with the data from one circonscription:
</p>
<pre class="r"><code>list_data_districts &lt;- map(districts, ~filter(.data = elections_district_raw_2018, sheet == .)) </code></pre>
<p>
Now I can easily map the function I defined above, <code>extract_party</code> to this list of datasets. Well, I say easily, but it‚Äôs a bit more complicated than before because I have now a list of datasets and a list of target rows:
</p>
<pre class="r"><code>elections_district_2018 &lt;- map2(.x = list_data_districts, .y = list_targets,
     ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = .y))</code></pre>
<p>
The way to understand this is that for each element of <code>list_data_districts</code> and <code>list_targets</code>, I have to map <code>extract_party</code> to each element of <code>position_parties_national</code>. This gives the intented result:
</p>
<pre class="r"><code>elections_district_2018</code></pre>
<pre><code>## [[1]]
## # A tibble: 208 x 4
##    Party    Year Variables               Values
##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;
##  1 PIRATEN  2018 Pourcentage             0.0514
##  2 PIRATEN  2018 CLEMENT Sven (1)     8007     
##  3 PIRATEN  2018 WEYER Jerry (2)      3446     
##  4 PIRATEN  2018 CLEMENT Pascal (3)   3418     
##  5 PIRATEN  2018 KUNAKOVA Lucie (4)   2860     
##  6 PIRATEN  2018 WAMPACH Jo (14)      2693     
##  7 PIRATEN  2018 LAUX Cynthia (6)     2622     
##  8 PIRATEN  2018 ISEKIN Christian (5) 2610     
##  9 PIRATEN  2018 SCHWEICH Georges (9) 2602     
## 10 PIRATEN  2018 LIESCH Mireille (8)  2551     
## # ‚Ä¶ with 198 more rows
## 
## [[2]]
## # A tibble: 112 x 4
##    Party    Year Variables                             Values
##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                                  &lt;dbl&gt;
##  1 PIRATEN  2018 Pourcentage                           0.0767
##  2 PIRATEN  2018 COLOMBERA Jean (2)                 5074     
##  3 PIRATEN  2018 ALLARD Ben (1)                     4225     
##  4 PIRATEN  2018 MAAR Andy (3)                      2764     
##  5 PIRATEN  2018 GINTER Joshua (8)                  2536     
##  6 PIRATEN  2018 DASBACH Angelika (4)               2473     
##  7 PIRATEN  2018 GR√úNEISEN Sam (6)                  2408     
##  8 PIRATEN  2018 BAUMANN Roy (5)                    2387     
##  9 PIRATEN  2018 CONRAD Pierre (7)                  2280     
## 10 PIRATEN  2018 TRAUT √©p. MOLITOR Angela Maria (9) 2274     
## # ‚Ä¶ with 102 more rows
## 
## [[3]]
## # A tibble: 224 x 4
##    Party    Year Variables                    Values
##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt;
##  1 PIRATEN  2018 Pourcentage                  0.0699
##  2 PIRATEN  2018 GOERGEN Marc (1)          9818     
##  3 PIRATEN  2018 FLOR Starsky (2)          6737     
##  4 PIRATEN  2018 KOHL Martine (3)          6071     
##  5 PIRATEN  2018 LIESCH Camille (4)        6025     
##  6 PIRATEN  2018 KOHL Sylvie (6)           5628     
##  7 PIRATEN  2018 WELTER Christian (5)      5619     
##  8 PIRATEN  2018 DA GRA√áA DIAS Yanick (10) 5307     
##  9 PIRATEN  2018 WEBER Jules (7)           5301     
## 10 PIRATEN  2018 CHMELIK Libor (8)         5247     
## # ‚Ä¶ with 214 more rows
## 
## [[4]]
## # A tibble: 96 x 4
##    Party    Year Variables                           Values
##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                                &lt;dbl&gt;
##  1 PIRATEN  2018 Pourcentage                         0.0698
##  2 PIRATEN  2018 FR√àRES Daniel (1)                4152     
##  3 PIRATEN  2018 CLEMENT Jill (7)                 1943     
##  4 PIRATEN  2018 HOUDREMONT Claire (2)            1844     
##  5 PIRATEN  2018 B√ñRGER Nancy (3)                 1739     
##  6 PIRATEN  2018 MARTINS DOS SANTOS Catarina (6)  1710     
##  7 PIRATEN  2018 BELLEVILLE Tatjana (4)           1687     
##  8 PIRATEN  2018 CONTRERAS Gerald (5)             1687     
##  9 PIRATEN  2018 Suffrages total                 14762     
## 10 PIRATEN  2018 Suffrages de liste              10248     
## # ‚Ä¶ with 86 more rows</code></pre>
<p>
I now need to add the <code>locality</code> and <code>division</code> columns:
</p>
<pre class="r"><code>elections_district_2018 &lt;- map2(.y = elections_district_2018, .x = districts, 
     ~mutate(.y, locality = .x, division = "Electoral district")) %&gt;%
    bind_rows()</code></pre>
<p>
We‚Äôre almost done! Now we need to do the same for the 102 remaining sheets, one for each <strong>commune</strong> of Luxembourg. This will now go very fast, because we got all the building blocks from before:
</p>
<pre class="r"><code>communes &lt;- xlsx_sheet_names("leg-2018-10-14-22-58-09-737.xlsx")

communes &lt;- communes %-l% 
    c("Le Grand-Duch√© de Luxembourg", "Centre", "Est", "Nord", "Sud", "Sommaire")</code></pre>
<p>
Let me introduce the following function: <code>%-l%</code>. This function removes elements from lists:
</p>
<pre class="r"><code>c("a", "b", "c", "d") %-l% c("a", "d")</code></pre>
<pre><code>## [1] "b" "c"</code></pre>
<p>
You can think of it as ‚Äúminus for lists‚Äù. This is called an infix operator.
</p>
<p>
So this function is very useful to get the list of communes, and is part of my package, <code>{brotools}</code>.
</p>
<p>
As before, I load the data:
</p>
<pre class="r"><code>elections_communes_raw_2018 &lt;- xlsx_cells("leg-2018-10-14-22-58-09-737.xlsx",
                                 sheets = communes)</code></pre>
<p>
Then get my list of targets, but I need to change the reference address. It‚Äôs ‚ÄúB8‚Äù now, not ‚ÄúB7‚Äù.
</p>
<pre class="r"><code># Get the target rows
list_targets &lt;- map(communes, get_target_rows, 
                    dataset = elections_communes_raw_2018, reference_address = "B8")</code></pre>
<p>
I now create a list of communes by mapping a filter function to the data:
</p>
<pre class="r"><code>list_data_communes &lt;- map(communes, ~filter(.data = elections_communes_raw_2018, sheet == .)) </code></pre>
<p>
And just as before, I get the data I need by using <code>extract_party</code>, and adding the ‚Äúlocality‚Äù and ‚Äúdivision‚Äù columns:
</p>
<pre class="r"><code>elections_communes_2018 &lt;- map2(.x = list_data_communes, .y = list_targets,
                                ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = .y))

elections_communes_2018 &lt;- map2(.y = elections_communes_2018, .x = communes,
                                ~mutate(.y, locality = .x, division = "Commune")) %&gt;%
    bind_rows()</code></pre>
<p>
The steps are so similar for the four circonscriptions and for the 102 <strong>communes</strong> that I could have write a big wrapper function and the use it for the circonscription and <strong>communes</strong> at once. But I was lazy.
</p>
<p>
Finally, I bind everything together and have a nice, tidy, flat file:
</p>
<pre class="r"><code># Final results

elections_2018 &lt;- bind_rows(list(elections_national_2018, elections_district_2018, elections_communes_2018))

glimpse(elections_2018)</code></pre>
<pre><code>## Observations: 15,544
## Variables: 6
## $ Party     &lt;chr&gt; "PIRATEN", "PIRATEN", "PIRATEN", "PIRATEN", "PIRATEN",‚Ä¶
## $ Year      &lt;dbl&gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, ‚Ä¶
## $ Variables &lt;chr&gt; "Pourcentage", "Suffrage total", "Suffrages de liste",‚Ä¶
## $ Values    &lt;dbl&gt; 6.446204e-02, 2.275490e+05, 1.815600e+05, 4.598900e+04‚Ä¶
## $ locality  &lt;chr&gt; "Grand-Duchy of Luxembourg", "Grand-Duchy of Luxembour‚Ä¶
## $ division  &lt;chr&gt; "National", "National", "National", "National", "Natio‚Ä¶</code></pre>
<p>
This blog post is already quite long, so I&nbsp;will analyze the data now that R can easily ingest it in a future blog post.
</p>



 ]]></description>
  <category>R</category>
  <category>programming</category>
  <guid>https://b-rodrigues.github.io/posts/2018-10-21-lux_elections.html</guid>
  <pubDate>Sun, 21 Oct 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Exporting editable plots from R to Powerpoint: making ggplot2 purrr with officer</title>
  <link>https://b-rodrigues.github.io/posts/2018-10-05-ggplot2_purrr_officer.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=oc9XOxUcvLY"> <img src="https://b-rodrigues.github.io/assets/img/officer_meme.jpg"></a>
</p>
</div>
<p>
A kind reader let me know that the function <code>create_pptx()</code> is now outdated, and proposed an update which you can find here: <a href="https://gist.github.com/b-rodrigues/ef4e97ed75028ca1ddd5987bb4085c1c">here</a>. Thank you <a href="https://twitter.com/jerry_stones/status/1239625489578254336"><span class="citation" data-cites="Jeremy">@Jeremy</span></a>!
</p>
<p>
I was recently confronted to the following problem: creating hundreds of plots that could still be edited by our client. What this meant was that I needed to export the graphs in Excel or Powerpoint or some other such tool that was familiar to the client, and not export the plots directly to pdf or png as I would normally do. I still wanted to use R to do it though, because I could do what I always do to when I need to perform repetitive tasks such as producing hundreds of plots; map over a list of, say, countries, and make one plot per country. This is something I discussed in a previous blog post, <a href="http://www.brodrigues.co/blog/2017-03-29-make-ggplot2-purrr/">Make ggplot2 purrr</a>.
</p>
<p>
So, after some online seaching, I found the <code>{officer}</code> package. This package allows you to put objects into Microsoft documents. For example, editable plots in a Powerpoint document. This is what I will show in this blog post.
</p>
<p>
Let‚Äôs start by loading the required packages:
</p>
<pre class="r"><code>library("tidyverse")
library("officer")
library("rvg")</code></pre>
<p>
Then, I will use the data from the time use survey, which I discussed in a previous blog post <a href="http://www.brodrigues.co/blog/2018-09-11-human_to_machine/">Going from a human readable Excel file to a machine-readable csv with {tidyxl}</a>.
</p>
<p>
You can download the data <a href="https://github.com/rbind/b-rodrigues.github.com/blob/master/content/blog/clean_data.csv">here</a>.
</p>
<p>
Let‚Äôs import and prepare it:
</p>
<pre class="r"><code>time_use &lt;- rio::import("clean_data.csv")


time_use &lt;- time_use %&gt;%
    filter(population %in% c("Male", "Female")) %&gt;%
    filter(activities %in% c("Personal care", "Sleep", "Eating", 
                             "Employment", "Household and family care")) %&gt;%
    group_by(day) %&gt;%
    nest()</code></pre>
<p>
I only kept two categories, ‚ÄúMale‚Äù and ‚ÄúFemale‚Äù and 5 activities. Then I grouped by day and nested the data. This is how it looks like:
</p>
<pre class="r"><code>time_use</code></pre>
<pre><code>## # A tibble: 3 x 2
##   day                         data             
##   &lt;chr&gt;                       &lt;list&gt;           
## 1 Year 2014_Monday til Friday &lt;tibble [10 √ó 4]&gt;
## 2 Year 2014_Saturday          &lt;tibble [10 √ó 4]&gt;
## 3 Year 2014_Sunday            &lt;tibble [10 √ó 4]&gt;</code></pre>
<p>
As shown, <code>time_use</code> is a tibble with 2 columns, the first <code>day</code> contains the days, and the second <code>data</code>, is of type list, and each element of these lists are tibbles themselves. Let‚Äôs take a look inside one:
</p>
<pre class="r"><code>time_use$data[1]</code></pre>
<pre><code>## [[1]]
## # A tibble: 10 x 4
##    population activities                time  time_in_minutes
##    &lt;chr&gt;      &lt;chr&gt;                     &lt;chr&gt;           &lt;int&gt;
##  1 Male       Personal care             11:00             660
##  2 Male       Sleep                     08:24             504
##  3 Male       Eating                    01:46             106
##  4 Male       Employment                08:11             491
##  5 Male       Household and family care 01:59             119
##  6 Female     Personal care             11:15             675
##  7 Female     Sleep                     08:27             507
##  8 Female     Eating                    01:48             108
##  9 Female     Employment                06:54             414
## 10 Female     Household and family care 03:49             229</code></pre>
<p>
I can now create plots for each of the days with the following code:
</p>
<pre class="r"><code>my_plots &lt;- time_use %&gt;%
    mutate(plots = map2(.y = day, .x = data, ~ggplot(data = .x) + theme_minimal() +
                       geom_col(aes(y = time_in_minutes, x = activities, fill = population), 
                                position = "dodge") +
                       ggtitle(.y) +
                       ylab("Time in minutes") +
                       xlab("Activities")))</code></pre>
<p>
These steps are all detailled in my blog post <a href="http://www.brodrigues.co/blog/2017-03-29-make-ggplot2-purrr/">Make ggplot2 purrr</a>. Let‚Äôs take a look at <code>my_plots</code>:
</p>
<pre class="r"><code>my_plots</code></pre>
<pre><code>## # A tibble: 3 x 3
##   day                         data              plots 
##   &lt;chr&gt;                       &lt;list&gt;            &lt;list&gt;
## 1 Year 2014_Monday til Friday &lt;tibble [10 √ó 4]&gt; &lt;gg&gt;  
## 2 Year 2014_Saturday          &lt;tibble [10 √ó 4]&gt; &lt;gg&gt;  
## 3 Year 2014_Sunday            &lt;tibble [10 √ó 4]&gt; &lt;gg&gt;</code></pre>
<p>
The last column, called <code>plots</code> is a list where each element is a plot! We can take a look at one:
</p>
<pre class="r"><code>my_plots$plots[1]</code></pre>
<pre><code>## [[1]]</code></pre>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/ggplot2_purrr_officer.png" class="img-fluid"></p>
</div>
<p>
Now, this is where I could export these plots as pdfs or pngs. But this is not what I need. I need to export these plots as editable charts for Powerpoint. To do this for one image, I would do the following (as per <code>{officer}</code>‚Äôs documentation):
</p>
<pre class="r"><code>read_pptx() %&gt;%
    add_slide(layout = "Title and Content", master = "Office Theme") %&gt;%
    ph_with_vg(code = print(one_plot), type = "body") %&gt;% 
    print(target = path)</code></pre>
<p>
To map this over a list of arguments, I wrote a wrapper:
</p>
<pre class="r"><code>create_pptx &lt;- function(plot, path){
    if(!file.exists(path)) {
        out &lt;- read_pptx()
    } else {
        out &lt;- read_pptx(path)
    }
    
    out %&gt;%
        add_slide(layout = "Title and Content", master = "Office Theme") %&gt;%
        ph_with_vg(code = print(plot), type = "body") %&gt;% 
        print(target = path)
}</code></pre>
<p>
This function takes two arguments, <code>plot</code> and <code>path</code>. <code>plot</code> must be an plot object such as the ones contained inside the <code>plots</code> column of <code>my_plots</code> tibble. <code>path</code> is the path of where I want to save the pptx.
</p>
<p>
The first lines check if the file exists, if yes, the slides get added to the existing file, if not a new pptx gets created. The rest of the code is very similar to the one from the documentation. Now, to create my pptx I simple need to map over the <code>plots</code> column and provide a <code>path</code>:
</p>
<pre class="r"><code>map(my_plots$plots, create_pptx, path = "test.pptx")</code></pre>
<pre><code>## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]</code></pre>
<pre><code>## [[1]]
## [1] "/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx"
## 
## [[2]]
## [1] "/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx"
## 
## [[3]]
## [1] "/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx"</code></pre>
<p>
Here is the end result:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/editable_plots.png"><!-- -->
</p>
<p>
Inside Powerpoint (or in this case Libreoffice), the plots are geometric shapes that can now be edited!
</p>



 ]]></description>
  <category>R</category>
  <category>programming</category>
  <guid>https://b-rodrigues.github.io/posts/2018-10-05-ggplot2_purrr_officer.html</guid>
  <pubDate>Fri, 05 Oct 2018 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
