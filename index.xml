<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Econometrics and Free Software</title>
<link>https://b-rodrigues.github.io/</link>
<atom:link href="https://b-rodrigues.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.6.37</generator>
<lastBuildDate>Thu, 09 Jan 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>github pages setup for this website</title>
  <link>https://b-rodrigues.github.io/posts/2025-01-09-github_pages_setup_with_quarto.html</link>
  <description><![CDATA[ 




<section id="desired-setup" class="level1">
<h1>Desired setup</h1>
<p>Setting up a Quarto website on github pages is fairly straightforward: <a href="https://quarto.org/docs/publishing/github-pages.html">you just need to follow the docs</a>! But in case you need some help, I’m sharing here all the steps I’ve went through.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span></code></pre></div>
</div>


</section>

 ]]></description>
  <category>quarto</category>
  <category>github pages</category>
  <guid>https://b-rodrigues.github.io/posts/2025-01-09-github_pages_setup_with_quarto.html</guid>
  <pubDate>Thu, 09 Jan 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>huhu</title>
  <link>https://b-rodrigues.github.io/posts/2024-12-09-huhu.html</link>
  <description><![CDATA[ 




<section id="undesired-setup" class="level1">
<h1>Undesired setup</h1>
<p>Setting up a Quarto website on github pages is fairly straightforward: <a href="https://quarto.org/docs/publishing/github-pages.html">you just need to follow the docs</a>! But in case you need some help, I’m sharing here all the steps I’ve went through.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span></code></pre></div>
</div>


</section>

 ]]></description>
  <category>quarto</category>
  <category>github pages</category>
  <guid>https://b-rodrigues.github.io/posts/2024-12-09-huhu.html</guid>
  <pubDate>Mon, 09 Dec 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>November blog post</title>
  <link>https://b-rodrigues.github.io/posts/2024-11-09-haha.html</link>
  <description><![CDATA[ 




<section id="undesired-setup" class="level1">
<h1>Undesired setup</h1>
<p>Setting up a Quarto website on github pages is fairly straightforward: <a href="https://quarto.org/docs/publishing/github-pages.html">you just need to follow the docs</a>! But in case you need some help, I’m sharing here all the steps I’ve went through.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span></code></pre></div>
</div>


</section>

 ]]></description>
  <category>quarto</category>
  <category>github pages</category>
  <guid>https://b-rodrigues.github.io/posts/2024-11-09-haha.html</guid>
  <pubDate>Sat, 09 Nov 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Getting the data from the Luxembourguish elections out of Excel</title>
  <link>https://b-rodrigues.github.io/posts/2018-10-21-lux_elections.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=yjzUxDhuXig"> <img src="https://b-rodrigues.github.io/assets/img/gambia.png"></a>
</p>
</div>
<p>
In this blog post, similar to a <a href="https://www.brodrigues.co/blog/2018-09-11-human_to_machine/">previous blog post</a> I am going to show you how we can go from an Excel workbook that contains data to flat file. I will taking advantage of the structure of the tables inside the Excel sheets by writing a function that extracts the tables and then mapping it to each sheet!
</p>
<p>
Last week, October 14th, Luxembourguish nationals went to the polls to elect the Grand Duke! No, actually, the Grand Duke does not get elected. But Luxembourguish citizen did go to the polls to elect the new members of the Chamber of Deputies (a sort of parliament if you will). The way the elections work in Luxembourg is quite interesting; you can vote for a party, or vote for individual candidates from different parties. The candidates that get the most votes will then seat in the parliament. If you vote for a whole party, each of the candidates get a vote. You get as many votes as there are candidates to vote for. So, for example, if you live in the capital city, also called Luxembourg, you get 21 votes to distribute. You could decide to give 10 votes to 10 candidates of party A and 11 to 11 candidates of party B. Why 21 votes? The chamber of Deputies is made up 60 deputies, and the country is divided into four legislative circonscriptions. So each voter in a circonscription gets an amount of votes that is proportional to the population size of that circonscription.
</p>
<p>
Now you certainly wonder why I put the flag of Gambia on top of this post? This is because the government that was formed after the 2013 elections was made up of a coalition of 3 parties; the Luxembourg Socialist Worker’s Party, the Democratic Party and The Greens. The LSAP managed to get 13 seats in the Chamber, while the DP got 13 and The Greens 6, meaning 32 seats out of 60. So because they made this coalition, they could form the government, and this coalition was named the Gambia coalition because of the colors of these 3 parties: red, blue and green. If you want to take a look at the ballot from 2013 for the southern circonscription, click <a href="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Specimen_Elections_legislatives_Luxembourg_2013.png/1280px-Specimen_Elections_legislatives_Luxembourg_2013.png">here</a>.
</p>
<p>
Now that you have the context, we can go back to some data science. The results of the elections of last week can be found on Luxembourg’s Open Data portal, right <a href="https://data.public.lu/fr/datasets/elections-legislatives-du-14-octobre-2018-donnees-officieuses/">here</a>. The data is trapped inside Excel sheets; just like I explained in a <a href="https://www.brodrigues.co/blog/2018-09-11-human_to_machine/">previous blog post</a> the data is easily read by human, but not easily digested by any type of data analysis software. So I am going to show you how we are going from this big Excel workbook to a flat file.
</p>
<p>
First of all, if you open the Excel workbook, you will notice that there are a lot of sheets; there is one for the whole country, named “Le Grand-Duché de Luxembourg”, one for the four circonscriptions, “Centre”, “Nord”, “Sud”, “Est” and 102 more for each <strong>commune</strong> of the country (a commune is an administrative division). However, the tables are all very similarly shaped, and roughly at the same position.
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/elections_data.png"><!-- -->
</p>
<p>
This is good, because we can write a function to extracts the data and then map it over all the sheets. First, let’s load some packages and the data for the country:
</p>
<pre class="r"><code>library("tidyverse")
library("tidyxl")
library("brotools")</code></pre>
<pre class="r"><code># National Level 2018
elections_raw_2018 &lt;- xlsx_cells("leg-2018-10-14-22-58-09-737.xlsx",
                        sheets = "Le Grand-Duché de Luxembourg")</code></pre>
<p>
<code>{brotools}</code> is my own package. You can install it with:
</p>
<pre class="r"><code>devtools::install_github("b-rodrigues/brotools")</code></pre>
<p>
it contains a function that I will use down below. The function I wrote to extract the tables is not very complex, but requires that you are familiar with how <code>{tidyxl}</code> imports Excel workbooks. So if you are not familiar with it, study the imported data frame for a few minutes. It will make understanding the next function easier:
</p>
<pre class="r"><code>extract_party &lt;- function(dataset, starting_col, target_rows){

    almost_clean &lt;- dataset %&gt;%
        filter(row %in% target_rows) %&gt;%
        filter(col %in% c(starting_col, starting_col + 1)) %&gt;%
        select(character, numeric) %&gt;%
        fill(numeric, .direction = "up") %&gt;%
        filter(!is.na(character))

    party_name &lt;- almost_clean$character[1] %&gt;%
        str_split("-", simplify = TRUE) %&gt;%
        .[2] %&gt;%
        str_trim()

    almost_clean$character[1] &lt;- "Pourcentage"

    almost_clean$party &lt;- party_name

    colnames(almost_clean) &lt;- c("Variables", "Values", "Party")

    almost_clean %&gt;%
        mutate(Year = 2018) %&gt;%
        select(Party, Year, Variables, Values)

}</code></pre>
<p>
This function has three arguments, <code>dataset</code>, <code>starting_col</code> and <code>target_rows</code>. <code>dataset</code> is the data I loaded with <code>xlsx_cells</code> from the <code>{tidyxl}</code> package. I think the following picture illustrates easily what the function does:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/elections_logic.png"><!-- -->
</p>
<p>
So the function first filters only the rows we are interested in, then the cols. I then select the columns I want which are called <code>character</code> and <code>numeric</code> (if the Excel cell contains characters then you will find them in the character column, if it contains numbers you will them in the numeric column), then I fill the empty cells with the values from the <code>numeric</code> column and the I remove the NA’s. These two last steps might not be so clear; this is how the data looks like up until the <code>select()</code> function:
</p>
<pre class="r"><code>&gt; elections_raw_2018 %&gt;%
+     filter(row %in% seq(11,19)) %&gt;%
+     filter(col %in% c(1, 2)) %&gt;%
+     select(character, numeric)
# A tibble: 18 x 2
   character                       numeric
   &lt;chr&gt;                             &lt;dbl&gt;
 1 1 - PIRATEN - PIRATEN           NA     
 2 NA                               0.0645
 3 Suffrage total                  NA     
 4 NA                          227549     
 5 Suffrages de liste              NA     
 6 NA                          181560     
 7 Suffrage nominatifs             NA     
 8 NA                           45989     
 9 Pourcentage pondéré             NA     
10 NA                               0.0661
11 Suffrage total pondéré          NA     
12 NA                           13394.    
13 Suffrages de liste pondéré      NA     
14 NA                           10308     
15 Suffrage nominatifs pondéré     NA     
16 NA                            3086.    
17 Mandats attribués               NA     
18 NA                               2  </code></pre>
<p>
So by filling the NA’s in the numeric the data now looks like this:
</p>
<pre class="r"><code>&gt; elections_raw_2018 %&gt;%
+     filter(row %in% seq(11,19)) %&gt;%
+     filter(col %in% c(1, 2)) %&gt;%
+     select(character, numeric) %&gt;%
+     fill(numeric, .direction = "up")
# A tibble: 18 x 2
   character                       numeric
   &lt;chr&gt;                             &lt;dbl&gt;
 1 1 - PIRATEN - PIRATEN            0.0645
 2 NA                               0.0645
 3 Suffrage total              227549     
 4 NA                          227549     
 5 Suffrages de liste          181560     
 6 NA                          181560     
 7 Suffrage nominatifs          45989     
 8 NA                           45989     
 9 Pourcentage pondéré              0.0661
10 NA                               0.0661
11 Suffrage total pondéré       13394.    
12 NA                           13394.    
13 Suffrages de liste pondéré   10308     
14 NA                           10308     
15 Suffrage nominatifs pondéré   3086.    
16 NA                            3086.    
17 Mandats attribués                2     
18 NA                               2 </code></pre>
<p>
And then I filter out the NA’s from the character column, and that’s almost it! I simply need to add a new column with the party’s name and rename the other columns. I also add a “Year” colmun.
</p>
<p>
Now, each party will have a different starting column. The table with the data for the first party starts on column 1, for the second party it starts on column 4, column 7 for the third party… So the following vector contains all the starting columns:
</p>
<pre class="r"><code>position_parties_national &lt;- seq(1, 24, by = 3)</code></pre>
<p>
(If you study the Excel workbook closely, you will notice that I do not extract the last two parties. This is because these parties were not present in all of the 4 circonscriptions and are very, very, very small.)
</p>
<p>
The target rows are always the same, from 11 to 19. Now, I simply need to map this function to this list of positions and I get the data for all the parties:
</p>
<pre class="r"><code>elections_national_2018 &lt;- map_df(position_parties_national, extract_party, 
                         dataset = elections_raw_2018, target_rows = seq(11, 19)) %&gt;%
    mutate(locality = "Grand-Duchy of Luxembourg", division = "National")</code></pre>
<p>
I also added the <code>locality</code> and <code>division</code> columns to the data.
</p>
<p>
Let’s take a look:
</p>
<pre class="r"><code>glimpse(elections_national_2018)</code></pre>
<pre><code>## Observations: 72
## Variables: 6
## $ Party     &lt;chr&gt; "PIRATEN", "PIRATEN", "PIRATEN", "PIRATEN", "PIRATEN",…
## $ Year      &lt;dbl&gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, …
## $ Variables &lt;chr&gt; "Pourcentage", "Suffrage total", "Suffrages de liste",…
## $ Values    &lt;dbl&gt; 6.446204e-02, 2.275490e+05, 1.815600e+05, 4.598900e+04…
## $ locality  &lt;chr&gt; "Grand-Duchy of Luxembourg", "Grand-Duchy of Luxembour…
## $ division  &lt;chr&gt; "National", "National", "National", "National", "Natio…</code></pre>
<p>
Very nice.
</p>
<p>
Now we need to do the same for the 4 electoral circonscriptions. First, let’s load the data:
</p>
<pre class="r"><code># Electoral districts 2018
districts &lt;- c("Centre", "Nord", "Sud", "Est")

elections_district_raw_2018 &lt;- xlsx_cells("leg-2018-10-14-22-58-09-737.xlsx",
                                      sheets = districts)</code></pre>
<p>
Now things get trickier. Remember I said that the number of seats is proportional to the population of each circonscription? We simply can’t use the same target rows as before. For example, for the “Centre” circonscription, the target rows go from 12 to 37, but for the “Est” circonscription only from 12 to 23. Ideally, we would need a function that would return the target rows.
</p>
<p>
This is that function:
</p>
<pre class="r"><code># The target rows I need to extract are different from district to district
get_target_rows &lt;- function(dataset, sheet_to_extract, reference_address){

    last_row &lt;- dataset %&gt;%
        filter(sheet == sheet_to_extract) %&gt;%
        filter(address == reference_address) %&gt;%
        pull(numeric)

    seq(12, (11 + 5 + last_row))
}</code></pre>
<p>
This function needs a <code>dataset</code>, a <code>sheet_to_extract</code> and a <code>reference_address</code>. The reference address is a cell that actually contains the number of seats in that circonscription, in our case “B5”. We can easily get the list of target rows now:
</p>
<pre class="r"><code># Get the target rows
list_targets &lt;- map(districts, get_target_rows, dataset = elections_district_raw_2018, 
                    reference_address = "B5")

list_targets</code></pre>
<pre><code>## [[1]]
##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34
## [24] 35 36 37
## 
## [[2]]
##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25
## 
## [[3]]
##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34
## [24] 35 36 37 38 39
## 
## [[4]]
##  [1] 12 13 14 15 16 17 18 19 20 21 22 23</code></pre>
<p>
Now, let’s split the data we imported into a list, where each element of the list is a dataframe with the data from one circonscription:
</p>
<pre class="r"><code>list_data_districts &lt;- map(districts, ~filter(.data = elections_district_raw_2018, sheet == .)) </code></pre>
<p>
Now I can easily map the function I defined above, <code>extract_party</code> to this list of datasets. Well, I say easily, but it’s a bit more complicated than before because I have now a list of datasets and a list of target rows:
</p>
<pre class="r"><code>elections_district_2018 &lt;- map2(.x = list_data_districts, .y = list_targets,
     ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = .y))</code></pre>
<p>
The way to understand this is that for each element of <code>list_data_districts</code> and <code>list_targets</code>, I have to map <code>extract_party</code> to each element of <code>position_parties_national</code>. This gives the intented result:
</p>
<pre class="r"><code>elections_district_2018</code></pre>
<pre><code>## [[1]]
## # A tibble: 208 x 4
##    Party    Year Variables               Values
##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;
##  1 PIRATEN  2018 Pourcentage             0.0514
##  2 PIRATEN  2018 CLEMENT Sven (1)     8007     
##  3 PIRATEN  2018 WEYER Jerry (2)      3446     
##  4 PIRATEN  2018 CLEMENT Pascal (3)   3418     
##  5 PIRATEN  2018 KUNAKOVA Lucie (4)   2860     
##  6 PIRATEN  2018 WAMPACH Jo (14)      2693     
##  7 PIRATEN  2018 LAUX Cynthia (6)     2622     
##  8 PIRATEN  2018 ISEKIN Christian (5) 2610     
##  9 PIRATEN  2018 SCHWEICH Georges (9) 2602     
## 10 PIRATEN  2018 LIESCH Mireille (8)  2551     
## # … with 198 more rows
## 
## [[2]]
## # A tibble: 112 x 4
##    Party    Year Variables                             Values
##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                                  &lt;dbl&gt;
##  1 PIRATEN  2018 Pourcentage                           0.0767
##  2 PIRATEN  2018 COLOMBERA Jean (2)                 5074     
##  3 PIRATEN  2018 ALLARD Ben (1)                     4225     
##  4 PIRATEN  2018 MAAR Andy (3)                      2764     
##  5 PIRATEN  2018 GINTER Joshua (8)                  2536     
##  6 PIRATEN  2018 DASBACH Angelika (4)               2473     
##  7 PIRATEN  2018 GRÜNEISEN Sam (6)                  2408     
##  8 PIRATEN  2018 BAUMANN Roy (5)                    2387     
##  9 PIRATEN  2018 CONRAD Pierre (7)                  2280     
## 10 PIRATEN  2018 TRAUT ép. MOLITOR Angela Maria (9) 2274     
## # … with 102 more rows
## 
## [[3]]
## # A tibble: 224 x 4
##    Party    Year Variables                    Values
##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt;
##  1 PIRATEN  2018 Pourcentage                  0.0699
##  2 PIRATEN  2018 GOERGEN Marc (1)          9818     
##  3 PIRATEN  2018 FLOR Starsky (2)          6737     
##  4 PIRATEN  2018 KOHL Martine (3)          6071     
##  5 PIRATEN  2018 LIESCH Camille (4)        6025     
##  6 PIRATEN  2018 KOHL Sylvie (6)           5628     
##  7 PIRATEN  2018 WELTER Christian (5)      5619     
##  8 PIRATEN  2018 DA GRAÇA DIAS Yanick (10) 5307     
##  9 PIRATEN  2018 WEBER Jules (7)           5301     
## 10 PIRATEN  2018 CHMELIK Libor (8)         5247     
## # … with 214 more rows
## 
## [[4]]
## # A tibble: 96 x 4
##    Party    Year Variables                           Values
##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                                &lt;dbl&gt;
##  1 PIRATEN  2018 Pourcentage                         0.0698
##  2 PIRATEN  2018 FRÈRES Daniel (1)                4152     
##  3 PIRATEN  2018 CLEMENT Jill (7)                 1943     
##  4 PIRATEN  2018 HOUDREMONT Claire (2)            1844     
##  5 PIRATEN  2018 BÖRGER Nancy (3)                 1739     
##  6 PIRATEN  2018 MARTINS DOS SANTOS Catarina (6)  1710     
##  7 PIRATEN  2018 BELLEVILLE Tatjana (4)           1687     
##  8 PIRATEN  2018 CONTRERAS Gerald (5)             1687     
##  9 PIRATEN  2018 Suffrages total                 14762     
## 10 PIRATEN  2018 Suffrages de liste              10248     
## # … with 86 more rows</code></pre>
<p>
I now need to add the <code>locality</code> and <code>division</code> columns:
</p>
<pre class="r"><code>elections_district_2018 &lt;- map2(.y = elections_district_2018, .x = districts, 
     ~mutate(.y, locality = .x, division = "Electoral district")) %&gt;%
    bind_rows()</code></pre>
<p>
We’re almost done! Now we need to do the same for the 102 remaining sheets, one for each <strong>commune</strong> of Luxembourg. This will now go very fast, because we got all the building blocks from before:
</p>
<pre class="r"><code>communes &lt;- xlsx_sheet_names("leg-2018-10-14-22-58-09-737.xlsx")

communes &lt;- communes %-l% 
    c("Le Grand-Duché de Luxembourg", "Centre", "Est", "Nord", "Sud", "Sommaire")</code></pre>
<p>
Let me introduce the following function: <code>%-l%</code>. This function removes elements from lists:
</p>
<pre class="r"><code>c("a", "b", "c", "d") %-l% c("a", "d")</code></pre>
<pre><code>## [1] "b" "c"</code></pre>
<p>
You can think of it as “minus for lists”. This is called an infix operator.
</p>
<p>
So this function is very useful to get the list of communes, and is part of my package, <code>{brotools}</code>.
</p>
<p>
As before, I load the data:
</p>
<pre class="r"><code>elections_communes_raw_2018 &lt;- xlsx_cells("leg-2018-10-14-22-58-09-737.xlsx",
                                 sheets = communes)</code></pre>
<p>
Then get my list of targets, but I need to change the reference address. It’s “B8” now, not “B7”.
</p>
<pre class="r"><code># Get the target rows
list_targets &lt;- map(communes, get_target_rows, 
                    dataset = elections_communes_raw_2018, reference_address = "B8")</code></pre>
<p>
I now create a list of communes by mapping a filter function to the data:
</p>
<pre class="r"><code>list_data_communes &lt;- map(communes, ~filter(.data = elections_communes_raw_2018, sheet == .)) </code></pre>
<p>
And just as before, I get the data I need by using <code>extract_party</code>, and adding the “locality” and “division” columns:
</p>
<pre class="r"><code>elections_communes_2018 &lt;- map2(.x = list_data_communes, .y = list_targets,
                                ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = .y))

elections_communes_2018 &lt;- map2(.y = elections_communes_2018, .x = communes,
                                ~mutate(.y, locality = .x, division = "Commune")) %&gt;%
    bind_rows()</code></pre>
<p>
The steps are so similar for the four circonscriptions and for the 102 <strong>communes</strong> that I could have write a big wrapper function and the use it for the circonscription and <strong>communes</strong> at once. But I was lazy.
</p>
<p>
Finally, I bind everything together and have a nice, tidy, flat file:
</p>
<pre class="r"><code># Final results

elections_2018 &lt;- bind_rows(list(elections_national_2018, elections_district_2018, elections_communes_2018))

glimpse(elections_2018)</code></pre>
<pre><code>## Observations: 15,544
## Variables: 6
## $ Party     &lt;chr&gt; "PIRATEN", "PIRATEN", "PIRATEN", "PIRATEN", "PIRATEN",…
## $ Year      &lt;dbl&gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, …
## $ Variables &lt;chr&gt; "Pourcentage", "Suffrage total", "Suffrages de liste",…
## $ Values    &lt;dbl&gt; 6.446204e-02, 2.275490e+05, 1.815600e+05, 4.598900e+04…
## $ locality  &lt;chr&gt; "Grand-Duchy of Luxembourg", "Grand-Duchy of Luxembour…
## $ division  &lt;chr&gt; "National", "National", "National", "National", "Natio…</code></pre>
<p>
This blog post is already quite long, so I&nbsp;will analyze the data now that R can easily ingest it in a future blog post.
</p>



 ]]></description>
  <category>R</category>
  <category>programming</category>
  <guid>https://b-rodrigues.github.io/posts/2018-10-21-lux_elections.html</guid>
  <pubDate>Sun, 21 Oct 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Exporting editable plots from R to Powerpoint: making ggplot2 purrr with officer</title>
  <link>https://b-rodrigues.github.io/posts/2018-10-05-ggplot2_purrr_officer.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=oc9XOxUcvLY"> <img src="https://b-rodrigues.github.io/assets/img/officer_meme.jpg"></a>
</p>
</div>
<p>
A kind reader let me know that the function <code>create_pptx()</code> is now outdated, and proposed an update which you can find here: <a href="https://gist.github.com/b-rodrigues/ef4e97ed75028ca1ddd5987bb4085c1c">here</a>. Thank you <a href="https://twitter.com/jerry_stones/status/1239625489578254336"><span class="citation" data-cites="Jeremy">@Jeremy</span></a>!
</p>
<p>
I was recently confronted to the following problem: creating hundreds of plots that could still be edited by our client. What this meant was that I needed to export the graphs in Excel or Powerpoint or some other such tool that was familiar to the client, and not export the plots directly to pdf or png as I would normally do. I still wanted to use R to do it though, because I could do what I always do to when I need to perform repetitive tasks such as producing hundreds of plots; map over a list of, say, countries, and make one plot per country. This is something I discussed in a previous blog post, <a href="http://www.brodrigues.co/blog/2017-03-29-make-ggplot2-purrr/">Make ggplot2 purrr</a>.
</p>
<p>
So, after some online seaching, I found the <code>{officer}</code> package. This package allows you to put objects into Microsoft documents. For example, editable plots in a Powerpoint document. This is what I will show in this blog post.
</p>
<p>
Let’s start by loading the required packages:
</p>
<pre class="r"><code>library("tidyverse")
library("officer")
library("rvg")</code></pre>
<p>
Then, I will use the data from the time use survey, which I discussed in a previous blog post <a href="http://www.brodrigues.co/blog/2018-09-11-human_to_machine/">Going from a human readable Excel file to a machine-readable csv with {tidyxl}</a>.
</p>
<p>
You can download the data <a href="https://github.com/rbind/b-rodrigues.github.com/blob/master/content/blog/clean_data.csv">here</a>.
</p>
<p>
Let’s import and prepare it:
</p>
<pre class="r"><code>time_use &lt;- rio::import("clean_data.csv")


time_use &lt;- time_use %&gt;%
    filter(population %in% c("Male", "Female")) %&gt;%
    filter(activities %in% c("Personal care", "Sleep", "Eating", 
                             "Employment", "Household and family care")) %&gt;%
    group_by(day) %&gt;%
    nest()</code></pre>
<p>
I only kept two categories, “Male” and “Female” and 5 activities. Then I grouped by day and nested the data. This is how it looks like:
</p>
<pre class="r"><code>time_use</code></pre>
<pre><code>## # A tibble: 3 x 2
##   day                         data             
##   &lt;chr&gt;                       &lt;list&gt;           
## 1 Year 2014_Monday til Friday &lt;tibble [10 × 4]&gt;
## 2 Year 2014_Saturday          &lt;tibble [10 × 4]&gt;
## 3 Year 2014_Sunday            &lt;tibble [10 × 4]&gt;</code></pre>
<p>
As shown, <code>time_use</code> is a tibble with 2 columns, the first <code>day</code> contains the days, and the second <code>data</code>, is of type list, and each element of these lists are tibbles themselves. Let’s take a look inside one:
</p>
<pre class="r"><code>time_use$data[1]</code></pre>
<pre><code>## [[1]]
## # A tibble: 10 x 4
##    population activities                time  time_in_minutes
##    &lt;chr&gt;      &lt;chr&gt;                     &lt;chr&gt;           &lt;int&gt;
##  1 Male       Personal care             11:00             660
##  2 Male       Sleep                     08:24             504
##  3 Male       Eating                    01:46             106
##  4 Male       Employment                08:11             491
##  5 Male       Household and family care 01:59             119
##  6 Female     Personal care             11:15             675
##  7 Female     Sleep                     08:27             507
##  8 Female     Eating                    01:48             108
##  9 Female     Employment                06:54             414
## 10 Female     Household and family care 03:49             229</code></pre>
<p>
I can now create plots for each of the days with the following code:
</p>
<pre class="r"><code>my_plots &lt;- time_use %&gt;%
    mutate(plots = map2(.y = day, .x = data, ~ggplot(data = .x) + theme_minimal() +
                       geom_col(aes(y = time_in_minutes, x = activities, fill = population), 
                                position = "dodge") +
                       ggtitle(.y) +
                       ylab("Time in minutes") +
                       xlab("Activities")))</code></pre>
<p>
These steps are all detailled in my blog post <a href="http://www.brodrigues.co/blog/2017-03-29-make-ggplot2-purrr/">Make ggplot2 purrr</a>. Let’s take a look at <code>my_plots</code>:
</p>
<pre class="r"><code>my_plots</code></pre>
<pre><code>## # A tibble: 3 x 3
##   day                         data              plots 
##   &lt;chr&gt;                       &lt;list&gt;            &lt;list&gt;
## 1 Year 2014_Monday til Friday &lt;tibble [10 × 4]&gt; &lt;gg&gt;  
## 2 Year 2014_Saturday          &lt;tibble [10 × 4]&gt; &lt;gg&gt;  
## 3 Year 2014_Sunday            &lt;tibble [10 × 4]&gt; &lt;gg&gt;</code></pre>
<p>
The last column, called <code>plots</code> is a list where each element is a plot! We can take a look at one:
</p>
<pre class="r"><code>my_plots$plots[1]</code></pre>
<pre><code>## [[1]]</code></pre>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/ggplot2_purrr_officer.png" class="img-fluid"></p>
</div>
<p>
Now, this is where I could export these plots as pdfs or pngs. But this is not what I need. I need to export these plots as editable charts for Powerpoint. To do this for one image, I would do the following (as per <code>{officer}</code>’s documentation):
</p>
<pre class="r"><code>read_pptx() %&gt;%
    add_slide(layout = "Title and Content", master = "Office Theme") %&gt;%
    ph_with_vg(code = print(one_plot), type = "body") %&gt;% 
    print(target = path)</code></pre>
<p>
To map this over a list of arguments, I wrote a wrapper:
</p>
<pre class="r"><code>create_pptx &lt;- function(plot, path){
    if(!file.exists(path)) {
        out &lt;- read_pptx()
    } else {
        out &lt;- read_pptx(path)
    }
    
    out %&gt;%
        add_slide(layout = "Title and Content", master = "Office Theme") %&gt;%
        ph_with_vg(code = print(plot), type = "body") %&gt;% 
        print(target = path)
}</code></pre>
<p>
This function takes two arguments, <code>plot</code> and <code>path</code>. <code>plot</code> must be an plot object such as the ones contained inside the <code>plots</code> column of <code>my_plots</code> tibble. <code>path</code> is the path of where I want to save the pptx.
</p>
<p>
The first lines check if the file exists, if yes, the slides get added to the existing file, if not a new pptx gets created. The rest of the code is very similar to the one from the documentation. Now, to create my pptx I simple need to map over the <code>plots</code> column and provide a <code>path</code>:
</p>
<pre class="r"><code>map(my_plots$plots, create_pptx, path = "test.pptx")</code></pre>
<pre><code>## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName 'xsi:xmlns:' [202]</code></pre>
<pre><code>## [[1]]
## [1] "/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx"
## 
## [[2]]
## [1] "/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx"
## 
## [[3]]
## [1] "/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx"</code></pre>
<p>
Here is the end result:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/editable_plots.png"><!-- -->
</p>
<p>
Inside Powerpoint (or in this case Libreoffice), the plots are geometric shapes that can now be edited!
</p>



 ]]></description>
  <category>R</category>
  <category>programming</category>
  <guid>https://b-rodrigues.github.io/posts/2018-10-05-ggplot2_purrr_officer.html</guid>
  <pubDate>Fri, 05 Oct 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>How Luxembourguish residents spend their time: a small {flexdashboard} demo using the Time use survey data</title>
  <link>https://b-rodrigues.github.io/posts/2018-09-15-time_use.html</link>
  <description><![CDATA[ 




<div style="text-align: center; transform: scale(0.7); transform-origin: center;">
<p>
<a href="https://brodriguesco.shinyapps.io/time_use_luxembourg/"> <img src="https://b-rodrigues.github.io/assets/img/time_use_dashboard.png"> </a>
</p>
</div>
<p>
In a <a href="../posts/2018-09-11-human_to_machine.html">previous blog post</a> I have showed how you could use the <code>{tidyxl}</code> package to go from a human readable Excel Workbook to a tidy data set (or flat file, as they are also called). Some people then contributed their solutions, which is always something I really enjoy when it happens. This way, I also get to learn things!
</p>
<p>
<a href="https://twitter.com/expersso"><code><span class="citation" data-cites="expersso">@expersso</span></code></a> proposed a solution without <code>{tidyxl}</code>:
</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
Interesting data wrangling exercise in <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a>. <br>My solution (without using {tidyxl}): <a href="https://t.co/VjuOoM82yX">https://t.co/VjuOoM82yX</a> <a href="https://t.co/VsXFyowigu">https://t.co/VsXFyowigu</a>
</p>
— Eric (<span class="citation"><span class="citation" data-cites="expersso">@expersso</span></span>) <a href="https://twitter.com/expersso/status/1039894727808757761?ref_src=twsrc%5Etfw">September 12, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>
<a href="https://www.benstenhaug.com/">Ben Stenhaug</a> also proposed a solution on his <a href="https://github.com/stenhaug/shared/blob/master/tidyxl_bruno_blog.md">github</a> which is simpler than my code in a lot of ways!
</p>
<p>
Update: <a href="https://twitter.com/nacnudus"><code><span class="citation" data-cites="nacnudus">@nacnudus</span></code></a> also contributed his own version using <code>{unpivotr}</code>:
</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
Here’s a version using unpivotr <a href="https://t.co/l2hy6zCuKj">https://t.co/l2hy6zCuKj</a>
</p>
— Duncan Garmonsway (<span class="citation"><span class="citation" data-cites="nacnudus">@nacnudus</span></span>) <a href="https://twitter.com/nacnudus/status/1040905626317217792?ref_src=twsrc%5Etfw">September 15, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>
Now, it would be too bad not to further analyze this data. I’ve been wanting to play around with the <code>{flexdashboard}</code> package for some time now, but never really got the opportunity to do so. The opportunity has now arrived. Using the cleaned data from the last post, I will further tweak it a little bit, and then produce a very simple dashboard using <code>{flexdashboard}</code>.
</p>
<p>
If you want to skip the rest of the blog post and go directly to the dashboard, just click <a href="https://brodriguesco.shinyapps.io/time_use_luxembourg/">here</a>.
</p>
<p>
To make the data useful, I need to convert the strings that represent the amount of time spent doing a task (for example “1:23”) to minutes. For this I use the <code>{chron}</code> package:
</p>
<pre class="r"><code>clean_data &lt;- clean_data %&gt;%
    mutate(time_in_minutes = paste0(time, ":00")) %&gt;% # I need to add ":00" for the seconds else it won't work
    mutate(time_in_minutes = 
               chron::hours(chron::times(time_in_minutes)) * 60 + 
               chron::minutes(chron::times(time_in_minutes)))

rio::export(clean_data, "clean_data.csv")</code></pre>
<p>
Now we’re ready to go! Below is the code to build the dashboard; if you want to try, you should copy and paste the code inside a Rmd document:
</p>
<pre><code>---
title: "Time Use Survey of Luxembourguish residents"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
runtime: shiny

---

`` `{r setup, include=FALSE}
library(flexdashboard)
library(shiny)
library(tidyverse)
library(plotly)
library(ggthemes)

main_categories &lt;- c("Personal care",
                     "Employment",
                     "Study",
                     "Household and family care",
                     "Voluntary work and meetings",
                     "Social life and entertainment",
                     "Sports and outdoor activities",
                     "Hobbies and games",
                     "Media",
                     "Travel")

df &lt;- read.csv("clean_data.csv") %&gt;%
    rename(Population = population) %&gt;%
    rename(Activities = activities)
`` `

Inputs {.sidebar}
-----------------------------------------------------------------------

`` `{r}

selectInput(inputId = "activitiesName", 
            label = "Choose an activity", 
            choices = unique(df$Activities))

selectInput(inputId = "dayName", 
            label = "Choose a day", 
            choices = unique(df$day), 
            selected = "Year 2014_Monday til Friday")

selectInput(inputId = "populationName", 
            label = "Choose a population", 
            choices = unique(df$Population), 
            multiple = TRUE, selected = c("Male", "Female"))

`` `

The Time Use Survey (TUS) aims to measure accurately how people allocate their time across different day-to-day activities. To this end, people are asked to keep records of all their activities in a time diary. For each activity, additional information is collected about whether or not the person was alone doing it or together with other persons, where did the activity take place, etc. The main studies on time use have been conducted to calculate indicators making possible comparative analysis of quality of life within the same population or between countries. International studies care more about specific activities such as work (unpaid or not), free time, leisure, personal care (including sleep), etc.
Source:&nbsp;http://statistiques.public.lu/en/surveys/espace-households/time-use/index.html

Layout based on https://jjallaire.shinyapps.io/shiny-biclust/

Row
-----------------------------------------------------------------------

### Minutes spent per day on certain activities
    
`` `{r}
dfInput &lt;- reactive({
        df %&gt;% filter(Activities == input$activitiesName,
                      Population %in% input$populationName,
                      day %in% input$dayName)
    })

    dfInput2 &lt;- reactive({
        df %&gt;% filter(Activities %in% main_categories,
                      Population %in% input$populationName,
                      day %in% input$dayName)
    })
    
  renderPlotly({

        df1 &lt;- dfInput()

        p1 &lt;- ggplot(df1, 
                     aes(x = Activities, y = time_in_minutes, fill = Population)) +
            geom_col(position = "dodge") + 
            theme_minimal() + 
            xlab("Activities") + 
            ylab("Time in minutes") +
            scale_fill_gdocs()

        ggplotly(p1)})
`` `

Row 
-----------------------------------------------------------------------

### Proportion of the day spent on main activities
    
`` `{r}
renderPlotly({
    
       df2 &lt;- dfInput2()
       
       p2 &lt;- ggplot(df2, 
                   aes(x = Population, y = time_in_minutes, fill = Activities)) +
           geom_bar(stat="identity", position="fill") + 
            xlab("Proportion") + 
            ylab("Proportion") +
           theme_minimal() +
           scale_fill_gdocs()
       
       ggplotly(p2)
   })
`` `</code></pre>
<p>
You will see that I have defined the following atomic vector:
</p>
<pre class="r"><code>main_categories &lt;- c("Personal care",
                     "Employment",
                     "Study",
                     "Household and family care",
                     "Voluntary work and meetings",
                     "Social life and entertainment",
                     "Sports and outdoor activities",
                     "Hobbies and games",
                     "Media",
                     "Travel")</code></pre>
<p>
If you go back to the raw Excel file, you will see that these main categories are then split into secondary activities. The first bar plot of the dashboard does not distinguish between the main and secondary activities, whereas the second barplot only considers the main activities. I could have added another column to the data that helped distinguish whether an activity was a main or secondary one, but I was lazy. The source code of the dashboard is very simple as it uses R Markdown. To have interactivity, I’ve used Shiny to dynamically filter the data, and built the plots with <code>{ggplot2}</code>. Finally, I’ve passed the plots to the <code>ggplotly()</code> function from the <code>{plotly}</code> package for some quick and easy javascript goodness!
</p>



 ]]></description>
  <category>R</category>
  <category>programming</category>
  <guid>https://b-rodrigues.github.io/posts/2018-09-15-time_use.html</guid>
  <pubDate>Fri, 14 Sep 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Going from a human readable Excel file to a machine-readable csv with {tidyxl}</title>
  <link>https://b-rodrigues.github.io/posts/2018-09-11-human_to_machine.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=m-PE3OkJ8XE"> <video width="854" height="480" controls="" autoplay="" muted="" loop=""> <source src="../assets/img/1a9.mp4" type="video/mp4"></video></a>
</p>

</div>
<p>
I won’t write a very long introduction; we all know that Excel is ubiquitous in business, and that it has a lot of very nice features, especially for business practitioners that do not know any programming. However, when people use Excel for purposes it was not designed for, it can be a hassle. Often, people use Excel as a reporting tool, which it is not; they create very elaborated and complicated spreadsheets that are human readable, but impossible to import within any other tool.
</p>
<p>
In this blog post (which will probably be part of a series), I show you how you can go from this:
</p>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/time_use.png" class="img-fluid"></p>
</div>
<p>
to this:
</p>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/time_use2.png" class="img-fluid"></p>
</div>
<p>
You can find the data I will use <a href="http://statistiques.public.lu/stat/ReportFolders/ReportFolder.aspx?IF_Language=eng&amp;MainTheme=3&amp;FldrName=1&amp;RFPath=14306">here</a>. Click on the “Time use” folder and you can download the workbook.
</p>
<p>
The Excel workbook contains several sheets (in French and English) of the amount of time Luxembourguish citizens spend from Monday to Sunday. For example, on average, people that are in employment spend almost 8 hours sleeping during the week days, and 8:45 hours on Saturday.
</p>
<p>
As you can see from the screenshot, each sheet contains several tables that have lots of headers and these tables are next to one another. Trying to import these sheets with good ol’ <code>readxl::read_excel()</code> produces a monster.
</p>
<p>
This is where <code>{tidyxl}</code> comes into play. Let’s import the workbook with <code>{tidyxl}</code>:
</p>
<pre class="r"><code>library(tidyverse)
library(tidyxl)

time_use_xl &lt;- xlsx_cells("time-use.xlsx")</code></pre>
<p>
Let’s see what happened:
</p>
<pre class="r"><code>head(time_use_xl)</code></pre>
<pre><code>## # A tibble: 6 x 21
##   sheet address   row   col is_blank data_type error logical numeric
##   &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;lgl&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;     &lt;dbl&gt;
## 1 Index A1          1     1 FALSE    character &lt;NA&gt;  NA           NA
## 2 Index B1          1     2 TRUE     blank     &lt;NA&gt;  NA           NA
## 3 Index C1          1     3 TRUE     blank     &lt;NA&gt;  NA           NA
## 4 Index D1          1     4 TRUE     blank     &lt;NA&gt;  NA           NA
## 5 Index E1          1     5 TRUE     blank     &lt;NA&gt;  NA           NA
## 6 Index F1          1     6 TRUE     blank     &lt;NA&gt;  NA           NA
## # … with 12 more variables: date &lt;dttm&gt;, character &lt;chr&gt;,
## #   character_formatted &lt;list&gt;, formula &lt;chr&gt;, is_array &lt;lgl&gt;,
## #   formula_ref &lt;chr&gt;, formula_group &lt;int&gt;, comment &lt;chr&gt;, height &lt;dbl&gt;,
## #   width &lt;dbl&gt;, style_format &lt;chr&gt;, local_format_id &lt;int&gt;</code></pre>
<p>
As you can see, the sheet was imported, but the result might be unexpected. Actually, <code>time_use_xl</code> is a <code>tibble</code> object, where each row is one cell of the Excel sheet. This might seem very complicated to handle, but you will see that it actually makes things way easier.
</p>
<p>
I only want to work on the English sheets so I use the following code to ignore the French ones:
</p>
<pre class="r"><code>sheets &lt;- xlsx_sheet_names("time-use.xlsx") %&gt;%
    keep(grepl(pattern = ".*day$", .))</code></pre>
<p>
Also, there’s a sheet that aggregates the results for week days and weekends, which I also ignore.
</p>
<p>
Now, to extract the tables from each sheet I wrote the following function:
</p>
<pre class="r"><code>extract_data &lt;- function(sheet){
    activities &lt;- sheet %&gt;%
        filter(col == 2) %&gt;%
        select(row, character) %&gt;%
        filter(row %in% seq(6,58)) %&gt;%
        rename(activities = character) %&gt;%
        select(-row)
    
    cols_to_extract &lt;- sheet %&gt;% 
        filter(grepl("Population who completed.*", character)) %&gt;% 
        pull(col)
    
    headers_pos &lt;- cols_to_extract - 1
    
    headers &lt;- sheet %&gt;%
        filter(col %in% headers_pos, row == 3) %&gt;%
        pull(character)
    
    cols_to_extract %&gt;% 
        map(~filter(sheet, col %in% .)) %&gt;%
        map(~select(., sheet, address, row, col, character)) %&gt;%
        map(~filter(., row %in% seq(6,58))) %&gt;%
        map(~select(., character)) %&gt;%
        map2(.x = ., .y = headers, ~mutate(.x, "population" = .y)) %&gt;%
        map(., ~bind_cols(activities, .)) %&gt;%
        bind_rows()
}</code></pre>
<p>
Let’s study it step by step and see how it works. First, there’s the argument, <code>sheet</code>. This function will be mapped to each sheet of the workbook. Then, the first block I wrote, extracts the activities:
</p>
<pre class="r"><code>    activities &lt;- sheet %&gt;%
        filter(col == 2) %&gt;%
        select(row, character) %&gt;%
        filter(row %in% seq(6,58)) %&gt;%
        rename(activities = character) %&gt;%
        select(-row)</code></pre>
<p>
I only keep the second column (<code>filter(col == 2)</code>); <code>col</code> is a column of the <code>tibble</code> and if you look inside the workbook, you will notice that the activities are on the second column, or the B column. Then, I select two columns, the <code>row</code> and the <code>character</code> column. <code>row</code> is self-explanatory and <code>character</code> actually contains whatever is written inside the cells. Then, I only keep rows 6 to 58, because that is what interests me; the rest is either empty cells, or unneeded. Finally, I rename the <code>character</code> column to activities and remove the <code>row</code> column.
</p>
<p>
The second block:
</p>
<pre class="r"><code>    cols_to_extract &lt;- sheet %&gt;% 
        filter(grepl("Population who completed.*", character)) %&gt;% 
        pull(col)</code></pre>
<p>
returns the index of the columns I want to extract. I am only interested in the people that have completed the activities, so using <code>grepl()</code> inside <code>filter()</code>, I located these columns, and use <code>pull()</code>… to pull them out of the data frame! <code>cols_to_extract</code> is thus a nice atomic vector of columns that I want to keep.
</p>
<p>
In the third block, I extract the headers:
</p>
<pre class="r"><code>    headers_pos &lt;- cols_to_extract - 1</code></pre>
<p>
Why <code>- 1</code>? This is because if you look in the Excel, you will see that the headers are one column before the column labeled “People who completed the activity”. For example on column G, I have “People who completed the activity” and on column F I have the header, in this case “Male”.
</p>
<p>
Now I actually extract the headers:
</p>
<pre class="r"><code>    headers &lt;- sheet %&gt;%
        filter(col %in% headers_pos, row == 3) %&gt;%
        pull(character)</code></pre>
<p>
Headers are always on the third row, but on different columns, hence the <code>col %in% headers_pos</code>. I then pull out the values inside the cells with <code>pull(character)</code>. So my <code>headers</code> object will be an atomic vector with “All”, “Male”, “Female”, “10 - 19 years”, etc… everything on row 3.
</p>
<p>
Finally, the last block, actually extracts the data:
</p>
<pre class="r"><code>    cols_to_extract %&gt;% 
        map(~filter(sheet, col %in% .)) %&gt;%
        map(~select(., sheet, address, row, col, character)) %&gt;%
        map(~filter(., row %in% seq(6,58))) %&gt;%
        map(~select(., character)) %&gt;%
        map2(.x = ., .y = headers, ~mutate(.x, "population" = .y)) %&gt;%
        map(., ~bind_cols(activities, .)) %&gt;%
        bind_rows()</code></pre>
<p>
<code>cols_to_extract</code> is a vector with the positions of the columns that interest me. So for example “4”, “7”, “10” and so on. I map this vector to the sheet, which returns me a list of extracted data frames. I pass this down to a <code>select()</code> (which is inside <code>map()</code>… why? Because the input parameter is a list of data frames). So for each data frame inside the list, I select the columns <code>sheet</code>, <code>address</code>, <code>row</code>, <code>col</code> and <code>character</code>. Then, for each data frame inside the list, I use <code>filter()</code> to only keep the rows from position 6 to 58. Then, I only select the <code>character</code> column, which actually contains the text inside the cell. Then, using <code>map2()</code>, I add the values inside the <code>headers</code> object as a new column, called <code>population</code>. Then, I bind the <code>activities</code> column to the data frame and bind all the rows together.
</p>
<p>
Time to use this function! Let’s see:
</p>
<pre class="r"><code>clean_data &lt;- sheets %&gt;%
    map(~filter(time_use_xl, sheet %in% .)) %&gt;%
    set_names(sheets) %&gt;%
    map(extract_data) %&gt;%
    map2(.x = ., .y = sheets, ~mutate(.x, "day" = .y)) %&gt;%
    bind_rows() %&gt;%
    select(day, population, activities, time = character)

glimpse(clean_data)</code></pre>
<pre><code>## Observations: 2,968
## Variables: 4
## $ day        &lt;chr&gt; "Year 2014_Monday til Friday", "Year 2014_Monday til …
## $ population &lt;chr&gt; "All", "All", "All", "All", "All", "All", "All", "All…
## $ activities &lt;chr&gt; "Personal care", "Sleep", "Eating", "Other personal c…
## $ time       &lt;chr&gt; "11:07", "08:26", "01:47", "00:56", "07:37", "07:47",…</code></pre>
<p>
So I map my list of sheets to the <code>tibble</code> I imported with <code>readxl</code>, use <code>set_names</code> to name the elements of my list (which is superfluous, but I wanted to show this; might interest you!) and then map this result to my little function. I could stop here, but I then add a new column to each data frame that contains the day on which the data was measured, bind the rows together and reorder the columns. Done!
</p>
<p>
Now, how did I come up with this function? I did not start with a function. I started by writing some code that did what I wanted for one table only, inside one sheet only. Only when I got something that worked, did I start to generalize to several tables and then to several sheets. Most of the time spent was actually in trying to find patterns in the Excel sheet that I could use to write my function (for example noticing that the headers I wanted where always one column before the column I was interested in). This is my advice when working with function programming; always solve the issue for one element, wrap this code inside a function, and then simply map this function to a list of elements!
</p>



 ]]></description>
  <category>R</category>
  <category>programming</category>
  <guid>https://b-rodrigues.github.io/posts/2018-09-11-human_to_machine.html</guid>
  <pubDate>Tue, 11 Sep 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The year of the GNU+Linux desktop is upon us: using user ratings of Steam Play compatibility to play around with regex and the tidyverse</title>
  <link>https://b-rodrigues.github.io/posts/2018-09-08-steam_linux.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=4QokOwvPxrE"> <img src="https://b-rodrigues.github.io/assets/img/want_to_believe.jpg"></a>
</p>
</div>
<p>
I’ve been using GNU+Linux distros for about 10 years now, and have settled for openSUSE as my main operating system around 3 years ago, perhaps even more. If you’re a gamer, you might have heard about SteamOS and how more and more games are available on GNU+Linux. I don’t really care about games, I play the occasional one (currently <a href="http://www.tangledeep.com/">Tangledeep</a>) when I find the time, but still follow the news about gaming on GNU+Linux. Last week, Valve announced something quite big; it is now possible to run Windows games on GNU+Linux directly from Steam, using a modified version of <a href="https://en.wikipedia.org/wiki/Wine_(software)">Wine</a> they call Proton. The feature is still in Beta, and Valve announced that they guarantee around 30 games to work already flawlessly. Of course, people have tried running a lot of other games, and, as was to be expected from Free Software and Open Source fans, GNU+Linux gamers created a Google Sheet that lists which games were tried and how they run. You can take a look at the sheet <a href="https://docs.google.com/spreadsheets/d/1DcZZQ4HL_Ol969UbXJmFG8TzOHNnHoj8Q1f8DIFe8-8/htmlview?sle=true&amp;pru=AAABZbqTTkc*IvT11ShwA2kjoe_4lPefiQ#gid=1003113831">here</a>.
</p>
<p>
In this blog post, I will play around with this sheet. This blog post lists some <code>{tidyverse}</code> tricks I find useful and use often. Perhaps these tricks will be useful to you too! Let’s start by loading the needed packages:
</p>
<pre class="r"><code>library(tidyverse)
library(magrittr)
library(readxl)</code></pre>
<p>
Since I’m lazy and don’t want to type the whole name of the file I’ll be using some little regex:
</p>
<pre class="r"><code>steam &lt;- read_excel(Sys.glob("Steam*"), sheet = "Main", skip = 2)

glimpse(steam)</code></pre>
<pre><code>## Observations: 8,570
## Variables: 9
## $ SteamDB   &lt;chr&gt; "LINK", "LINK", "LINK", "LINK", "LINK", "LINK", "LINK"…
## $ Game      &lt;chr&gt; "64", "1849", "1982", "1982", "am Weapon: Revival", ".…
## $ Submitted &lt;chr&gt; "5 days ago", "12 days ago", "11 days ago", "11 days a…
## $ Status    &lt;chr&gt; "Garbage", "Platinum", "Gold", "Platinum", "Platinum",…
## $ Notes     &lt;chr&gt; "Crashes with a debug log", "Plays OK.", "Gamepad supp…
## $ Distro    &lt;chr&gt; "Arch (4.18.5)", "Manjaro XFCE", "Gentoo AMD64 (Kernel…
## $ Driver    &lt;chr&gt; "Nvidia 396.54 / Intel xf86-video-intel (1:2.99.917+83…
## $ Specs     &lt;chr&gt; "Intel Core i7-7700HQ / Nvidia GTX 1050 (Mobile)", "Ry…
## $ Proton    &lt;chr&gt; "3.7 Beta", "3.7-4 Beta", "3.7-4 Beta", "Default", "3.…</code></pre>
<p>
Let’s count how many unique games are in the data:
</p>
<pre class="r"><code>steam %&gt;%
    count(Game)</code></pre>
<pre><code>## # A tibble: 3,855 x 2
##    Game                                                                   n
##    &lt;chr&gt;                                                              &lt;int&gt;
##  1 .hack//G.U. Last Recode                                                2
##  2 $1 Ride                                                                1
##  3 0rbitalis                                                              1
##  4 10 Second Ninja                                                        4
##  5 100% Orange Juice                                                     17
##  6 1000 Amps                                                              3
##  7 12 Labours of Hercules VII: Fleecing the Fleece (Platinum Edition)     1
##  8 16bit trader                                                           1
##  9 1849                                                                   1
## 10 1953 - KGB Unleased                                                    1
## # … with 3,845 more rows</code></pre>
<p>
That’s quite a lot of games! However, not everyone of them is playable:
</p>
<pre class="r"><code>steam %&gt;%
    count(Status)</code></pre>
<pre><code>## # A tibble: 8 x 2
##   Status       n
##   &lt;chr&gt;    &lt;int&gt;
## 1 Borked     205
## 2 bronze       1
## 3 Bronze     423
## 4 Garbage   2705
## 5 Gold       969
## 6 Platinum  2596
## 7 Primary      1
## 8 Silver    1670</code></pre>
<p>
Around 2500 have the status “Platinum”, but some games might have more than one status:
</p>
<pre class="r"><code>steam %&gt;%
    filter(Game == "100% Orange Juice") %&gt;%
    count(Status)</code></pre>
<pre><code>## # A tibble: 5 x 2
##   Status       n
##   &lt;chr&gt;    &lt;int&gt;
## 1 Bronze       5
## 2 Garbage      3
## 3 Gold         2
## 4 Platinum     6
## 5 Silver       1</code></pre>
<p>
More games run like <em>Garbage</em> than <em>Platinum</em>. But perhaps we can dig a little deeper and see if we find some patterns.
</p>
<p>
Let’s take a look at the GNU+Linux distros:
</p>
<pre class="r"><code>steam %&gt;%
    count(Distro) </code></pre>
<pre><code>## # A tibble: 2,085 x 2
##    Distro                                         n
##    &lt;chr&gt;                                      &lt;int&gt;
##  1 &lt;NA&gt;                                           1
##  2 ?                                              2
##  3 "\"Arch Linux\" (64 bit)"                      1
##  4 "\"Linux Mint 18.3 Sylvia 64bit"               1
##  5 "\"Manjaro Stable 64-bit (Kernel 4.14.66)"     1
##  6 "\"Solus\" (64 bit)"                           2
##  7 (K)ubuntu 18.04 64-bit (Kernel 4.15.0)         2
##  8 (L)Ubuntu 18.04.1 LTS                          1
##  9 18.04.1                                        1
## 10 18.04.1 LTS                                    2
## # … with 2,075 more rows</code></pre>
<p>
Ok the distro column is pretty messy. Let’s try to bring some order to it:
</p>
<pre class="r"><code>steam %&lt;&gt;%
    mutate(distribution = as_factor(case_when(
        grepl("buntu|lementary|antergos|steam|mint|18.|pop|neon", Distro, ignore.case = TRUE) ~ "Ubuntu",
        grepl("arch|manjaro", Distro, ignore.case = TRUE) ~ "Arch Linux",
        grepl("gentoo", Distro, ignore.case = TRUE) ~ "Gentoo",
        grepl("fedora", Distro, ignore.case = TRUE) ~ "Fedora",
        grepl("suse", Distro, ignore.case = TRUE) ~ "openSUSE",
        grepl("debian|sid|stretch|lmde", Distro, ignore.case = TRUE) ~ "Debian",
        grepl("solus", Distro, ignore.case = TRUE) ~ "Solus",
        grepl("slackware", Distro, ignore.case = TRUE) ~ "Slackware",
        grepl("void", Distro, ignore.case = TRUE) ~ "Void Linux",
        TRUE ~ "Other"
    )))</code></pre>
<p>
The <code>%&lt;&gt;%</code> operator is shorthand for <code>a &lt;- a %&gt;% f()</code>. It passes <code>a</code> to <code>f()</code> and assigns the result back to <code>a</code>. Anyways, let’s take a look at the <code>distribution</code> column:
</p>
<pre class="r"><code>steam %&gt;%
    count(distribution)</code></pre>
<pre><code>## # A tibble: 10 x 2
##    distribution     n
##    &lt;fct&gt;        &lt;int&gt;
##  1 Ubuntu        6632
##  2 Arch Linux     805
##  3 Solus          175
##  4 Debian         359
##  5 Fedora         355
##  6 Gentoo          42
##  7 Void Linux      38
##  8 Other           76
##  9 openSUSE        66
## 10 Slackware       22</code></pre>
<p>
I will group distributions that have less than 100 occurrences into a single category (meaning I will keep the 5 more common values):
</p>
<pre class="r"><code>steam %&lt;&gt;%
    mutate(distribution = fct_lump(distribution, n = 5, other_level = "Other")) 

steam %&gt;%
    count(distribution)</code></pre>
<pre><code>## # A tibble: 6 x 2
##   distribution     n
##   &lt;fct&gt;        &lt;int&gt;
## 1 Ubuntu        6632
## 2 Arch Linux     805
## 3 Solus          175
## 4 Debian         359
## 5 Fedora         355
## 6 Other          244</code></pre>
<p>
Let’s do the same for the CPUs:
</p>
<pre class="r"><code>steam %&lt;&gt;%
    mutate(CPU = as_factor(case_when(
        grepl("intel|i\\d|xeon|core2|\\d{4}k|q\\d{4}|pentium", Specs, ignore.case = TRUE) ~ "Intel",
        grepl("ryzen|threadripper|tr|amd|fx|r\\d|\\d{4}x|phenom", Specs, ignore.case = TRUE) ~ "AMD",
        TRUE ~ NA_character_
    )))

steam %&gt;%
    count(CPU)</code></pre>
<pre><code>## Warning: Factor `CPU` contains implicit NA, consider using
## `forcats::fct_explicit_na`</code></pre>
<pre><code>## # A tibble: 3 x 2
##   CPU       n
##   &lt;fct&gt; &lt;int&gt;
## 1 Intel  5768
## 2 AMD    2319
## 3 &lt;NA&gt;    483</code></pre>
<p>
And the same for the GPUs:
</p>
<pre class="r"><code>steam %&lt;&gt;%
    mutate(GPU = as_factor(case_when(
        grepl("nvidia|geforce|3\\d{2}|nouveau|gtx|gt\\s?\\d{1,}|9\\d0|1060|1070|1080", Specs, ignore.case = TRUE) ~ "Nvidia",
        grepl("amd|radeon|ati|rx|vega|r9", Specs, ignore.case = TRUE) ~ "AMD",
        grepl("intel|igpu|integrated|hd\\d{4}|hd\\sgraphics", Specs, ignore.case = TRUE) ~ "Intel",
        TRUE ~ NA_character_
    )))

steam %&gt;%
    count(GPU)</code></pre>
<pre><code>## Warning: Factor `GPU` contains implicit NA, consider using
## `forcats::fct_explicit_na`</code></pre>
<pre><code>## # A tibble: 4 x 2
##   GPU        n
##   &lt;fct&gt;  &lt;int&gt;
## 1 Nvidia  6086
## 2 AMD     1374
## 3 Intel    413
## 4 &lt;NA&gt;     697</code></pre>
<p>
I will also add a rank for the <code>Status</code> column:
</p>
<pre class="r"><code>steam %&lt;&gt;%
    mutate(rank_status = case_when(
        Status == "Platinum" ~ 5,
        Status == "Gold" ~ 4,
        Status == "Silver" ~ 3,
        Status == "Bronze" ~ 2,
        Status == "Garbage" ~ 1
    ))</code></pre>
<p>
Now, what are the top 5 most frequent combinations of Status, distribution, CPU and GPU?
</p>
<pre class="r"><code>steam %&gt;%
    filter(!is.na(CPU), !is.na(GPU)) %&gt;%
    count(Status, distribution, CPU, GPU) %&gt;%
    mutate(total = sum(n)) %&gt;%
    mutate(freq = n / total) %&gt;%
    top_n(5)</code></pre>
<pre><code>## Selecting by freq</code></pre>
<pre><code>## # A tibble: 5 x 7
##   Status   distribution CPU   GPU        n total   freq
##   &lt;chr&gt;    &lt;fct&gt;        &lt;fct&gt; &lt;fct&gt;  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;
## 1 Garbage  Ubuntu       Intel Nvidia  1025  7443 0.138 
## 2 Gold     Ubuntu       Intel Nvidia   361  7443 0.0485
## 3 Platinum Ubuntu       Intel Nvidia  1046  7443 0.141 
## 4 Platinum Ubuntu       AMD   Nvidia   338  7443 0.0454
## 5 Silver   Ubuntu       Intel Nvidia   650  7443 0.0873</code></pre>
<p>
Unsurprisingly, Ubuntu, or distributions using Ubuntu as a base, are the most popular ones. Nvidia is the most popular GPU, Intel for CPUs and in most cases, this combo of hardware and distribution is associated with positive ratings (even though there are almost as many “Garbage” ratings than “Platinum” ratings).
</p>
<p>
Now let’s compute some dumb averages of Statuses by distribution, CPU and GPU. Since I’m going to run the same computation three times, I’ll write a function to do that.
</p>
<pre class="r"><code>compute_avg &lt;- function(dataset, var){
    var &lt;- enquo(var)
    dataset %&gt;%
        select(rank_status, (!!var)) %&gt;%
        group_by((!!var)) %&gt;%
        mutate(wt = n()) %&gt;%
        summarise(average_rating = weighted.mean(rank_status, (!!var), wt, na.rm = TRUE))
}</code></pre>
<p>
Let’s see now if we can rank distribution by Steam play rating:
</p>
<pre class="r"><code>compute_avg(steam, distribution)</code></pre>
<pre><code>## # A tibble: 6 x 2
##   distribution average_rating
##   &lt;fct&gt;                 &lt;dbl&gt;
## 1 Ubuntu                 3.03
## 2 Arch Linux             3.05
## 3 Solus                  3.03
## 4 Debian                 3.01
## 5 Fedora                 3.07
## 6 Other                  3.16</code></pre>
<p>
How about for hardware?
</p>
<pre class="r"><code>compute_avg(steam, GPU)</code></pre>
<pre><code>## Warning: Factor `GPU` contains implicit NA, consider using
## `forcats::fct_explicit_na`

## Warning: Factor `GPU` contains implicit NA, consider using
## `forcats::fct_explicit_na`</code></pre>
<pre><code>## # A tibble: 4 x 2
##   GPU    average_rating
##   &lt;fct&gt;           &lt;dbl&gt;
## 1 Nvidia           3.07
## 2 AMD              2.90
## 3 Intel            3.01
## 4 &lt;NA&gt;            NA</code></pre>
<pre class="r"><code>compute_avg(steam, CPU)</code></pre>
<pre><code>## Warning: Factor `CPU` contains implicit NA, consider using
## `forcats::fct_explicit_na`

## Warning: Factor `CPU` contains implicit NA, consider using
## `forcats::fct_explicit_na`</code></pre>
<pre><code>## # A tibble: 3 x 2
##   CPU   average_rating
##   &lt;fct&gt;          &lt;dbl&gt;
## 1 Intel           3.03
## 2 AMD             3.06
## 3 &lt;NA&gt;           NA</code></pre>
<p>
To wrap this up, what are the games with the most ratings? Perhaps this can give us a hint about which games GNU+Linux users prefer:
</p>
<pre class="r"><code>steam %&gt;%
    count(Game) %&gt;%
    top_n(10)</code></pre>
<pre><code>## Selecting by n</code></pre>
<pre><code>## # A tibble: 10 x 2
##    Game                              n
##    &lt;chr&gt;                         &lt;int&gt;
##  1 Age of Empires II: HD Edition    43
##  2 Borderlands                      39
##  3 DiRT 3 Complete Edition          32
##  4 DOOM                             62
##  5 Fallout: New Vegas               45
##  6 Grim Dawn                        34
##  7 No Man's Sky                     40
##  8 Path of Exile                    35
##  9 Quake Champions                  32
## 10 The Elder Scrolls V: Skyrim      46</code></pre>
<p>
I actually laughed out loud when I saw that DOOM was the game with the most ratings! What else was I expecting, really.
</p>



 ]]></description>
  <category>R</category>
  <category>programming</category>
  <guid>https://b-rodrigues.github.io/posts/2018-09-08-steam_linux.html</guid>
  <pubDate>Sat, 08 Sep 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Dealing with heteroskedasticity; regression with robust standard errors using R</title>
  <link>https://b-rodrigues.github.io/posts/2018-07-08-rob_stderr.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://cran.r-project.org/web/packages/sandwich/index.html"> <img src="https://b-rodrigues.github.io/assets/img/bread-breakfast-bun-5678.jpg" width="640" height="360"></a>
</p>
</div>
<p>
First of all, is it heteros<strong>k</strong>edasticity or heteros<strong>c</strong>edasticity? According to <a href="https://www.jstor.org/stable/1911250">McCulloch (1985)</a>, heteros<strong>k</strong>edasticity is the proper spelling, because when transliterating Greek words, scientists use the Latin letter k in place of the Greek letter κ (kappa). κ sometimes is transliterated as the Latin letter c, but only when these words entered the English language through French, such as scepter.
</p>
<p>
Now that this is out of the way, we can get to the meat of this blogpost (foreshadowing pun). A random variable is said to be heteroskedastic, if its variance is not constant. For example, the variability of expenditures may increase with income. Richer families may spend a similar amount on groceries as poorer people, but some rich families will sometimes buy expensive items such as lobster. The variability of expenditures for rich families is thus quite large. However, the expenditures on food of poorer families, who cannot afford lobster, will not vary much. Heteroskedasticity can also appear when data is clustered; for example, variability of expenditures on food may vary from city to city, but is quite constant within a city.
</p>
<p>
To illustrate this, let’s first load all the packages needed for this blog post:
</p>
<pre class="r"><code>library(robustbase)
library(tidyverse)
library(sandwich)
library(lmtest)
library(modelr)
library(broom)</code></pre>
<p>
First, let’s load and prepare the data:
</p>
<pre class="r"><code>data("education")

education &lt;- education %&gt;% 
    rename(residents = X1,
           per_capita_income = X2,
           young_residents = X3,
           per_capita_exp = Y,
           state = State) %&gt;% 
    mutate(region = case_when(
        Region == 1 ~ "northeast",
        Region == 2 ~ "northcenter",
        Region == 3 ~ "south",
        Region == 4 ~ "west"
    )) %&gt;% 
    select(-Region)</code></pre>
<p>
I will be using the <code>education</code> data set from the <code>{robustbase}</code> package. I renamed some columns and changed the values of the <code>Region</code> column. Now, let’s do a scatterplot of per capita expenditures on per capita income:
</p>
<pre class="r"><code>ggplot(education, aes(per_capita_income, per_capita_exp)) + 
    geom_point() +
    theme_dark()</code></pre>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/rob_stderr-4-1.png" class="img-fluid"></p>
</div>
<p>
It would seem that, as income increases, variability of expenditures increases too. Let’s look at the same plot by <code>region</code>:
</p>
<pre class="r"><code>ggplot(education, aes(per_capita_income, per_capita_exp)) + 
    geom_point() + 
    facet_wrap(~region) + 
    theme_dark()</code></pre>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/rob_stderr-5-1.png" class="img-fluid"></p>
</div>
<p>
I don’t think this shows much; it would seem that observations might be clustered, but there are not enough observations to draw any conclusion from this plot (in any case, drawing conclusions from only plots is dangerous).
</p>
<p>
Let’s first run a good ol’ linear regression:
</p>
<pre class="r"><code>lmfit &lt;- lm(per_capita_exp ~ region + residents + young_residents + per_capita_income, data = education)

summary(lmfit)</code></pre>
<pre><code>## 
## Call:
## lm(formula = per_capita_exp ~ region + residents + young_residents + 
##     per_capita_income, data = education)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -77.963 -25.499  -2.214  17.618  89.106 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       -467.40283  142.57669  -3.278 0.002073 ** 
## regionnortheast     15.72741   18.16260   0.866 0.391338    
## regionsouth          7.08742   17.29950   0.410 0.684068    
## regionwest          34.32416   17.49460   1.962 0.056258 .  
## residents           -0.03456    0.05319  -0.650 0.519325    
## young_residents      1.30146    0.35717   3.644 0.000719 ***
## per_capita_income    0.07204    0.01305   5.520 1.82e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 39.88 on 43 degrees of freedom
## Multiple R-squared:  0.6292, Adjusted R-squared:  0.5774 
## F-statistic: 12.16 on 6 and 43 DF,  p-value: 6.025e-08</code></pre>
<p>
Let’s test for heteroskedasticity using the Breusch-Pagan test that you can find in the <code>{lmtest}</code> package:
</p>
<pre class="r"><code>bptest(lmfit)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  lmfit
## BP = 17.921, df = 6, p-value = 0.006432</code></pre>
<p>
This test shows that we can reject the null that the variance of the residuals is constant, thus heteroskedacity is present. To get the correct standard errors, we can use the <code>vcovHC()</code> function from the <code>{sandwich}</code> package (hence the choice for the header picture of this post):
</p>
<pre class="r"><code>lmfit %&gt;% 
    vcovHC() %&gt;% 
    diag() %&gt;% 
    sqrt()</code></pre>
<pre><code>##       (Intercept)   regionnortheast       regionsouth        regionwest 
##      311.31088691       25.30778221       23.56106307       24.12258706 
##         residents   young_residents per_capita_income 
##        0.09184368        0.68829667        0.02999882</code></pre>
<p>
By default <code>vcovHC()</code> estimates a heteroskedasticity consistent (HC) variance covariance matrix for the parameters. There are several ways to estimate such a HC matrix, and by default <code>vcovHC()</code> estimates the “HC3” one. You can refer to <a href="https://www.jstatsoft.org/article/view/v011i10">Zeileis (2004)</a> for more details.
</p>
<p>
We see that the standard errors are much larger than before! The intercept and <code>regionwest</code> variables are not statistically significant anymore.
</p>
<p>
You can achieve the same in one single step:
</p>
<pre class="r"><code>coeftest(lmfit, vcov = vcovHC(lmfit))</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##                      Estimate  Std. Error t value Pr(&gt;|t|)  
## (Intercept)       -467.402827  311.310887 -1.5014  0.14056  
## regionnortheast     15.727405   25.307782  0.6214  0.53759  
## regionsouth          7.087424   23.561063  0.3008  0.76501  
## regionwest          34.324157   24.122587  1.4229  0.16198  
## residents           -0.034558    0.091844 -0.3763  0.70857  
## young_residents      1.301458    0.688297  1.8908  0.06540 .
## per_capita_income    0.072036    0.029999  2.4013  0.02073 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
<p>
It’s is also easy to change the estimation method for the variance-covariance matrix:
</p>
<pre class="r"><code>coeftest(lmfit, vcov = vcovHC(lmfit, type = "HC0"))</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##                      Estimate  Std. Error t value  Pr(&gt;|t|)    
## (Intercept)       -467.402827  172.577569 -2.7084  0.009666 ** 
## regionnortheast     15.727405   20.488148  0.7676  0.446899    
## regionsouth          7.087424   17.755889  0.3992  0.691752    
## regionwest          34.324157   19.308578  1.7777  0.082532 .  
## residents           -0.034558    0.054145 -0.6382  0.526703    
## young_residents      1.301458    0.387743  3.3565  0.001659 ** 
## per_capita_income    0.072036    0.016638  4.3296 8.773e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
<p>
As I wrote above, by default, the <code>type</code> argument is equal to “HC3”.
</p>
<p>
Another way of dealing with heteroskedasticity is to use the <code>lmrob()</code> function from the <code>{robustbase}</code> package. This package is quite interesting, and offers quite a lot of functions for robust linear, and nonlinear, regression models. Running a robust linear regression is just the same as with <code>lm()</code>:
</p>
<pre class="r"><code>lmrobfit &lt;- lmrob(per_capita_exp ~ region + residents + young_residents + per_capita_income, 
                  data = education)

summary(lmrobfit)</code></pre>
<pre><code>## 
## Call:
## lmrob(formula = per_capita_exp ~ region + residents + young_residents + per_capita_income, 
##     data = education)
##  \--&gt; method = "MM"
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -57.074 -14.803  -0.853  24.154 174.279 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)       -156.37169  132.73828  -1.178  0.24526   
## regionnortheast     20.64576   26.45378   0.780  0.43940   
## regionsouth         10.79695   29.42746   0.367  0.71549   
## regionwest          45.22589   33.07950   1.367  0.17867   
## residents            0.03406    0.04412   0.772  0.44435   
## young_residents      0.57896    0.25512   2.269  0.02832 * 
## per_capita_income    0.04328    0.01442   3.000  0.00447 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Robust residual standard error: 26.4 
## Multiple R-squared:  0.6235, Adjusted R-squared:  0.571 
## Convergence in 24 IRWLS iterations
## 
## Robustness weights: 
##  observation 50 is an outlier with |weight| = 0 ( &lt; 0.002); 
##  7 weights are ~= 1. The remaining 42 ones are summarized as
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## 0.05827 0.85200 0.93870 0.85250 0.98700 0.99790 
## Algorithmic parameters: 
##        tuning.chi                bb        tuning.psi        refine.tol 
##         1.548e+00         5.000e-01         4.685e+00         1.000e-07 
##           rel.tol         scale.tol         solve.tol       eps.outlier 
##         1.000e-07         1.000e-10         1.000e-07         2.000e-03 
##             eps.x warn.limit.reject warn.limit.meanrw 
##         1.071e-08         5.000e-01         5.000e-01 
##      nResample         max.it       best.r.s       k.fast.s          k.max 
##            500             50              2              1            200 
##    maxit.scale      trace.lev            mts     compute.rd fast.s.large.n 
##            200              0           1000              0           2000 
##                   psi           subsampling                   cov 
##            "bisquare"         "nonsingular"         ".vcov.avar1" 
## compute.outlier.stats 
##                  "SM" 
## seed : int(0)</code></pre>
<p>
This however, gives you different estimates than when fitting a linear regression model. The estimates should be the same, only the standard errors should be different. This is because the estimation method is different, and is also robust to outliers (at least that’s my understanding, I haven’t read the theoretical papers behind the package yet).
</p>
<p>
Finally, it is also possible to bootstrap the standard errors. For this I will use the <code>bootstrap()</code> function from the <code>{modelr}</code> package:
</p>
<pre class="r"><code>resamples &lt;- 100

boot_education &lt;- education %&gt;% 
 modelr::bootstrap(resamples)</code></pre>
<p>
Let’s take a look at the <code>boot_education</code> object:
</p>
<pre class="r"><code>boot_education</code></pre>
<pre><code>## # A tibble: 100 x 2
##    strap      .id  
##    &lt;list&gt;     &lt;chr&gt;
##  1 &lt;resample&gt; 001  
##  2 &lt;resample&gt; 002  
##  3 &lt;resample&gt; 003  
##  4 &lt;resample&gt; 004  
##  5 &lt;resample&gt; 005  
##  6 &lt;resample&gt; 006  
##  7 &lt;resample&gt; 007  
##  8 &lt;resample&gt; 008  
##  9 &lt;resample&gt; 009  
## 10 &lt;resample&gt; 010  
## # … with 90 more rows</code></pre>
<p>
The column <code>strap</code> contains resamples of the original data. I will run my linear regression from before on each of the resamples:
</p>
<pre class="r"><code>(
    boot_lin_reg &lt;- boot_education %&gt;% 
        mutate(regressions = 
                   map(strap, 
                       ~lm(per_capita_exp ~ region + residents + 
                               young_residents + per_capita_income, 
                           data = .))) 
)</code></pre>
<pre><code>## # A tibble: 100 x 3
##    strap      .id   regressions
##    &lt;list&gt;     &lt;chr&gt; &lt;list&gt;     
##  1 &lt;resample&gt; 001   &lt;lm&gt;       
##  2 &lt;resample&gt; 002   &lt;lm&gt;       
##  3 &lt;resample&gt; 003   &lt;lm&gt;       
##  4 &lt;resample&gt; 004   &lt;lm&gt;       
##  5 &lt;resample&gt; 005   &lt;lm&gt;       
##  6 &lt;resample&gt; 006   &lt;lm&gt;       
##  7 &lt;resample&gt; 007   &lt;lm&gt;       
##  8 &lt;resample&gt; 008   &lt;lm&gt;       
##  9 &lt;resample&gt; 009   &lt;lm&gt;       
## 10 &lt;resample&gt; 010   &lt;lm&gt;       
## # … with 90 more rows</code></pre>
<p>
I have added a new column called <code>regressions</code> which contains the linear regressions on each bootstrapped sample. Now, I will create a list of tidied regression results:
</p>
<pre class="r"><code>(
    tidied &lt;- boot_lin_reg %&gt;% 
        mutate(tidy_lm = 
                   map(regressions, broom::tidy))
)</code></pre>
<pre><code>## # A tibble: 100 x 4
##    strap      .id   regressions tidy_lm         
##    &lt;list&gt;     &lt;chr&gt; &lt;list&gt;      &lt;list&gt;          
##  1 &lt;resample&gt; 001   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;
##  2 &lt;resample&gt; 002   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;
##  3 &lt;resample&gt; 003   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;
##  4 &lt;resample&gt; 004   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;
##  5 &lt;resample&gt; 005   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;
##  6 &lt;resample&gt; 006   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;
##  7 &lt;resample&gt; 007   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;
##  8 &lt;resample&gt; 008   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;
##  9 &lt;resample&gt; 009   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;
## 10 &lt;resample&gt; 010   &lt;lm&gt;        &lt;tibble [7 × 5]&gt;
## # … with 90 more rows</code></pre>
<p>
<code>broom::tidy()</code> creates a data frame of the regression results. Let’s look at one of these:
</p>
<pre class="r"><code>tidied$tidy_lm[[1]]</code></pre>
<pre><code>## # A tibble: 7 x 5
##   term              estimate std.error statistic  p.value
##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)       -571.     109.        -5.22  4.92e- 6
## 2 regionnortheast    -48.0     17.2       -2.80  7.71e- 3
## 3 regionsouth        -21.3     15.1       -1.41  1.66e- 1
## 4 regionwest           1.88    13.9        0.135 8.93e- 1
## 5 residents           -0.134    0.0608    -2.21  3.28e- 2
## 6 young_residents      1.50     0.308      4.89  1.47e- 5
## 7 per_capita_income    0.100    0.0125     8.06  3.85e-10</code></pre>
<p>
This format is easier to handle than the standard <code>lm()</code> output:
</p>
<pre class="r"><code>tidied$regressions[[1]]</code></pre>
<pre><code>## 
## Call:
## lm(formula = per_capita_exp ~ region + residents + young_residents + 
##     per_capita_income, data = .)
## 
## Coefficients:
##       (Intercept)    regionnortheast        regionsouth  
##         -571.0568           -48.0018           -21.3019  
##        regionwest          residents    young_residents  
##            1.8808            -0.1341             1.5042  
## per_capita_income  
##            0.1005</code></pre>
<p>
Now that I have all these regression results, I can compute any statistic I need. But first, let’s transform the data even further:
</p>
<pre class="r"><code>list_mods &lt;- tidied %&gt;% 
    pull(tidy_lm)</code></pre>
<p>
<code>list_mods</code> is a list of the <code>tidy_lm</code> data frames. I now add an index and bind the rows together (by using <code>map2_df()</code> instead of <code>map2()</code>):
</p>
<pre class="r"><code>mods_df &lt;- map2_df(list_mods, 
                   seq(1, resamples), 
                   ~mutate(.x, resample = .y))</code></pre>
<p>
Let’s take a look at the final object:
</p>
<pre class="r"><code>head(mods_df, 25)</code></pre>
<pre><code>## # A tibble: 25 x 6
##    term              estimate std.error statistic  p.value resample
##    &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;
##  1 (Intercept)       -571.     109.        -5.22  4.92e- 6        1
##  2 regionnortheast    -48.0     17.2       -2.80  7.71e- 3        1
##  3 regionsouth        -21.3     15.1       -1.41  1.66e- 1        1
##  4 regionwest           1.88    13.9        0.135 8.93e- 1        1
##  5 residents           -0.134    0.0608    -2.21  3.28e- 2        1
##  6 young_residents      1.50     0.308      4.89  1.47e- 5        1
##  7 per_capita_income    0.100    0.0125     8.06  3.85e-10        1
##  8 (Intercept)        -97.2    145.        -0.672 5.05e- 1        2
##  9 regionnortheast     -1.48    10.8       -0.136 8.92e- 1        2
## 10 regionsouth         12.5     11.4        1.09  2.82e- 1        2
## # … with 15 more rows</code></pre>
<p>
Now this is a very useful format, because I now can group by the <code>term</code> column and compute any statistics I need, in the present case the standard deviation:
</p>
<pre class="r"><code>(
    r.std.error &lt;- mods_df %&gt;% 
        group_by(term) %&gt;% 
        summarise(r.std.error = sd(estimate))
)</code></pre>
<pre><code>## # A tibble: 7 x 2
##   term              r.std.error
##   &lt;chr&gt;                   &lt;dbl&gt;
## 1 (Intercept)          220.    
## 2 per_capita_income      0.0197
## 3 regionnortheast       24.5   
## 4 regionsouth           21.1   
## 5 regionwest            22.7   
## 6 residents              0.0607
## 7 young_residents        0.498</code></pre>
<p>
We can append this column to the linear regression model result:
</p>
<pre class="r"><code>lmfit %&gt;% 
    broom::tidy() %&gt;% 
    full_join(r.std.error) %&gt;% 
    select(term, estimate, std.error, r.std.error)</code></pre>
<pre><code>## Joining, by = "term"</code></pre>
<pre><code>## # A tibble: 7 x 4
##   term               estimate std.error r.std.error
##   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;
## 1 (Intercept)       -467.      143.        220.    
## 2 regionnortheast     15.7      18.2        24.5   
## 3 regionsouth          7.09     17.3        21.1   
## 4 regionwest          34.3      17.5        22.7   
## 5 residents           -0.0346    0.0532      0.0607
## 6 young_residents      1.30      0.357       0.498 
## 7 per_capita_income    0.0720    0.0131      0.0197</code></pre>
<p>
As you see, using the whole bootstrapping procedure is longer than simply using either one of the first two methods. However, this procedure is very flexible and can thus be adapted to a very large range of situations. Either way, in the case of heteroskedasticity, you can see that results vary a lot depending on the procedure you use, so I would advise to use them all as robustness tests and discuss the differences.
</p>



 ]]></description>
  <category>R</category>
  <category>econometrics</category>
  <guid>https://b-rodrigues.github.io/posts/2018-07-08-rob_stderr.html</guid>
  <pubDate>Sun, 08 Jul 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Missing data imputation and instrumental variables regression: the tidy approach</title>
  <link>https://b-rodrigues.github.io/posts/2018-07-01-tidy_ive.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=o5S7CreWiBY/"> <img src="https://b-rodrigues.github.io/assets/img/trumpet_boy.jpg" width="640" height="360"></a>
</p>
</div>
<p>
In this blog post I will discuss missing data imputation and instrumental variables regression. This is based on a short presentation I will give at my job. You can find the data used here on this website: <a href="http://eclr.humanities.manchester.ac.uk/index.php/IV_in_R" class="uri">http://eclr.humanities.manchester.ac.uk/index.php/IV_in_R</a>
</p>
<p>
The data is used is from Wooldridge’s book, <em>Econometrics: A modern Approach</em>. You can download the data by clicking <a href="http://eclr.humanities.manchester.ac.uk/images/5/5f/Mroz.csv">here</a>.
</p>
<p>
This is the variable description:
</p>
<pre><code> 1. inlf                     =1 if in labor force, 1975
 2. hours                    hours worked, 1975
 3. kidslt6                  # kids &lt; 6 years
 4. kidsge6                  # kids 6-18
 5. age                      woman's age in yrs
 6. educ                     years of schooling
 7. wage                     estimated wage from earns., hours
 8. repwage                  reported wage at interview in 1976
 9. hushrs                   hours worked by husband, 1975
10. husage                   husband's age
11. huseduc                  husband's years of schooling
12. huswage                  husband's hourly wage, 1975
13. faminc                   family income, 1975
14. mtr                      fed. marginal tax rate facing woman
15. motheduc                 mother's years of schooling
16. fatheduc                 father's years of schooling
17. unem                     unem. rate in county of resid.
18. city                     =1 if live in SMSA
19. exper                    actual labor mkt exper
20. nwifeinc                 (faminc - wage*hours)/1000
21. lwage                    log(wage)
22. expersq                  exper^2</code></pre>
<p>
The goal is to first impute missing data in the data set, and then determine the impact of one added year of education on wages. If one simply ignores missing values, bias can be introduced depending on the missingness mechanism. The second problem here is that education is likely to be endogeneous (and thus be correlated to the error term), as it is not randomly assigned. This causes biased estimates and may lead to seriously wrong conclusions. So missingness and endogeneity should be dealt with, but dealing with both issues is more of a programming challenge than an econometrics challenge. Thankfully, the packages contained in the <code>{tidyverse}</code> as well as <code>{mice}</code> will save the day!
</p>
<p>
If you inspect the data, you will see that there are no missing values. So I will use the <code>{mice}</code> package to first <em>ampute</em> the data (which means adding missing values). This, of course, is done for education purposes. If you’re lucky enough to not have missing values in your data, you shouldn’t add them!
</p>
<p>
Let’s load all the packages needed:
</p>
<pre class="r"><code>library(tidyverse)
library(AER)
library(naniar)
library(mice)</code></pre>
<p>
So first, let’s read in the data, and ampute it:
</p>
<pre class="r"><code>wages_data &lt;- read_csv("http://eclr.humanities.manchester.ac.uk/images/5/5f/Mroz.csv")</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   .default = col_integer(),
##   wage = col_character(),
##   repwage = col_double(),
##   huswage = col_double(),
##   mtr = col_double(),
##   unem = col_double(),
##   nwifeinc = col_double(),
##   lwage = col_character()
## )</code></pre>
<pre><code>## See spec(...) for full column specifications.</code></pre>
<p>
First, I only select the variables I want to use and convert them to the correct class:
</p>
<pre class="r"><code>wages_data &lt;- wages_data %&gt;% 
    select(wage, educ, fatheduc, motheduc, inlf, hours, 
               kidslt6, kidsge6, age, huswage, 
               mtr, unem, city, exper) %&gt;% 
    mutate_at(vars(kidslt6, kidsge6, hours, educ, age, wage, huswage, mtr,
                    motheduc, fatheduc, unem, exper), as.numeric) %&gt;% 
    mutate_at(vars(city, inlf), as.character)</code></pre>
<pre><code>## Warning in evalq(as.numeric(wage), &lt;environment&gt;): NAs introduced by
## coercion</code></pre>
<p>
In the data, some women are not in the labour force, and thus do not have any wages; meaning they should have a 0 there. Instead, this is represented with the following symbol: “.”. So I convert these dots to 0. One could argue that the wages should not be 0, but that they’re truly missing. This is true, and there are ways to deal with such questions (Heckman’s selection model for instance), but this is not the point here.
</p>
<pre class="r"><code>wages_data &lt;- wages_data %&gt;% 
    mutate(wage = ifelse(is.na(wage), 0, wage))</code></pre>
<p>
Let’s double check if there are any missing values in the data, using <code>naniar::vis_miss()</code>:
</p>
<pre class="r"><code>vis_miss(wages_data)</code></pre>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/tidy_ive-6-1.png" class="img-fluid"></p>
</div>
<p>
Nope! Let’s ampute it:
</p>
<pre class="r"><code>wages_mis &lt;- ampute(wages_data)$amp</code></pre>
<pre><code>## Warning: Data is made numeric because the calculation of weights requires
## numeric data</code></pre>
<p>
<code>ampute()</code> returns an object where the <code>amp</code> element is the amputed data. This is what I save into the new variable <code>wages_mis</code>.
</p>
<p>
Let’s take a look:
</p>
<pre class="r"><code>vis_miss(wages_mis)</code></pre>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/tidy_ive-8-1.png" class="img-fluid"></p>
</div>
<p>
Ok, so now we have missing values. Let’s use the recently added <code>mice::parlmice()</code> function to impute the dataset, in parallel:
</p>
<pre class="r"><code>imp_wages &lt;- parlmice(data = wages_mis, m = 10, maxit = 20, cl.type = "FORK")</code></pre>
<p>
For reproducibility, I save these objects to disk:
</p>
<pre class="r"><code>write_csv(wages_mis, "wages_mis.csv")

saveRDS(imp_wages, "imp_wages.rds")</code></pre>
<p>
As a sanity check, let’s look at the missingness pattern for the first completed dataset:
</p>
<pre class="r"><code>vis_miss(complete(imp_wages))</code></pre>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/tidy_ive-11-1.png" class="img-fluid"></p>
</div>
<p>
<code>mice::parlmice()</code> was able to impute the dataset. I imputed it 10 times, so now I have 10 imputed datasets. If I want to estimate a model using this data, I will need to do so 10 times. This is where the tidyverse comes into play. First, let’s combine all the 10 imputed datasets into one long dataset, with an index to differentiate them. This is done easily with <code>mice::complete()</code>:
</p>
<pre class="r"><code>imp_wages_df &lt;- mice::complete(imp_wages, "long")</code></pre>
<p>
Let’s take a look at the data:
</p>
<pre class="r"><code>head(imp_wages_df)</code></pre>
<pre><code>##   .imp .id   wage educ fatheduc motheduc inlf hours kidslt6 kidsge6 age
## 1    1   1 3.3540   12        7       12    1  1610       1       0  32
## 2    1   2 1.3889   12        7        7    1  1656       0       2  30
## 3    1   3 4.5455   12        7       12    1  1980       0       3  35
## 4    1   4 1.0965   12        7        7    1   456       0       3  34
## 5    1   5 4.5918   14       14       12    1  1568       1       2  31
## 6    1   6 4.7421   12        7       14    1  2032       0       0  54
##   huswage    mtr unem city exper
## 1  4.0288 0.7215  5.0    0    14
## 2  8.4416 0.6615 11.0    1     5
## 3  3.5807 0.6915  5.0    0    15
## 4  3.5417 0.7815  5.0    0     6
## 5 10.0000 0.6215  9.5    1    14
## 6  4.7364 0.6915  7.5    1    33</code></pre>
<p>
As you can see, there are two new columns, <code>.id</code> and <code>.imp</code>. <code>.imp</code> equals <code>i</code> means that it is the <code>i</code>th imputed dataset.
</p>
<p>
Because I have 0’s in my dependent variable, I will not log the wages but instead use the Inverse Hyperbolic Sine transformation. Marc F. Bellemare wrote a nice post about it <a href="http://marcfbellemare.com/wordpress/12856">here</a>.
</p>
<pre class="r"><code>ihs &lt;- function(x){
    log(x + sqrt(x**2 + 1))
}</code></pre>
<p>
I can now apply this function, but first I have to group by <code>.imp</code>. Remember, these are 10 separated datasets. I also create the experience squared:
</p>
<pre class="r"><code>imp_wages_df &lt;- imp_wages_df %&gt;% 
    group_by(.imp) %&gt;% 
    mutate(ihs_wage = ihs(wage),
           exper2 = exper**2)</code></pre>
<p>
Now comes some tidyverse magic. I will create a new dataset by using the <code>nest()</code> function from <code>tidyr</code>.
</p>
<pre class="r"><code>(imp_wages &lt;- imp_wages_df %&gt;% 
    group_by(.imp) %&gt;% 
    nest())</code></pre>
<pre><code>## # A tibble: 10 x 2
##     .imp data               
##    &lt;int&gt; &lt;list&gt;             
##  1     1 &lt;tibble [753 × 17]&gt;
##  2     2 &lt;tibble [753 × 17]&gt;
##  3     3 &lt;tibble [753 × 17]&gt;
##  4     4 &lt;tibble [753 × 17]&gt;
##  5     5 &lt;tibble [753 × 17]&gt;
##  6     6 &lt;tibble [753 × 17]&gt;
##  7     7 &lt;tibble [753 × 17]&gt;
##  8     8 &lt;tibble [753 × 17]&gt;
##  9     9 &lt;tibble [753 × 17]&gt;
## 10    10 &lt;tibble [753 × 17]&gt;</code></pre>
<p>
As you can see, <code>imp_wages</code> is now a dataset with two columns: <code>.imp</code>, indexing the imputed datasets, and a column called <code>data</code>, where each element is itself a tibble! <code>data</code> is a so-called list-column. You can read more about it on the <a href="https://jennybc.github.io/purrr-tutorial/ls13_list-columns.html"><code>purrr</code> tutorial</a> written by <a href="https://twitter.com/JennyBryan">Jenny Bryan</a>.
</p>
<p>
Estimating a model now is easy, if you’re familiar with <code>purrr</code>. This is how you do it:
</p>
<pre class="r"><code>imp_wages_reg = imp_wages %&gt;% 
    mutate(lin_reg = map(data, 
                         ~lm(ihs_wage ~ educ + inlf + hours + 
                                 kidslt6 + kidsge6 + age + huswage + 
                                 mtr + unem + city + exper + exper2, 
                             data = .)))</code></pre>
<p>
Ok, so what happened here? <code>imp_wages</code> is a data frame, so it’s possible to add a column to it with <code>mutate()</code>. I call that column <code>lin_reg</code> and use <code>map()</code> on the column called <code>data</code> (remember, this column is actually a list of data frame objects, and <code>map()</code> takes a list as an argument, and then a function or formula) with the following formula:
</p>
<pre class="r"><code>~lm(ihs_wage ~ educ + inlf + hours + 
        kidslt6 + kidsge6 + age + huswage + 
        mtr + unem + city + exper + exper2, 
    data = .)</code></pre>
<p>
This formula is nothing more that a good old linear regression. The last line <code>data = .</code> means that the data to be used inside <code>lm()</code> should be coming from the list called <code>data</code>, which is the second column of <code>imp_wages</code>. As I’m writing these lines, I realize it is confusing as hell. But I promise you that learning to use <code>purrr</code> is a bit like learning how to use a bicycle. Very difficult to explain, but once you know how to do it, it feels super natural. Take some time to play with the lines above to really understand what happened.
</p>
<p>
Now, let’s take a look at the result:
</p>
<pre class="r"><code>imp_wages_reg</code></pre>
<pre><code>## # A tibble: 10 x 3
##     .imp data                lin_reg
##    &lt;int&gt; &lt;list&gt;              &lt;list&gt; 
##  1     1 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   
##  2     2 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   
##  3     3 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   
##  4     4 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   
##  5     5 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   
##  6     6 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   
##  7     7 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   
##  8     8 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   
##  9     9 &lt;tibble [753 × 17]&gt; &lt;lm&gt;   
## 10    10 &lt;tibble [753 × 17]&gt; &lt;lm&gt;</code></pre>
<p>
<code>imp_wages_reg</code> now has a third column called <code>lin_reg</code> where each element is a linear model, estimated on the data from the <code>data</code> column! We can now pool the results of these 10 regressions using <code>mice::pool()</code>:
</p>
<pre class="r"><code>pool_lin_reg &lt;- pool(imp_wages_reg$lin_reg)

summary(pool_lin_reg)</code></pre>
<pre><code>##                  estimate    std.error  statistic       df      p.value
## (Intercept)  1.2868701172 3.214473e-01  4.0033628 737.9337 6.876133e-05
## educ         0.0385310276 8.231906e-03  4.6806931 737.9337 3.401935e-06
## inlf         1.8845418354 5.078235e-02 37.1101707 737.9337 0.000000e+00
## hours       -0.0001164143 3.011378e-05 -3.8658143 737.9337 1.204773e-04
## kidslt6     -0.0438925013 3.793152e-02 -1.1571510 737.9337 2.475851e-01
## kidsge6     -0.0117978229 1.405226e-02 -0.8395678 737.9337 4.014227e-01
## age         -0.0030084595 2.666614e-03 -1.1281946 737.9337 2.596044e-01
## huswage     -0.0231736955 5.607364e-03 -4.1327255 737.9337 3.995866e-05
## mtr         -2.2109176781 3.188827e-01 -6.9333267 737.9337 8.982592e-12
## unem         0.0028775444 5.462973e-03  0.5267360 737.9337 5.985352e-01
## city         0.0157414671 3.633755e-02  0.4332011 737.9337 6.649953e-01
## exper        0.0164364027 6.118875e-03  2.6861806 737.9337 7.389936e-03
## exper2      -0.0002022602 1.916146e-04 -1.0555575 737.9337 2.915159e-01</code></pre>
<p>
This function averages the results from the 10 regressions and computes correct standard errors. This is based on Rubin’s rules (Rubin, 1987, p.&nbsp;76). As you can see, the linear regression indicates that one year of added education has a positive, significant effect of log wages (they’re not log wages, I used the IHS transformation, but <em>log wages</em> just sounds better than <em>inverted hyperbolic sined wages</em>). This effect is almost 4%.
</p>
<p>
But education is not randomly assigned, and as such might be endogenous. This is where instrumental variables come into play. An instrument is a variables that impacts the dependent variable only through the endogenous variable (here, education). For example, the education of the parents do not have a direct impact over one’s wage, but having college-educated parents means that you are likely college-educated yourself, and thus have a higher wage that if you only have a high school diploma.
</p>
<p>
I am thus going to instrument education with both parents’ education:
</p>
<pre class="r"><code>imp_wages_reg = imp_wages_reg %&gt;% 
    mutate(iv_reg = map(data, 
                         ~ivreg(ihs_wage ~ educ + inlf + hours + 
                                 kidslt6 + kidsge6 + age + huswage + 
                                 mtr + unem + city + exper + exper2 |.-educ + fatheduc + motheduc, 
                             data = .)))</code></pre>
<p>
The only difference from before is the formula:
</p>
<pre class="r"><code>~ivreg(ihs_wage ~ educ + inlf + hours + 
           kidslt6 + kidsge6 + age + huswage + 
           mtr + unem + city + exper + exper2 |.-educ + fatheduc + motheduc, 
       data = .)</code></pre>
<pre><code>## ~ivreg(ihs_wage ~ educ + inlf + hours + kidslt6 + kidsge6 + age + 
##     huswage + mtr + unem + city + exper + exper2 | . - educ + 
##     fatheduc + motheduc, data = .)</code></pre>
<p>
Instead of <code>lm()</code> I use <code>AER::ivreg()</code> and the formula has a second part, after the <code>|</code> symbol. This is where I specify that I instrument education with the parents’ education.
</p>
<p>
<code>imp_wages_reg</code> now looks like this:
</p>
<pre class="r"><code>imp_wages_reg</code></pre>
<pre><code>## # A tibble: 10 x 4
##     .imp data                lin_reg iv_reg 
##    &lt;int&gt; &lt;list&gt;              &lt;list&gt;  &lt;list&gt; 
##  1     1 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;
##  2     2 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;
##  3     3 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;
##  4     4 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;
##  5     5 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;
##  6     6 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;
##  7     7 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;
##  8     8 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;
##  9     9 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;
## 10    10 &lt;tibble [753 × 17]&gt; &lt;lm&gt;    &lt;ivreg&gt;</code></pre>
<p>
Let’s take a look at the results:
</p>
<pre class="r"><code>pool_iv_reg &lt;- pool(imp_wages_reg$iv_reg)

summary(pool_iv_reg)</code></pre>
<pre><code>##                  estimate    std.error  statistic       df      p.value
## (Intercept)  2.0091904157 5.146812e-01  3.9037568 737.9337 1.033832e-04
## educ         0.0038859137 2.086592e-02  0.1862326 737.9337 8.523136e-01
## inlf         1.9200207113 5.499457e-02 34.9129122 737.9337 0.000000e+00
## hours       -0.0001313866 3.157375e-05 -4.1612608 737.9337 3.537881e-05
## kidslt6     -0.0234593391 4.000689e-02 -0.5863824 737.9337 5.577979e-01
## kidsge6     -0.0123239220 1.422241e-02 -0.8665145 737.9337 3.864897e-01
## age         -0.0040874625 2.763340e-03 -1.4791748 737.9337 1.395203e-01
## huswage     -0.0242737100 5.706497e-03 -4.2536970 737.9337 2.373189e-05
## mtr         -2.6385172445 3.998419e-01 -6.5989008 737.9337 7.907430e-11
## unem         0.0047331976 5.622137e-03  0.8418859 737.9337 4.001246e-01
## city         0.0255647706 3.716783e-02  0.6878197 737.9337 4.917824e-01
## exper        0.0180917073 6.258779e-03  2.8906127 737.9337 3.957817e-03
## exper2      -0.0002291007 1.944599e-04 -1.1781381 737.9337 2.391213e-01</code></pre>
<p>
As you can see, education is not statistically significant anymore! This is why it is quite important to think about endogeneity issues. However, it is not always very easy to find suitable instruments. A series of tests exist to determine if you have relevant and strong instruments, but this blog post is already long enough. I will leave this for a future blog post.
</p>



 ]]></description>
  <category>R</category>
  <category>econometrics</category>
  <guid>https://b-rodrigues.github.io/posts/2018-07-01-tidy_ive.html</guid>
  <pubDate>Sun, 01 Jul 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Forecasting my weight with R</title>
  <link>https://b-rodrigues.github.io/posts/2018-06-24-fun_ts.html</link>
  <description><![CDATA[ 




<p>
I’ve been measuring my weight almost daily for almost 2 years now; I actually started earlier, but not as consistently. The goal of this blog post is to get re-acquaiented with time series; I haven’t had the opportunity to work with time series for a long time now and I have seen that quite a few packages that deal with time series have been released on CRAN. In this blog post, I will explore my weight measurements using some functions from the <code>{tsibble}</code> and <code>{tibbletime}</code> packages, and then do some predictions with the <code>{forecast}</code> package.
</p>
<p>
First, let’s load the needed packages, read in the data and convert it to a <code>tsibble</code>:
</p>
<pre class="r"><code>library("tidyverse")
library("readr")
library("forecast")
library("tsibble")
library("tibbletime")
library("mice")</code></pre>
<pre class="r"><code>weight &lt;- read_csv("https://gist.githubusercontent.com/b-rodrigues/ea60679135f8dbed448ccf66a216811f/raw/18b469f3b0720f76ce5ee2715d0f9574b615f170/gistfile1.txt") %&gt;% 
    as_tsibble()</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   Date = col_date(format = ""),
##   Poids = col_double()
## )</code></pre>
<pre><code>## The `index` is `Date`.</code></pre>
<p>
You can read more about <code>{tsibble}</code> <a href="https://pkg.earo.me/tsibble/">here</a>. Here, I use <code>{tsibble}</code> mostly for the next step, which is using the function <code>fill_na()</code> on the tsibble. <code>fill_na()</code> turns implicit missing values into explicit missing values. These are implicit missing values:
</p>
<pre><code>          Date Poids
1   2013-01-01 84.10
2   2013-01-04 85.60</code></pre>
<p>
and this is the same view, but with explicit missing values:
</p>
<pre><code>          Date Poids
1   2013-01-01 84.10
2   2013-01-02 NA
3   2013-01-03 NA
4   2013-01-04 85.60</code></pre>
<p>
This is useful to do, because I want to impute the missing values using the <code>{mice}</code> package. Let’s do this:
</p>
<pre class="r"><code>weight &lt;- weight %&gt;% 
    fill_na()

imp_weight &lt;- mice(data = weight) %&gt;% 
    mice::complete("long")</code></pre>
<pre><code>## 
##  iter imp variable
##   1   1  Poids
##   1   2  Poids
##   1   3  Poids
##   1   4  Poids
##   1   5  Poids
##   2   1  Poids
##   2   2  Poids
##   2   3  Poids
##   2   4  Poids
##   2   5  Poids
##   3   1  Poids
##   3   2  Poids
##   3   3  Poids
##   3   4  Poids
##   3   5  Poids
##   4   1  Poids
##   4   2  Poids
##   4   3  Poids
##   4   4  Poids
##   4   5  Poids
##   5   1  Poids
##   5   2  Poids
##   5   3  Poids
##   5   4  Poids
##   5   5  Poids</code></pre>
<p>
Let’s take a look at <code>imp_weight</code>:
</p>
<pre class="r"><code>head(imp_weight)</code></pre>
<pre><code>##   .imp .id       Date Poids
## 1    1   1 2013-10-28  84.1
## 2    1   2 2013-10-29  84.4
## 3    1   3 2013-10-30  83.5
## 4    1   4 2013-10-31  84.1
## 5    1   5 2013-11-01  85.6
## 6    1   6 2013-11-02  85.2</code></pre>
<p>
Let’s select the relevant data. I filter from the 11th of July 2016, which is where I started weighing myself almost every day, to the 31st of May 2018. I want to predict my weight for the month of June (you might think of the month of June 2018 as the test data, and the rest as training data):
</p>
<pre class="r"><code>imp_weight_train &lt;- imp_weight %&gt;% 
    filter(Date &gt;= "2016-07-11", Date &lt;= "2018-05-31")</code></pre>
<p>
In the next lines, I create a column called <code>imputation</code> which is simply the same as the column <code>.imp</code> but of character class, remove unneeded columns and rename some other columns (“Poids” is French for weight):
</p>
<pre class="r"><code>imp_weight_train &lt;- imp_weight_train %&gt;% 
    mutate(imputation = as.character(.imp)) %&gt;% 
    select(-.id, -.imp) %&gt;% 
    rename(date = Date) %&gt;% 
    rename(weight = Poids)</code></pre>
<p>
Let’s take a look at the data:
</p>
<pre class="r"><code>ggplot(imp_weight_train, aes(date, weight, colour = imputation)) +
    geom_line() + 
    theme(legend.position = "bottom")</code></pre>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/fun_ts-8-1.png" class="img-fluid"></p>
</div>
<p>
This plots gives some info, but it might be better to smooth the lines. This is possible by computing a rolling mean. For this I will use the <code>rollify()</code> function of the <code>{tibbletime}</code> package:
</p>
<pre class="r"><code>mean_roll_5 &lt;- rollify(mean, window = 5)
mean_roll_10 &lt;- rollify(mean, window = 10)</code></pre>
<p>
<code>rollify()</code> can be seen as an adverb, pretty much like <code>purrr::safely()</code>; <code>rollify()</code> is a higher order function that literally rollifies a function, in this case <code>mean()</code> which means that rollifying the mean creates a function that returns the rolling mean. The <code>window</code> argument lets you decide how smooth you want the curve to be: the higher the smoother. However, you will lose some observations. Let’s use this functions to add the rolling means to the data frame:
</p>
<pre class="r"><code>imp_weight_train &lt;- imp_weight_train %&gt;% 
    group_by(imputation) %&gt;% 
    mutate(roll_5 = mean_roll_5(weight),
           roll_10 = mean_roll_10(weight))</code></pre>
<p>
Now, let’s plot these new curves:
</p>
<pre class="r"><code>ggplot(imp_weight_train, aes(date, roll_5, colour = imputation)) +
    geom_line() + 
    theme(legend.position = "bottom")</code></pre>
<pre><code>## Warning: Removed 20 rows containing missing values (geom_path).</code></pre>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/fun_ts-11-1.png" class="img-fluid"></p>
</div>
<pre class="r"><code>ggplot(imp_weight_train, aes(date, roll_10, colour = imputation)) +
    geom_line() + 
    theme(legend.position = "bottom")</code></pre>
<pre><code>## Warning: Removed 45 rows containing missing values (geom_path).</code></pre>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/fun_ts-11-2.png" class="img-fluid"></p>
</div>
<p>
That’s easier to read, isn’t it?
</p>
<p>
Now, I will use the <code>auto.arima()</code> function to train a model on the data to forecast my weight for the month of June. However, my data, <code>imp_weight_train</code> is a list of datasets. <code>auto.arima()</code> does not take a data frame as an argument, much less so a list of datasets. I’ll create a wrapper around <code>auto.arima()</code> that works on a dataset, and then map it to the list of datasets:
</p>
<pre class="r"><code>auto.arima.df &lt;- function(data, y, ...){

    y &lt;- enquo(y)

    yts &lt;- data %&gt;% 
        pull(!!y) %&gt;% 
        as.ts()

    auto.arima(yts, ...)
}</code></pre>
<p>
<code>auto.arima.df()</code> takes a data frame as argument, and then <code>y</code>, which is the column that contains the univariate time series. This column then gets pulled out of the data frame, converted to a time series object with <code>as.ts()</code>, and then passed down to <code>auto.arima()</code>. I can now use this function on my list of data sets. The first step is to nest the data:
</p>
<pre class="r"><code>nested_data &lt;- imp_weight_train %&gt;% 
    group_by(imputation) %&gt;% 
    nest() </code></pre>
<p>
Let’s take a look at <code>nested_data</code>:
</p>
<pre class="r"><code>nested_data</code></pre>
<pre><code>## # A tibble: 5 x 2
##   imputation data              
##   &lt;chr&gt;      &lt;list&gt;            
## 1 1          &lt;tibble [690 × 4]&gt;
## 2 2          &lt;tibble [690 × 4]&gt;
## 3 3          &lt;tibble [690 × 4]&gt;
## 4 4          &lt;tibble [690 × 4]&gt;
## 5 5          &lt;tibble [690 × 4]&gt;</code></pre>
<p>
<code>nested_data</code> is a tibble with a column called <code>data</code>, which is a so-called list-column. Each element of <code>data</code> is itself a tibble. This is a useful structure, because now I can map <code>auto.arima.df()</code> to the data frame:
</p>
<pre class="r"><code>models &lt;- nested_data %&gt;% 
    mutate(model = map(data, auto.arima.df, y = weight))</code></pre>
<p>
This trick can be a bit difficult to follow the first time you see it. The idea is the following: <code>nested_data</code> is a tibble. Thus, I can add a column to it using <code>mutate()</code>. So far so good. Now that I am “inside” the mutate call, I can use <code>purrr::map()</code>. Why? <code>purrr::map()</code> takes a list and then a function as arguments. Remember that <code>data</code> is a list column; you can see it above, the type of the column <code>data</code> is list. So <code>data</code> is a list, and thus can be used inside <code>purrr::map()</code>. Great. Now, what is inside <code>data</code>? tibbles, where inside each of them is a column called <code>weight</code>. This is the column that contains my univariate time series I want to model. Let’s take a look at <code>models</code>:
</p>
<pre class="r"><code>models</code></pre>
<pre><code>## # A tibble: 5 x 3
##   imputation data               model      
##   &lt;chr&gt;      &lt;list&gt;             &lt;list&gt;     
## 1 1          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;
## 2 2          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;
## 3 3          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;
## 4 4          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;
## 5 5          &lt;tibble [690 × 4]&gt; &lt;S3: ARIMA&gt;</code></pre>
<p>
<code>models</code> is a tibble with a column called <code>model</code>, where each element is a model of type <code>ARIMA</code>.
</p>
<p>
Adding forecasts is based on the same trick as above, and we use the <code>forecast()</code> function:
</p>
<pre class="r"><code>forecasts &lt;- models %&gt;% 
    mutate(predictions = map(model, forecast, h = 24)) %&gt;% 
    mutate(predictions = map(predictions, as_tibble)) %&gt;% 
    pull(predictions) </code></pre>
<p>
I forecast 24 days (I am writing this on the 24th of June), and convert the predictions to tibbles, and then pull only the predictions tibble:
</p>
<pre class="r"><code>forecasts</code></pre>
<pre><code>## [[1]]
## # A tibble: 24 x 5
##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`
##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1             71.5    70.7    72.3    70.2    72.8
##  2             71.5    70.7    72.4    70.3    72.8
##  3             71.5    70.6    72.3    70.1    72.8
##  4             71.5    70.6    72.4    70.1    72.9
##  5             71.4    70.5    72.4    70.0    72.9
##  6             71.5    70.5    72.4    70.0    72.9
##  7             71.4    70.5    72.4    69.9    72.9
##  8             71.4    70.4    72.4    69.9    72.9
##  9             71.4    70.4    72.4    69.9    72.9
## 10             71.4    70.4    72.4    69.8    73.0
## # ... with 14 more rows
## 
## [[2]]
## # A tibble: 24 x 5
##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`
##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1             71.6    70.8    72.3    70.3    72.8
##  2             71.6    70.8    72.5    70.3    72.9
##  3             71.5    70.6    72.4    70.2    72.9
##  4             71.5    70.6    72.5    70.1    72.9
##  5             71.5    70.5    72.5    70.0    73.0
##  6             71.5    70.5    72.5    70.0    73.0
##  7             71.5    70.5    72.5    69.9    73.0
##  8             71.5    70.4    72.5    69.9    73.1
##  9             71.5    70.4    72.5    69.8    73.1
## 10             71.4    70.3    72.6    69.7    73.1
## # ... with 14 more rows
## 
## [[3]]
## # A tibble: 24 x 5
##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`
##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1             71.6    70.8    72.4    70.4    72.8
##  2             71.5    70.7    72.4    70.2    72.8
##  3             71.5    70.6    72.4    70.2    72.9
##  4             71.5    70.6    72.4    70.1    72.9
##  5             71.5    70.5    72.4    70.0    72.9
##  6             71.5    70.5    72.4    70.0    73.0
##  7             71.5    70.5    72.5    69.9    73.0
##  8             71.4    70.4    72.5    69.9    73.0
##  9             71.4    70.4    72.5    69.8    73.0
## 10             71.4    70.4    72.5    69.8    73.1
## # ... with 14 more rows
## 
## [[4]]
## # A tibble: 24 x 5
##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`
##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1             71.5    70.8    72.3    70.3    72.8
##  2             71.5    70.7    72.4    70.3    72.8
##  3             71.5    70.7    72.4    70.2    72.8
##  4             71.5    70.6    72.4    70.1    72.9
##  5             71.5    70.6    72.4    70.1    72.9
##  6             71.5    70.5    72.5    70.0    73.0
##  7             71.5    70.5    72.5    69.9    73.0
##  8             71.5    70.4    72.5    69.9    73.0
##  9             71.4    70.4    72.5    69.8    73.1
## 10             71.4    70.3    72.5    69.8    73.1
## # ... with 14 more rows
## 
## [[5]]
## # A tibble: 24 x 5
##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`
##  *            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1             71.5    70.8    72.3    70.3    72.8
##  2             71.5    70.7    72.4    70.3    72.8
##  3             71.5    70.7    72.4    70.2    72.8
##  4             71.5    70.6    72.4    70.1    72.9
##  5             71.5    70.6    72.4    70.1    72.9
##  6             71.5    70.5    72.4    70.0    73.0
##  7             71.5    70.5    72.5    69.9    73.0
##  8             71.5    70.4    72.5    69.9    73.0
##  9             71.4    70.4    72.5    69.8    73.1
## 10             71.4    70.3    72.5    69.8    73.1
## # ... with 14 more rows</code></pre>
<p>
So <code>forecasts</code> is a list of tibble, each containing a forecast. Remember that I have 5 tibbles, because I imputed the data 5 times. I will merge this list of data sets together into one, but before I need to add a column that indices the forecasts:
</p>
<pre class="r"><code>forecasts &lt;- map2(.x = forecasts, .y = as.character(seq(1, 5)), 
     ~mutate(.x, id = .y)) %&gt;% 
    bind_rows() %&gt;% 
    select(-c(`Lo 80`, `Hi 80`))

colnames(forecasts) &lt;- c("point_forecast", "low_95", "hi_95", "id")</code></pre>
<p>
Let’s take a look again at <code>forecasts</code>:
</p>
<pre class="r"><code>forecasts</code></pre>
<pre><code>## # A tibble: 120 x 4
##    point_forecast low_95 hi_95 id   
##             &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;
##  1           71.5   70.2  72.8 1    
##  2           71.5   70.3  72.8 1    
##  3           71.5   70.1  72.8 1    
##  4           71.5   70.1  72.9 1    
##  5           71.4   70.0  72.9 1    
##  6           71.5   70.0  72.9 1    
##  7           71.4   69.9  72.9 1    
##  8           71.4   69.9  72.9 1    
##  9           71.4   69.9  72.9 1    
## 10           71.4   69.8  73.0 1    
## # ... with 110 more rows</code></pre>
<p>
I now select the true values for the month of June. I also imputed this data, but here I will simply keep the average of the imputations:
</p>
<pre class="r"><code>weight_june &lt;- imp_weight %&gt;% 
    filter(Date &gt;= "2018-06-01") %&gt;% 
    select(-.id) %&gt;% 
    group_by(Date) %&gt;% 
    summarise(true_weight = mean(Poids)) %&gt;% 
    rename(date = Date)</code></pre>
<p>
Let’s take a look at <code>weight_june</code>:
</p>
<pre class="r"><code>weight_june</code></pre>
<pre><code>## # A tibble: 24 x 2
##    date       true_weight
##    &lt;date&gt;           &lt;dbl&gt;
##  1 2018-06-01        71.8
##  2 2018-06-02        70.8
##  3 2018-06-03        71.2
##  4 2018-06-04        71.4
##  5 2018-06-05        70.9
##  6 2018-06-06        70.8
##  7 2018-06-07        70.5
##  8 2018-06-08        70.1
##  9 2018-06-09        70.3
## 10 2018-06-10        71.0
## # ... with 14 more rows</code></pre>
<p>
Let’s repeat <code>weight_june</code> 5 times, and add the index 1 to 5. Why? Because I want to merge the true data with the forecasts, and having the data in this form makes things easier:
</p>
<pre class="r"><code>weight_june &lt;- modify(list_along(1:5), ~`&lt;-`(., weight_june)) %&gt;% 
    map2(.y = as.character(seq(1, 5)), 
         ~mutate(.x, id = .y)) %&gt;% 
    bind_rows()</code></pre>
<p>
The first line:
</p>
<pre><code>modify(list_along(1:5), ~`&lt;-`(., weight_june)) </code></pre>
<p>
looks quite complicated, but you will see that it is not, once we break it apart. <code>modify()</code> modifies a list. The list to modify is <code>list_along(1:5)</code>, which create a list of <code>NULL</code>s:
</p>
<pre class="r"><code>list_along(1:5)</code></pre>
<pre><code>## [[1]]
## NULL
## 
## [[2]]
## NULL
## 
## [[3]]
## NULL
## 
## [[4]]
## NULL
## 
## [[5]]
## NULL</code></pre>
<p>
The second argument of <code>modify()</code> is either a function or a formula. I created the following formula:
</p>
<pre><code>~`&lt;-`(., weight_june)</code></pre>
<p>
We all know the function <code>&lt;-()</code>, but are not used to see it that way. But consider the following:
</p>
<pre class="r"><code>a &lt;- 3</code></pre>
<pre class="r"><code>`&lt;-`(a, 3)</code></pre>
<p>
These two formulations are equivalent. So these lines fill the empty element of the list of <code>NULL</code>s with the data frame <code>weight_june</code>. Then I add the <code>id</code> column and then bind the rows together: <code>bind_rows()</code>.
</p>
<p>
Let’s bind the columns of <code>weight_june</code> and <code>forecasts</code> and take a look at it:
</p>
<pre class="r"><code>forecasts &lt;- bind_cols(weight_june, forecasts) %&gt;% 
    select(-id1)

forecasts</code></pre>
<pre><code>## # A tibble: 120 x 6
##    date       true_weight id    point_forecast low_95 hi_95
##    &lt;date&gt;           &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
##  1 2018-06-01        71.8 1               71.5   70.2  72.8
##  2 2018-06-02        70.8 1               71.5   70.3  72.8
##  3 2018-06-03        71.2 1               71.5   70.1  72.8
##  4 2018-06-04        71.4 1               71.5   70.1  72.9
##  5 2018-06-05        70.9 1               71.4   70.0  72.9
##  6 2018-06-06        70.8 1               71.5   70.0  72.9
##  7 2018-06-07        70.5 1               71.4   69.9  72.9
##  8 2018-06-08        70.1 1               71.4   69.9  72.9
##  9 2018-06-09        70.3 1               71.4   69.9  72.9
## 10 2018-06-10        71.0 1               71.4   69.8  73.0
## # ... with 110 more rows</code></pre>
<p>
Now, for the last plot:
</p>
<pre class="r"><code>ggplot(forecasts, aes(x = date, colour = id)) +
    geom_line(aes(y = true_weight), size = 2) + 
    geom_line(aes(y = hi_95)) + 
    geom_line(aes(y = low_95)) + 
    theme(legend.position = "bottom")</code></pre>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/fun_ts-28-1.png" class="img-fluid"></p>
</div>
<p>
The true data fall within all the confidence intervals, but I am a bit surprised by the intervals, especially the upper confidence intervals; they all are way above 72kg, however my true weight has been fluctuating around 71kg for quite some months now. I think I have to refresh my memory on time series, because I am certainly missing something!
</p>
<p>
If you found this blog post useful, you might want to follow me on <a href="https://www.twitter.com/brodriguesco">twitter</a> for blog post updates.
</p>



 ]]></description>
  <category>R</category>
  <category>econometrics</category>
  <guid>https://b-rodrigues.github.io/posts/2018-06-24-fun_ts.html</guid>
  <pubDate>Sun, 24 Jun 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Getting data from pdfs using the pdftools package</title>
  <link>https://b-rodrigues.github.io/posts/2018-06-10-scraping_pdfs.html</link>
  <description><![CDATA[ 




<p>
It is often the case that data is trapped inside pdfs, but thankfully there are ways to extract it from the pdfs. A very nice package for this task is <code>pdftools</code> (<a href="https://github.com/ropensci/pdftools">Github link</a>) and this blog post will describe some basic functionality from that package.
</p>
<p>
First, let’s find some pdfs that contain interesting data. For this post, I’m using the diabetes country profiles from the World Health Organization. You can find them <a href="http://www.who.int/diabetes/country-profiles/en/#U">here</a>. If you open one of these pdfs, you are going to see this:
</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://b-rodrigues.github.io/assets/img/diabetes_lux.png" class="img-fluid figure-img"></p>
<figcaption>http://www.who.int/diabetes/country-profiles/lux_en.pdf?ua=1</figcaption>
</figure>
</div>
</div>
<p>
I’m interested in this table here in the middle:
</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://b-rodrigues.github.io/assets/img/diabetes_table.png" class="img-fluid figure-img"></p>
<figcaption>http://www.who.int/diabetes/country-profiles/lux_en.pdf?ua=1</figcaption>
</figure>
</div>
</div>
<p>
I want to get the data from different countries, put it all into a nice data frame and make a simple plot.
</p>
<p>
Let’s first start by loading the needed packages:
</p>
<pre class="r"><code>library("pdftools")
library("glue")
library("tidyverse")</code></pre>
<pre><code>## ── Attaching packages ────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──</code></pre>
<pre><code>## ✔ ggplot2 2.2.1     ✔ purrr   0.2.5
## ✔ tibble  1.4.2     ✔ dplyr   0.7.5
## ✔ tidyr   0.8.1     ✔ stringr 1.3.1
## ✔ readr   1.1.1     ✔ forcats 0.3.0</code></pre>
<pre><code>## ── Conflicts ───────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::collapse() masks glue::collapse()
## ✖ dplyr::filter()   masks stats::filter()
## ✖ dplyr::lag()      masks stats::lag()</code></pre>
<pre class="r"><code>library("ggthemes")

country &lt;- c("lux", "fra", "deu", "usa", "prt", "gbr")

url &lt;- "http://www.who.int/diabetes/country-profiles/{country}_en.pdf?ua=1"</code></pre>
<p>
The first 4 lines load the needed packages for this exercise: <code>pdftools</code> is the package that I described in the beginning of the post, <code>glue</code> is optional but offers a nice alternative to the <code>paste()</code> and <code>paste0()</code> functions. Take a closer look at the url: you’ll see that I wrote <code>{country}</code>. This is not in the original links; the original links look like this (for example for the USA):
</p>
<pre><code>"http://www.who.int/diabetes/country-profiles/usa_en.pdf?ua=1"</code></pre>
<p>
So because I’m interested in several countries, I created a vector with the country codes of the countries I’m interested in. Now, using the <code>glue()</code> function, something magical happens:
</p>
<pre class="r"><code>(urls &lt;- glue(url))</code></pre>
<pre><code>## http://www.who.int/diabetes/country-profiles/lux_en.pdf?ua=1
## http://www.who.int/diabetes/country-profiles/fra_en.pdf?ua=1
## http://www.who.int/diabetes/country-profiles/deu_en.pdf?ua=1
## http://www.who.int/diabetes/country-profiles/usa_en.pdf?ua=1
## http://www.who.int/diabetes/country-profiles/prt_en.pdf?ua=1
## http://www.who.int/diabetes/country-profiles/gbr_en.pdf?ua=1</code></pre>
<p>
This created a vector with all the links where <code>{country}</code> is replaced by each of the codes contained in the variable <code>country</code>.
</p>
<p>
I use the same trick to create the names of the pdfs that I will download:
</p>
<pre class="r"><code>pdf_names &lt;- glue("report_{country}.pdf")</code></pre>
<p>
And now I can download them:
</p>
<pre class="r"><code>walk2(urls, pdf_names, download.file, mode = "wb")</code></pre>
<p>
<code>walk2()</code> is a function from the <code>purrr</code> package that is similar to <code>map2()</code>. You could use <code>map2()</code> for this, but <code>walk2()</code> is cleaner here, because <code>dowload.file()</code> is a function with a so-called side effect; it downloads files. <code>map2()</code> is used for functions without side effects.
</p>
<p>
Now, I can finally use the <code>pdf_text()</code> function from the <code>pdftools</code> function to get the text from the pdfs:
</p>
<pre class="r"><code>raw_text &lt;- map(pdf_names, pdf_text)</code></pre>
<p>
<code>raw_text</code> is a list of where each element is the text from one of the pdfs. Let’s take a look:
</p>
<pre class="r"><code>str(raw_text)</code></pre>
<pre><code>## List of 6
##  $ : chr "Luxembourg                                                                                                     "| __truncated__
##  $ : chr "France                                                                                                         "| __truncated__
##  $ : chr "Germany                                                                                                        "| __truncated__
##  $ : chr "United States Of America                                                                                       "| __truncated__
##  $ : chr "Portugal                                                                                                       "| __truncated__
##  $ : chr "United Kingdom                                                                                                 "| __truncated__</code></pre>
<p>
Let’s take a look at one of these elements, which is nothing but a very long character:
</p>
<pre class="r"><code>raw_text[[1]]</code></pre>
<pre><code>## [1] "Luxembourg                                                                                                                                          Total population: 567 000\n                                                                                                                                                         Income group: High\nMortality\nNumber of diabetes deaths                                                                     Number of deaths attributable to high blood glucose\n                                                                     males         females                                                            males       females\nages 30–69                                                           &lt;100            &lt;100     ages 30–69                                              &lt;100          &lt;100\nages 70+                                                             &lt;100            &lt;100     ages 70+                                                &lt;100          &lt;100\nProportional mortality (% of total deaths, all ages)                                          Trends in age-standardized prevalence of diabetes\n                    Communicable,\n                   maternal, perinatal              Injuries                                                    35%\n                    and nutritional                   6%                     Cardiovascular\n                      conditions                                               diseases\n                          6%                                                      33%\n                                                                                                                30%\n                                                                                                                25%\n                                                                                              % of population\n               Other NCDs\n                  16%                                                                                           20%\n                                     No data available                                                          15%           No data available\n              Diabetes                                                                                          10%\n                 2%\n                                                                                                                5%\n                   Respiratory\n                    diseases\n                       6%                                                                                       0%\n                                                           Cancers\n                                                            31%\n                                                                                                                                  males     females\nPrevalence of diabetes and related risk factors\n                                                                                                                      males               females               total\nDiabetes                                                                                                              8.3%                 5.3%                 6.8%\nOverweight                                                                                                            70.7%               51.5%                61.0%\nObesity                                                                                                               28.3%               21.3%                24.8%\nPhysical inactivity                                                                                                   28.2%               31.7%                30.0%\nNational response to diabetes\nPolicies, guidelines and monitoring\nOperational policy/strategy/action plan for diabetes                                                                                                ND\nOperational policy/strategy/action plan to reduce overweight and obesity                                                                            ND\nOperational policy/strategy/action plan to reduce physical inactivity                                                                               ND\nEvidence-based national diabetes guidelines/protocols/standards                                                                                     ND\nStandard criteria for referral of patients from primary care to higher level of care                                                                ND\nDiabetes registry                                                                                                                                   ND\nRecent national risk factor survey in which blood glucose was measured                                                                              ND\nAvailability of medicines, basic technologies and procedures in the public health sector\nMedicines in primary care facilities                                                          Basic technologies in primary care facilities\nInsulin                                                                               ND      Blood glucose measurement                                             ND\nMetformin                                                                             ND      Oral glucose tolerance test                                           ND\nSulphonylurea                                                                         ND      HbA1c test                                                            ND\nProcedures                                                                                    Dilated fundus examination                                            ND\nRetinal photocoagulation                                                              ND      Foot vibration perception by tuning fork                              ND\nRenal replacement therapy by dialysis                                                 ND      Foot vascular status by Doppler                                       ND\nRenal replacement therapy by transplantation                                          ND      Urine strips for glucose and ketone measurement                       ND\nND = country did not respond to country capacity survey\n〇 = not generally available   ● = generally available\nWorld Health Organization – Diabetes country profiles, 2016.\n"</code></pre>
<p>
As you can see, this is a very long character string with some line breaks (the <code>""</code> character). So first, we need to split this string into a character vector by the <code>""</code> character. Also, it might be difficult to see, but the table starts at the line with the following string: <code>"Prevalence of diabetes"</code> and ends with <code>"National response to diabetes"</code>. Also, we need to get the name of the country from the text and add it as a column. As you can see, a whole lot of operations are needed, so what I do is put all these operations into a function that I will apply to each element of <code>raw_text</code>:
</p>
<pre class="r"><code>clean_table &lt;- function(table){
    table &lt;- str_split(table, "\n", simplify = TRUE)
    country_name &lt;- table[1, 1] %&gt;% 
        stringr::str_squish() %&gt;% 
        stringr::str_extract(".+?(?=\\sTotal)")
    table_start &lt;- stringr::str_which(table, "Prevalence of diabetes")
    table_end &lt;- stringr::str_which(table, "National response to diabetes")
    table &lt;- table[1, (table_start +1 ):(table_end - 1)]
    table &lt;- str_replace_all(table, "\\s{2,}", "|")
    text_con &lt;- textConnection(table)
    data_table &lt;- read.csv(text_con, sep = "|")
    colnames(data_table) &lt;- c("Condition", "Males", "Females", "Total")
    dplyr::mutate(data_table, Country = country_name)
}</code></pre>
<p>
I advise you to go through all these operations and understand what each does. However, I will describe some of the lines, such as this one:
</p>
<pre><code>stringr::str_extract(".+?(?=\\sTotal)")</code></pre>
<p>
This uses a very bizarre looking regular expression: <code>".+?(?=\sTotal)"</code>. This extracts everything before a space, followed by the string <code>"Total"</code>. This is because the first line, the one that contains the name of the country looks like this: <code>"Luxembourg Total population: 567 000"</code>. So everything before a space followed by the word <code>"Total"</code> is the country name. Then there’s these lines:
</p>
<pre><code>table &lt;- str_replace_all(table, "\\s{2,}", "|")
text_con &lt;- textConnection(table)
data_table &lt;- read.csv(text_con, sep = "|")</code></pre>
<p>
The first lines replaces 2 spaces or more (“<code>\s{2,}</code>”) with <code>"|"</code>. The reason I do this is because then I can read the table back into R as a data frame by specifying the separator as the “|” character. On the second line, I define <code>table</code> as a text connection, that I can then read back into R using <code>read.csv()</code>. On the second to the last line I change the column names and then I add a column called <code>"Country"</code> to the data frame.
</p>
<p>
Now, I can map this useful function to the list of raw text extracted from the pdfs:
</p>
<pre class="r"><code>diabetes &lt;- map_df(raw_text, clean_table) %&gt;% 
    gather(Sex, Share, Males, Females, Total) %&gt;% 
    mutate(Share = as.numeric(str_extract(Share, "\\d{1,}\\.\\d{1,}")))</code></pre>
<p>
I reshape the data with the <code>gather()</code> function (see what the data looks like before and after reshaping). I then convert the <code>"Share"</code> column into a numeric (it goes from something that looks like <code>"12.3 %"</code> into <code>12.3</code>) and then I can create a nice plot. But first let’s take a look at the data:
</p>
<pre class="r"><code>diabetes</code></pre>
<pre><code>##              Condition                  Country     Sex Share
## 1             Diabetes               Luxembourg   Males   8.3
## 2           Overweight               Luxembourg   Males  70.7
## 3              Obesity               Luxembourg   Males  28.3
## 4  Physical inactivity               Luxembourg   Males  28.2
## 5             Diabetes                   France   Males   9.5
## 6           Overweight                   France   Males  69.9
## 7              Obesity                   France   Males  25.3
## 8  Physical inactivity                   France   Males  21.2
## 9             Diabetes                  Germany   Males   8.4
## 10          Overweight                  Germany   Males  67.0
## 11             Obesity                  Germany   Males  24.1
## 12 Physical inactivity                  Germany   Males  20.1
## 13            Diabetes United States Of America   Males   9.8
## 14          Overweight United States Of America   Males  74.1
## 15             Obesity United States Of America   Males  33.7
## 16 Physical inactivity United States Of America   Males  27.6
## 17            Diabetes                 Portugal   Males  10.7
## 18          Overweight                 Portugal   Males  65.0
## 19             Obesity                 Portugal   Males  21.4
## 20 Physical inactivity                 Portugal   Males  33.5
## 21            Diabetes           United Kingdom   Males   8.4
## 22          Overweight           United Kingdom   Males  71.1
## 23             Obesity           United Kingdom   Males  28.5
## 24 Physical inactivity           United Kingdom   Males  35.4
## 25            Diabetes               Luxembourg Females   5.3
## 26          Overweight               Luxembourg Females  51.5
## 27             Obesity               Luxembourg Females  21.3
## 28 Physical inactivity               Luxembourg Females  31.7
## 29            Diabetes                   France Females   6.6
## 30          Overweight                   France Females  58.6
## 31             Obesity                   France Females  26.1
## 32 Physical inactivity                   France Females  31.2
## 33            Diabetes                  Germany Females   6.4
## 34          Overweight                  Germany Females  52.7
## 35             Obesity                  Germany Females  21.4
## 36 Physical inactivity                  Germany Females  26.5
## 37            Diabetes United States Of America Females   8.3
## 38          Overweight United States Of America Females  65.3
## 39             Obesity United States Of America Females  36.3
## 40 Physical inactivity United States Of America Females  42.1
## 41            Diabetes                 Portugal Females   7.8
## 42          Overweight                 Portugal Females  55.0
## 43             Obesity                 Portugal Females  22.8
## 44 Physical inactivity                 Portugal Females  40.8
## 45            Diabetes           United Kingdom Females   6.9
## 46          Overweight           United Kingdom Females  62.4
## 47             Obesity           United Kingdom Females  31.1
## 48 Physical inactivity           United Kingdom Females  44.3
## 49            Diabetes               Luxembourg   Total   6.8
## 50          Overweight               Luxembourg   Total  61.0
## 51             Obesity               Luxembourg   Total  24.8
## 52 Physical inactivity               Luxembourg   Total  30.0
## 53            Diabetes                   France   Total   8.0
## 54          Overweight                   France   Total  64.1
## 55             Obesity                   France   Total  25.7
## 56 Physical inactivity                   France   Total  26.4
## 57            Diabetes                  Germany   Total   7.4
## 58          Overweight                  Germany   Total  59.7
## 59             Obesity                  Germany   Total  22.7
## 60 Physical inactivity                  Germany   Total  23.4
## 61            Diabetes United States Of America   Total   9.1
## 62          Overweight United States Of America   Total  69.6
## 63             Obesity United States Of America   Total  35.0
## 64 Physical inactivity United States Of America   Total  35.0
## 65            Diabetes                 Portugal   Total   9.2
## 66          Overweight                 Portugal   Total  59.8
## 67             Obesity                 Portugal   Total  22.1
## 68 Physical inactivity                 Portugal   Total  37.3
## 69            Diabetes           United Kingdom   Total   7.7
## 70          Overweight           United Kingdom   Total  66.7
## 71             Obesity           United Kingdom   Total  29.8
## 72 Physical inactivity           United Kingdom   Total  40.0</code></pre>
<p>
Now let’s go for the plot:
</p>
<pre class="r"><code>ggplot(diabetes) + theme_fivethirtyeight() + scale_fill_hc() +
    geom_bar(aes(y = Share, x = Sex, fill = Country), 
             stat = "identity", position = "dodge") +
    facet_wrap(~Condition)</code></pre>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/scraping_pdfs_diabetes.png" class="img-fluid"></p>
</div>
<p>
That was a whole lot of work for such a simple plot!
</p>
<p>
If you found this blog post useful, you might want to follow me on <a href="https://www.twitter.com/brodriguesco">twitter</a> for blog post updates.
</p>



 ]]></description>
  <category>R</category>
  <category>programming</category>
  <guid>https://b-rodrigues.github.io/posts/2018-06-10-scraping_pdfs.html</guid>
  <pubDate>Sun, 10 Jun 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>{pmice}, an experimental package for missing data imputation in parallel using {mice} and {furrr}</title>
  <link>https://b-rodrigues.github.io/posts/2018-04-15-announcing_pmice.html</link>
  <description><![CDATA[ 




<p>
Yesterday I wrote <a href="../posts/2018-04-14-playing_with_furrr.html">this blog post</a> which showed how one could use <code>{furrr}</code> and <code>{mice}</code> to impute missing data in parallel, thus speeding up the process tremendously.
</p>
<p>
To make using this snippet of code easier, I quickly cobbled together an experimental package called <code>{pmice}</code> that you can install from Github:
</p>
<pre class="r"><code>devtools::install_github("b-rodrigues/pmice")</code></pre>
<p>
For now, it returns a list of <code>mids</code> objects and not a <code>mids</code> object like <code>mice::mice()</code> does, but I’ll be working on it. Contributions welcome!
</p>
<p>
If you found this blog post useful, you might want to follow me on <a href="https://www.twitter.com/brodriguesco">twitter</a> for blog post updates.
</p>



 ]]></description>
  <category>R</category>
  <category>programming</category>
  <guid>https://b-rodrigues.github.io/posts/2018-04-15-announcing_pmice.html</guid>
  <pubDate>Sun, 15 Apr 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Imputing missing values in parallel using {furrr}</title>
  <link>https://b-rodrigues.github.io/posts/2018-04-14-playing_with_furrr.html</link>
  <description><![CDATA[ 




<p>
Today I saw this tweet on my timeline:
</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
For those of us that just can't wait until RStudio officially supports parallel purrr in <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a>, boy have I got something for you. <br><br>Introducing <code>furrr</code>, parallel purrr through the use of futures. Go ahead, break things, you know you want to:<a href="https://t.co/l9z1UC2Tew">https://t.co/l9z1UC2Tew</a>
</p>
— Davis Vaughan (<span class="citation" data-cites="dvaughan32">@dvaughan32</span>) <a href="https://twitter.com/dvaughan32/status/984828716181319680?ref_src=twsrc%5Etfw">April 13, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>
and as a heavy <code>{purrr}</code> user, as well as the happy owner of a 6-core AMD Ryzen 5 1600X cpu, I was very excited to try out <code>{furrr}</code>. For those unfamiliar with <code>{purrr}</code>, you can read some of my previous blog posts on it <a href="http://www.brodrigues.co/blog/2017-03-24-lesser_known_purrr/">here</a>, <a href="http://www.brodrigues.co/blog/2017-03-29-make-ggplot2-purrr/">here</a> or <a href="http://www.brodrigues.co/blog/2018-01-19-mapping_functions_with_any_cols/">here</a>.
</p>
<p>
To summarize very quickly: <code>{purrr}</code> contains so-called higher order functions, which are functions that take other functions as argument. One such function is <code>map()</code>. Consider the following simple example:
</p>
<pre class="r"><code>numbers &lt;- seq(1, 10)</code></pre>
<p>
If you want the square root of this numbers, you can of course simply use the <code>sqrt()</code> function, because it is vectorized:
</p>
<pre class="r"><code>sqrt(numbers)</code></pre>
<pre><code>##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751
##  [8] 2.828427 3.000000 3.162278</code></pre>
<p>
But in a lot of situations, the solution is not so simple. Sometimes you have to loop over the values. This is what we would need to do if <code>sqrt()</code> was not vectorized:
</p>
<pre class="r"><code>sqrt_numbers &lt;- rep(0, 10)

for(i in length(numbers)){
  sqrt_numbers[i] &lt;- sqrt(numbers[i])
}</code></pre>
<p>
First, you need to initialize a container, and then you have to populate the <code>sqrt_numbers</code> list with the results. Using, <code>{purrr}</code> is way easier:
</p>
<pre class="r"><code>library(tidyverse)
map(numbers, sqrt)</code></pre>
<pre><code>## [[1]]
## [1] 1
## 
## [[2]]
## [1] 1.414214
## 
## [[3]]
## [1] 1.732051
## 
## [[4]]
## [1] 2
## 
## [[5]]
## [1] 2.236068
## 
## [[6]]
## [1] 2.44949
## 
## [[7]]
## [1] 2.645751
## 
## [[8]]
## [1] 2.828427
## 
## [[9]]
## [1] 3
## 
## [[10]]
## [1] 3.162278</code></pre>
<p>
<code>map()</code> is only one of the nice functions that are bundled inside <code>{purrr}</code>. Mastering <code>{purrr}</code> can really make you a much more efficient R programmer. Anyways, recently, I have been playing around with imputation and the <code>{mice}</code> package. <code>{mice}</code> comes with an example dataset called <code>boys</code>, let’s take a look at it:
</p>
<pre class="r"><code>library(mice)

data(boys)

brotools::describe(boys) %&gt;%
  select(variable, type, n_missing, everything())</code></pre>
<pre><code>## # A tibble: 9 x 13
##   variable type    n_missing  nobs   mean    sd mode     min   max   q25
##   &lt;chr&gt;    &lt;chr&gt;       &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 age      Numeric         0   748   9.16  6.89 0.035  0.035  21.2  1.58
## 2 bmi      Numeric        21   748  18.1   3.05 14.54 11.8    31.7 15.9 
## 3 hc       Numeric        46   748  51.5   5.91 33.7  33.7    65   48.1 
## 4 hgt      Numeric        20   748 132.   46.5  50.1  50     198   84.9 
## 5 tv       Numeric       522   748  11.9   7.99 &lt;NA&gt;   1      25    4   
## 6 wgt      Numeric         4   748  37.2  26.0  3.65   3.14  117.  11.7 
## 7 gen      Factor        503   748  NA    NA    &lt;NA&gt;  NA      NA   NA   
## 8 phb      Factor        503   748  NA    NA    &lt;NA&gt;  NA      NA   NA   
## 9 reg      Factor          3   748  NA    NA    south NA      NA   NA   
## # ... with 3 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, n_unique &lt;int&gt;</code></pre>
<p>
In the code above I use the <code>describe()</code> function from my personal package to get some summary statistics of the <code>boys</code> dataset (you can read more about this function <a href="http://www.brodrigues.co/blog/2018-04-10-brotools_describe">here</a>). I am especially interested in the number of missing values, which is why I re-order the columns. If I did not re-order the columns, it would not appear in the output on my blog.
</p>
<p>
We see that some columns have a lot of missing values. Using the <code>mice</code> function, it is very easy to impute them:
</p>
<pre class="r"><code>start &lt;- Sys.time()
imp_boys &lt;- mice(boys, m = 10, maxit = 100, printFlag = FALSE)
end &lt;- Sys.time() - start

print(end)</code></pre>
<pre><code>## Time difference of 3.290611 mins</code></pre>
<p>
Imputation on a single core took around 3 minutes on my computer. This might seem ok, but if you have a larger data set with more variables, 3 minutes can become 3 hours. And if you increase <code>maxit</code>, which helps convergence, or the number of imputations, 3 hours can become 30 hours. With a 6-core CPU this could potentially be brought down to 5 hours (in theory). Let’s see if we can go faster, but first let’s take a look at the imputed data.
</p>
<p>
The <code>mice()</code> function returns a <code>mids</code> object. If you want to look at the data, you have to use the <code>complete()</code> function (careful, there is also a <code>complete()</code> function in the <code>{tidyr}</code> package, so to avoid problems, I suggest you explicitely call <code>mice::complete()</code>):
</p>
<pre class="r"><code>imp_boys &lt;- mice::complete(imp_boys, "long")

brotools::describe(imp_boys) %&gt;%
  select(variable, type, n_missing, everything())</code></pre>
<pre><code>## # A tibble: 11 x 13
##    variable type   n_missing  nobs   mean     sd mode     min   max    q25
##    &lt;chr&gt;    &lt;chr&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
##  1 .id      Numer…         0  7480 374.   216.   1      1     748   188.  
##  2 .imp     Numer…         0  7480   5.5    2.87 1      1      10     3   
##  3 age      Numer…         0  7480   9.16   6.89 0.035  0.035  21.2   1.58
##  4 bmi      Numer…         0  7480  18.0    3.03 14.54 11.8    31.7  15.9 
##  5 hc       Numer…         0  7480  51.6    5.89 33.7  33.7    65    48.3 
##  6 hgt      Numer…         0  7480 131.    46.5  50.1  50     198    83   
##  7 tv       Numer…         0  7480   8.39   8.09 2      1      25     2   
##  8 wgt      Numer…         0  7480  37.1   26.0  3.65   3.14  117.   11.7 
##  9 gen      Factor         0  7480  NA     NA    G1    NA      NA    NA   
## 10 phb      Factor         0  7480  NA     NA    P1    NA      NA    NA   
## 11 reg      Factor         0  7480  NA     NA    south NA      NA    NA   
## # ... with 3 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, n_unique &lt;int&gt;</code></pre>
<p>
As expected, no more missing values. The “long” argument inside <code>mice::complete()</code> is needed if you want the <code>complete()</code> function to return a long dataset. Doing the above “manually” using <code>{purrr}</code> is possible with the following code:
</p>
<pre class="r"><code>start &lt;- Sys.time()
imp_boys_purrr &lt;- map(rep(1, 10), ~mice(data = boys, m = ., maxit = 100, printFlag = FALSE))
end &lt;- Sys.time() - start

print(end)</code></pre>
<pre><code>## Time difference of 3.393966 mins</code></pre>
<p>
What this does is map the function <code>~mice(data = boys, m = ., maxit = 100, printFlag = FALSE)</code> to a list of <code>1</code>s, and creates 10 imputed data sets. <code>m = .</code> means that <code>m</code> will be equal to whatever is inside the list we are mapping our function over, so <code>1</code>, then <code>1</code> then another <code>1</code> etc…. It took around the same amount of time as using <code>mice()</code> directly.
</p>
<p>
<code>imp_boys_purrr</code> is now a list of 10 <code>mids</code> objects. We thus need to map <code>mice::complete()</code> to <code>imp_boys_purrr</code> to get the data:
</p>
<pre class="r"><code>imp_boys_purrr_complete &lt;- map(imp_boys_purrr, mice::complete)</code></pre>
<p>
Now, <code>imp_boys_purrr_complete</code> is a list of 10 datasets. Let’s map <code>brotools::describe()</code> to it:
</p>
<pre class="r"><code>map(imp_boys_purrr_complete, brotools::describe)</code></pre>
<pre><code>## [[1]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.0   3.03 14.54 11.8    31.7 15.9    17.4  19.5
## 3 hc       Numer…   748  51.7   5.90 33.7  33.7    65   48.3    53.1  56  
## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   84     146.  175. 
## 5 tv       Numer…   748   8.35  8.00 3      1      25    2       3    15  
## 6 wgt      Numer…   748  37.2  26.0  3.65   3.14  117.  11.7    34.7  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;
## 
## [[2]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.0   3.03 14.54 11.8    31.7 15.9    17.5  19.5
## 3 hc       Numer…   748  51.6   5.88 33.7  33.7    65   48.3    53.2  56  
## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.5   145.  175  
## 5 tv       Numer…   748   8.37  8.02 1      1      25    2       3    15  
## 6 wgt      Numer…   748  37.1  26.0  3.65   3.14  117.  11.9    34.6  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P2    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;
## 
## [[3]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.1   3.04 14.54 11.8    31.7 15.9    17.5  19.5
## 3 hc       Numer…   748  51.6   5.87 33.7  33.7    65   48.5    53.3  56  
## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.0   145.  175  
## 5 tv       Numer…   748   8.46  8.14 2      1      25    2       3    15  
## 6 wgt      Numer…   748  37.2  26.1  3.65   3.14  117.  11.7    34.6  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;
## 
## [[4]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.1   3.02 14.54 11.8    31.7 15.9    17.5  19.4
## 3 hc       Numer…   748  51.7   5.93 33.7  33.7    65   48.5    53.4  56  
## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   82.9   145.  175  
## 5 tv       Numer…   748   8.45  8.11 2      1      25    2       3    15  
## 6 wgt      Numer…   748  37.2  26.0  3.65   3.14  117.  11.7    34.7  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;
## 
## [[5]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.0   3.03 14.54 11.8    31.7 15.9    17.5  19.5
## 3 hc       Numer…   748  51.6   5.91 33.7  33.7    65   48.3    53.2  56  
## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.0   146.  175. 
## 5 tv       Numer…   748   8.21  8.02 3      1      25    2       3    15  
## 6 wgt      Numer…   748  37.1  26.0  3.65   3.14  117.  11.7    34.6  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;
## 
## [[6]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.0   3.05 14.54 11.8    31.7 15.9    17.4  19.5
## 3 hc       Numer…   748  51.7   5.89 33.7  33.7    65   48.3    53.2  56  
## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   83.0   146.  175  
## 5 tv       Numer…   748   8.44  8.24 3      1      25    2       3    15  
## 6 wgt      Numer…   748  37.1  26.0  3.65   3.14  117.  11.7    34.6  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;
## 
## [[7]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.1   3.04 14.54 11.8    31.7 15.9    17.4  19.5
## 3 hc       Numer…   748  51.6   5.88 33.7  33.7    65   48.2    53.2  56  
## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.5   146.  175  
## 5 tv       Numer…   748   8.47  8.15 2      1      25    2       3    15  
## 6 wgt      Numer…   748  37.2  26.1  3.65   3.14  117.  11.7    34.6  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;
## 
## [[8]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.0   3.04 14.54 11.8    31.7 15.9    17.4  19.4
## 3 hc       Numer…   748  51.6   5.85 33.7  33.7    65   48.2    53.3  56  
## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   83.0   146.  175  
## 5 tv       Numer…   748   8.36  8.06 2      1      25    2       3    15  
## 6 wgt      Numer…   748  37.2  26.1  3.65   3.14  117.  11.7    34.6  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;
## 
## [[9]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.0   3.05 14.54 11.8    31.7 15.9    17.4  19.5
## 3 hc       Numer…   748  51.6   5.90 33.7  33.7    65   48.3    53.2  56  
## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.9   146.  175  
## 5 tv       Numer…   748   8.57  8.25 1      1      25    2       3    15  
## 6 wgt      Numer…   748  37.1  26.1  3.65   3.14  117.  11.7    34.6  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;
## 
## [[10]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.0   3.04 14.54 11.8    31.7 15.9    17.4  19.5
## 3 hc       Numer…   748  51.6   5.89 33.7  33.7    65   48.3    53.1  56  
## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.0   146.  175  
## 5 tv       Numer…   748   8.49  8.18 2      1      25    2       3    15  
## 6 wgt      Numer…   748  37.1  26.1  3.65   3.14  117.  11.7    34.6  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;</code></pre>
<p>
Before merging this 10 datasets together into one, it would be nice to have a column with the id of the datasets. This can easily be done with a variant of <code>purrr::map()</code>, called <code>map2()</code>:
</p>
<pre class="r"><code>imp_boys_purrr &lt;- map2(.x = seq(1,10), .y = imp_boys_purrr_complete, ~mutate(.y, imp_id = as.character(.x)))</code></pre>
<p>
<code>map2()</code> applies a function, say <code>f()</code>, to 2 lists sequentially: <code>f(x_1, y_1)</code>, then <code>f(x_2, y_2)</code>, etc… So here I map <code>mutate()</code> to create a new column, <code>imp_id</code> in each dataset. Now let’s bind the rows and take a look at the data:
</p>
<pre class="r"><code>imp_boys_purrr &lt;- bind_rows(imp_boys_purrr)

imp_boys_purrr %&gt;%
  brotools::describe() %&gt;%
  select(variable, type, n_missing, everything())</code></pre>
<pre><code>## # A tibble: 10 x 13
##    variable type     n_missing  nobs   mean    sd mode     min   max   q25
##    &lt;chr&gt;    &lt;chr&gt;        &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 age      Numeric          0  7480   9.16  6.89 0.035  0.035  21.2  1.58
##  2 bmi      Numeric          0  7480  18.0   3.04 14.54 11.8    31.7 15.9 
##  3 hc       Numeric          0  7480  51.6   5.89 33.7  33.7    65   48.3 
##  4 hgt      Numeric          0  7480 131.   46.5  50.1  50     198   83   
##  5 tv       Numeric          0  7480   8.42  8.11 3      1      25    2   
##  6 wgt      Numeric          0  7480  37.1  26.0  3.65   3.14  117.  11.7 
##  7 imp_id   Charact…         0  7480  NA    NA    1     NA      NA   NA   
##  8 gen      Factor           0  7480  NA    NA    G1    NA      NA   NA   
##  9 phb      Factor           0  7480  NA    NA    P1    NA      NA   NA   
## 10 reg      Factor           0  7480  NA    NA    south NA      NA   NA   
## # ... with 3 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, n_unique &lt;int&gt;</code></pre>
<p>
You may ask yourself why I am bothering with all this. This will become apparent now. We can now use the code we wrote to get our 10 imputed datasets using <code>purrr::map()</code> and simply use <code>furrr::future_map()</code> to parallelize the imputation process:
</p>
<pre class="r"><code>library(furrr)</code></pre>
<pre><code>## Loading required package: future</code></pre>
<pre class="r"><code>plan(multiprocess)

start &lt;- Sys.time()
imp_boys_future &lt;- future_map(rep(1, 10), ~mice(data = boys, m = ., maxit = 100, printFlag = FALSE))
end &lt;- Sys.time() - start

print(end)</code></pre>
<pre><code>## Time difference of 33.73772 secs</code></pre>
<p>
Boooom! Much faster! And simply by loading <code>{furrr}</code>, then using <code>plan(multiprocess)</code> to run the code in parallel (if you forget that, the code will run on a single core) and using <code>future_map()</code> instead of <code>map()</code>.
</p>
<p>
Let’s take a look at the data:
</p>
<pre class="r"><code>imp_boys_future_complete &lt;- map(imp_boys_future, mice::complete)

imp_boys_future &lt;- map2(.x = seq(1,10), .y = imp_boys_future_complete, ~mutate(.y, imp_id = as.character(.x)))

imp_boys_future &lt;- bind_rows(imp_boys_future)

imp_boys_future %&gt;%
  brotools::describe() %&gt;%
  select(variable, type, n_missing, everything())</code></pre>
<pre><code>## # A tibble: 10 x 13
##    variable type     n_missing  nobs   mean    sd mode     min   max   q25
##    &lt;chr&gt;    &lt;chr&gt;        &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 age      Numeric          0  7480   9.16  6.89 0.035  0.035  21.2  1.58
##  2 bmi      Numeric          0  7480  18.0   3.04 14.54 11.8    31.7 15.9 
##  3 hc       Numeric          0  7480  51.6   5.89 33.7  33.7    65   48.4 
##  4 hgt      Numeric          0  7480 131.   46.5  50.1  50     198   83   
##  5 tv       Numeric          0  7480   8.35  8.09 3      1      25    2   
##  6 wgt      Numeric          0  7480  37.1  26.0  3.65   3.14  117.  11.7 
##  7 imp_id   Charact…         0  7480  NA    NA    1     NA      NA   NA   
##  8 gen      Factor           0  7480  NA    NA    G1    NA      NA   NA   
##  9 phb      Factor           0  7480  NA    NA    P1    NA      NA   NA   
## 10 reg      Factor           0  7480  NA    NA    south NA      NA   NA   
## # ... with 3 more variables: median &lt;dbl&gt;, q75 &lt;dbl&gt;, n_unique &lt;int&gt;</code></pre>
<p>
So imputation went from 3.4 minutes (around 200 seconds) to 30 seconds. How cool is that? If you want to play around with <code>{furrr}</code> you must install it from Github, as it is not yet available on CRAN:
</p>
<pre class="r"><code>devtools::install_github("DavisVaughan/furrr")</code></pre>
<p>
If you are not comfortable with <code>map()</code> (and thus <code>future_map()</code>) but still want to impute in parallel, there is this very nice script <a href="https://github.com/gerkovink/parlMICE">here</a> to do just that. I created a package around this script, called <a href="https://github.com/b-rodrigues/parlMICE">parlMICE</a> (the same name as the script), to make installation and usage easier. You can install it like so:
</p>
<pre class="r"><code>devtools::install_github("b-rodrigues/parlMICE")</code></pre>
<p>
If you found this blog post useful, you might want to follow me on <a href="https://www.twitter.com/brodriguesco">twitter</a> for blog post updates.
</p>



 ]]></description>
  <category>R</category>
  <category>programming</category>
  <guid>https://b-rodrigues.github.io/posts/2018-04-14-playing_with_furrr.html</guid>
  <pubDate>Sat, 14 Apr 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Get basic summary statistics for all the variables in a data frame</title>
  <link>https://b-rodrigues.github.io/posts/2018-04-10-brotools_describe.html</link>
  <description><![CDATA[ 




<p>
I have added a new function to my <code>{brotools}</code> package, called <code>describe()</code>, which takes a data frame as an argument, and returns another data frame with descriptive statistics. It is very much inspired by the <a href="https://github.com/ropenscilabs/skimr"><code>{skmir}</code></a> package but also by <a href="https://github.com/bjornerstedt/assist/blob/master/R/describe.R"><code>assist::describe()</code></a> (click on the packages to be redirected to the respective Github repos) but I wanted to write my own for two reasons: first, as an exercice, and second I really only needed the function <code>skim_to_wide()</code> from <code>{skimr}</code>. So instead of installing a whole package for a single function, I decided to write my own (since I use <code>{brotools}</code> daily).
</p>
<p>
Below you can see it in action:
</p>
<pre class="r"><code>library(dplyr)
data(starwars)</code></pre>
<pre class="r"><code>brotools::describe(starwars)</code></pre>
<pre><code>## # A tibble: 10 x 13
##    variable  type   nobs  mean    sd mode     min   max   q25 median   q75
##    &lt;chr&gt;     &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
##  1 birth_ye… Nume…    87  87.6 155.  19         8   896  35       52  72  
##  2 height    Nume…    87 174.   34.8 172       66   264 167      180 191  
##  3 mass      Nume…    87  97.3 169.  77        15  1358  55.6     79  84.5
##  4 eye_color Char…    87  NA    NA   blue      NA    NA  NA       NA  NA  
##  5 gender    Char…    87  NA    NA   male      NA    NA  NA       NA  NA  
##  6 hair_col… Char…    87  NA    NA   blond     NA    NA  NA       NA  NA  
##  7 homeworld Char…    87  NA    NA   Tatoo…    NA    NA  NA       NA  NA  
##  8 name      Char…    87  NA    NA   Luke …    NA    NA  NA       NA  NA  
##  9 skin_col… Char…    87  NA    NA   fair      NA    NA  NA       NA  NA  
## 10 species   Char…    87  NA    NA   Human     NA    NA  NA       NA  NA  
## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;</code></pre>
<p>
As you can see, the object that is returned by <code>describe()</code> is a <code>tibble</code>.
</p>
<p>
For now, this function does not handle dates, but it’s in the pipeline.
</p>
<p>
You can also only describe certain columns:
</p>
<pre class="r"><code>brotools::describe(starwars, height, mass, name)</code></pre>
<pre><code>## # A tibble: 3 x 13
##   variable type    nobs  mean    sd mode      min   max   q25 median   q75
##   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1 height   Numer…    87 174.   34.8 172        66   264 167      180 191  
## 2 mass     Numer…    87  97.3 169.  77         15  1358  55.6     79  84.5
## 3 name     Chara…    87  NA    NA   Luke S…    NA    NA  NA       NA  NA  
## # ... with 2 more variables: n_missing &lt;int&gt;, n_unique &lt;int&gt;</code></pre>
<p>
If you want to try it out, you can install <code>{brotools}</code> from Github:
</p>
<pre><code>devtools::install_github("b-rodrigues/brotools")</code></pre>
<p>
If you found this blog post useful, you might want to follow me on <a href="https://www.twitter.com/brodriguesco">twitter</a> for blog post updates.
</p>



 ]]></description>
  <category>R</category>
  <category>programming</category>
  <guid>https://b-rodrigues.github.io/posts/2018-04-10-brotools_describe.html</guid>
  <pubDate>Tue, 10 Apr 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Getting {sparklyr}, {h2o}, {rsparkling} to work together and some fun with bash</title>
  <link>https://b-rodrigues.github.io/posts/2018-03-03-sparklyr_h2o_rsparkling.html</link>
  <description><![CDATA[ 




<p>
This is going to be the type of blog posts that would perhaps be better as a gist, but it is easier for me to use my blog as my own personal collection of gists. Plus, someone else might find this useful, so here it is! In this blog post I am going to show a little trick to randomly sample rows from a text file using bash, and then train a model using the <code>{h2o}</code> package. I will also use the <code>{rsparkling}</code> package. From <code>{rsparkling}</code>’s documentation: <em><code>{rsparkling}</code> is a package that provides an R interface to the <code>H2O</code> Sparkling Water machine learning library.</em> and will be needed to transfer the data from Spark to H2O.
</p>
<p>
In a <a href="http://www.brodrigues.co/blog/2018-02-16-importing_30gb_of_data/">previous blog post</a> I used the <code>{sparklyr}</code> package to load a 30GB csv file into R. I created the file by combining around 300 csv files, each around 80MB big. Here, I would like to use the machine learning functions included in the <code>{h2o}</code> packages to train a random forest on this data. However, I only want to have a simple prototype that simply runs, and check if all the packages work well together. If everything is ok, I’ll keep iterating to make the model better (in a possible subsequent post).
</p>
<p>
For fast prototyping, using 30GB of data is not a good idea, so I am going to sample 500000 from this file using the linux command line (works on macOS too and also on Windows if you installed the linux subsystem). Why not use R to sample 500000 rows? Because on my machine, loading the 30GB file takes 25 minutes. Sampling half a million lines from it would take quite long too. So here are some bash lines that do that directly on the file, without needing to load it into R beforehand:
</p>
<pre><code>[18-03-03 21:50] brodriguesco in /Documents/AirOnTimeCSV ➤ get_seeded_random()
{
  seed="$1"
  openssl enc -aes-256-ctr -pass pass:"$seed" -nosalt \
  &lt;/dev/zero 2&gt;/dev/null
}

[18-03-03 21:50] brodriguesco in /Documents/AirOnTimeCSV ➤ sed "1 d" combined.csv | shuf --random-source=&lt;(get_seeded_random 42) -n 500000 &gt; small_combined_temp.csv

[18-03-03 21:56] brodriguesco in /Documents/AirOnTimeCSV ➤ head -1 combined.csv &gt; colnames.csv

[18-03-03 21:56] brodriguesco in /Documents/AirOnTimeCSV ➤ cat colnames.csv small_combined_temp.csv &gt; small_combined.csv</code></pre>
<p>
The first function I took from the <a href="https://www.gnu.org/software/coreutils/manual/html_node/Random-sources.html">gnu coreutils manual</a> which allows me to fix the random seed to reproduce the same sampling of the file. Then I use <code>"sed 1 d" cobmined.csv</code> to remove the first line of <code>combined.csv</code> which is the header of the file. Then, I pipe the result of <code>sed</code> using <code>|</code> to <code>shuf</code> which does the shuffling. The option <code>–random-source=&lt;(get_seeded_random 42)</code> fixes the seed, and <code>-n 500000</code> only shuffles 500000 and not the whole file. The final bit of the line, <code>&gt; small_combined_temp.csv</code>, saves the result to <code>small_cobmined_temp.csv</code>. Because I need to add back the header, I use <code>head -1</code> to extract the first line of <code>combined.csv</code> and save it into <code>colnames.csv</code>. Finally, I bind the rows of both files using <code>cat colnames.csv small_combined_temp.csv</code> and save the result into <code>small_combined.cvs</code>. Taken together, all these steps took about 5 minutes (without counting the googling around for finding how to pass a fixed seed to <code>shuf</code>).
</p>
<p>
Now that I have this small dataset, I can write a small prototype:
</p>
<p>
First, you need to install <code>{sparklyr}</code>, <code>{rsparkling}</code> and <code>{h2o}</code>. Refer to <a href="https://github.com/h2oai/rsparkling">this</a> to know how to install the packages. I had a mismatch between the version of H2O that was automatically installed when I installed the <code>{h2o}</code> package, and the version of Spark that <code>{sparklyr}</code> installed but thankfully the <code>{h2o}</code> package returns a very helpful error message with the following lines:
</p>
<pre><code>detach("package:rsparkling", unload = TRUE)
                       if ("package:h2o" %in% search()) { detach("package:h2o", unload = TRUE) }
                       if (isNamespaceLoaded("h2o")){ unloadNamespace("h2o") }
                       remove.packages("h2o")
                       install.packages("h2o", type = "source", repos = "https://h2o-release.s3.amazonaws.com/h2o/rel-weierstrass/2/R")</code></pre>
<p>
which tells you which version to install.
</p>
<p>
So now, let’s load everything:
</p>
<pre class="r"><code>library(sparklyr)
library(rsparkling)
library(h2o)</code></pre>
<pre><code>## 
## ----------------------------------------------------------------------
## 
## Your next step is to start H2O:
##     &gt; h2o.init()
## 
## For H2O package documentation, ask for help:
##     &gt; ??h2o
## 
## After starting H2O, you can use the Web UI at http://localhost:54321
## For more information visit http://docs.h2o.ai
## 
## ----------------------------------------------------------------------</code></pre>
<pre><code>## 
## Attaching package: 'h2o'</code></pre>
<pre><code>## The following objects are masked from 'package:stats':
## 
##     cor, sd, var</code></pre>
<pre><code>## The following objects are masked from 'package:base':
## 
##     &amp;&amp;, %*%, %in%, ||, apply, as.factor, as.numeric, colnames,
##     colnames&lt;-, ifelse, is.character, is.factor, is.numeric, log,
##     log10, log1p, log2, round, signif, trunc</code></pre>
<pre class="r"><code>h2o.init()</code></pre>
<pre><code>## 
## H2O is not running yet, starting it now...
## 
## Note:  In case of errors look at the following log files:
##     /tmp/Rtmph48vf9/h2o_cbrunos_started_from_r.out
##     /tmp/Rtmph48vf9/h2o_cbrunos_started_from_r.err
## 
## 
## Starting H2O JVM and connecting: .. Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         1 seconds 944 milliseconds 
##     H2O cluster version:        3.16.0.2 
##     H2O cluster version age:    4 months and 15 days !!! 
##     H2O cluster name:           H2O_started_from_R_cbrunos_bpn152 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   6.98 GB 
##     H2O cluster total cores:    12 
##     H2O cluster allowed cores:  12 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.4.4 (2018-03-15)</code></pre>
<pre><code>## Warning in h2o.clusterInfo(): 
## Your H2O cluster version is too old (4 months and 15 days)!
## Please download and install the latest version from http://h2o.ai/download/</code></pre>
<p>
I left all the startup messages because they’re quite helpful. Especially that bit telling you to start <code>H2O</code> with <code>h2o.init()</code>. If something’s wrong, <code>h2o.init()</code> will give you helpful information.
</p>
<p>
Now that all this is loaded, I can start working on the data (the steps below are explained in detail in my <a href="http://www.brodrigues.co/blog/2018-02-16-importing_30gb_of_data/">previous blog post</a>):
</p>
<pre><code>spark_dir = "/my_2_to_disk/spark/"

config = spark_config()

config$`sparklyr.shell.driver-memory` &lt;- "4G"
config$`sparklyr.shell.executor-memory` &lt;- "4G"
config$`spark.yarn.executor.memoryOverhead` &lt;- "512"
config$`sparklyr.shell.driver-java-options` = paste0("-Djava.io.tmpdir=", spark_dir)

sc = spark_connect(master = "local", config = config)</code></pre>
<p>
Another useful function that allows you to check if everything is alright is <code>h2o_context()</code>:
</p>
<pre><code>h2o_context(sc)</code></pre>
<pre><code>&lt;jobj[12]&gt;
  org.apache.spark.h2o.H2OContext

Sparkling Water Context:
 * H2O name: sparkling-water-cbrunos_local-1520111879840
 * cluster size: 1
 * list of used nodes:
  (executorId, host, port)
  ------------------------
  (driver,127.0.0.1,54323)
  ------------------------

  Open H2O Flow in browser: http://127.0.0.1:54323 (CMD + click in Mac OSX)
</code></pre>
<p>
Now, let’s load the data into R with <code>{sparklyr}</code>:
</p>
<pre><code>air = spark_read_csv(sc, name = "air", path = "small_combined.csv")</code></pre>
<p>
Of course, here, using Spark is overkill, because <code>small_combined.csv</code> is only around 100MB big, so no need for <code>{sparklyr}</code> but as stated in the beginning this is only to have a quick and dirty prototype. Once all the pieces are working together, I can iterate on the real data, for which <code>{sparklyr}</code> will be needed. Now, if I needed to use <code>{dplyr}</code> I could use it on <code>air</code>, but I don’t want to do anything on it, so I convert it to a <code>h2o</code> data frame. <code>h2o</code> data frames are needed as arguments for the machine learning algorithms included in the <code>{h2o}</code> package. <code>as_h2o_frame()</code> is a function included in <code>{rsparkling}</code>:
</p>
<pre><code>air_hf = as_h2o_frame(sc, air)</code></pre>
<p>
Then, I convert the columns I need to factors (I am only using factors here):
</p>
<pre><code>air_hf$ORIGIN = as.factor(air_hf$ORIGIN)
air_hf$UNIQUE_CARRIER = as.factor(air_hf$UNIQUE_CARRIER)
air_hf$DEST = as.factor(air_hf$DEST)</code></pre>
<p>
<code>{h2o}</code> functions need the names of the predictors and of the target columns, so let’s define that:
</p>
<pre><code>target = "ARR_DELAY"
predictors = c("UNIQUE_CARRIER", "ORIGIN", "DEST")</code></pre>
<p>
Now, let’s train a random Forest, without any hyper parameter tweaking:
</p>
<pre><code>model = h2o.randomForest(predictors, target, training_frame = air_hf)</code></pre>
<p>
Now that this runs, I will in the future split the data into training, validation and test set, and train a model with better hyper parameters. For now, let’s take a look at the summary of <code>model</code>:
</p>
<pre><code>summary(model)</code></pre>
<pre><code>Model Details:
==============

H2ORegressionModel: drf
Model Key:  DRF_model_R_1520111880605_1
Model Summary:
  number_of_trees number_of_internal_trees model_size_in_bytes min_depth
1              50                       50            11055998        20
  max_depth mean_depth min_leaves max_leaves mean_leaves
1        20   20.00000       1856       6129  4763.42000

H2ORegressionMetrics: drf
** Reported on training data. **
** Metrics reported on Out-Of-Bag training samples **

MSE:  964.9246
RMSE:  31.06324
MAE:  17.65517
RMSLE:  NaN
Mean Residual Deviance :  964.9246





Scoring History:
             timestamp   duration number_of_trees training_rmse training_mae
1  2018-03-03 22:52:24  0.035 sec               0
2  2018-03-03 22:52:25  1.275 sec               1      30.93581     17.78216
3  2018-03-03 22:52:25  1.927 sec               2      31.36998     17.78867
4  2018-03-03 22:52:26  2.272 sec               3      31.36880     17.80359
5  2018-03-03 22:52:26  2.564 sec               4      31.29683     17.79467
6  2018-03-03 22:52:26  2.854 sec               5      31.31226     17.79467
7  2018-03-03 22:52:27  3.121 sec               6      31.26214     17.78542
8  2018-03-03 22:52:27  3.395 sec               7      31.20749     17.75703
9  2018-03-03 22:52:27  3.666 sec               8      31.19706     17.74753
10 2018-03-03 22:52:27  3.935 sec               9      31.16108     17.73547
11 2018-03-03 22:52:28  4.198 sec              10      31.13725     17.72493
12 2018-03-03 22:52:32  8.252 sec              27      31.07608     17.66648
13 2018-03-03 22:52:36 12.462 sec              44      31.06325     17.65474
14 2018-03-03 22:52:38 14.035 sec              50      31.06324     17.65517
   training_deviance
1
2          957.02450
3          984.07580
4          984.00150
5          979.49147
6          980.45794
7          977.32166
8          973.90720
9          973.25655
10         971.01272
11         969.52856
12         965.72249
13         964.92530
14         964.92462

Variable Importances: (Extract with `h2o.varimp`)
=================================================

Variable Importances:
        variable relative_importance scaled_importance percentage
1         ORIGIN    291883392.000000          1.000000   0.432470
2           DEST    266749168.000000          0.913890   0.395230
3 UNIQUE_CARRIER    116289536.000000          0.398411   0.172301
&gt;</code></pre>
<p>
If you found this blog post useful, you might want to follow me on <a href="https://www.twitter.com/brodriguesco">twitter</a> for blog post updates.
</p>



 ]]></description>
  <category>R</category>
  <category>programming</category>
  <guid>https://b-rodrigues.github.io/posts/2018-03-03-sparklyr_h2o_rsparkling.html</guid>
  <pubDate>Sat, 03 Mar 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Keep trying that api call with purrr::possibly()</title>
  <link>https://b-rodrigues.github.io/posts/2018-03-12-keep_trying.html</link>
  <description><![CDATA[ 




<p>
Sometimes you need to call an api to get some result from a web service, but sometimes this call might fail. You might get an error 500 for example, or maybe you’re making too many calls too fast. Regarding this last point, I really encourage you to read <a href="https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01">Ethics in Web Scraping</a>.
</p>
<p>
In this blog post I will show you how you can keep trying to make this api call using <code>purrr::possibly()</code>.
</p>
<p>
For this, let’s use this function that will simulate an api call:
</p>
<pre class="r"><code>get_data = function(){
  number = rbinom(1, 1, 0.9)
  ifelse(number == 0, "OK", stop("Error: too many calls!"))
}</code></pre>
<p>
This function simply returns a random draw from a binomial distribution. If this number equals 0 with probability 0.1, the function returns “OK”, if not, it throws an error. Because the probability of success is only 10%, your api call might be unsuccessful:
</p>
<pre class="r"><code>get_data()</code></pre>
<pre><code>Error in ifelse(number == 0, "OK", stop("Error: too many calls!")) :
  Error: too many calls!</code></pre>
<p>
How to keep trying until it works? For this, we’re going to use <code>purrr::possibly()</code>; this function takes another function as argument and either returns the result, or another output in case of error, that the user can define:
</p>
<pre class="r"><code>possibly_get_data = purrr::possibly(get_data, otherwise = NULL)</code></pre>
<p>
Let’s try it:
</p>
<pre class="r"><code>set.seed(12)
possibly_get_data()</code></pre>
<pre><code>## NULL</code></pre>
<p>
With <code>set.seed(12)</code>, the function returns a number different from 0, and thus throws an error: but because we’re wrapping the function around <code>purrr::possibly()</code>, the function now returns <code>NULL</code>. The first step is done; now we can use this to our advantage:
</p>
<pre class="r"><code>definitely_get_data = function(func, n_tries, sleep, ...){

  possibly_func = purrr::possibly(func, otherwise = NULL)

  result = NULL
  try_number = 1

  while(is.null(result) &amp;&amp; try_number &lt;= n_tries){
    print(paste("Try number: ", try_number))
    try_number = try_number + 1
    result = possibly_func(...)
    Sys.sleep(sleep)
  }

  return(result)
}</code></pre>
<p>
<code>definitely_get_data()</code> is a function that takes any function as argument, as well as a user provided number of tries (as well as <code>…</code> to pass further arguments to <code>func()</code>). Remember, if <code>func()</code> fails, it will return <code>NULL</code>; the while loop ensures that while the result is <code>NULL</code>, and the number of tries is below what you provided, the function will keep getting called. I didn’t talk about <code>sleep</code>; this argument is provided to <code>Sys.sleep()</code> which introduces a break between calls that is equal to <code>sleep</code> seconds. This ensures you don’t make too many calls too fast. Let’s try it out:
</p>
<pre class="r"><code>set.seed(123)
definitely_get_data(get_data, 10, 1)</code></pre>
<pre><code>## [1] "Try number:  1"
## [1] "Try number:  2"
## [1] "Try number:  3"
## [1] "Try number:  4"
## [1] "Try number:  5"</code></pre>
<pre><code>## [1] "OK"</code></pre>
<p>
It took 5 tries to get the result! However, if after 10 tries <code>get_data()</code> fails to return what you need it will stop (but you can increase the number of tries…).
</p>
<p>
If you found this blog post useful, you might want to follow me on <a href="https://www.twitter.com/brodriguesco">twitter</a> for blog post updates.
</p>



 ]]></description>
  <category>R</category>
  <category>programming</category>
  <guid>https://b-rodrigues.github.io/posts/2018-03-12-keep_trying.html</guid>
  <pubDate>Sat, 03 Mar 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Importing 30GB of data into R with sparklyr</title>
  <link>https://b-rodrigues.github.io/posts/2018-02-16-importing_30gb_of_data.html</link>
  <description><![CDATA[ 




<p>
Disclaimer: the first part of this blog post draws heavily from <a href="http://bconnelly.net/working-with-csvs-on-the-command-line/">Working with CSVs on the Command Line</a>, which is a beautiful resource that lists very nice tips and tricks to work with CSV files before having to load them into R, or any other statistical software. I highly recommend it! Also, if you find this interesting, read also <a href="https://www.datascienceatthecommandline.com/">Data Science at the Command Line</a> another great resource!
</p>
<p>
In this blog post I am going to show you how to analyze 30GB of data. 30GB of data does not qualify as big data, but it’s large enough that you cannot simply import it into R and start working on it, unless you have a machine with <em>a lot</em> of RAM.
</p>
<p>
Let’s start by downloading some data. I am going to import and analyze (very briefly) the airline dataset that you can download from Microsoft <a href="https://packages.revolutionanalytics.com/datasets/">here</a>. I downloaded the file <code>AirOnTimeCSV.zip</code> from <code>AirOnTime87to12</code>. Once you decompress it, you’ll end up with 303 csv files, each around 80MB. Before importing them into R, I will use command line tools to bind the rows together. But first, let’s make sure that the datasets all have the same columns. I am using Linux, and if you are too, or if you are using macOS, you can follow along. Windows users that installed the Linux Subsystem can also use the commands I am going to show! First, I’ll use the <code>head</code> command in bash. If you’re familiar with <code>head()</code> from R, the <code>head</code> command in bash works exactly the same:
</p>
<pre><code>[18-02-15 21:12] brodriguesco in /Documents/AirOnTimeCSV ➤ head -5 airOT198710.csv
"YEAR","MONTH","DAY_OF_MONTH","DAY_OF_WEEK","FL_DATE","UNIQUE_CARRIER","TAIL_NUM","FL_NUM",
1987,10,1,4,1987-10-01,"AA","","1",12478,"JFK","NY",12892,"LAX","CA","0900","0901",1.00,
1987,10,2,5,1987-10-02,"AA","","1",12478,"JFK","NY",12892,"LAX","CA","0900","0901",1.00
1987,10,3,6,1987-10-03,"AA","","1",12478,"JFK","NY",12892,"LAX","CA","0900","0859",-1.00
1987,10,4,7,1987-10-04,"AA","","1",12478,"JFK","NY",12892,"LAX","CA","0900","0900",0.00,</code></pre>
<p>
let’s also check the 5 first lines of the last file:
</p>
<pre><code>[18-02-15 21:13] cbrunos in brodriguesco in /Documents/AirOnTimeCSV ➤ head -5 airOT201212.csv
"YEAR","MONTH","DAY_OF_MONTH","DAY_OF_WEEK","FL_DATE","UNIQUE_CARRIER","TAIL_NUM","FL_NUM",
2012,12,1,6,2012-12-01,"AA","N322AA","1",12478,"JFK","NY",12892,"LAX","CA","0900","0852",
2012,12,2,7,2012-12-02,"AA","N327AA","1",12478,"JFK","NY",12892,"LAX","CA","0900","0853",
2012,12,3,1,2012-12-03,"AA","N319AA","1",12478,"JFK","NY",12892,"LAX","CA","0900","0856"
2012,12,4,2,2012-12-04,"AA","N329AA","1",12478,"JFK","NY",12892,"LAX","CA","0900","1006"</code></pre>
<p>
Why do that in bash instead of R? This way, I don’t need to import the data into R before checking its contents!
</p>
<p>
It does look like the structure did not change. Before importing the data into R, I am going to bind the rows of the datasets using other command line tools. Again, the reason I don’t import all the files into R is because I would need around 30GB of RAM to do so. So it’s easier to do it with bash:
</p>
<pre><code>head -1 airOT198710.csv &gt; combined.csv
for file in $(ls airOT*); do cat $file | sed "1 d" &gt;&gt; combined.csv; done</code></pre>
<p>
On the first line I use <code>head</code> again to only copy the column names (the first line of the first file) into a new file called <code>combined.csv</code>.
</p>
<p>
This <code>&gt;</code> operator looks like the now well known pipe operator in R, <code>%&gt;%</code>, but in bash, <code>%&gt;%</code> is actually <code>|</code>, not <code>&gt;</code>. <code>&gt;</code> redirects the output of the left hand side to a file on the right hand side, not to another command. On the second line, I loop over the files. I list the files with <code>ls</code>, and because I want only to loop over those that are named <code>airOTxxxxx</code> I use a regular expression, <code>airOT*</code> to only list those. The second part is <code>do cat $file</code>. <code>do</code> is self-explanatory, and <code>cat</code> stands for <code>catenate</code>. Think of it as <code>head</code>, but on all rows instead of just 5; it prints <code>$file</code> to the terminal. <code>$file</code> one element of the list of files I am looping over. But because I don’t want to see the contents of <code>$file</code> on my terminal, I redirect the output with the pipe, <code>|</code> to another command, <code>sed</code>. <code>sed</code> has an option, <code>"1 d"</code>, and what this does is filtering out the first line, containing the header, from <code>$file</code> before appending it with <code>&gt;&gt;</code> to <code>combined.csv</code>. If you found this interesting, read more about it <a href="http://bconnelly.net/working-with-csvs-on-the-command-line/#combining-rows-from-two-or-more-csvs">here</a>.
</p>
<p>
This creates a 30GB CSV file that you can then import. But how? There seems to be different ways to import and work with larger than memory data in R using your personal computer. I chose to use <code>{sparklyr}</code>, an R package that allows you to work with Apache Spark from R. Apache Spark is a <em>fast and general engine for large-scale data processing</em>, and <code>{sparklyr}</code> not only offers bindings to it, but also provides a complete <code>{dplyr}</code> backend. Let’s start:
</p>
<pre class="r"><code>library(sparklyr)
library(tidyverse)

spark_dir = "/my_2_to_disk/spark/"</code></pre>
<p>
I first load <code>{sparklyr}</code> and the <code>{tidyverse}</code> and also define a <code>spark_dir</code>. This is because Spark creates a lot of temporary files that I want to save there instead of my root partition, which is on my SSD. My root partition only has around 20GO of space left, so whenever I tried to import the data I would get the following error:
</p>
<pre><code>java.io.IOException: No space left on device</code></pre>
<p>
In order to avoid this error, I define this directory on my 2TO hard disk. I then define the temporary directory using the two lines below:
</p>
<pre class="r"><code>config = spark_config()

config$`sparklyr.shell.driver-java-options` &lt;-  paste0("-Djava.io.tmpdir=", spark_dir)</code></pre>
<p>
This is not sufficient however; when I tried to read in the data, I got another error:
</p>
<pre><code>java.lang.OutOfMemoryError: Java heap space</code></pre>
<p>
The solution for this one is to add the following lines to your <code>config()</code>:
</p>
<pre class="r"><code>config$`sparklyr.shell.driver-memory` &lt;- "4G"
config$`sparklyr.shell.executor-memory` &lt;- "4G"
config$`spark.yarn.executor.memoryOverhead` &lt;- "512"</code></pre>
<p>
Finally, I can load the data. Because I am working on my machine, I <em>connect</em> to a <code>"local"</code> Spark instance. Then, using <code>spark_read_csv()</code>, I specify the Spark connection, <code>sc</code>, I give a name to the data that will be inside the database and the path to it:
</p>
<pre class="r"><code>sc = spark_connect(master = "local", config = config)

air = spark_read_csv(sc, name = "air", path = "combined.csv")</code></pre>
<p>
On my machine, this took around 25 minutes, and RAM usage was around 6GO.
</p>
<p>
It is possible to use standard <code>{dplyr}</code> verbs with <code>{sparklyr}</code> objects, so if I want the mean delay at departure per day, I can simply write:
</p>
<pre class="r"><code>tic = Sys.time()
mean_dep_delay = air %&gt;%
  group_by(YEAR, MONTH, DAY_OF_MONTH) %&gt;%
  summarise(mean_delay = mean(DEP_DELAY))
(toc = Sys.time() - tic)
Time difference of 0.05634999 secs</code></pre>
<p>
That’s amazing, only 0.06 seconds to compute these means! Wait a minute, that’s weird… I mean my computer is brand new and quite powerful but still… Let’s take a look at <code>mean_dep_delay</code>:
</p>
<pre class="r"><code>head(mean_dep_delay)</code></pre>
<pre class="r"><code># Source:   lazy query [?? x 4]
# Database: spark_connection
# Groups:   YEAR, MONTH
   YEAR MONTH DAY_OF_MONTH mean_delay
  &lt;int&gt; &lt;int&gt;        &lt;int&gt;      &lt;dbl&gt;
1  1987    10            9       6.71
2  1987    10           10       3.72
3  1987    10           12       4.95
4  1987    10           14       4.53
5  1987    10           23       6.48
6  1987    10           29       5.77
Warning messages:
1: Missing values are always removed in SQL.
Use `AVG(x, na.rm = TRUE)` to silence this warning
2: Missing values are always removed in SQL.
Use `AVG(x, na.rm = TRUE)` to silence this warning</code></pre>
<p>
Surprisingly, this takes around 5 minutes to print? Why? Look at the class of <code>mean_dep_delay</code>: it’s a lazy query that only gets evaluated once I need it. Look at the first line; <code>lazy query [?? x 4]</code>. This means that I don’t even know how many rows are in <code>mean_dep_delay</code>! The contents of <code>mean_dep_delay</code> only get computed once I explicitly ask for them. I do so with the <code>collect()</code> function, which transfers the Spark object into R’s memory:
</p>
<pre class="r"><code>tic = Sys.time()
r_mean_dep_delay = collect(mean_dep_delay)
(toc = Sys.time() - tic)
Time difference of 5.2399 mins</code></pre>
<p>
Also, because it took such a long time to compute: I save it to disk:
</p>
<pre class="r"><code>saveRDS(r_mean_dep_delay, "mean_dep_delay.rds")</code></pre>
<p>
So now that I <em>transferred</em> this sparklyr table to a standard tibble in R, I can create a nice plot of departure delays:
</p>
<pre class="r"><code>library(lubridate)

dep_delay =  r_mean_dep_delay %&gt;%
  arrange(YEAR, MONTH, DAY_OF_MONTH) %&gt;%
  mutate(date = ymd(paste(YEAR, MONTH, DAY_OF_MONTH, sep = "-")))

ggplot(dep_delay, aes(date, mean_delay)) + geom_smooth()</code></pre>
<pre><code>## `geom_smooth()` using method = 'gam'</code></pre>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/import_30_gb_of_data.png" class="img-fluid"></p>
</div>
<p>
That’s it for now, but in a future blog post I will continue to explore this data!
</p>
<p>
If you found this blog post useful, you might want to follow me on <a href="https://www.twitter.com/brodriguesco">twitter</a> for blog post updates.
</p>



 ]]></description>
  <category>R</category>
  <category>programming</category>
  <guid>https://b-rodrigues.github.io/posts/2018-02-16-importing_30gb_of_data.html</guid>
  <pubDate>Fri, 16 Feb 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Predicting job search by training a random forest on an unbalanced dataset</title>
  <link>https://b-rodrigues.github.io/posts/2018-02-11-census-random_forest.html</link>
  <description><![CDATA[ 




<p>
Update 2022: there some literature advising against using techniques to artificially balance a dataset, for example <a href="https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocac093/6605096?searchresult=1&amp;login=false#">one</a>. Use at your own risks!
</p>
<p>
In this blog post, I am going to train a random forest on census data from the US to predict the probability that someone is looking for a job. To this end, I downloaded the US 1990 census data from the UCI <a href="https://archive.ics.uci.edu/ml/datasets/US+Census+Data+%281990%29">Machine Learning Repository</a>. Having a background in economics, I am always quite interested by such datasets. I downloaded the raw data which is around 820mb uncompressed. You can download it from this folder <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/census1990-mld/">here</a>.
</p>
<p>
Before training a random forest on it, some preprocessing is needed. First problem: the columns in the data do not have names. Actually, training a random forest on unamed variables is possible, but I like my columns to have names. The names are on a separate file, called <code>USCensus1990raw.attributes.txt</code>. This is how this file looks like:
</p>
<pre><code>VAR:        TYP:   DES:    LEN:   CAT:    VARIABLE/CATEGORY LABEL:
__________________________________________________________________________________
HISPANIC     C       X      3             Detailed Hispanic Origin Code See Append
                                  000     Not Hispanic 006 199
                                  001     Mexican, Mex Am 210 220
                                  002     Puerto Rican 261 270
                                  003     Cuban 271 274
                                  004     Other Hispanic 200 209, 250 260, 290 401

VAR:        TYP:   DES:    LEN:   CAT:    VARIABLE/CATEGORY LABEL:
__________________________________________________________________________________
HOUR89       C       X      2             Usual Hrs. Worked Per Week Last Yr. 1989
                                  00      N/a Less Than 16 Yrs. Old/did Not Work i
                                  99      99 or More Usual Hrs.

VAR:        TYP:   DES:    LEN:   CAT:    VARIABLE/CATEGORY LABEL:
__________________________________________________________________________________
HOURS        C       X      2             Hrs. Worked Last Week
                                  00      N/a Less Than 16 Yrs. Old/not At Work/un
                                  99      99 or More Hrs. Worked Last Week

VAR:        TYP:   DES:    LEN:   CAT:    VARIABLE/CATEGORY LABEL:
__________________________________________________________________________________
IMMIGR       C       X      2             Yr. of Entry
                                  00      Born in the U.S.
                                  01      1987 to 1990
                                  02      1985 to 1986
                                  03      1982 to 1984


                                  04      1980 or 1981
                                  05      1975 to 1979
                                  06      1970 to 1974
                                  07      1965 to 1969
                                  08      1960 to 1964
                                  09      1950 to 1959
                                  10      Before 1950</code></pre>
<p>
The variable names are always written in upper case and sometimes end with some numbers. Regular expressions will help extract these column names:
</p>
<pre class="r"><code>library(tidyverse)

census_raw = import("USCensus1990raw.data.txt")

attributes_raw = readLines("USCensus1990raw.attributes.txt")

column_names = str_extract_all(attributes_raw, "^[A-Z]+(\\d{1,}|[A-Z])\\s+") %&gt;%
  flatten %&gt;%
  str_trim %&gt;%
  tolower</code></pre>
<p>
Using <code>readLines</code> I load this text file into R. Then with <code>stringr::str_extract_all</code>, I can extract the variable names from this text file. The regular expression, <code><sup>1</sup>+(\d{1,}|[A-Z])\s+</code> can seem complicated, but by breaking it up, it’ll be clear:
</p>
<ul>
<li>
<code><sup>2</sup>+</code>: matches one or more uppercase letter, at the beginning of the line (hence the <code>^</code>)
</li>
<li>
<code>\d{1,}</code>: matches one or more digits
</li>
<li>
<code>[A-Z]\s+</code>: matches one uppercase letter, followed by one or more spaces
</li>
<li>
<code>(\d{1,}|[A-Z])\s+</code>: matches one or more digits OR (the <code>|</code>) matches one uppercase letter, followed by one or more spaces
</li>
</ul>
<p>
This regular expression matches only the variable names. By using <code>^</code> I only limit myself to the uppercase letters at the start of the line, which already removes a lot of unneeded lines from the text. Then, by matching numbers or letters, followed by spaces, I avoid matching strings such as <code>VAR:</code>. There’s probably a shorter way to write this regular expression, but since this one works, I stopped looking for another solution.
</p>
<p>
Now that I have a vector called <code>column_names</code>, I can baptize the columns in my dataset:
</p>
<pre class="r"><code>colnames(census_raw) &lt;- column_names</code></pre>
<p>
I also add a column called <code>caseid</code> to the dataset, but it’s actually not really needed. But it made me look for and find <code>rownames_to_column()</code>, which can be useful:
</p>
<pre class="r"><code>census = census_raw %&gt;%
  rownames_to_column("caseid")</code></pre>
<p>
Now I select the variables I need. I use <code>dplyr::select()</code> to select the columns I need (actually, I will remove some of these later for the purposes of the blog post, but will continue exploring them. Maybe write a part 2?):
</p>
<pre class="r"><code>census %&lt;&gt;%
  select(caseid, age, citizen, class, disabl1, disabl2, lang1, looking, fertil, hour89, hours, immigr,
         industry, means, occup, powpuma, powstate, pwgt1, race, ragechld, rearning,
         relat1, relat2, remplpar, rlabor, rpincome, rpob, rspouse, rvetserv, school, sex, tmpabsnt,
         travtime, week89, work89, worklwk, yearsch, yearwrk, yrsserv)</code></pre>
<p>
Now, I convert factor variables to factors and only relevel the <code>race</code> variable:
</p>
<pre class="r"><code>census %&lt;&gt;%
  mutate(race = case_when(race == 1 ~ "white",
                          race == 2 ~ "black",
                          !(race %in% c(1, 2)) ~ "other",
                          is.na(race) ~ NA_character_)) %&gt;%
  filter(looking != 0) %&gt;%
  mutate_at(vars(class, disabl1, disabl2, lang1, looking, fertil, immigr, industry, means,
                 occup, powstate, race, ragechld, remplpar, rlabor, rpob, rspouse,
                 rvetserv, school, sex, tmpabsnt, work89, worklwk, yearwrk),
            as.factor) %&gt;%
  select(looking, age, class, disabl1, disabl2, lang1, fertil, immigr,
         race, ragechld, remplpar, rlabor, rpob, rspouse,
         rvetserv, school, sex, tmpabsnt, work89, worklwk, yearwrk, rpincome, rearning,
         travtime, week89, work89, hours, yearsch, yrsserv) %&gt;%
  as_tibble

export(census, "regression_data.rds")</code></pre>
<p>
So the variable I want to predict is <code>looking</code> which has 2 levels (I removed the level <code>0</code>, which stands for <code>NA</code>). I convert all the variables that are supposed to be factors into factors using <code>mutate_at()</code> and then reselect a subsample of the columns. <code>census</code> is now a tibble with 39 columns and 2458285 rows. I will train the forest on a subsample only, because with cross validation it would take forever on the whole dataset.
</p>
<p>
I run the training on another script, that I will then run using the <code>Rscript</code> command instead of running it from Spacemacs (yes, I don’t use RStudio at home but Spacemacs + ESS). Here’s the script:
</p>
<pre class="r"><code>library(caret)
library(doParallel)
library(rio)

reg_data = import("regression_data.rds")</code></pre>
<pre class="r"><code>janitor::tabyl(reg_data$looking)</code></pre>
<pre class="r"><code>reg_data$looking      n   percent
1                1  75792 0.1089562
2                2 619827 0.8910438</code></pre>
<p>
90% of the individuals in the sample are not looking for a new job. For training purposes, I will only use 50000 observations instead of the whole sample. I’m already thinking about writing another blog post where I show how to use the whole data. But 50000 observations should be more than enough to have a pretty nice model. However, having 90% of observations belonging to a single class can cause problems with the model; the model might predict that everyone should belong to class 2 and in doing so, the model would be 90% accurate! Let’s ignore this for now, but later I am going to tackle this issue with a procedure calleds SMOTE.
</p>
<pre class="r"><code>set.seed(1234)
sample_df = sample_n(reg_data, 50000)</code></pre>
<p>
Now, using <code>caret::trainIndex()</code>, I partition the data into a training sample and a testing sample:
</p>
<pre class="r"><code>trainIndex = createDataPartition(sample_df$looking, p = 0.8,
                                 list = FALSE,
                                 times = 1)

train_data = sample_df[trainIndex, ]
test_data = sample_df[-trainIndex, ]</code></pre>
<p>
I also save the testing data to disk, because when the training is done I’ll lose my R session (remember, I’ll run the training using Rscript):
</p>
<pre class="r"><code>saveRDS(test_data, "test_data.rds")</code></pre>
<p>
Before training the model, I’ll change some options; I’ll do 5-fold cross validation that I repeat 5 times. This will further split the training set into training/testing sets which will increase my confidence in the metrics that I get from the training. This will ensure that the best model really is the best, and not a fluke resulting from the splitting of the data that I did beforehand. Then, I will test the best model on the testing data from above:
</p>
<pre class="r"><code>fitControl &lt;- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 5)</code></pre>
<p>
A very nice feature from the <code>caret</code> package is the possibility to make the training in parallel. For this, load the <code>doParallel</code> package (which I did above), and then register the number of cores you want to use for training with <code>makeCluster()</code>. You can replace <code>detectCores()</code> by the number of cores you want to use:
</p>
<pre class="r"><code>cl = makeCluster(detectCores())
registerDoParallel(cl)</code></pre>
<p>
Finally, we can train the model:
</p>
<pre class="r"><code>fit_caret = train(looking ~ .,
                  data = train_data,
                  trainControl = fitControl)</code></pre>
<p>
Because it takes around 1 and a half hours to train, I save the model to disk using <code>saveRDS()</code>:
</p>
<pre class="r"><code>saveRDS(fit_caret, "model_unbalanced.rds")</code></pre>
<p>
The picture below shows all the cores from my computer running and RAM usage being around 20gb during the training process:
</p>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/training_cpu.png" class="img-fluid"></p>
</div>
<p>
And this the results of training the random forest on the unbalanced data:
</p>
<pre class="r"><code>model_unbalanced = readRDS("model_unbalanced.rds")

test_data = readRDS("test_data.rds")

plot(model_unbalanced)

preds = predict.train(model_unbalanced, newdata = test_data)

confusionMatrix(preds, reference = test_data$looking)</code></pre>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/plot_acc_unbalanced.png" class="img-fluid"></p>
</div>
<pre class="r"><code>Confusion Matrix and Statistics

Reference
Prediction     1     2
1  1287   112
2   253 12348

Accuracy : 0.9739
95% CI : (0.9712, 0.9765)
    No Information Rate : 0.89
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16

                  Kappa : 0.8613
 Mcnemar's Test P-Value : 2.337e-13

            Sensitivity : 0.83571
            Specificity : 0.99101
         Pos Pred Value : 0.91994
         Neg Pred Value : 0.97992
             Prevalence : 0.11000
         Detection Rate : 0.09193
   Detection Prevalence : 0.09993
      Balanced Accuracy : 0.91336

       'Positive' Class : 1</code></pre>
<p>
If someone really is looking for a job, the model is able to predict it correctly 92% of the times and 98% of the times if that person is not looking for a job. It’s slightly better than simply saying than no one is looking for a job, which would be right 90% of the times, but not great either.
</p>
<p>
To train to make the model more accurate in predicting class 1, I will resample the training set, but by downsampling class 2 and upsampling class 1. This can be done with the function <code>SMOTE()</code> from the <code>{DMwR}</code> package. However, the testing set should have the same distribution as the population, so I should not apply <code>SMOTE()</code> to the testing set. I will resplit the data, but this time with a 95/5 % percent split; this way I have 5% of the original dataset used for testing, I can use <code>SMOTE()</code> on the 95% remaining training set. Because <code>SMOTE</code>ing takes some time, I save the <em>SMOTE</em>d training set using <code>readRDS()</code> for later use:
</p>
<pre class="r"><code>reg_data = import("regression_data.rds")


set.seed(1234)
trainIndex = createDataPartition(reg_data$looking, p = 0.95,
                                 list = FALSE,
                                 times = 1)

test_data = reg_data[-trainIndex, ]

saveRDS(test_data, "test_smote.rds")


# Balance training set
train_data = reg_data[trainIndex, ]

train_smote = DMwR::SMOTE(looking ~ ., train_data, perc.over = 100, perc.under=200)

saveRDS(train_smote, "train_smote.rds")</code></pre>
<p>
The testing set has 34780 observations and below you can see the distribution of the target variable, <code>looking</code>:
</p>
<pre class="r"><code>janitor::tabyl(test_data$looking)
  test_data$looking     n   percent
1                 1  3789 0.1089419
2                 2 30991 0.8910581</code></pre>
<p>
Here are the results:
</p>
<pre class="r"><code>model_smote = readRDS("model_smote.rds")

test_smote = readRDS("test_smote.rds")

plot(model_smote)

preds = predict.train(model_smote, newdata = test_smote)

confusionMatrix(preds, reference = test_smote$looking)</code></pre>
<pre class="r"><code>Confusion Matrix and Statistics

Reference
Prediction     1     2
1  3328  1142
2   461 29849

Accuracy : 0.9539
95% CI : (0.9517, 0.9561)
    No Information Rate : 0.8911
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16

                  Kappa : 0.78
 Mcnemar's Test P-Value : &lt; 2.2e-16

            Sensitivity : 0.87833
            Specificity : 0.96315
         Pos Pred Value : 0.74452
         Neg Pred Value : 0.98479
             Prevalence : 0.10894
         Detection Rate : 0.09569
   Detection Prevalence : 0.12852
      Balanced Accuracy : 0.92074

       'Positive' Class : 1</code></pre>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/plot_acc_unbalanced.png" class="img-fluid"></p>
</div>
<p>
The balanced accuracy is higher, but unlike what I expected (and hoped), this model is worse in predicting class 1! I will be trying one last thing; since I have a lot of data at my disposal, I will simply sample 25000 observations where the target variable <code>looking</code> equals 1, and then sample another 25000 observations where the target variable equals 2 (without using <code>SMOTE()</code>). Then I’ll simply bind the rows and train the model on that:
</p>
<pre class="r"><code>reg_data = import("regression_data.rds")


set.seed(1234)
trainIndex = createDataPartition(reg_data$looking, p = 0.95,
                                 list = FALSE,
                                 times = 1)

test_data = reg_data[-trainIndex, ]

saveRDS(test_data, "test_up_down.rds")


# Balance training set
train_data = reg_data[trainIndex, ]

train_data1 = train_data %&gt;%
  filter(looking == 1)

set.seed(1234)
train_data1 = sample_n(train_data1, 25000)


train_data2 = train_data %&gt;%
  filter(looking == 2)

set.seed(1234)
train_data2 = sample_n(train_data2, 25000)

train_up_down = bind_rows(train_data1, train_data2)


fitControl &lt;- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 5)

cl = makeCluster(detectCores())
registerDoParallel(cl)

fit_caret = train(looking ~ .,
                  data = train_up_down,
                  trControl = fitControl,
                  preProcess = c("center", "scale"))

saveRDS(fit_caret, "model_up_down.rds")</code></pre>
<p>
And here are the results:
</p>
<pre class="r"><code>model_up_down = readRDS("model_up_down.rds")

test_up_down = readRDS("test_up_down.rds")

plot(model_up_down)

preds = predict.train(model_up_down, newdata = test_up_down)

confusionMatrix(preds, reference = test_up_down$looking)</code></pre>
<pre class="r"><code>Confusion Matrix and Statistics

Reference
Prediction     1     2
1  3403  1629
2   386 29362

Accuracy : 0.9421
95% CI : (0.9396, 0.9445)
    No Information Rate : 0.8911
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16

                  Kappa : 0.7391
 Mcnemar's Test P-Value : &lt; 2.2e-16

            Sensitivity : 0.89813
            Specificity : 0.94744
         Pos Pred Value : 0.67627
         Neg Pred Value : 0.98702
             Prevalence : 0.10894
         Detection Rate : 0.09784
   Detection Prevalence : 0.14468
      Balanced Accuracy : 0.92278

       'Positive' Class : 1</code></pre>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/plot_acc_up_down.png" class="img-fluid"></p>
</div>
<p>
Looks like it’s not much better than using <code>SMOTE()</code>!
</p>
<p>
There are several ways I could achieve better predictions; tuning the model is one possibility, or perhaps going with another type of model altogether. I will certainly come back to this dataset in future blog posts!
</p>
<p>
Using the best model, let’s take a look at which variables are the most important for predicting job search:
</p>
<pre class="r"><code>&gt; varImp(model_unbalanced)
rf variable importance

only 20 most important variables shown (out of 109)

Overall
rlabor3   100.0000
rlabor6    35.2702
age         6.3758
rpincome    6.2964
tmpabsnt1   5.8047
rearning    5.3560
week89      5.2863
tmpabsnt2   4.0195
yearsch     3.4892
tmpabsnt3   1.7434
work892     1.3231
racewhite   0.9002
class1      0.7866
school2     0.7117
yearwrk2    0.6970
sex1        0.6955
disabl12    0.6809
lang12      0.6619
rpob23      0.6507
rspouse6    0.6330</code></pre>
<p>
It’s also possible to have a plot of the above:
</p>
<pre class="r"><code>plot(varImp(model_unbalanced))</code></pre>
<div style="text-align:center;">
<p><img src="https://b-rodrigues.github.io/assets/img/varimp.png" class="img-fluid"></p>
</div>
<p>
To make sense of this, we have to read the description of the features <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/census1990-mld/USCensus1990raw.attributes.txt">here</a>.
</p>
<p>
<code>rlabor3</code> is the most important variable, and means that the individual is unemployed. <code>rlabor6</code> means not in the labour force. Then the age of the individual as well as the individual’s income play a role. <code>tmpabsnt</code> is a variable that equals 1 if the individual is temporary absent from work, due to a layoff. All these variables having an influence on the probability of looking for a job make sense, but looks like a very simple model focusing on just a couple of variables would make as good a job as the random forest.
</p>
<p>
If you found this blog post useful, you might want to follow me on <a href="https://www.twitter.com/brodriguesco">twitter</a> for blog post updates.
</p>




<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>A-Z↩︎</p></li>
<li id="fn2"><p>A-Z↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>R</category>
  <category>econometrics</category>
  <guid>https://b-rodrigues.github.io/posts/2018-02-11-census-random_forest.html</guid>
  <pubDate>Sun, 11 Feb 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Mapping a list of functions to a list of datasets with a list of columns as arguments</title>
  <link>https://b-rodrigues.github.io/posts/2018-01-19-mapping_functions_with_any_cols.html</link>
  <description><![CDATA[ 




<p>
This week I had the opportunity to teach R at my workplace, again. This course was the “advanced R” course, and unlike the one I taught at the end of last year, I had one more day (so 3 days in total) where I could show my colleagues the joys of the <code>tidyverse</code> and R.
</p>
<p>
To finish the section on programming with R, which was the very last section of the whole 3 day course I wanted to blow their minds; I had already shown them packages from the <code>tidyverse</code> in the previous days, such as <code>dplyr</code>, <code>purrr</code> and <code>stringr</code>, among others. I taught them how to use <code>ggplot2</code>, <code>broom</code> and <code>modelr</code>. They also liked <code>janitor</code> and <code>rio</code> very much. I noticed that it took them a bit more time and effort for them to digest <code>purrr::map()</code> and <code>purrr::reduce()</code>, but they all seemed to see how powerful these functions were. To finish on a very high note, I showed them the ultimate <code>purrr::map()</code> use case.
</p>
<p>
Consider the following; imagine you have a situation where you are working on a list of datasets. These datasets might be the same, but for different years, or for different countries, or they might be completely different datasets entirely. If you used <code>rio::import_list()</code> to read them into R, you will have them in a nice list. Let’s consider the following list as an example:
</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre class="r"><code>data(mtcars)
data(iris)

data_list = list(mtcars, iris)</code></pre>
<p>
I made the choice to have completely different datasets. Now, I would like to map some functions to the columns of these datasets. If I only worked on one, for example on <code>mtcars</code>, I would do something like:
</p>
<pre class="r"><code>my_summarise_f = function(dataset, cols, funcs){
  dataset %&gt;%
    summarise_at(vars(!!!cols), funs(!!!funcs))
}</code></pre>
<p>
And then I would use my function like so:
</p>
<pre class="r"><code>mtcars %&gt;%
  my_summarise_f(quos(mpg, drat, hp), quos(mean, sd, max))</code></pre>
<pre><code>##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max
## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93
##   hp_max
## 1    335</code></pre>
<p>
<code>my_summarise_f()</code> takes a dataset, a list of columns and a list of functions as arguments and uses tidy evaluation to apply <code>mean()</code>, <code>sd()</code>, and <code>max()</code> to the columns <code>mpg</code>, <code>drat</code> and <code>hp</code> of <code>mtcars</code>. That’s pretty useful, but not useful enough! Now I want to apply this to the list of datasets I defined above. For this, let’s define the list of columns I want to work on:
</p>
<pre class="r"><code>cols_mtcars = quos(mpg, drat, hp)
cols_iris = quos(Sepal.Length, Sepal.Width)

cols_list = list(cols_mtcars, cols_iris)</code></pre>
<p>
Now, let’s use some <code>purrr</code> magic to apply the functions I want to the columns I have defined in <code>list_cols</code>:
</p>
<pre class="r"><code>map2(data_list,
     cols_list,
     my_summarise_f, funcs = quos(mean, sd, max))</code></pre>
<pre><code>## [[1]]
##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max
## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93
##   hp_max
## 1    335
## 
## [[2]]
##   Sepal.Length_mean Sepal.Width_mean Sepal.Length_sd Sepal.Width_sd
## 1          5.843333         3.057333       0.8280661      0.4358663
##   Sepal.Length_max Sepal.Width_max
## 1              7.9             4.4</code></pre>
<p>
That’s pretty useful, but not useful enough! I want to also use different functions to different datasets!
</p>
<p>
Well, let’s define a list of functions then:
</p>
<pre class="r"><code>funcs_mtcars = quos(mean, sd, max)
funcs_iris = quos(median, min)

funcs_list = list(funcs_mtcars, funcs_iris)</code></pre>
<p>
Because there is no <code>map3()</code>, we need to use <code>pmap()</code>:
</p>
<pre class="r"><code>pmap(
  list(
    dataset = data_list,
    cols = cols_list,
    funcs = funcs_list
  ),
  my_summarise_f)</code></pre>
<pre><code>## [[1]]
##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max
## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93
##   hp_max
## 1    335
## 
## [[2]]
##   Sepal.Length_median Sepal.Width_median Sepal.Length_min Sepal.Width_min
## 1                 5.8                  3              4.3               2</code></pre>
<p>
Now I’m satisfied! Let me tell you, this blew their minds 😄!
</p>
<p>
To be able to use things like that, I told them to always solve a problem for a single example, and from there, try to generalize their solution using functional programming tools found in <code>purrr</code>.
</p>
<p>
If you found this blog post useful, you might want to follow me on <a href="https://www.twitter.com/brodriguesco">twitter</a> for blog post updates.
</p>



 ]]></description>
  <category>R</category>
  <category>programming</category>
  <guid>https://b-rodrigues.github.io/posts/2018-01-19-mapping_functions_with_any_cols.html</guid>
  <pubDate>Fri, 19 Jan 2018 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
