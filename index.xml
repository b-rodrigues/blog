<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Econometrics and Free Software</title>
<link>https://b-rodrigues.github.io/</link>
<atom:link href="https://b-rodrigues.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.6.37</generator>
<lastBuildDate>Sat, 17 Aug 2019 00:00:00 GMT</lastBuildDate>
<item>
  <title>Modern R with the tidyverse is available on Leanpub</title>
  <link>https://b-rodrigues.github.io/posts/2019-08-17-modern_R.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://leanpub.com/modern_tidyverse"> <img src="https://b-rodrigues.github.io/assets/img/cover_modern.png" title="Click here to go to Leanpub" width="500" height="647"></a>
</p>
</div>
<p>
Yesterday I released an ebook on <a href="https://leanpub.com/modern_tidyverse">Leanpub</a>, called <em>Modern R with the tidyverse</em>, which you can also read for free <a href="https://b-rodrigues.github.io/modern_R/">here</a>.
</p>
<p>
In this blog post, I want to give some context.
</p>
<p>
<em>Modern R with the tidyverse</em> is the second ebook I release on Leanpub. I released the first one, called <a href="https://leanpub.com/fput">Functional programming and unit testing for data munging with R</a> around Christmas 2016 (I’ve retired it on Leanpub, but you can still read it for free <a href="https://b-rodrigues.github.io/fput/">here</a>) . I just had moved back to my home country of Luxembourg and started a new job as a research assistant at the statistical national institute. Since then, lots of things happened; I’ve changed jobs and joined PwC Luxembourg as a data scientist, was promoted to manager, finished my PhD, and most importantly of all, I became a father.
</p>
<p>
Through all this, I continued blogging and working on a new ebook, called <em>Modern R with the tidyverse</em>. At first, this was supposed to be a separate book from the first one, but as I continued writing, I realized that updating and finishing the first one, would take a lot of effort, and also, that it wouldn’t make much sense in keeping both separated. So I decided to merge the content from the first ebook with the second, and update everything in one go.
</p>
<p>
My very first notes were around 50 pages if memory serves, and I used them to teach R at the University of Strasbourg while I employed there as a research and teaching assistant and working on my PhD. These notes were the basis of <em>Functional programming and unit testing for data munging with R</em> and now <em>Modern R</em>. Chapter 2 of <em>Modern R</em> is almost a simple copy and paste from these notes (with more sections added). These notes were first written around 2012-2013ish.
</p>
<p>
<em>Modern R</em> is the kind of text I would like to have had when I first started playing around with R, sometime around 2009-2010. It starts from the beginning, but also goes quite into details in the later chapters. For instance, the section on <a href="https://b-rodrigues.github.io/modern_R/functional-programming.html#modeling-with-functional-programming">modeling with functional programming</a> is quite advanced, but I believe that readers that read through all the book and reached that part would be armed with all the needed knowledge to follow. At least, this is my hope.
</p>
<p>
Now, the book is still not finished. Two chapters are missing, but it should not take me long to finish them as I already have drafts lying around. However, exercises might still be in wrong places, and more are required. Also, generally, more polishing is needed.
</p>
<p>
As written in the first paragraph of this section, the book is available on <a href="https://leanpub.com/modern_tidyverse">Leanpub</a>. Unlike my previous ebook, this one costs money; a minimum price of 4.99$ and a recommended price of 14.99$, but as mentioned you can read it for free <a href="https://b-rodrigues.github.io/modern_R/">online</a>. I’ve hesitated to give it a minimum price of 0$, but I figured that since the book can be read for free online, and that Leanpub has a 45 days return policy where readers can get 100% reimbursed, no questions asked (and keep the downloaded ebook), readers were not taking a lot of risks by buying it for 5 bucks. I sure hope however that readers will find that this ebook is worth at least 5 bucks!
</p>
<p>
Now why should you read it? There’s already a lot of books on learning how to use R. Well, I don’t really want to convince you to read it. But some people do seem to like my style of writing and my blog posts, so I guess these same people, or similar people, might like the ebook. Also, I think that this ebook covers a lot of different topics, enough of them to make you an efficient R user. But as I’ve written in the introduction of <em>Modern R</em>:
</p>
<p>
<em>So what you can expect from this book is that this book is not the only one you should read.</em>
</p>
<p>
Anyways, hope you’ll enjoy <em>Modern R</em>, suggestions, criticisms and reviews welcome!
</p>
<p>
By the way, the cover of the book is a painting by John William Waterhouse, depicting Diogenes of Sinope, an ancient Greek philosopher, an absolute mad lad. Read his Wikipedia page, it’s worth it.
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-08-17-modern_R.html</guid>
  <pubDate>Sat, 17 Aug 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Using linear models with binary dependent variables, a simulation study</title>
  <link>https://b-rodrigues.github.io/posts/2019-08-14-lpm.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://psyarxiv.com/4gmbv"> <img src="https://b-rodrigues.github.io/assets/img/illegal.png" title="Even psychologists are not safe" width="800" height="612"></a>
</p>
</div>
<p>
This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free <a href="https://b-rodrigues.github.io/modern_R/functional-programming.html#modeling-with-functional-programming">here</a>. This is taken from Chapter 8, in which I discuss advanced functional programming methods for modeling.
</p>
<p>
As written just above (note: as written above <em>in the book</em>), <code>map()</code> simply applies a function to a list of inputs, and in the previous section we mapped <code>ggplot()</code> to generate many plots at once. This approach can also be used to map any modeling functions, for instance <code>lm()</code> to a list of datasets.
</p>
<p>
For instance, suppose that you wish to perform a Monte Carlo simulation. Suppose that you are dealing with a binary choice problem; usually, you would use a logistic regression for this.
</p>
<p>
However, in certain disciplines, especially in the social sciences, the so-called Linear Probability Model is often used as well. The LPM is a simple linear regression, but unlike the standard setting of a linear regression, the dependent variable, or target, is a binary variable, and not a continuous variable. Before you yell “Wait, that’s illegal”, you should know that in practice LPMs do a good job of estimating marginal effects, which is what social scientists and econometricians are often interested in. Marginal effects are another way of interpreting models, giving how the outcome (or the target) changes given a change in a independent variable (or a feature). For instance, a marginal effect of 0.10 for age would mean that probability of success would increase by 10% for each added year of age.
</p>
<p>
There has been a lot of discussion on logistic regression vs LPMs, and there are pros and cons of using LPMs. Micro-econometricians are still fond of LPMs, even though the pros of LPMs are not really convincing. However, quoting Angrist and Pischke:
</p>
<p>
“While a nonlinear model may fit the CEF (population conditional expectation function) for LDVs (limited dependent variables) more closely than a linear model, when it comes to marginal effects, this probably matters little” (source: <em>Mostly Harmless Econometrics</em>)
</p>
<p>
so LPMs are still used for estimating marginal effects.
</p>
<p>
Let us check this assessment with one example. First, we simulate some data, then run a logistic regression and compute the marginal effects, and then compare with a LPM:
</p>
<pre class="r"><code>set.seed(1234)
x1 &lt;- rnorm(100)
x2 &lt;- rnorm(100)
  
z &lt;- .5 + 2*x1 + 4*x2

p &lt;- 1/(1 + exp(-z))

y &lt;- rbinom(100, 1, p)

df &lt;- tibble(y = y, x1 = x1, x2 = x2)</code></pre>
<p>
This data generating process generates data from a binary choice model. Fitting the model using a logistic regression allows us to recover the structural parameters:
</p>
<pre class="r"><code>logistic_regression &lt;- glm(y ~ ., data = df, family = binomial(link = "logit"))</code></pre>
<p>
Let’s see a summary of the model fit:
</p>
<pre class="r"><code>summary(logistic_regression)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ ., family = binomial(link = "logit"), data = df)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.91941  -0.44872   0.00038   0.42843   2.55426  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   0.0960     0.3293   0.292 0.770630    
## x1            1.6625     0.4628   3.592 0.000328 ***
## x2            3.6582     0.8059   4.539 5.64e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 138.629  on 99  degrees of freedom
## Residual deviance:  60.576  on 97  degrees of freedom
## AIC: 66.576
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<p>
We do recover the parameters that generated the data, but what about the marginal effects? We can get the marginal effects easily using the <code>{margins}</code> package:
</p>
<pre class="r"><code>library(margins)

margins(logistic_regression)</code></pre>
<pre><code>## Average marginal effects</code></pre>
<pre><code>## glm(formula = y ~ ., family = binomial(link = "logit"), data = df)</code></pre>
<pre><code>##      x1     x2
##  0.1598 0.3516</code></pre>
<p>
Or, even better, we can compute the <em>true</em> marginal effects, since we know the data generating process:
</p>
<pre class="r"><code>meffects &lt;- function(dataset, coefs){
  X &lt;- dataset %&gt;% 
  select(-y) %&gt;% 
  as.matrix()
  
  dydx_x1 &lt;- mean(dlogis(X%*%c(coefs[2], coefs[3]))*coefs[2])
  dydx_x2 &lt;- mean(dlogis(X%*%c(coefs[2], coefs[3]))*coefs[3])
  
  tribble(~term, ~true_effect,
          "x1", dydx_x1,
          "x2", dydx_x2)
}

(true_meffects &lt;- meffects(df, c(0.5, 2, 4)))</code></pre>
<pre><code>## # A tibble: 2 x 2
##   term  true_effect
##   &lt;chr&gt;       &lt;dbl&gt;
## 1 x1          0.175
## 2 x2          0.350</code></pre>
<p>
Ok, so now what about using this infamous Linear Probability Model to estimate the marginal effects?
</p>
<pre class="r"><code>lpm &lt;- lm(y ~ ., data = df)

summary(lpm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ ., data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.83953 -0.31588 -0.02885  0.28774  0.77407 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.51340    0.03587  14.314  &lt; 2e-16 ***
## x1           0.16771    0.03545   4.732 7.58e-06 ***
## x2           0.31250    0.03449   9.060 1.43e-14 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.3541 on 97 degrees of freedom
## Multiple R-squared:  0.5135, Adjusted R-squared:  0.5034 
## F-statistic: 51.18 on 2 and 97 DF,  p-value: 6.693e-16</code></pre>
<p>
It’s not too bad, but maybe it could have been better in other circumstances. Perhaps if we had more observations, or perhaps for a different set of structural parameters the results of the LPM would have been closer. The LPM estimates the marginal effect of <code>x1</code> to be 0.1677134 vs 0.1597956 for the logistic regression and for <code>x2</code>, the LPM estimation is 0.3124966 vs 0.351607. The <em>true</em> marginal effects are 0.1750963 and 0.3501926 for <code>x1</code> and <code>x2</code> respectively.
</p>
<p>
Just as to assess the accuracy of a model data scientists perform cross-validation, a Monte Carlo study can be performed to asses how close the estimation of the marginal effects using a LPM is to the marginal effects derived from a logistic regression. It will allow us to test with datasets of different sizes, and generated using different structural parameters.
</p>
<p>
First, let’s write a function that generates data. The function below generates 10 datasets of size 100 (the code is inspired by this <a href="https://stats.stackexchange.com/a/46525">StackExchange answer</a>):
</p>
<pre class="r"><code>generate_datasets &lt;- function(coefs = c(.5, 2, 4), sample_size = 100, repeats = 10){

  generate_one_dataset &lt;- function(coefs, sample_size){
  x1 &lt;- rnorm(sample_size)
  x2 &lt;- rnorm(sample_size)
  
  z &lt;- coefs[1] + coefs[2]*x1 + coefs[3]*x2

  p &lt;- 1/(1 + exp(-z))

  y &lt;- rbinom(sample_size, 1, p)

  df &lt;- tibble(y = y, x1 = x1, x2 = x2)
  }

  simulations &lt;- rerun(.n = repeats, generate_one_dataset(coefs, sample_size))
 
  tibble("coefs" = list(coefs), "sample_size" = sample_size, "repeats" = repeats, "simulations" = list(simulations))
}</code></pre>
<p>
Let’s first generate one dataset:
</p>
<pre class="r"><code>one_dataset &lt;- generate_datasets(repeats = 1)</code></pre>
<p>
Let’s take a look at <code>one_dataset</code>:
</p>
<pre class="r"><code>one_dataset</code></pre>
<pre><code>## # A tibble: 1 x 4
##   coefs     sample_size repeats simulations
##   &lt;list&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;     
## 1 &lt;dbl [3]&gt;         100       1 &lt;list [1]&gt;</code></pre>
<p>
As you can see, the tibble with the simulated data is inside a list-column called <code>simulations</code>. Let’s take a closer look:
</p>
<pre class="r"><code>str(one_dataset$simulations)</code></pre>
<pre><code>## List of 1
##  $ :List of 1
##   ..$ :Classes 'tbl_df', 'tbl' and 'data.frame': 100 obs. of  3 variables:
##   .. ..$ y : int [1:100] 0 1 1 1 0 1 1 0 0 1 ...
##   .. ..$ x1: num [1:100] 0.437 1.06 0.452 0.663 -1.136 ...
##   .. ..$ x2: num [1:100] -2.316 0.562 -0.784 -0.226 -1.587 ...</code></pre>
<p>
The structure is quite complex, and it’s important to understand this, because it will have an impact on the next lines of code; it is a list, containing a list, containing a dataset! No worries though, we can still map over the datasets directly, by using <code>modify_depth()</code> instead of <code>map()</code>.
</p>
<p>
Now, let’s fit a LPM and compare the estimation of the marginal effects with the <em>true</em> marginal effects. In order to have some confidence in our results, we will not simply run a linear regression on that single dataset, but will instead simulate hundreds, then thousands and ten of thousands of data sets, get the marginal effects and compare them to the true ones (but here I won’t simulate more than 500 datasets).
</p>
<p>
Let’s first generate 10 datasets:
</p>
<pre class="r"><code>many_datasets &lt;- generate_datasets()</code></pre>
<p>
Now comes the tricky part. I have this object, <code>many_datasets</code> looking like this:
</p>
<pre class="r"><code>many_datasets</code></pre>
<pre><code>## # A tibble: 1 x 4
##   coefs     sample_size repeats simulations
##   &lt;list&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;     
## 1 &lt;dbl [3]&gt;         100      10 &lt;list [10]&gt;</code></pre>
<p>
I would like to fit LPMs to the 10 datasets. For this, I will need to use all the power of functional programming and the <code>{tidyverse}</code>. I will be adding columns to this data frame using <code>mutate()</code> and mapping over the <code>simulations</code> list-column using <code>modify_depth()</code>. The list of data frames is at the second level (remember, it’s a list containing a list containing data frames).
</p>
<p>
I’ll start by fitting the LPMs, then using <code>broom::tidy()</code> I will get a nice data frame of the estimated parameters. I will then only select what I need, and then bind the rows of all the data frames. I will do the same for the <em>true</em> marginal effects.
</p>
<p>
I highly suggest that you run the following lines, one after another. It is complicated to understand what’s going on if you are not used to such workflows. However, I hope to convince you that once it will click, it’ll be much more intuitive than doing all this inside a loop. Here’s the code:
</p>
<pre class="r"><code>results &lt;- many_datasets %&gt;% 
  mutate(lpm = modify_depth(simulations, 2, ~lm(y ~ ., data = .x))) %&gt;% 
  mutate(lpm = modify_depth(lpm, 2, broom::tidy)) %&gt;% 
  mutate(lpm = modify_depth(lpm, 2, ~select(., term, estimate))) %&gt;% 
  mutate(lpm = modify_depth(lpm, 2, ~filter(., term != "(Intercept)"))) %&gt;% 
  mutate(lpm = map(lpm, bind_rows)) %&gt;% 
  mutate(true_effect = modify_depth(simulations, 2, ~meffects(., coefs = coefs[[1]]))) %&gt;% 
  mutate(true_effect = map(true_effect, bind_rows))</code></pre>
<p>
This is how results looks like:
</p>
<pre class="r"><code>results</code></pre>
<pre><code>## # A tibble: 1 x 6
##   coefs     sample_size repeats simulations lpm             true_effect    
##   &lt;list&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;      &lt;list&gt;          &lt;list&gt;         
## 1 &lt;dbl [3]&gt;         100      10 &lt;list [10]&gt; &lt;tibble [20 × … &lt;tibble [20 × …</code></pre>
<p>
Let’s take a closer look to the <code>lpm</code> and <code>true_effect</code> columns:
</p>
<pre class="r"><code>results$lpm</code></pre>
<pre><code>## [[1]]
## # A tibble: 20 x 2
##    term  estimate
##    &lt;chr&gt;    &lt;dbl&gt;
##  1 x1       0.228
##  2 x2       0.353
##  3 x1       0.180
##  4 x2       0.361
##  5 x1       0.165
##  6 x2       0.374
##  7 x1       0.182
##  8 x2       0.358
##  9 x1       0.125
## 10 x2       0.345
## 11 x1       0.171
## 12 x2       0.331
## 13 x1       0.122
## 14 x2       0.309
## 15 x1       0.129
## 16 x2       0.332
## 17 x1       0.102
## 18 x2       0.374
## 19 x1       0.176
## 20 x2       0.410</code></pre>
<pre class="r"><code>results$true_effect</code></pre>
<pre><code>## [[1]]
## # A tibble: 20 x 2
##    term  true_effect
##    &lt;chr&gt;       &lt;dbl&gt;
##  1 x1          0.183
##  2 x2          0.366
##  3 x1          0.166
##  4 x2          0.331
##  5 x1          0.174
##  6 x2          0.348
##  7 x1          0.169
##  8 x2          0.339
##  9 x1          0.167
## 10 x2          0.335
## 11 x1          0.173
## 12 x2          0.345
## 13 x1          0.157
## 14 x2          0.314
## 15 x1          0.170
## 16 x2          0.340
## 17 x1          0.182
## 18 x2          0.365
## 19 x1          0.161
## 20 x2          0.321</code></pre>
<p>
Let’s bind the columns, and compute the difference between the <em>true</em> and estimated marginal effects:
</p>
<pre class="r"><code>simulation_results &lt;- results %&gt;% 
  mutate(difference = map2(.x = lpm, .y = true_effect, bind_cols)) %&gt;% 
  mutate(difference = map(difference, ~mutate(., difference = true_effect - estimate))) %&gt;% 
  mutate(difference = map(difference, ~select(., term, difference))) %&gt;% 
  pull(difference) %&gt;% 
  .[[1]]</code></pre>
<p>
Let’s take a look at the simulation results:
</p>
<pre class="r"><code>simulation_results %&gt;% 
  group_by(term) %&gt;% 
  summarise(mean = mean(difference), 
            sd = sd(difference))</code></pre>
<pre><code>## # A tibble: 2 x 3
##   term     mean     sd
##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1 x1     0.0122 0.0370
## 2 x2    -0.0141 0.0306</code></pre>
<p>
Already with only 10 simulated datasets, the difference in means is not significant. Let’s rerun the analysis, but for difference sizes. In order to make things easier, we can put all the code into a nifty function:
</p>
<pre class="r"><code>monte_carlo &lt;- function(coefs, sample_size, repeats){
  many_datasets &lt;- generate_datasets(coefs, sample_size, repeats)
  
  results &lt;- many_datasets %&gt;% 
    mutate(lpm = modify_depth(simulations, 2, ~lm(y ~ ., data = .x))) %&gt;% 
    mutate(lpm = modify_depth(lpm, 2, broom::tidy)) %&gt;% 
    mutate(lpm = modify_depth(lpm, 2, ~select(., term, estimate))) %&gt;% 
    mutate(lpm = modify_depth(lpm, 2, ~filter(., term != "(Intercept)"))) %&gt;% 
    mutate(lpm = map(lpm, bind_rows)) %&gt;% 
    mutate(true_effect = modify_depth(simulations, 2, ~meffects(., coefs = coefs[[1]]))) %&gt;% 
    mutate(true_effect = map(true_effect, bind_rows))

  simulation_results &lt;- results %&gt;% 
    mutate(difference = map2(.x = lpm, .y = true_effect, bind_cols)) %&gt;% 
    mutate(difference = map(difference, ~mutate(., difference = true_effect - estimate))) %&gt;% 
    mutate(difference = map(difference, ~select(., term, difference))) %&gt;% 
    pull(difference) %&gt;% 
    .[[1]]

  simulation_results %&gt;% 
    group_by(term) %&gt;% 
    summarise(mean = mean(difference), 
              sd = sd(difference))
}</code></pre>
<p>
And now, let’s run the simulation for different parameters and sizes:
</p>
<pre class="r"><code>monte_carlo(c(.5, 2, 4), 100, 10)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   term      mean     sd
##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;
## 1 x1    -0.00826 0.0291
## 2 x2    -0.00732 0.0412</code></pre>
<pre class="r"><code>monte_carlo(c(.5, 2, 4), 100, 100)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   term     mean     sd
##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1 x1    0.00360 0.0392
## 2 x2    0.00517 0.0446</code></pre>
<pre class="r"><code>monte_carlo(c(.5, 2, 4), 100, 500)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   term       mean     sd
##   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;
## 1 x1    -0.00152  0.0371
## 2 x2    -0.000701 0.0423</code></pre>
<pre class="r"><code>monte_carlo(c(pi, 6, 9), 100, 10)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   term      mean     sd
##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;
## 1 x1    -0.00829 0.0546
## 2 x2     0.00178 0.0370</code></pre>
<pre class="r"><code>monte_carlo(c(pi, 6, 9), 100, 100)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   term     mean     sd
##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1 x1    0.0107  0.0608
## 2 x2    0.00831 0.0804</code></pre>
<pre class="r"><code>monte_carlo(c(pi, 6, 9), 100, 500)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   term     mean     sd
##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1 x1    0.00879 0.0522
## 2 x2    0.0113  0.0668</code></pre>
<p>
We see that, at least for this set of parameters, the LPM does a good job of estimating marginal effects.
</p>
<p>
Now, this study might in itself not be very interesting to you, but I believe the general approach is quite useful and flexible enough to be adapted to all kinds of use-cases.
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-08-14-lpm.html</guid>
  <pubDate>Wed, 14 Aug 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Statistical matching, or when one single data source is not enough</title>
  <link>https://b-rodrigues.github.io/posts/2019-07-19-statmatch.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Row_and_column_vectors"> <img src="https://b-rodrigues.github.io/assets/img/columns.jpg" title="Not that kind of columns" width="1119" height="720"></a>
</p>
</div>
<p>
I was recently asked how to go about matching several datasets where different samples of individuals were interviewed. This sounds like a big problem; say that you have dataset A and B, and that A contain one sample of individuals, and B another sample of individuals, then how could you possibly match the datasets? Matching datasets requires a common identifier, for instance, suppose that A contains socio-demographic information on a sample of individuals I, while B, contains information on wages and hours worked on the same sample of individuals I, then yes, it will be possible to match/merge/join both datasets.
</p>
<p>
But that was not what I was asked about; I was asked about a situation where the same population gets sampled twice, and each sample answers to a different survey. For example the first survey is about labour market information and survey B is about family structure. Would it be possible to combine the information from both datasets?
</p>
<p>
To me, this sounded a bit like missing data imputation problem, but where all the information about the variables of interest was missing! I started digging a bit, and found that not only there was already quite some literature on it, there is even a package for this, called <code>{StatMatch}</code> with a very detailed <a href="https://cran.r-project.org/web/packages/StatMatch/vignettes/Statistical_Matching_with_StatMatch.pdf">vignette</a>. The vignette is so detailed, that I will not write any code, I just wanted to share this package!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-07-19-statmatch.html</guid>
  <pubDate>Fri, 19 Jul 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Curly-Curly, the successor of Bang-Bang</title>
  <link>https://b-rodrigues.github.io/posts/2019-06-20-tidy_eval_saga.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Row_and_column_vectors"> <img src="https://b-rodrigues.github.io/assets/img/curly.jpg" title="Not that kind of columns" width="1119" height="720"></a>
</p>
</div>
<p>
Writing functions that take data frame columns as arguments is a problem that most R users have been confronted with at some point. There are different ways to tackle this issue, and this blog post will focus on the solution provided by the latest release of the <code>{rlang}</code> package. You can read the announcement <a href="https://www.tidyverse.org/articles/2019/06/rlang-0-4-0/">here</a>, which explains really well what was wrong with the old syntax, and how the new syntax works now.
</p>
<p>
I have written about the problem of writing functions that use data frame columns as arguments <a href="../posts/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions.html">three years ago</a> and <a href="../posts/2017-08-27-why_tidyeval.html">two year ago</a> too. <a href="../posts/2018-01-19-mapping_functions_with_any_cols.html">Last year</a>, I wrote a blog post that showed how to map a list of functions to a list of datasets with a list of columns as arguments that used the <code>!!quo(column_name)</code> syntax (the <code>!!</code> is pronounced <em>bang-bang</em>). Now, there is a new sheriff in town, <code>{{}}</code>, introduced in <code>{rlang}</code> version 0.4.0 that makes things even easier. The suggested pronunciation of <code>{{}}</code> is <em>curly-curly</em>, but there is no <a href="https://twitter.com/JonTheGeek/status/1144815369766547456">consensus yet</a>.
</p>
<p>
First, let’s load the <code>{tidyverse}</code>:
</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<p>
Let’s suppose that I need to write a function that takes a data frame, as well as a column from this data frame as arguments:
</p>
<pre class="r"><code>how_many_na &lt;- function(dataframe, column_name){
  dataframe %&gt;%
    filter(is.na(column_name)) %&gt;%
    count()
}</code></pre>
<p>
Let’s try this function out on the <code>starwars</code> data:
</p>
<pre class="r"><code>data(starwars)

head(starwars)</code></pre>
<pre><code>## # A tibble: 6 x 13
##   name  height  mass hair_color skin_color eye_color birth_year gender
##   &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; 
## 1 Luke…    172    77 blond      fair       blue            19   male  
## 2 C-3PO    167    75 &lt;NA&gt;       gold       yellow         112   &lt;NA&gt;  
## 3 R2-D2     96    32 &lt;NA&gt;       white, bl… red             33   &lt;NA&gt;  
## 4 Dart…    202   136 none       white      yellow          41.9 male  
## 5 Leia…    150    49 brown      light      brown           19   female
## 6 Owen…    178   120 brown, gr… light      blue            52   male  
## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,
## #   vehicles &lt;list&gt;, starships &lt;list&gt;</code></pre>
<p>
As you can see, there are missing values in the <code>hair_color</code> column. Let’s try to count how many missing values are in this column:
</p>
<pre class="r"><code>how_many_na(starwars, hair_color)</code></pre>
<pre><code>Error: object 'hair_color' not found</code></pre>
<p>
R cannot find the <code>hair_color</code> column, and yet it is in the data! Well, this is actually exactly the issue. The issue is that the column is inside the dataframe, but when calling the function with <code>hair_color</code> as the second argument, R is looking for a variable called <code>hair_color</code> that does not exist. What about trying with <code>"hair_color"</code>?
</p>
<pre class="r"><code>how_many_na(starwars, "hair_color")</code></pre>
<pre><code>## # A tibble: 1 x 1
##       n
##   &lt;int&gt;
## 1     0</code></pre>
<p>
Now we get something, but something wrong!
</p>
<p>
One way to solve this issue, is to not use the <code>filter()</code> function, and instead rely on base R:
</p>
<pre class="r"><code>how_many_na_base &lt;- function(dataframe, column_name){
  na_index &lt;- is.na(dataframe[, column_name])
  nrow(dataframe[na_index, column_name])
}

how_many_na_base(starwars, "hair_color")</code></pre>
<pre><code>## [1] 5</code></pre>
<p>
This works, but not using the <code>{tidyverse}</code> at all is not an option, at least for me. For instance, the next function, which uses a grouping variable, would be difficult to implement without the <code>{tidyverse}</code>:
</p>
<pre class="r"><code>summarise_groups &lt;- function(dataframe, grouping_var, column_name){
  dataframe %&gt;%
    group_by(grouping_var) %&gt;%  
    summarise(mean(column_name, na.rm = TRUE))
}</code></pre>
<p>
Calling this function results in the following error message:
</p>
<pre><code>Error: Column `grouping_var` is unknown</code></pre>
<p>
Before the release of <code>{rlang}</code> 0.4.0 this is was the solution:
</p>
<pre class="r"><code>summarise_groups &lt;- function(dataframe, grouping_var, column_name){

  grouping_var &lt;- enquo(grouping_var)
  column_name &lt;- enquo(column_name)
  mean_name &lt;- paste0("mean_", quo_name(column_name))

  dataframe %&gt;%
    group_by(!!grouping_var) %&gt;%  
    summarise(!!(mean_name) := mean(!!column_name, na.rm = TRUE))
}</code></pre>
<p>
The core of the function remained very similar to the version from before, but now one has to use the <code>enquo()</code>-<code>!!</code> syntax. While not overly difficult to use, it is cumbersome.
</p>
<p>
Now this can be simplified using the new <code>{{}}</code> syntax:
</p>
<pre class="r"><code>summarise_groups &lt;- function(dataframe, grouping_var, column_name){

  dataframe %&gt;%
    group_by({{grouping_var}}) %&gt;%  
    summarise({{column_name}} := mean({{column_name}}, na.rm = TRUE))
}</code></pre>
<p>
Much easier and cleaner! You still have to use the <code>:=</code> operator instead of <code>=</code> for the column name however. Also, from my understanding, if you want to modify the column names, for instance in this case return <code>"mean_height"</code> instead of <code>height</code> you have to keep using the <code>enquo()</code>-<code>!!</code> syntax.
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-06-20-tidy_eval_saga.html</guid>
  <pubDate>Sat, 29 Jun 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Intermittent demand, Croston and Die Hard</title>
  <link>https://b-rodrigues.github.io/posts/2019-06-12-intermittent.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/List_of_Christmas_films"> <img src="https://b-rodrigues.github.io/assets/img/diehard.jpg" title="Die Hard is the best Christmas movie" width="600" height="400"></a>
</p>
</div>
<p>
I have recently been confronted to a kind of data set and problem that I was not even aware existed: intermittent demand data. Intermittent demand arises when the demand for a certain good arrives sporadically. Let’s take a look at an example, by analyzing the number of downloads for the <code>{RDieHarder}</code> package:
</p>
<pre class="r"><code>library(tidyverse)
library(tsintermittent)
library(nnfor)
library(cranlogs)
library(brotools)</code></pre>
<pre class="r"><code>rdieharder &lt;- cran_downloads("RDieHarder", from = "2017-01-01")

ggplot(rdieharder) +
  geom_line(aes(y = count, x = date), colour = "#82518c") +
  theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/intermittent-3-1.png" width="672">
</p>
<p>
Let’s take a look at just one month of data, because the above plot is not very clear, because of the outlier just before 2019… I wonder now, was that on Christmas day?
</p>
<pre class="r"><code>rdieharder %&gt;%
  filter(count == max(count))</code></pre>
<pre><code>##         date count    package
## 1 2018-12-21   373 RDieHarder</code></pre>
<p>
Not exactly on Christmas day, but almost! Anyways, let’s look at one month of data:
</p>
<pre class="r"><code>january_2018 &lt;- rdieharder %&gt;%
  filter(between(date, as.Date("2018-01-01"), as.Date("2018-02-01")))

ggplot(january_2018) +
  geom_line(aes(y = count, x = date), colour = "#82518c") +
  theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/intermittent-5-1.png" width="672">
</p>
<p>
Now, it is clear that this will be tricky to forecast. There is no discernible pattern, no trend, no seasonality… nothing that would make it “easy” for a model to learn how to forecast such data.
</p>
<p>
This is typical intermittent demand data. Specific methods have been developed to forecast such data, the most well-known being Croston, as detailed in <a href="https://www.jstor.org/stable/3007885?seq=1#page_scan_tab_contents">this paper</a>. A function to estimate such models is available in the <code>{tsintermittent}</code> package, written by <a href="https://kourentzes.com/forecasting/2014/06/23/intermittent-demand-forecasting-package-for-r/">Nikolaos Kourentzes</a> who also wrote another package, <code>{nnfor}</code>, which uses Neural Networks to forecast time series data. I am going to use both to try to forecast the intermittent demand for the <code>{RDieHarder}</code> package for the year 2019.
</p>
<p>
Let’s first load these packages:
</p>
<pre class="r"><code>library(tsintermittent)
library(nnfor)</code></pre>
<p>
And as usual, split the data into training and testing sets:
</p>
<pre class="r"><code>train_data &lt;- rdieharder %&gt;%
  filter(date &lt; as.Date("2019-01-01")) %&gt;%
  pull(count) %&gt;%
  ts()

test_data &lt;- rdieharder %&gt;%
  filter(date &gt;= as.Date("2019-01-01"))</code></pre>
<p>
Let’s consider three models; a naive one, which simply uses the mean of the training set as the forecast for all future periods, Croston’s method, and finally a Neural Network from the <code>{nnfor}</code> package:
</p>
<pre class="r"><code>naive_model &lt;- mean(train_data)

croston_model &lt;- crost(train_data, h = 163)

nn_model &lt;- mlp(train_data, reps = 1, hd.auto.type = "cv")</code></pre>
<pre><code>## Warning in preprocess(y, m, lags, keep, difforder, sel.lag,
## allow.det.season, : No inputs left in the network after pre-selection,
## forcing AR(1).</code></pre>
<pre class="r"><code>nn_model_forecast &lt;- forecast(nn_model, h = 163)</code></pre>
<p>
The <code>crost()</code> function estimates Croston’s model, and the <code>h</code> argument produces the forecast for the next 163 days. <code>mlp()</code> trains a multilayer perceptron, and the <code>hd.auto.type = "cv"</code> argument means that 5-fold cross-validation will be used to find the best number of hidden nodes. I then obtain the forecast using the <code>forecast()</code> function. As you can read from the Warning message above, the Neural Network was replaced by an auto-regressive model, AR(1), because no inputs were left after pre-selection… I am not exactly sure what that means, but if I remove the big outlier from before, this warning message disappears, and a Neural Network is successfully trained.
</p>
<p>
In order to rank the models, I follow <a href="https://www.sciencedirect.com/science/article/pii/S0169207006000239">this paper</a> from Rob J. Hyndman, who wrote a very useful book titled <a href="https://otexts.com/fpp2/">Forecasting: Principles and Practice</a>, and use the Mean Absolute Scaled Error, or MASE. You can also read <a href="https://robjhyndman.com/papers/foresight.pdf">this shorter pdf</a> which also details how to use MASE to measure the accuracy for intermittent demand. Here is the function:
</p>
<pre class="r"><code>mase &lt;- function(train_ts, test_ts, outsample_forecast){

  naive_insample_forecast &lt;- stats::lag(train_ts)

  insample_mae &lt;- mean(abs(train_ts - naive_insample_forecast), na.rm = TRUE)
  error_outsample &lt;- test_ts - outsample_forecast

  ase &lt;- error_outsample / insample_mae
  mean(abs(ase), na.rm = TRUE)
}</code></pre>
<p>
It is now easy to compute the models’ accuracies:
</p>
<pre class="r"><code>mase(train_data, test_data$count, naive_model)</code></pre>
<pre><code>## [1] 1.764385</code></pre>
<pre class="r"><code>mase(train_data, test_data$count, croston_model$component$c.out[1])</code></pre>
<pre><code>## [1] 1.397611</code></pre>
<pre class="r"><code>mase(train_data, test_data$count, nn_model_forecast$mean)</code></pre>
<pre><code>## [1] 1.767357</code></pre>
<p>
Croston’s method is the one that performs best from the three. Maybe surprisingly, the naive method performs just as well as the Neural Network! (or rather, the AR(1) model) Let’s also plot the predictions with the true values from the test set:
</p>
<pre class="r"><code>test_data &lt;- test_data %&gt;%
  mutate(naive_model_forecast = naive_model,
         croston_model_forecast = croston_model$component$c.out[1],
         nn_model_forecast = nn_model_forecast$mean) %&gt;%
  select(-package) %&gt;%
  rename(actual_value = count)


test_data_longer &lt;- test_data %&gt;%
  gather(models, value,
         actual_value, naive_model_forecast, croston_model_forecast, nn_model_forecast)</code></pre>
<pre><code>## Warning: attributes are not identical across measure variables;
## they will be dropped</code></pre>
<pre class="r"><code>ggplot(test_data_longer) +
  geom_line(aes(y = value, x = date, colour = models)) +
  theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/intermittent-13-1.png" width="672">
</p>
<p>
Just to make sure I didn’t make a mistake when writing the <code>mase()</code> function, let’s use the <code>accuracy()</code> function from the <code>{forecast}</code> package and compare the result for the Neural Network:
</p>
<pre class="r"><code>library(forecast)
accuracy(nn_model_forecast, x = test_data$actual_value)</code></pre>
<pre><code>##                       ME     RMSE      MAE  MPE MAPE      MASE       ACF1
## Training set 0.001929409 14.81196 4.109577  NaN  Inf 0.8437033 0.05425074
## Test set     8.211758227 12.40199 8.635563 -Inf  Inf 1.7673570         NA</code></pre>
<p>
The result is the same, so it does seem like the naive method is not that bad, actually! Now, in general, intermittent demand series have a lot of 0 values, which is not really the case here. I still think that the methodology fits to this particular data set.
</p>
<p>
How else would you have forecast this data? Let me know via twitter!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-06-12-intermittent.html</guid>
  <pubDate>Wed, 12 Jun 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Using cosine similarity to find matching documents: a tutorial using Seneca’s letters to his friend Lucilius</title>
  <link>https://b-rodrigues.github.io/posts/2019-06-04-cosine_sim.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Seneca_the_Younger"> <img src="https://b-rodrigues.github.io/assets/img/seneca.png" title="Seneca the Younger" width="400" height="600"></a>
</p>
</div>
<p>
Lately I’ve been interested in trying to cluster documents, and to find similar documents based on their contents. In this blog post, I will use <a href="https://en.wikisource.org/wiki/Moral_letters_to_Lucilius">Seneca’s <em>Moral letters to Lucilius</em></a> and compute the pairwise <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a> of his 124 letters. Computing the cosine similarity between two vectors returns how similar these vectors are. A cosine similarity of 1 means that the angle between the two vectors is 0, and thus both vectors have the same direction. Seneca’s Moral letters to Lucilius deal mostly with philosophical topics, as Seneca was, among many other things, a philosopher of the stoic school. The stoic school of philosophy is quite interesting, but it has been unfortunately misunderstood, especially in modern times. There is now a renewed interest for this school, see <a href="https://en.wikipedia.org/wiki/Modern_Stoicism">Modern Stoicism</a>.
</p>
<p>
The first step is to scrape the letters. The code below scrapes the letters, and saves them into a list. I first start by writing a function that gets the raw text. Note the <code>xpath</code> argument of the <code>html_nodes()</code> function. I obtained this complex expression by using the <a href="https://selectorgadget.com/">SelectorGadget</a> extension for Google Chrome, and then selecting the right element of the web page. See this <a href="https://i.imgur.com/2cntugt.png">screenshot</a> if my description was not very clear.
</p>
<p>
Then, the <code>extract_text()</code> function extracts the text from the letter. The only line that might be a bit complex is <code>discard(~<code>==</code>(., ""))</code> which removes every empty line.
</p>
<p>
Finally, there’s the <code>get_letter()</code> function that actually gets the letter by calling the first two functions. In the last line, I get all the letters into a list by mapping the list of urls to the <code>get_letter()</code> function.
</p>
<pre class="r"><code>library(tidyverse)
library(rvest)

base_url &lt;- "https://en.wikisource.org/wiki/Moral_letters_to_Lucilius/Letter_"

letter_numbers &lt;- seq(1, 124)

letter_urls &lt;- paste0(base_url, letter_numbers)

get_raw_text &lt;- function(base_url, letter_number){
  paste0(base_url, letter_number) %&gt;%
    read_html() %&gt;%
    html_nodes(xpath ='//*[contains(concat( " ", @class, " " ), concat( " ", "mw-parser-output", " " ))]') %&gt;%  
    html_text()
}


extract_text &lt;- function(raw_text, letter_number){
  raw_text &lt;- raw_text %&gt;%
    str_split("\n") %&gt;%  
    flatten_chr() %&gt;%  
    discard(~`==`(., ""))

  start &lt;- 5

  end &lt;- str_which(raw_text, "Footnotes*")

  raw_text[start:(end-1)] %&gt;%
    str_remove_all("\\[\\d{1,}\\]") %&gt;%
    str_remove_all("\\[edit\\]")
}

get_letter &lt;- function(base_url, letter_number){

  raw_text &lt;- get_raw_text(base_url, letter_number)

  extract_text(raw_text, letter_number)
}

letters_to_lucilius &lt;- map2(base_url, letter_numbers, get_letter)</code></pre>
<p>
Now that we have the letters saved in a list, we need to process the text a little bit. In order to compute the cosine similarity between the letters, I need to somehow represent them as vectors. There are several ways of doing this, and I am going to compute the tf-idf of each letter. The tf-idf will give me a vector for each letter, with zero and non-zero values. Zero values represent words that are common to all letters, and thus do not have any <em>predictive power</em>. Non-zero values are words that are not present in all letters, but maybe only a few. I expect that letters that discuss death for example, will have the word death in them, and letters that do not discuss death will not have this word. The word death thus has what I call <em>predictive power</em>, in that it helps us distinguish the letters discussing death from the other letters that do not discuss it. The same reasoning can be applied for any topic.
</p>
<p>
So, to get the tf-idf of each letter, I first need to put them in a tidy dataset. I will use the <code>{tidytext}</code> package for this. First, I load the required packages, convert each letter to a dataframe of one column that contains the text, and save the letter’s titles into another list:
</p>
<pre class="r"><code>library(tidytext)
library(SnowballC)
library(stopwords)
library(text2vec)

letters_to_lucilius_df &lt;- map(letters_to_lucilius, ~tibble("text" = .))

letter_titles &lt;- letters_to_lucilius_df %&gt;%
  map(~slice(., 1)) %&gt;%
  map(pull)</code></pre>
<p>
Now, I add this title to each dataframe as a new column, called title:
</p>
<pre class="r"><code>letters_to_lucilius_df &lt;-  map2(.x = letters_to_lucilius_df, .y = letter_titles,
                                ~mutate(.x, title = .y)) %&gt;%
  map(~slice(., -1))</code></pre>
<p>
I can now use <code>unnest_tokens()</code> to transform the datasets. Before, I had the whole text of the letter in one column. After using <code>unnest_tokens()</code> I now have a dataset with one row per word. This will make it easy to compute frequencies by letters, or what I am interested in, the tf-idf of each letter:
</p>
<pre class="r"><code>tokenized_letters &lt;- letters_to_lucilius_df %&gt;%
  bind_rows() %&gt;%
  group_by(title) %&gt;%
  unnest_tokens(word, text)</code></pre>
<p>
I can now remove stopwords, using the data containing in the <code>{stopwords}</code> package:
</p>
<pre class="r"><code>stopwords_en &lt;- tibble("word" = stopwords("en", source  = "smart"))

tokenized_letters &lt;- tokenized_letters %&gt;%
  anti_join(stopwords_en) %&gt;%
  filter(!str_detect(word, "\\d{1,}"))</code></pre>
<pre><code>## Joining, by = "word"</code></pre>
<p>
Next step, wordstemming, meaning, going from “dogs” to “dog”, or from “was” to “be”. If you do not do wordstemming, “dogs” and “dog” will be considered different words, even though they are not. <code>wordStem()</code> is a function from <code>{SnowballC}</code>.
</p>
<pre class="r"><code>tokenized_letters &lt;- tokenized_letters %&gt;%
  mutate(word = wordStem(word, language = "en"))</code></pre>
<p>
Finally, I can compute the tf-idf of each letter and cast the data as a sparse matrix:
</p>
<pre class="r"><code>tfidf_letters &lt;- tokenized_letters %&gt;%
  count(title, word, sort  = TRUE) %&gt;%
  bind_tf_idf(word, title, n)

sparse_matrix &lt;- tfidf_letters %&gt;%
  cast_sparse(title, word, tf)</code></pre>
<p>
Let’s take a look at the sparse matrix:
</p>
<pre class="r"><code>sparse_matrix[1:10, 1:4]</code></pre>
<pre><code>## 10 x 4 sparse Matrix of class "dgCMatrix"
##                                                                   thing
## CXIII. On the Vitality of the Soul and Its Attributes       0.084835631
## LXVI. On Various Aspects of Virtue                          0.017079890
## LXXXVII. Some Arguments in Favour of the Simple Life        0.014534884
## CXVII. On Real Ethics as Superior to Syllogistic Subtleties 0.025919732
## LXXVI. On Learning Wisdom in Old Age                        0.021588946
## CII. On the Intimations of Our Immortality                  0.014662757
## CXXIV. On the True Good as Attained by Reason               0.010139417
## XCIV. On the Value of Advice                                0.009266409
## LXXXI. On Benefits                                          0.007705479
## LXXXV. On Some Vain Syllogisms                              0.013254786
##                                                                     live
## CXIII. On the Vitality of the Soul and Its Attributes       0.0837751856
## LXVI. On Various Aspects of Virtue                          .           
## LXXXVII. Some Arguments in Favour of the Simple Life        0.0007267442
## CXVII. On Real Ethics as Superior to Syllogistic Subtleties 0.0050167224
## LXXVI. On Learning Wisdom in Old Age                        0.0025906736
## CII. On the Intimations of Our Immortality                  0.0019550342
## CXXIV. On the True Good as Attained by Reason               .           
## XCIV. On the Value of Advice                                0.0023166023
## LXXXI. On Benefits                                          0.0008561644
## LXXXV. On Some Vain Syllogisms                              0.0022091311
##                                                                   good
## CXIII. On the Vitality of the Soul and Its Attributes       0.01166490
## LXVI. On Various Aspects of Virtue                          0.04132231
## LXXXVII. Some Arguments in Favour of the Simple Life        0.04578488
## CXVII. On Real Ethics as Superior to Syllogistic Subtleties 0.04849498
## LXXVI. On Learning Wisdom in Old Age                        0.04663212
## CII. On the Intimations of Our Immortality                  0.05180841
## CXXIV. On the True Good as Attained by Reason               0.06717364
## XCIV. On the Value of Advice                                0.01081081
## LXXXI. On Benefits                                          0.01626712
## LXXXV. On Some Vain Syllogisms                              0.01472754
##                                                                 precept
## CXIII. On the Vitality of the Soul and Its Attributes       .          
## LXVI. On Various Aspects of Virtue                          .          
## LXXXVII. Some Arguments in Favour of the Simple Life        .          
## CXVII. On Real Ethics as Superior to Syllogistic Subtleties .          
## LXXVI. On Learning Wisdom in Old Age                        .          
## CII. On the Intimations of Our Immortality                  .          
## CXXIV. On the True Good as Attained by Reason               0.001267427
## XCIV. On the Value of Advice                                0.020463320
## LXXXI. On Benefits                                          .          
## LXXXV. On Some Vain Syllogisms                              .</code></pre>
<p>
We can consider each row of this matrix as the vector representing a letter, and thus compute the cosine similarity between letters. For this, I am using the <code>sim2()</code> function from the <code>{text2vec}</code> package. I then create the <code>get_similar_letters()</code> function that returns similar letters for a given reference letter:
</p>
<pre class="r"><code>similarities &lt;- sim2(sparse_matrix, method = "cosine", norm = "l2") 

get_similar_letters &lt;- function(similarities, reference_letter, n_recommendations = 3){
  sort(similarities[reference_letter, ], decreasing = TRUE)[1:(2 + n_recommendations)]
}</code></pre>
<pre class="r"><code>get_similar_letters(similarities, 19)</code></pre>
<pre><code>##          XXX. On Conquering the Conqueror 
##                                 1.0000000 
##                  XXIV. On Despising Death 
##                                 0.6781600 
##      LXXXII. On the Natural Fear of Death 
##                                 0.6639736 
## LXX. On the Proper Time to Slip the Cable 
##                                 0.5981706 
## LXXVIII. On the Healing Power of the Mind 
##                                 0.4709679</code></pre>
<pre class="r"><code>get_similar_letters(similarities, 99)</code></pre>
<pre><code>##                              LXI. On Meeting Death Cheerfully 
##                                                     1.0000000 
##                     LXX. On the Proper Time to Slip the Cable 
##                                                     0.5005015 
## XCIII. On the Quality, as Contrasted with the Length, of Life 
##                                                     0.4631796 
##                         CI. On the Futility of Planning Ahead 
##                                                     0.4503093 
##                              LXXVII. On Taking One's Own Life 
##                                                     0.4147019</code></pre>
<pre class="r"><code>get_similar_letters(similarities, 32)</code></pre>
<pre><code>##                                    LIX. On Pleasure and Joy 
##                                                   1.0000000 
##          XXIII. On the True Joy which Comes from Philosophy 
##                                                   0.4743672 
##                          CIX. On the Fellowship of Wise Men 
##                                                   0.4526835 
## XC. On the Part Played by Philosophy in the Progress of Man 
##                                                   0.4498278 
##         CXXIII. On the Conflict between Pleasure and Virtue 
##                                                   0.4469312</code></pre>
<pre class="r"><code>get_similar_letters(similarities, 101)</code></pre>
<pre><code>##                    X. On Living to Oneself 
##                                  1.0000000 
##          LXXIII. On Philosophers and Kings 
##                                  0.3842292 
##                  XLI. On the God within Us 
##                                  0.3465457 
##                       XXXI. On Siren Songs 
##                                  0.3451388 
## XCV. On the Usefulness of Basic Principles 
##                                  0.3302794</code></pre>
<p>
As we can see from these examples, this seems to be working quite well: the first title is the title of the reference letter, will the next 3 are the suggested letters. The problem is that my matrix is not in the right order, and thus reference letter 19 does not correspond to letter 19 of Seneca… I have to correct that, but not today.
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-06-04-cosine_sim.html</guid>
  <pubDate>Tue, 04 Jun 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The never-ending editor war (?)</title>
  <link>https://b-rodrigues.github.io/posts/2019-05-19-spacemacs.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Death_mask"> <img src="https://b-rodrigues.github.io/assets/img/typical_emacs_user.gif" title="typical emacs user working"></a>
</p>
</div>
<p>
The creation of this blog post was prompted by this tweet, asking an age-old question:
</p>
{{% tweet “1128981852558123008” %}}
<p>
This is actually a very important question, that I have been asking myself for a long time. An IDE, and plain text editors, are a very important tools to anyone writing code. Most working hours are spent within such a program, which means that one has to be careful about choosing the right one, and once a choice is made, one has, in my humble opinion, learn as many features of this program as possible to become as efficient as possible.
</p>
<p>
As you can notice from the tweet above, I suggested the use of <a href="http://spacemacs.org/">Spacemacs</a>… and my tweet did not get any likes or retweets (as of the 19th of May, sympathetic readers of this blog have liked the tweet). It is to set this great injustice straight that I decided to write this blog post.
</p>
<p>
Spacemacs is a strange beast; if vi and Emacs had a baby, it would certainly look like Spacemacs. So first of all, to understand what is Spacemacs, one has to know a bit about vi and Emacs.
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/vim.png" width="100%" style="display: block; margin: auto;">
</p>
<p>
vi is a text editor with 43 years of history now. You might have heard of Vim (Vi IMproved) which is a modern clone of vi, from 1991. More recently, another clone has been getting popular, Neovim, started in 2014. Whatever version of vi however, its basic way of functioning remains the same. vi is a modal editor, meaning that the user has to switch between different modes to work on a text file. When vi is first started, the program will be in <em>Normal</em> mode. In this mode, trying to type a word will likely result in nothing, or unexpected behaviour; unexpected, if you’re not familiar with vi. For instance, in <em>Normal</em> mode, typing <strong>j</strong> will not show the character <strong>j</strong> on your screen. Instead, this will move the cursor down one line. Typing <strong>p</strong> will paste, <strong>u</strong> will undo the last action, <strong>y</strong> will yank (copy) etc…
</p>
<p>
To type text, first, one has to enter <em>Insert</em> mode, by typing <strong>i</strong> while in <em>Normal</em> mode. Only then is it possible to write text. To go back to <em>Normal</em> mode, type <strong>ESC</strong>. Other modes are <em>Visual</em> mode (from <em>Normal</em> mode press <strong>v</strong>), which allows the user to select text and <em>Command-line</em> mode which can be entered by keying <strong>:</strong> from <em>Normal</em> mode and allows to enter commands.
</p>
<p>
Now you might be wondering why anyone would use such a convoluted way to type text. Well, this is because one can chain these commands quite easily to perform repetitive tasks very quickly. For instance, to delete a word, one types <strong>daw</strong> (in <em>Normal</em> mode), <strong>d</strong>elete <strong>a</strong> <strong>w</strong>ord. To delete the next 3 words, you can type <strong>3daw</strong>. To edit the text between, for instance, <strong>()</strong> you would type <strong>ci(</strong> (while in <em>Normal</em> mode and anywhere between the braces containing the text to edit), <strong>c</strong>hange <strong>i</strong>n <strong>(</strong>. Same logic applies for <strong>ci[</strong> for instance. Can you guess what <strong>ciw</strong> does? If you are in <em>Normal</em> mode, and you want to change the word the cursor is on, this command will erase the word and put you in <em>Insert</em> mode so that you can write the new word.
</p>
<p>
These are just basic reasons why vi (or its clones) are awesome. It is also possible to automate very long and complex tasks using macros. One starts a macro by typing <strong>q</strong> and then any letter of the alphabet to name it, for instance <strong>a</strong>. The user then performs the actions needed, types <strong>q</strong> again to stop the recording of the macro, and can then execute the macro with <strong><span class="citation"><span class="citation" data-cites="a">@a</span></span></strong>. If the user needs to execute the macro say, 10 times, <strong>10@‌‌a</strong> does the trick. It is possible to extend vi’s functionalities by using plugins, but more on that down below.
</p>
<p>
vi keybindings have inspired a lot of other programs. For instance, you can get extensions for popular web browsers that mimick vi keybindings, such as <a href="https://github.com/tridactyl/tridactyl">Tridayctl</a> for Firefox, or <a href="http://vimium.github.io/">Vivium</a> for Chromium (or Google Chrome). There are even browsers that are built from scratch with support for vi keybinds, such as my personal favorite, <a href="http://qutebrowser.org/">qutebrowser</a>. You can even go further and use a tiling window manager on GNU-Linux, for instance <a href="https://i3wm.org/">i3</a>, which I use, or <a href="https://xmonad.org/">xmonad</a>. You might need to configure those to behave more like vi, but it is possible. This means that by learning one set of keyboard shortcuts, (and the logic behind chaining the keystrokes to achieve what you want), you can master several different programs. This blog post only deals with the editor part, but as you can see, if you go down the rabbit hole enough, a new exciting world opens up.
</p>
<p>
I will show some common vi operations below, but before that let’s discuss Emacs.
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/emacs.png" width="80%" style="display: block; margin: auto;">
</p>
<p>
I am not really familiar with Emacs; I know that Emacs users only swear by it (just like vi users only swear by vi), and that Emacs is not a modal editor. However, it contains a lot of functions that you can use by pressing <strong>ESC</strong>, <strong>CTRL</strong>, <strong>ALT</strong> or <strong>META</strong> (<strong>META</strong> is the Windows key on a regular PC keyboard) followed by regular keys. So the approach is different, but it is widely accepted that productivity of proficient Emacs users is very high too. Emacs was started in 1985, and the most popular clone is GNU Emacs. Emacs also features modes, but not in the same sense as vi. There are major and minor modes. For instance, if you’re editing a Python script, Emacs will be in Python mode, or if editing a Markdown file Emacs will be in Markdown mode. This will change the available functions to the user, as well as provide other niceties, such as auto-completion. Emacs is also easily extensible, which is another reason why it is so popular. Users can install packages for Emacs, just like R users would do for R, to extend Emacs’ capabilities. For instance, a very important package if you plan to use Emacs for statistics or data science is <code>ESS</code>, <code>E</code>macs <code>S</code>peaks <code>S</code>tatistics. Emacs contains other very high quality packages, and it seems to me (but don’t quote me on that) that Emacs’ packages are more mature and feature-rich than vi’s plugins. However, vi keybindings are really awesome. This is, I believe, what <a href="https://twitter.com/syl20bnr">Sylvain Benner</a> was thinking when he developed Spacemacs.
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/spacemacs.png" width="30%" style="display: block; margin: auto;">
</p>
<p>
Spacemacs’ motto is that <em>The best editor is neither Emacs nor Vim, it’s Emacs and Vim!</em>. Spacemacs is a version, or distribution of Emacs, that has a very specific way of doing things. However, since it’s built on top of Emacs, all of Emacs’ packages are available to the user, notably <em>Evil</em>, which is a package that makes Emacs mimick vi’s modal mode and keybindings (the name of this package tells you everything you need to know about what Emacs users think of vi users 😀)
</p>
<p>
Not only does Spacemacs support Emacs packages, but Spacemacs also features so-called <em>layers</em>, which are configuration files that integrate one, or several packages, seamlessly into Spacemacs particular workflow. This particular workflow is what gave Spacemacs its name. Instead of relying on <strong>ESC</strong>, <strong>CTRL</strong>, <strong>ALT</strong> or <strong>META</strong> like Emacs, users can launch functions by typing <strong>Space</strong> in <em>Normal</em> mode and then a sequence of letters. For instance, <strong>Spaceqr</strong> restarts Spacemacs. And what’s more, you don’t actually need to learn these new key sequences. When you type <strong>Space</strong>, the minibuffer, a little popup window at the bottom of Spacemacs, appears and shows you all the options that you can type. For instance, typing <strong>b</strong> after <strong>Space</strong> opens up the buffer menu. Buffers are what could be called tabs in Rstudio. Here you can chose to <em>delete</em> a buffer, with <strong>d</strong>, create a new buffer with <strong>N</strong>, and many more options.
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/minibuffer.png" width="100%" style="display: block; margin: auto;">
</p>
<p>
Enough text, let’s get into the videos. But keep in mind the following: the videos below show the keystrokes I am typing to perform the actions. However, because I use the BÉPO keyboard layout, which is the french equivalent of the DVORAK layout, the keystrokes will be different than those in a regular vi guide, which are mainly written for the QWERTY layout. Also, to use Spacemacs for R, you need to enable the <strong>ESS</strong> layer, which I show how to do at the end. Enabling this layer will turn on auto-completion, as well as provide documentation in real time for your function in the minibuffer:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/spacemacs_autocompletion.png" style="display: block; margin: auto;">
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/spacemacs_doc.png" style="display: block; margin: auto;">
</p>
<p>
The first video shows Spacemacs divided into two windows. On the left, I am navigating around code using the <strong>T</strong> (move down) and <strong>S</strong> (move up) keys. To execute a region that I select, I type <strong>Spacemrr</strong> (this stands for <strong>M</strong>ajor mode <strong>R</strong>un <strong>R</strong>egion). Then around second 5, I key <strong>O</strong> which switches to <em>Insert</em> mode one line below the line I was, type <code>head(mtcars)</code> and then <strong>ESC</strong> to switch back to <em>Normal</em> mode and run the line with <strong>Spacemrl</strong> (<strong>M</strong>ajor mode <strong>R</strong>un <strong>L</strong>ine).
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_01_running_lines.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
In this video, I show you how to switch between windows. Type <strong>SpaceN</strong> to switch to window N. At the end, I key <strong>dd</strong> which deletes a whole line.
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_02_switching_windows.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
In the video below, I show how to use the pipe operator with <strong>Spacemm</strong>. This is a keyboard shortcut that I have defined myself. You can also spot the auto-completion at work in this video. To run the code, I first select it with <strong>V</strong>, which selects the whole line the cursor is currently at and enters <em>Visual</em> mode. I then select the lines below with <strong>T</strong> and run the region with <strong>Spacemrr</strong>.
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_03_pipe.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Here I show how plotting behaves. When a plot is created, a new window is opened with the plot. This is a major shortcoming of using Spacemacs for R programming; there is not a dedicated buffer for plots, and it only shows the very last one created, so there is no way to keep all the plots created in the current session in a neat, dedicated buffer. It seems to be possible using <a href="https://github.com/erikriverson/org-mode-R-tutorial/blob/master/org-mode-R-tutorial.org">Org-mode</a>, which is an Emacs mode for writing notes, todos, and authoring documents. But I haven’t explored this option yet, mainly because in my case, only looking at one plot at a time is ok.
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_04_ggplot.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Here I show how to quickly add text to the top of the document when at the cursor is at the bottom: I try to use the <code>tabyl()</code> function found in the <code>{janitor}</code> package, which I forgot to load. I quickly go all the way up with <strong>gg</strong>, then key <strong>yy</strong> to copy the first line, then <strong>P</strong> to paste it on the line below (<strong>p</strong> would paste it on the same line), type <strong>fv</strong>, to <strong>f</strong>ind the letter v from the word “tidyverse”, then type <strong>liw</strong> (which is the BÉPO equivalent of <strong>ciw</strong> for <strong>C</strong>hange <strong>I</strong>n <strong>W</strong>ord) and finally change “tidyverse” to “janitor”. This seems overly complex, but once you get used to this way of working, you will wonder why you hadn’t tried vi sooner.
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_05_janitor.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Here I show how to do block comment. <strong>8gg</strong> jumps to the 8th line, <strong>CTRLv</strong> starts block visual mode, which allows me to select a block of text. I select the first column of the text, <strong>G</strong> to jump all the way down, then <strong>A</strong> to enter insert mode at the end of the selection (actually, it would have been more logical to use <strong>I</strong>, which enters insert mode at the beginning of the selection) of the line and then add “#” to comment.
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_06_block_comment.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Here I show how to delete a block of text:
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_07_block_delete.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Search and replace, by entering <em>command-line</em> mode (look at the very bottom of the window):
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_08_search_replace_undo.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
I forgot to add “,” characters on a bunch of lines. I add the first “,” to the first line, go down and press <strong>ESC</strong> to exit <em>Insert</em> mode. Now in <strong>Normal</strong> mode, I type <strong>.</strong> to execute the last command, which is <em>inserting a “,” character and going down a line</em>. This <em>dot command</em> is a feature of vi, and it will always redo the last performed change.
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_09_dot.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
But instead of typing <strong>.</strong> six times, just type <strong>6.</strong> and be done with it:
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_09b_repeated_dot.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
What if you want to do something more complex, involving several commands? Here the <em>dot command</em> won’t be enough, since it only replicates the last command, not more. For this you can define macros with **<span class="citation"><span class="citation" data-cites="*">@*</span></span>*. I look for the “,” character, twice, and put the rest of the characters in the next line with enter. I then repeat this operation by executing the macro using <strong>@‌‌a</strong> repeatedly (<strong>@‌‌a</strong> because I saved the actions in <strong>a</strong>, but it could have been any other letter). I then undo my changes and execute the macro 5 times with <strong>5@‌‌a</strong>.
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_10_macros.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Here I show the undo tree (by typing <strong>Spaceua</strong>), which is a feature Spacemacs inherited from Emacs: it makes undoing changes and going back to a previous version of your script very easily:
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_11_undo_tree.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Finally, I show my Spacemacs configuration file. I show where one needs to specify the layers one wishes to use. For R, the ESS layer (which is a configuration file for the ESS Emacs package) is mandatory. As I explained above, it is also possible to use Emacs packages for which no layer is available. These are the packages under <code>dotspacemacs-additional-packages</code>. In my case I use:
</p>
<pre><code>dotspacemacs-additional-packages '(polymode
                                  poly-R
                                  poly-noweb
                                  poly-markdown)</code></pre>
<p>
which makes working with RMarkdown possible. <code>polymode</code> enables simultaneous Major modes, which is needed for RMarkdown (because RMarkdown files mix Markdown and R).
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_12_config.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
That’s the end of this long post. Spacemacs is really a joy to use, but the learning curve is quite steep. However, it is definitely worth it. There are so many packages available for Emacs (and hence Spacemacs) that allow you to browse the web, play games, listen to music, send and read emails… that a recurrent joke is that Emacs is <em>a very nice operating system, but it lacks a decent editor</em>. If that’s the case, Spacemacs is the perfect operating system, because it includes the greatest editor, vi.
</p>
<p>
If you’re interested and and want to learn more about vi, I advise you to read the following book <a href="https://www.ossblog.org/wp-content/uploads/2017/06/vim-recipes.pdf">Vim Recipes</a> (pdf warning, free) or <a href="https://pragprog.com/book/dnvim2/practical-vim-second-edition">Practical Vim, Edit Text at the Speed of thought</a> (not free, but worth every cent), and <a href="https://leanpub.com/VimLikeAPro">Use Vim Like a Pro</a>, which I have not read, but it looks quite good, and is free too if you want. Now this only covers the vi part, not the Emacs aspects of Spacemacs, but you don’t really need to know about Emacs to use Spacemacs. I had 0 experience with Emacs, and still have 0 experience with it. I only learned how to configure Spacemacs, which does not require any previous experience. To find the packages you need, as usual, use any search engine of your liking.
</p>
<p>
The last point I want to address is the built-in Vim mode of Rstudio. While it works, it does not work 100% as regular Vim, and worst of all, does not support, as far as I know, any other keyboard layout than QWERTY, which is a nogo for me.
</p>
<p>
In any case, if you’re looking to learn something new that you can use for many programs, including Rstudio, learn Vim, and then give Spacemacs a try. Chaining keystrokes to edit text gets addictive very quickly.
</p>
<p>
For reference, here is my <code>dotspacemacs/user-config</code>, which is where I defined the shortcut for the <code>%&gt;%</code> operator.
</p>
<pre><code>(defun dotspacemacs/user-config ()
  "Configuration for user code:
This function is called at the very end of Spacemacs startup, after layer
configuration.
Put your configuration code here, except for variables that should be set
before packages are loaded."
;;; R modes
  (add-to-list 'auto-mode-alist '("\\.md" . poly-markdown-mode))
  (add-to-list 'auto-mode-alist '("\\.Snw" . poly-noweb+r-mode))
  (add-to-list 'auto-mode-alist '("\\.Rnw" . poly-noweb+r-mode))
  (add-to-list 'auto-mode-alist '("\\.Rmd" . poly-markdown+r-mode))

  ;; (require 'poly-R)
  ;; (require 'poly-markdown)
  ;; (add-to-list 'auto-mode-alist '("\\.Rmd" . poly-markdown+r-mode))

  (global-company-mode t)
  (global-hl-line-mode 1) ; Enable/Disable current line highlight
  (setq-default fill-column 99)
  (setq-default auto-fill-mode t)
  ;; ESS shortcuts
  (spacemacs/set-leader-keys "mdt" 'ess-r-devtools-test-package)
  (spacemacs/set-leader-keys "mrl" 'ess-eval-line)
  (spacemacs/set-leader-keys "mrr" 'ess-eval-region)
  (spacemacs/set-leader-keys "mdb" 'ess-r-devtools-build-package)
  (spacemacs/set-leader-keys "mdd" 'ess-r-devtools-document-package)
  (spacemacs/set-leader-keys "mdl" 'ess-r-devtools-load-package)
  (spacemacs/set-leader-keys "mdc" 'ess-r-devtools-check-package)
  (spacemacs/set-leader-keys "mdp" 'ess-r-package-mode)
  (add-hook 'ess-mode-hook
            (lambda ()
              (ess-toggle-underscore nil)))
  (define-key evil-normal-state-map (kbd "SPC mm")
            (lambda ()
              (interactive)
              (insert " %&gt;% ")
              (evil-insert-state)
              ))
  ;; Move lines around
  (spacemacs/set-leader-keys "MS" 'move-text-line-up)
  (spacemacs/set-leader-keys "MT" 'move-text-line-down)
  (setq-default whitespace-mode t)
  (setq-default whitespace-style (quote (spaces tabs newline space-mark tab-mark newline-mark)))
  (setq-default whitespace-display-mappings
        ;; all numbers are Unicode codepoint in decimal. try (insert-char 182 ) to see it
        '(
          (space-mark 32 [183] [46]) ; 32 SPACE, 183 MIDDLE DOT 「·」, 46 FULL STOP 「.」
          (newline-mark 10 [9226 10]) ; 10 LINE FEED
          (tab-mark 9 [9655 9] [92 9]) ; 9 TAB, 9655 WHITE RIGHT-POINTING TRIANGLE 「▷」
          ))
  (setq-default TeX-view-program-selection
         '((output-pdf "PDF Viewer")))
  (setq-default TeX-view-program-list
        '(("PDF Viewer" "okular %o")))
  (setq-default indent-tabs-mode nil)
  (setq-default tab-width 2)
   ;; (setq org-default-notes-file (concat org-directory "/agenda/notes.org"))
   (add-hook 'prog-mode-hook 'spacemacs/toggle-fill-column-indicator-on)
   (add-hook 'text-mode-hook 'spacemacs/toggle-fill-column-indicator-on)
   (add-hook 'markdown-mode-hook 'spacemacs/toggle-fill-column-indicator-on)
  )</code></pre>


 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-05-19-spacemacs.html</guid>
  <pubDate>Sun, 19 May 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>For posterity: install {xml2} on GNU/Linux distros</title>
  <link>https://b-rodrigues.github.io/posts/2019-05-18-xml2.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Death_mask"> <img src="https://b-rodrigues.github.io/assets/img/napoleon_death_mask.jpg" title="I will probably be the only reader of this blog post"></a>
</p>
</div>
<p>
Today I’ve removed my system’s R package and installed MRO instead. While re-installing all packages, I’ve encountered one of the most frustrating error message for someone installing packages from source:
</p>
<pre><code>Error : /tmp/Rtmpw60aCp/R.INSTALL7819efef27e/xml2/man/read_xml.Rd:47: unable to load shared object
'/usr/lib64/R/library/xml2/libs/xml2.so': 
libicui18n.so.58: cannot open shared object file: No such file or directory ERROR: 
installing Rd objects failed for package ‘xml2’ </code></pre>
<p>
This library, <code>libicui18n.so.58</code> is a pain in the butt. However, you can easily install it if you install miniconda. After installing miniconda, you can look for it with:
</p>
<pre><code>[19-05-18 18:26] cbrunos in ~/ ➤ locate libicui18n.so.58

/home/cbrunos/miniconda3/lib/libicui18n.so.58
/home/cbrunos/miniconda3/lib/libicui18n.so.58.2
/home/cbrunos/miniconda3/pkgs/icu-58.2-h9c2bf20_1/lib/libicui18n.so.58
/home/cbrunos/miniconda3/pkgs/icu-58.2-h9c2bf20_1/lib/libicui18n.so.58.2
</code></pre>
<p>
So now you need to tell R where to look for this library. The <a href="https://stackoverflow.com/a/47851648">following Stackoverflow</a> answer saved the day. Add the following lines to <code>R_HOME/etc/ldpaths</code> (in my case, it was in <code>/opt/microsoft/ropen/3.5.2/lib64/R/etc/</code>):
</p>
<pre><code>LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/username/miniconda3/lib/
export LD_LIBRARY_PATH</code></pre>
<p>
and try to install <code>xml2</code> again, and it should work! If not, just abandon the idea of using R and switch to doing data science with VBA, it’ll be less frustrating.
</p>
<p>
Something else, if you install Microsoft R Open, you’ll be stuck with some older packages, because by default MRO uses a snapshot of CRAN from a given day as a mirror. To get the freshest packages, add the following line to your <code>.Rprofile</code> file (which should be located in your <code>HOME</code>):
</p>
<pre><code>options(repos = c(CRAN = "http://cran.rstudio.com/"))</code></pre>
<p>
And to finish this short blog post, add the following line to your <code>.Rprofile</code> if you get the following error messages when trying to install a package from github:
</p>
<pre><code>remotes::install_github('rstudio/DT') Downloading GitHub repo rstudio/DT@master tar: 
This does not look like a tar archive gzip: stdin: unexpected end of file tar: Child returned 
status 1 tar: Error is not recoverable: exiting now tar: This does not look like a tar archive 
gzip: stdin: unexpected end of file tar: Child returned status 1 tar: Error is not recoverable: 
exiting now Error in getrootdir(untar(src, list = TRUE)) : length(file_list) &gt; 0 is not TRUE Calls: 
&lt;Anonymous&gt; ... source_pkg -&gt; decompress -&gt; getrootdir -&gt; stopifnot In addition: Warning messages: 1: 
In utils::untar(tarfile, ...) : ‘tar -xf '/tmp/RtmpitCFRe/file2677442609b8.tar.gz' -C 
'/tmp/RtmpitCFRe/remotes267752f2629f'’ returned error code 2 2: 
In system(cmd, intern = TRUE) : running command 'tar -tf '/tmp/RtmpitCFRe/file2677442609b8.tar.gz'' 
had status 2 Execution halted</code></pre>
<p>
The solution, which can found <a href="https://github.com/r-lib/remotes/issues/350#issuecomment-493649792">here</a>
</p>
<pre><code>options("download.file.method" = "libcurl")</code></pre>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-05-18-xml2.html</guid>
  <pubDate>Sat, 18 May 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Fast food, causality and R packages, part 2</title>
  <link>https://b-rodrigues.github.io/posts/2019-05-04-diffindiff_part2.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Joke"> <img src="https://b-rodrigues.github.io/assets/img/distracted_economist.jpg" title="Soon, humanity will only communicate in memes"></a>
</p>
</div>
<p>
I am currently working on a package for the R programming language; its initial goal was to simply distribute the data used in the Card and Krueger 1994 paper that you can read <a href="http://davidcard.berkeley.edu/papers/njmin-aer.pdf">here</a> (PDF warning). However, I decided that I would add code to perform diff-in-diff.
</p>
<p>
In my <a href="https://www.brodrigues.co/blog/2019-04-28-diffindiff_part1/">previous blog post</a> I showed how to set up the structure of your new package. In this blog post, I will only focus on getting Card and Krueger’s data and prepare it for distribution. The next blog posts will focus on writing a function to perform difference-in-differences.
</p>
<p>
If you want to distribute data through a package, you first need to use the <code>usethis::use_data_raw()</code> function (as shown in part 1).
</p>
<p>
This creates a <code>data-raw</code> folder, and inside you will find the <code>DATASET.R</code> script. You can edit this script to prepare the data.
</p>
<p>
First, let’s download the data from Card’s website, unzip it and load the data into R. All these operations will be performed from R:
</p>
<pre class="r"><code>library(tidyverse)

tempfile_path &lt;- tempfile()

download.file("http://davidcard.berkeley.edu/data_sets/njmin.zip", destfile = tempfile_path)

tempdir_path &lt;- tempdir()

unzip(tempfile_path, exdir = tempdir_path)</code></pre>
<p>
To download and unzip a file from R, first, you need to define where you want to save the file. Because I am not interested in keeping the downloaded file, I use the <code>tempfile()</code> function to get a temporary file in my <code>/tmp/</code> folder (which is the folder that contains temporary files and folders in a GNU+Linux system). Then, using <code>download.file()</code> I download the file, and save it in my temporary file. I then create a temporary directory using <code>tempdir()</code> (the idea is the same as with <code>tempfile()</code>), and use this folder to save the files that I will unzip, using the <code>unzip()</code> function. This folder now contains several files:
</p>
<pre><code>check.sas
codebook
public.csv
read.me
survey1.nj
survey2.nj</code></pre>
<p>
<code>check.sas</code> is the SAS script Card and Krueger used. It’s interesting, because it is quite simple, quite short (170 lines long) and yet the impact of Card and Krueger’s research was and has been very important for the field of econometrics. This script will help me define my own functions. <code>codebook</code>, you guessed it, contains the variables’ descriptions. I will use this to name the columns of the data and to write the dataset’s documentation.
</p>
<p>
<code>public.csv</code> is the data. It does not contain any column names:
</p>
<pre><code> 46 1 0 0 0 0 0 1 0 0  0 30.00 15.00  3.00   .    19.0   .   1    .  2  6.50 16.50  1.03  1.03  0.52  3  3 1 1 111792  1  3.50 35.00  3.00  4.30  26.0  0.08 1 2  6.50 16.50  1.03   .    0.94  4  4    
 49 2 0 0 0 0 0 1 0 0  0  6.50  6.50  4.00   .    26.0   .   0    .  2 10.00 13.00  1.01  0.90  2.35  4  3 1 1 111292  .  0.00 15.00  4.00  4.45  13.0  0.05 0 2 10.00 13.00  1.01  0.89  2.35  4  4    
506 2 1 0 0 0 0 1 0 0  0  3.00  7.00  2.00   .    13.0  0.37 0  30.0 2 11.00 10.00  0.95  0.74  2.33  3  3 1 1 111292  .  3.00  7.00  4.00  5.00  19.0  0.25 . 1 11.00 11.00  0.95  0.74  2.33  4  3    
 56 4 1 0 0 0 0 1 0 0  0 20.00 20.00  4.00  5.00  26.0  0.10 1   0.0 2 10.00 12.00  0.87  0.82  1.79  2  2 1 1 111492  .  0.00 36.00  2.00  5.25  26.0  0.15 0 2 10.00 12.00  0.92  0.79  0.87  2  2    
 61 4 1 0 0 0 0 1 0 0  0  6.00 26.00  5.00  5.50  52.0  0.15 1   0.0 3 10.00 12.00  0.87  0.77  1.65  2  2 1 1 111492  . 28.00  3.00  6.00  4.75  13.0  0.15 0 2 10.00 12.00  1.01  0.84  0.95  2  2    
 62 4 1 0 0 0 0 1 0 0  2  0.00 31.00  5.00  5.00  26.0  0.07 0  45.0 2 10.00 12.00  0.87  0.77  0.95  2  2 1 1 111492  .   .     .     .     .    26.0   .   0 2 10.00 12.00   .    0.84  1.79  3  3    </code></pre>
<p>
Missing data is defined by <code>.</code> and the delimiter is the space character. <code>read.me</code> is a README file. Finally, <code>survey1.nj</code> and <code>survey2.nj</code> are the surveys that were administered to the fast food restaurants’ managers; one in February (before the raise) and the second one in November (after the minimum wage raise).
</p>
<p>
The next lines import the codebook:
</p>
<pre class="r"><code>codebook &lt;- read_lines(file = paste0(tempdir_path, "/codebook"))

variable_names &lt;- codebook %&gt;%
    `[`(8:59) %&gt;%
    `[`(-c(5, 6, 13, 14, 32, 33)) %&gt;%
    str_sub(1, 13) %&gt;%
    str_squish() %&gt;%
    str_to_lower()</code></pre>
<p>
Once I import the codebook, I select lines 8 to 59 using the <code><code>[</code>()</code> function. If you’re not familiar with this notation, try the following in a console:
</p>
<pre class="r"><code>seq(1, 100)[1:10]</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10</code></pre>
<p>
and compare:
</p>
<pre class="r"><code>seq(1, 100) %&gt;% 
  `[`(., 1:10)</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10</code></pre>
<p>
both are equivalent, as you can see. You can also try the following:
</p>
<pre class="r"><code>1 + 10</code></pre>
<pre><code>## [1] 11</code></pre>
<pre class="r"><code>1 %&gt;% 
  `+`(., 10)</code></pre>
<pre><code>## [1] 11</code></pre>
<p>
Using the same trick, I remove lines that I do not need, and then using <code>stringr::str_sub(1, 13)</code> I only keep the first 13 characters (which are the variable names, plus some white space characters) and then, to remove all the unneeded white space characters I use <code>stringr::squish()</code>, and then change the column names to lowercase.
</p>
<p>
I then load the data, and add the column names that I extracted before:
</p>
<pre class="r"><code>dataset &lt;- read_table2(paste0(tempdir_path, "/public.dat"),
                      col_names = FALSE)

dataset &lt;- dataset %&gt;%
    select(-X47) %&gt;%
    `colnames&lt;-`(., variable_names) %&gt;%
    mutate_all(as.numeric) %&gt;%
    mutate(sheet = as.character(sheet))</code></pre>
<p>
I use the same trick as before. I rename the 47th column, which is empty, I name the columns with <code><code>colnames&amp;lt;-</code>()</code>.
</p>
<p>
After this, I perform some data cleaning. It’s mostly renaming categories of categorical variables, and creating a “true” panel format. Several variables were measured at several points in time. Variables that were measured a second time have a “2” at the end of their name. I remove these variables, and add an observation data variable. So my data as twice as many rows as the original data, but that format makes it way easier to work with. Below you can read the full code:
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code>dataset &lt;- dataset %&gt;%
    mutate(chain = case_when(chain == 1 ~ "bk",
                             chain == 2 ~ "kfc",
                             chain == 3 ~ "roys",
                             chain == 4 ~ "wendys")) %&gt;%
    mutate(state = case_when(state == 1 ~ "New Jersey",
                             state == 0 ~ "Pennsylvania")) %&gt;%
    mutate(region = case_when(southj == 1 ~ "southj",
              centralj == 1 ~ "centralj",
              northj == 1 ~ "northj",
              shore == 1 ~ "shorej",
              pa1 == 1 ~ "pa1",
              pa2 == 1 ~ "pa2")) %&gt;%
    mutate(meals = case_when(meals == 0 ~ "None",
                             meals == 1 ~ "Free meals",
                             meals == 2 ~ "Reduced price meals",
                             meals == 3 ~ "Both free and reduced price meals")) %&gt;%
    mutate(meals2 = case_when(meals2 == 0 ~ "None",
                             meals2 == 1 ~ "Free meals",
                             meals2 == 2 ~ "Reduced price meals",
                             meals2 == 3 ~ "Both free and reduced price meals")) %&gt;%
    mutate(status2 = case_when(status2 == 0 ~ "Refused 2nd interview",
                               status2 == 1 ~ "Answered 2nd interview",
                               status2 == 2 ~ "Closed for renovations",
                               status2 == 3 ~ "Closed permanently",
                               status2 == 4 ~ "Closed for highway construction",
                               status2 == 5 ~ "Closed due to Mall fire")) %&gt;%
    mutate(co_owned = if_else(co_owned == 1, "Yes", "No")) %&gt;%
    mutate(bonus = if_else(bonus == 1, "Yes", "No")) %&gt;%
    mutate(special2 = if_else(special2 == 1, "Yes", "No")) %&gt;%
    mutate(type2 = if_else(type2 == 1, "Phone", "Personal")) %&gt;%
    select(sheet, chain, co_owned, state, region, everything()) %&gt;%
    select(-southj, -centralj, -northj, -shore, -pa1, -pa2) %&gt;%
    mutate(date2 = lubridate::mdy(date2)) %&gt;%
    rename(open2 = open2r) %&gt;%
    rename(firstinc2 = firstin2)

dataset1 &lt;- dataset %&gt;%
    select(-ends_with("2"), -sheet, -chain, -co_owned, -state, -region, -bonus) %&gt;%
    mutate(type = NA_character_,
           status = NA_character_,
           date = NA)

dataset2 &lt;- dataset %&gt;%
    select(ends_with("2")) %&gt;%
    #mutate(bonus = NA_character_) %&gt;%
    rename_all(~str_remove(., "2"))

other_cols &lt;- dataset %&gt;%
    select(sheet, chain, co_owned, state, region, bonus)

other_cols_1 &lt;- other_cols %&gt;%
    mutate(observation = "February 1992")

other_cols_2 &lt;- other_cols %&gt;%
    mutate(observation = "November 1992")

dataset1 &lt;- bind_cols(other_cols_1, dataset1)
dataset2 &lt;- bind_cols(other_cols_2, dataset2)

njmin &lt;- bind_rows(dataset1, dataset2) %&gt;%
    select(sheet, chain, state, region, observation, everything())</code></pre>
</details>
<p>
The line I would like to comment is the following:
</p>
<pre class="r"><code>dataset %&gt;%
    select(-ends_with("2"), -sheet, -chain, -co_owned, -state, -region, -bonus)</code></pre>
<p>
This select removes every column that ends with the character “2” (among others). I split the data in two, to then bind the rows together and thus create my long dataset. I then save the data into the <code>data/</code> folder:
</p>
<pre class="r"><code>usethis::use_data(njmin, overwrite = TRUE)</code></pre>
<p>
This saves the data as an <code>.rda</code> file. To enable users to read the data by typing <code>data(“njmin”)</code>, you need to create a <code>data.R</code> script in the <code>R/</code> folder. You can read my <code>data.R</code> script below:
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code>#' Data from the Card and Krueger 1994 paper *Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania*
#'
#' This dataset was downloaded and distributed with the permission of David Card. The original
#' data contains 410 observations and 46 variables. The data distributed in this package is
#' exactly the same, but was changed from a wide to a long dataset, which is better suited for
#' manipulation with *tidyverse* functions.
#'
#' @format A data frame with 820 rows and 28 variables:
#' \describe{
#'   \item{\code{sheet}}{Sheet number (unique store id).}
#'   \item{\code{chain}}{The fastfood chain: bk is Burger King, kfc is Kentucky Fried Chicken, wendys is Wendy's, roys is Roy Rogers.}
#'   \item{\code{state}}{State where the restaurant is located.}
#'   \item{\code{region}}{pa1 is northeast suburbs of Phila, pa2 is Easton etc, centralj is central NJ, northj is northern NJ, southj is south NJ.}
#'   \item{\code{observation}}{Date of first (February 1992) and second (November 1992) observation.}
#'   \item{\code{co_owned}}{"Yes" if company owned.}
#'   \item{\code{ncalls}}{Number of call-backs. Is 0 if contacted on first call.}
#'   \item{\code{empft}}{Number full-time employees.}
#'   \item{\code{emppt}}{Number part-time employees.}
#'   \item{\code{nmgrs}}{Number of managers/assistant managers.}
#'   \item{\code{wage_st}}{Starting wage ($/hr).}
#'   \item{\code{inctime}}{Months to usual first raise.}
#'   \item{\code{firstinc}}{Usual amount of first raise (\$/hr).}
#'   \item{\code{bonus}}{"Yes" if cash bounty for new workers.}
#'   \item{\code{pctaff}}{\% of employees affected by new minimum.}
#'   \item{\code{meals}}{Free/reduced priced code.}
#'   \item{\code{open}}{Hour of opening.}
#'   \item{\code{hrsopen}}{Number of hours open per day.}
#'   \item{\code{psode}}{Price of medium soda, including tax.}
#'   \item{\code{pfry}}{Price of small fries, including tax.}
#'   \item{\code{pentree}}{Price of entree, including tax.}
#'   \item{\code{nregs}}{Number of cash registers in store.}
#'   \item{\code{nregs11}}{Number of registers open at 11:00 pm.}
#'   \item{\code{type}}{Type of 2nd interview.}
#'   \item{\code{status}}{Status of 2nd interview.}
#'   \item{\code{date}}{Date of 2nd interview.}
#'   \item{\code{nregs11}}{"Yes" if special program for new workers.}
#' }
#' @source \url{http://davidcard.berkeley.edu/data_sets.html}
"njmin"</code></pre>
</details>
<p>
I have documented the data, and using <code>roxygen2::royxgenise()</code> to create the dataset’s documentation.
</p>
<p>
The data can now be used to create some nifty plots:
</p>
<pre class="r"><code>ggplot(njmin, aes(wage_st)) + geom_density(aes(fill = state), alpha = 0.3) +
    facet_wrap(vars(observation)) + theme_blog() +
    theme(legend.title = element_blank(), plot.caption = element_text(colour = "white")) +
    labs(title = "Distribution of starting wage rates in fast food restaurants",
         caption = "On April 1st, 1992, New Jersey's minimum wage rose from $4.25 to $5.05. Source: Card and Krueger (1994)")</code></pre>
<pre><code>## Warning: Removed 41 rows containing non-finite values (stat_density).</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/diffindiff_part2-6-1.png" width="672">
</p>
<p>
In the next blog post, I am going to write a first function to perform diff and diff, and we will learn how to make it available to users, document and test it!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-05-04-diffindiff_part2.html</guid>
  <pubDate>Sat, 04 May 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Fast food, causality and R packages, part 1</title>
  <link>https://b-rodrigues.github.io/posts/2019-04-28-diffindiff_part1.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Joke"> <img src="https://b-rodrigues.github.io/assets/img/distracted_economist.jpg" title="Soon, humanity will only communicate in memes"></a>
</p>
</div>
<p>
I am currently working on a package for the R programming language; its initial goal was to simply distribute the data used in the Card and Krueger 1994 paper that you can read <a href="http://davidcard.berkeley.edu/papers/njmin-aer.pdf">here</a> (PDF warning).
</p>
<p>
The gist of the paper is to try to answer the following question: <em>Do increases in minimum wages reduce employment?</em> According to Card and Krueger’s paper from 1994, no. The authors studied a change in legislation in New Jersey which increased the minimum wage from $4.25 an hour to $5.05 an hour. The neighbourghing state of Pennsylvania did not introduce such an increase. The authors thus used the State of Pennsylvania as a control for the State of New Jersey and studied how the increase in minimum wage impacted the employment in fast food restaurants and found, against what economic theory predicted, an increase and not a decrease in employment. The authors used a method called difference-in-differences to asses the impact of the minimum wage increase.
</p>
<p>
This result was and still is controversial, with subsequent studies finding subtler results. For instance, showing that there is a reduction in employment following an increase in minimum wage, but only for large restaurants (see Ropponen and Olli, 2011).
</p>
<p>
Anyways, this blog post will discuss how to create a package using to distribute the data. In a future blog post, I will discuss preparing the data to make it available as a demo dataset inside the package, and then writing and documenting functions.
</p>
<p>
The first step to create a package, is to create a new project:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/package_01.png"><!-- -->
</p>
<p>
Select “New Directory”:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/package_02.png"><!-- -->
</p>
<p>
Then “R package”:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/package_03.png"><!-- -->
</p>
<p>
and on the window that appears, you can choose the name of the package, as well as already some starting source files:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/package_04.png"><!-- -->
</p>
<p>
Also, I’d highly recommend you click on the “Create a git repository” box and use git within your project for reproducibility and sharing your code more easily. If you do not know git, there’s a lot of online resources to get you started. It’s not super difficult, but it does require making some new habits, which can take some time.
</p>
<p>
I called my package <code>{diffindiff}</code>, and clicked on “Create Project”. This opens up a new project with a <code>hello.R</code> script, which gives you some pointers:
</p>
<pre><code># Hello, world!
#
# This is an example function named 'hello' 
# which prints 'Hello, world!'.
#
# You can learn more about package authoring with RStudio at:
#
#   http://r-pkgs.had.co.nz/
#
# Some useful keyboard shortcuts for package authoring:
#
#   Install Package:           'Ctrl + Shift + B'
#   Check Package:             'Ctrl + Shift + E'
#   Test Package:              'Ctrl + Shift + T'

hello &lt;- function() {
  print("Hello, world!")
}</code></pre>
<p>
Now, to simplify the creation of your package, I highly recommend you use the <code>{usethis}</code> package. <code>{usethis}</code> removes a lot of the pain involved in creating packages.
</p>
<p>
For instance, want to start by adding a README file? Simply run:
</p>
<pre class="r"><code>usethis::use_readme_md()</code></pre>
<pre class="r"><code>✔ Setting active project to '/path/to/your/package/diffindiff'
✔ Writing 'README.md'
● Modify 'README.md'</code></pre>
<p>
This creates a <code>README.md</code> file in the root directory of your package. Simply change that file, and that’s it.
</p>
<p>
The next step could be setting up your package to work with <code>{roxygen2}</code>, which is very useful for writing documentation:
</p>
<pre class="r"><code>usethis::use_roxygen_md()</code></pre>
<pre class="r"><code>✔ Setting Roxygen field in DESCRIPTION to 'list(markdown = TRUE)'
✔ Setting RoxygenNote field in DESCRIPTION to '6.1.1'
● Run `devtools::document()`</code></pre>
<p>
See how the output tells you to run <code>devtools::document()</code>? This function will document your package, transforming the comments you write to describe your functions to documentation and managing the NAMESPACE file. Let’s run this function too:
</p>
<pre class="r"><code>devtools::document()</code></pre>
<pre class="r"><code>Updating diffindiff documentation
First time using roxygen2. Upgrading automatically...
Loading diffindiff
Warning: The existing 'NAMESPACE' file was not generated by roxygen2, and will not be overwritten.</code></pre>
<p>
You might have a similar message than me, telling you that the NAMESPACE file was not generated by <code>{roxygen2}</code>, and will thus not be overwritten. Simply remove the file and run <code>devtools::document()</code> again:
</p>
<pre class="r"><code>devtools::document()</code></pre>
<pre class="r"><code>Updating diffindiff documentation
First time using roxygen2. Upgrading automatically...
Writing NAMESPACE
Loading diffindiff</code></pre>
<p>
But what is actually the NAMESPACE file? This file is quite important, as it details where your package’s functions have to look for in order to use other functions. This means that if your package needs function <code>foo()</code> from package <code>{bar}</code>, it will consistently look for <code>foo()</code> inside <code>{bar}</code> and not confuse it with, say, the <code>foo()</code> function from the <code>{barley}</code> package, even if you load <code>{barley}</code> after <code>{bar}</code> in your interactive session. This can seem confusing now, but in the next blog posts I will detail this, and you will see that it’s not that difficult. Just know that it is an important file, and that you do not have to edit it by hand.
</p>
<p>
Next, I like to run the following:
</p>
<pre class="r"><code>usethis::use_pipe()</code></pre>
<pre class="r"><code>✔ Adding 'magrittr' to Imports field in DESCRIPTION
✔ Writing 'R/utils-pipe.R'
● Run `devtools::document()`</code></pre>
<p>
This makes the now famous <code>%&gt;%</code> function available internally to your package (so you can use it to write the functions that will be included in your package) but also available to the users that will load the package.
</p>
<p>
Your package is still missing a license. If you plan on writing a package for your own personal use, for instance, a collection of functions, there is no need to think about licenses. But if you’re making your package available through CRAN, then you definitely need to think about it. For this package, I’ll be using the MIT license, because the package will distribute data which I do not own (I’ve got permission from Card to re-distribute it) and thus I think it would be better to use a permissive license (I don’t know if the GPL, another license, which is stricter in terms of redistribution, could be used in this case).
</p>
<pre class="r"><code>usethis::use_mit_license()</code></pre>
<pre class="r"><code>✔ Setting License field in DESCRIPTION to 'MIT + file LICENSE'
✔ Writing 'LICENSE.md'
✔ Adding '^LICENSE\\.md$' to '.Rbuildignore'
✔ Writing 'LICENSE'</code></pre>
<p>
We’re almost done setting up the structure of the package. If we forget something though, it’s not an issue, we’ll just have to run the right <code>use_*</code> function later on. Let’s finish by preparing the folder that will contains the script to prepare the data:
</p>
<pre class="r"><code>usethis::use_data_raw()</code></pre>
<pre class="r"><code>✔ Creating 'data-raw/'
✔ Adding '^data-raw$' to '.Rbuildignore'
✔ Writing 'data-raw/DATASET.R'
● Modify 'data-raw/DATASET.R'
● Finish the data preparation script in 'data-raw/DATASET.R'
● Use `usethis::use_data()` to add prepared data to package</code></pre>
<p>
This creates the <code>data-raw</code> folder with the <code>DATASET.R</code> script inside. This is the script that will contain the code to download and prepare datasets that you want to include in your package. This will be the subject of the next blog post.
</p>
<p>
Let’s now finish by documenting the package, and pushing everything to Github:
</p>
<pre class="r"><code>devtools::document()</code></pre>
<p>
The following lines will only work if you set up the Github repo:
</p>
<pre><code>git add .
git commit -am "first commit"
git push origin master</code></pre>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-04-28-diffindiff_part1.html</guid>
  <pubDate>Sun, 28 Apr 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Historical newspaper scraping with {tesseract} and R</title>
  <link>https://b-rodrigues.github.io/posts/2019-04-07-historical_newspaper_scraping_tesseract.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Cliometrics"> <img src="https://b-rodrigues.github.io/assets/img/clio.jpg" title="Historical newspapers as a source to practice cliometrics?"></a>
</p>
</div>
<p>
I have been playing around with historical newspapers data for some months now. The “obvious” type of analysis to do is NLP, but there is also a lot of numerical data inside historical newspapers. For instance, you can find these tables that show the market prices of the day in the <em>L’Indépendance Luxembourgeoise</em>:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/market_price_table.png"><!-- -->
</p>
<p>
I wanted to see how easy it was to extract these tables from the newspapers and then make it available. It was a bit more complicated than anticipated.
</p>
<section id="download-data" class="level2">
<h2 class="anchored" data-anchor-id="download-data">
Download data
</h2>
<p>
The first step is to download the data. For this, I have used the code <a href="https://twitter.com/yvesmaurer"><code><span class="citation" data-cites="yvesmaurer">@yvesmaurer</span></code></a> which you can find <a href="https://github.com/ymaurer/eluxemburgensia-opendata-ark">here</a>. This code makes it easy to download individual pages of certain newspapers, for instance <a href="https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F1drtkj%2Fpages%2F1/full/full/0/default.jpg">this one</a>. The pages I am interested in are pages 3, which contain the tables I need, for example <a href="https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F1drtkj%2Fpages%2F3/full/full/0/default.jpg">here</a>. <a href="https://twitter.com/yvesmaurer"><code><span class="citation" data-cites="yvesmaurer">@yvesmaurer</span></code></a>’s code makes it easy to find the download links, which look like this: <code>https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F1drtkj%2Fpages%2F3/full/full/0/default.jpg</code>. It is also possible to crop the image by changing some parameters <a href="https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fwsvhwh%2Fpages%2F3/pct:74,0,100,100/full/0/default.jpg">like so</a>. This is helpful, because it makes the image smaller. The tables I’m interested in are always in the last column, so I can can use this feature to get smaller images. However, not every issue contains these tables, and I only want to download the ones that have these tables. So I wrote the following code to download the images I’m interested in:
</p>
<pre class="r"><code>library(tidyverse)
library(magick)
library(tesseract)
library(furrr)

download_image &lt;- function(link){

    print(link)

    isok &lt;- image_read(link) %&gt;%
        ocr(engine = "fra") %&gt;%
        str_to_lower() %&gt;%
        str_detect("marché de luxembourg")

    if(isok){
        date_link &lt;- link %&gt;%
            str_replace("pages%2f3", "pages%2f1") %&gt;%
            str_replace("pct:74,0,100,100", "pct:76,1,17,5")

        paper_date &lt;- image_read(date_link) %&gt;%
            ocr(engine = "fra") %&gt;%
            str_squish() %&gt;%
            str_remove("%") %&gt;%
            str_remove("&amp;") %&gt;%
            str_remove("/")

        ark &lt;- link %&gt;%
            str_sub(53, 60)

        download.file(link, paste0("indep_pages/", ark, "-", paper_date, ".jpg"))
    } else {
        NULL
        }
}</code></pre>
<p>
This code only downloads an image if the <code>ocr()</code> from the {tesseract} (which does, you guessed it, OCR) detects the string “marché de luxembourg” which is the title of the tables. This is a bit extreme, because if a single letter cannot be correctly detected by the OCR, the page will not be downloaded. But I figured that if this string could not be easily recognized, this would be a canary telling me that the text inside the table would also not be easily recognized. So it might be extreme, but my hope was that it would make detecting the table itself easier. Turned out it wasn’t so easy, but more on this later.
</p>
</section>
<section id="preparing-images" class="level2">
<h2 class="anchored" data-anchor-id="preparing-images">
Preparing images
</h2>
<p>
Now that I have the images, I will prepare them to make character recognition easier. To do this, I’m using the <code>{magick}</code> package:
</p>
<pre class="r"><code>library(tidyverse)
library(magick)
library(tesseract)
library(furrr)

prepare_image &lt;- function(image_path){
    image &lt;- image_read(image_path)

    image &lt;- image %&gt;%
        image_modulate(brightness = 150) %&gt;%
        image_convolve('DoG:0,0,2', scaling = '1000, 100%') %&gt;%
        image_despeckle(times = 10)

    image_write(image, paste0(getwd(), "/edited/", str_remove(image_path, ".jpg"), "edited.jpg"))
}


image_paths &lt;- dir(path = "indep_pages", pattern = "*.jpg", full.names = TRUE)

plan(multiprocess, workers = 8)

image_paths %&gt;%
    future_map(prepare_image)</code></pre>
<p>
The picture below shows the result:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/table_and_edit.jpg"><!-- -->
</p>
<p>
Now comes the complicated part, which is going from the image above, to the dataset below:
</p>
<pre><code>good_fr,good_en,unit,market_date,price,source_url
Froment,Wheat,hectolitre,1875-08-28,23,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Métail,Meslin,hectolitre,1875-08-28,21,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Seigle,Rye,hectolitre,1875-08-28,15,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Orge,Barley,hectolitre,1875-08-28,16,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Orge mondé,Pot Barley,kilogram,1875-08-28,0.85,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Orge perlé,Pearl barley,kilogram,1875-08-28,0.8,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Avoine,Oats,hectolitre,1875-08-28,8.5,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Pois,Peas,hectolitre,1875-08-28,NA,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg</code></pre>
</section>
<section id="ocr-with-tesseract" class="level2">
<h2 class="anchored" data-anchor-id="ocr-with-tesseract">
OCR with {tesseract}
</h2>
<p>
The first step was to get the date. For this, I have used the following function, which will then be used inside another function, which will extract the data and prices.
</p>
<pre class="r"><code>library(tidyverse)
library(lubridate)
library(magick)
library(tesseract)
library(furrr)
library(janitor)

is_empty_line &lt;- function(line){
    ifelse(line == "", TRUE, FALSE)
}

Sys.setlocale('LC_TIME', "fr_FR")

get_date &lt;- function(string, annee){

    liste_mois &lt;- c("janvier", "février", "mars", "avril", "mai", "juin", "juillet",
                    "août", "septembre", "octobre", "novembre", "décembre")

    raw_date &lt;- string %&gt;%
      str_to_lower() %&gt;%
        str_remove_all("\\.") %&gt;%
        str_extract("\\d{1,2} .{3,9}(\\s+)?\\d{0,4}") %&gt;%
        str_split("\\s+", simplify = TRUE)

    if(ncol(raw_date) == 2){
        raw_date &lt;- cbind(raw_date, "annee")
    }

    raw_date[1, 3] &lt;- annee

    raw_date &lt;- str_to_lower(raw_date[1:1, 1:3])

    long_month &lt;- case_when(
      raw_date[2] == "janv" ~ "janvier",
      raw_date[2] == "févr" ~ "février",
      raw_date[2] == "sept" ~ "septembre",
      raw_date[2] == "oct" ~ "octobre",
      raw_date[2] == "nov" ~ "novembre",
      raw_date[2] == "dec" ~ "décembre",
      TRUE ~ as.character(raw_date[2]))

    raw_date[2] &lt;- long_month

    is_it_date &lt;- as.Date(paste0(raw_date, collapse = "-"), format = "%d-%b-%Y") %&gt;%
        is.na() %&gt;% `!`()

    if(is_it_date){
        return(as.Date(paste0(raw_date, collapse = "-"), format = "%d-%b-%Y"))
    } else {
        if(!(raw_date[2] %in% liste_mois)){
            raw_date[2] &lt;- liste_mois[stringdist::amatch(raw_date[2], liste_mois, maxDist = 2)]
            return(as.Date(paste0(raw_date, collapse = "-"), format = "%d-%b-%Y"))
        }
    }
}</code></pre>
<p>
This function is more complicated than I had hoped. This is because dates come in different formats. For example, there are dates written like this “21 Janvier 1872”, or “12 Septembre” or “12 sept.”. The biggest problem here is that sometimes the year is missing. I deal with this in the next function, which is again, more complicated than what I had hoped. I won’t go into details and explain every step of the function above, but the idea is to extract the data from the raw text, replace abbreviated months with the full month name if needed, and then check if I get a valid date. If not, I try my luck with <code>stringdist::amatch()</code>, to try to match, say “jonvier” with “janvier”. This is in case the OCR made a mistake. I am not very happy with this solution, because it is very approximative, but oh well.
</p>
<p>
The second step is to get the data. I noticed that the rows stay consistent, but do change after June 1st 1876. So I simply hardcoded the goods names, and was only concerned with extracting the prices. I also apply some manual corrections inside the function; mainly dates that were wrongly recognized by the OCR engine, and which were causing problems. Again, not an optimal solution, the other alternative was to simply drop this data, which I did not want to do. Here is the function:
</p>
<pre class="r"><code>extract_table &lt;- function(image_path){

  image &lt;- image_read(image_path)

  annee &lt;- image_path %&gt;%
    str_extract("187\\d")

  ark &lt;- image_path %&gt;%
    str_sub(22, 27)

  source_url &lt;- str_glue("https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F{ark}%2Fpages%2F1/full/full/0/default.jpg",
                         ark = ark)

  text &lt;- ocr(image, engine = "fra")

    text &lt;- text %&gt;%
      str_split("\n") %&gt;%
      unlist %&gt;%
      str_squish() %&gt;%
      str_remove_all("^.{1,10}$") %&gt;%
      discard(is_empty_line) %&gt;%
      str_replace("Mercuriale du \\+ Nov. 1831.", "Mercuriale du 4 Nov. 1831.") %&gt;%
      str_replace("….u .T juillet.", "du 7 juillet") %&gt;%
      str_replace("octobré", "octobre") %&gt;%
      str_replace("AT octobre", "17 octobre") %&gt;% # correction for "f8g6kq8-18  LUNDI 19 OCTOBRÉ 1874. BUREAUX de fa RÉDACTIGedited.jpg"
      str_replace("T norembre", "7 novembre") %&gt;%  # correction for fcrhrn5-LE 8  LUNDI 9 NOVEMBRE 1874 BUREAUX de la RÉDedited.jpg
      str_replace("À oc demain 5", "27 mai") %&gt;% # correction for fd61vzp-MARDI 50. MAI 1876 BUREAUX de la. RED, n VE DE L’ADMINISTRAedited.jpg
      str_replace("G", "6") %&gt;%
      str_replace("Hercariale du 80 nov. 1872,", "du 30 novembre 1872") %&gt;%
      str_replace("….u .T juillet.", "du 7 juillet") %&gt;%
      str_replace("Rs ne its du 28-octobré.: :!: :", "28 octobre") %&gt;%
      str_replace("De routes due 98-juilléle. à eat", "28 juillet") %&gt;%
      str_replace("\\| Mereariale dn 14 dre. 1872,", "14 décembre 1872")


  start &lt;- text %&gt;%
    str_which("MARCH(É|E).*D(E|É).*LUXEMBOUR(G|6)") + 2

  start &lt;- ifelse(is_empty(start), str_which(text, ".*D.*UXEM.*") + 2, start)

  end &lt;- start + 40

  pricing_date &lt;- text[start - 1] %&gt;%
    str_remove("%") %&gt;%
    str_remove("er") %&gt;%
    str_remove("\\.+") %&gt;%
    str_remove("\\*") %&gt;%
    str_remove("®") %&gt;%
    str_remove(":") %&gt;%
    str_remove("\\?") %&gt;%
    str_replace("\\$", "9") %&gt;%
    str_remove("°") %&gt;%
    str_replace("‘du 14août.. - ; En", "14 août") %&gt;%
    str_replace("OP PE CN AP PP", "du 28 juin") %&gt;%
    str_replace("‘ du 81 janvi Le", "31 janvier") %&gt;%
    str_replace("\\| \\| du AT août", "17 août") %&gt;%
    str_replace("Su”  du 81 juillet. L", "31 juillet") %&gt;%
    str_replace("0 du 29 avril \" \\|", "29 avril") %&gt;%
    str_replace("LU 0 du 28 ail", "28 avril") %&gt;%
    str_replace("Rs ne its du 28-octobre :!: :", "23 octobre") %&gt;%
    str_replace("7 F \\|  du 13 octobre LA LOTS", "13 octobre") %&gt;%
    str_replace("À. du 18 juin UT ET", "13 juin")


  market_date &lt;- get_date(pricing_date, annee)

  items &lt;- c("Froment", "Métail", "Seigle", "Orge", "Orge mondé", "Orge perlé", "Avoine", "Pois", "Haricots",
             "Lentilles", "Pommes de terre", "Bois de hêtre", "Bois de chêne", "Beurre", "Oeufs", "Foin",
             "Paille", "Viande de boeuf", "Viande de vache", "Viande de veau", "Viande de mouton",
             "Viande fraîche de cochon", "Viande fumée de cochon", "Haricots", "Pois", "Lentilles",
             "Farines de froment", "Farines de méteil", "Farines de seigle")

  items_en &lt;- c("Wheat", "Meslin", "Rye", "Barley", "Pot Barley", "Pearl barley", "Oats", "Peas", "Beans",
    "Lentils", "Potatoes", "Beech wood", "Oak wood", "Butter", "Eggs", "Hay", "Straw", "Beef meat",
    "Cow meat", "Veal meat", "Sheep meat", "Fresh pig meat", "Smoked pig meat", "Beans", "Peas",
    "Lentils", "Wheat flours", "Meslin flours", "Rye flours")


  unit &lt;- c("hectolitre", "hectolitre", "hectolitre", "hectolitre", "kilogram", "kilogram", "hectolitre",
            "hectolitre", "hectolitre", "hectolitre", "hectolitre", "stere", "stere", "kilogram", "dozen",
            "500 kilogram", "500 kilogram", "kilogram", "kilogram", "kilogram", "kilogram", "kilogram",
            "kilogram", "litre", "litre", "litre", "kilogram", "kilogram", "kilogram")

  # starting with june 1876, the order of the items changes
  items_06_1876 &lt;- c("Froment", "Métail", "Seigle", "Orge", "Avoine", "Pois", "Haricots", "Lentilles",
                     "Pommes de terre", "Farines de froment", "Farines de méteil", "Farines de seigle", "Orge mondé",
                     "Beurre", "Oeufs", "Foins", "Paille", "Bois de hêtre", "Bois de chêne", "Viande de boeuf", "Viande de vache",
                     "Viande de veau", "Viande de mouton", "Viande fraîche de cochon", "Viande fumée de cochon")

  items_06_1876_en &lt;- c("Wheat", "Meslin", "Rye", "Barley", "Oats", "Peas", "Beans", "Lentils",
                        "Potatoes", "Wheat flours", "Meslin flours", "Rye flours", "Pot barley",
                        "Butter", "Eggs", "Hay", "Straw", "Beechwood", "Oakwood", "Beef meat", "Cow meat",
                        "Veal meat", "Sheep meat", "Fresh pig meat", "Smoked pig meat")

  units_06_1876 &lt;- c(rep("hectolitre", 9), rep("kilogram", 5), "douzaine", rep("500 kilogram", 2),
                     "stere", "stere", rep("kilogram", 6))

  raw_data &lt;- text[start:end]

  prices &lt;- raw_data %&gt;%
    str_replace_all("©", "0") %&gt;%
    str_extract("\\d{1,2}\\s\\d{2}") %&gt;%
    str_replace("\\s", "\\.") %&gt;%
    as.numeric

  if(is.na(prices[1])){
    prices &lt;- tail(prices, -1)
  } else {
    prices &lt;- prices
  }

  if(market_date &lt; as.Date("01-06-1876", format = "%d-%m-%Y")){
    prices &lt;- prices[1:length(items)]
    tibble("good_fr" = items, "good_en" = items_en, "unit" = unit, "market_date" = market_date,
           "price" = prices, "source_url" = source_url)
  } else {
    prices &lt;- prices[1:length(items_06_1876_en)]
    tibble("good_fr" = items_06_1876, "good_en" = items_06_1876_en, "unit" = units_06_1876,
           "market_date" = market_date, "price" = prices, "source_url" = source_url)
  }
}</code></pre>
<p>
As I wrote previously, I had to deal with the missing year in the date inside this function. To do that, I extracted the year from the name of the file, and pasted it then into the date. The file name contains the data because the function in the function that downloads the files I also performed OCR on the first page, to get the date of the newspaper issue. The sole purpose of this was to get the year. Again, the function is more complex than what I hoped, but it did work well overall. There are still mistakes in the data, for example sometimes the prices are in the wrong order; meaning that they’re “shifted”, for example instead of the prices for eggs, I have the prices of the good that comes next. So obviously be careful if you decide to analyze the data, and double-check if something seems weird. I have made the data available on Luxembourg Open Data Portal, <a href="https://data.public.lu/fr/datasets/digitised-luxembourg-historical-newspapers-journaux-historiques-luxembourgeois-numerises/#resource-community-27293c42-22e5-4811-aee8-89d6f7fa9533">here</a>.
</p>
</section>
<section id="analyzing-the-data" class="level2">
<h2 class="anchored" data-anchor-id="analyzing-the-data">
Analyzing the data
</h2>
<p>
And now, to the fun part. I want to know what was the price of smoked pig meat, and how it varied through time:
</p>
<pre class="r"><code>library(tidyverse)
library(ggplot2)
library(brotools)</code></pre>
<pre class="r"><code>market_price &lt;- read_csv("https://download.data.public.lu/resources/digitised-luxembourg-historical-newspapers-journaux-historiques-luxembourgeois-numerises/20190407-183605/market-price.csv")</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   good_fr = col_character(),
##   good_en = col_character(),
##   unit = col_character(),
##   market_date = col_date(format = ""),
##   price = col_double(),
##   source_url = col_character()
## )</code></pre>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Smoked pig meat") %&gt;%
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of smoked pig meat at the Luxembourg-City market in the 19th century")</code></pre>
<pre><code>## Warning: Removed 2 rows containing missing values (geom_path).</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-9-1.png" width="672">
</p>
<p>
As you can see, there is a huge spike somewhere in 1874. Maybe there was a very severe smoked pig meat shortage that caused the prices to increase dramatically, but the more likely explanation is that there was some sort of mistake, either in the OCR step, or when I extracted the prices, and somehow that particular price of smoked pig meat is actually the price of another, more expensive good.
</p>
<p>
So let’s only consider prices that are below, say, 20 franks, which is already very high:
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Smoked pig meat") %&gt;%
    filter(price &lt; 20) %&gt;% 
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of smoked pig meat at the Luxembourg-City market in the 1870s")</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-10-1.png" width="672">
</p>
<p>
Now, some prices are very high. Let’s check if it’s a mistake:
</p>
<pre class="r"><code>market_price %&gt;% 
    filter(good_en == "Smoked pig meat") %&gt;% 
    filter(between(price, 5, 20)) %&gt;% 
    pull(source_url)</code></pre>
<pre><code>## [1] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fbs2fs6%2Fpages%2F1/full/full/0/default.jpg"
## [2] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fd61vzp%2Fpages%2F1/full/full/0/default.jpg"
## [3] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fjdwb6m%2Fpages%2F1/full/full/0/default.jpg"
## [4] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fng14m3%2Fpages%2F1/full/full/0/default.jpg"
## [5] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fw9jdrb%2Fpages%2F1/full/full/0/default.jpg"</code></pre>
<p>
If you go to the first url, you will land on the first page of the newspaper. To check the table, you need to check the third page, by changing this part of the url “pages%2F1” to this “pages%2F3”.
</p>
<p>
You will then find the following:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/price_smoked_pig.png"><!-- -->
</p>
<p>
As you can see, the price was 2.5, but the OCR returned 7.5. This is a problem that is unavoidable with OCR; there is no way of knowing a priori if characters were not well recognized. It is actually quite interesting how the price for smoked pig meat stayed constant through all these years. A density plot shows that most prices were around 2.5:
</p>
<pre class="r"><code>market_price %&gt;% 
    filter(good_en == "Smoked pig meat") %&gt;% 
    filter(price &lt; 20) %&gt;% 
    ggplot() + 
    geom_density(aes(price), colour = "#82518c") + 
    theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-13-1.png" width="672">
</p>
<p>
What about another good, say, barley?
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Barley") %&gt;%
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of barley at the Luxembourg-City market in the 1870s")</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-14-1.png" width="672">
</p>
<p>
Here again, we see some very high spikes, most likely due to errors. Let’s try to limit the prices to likely values:
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Barley") %&gt;%
    filter(between(price, 10, 40)) %&gt;%
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of barley at the Luxembourg-City market in the 1870s")</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-15-1.png" width="672">
</p>
<pre class="r"><code>market_price %&gt;% 
    filter(good_en == "Barley") %&gt;% 
    ggplot() + 
    geom_density(aes(price), colour = "#82518c") + 
    theme_blog()</code></pre>
<pre><code>## Warning: Removed 39 rows containing non-finite values (stat_density).</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-16-1.png" width="672">
</p>
<p>
Let’s finish this with one of my favourite legume, lentils:
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Lentils") %&gt;%
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of lentils at the Luxembourg-City market in the 1870s")</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-17-1.png" width="672">
</p>
<pre class="r"><code>market_price %&gt;% 
    filter(good_en == "Lentils") %&gt;% 
    ggplot() + 
    geom_density(aes(price), colour = "#82518c") + 
    theme_blog()</code></pre>
<pre><code>## Warning: Removed 79 rows containing non-finite values (stat_density).</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-18-1.png" width="672">
</p>
<p>
All these 0’s might be surprising, but in most cases, they are actually true zeros! For example, you can check this <a href="https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fwsvhwh%2Fpages%2F3/pct:74,0,100,100/full/0/default.jpg">issue</a>. This very likely means that no lentils were available that day at the market. Let’s get rid of the 0s and other extreme values:
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Lentils") %&gt;%
    filter(between(price, 1, 40)) %&gt;% 
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of lentils at the Luxembourg-City market in the 1870s")</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-19-1.png" width="672">
</p>
<p>
I would like to see if the spikes above 30 are errors or not:
</p>
<pre class="r"><code>market_price %&gt;% 
    filter(good_en == "Lentils") %&gt;% 
    filter(between(price, 30, 40)) %&gt;% 
    pull(source_url)</code></pre>
<pre><code>## [1] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F04mb5t%2Fpages%2F1/full/full/0/default.jpg"
## [2] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fb8zp31%2Fpages%2F1/full/full/0/default.jpg"
## [3] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fkzrj53%2Fpages%2F1/full/full/0/default.jpg"
## [4] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fs8sw2v%2Fpages%2F1/full/full/0/default.jpg"
## [5] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fsjptsk%2Fpages%2F1/full/full/0/default.jpg"
## [6] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fwk65b6%2Fpages%2F1/full/full/0/default.jpg"</code></pre>
<p>
The price was recognized as being 35, and turns out it was correct as you can see <a href="https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F04mb5t%2Fpages%2F3/full/full/0/default.jpg">here</a>. This is quite interesting, because the average price was way lower than that:
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Lentils") %&gt;%
    filter(between(price, 1, 40)) %&gt;% 
    summarise(mean_price = mean(price), 
              sd_price = sd(price))</code></pre>
<pre><code>## # A tibble: 1 x 2
##   mean_price sd_price
##        &lt;dbl&gt;    &lt;dbl&gt;
## 1       20.8     5.82</code></pre>
<p>
I’m going to finish here; it was an interesting project, and I can’t wait for more newspapers to be digitized and OCR to work even better. There is a lot more historical data trapped in these newspapers that could provide a lot insights on Luxembourg’s society in the 19th century.
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-04-07-historical_newspaper_scraping_tesseract.html</guid>
  <pubDate>Sun, 07 Apr 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Get text from pdfs or images using OCR: a tutorial with {tesseract} and {magick}</title>
  <link>https://b-rodrigues.github.io/posts/2019-03-31-tesseract.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Michel_Rodange"> <img src="https://b-rodrigues.github.io/assets/img/michelrodange.jpg" title="The high school I attended was named after this gentleman"></a>
</p>
</div>
<p>
In this blog post I’m going to show you how you can extract text from scanned pdf files, or pdf files where no text recognition was performed. (For pdfs where text recognition was performed, you can read my <a href="../posts/2018-06-10-scraping_pdfs.html">other blog post</a>).
</p>
<p>
The pdf I’m going to use can be downloaded from <a href="http://www.luxemburgensia.bnl.lu/cgi/getPdf1_2.pl?mode=item&amp;id=7110">here</a>. It’s a poem titled, <em>D’Léierchen (Dem Léiweckerche säi Lidd)</em>, written by Michel Rodange, arguably Luxembourg’s most well known writer and poet. Michel Rodange is mostly known for his fable, <em>Renert oder De Fuuß am Frack an a Ma’nsgrëßt</em>, starring a central European <a href="https://en.wikipedia.org/wiki/Reynard_the_Fox">trickster anthropomorphic red fox</a>.
</p>
<p>
<img src="https://upload.wikimedia.org/wikipedia/commons/d/d4/Reynard-the-fox.jpg"><!-- -->
</p>
<p>
Anyway, back to the point of this blog post. How can we get data from a pdf where no text recognition was performed (or, how can we get text from an image)? The pdf we need the text from looks like this:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/dleierchen_03.png"><!-- -->
</p>
<p>
To get the text from the pdf, we can use the <code>{tesseract}</code> package, which provides bindings to the <code>tesseract</code> program. <code>tesseract</code> is an open source OCR engine developed by Google. This means that first you will need to install the <code>tesseract</code> program on your system. You can follow the intructions from <code>tesseract</code>’s github <a href="https://github.com/tesseract-ocr/tesseract">page</a>. <code>tesseract</code> is currently at version 4.
</p>
<p>
Before applying OCR to a pdf, let’s first use the <code>{pdftools}</code> package to convert the pdf to png. This is because <code>{tesseract}</code> requires images as input (if you provide a pdf file, it will converted on the fly). Let’s first load the needed packages:
</p>
<pre class="r"><code>library(tidyverse)
library(tesseract)
library(pdftools)
library(magick)</code></pre>
<p>
And now let’s convert the pdf to png files (in plural, because we’ll get one image per page of the pdf):
</p>
<pre class="r"><code>pngfile &lt;- pdftools::pdf_convert("path/to/pdf", dpi = 600)</code></pre>
<p>
This will generate 14 png files. I erase the ones that are not needed, such as the title page. Now, let’s read in all the image files:
</p>
<pre class="r"><code>path &lt;- dir(path = "path/to/pngs", pattern = "*.png", full.names = TRUE)

images &lt;- map(path, magick::image_read)</code></pre>
<p>
The <code>images</code> object is a list of <code>magick-image</code>s, which we can parse. BUUUUUT! There’s a problem. The text is laid out in two columns. Which means that the first line after performing OCR will be the first line of the first column, and the first line of the second column joined together. Same for the other lines of course. So ideally, I’d need to split the file in the middle, and then perform OCR. This is easily done with the <code>{magick}</code> package:
</p>
<pre class="r"><code>first_half &lt;- map(images, ~image_crop(., geometry = "2307x6462"))

second_half &lt;- map(images, ~image_crop(., geometry = "2307x6462+2307+0"))</code></pre>
<p>
Because the pngs are 4614 by 6962 pixels, I can get the first half of the png by cropping at “2307x6462” (I decrease the height a bit to get rid of the page number), and the second half by applying the same logic, but starting the cropping at the “2307+0” position. The result looks like this:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/dleierchen_cropped.png"><!-- -->
</p>
<p>
Much better! Now I need to join these two lists together. I cannot simply join them. Consider the following example:
</p>
<pre class="r"><code>one &lt;- list(1, 3, 5)

two &lt;- list(2, 4, 6)</code></pre>
<p>
This is the setup I currently have; <code>first_half</code> contains odd pages, and <code>second_half</code> contains even pages. The result I want would look like this:
</p>
<pre class="r"><code>list(1, 2, 3, 4, 5, 6)</code></pre>
<pre><code>## [[1]]
## [1] 1
## 
## [[2]]
## [1] 2
## 
## [[3]]
## [1] 3
## 
## [[4]]
## [1] 4
## 
## [[5]]
## [1] 5
## 
## [[6]]
## [1] 6</code></pre>
<p>
There is a very elegant solution, with <code>reduce2()</code> from the <code>{purrr}</code> package. <code>reduce()</code> takes one list and a function, and … <em>reduces</em> the list to a single element. For instance:
</p>
<pre class="r"><code>reduce(list(1, 2, 3), paste)</code></pre>
<pre><code>## [1] "1 2 3"</code></pre>
<p>
<code>reduce2()</code> is very similar, but takes in two lists, but the second list must be one element shorter:
</p>
<pre class="r"><code>reduce2(list(1, 2, 3), list("a", "b"), paste)</code></pre>
<pre><code>## [1] "1 2 a 3 b"</code></pre>
<p>
So we cannot simply use <code>reduce2()</code> on lists <code>one</code> and <code>two</code>, because they’re the same length. So let’s prepend a value to <code>one</code>, using the <code>prepend()</code> function of <code>{purrr}</code>:
</p>
<pre class="r"><code>prepend(one, 0) %&gt;% 
    reduce2(two, c)</code></pre>
<pre><code>## [1] 0 1 2 3 4 5 6</code></pre>
<p>
Exactly what we need! Let’s apply this trick to our lists:
</p>
<pre class="r"><code>merged_list &lt;- prepend(first_half, NA) %&gt;% 
    reduce2(second_half, c) %&gt;% 
    discard(is.na)</code></pre>
<p>
I’ve prepended <code>NA</code> to the first list, and then used <code>reduce2()</code> and then used <code>discard(is.na)</code> to remove the <code>NA</code> I’ve added at the start. Now, we can use OCR to get the text:
</p>
<pre class="r"><code>text_list &lt;- map(merged_list, ocr)</code></pre>
<p>
<code>ocr()</code> uses a model trained on English by default, and even though there is a model trained on Luxembourguish, the one trained on English works better! Very likely because the English model was trained on a lot more data than the Luxembourguish one. I was worried the English model was not going to recognize characters such as <code>é</code>, but no, it worked quite well.
</p>
<p>
This is how it looks like:
</p>
<pre class="r"><code>text_list

[[1]]
[1] "Lhe\n| Kaum huet d’Feld dat fréndlecht Feier\nVun der Aussentssonn gesunn\nAs mam Plou aus Stall a Scheier\n* D’lescht e Bauer ausgezunn.\nFir de Plou em nach ze dreiwen\nWar sai Jéngelchen alaert,\nDeen nét wéllt doheem méi bleiwen\n8 An esouz um viischte Paerd.\nOp der Schéllche stoung ze denken\nD’Léierche mam Hierz voll Lidder\nFir de Béifchen nach ze zanken\n12 Duckelt s’an de Som sech nidder.\nBis e laascht war, an du stémmt se\nUn e Liddchen, datt et kraacht\nOp der Nouteleder klémmt se\n16 Datt dem Béifchen d’Haerz alt laacht.\nAn du sot en: Papp, ech mengen\nBal de Vull dee kénnt och schwatzen.\nLauschter, sot de Papp zum Klengen,\n20 Ech kann d’Liddchen iwersetzen.\nI\nBas de do, mii léiwe Fréndchen\nMa de Wanter dee war laang!\nKuck, ech hat keng fréilech Sténnchen\n24 *T war fir dech a mech mer baang.\nAn du koum ech dech besichen\nWell du goungs nét méi eraus\nMann wat hues jo du eng Kichen\n28 Wat eng Scheier wat en Haus.\nWi zerguttster, a wat Saachen!\nAn déng Frache gouf mer Brout.\nAn déng Kanner, wi se laachen,\n32, An hir Backelcher, wi rout!\nJo, bei dir as Rot nét deier!\nJo a kuck mer wat eng Méscht.\nDat gét Saache fir an d’Scheier\n36 An och Sué fir an d’Késcht.\nMuerges waars de schuns um Dreschen\nIr der Daudes d’Schung sech stréckt\nBas am Do duurch Wis a Paschen\n40 Laascht all Waassergruef geschréckt.\n"
....
....</code></pre>
<p>
We still need to split at the <code>“”</code> character:
</p>
<pre class="r"><code>text_list &lt;- text_list %&gt;% 
    map(., ~str_split(., "\n"))</code></pre>
<p>
The end result:
</p>
<pre class="r"><code>text_list

[[1]]
[[1]][[1]]
 [1] "Lhe"                                      "| Kaum huet d’Feld dat fréndlecht Feier" 
 [3] "Vun der Aussentssonn gesunn"              "As mam Plou aus Stall a Scheier"         
 [5] "* D’lescht e Bauer ausgezunn."            "Fir de Plou em nach ze dreiwen"          
 [7] "War sai Jéngelchen alaert,"               "Deen nét wéllt doheem méi bleiwen"       
 [9] "8 An esouz um viischte Paerd."            "Op der Schéllche stoung ze denken"       
[11] "D’Léierche mam Hierz voll Lidder"         "Fir de Béifchen nach ze zanken"          
[13] "12 Duckelt s’an de Som sech nidder."      "Bis e laascht war, an du stémmt se"      
[15] "Un e Liddchen, datt et kraacht"           "Op der Nouteleder klémmt se"             
[17] "16 Datt dem Béifchen d’Haerz alt laacht." "An du sot en: Papp, ech mengen"          
[19] "Bal de Vull dee kénnt och schwatzen."     "Lauschter, sot de Papp zum Klengen,"     
[21] "20 Ech kann d’Liddchen iwersetzen."       "I"                                       
[23] "Bas de do, mii léiwe Fréndchen"           "Ma de Wanter dee war laang!"             
[25] "Kuck, ech hat keng fréilech Sténnchen"    "24 *T war fir dech a mech mer baang."    
[27] "An du koum ech dech besichen"             "Well du goungs nét méi eraus"            
[29] "Mann wat hues jo du eng Kichen"           "28 Wat eng Scheier wat en Haus."         
[31] "Wi zerguttster, a wat Saachen!"           "An déng Frache gouf mer Brout."          
[33] "An déng Kanner, wi se laachen,"           "32, An hir Backelcher, wi rout!"         
[35] "Jo, bei dir as Rot nét deier!"            "Jo a kuck mer wat eng Méscht."           
[37] "Dat gét Saache fir an d’Scheier"          "36 An och Sué fir an d’Késcht."          
[39] "Muerges waars de schuns um Dreschen"      "Ir der Daudes d’Schung sech stréckt"     
[41] "Bas am Do duurch Wis a Paschen"           "40 Laascht all Waassergruef geschréckt." 
[43] ""  
...
...</code></pre>
<p>
Perfect! Some more cleaning would be needed though. For example, I need to remove the little annotations that are included:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/dleierchen_anot.png"><!-- -->
</p>
<p>
I don’t know yet how I’m going to do that.I also need to remove the line numbers at the beginning of every fourth line, but this is easily done with a simple regular expression:
</p>
<pre class="r"><code>str_remove_all(c("12 bla", "blb", "123 blc"), "^\\d{1,}\\s+")</code></pre>
<pre><code>## [1] "bla" "blb" "blc"</code></pre>
<p>
But this will be left for a future blog post!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-03-31-tesseract.html</guid>
  <pubDate>Sun, 31 Mar 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Pivoting data frames just got easier thanks to pivot_wide() and pivot_long()</title>
  <link>https://b-rodrigues.github.io/posts/2019-03-20-pivot.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://youtu.be/R2u0sN9stbA?t=69"> <img src="https://b-rodrigues.github.io/assets/img/pivot.jpg" title="You know where this leads"></a>
</p>
</div>
<p>
Update: <code>pivot_wide()</code> and <code>pivot_long()</code> are now called <code>pivot_wider()</code> and <code>pivot_longer()</code>, so the code below needs to be updated accondingly.
</p>
<p>
There’s a lot going on in the development version of <code>{tidyr}</code>. New functions for pivoting data frames, <code>pivot_wide()</code> and <code>pivot_long()</code> are coming, and will replace the current functions, <code>spread()</code> and <code>gather()</code>. <code>spread()</code> and <code>gather()</code> will remain in the package though:
</p>
{{% tweet “1108107722128613377” %}}
<p>
If you want to try out these new functions, you need to install the development version of <code>{tidyr}</code>:
</p>
<pre class="r"><code>devtools::install_github("tidyverse/tidyr")</code></pre>
<p>
and you can read the vignette <a href="https://tidyr.tidyverse.org/dev/articles/pivot.html#many-variables-in-column-names">here</a>. Because these functions are still being developed, some more changes might be introduced, but I guess that the main functionality will not change much.
</p>
<p>
Let’s play around with these functions and the <code>mtcars</code> data set. First let’s load the packages and the data:
</p>
<pre class="r"><code>library(tidyverse)
data(mtcars)</code></pre>
<p>
First, let’s create a wide dataset, by <em>spreading</em> the levels of the “am” column to two new columns:
</p>
<pre class="r"><code>mtcars_wide1 &lt;- mtcars %&gt;% 
    pivot_wide(names_from = "am", values_from = "mpg") 

mtcars_wide1 %&gt;% 
    select(`0`, `1`, everything())</code></pre>
<pre><code>## # A tibble: 32 x 11
##      `0`   `1`   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  NA    21       6  160    110  3.9   2.62  16.5     0     4     4
##  2  NA    21       6  160    110  3.9   2.88  17.0     0     4     4
##  3  NA    22.8     4  108     93  3.85  2.32  18.6     1     4     1
##  4  21.4  NA       6  258    110  3.08  3.22  19.4     1     3     1
##  5  18.7  NA       8  360    175  3.15  3.44  17.0     0     3     2
##  6  18.1  NA       6  225    105  2.76  3.46  20.2     1     3     1
##  7  14.3  NA       8  360    245  3.21  3.57  15.8     0     3     4
##  8  24.4  NA       4  147.    62  3.69  3.19  20       1     4     2
##  9  22.8  NA       4  141.    95  3.92  3.15  22.9     1     4     2
## 10  19.2  NA       6  168.   123  3.92  3.44  18.3     1     4     4
## # … with 22 more rows</code></pre>
<p>
<code>pivot_wide()</code>’s arguments are quite explicit: <code>names_from =</code> is where you specify the column that will be spread across the data frame, meaning, the levels of this column will become new columns. <code>values_from =</code> is where you specify the column that will fill in the values of the new columns.
</p>
<p>
“0” and “1” are the new columns (“am” had two levels, <code>0</code> and <code>1</code>), which contain the miles per gallon for manual and automatic cars respectively. Let’s also take a look at the data frame itself:
</p>
<pre class="r"><code>mtcars_wide1 %&gt;% 
    select(`0`, `1`, everything())</code></pre>
<pre><code>## # A tibble: 32 x 11
##      `0`   `1`   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  NA    21       6  160    110  3.9   2.62  16.5     0     4     4
##  2  NA    21       6  160    110  3.9   2.88  17.0     0     4     4
##  3  NA    22.8     4  108     93  3.85  2.32  18.6     1     4     1
##  4  21.4  NA       6  258    110  3.08  3.22  19.4     1     3     1
##  5  18.7  NA       8  360    175  3.15  3.44  17.0     0     3     2
##  6  18.1  NA       6  225    105  2.76  3.46  20.2     1     3     1
##  7  14.3  NA       8  360    245  3.21  3.57  15.8     0     3     4
##  8  24.4  NA       4  147.    62  3.69  3.19  20       1     4     2
##  9  22.8  NA       4  141.    95  3.92  3.15  22.9     1     4     2
## 10  19.2  NA       6  168.   123  3.92  3.44  18.3     1     4     4
## # … with 22 more rows</code></pre>
<p>
Now suppose that we want to spread the values of “am” times “cyl”, and filling the data with the values of “mpg”:
</p>
<pre class="r"><code>mtcars_wide2 &lt;- mtcars %&gt;% 
    pivot_wide(names_from = c("am", "cyl"), values_from = "mpg") 

mtcars_wide2 %&gt;% 
    select(matches("^0|1"), everything())</code></pre>
<pre><code>## # A tibble: 32 x 14
##    `1_6` `1_4` `0_6` `0_8` `0_4` `1_8`  disp    hp  drat    wt  qsec    vs
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1    21  NA    NA    NA    NA      NA  160    110  3.9   2.62  16.5     0
##  2    21  NA    NA    NA    NA      NA  160    110  3.9   2.88  17.0     0
##  3    NA  22.8  NA    NA    NA      NA  108     93  3.85  2.32  18.6     1
##  4    NA  NA    21.4  NA    NA      NA  258    110  3.08  3.22  19.4     1
##  5    NA  NA    NA    18.7  NA      NA  360    175  3.15  3.44  17.0     0
##  6    NA  NA    18.1  NA    NA      NA  225    105  2.76  3.46  20.2     1
##  7    NA  NA    NA    14.3  NA      NA  360    245  3.21  3.57  15.8     0
##  8    NA  NA    NA    NA    24.4    NA  147.    62  3.69  3.19  20       1
##  9    NA  NA    NA    NA    22.8    NA  141.    95  3.92  3.15  22.9     1
## 10    NA  NA    19.2  NA    NA      NA  168.   123  3.92  3.44  18.3     1
## # … with 22 more rows, and 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>
As you can see, this is easily achieved by simply providing more columns to <code>names_from =</code>.
</p>
<p>
Finally, it is also possible to use an optional data set which contains the specifications of the new columns:
</p>
<pre class="r"><code>mtcars_spec &lt;- mtcars %&gt;% 
    expand(am, cyl, .value = "mpg") %&gt;%
    unite(".name", am, cyl, remove = FALSE)

mtcars_spec</code></pre>
<pre><code>## # A tibble: 6 x 4
##   .name    am   cyl .value
##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 
## 1 0_4       0     4 mpg   
## 2 0_6       0     6 mpg   
## 3 0_8       0     8 mpg   
## 4 1_4       1     4 mpg   
## 5 1_6       1     6 mpg   
## 6 1_8       1     8 mpg</code></pre>
<p>
This optional data set defines how the columns “0_4”, “0_6” etc are constructed, and also the value that shall be used to fill in the values. “am” and “cyl” will be used to create the “.name” and the “mpg” column will be used for the “.value”:
</p>
<pre class="r"><code>mtcars %&gt;% 
    pivot_wide(spec = mtcars_spec) %&gt;% 
    select(matches("^0|1"), everything())</code></pre>
<pre><code>## # A tibble: 32 x 14
##    `0_4` `0_6` `0_8` `1_4` `1_6` `1_8`  disp    hp  drat    wt  qsec    vs
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  NA    NA    NA    NA      21    NA  160    110  3.9   2.62  16.5     0
##  2  NA    NA    NA    NA      21    NA  160    110  3.9   2.88  17.0     0
##  3  NA    NA    NA    22.8    NA    NA  108     93  3.85  2.32  18.6     1
##  4  NA    21.4  NA    NA      NA    NA  258    110  3.08  3.22  19.4     1
##  5  NA    NA    18.7  NA      NA    NA  360    175  3.15  3.44  17.0     0
##  6  NA    18.1  NA    NA      NA    NA  225    105  2.76  3.46  20.2     1
##  7  NA    NA    14.3  NA      NA    NA  360    245  3.21  3.57  15.8     0
##  8  24.4  NA    NA    NA      NA    NA  147.    62  3.69  3.19  20       1
##  9  22.8  NA    NA    NA      NA    NA  141.    95  3.92  3.15  22.9     1
## 10  NA    19.2  NA    NA      NA    NA  168.   123  3.92  3.44  18.3     1
## # … with 22 more rows, and 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>
Using a spec is especially useful if you need to make new levels that are not in the data. For instance, suppose that there are actually 10-cylinder cars too, but they do not appear in our sample. We would like to make the fact that they’re missing explicit:
</p>
<pre class="r"><code>mtcars_spec2 &lt;- mtcars %&gt;% 
    expand(am, "cyl" = c(cyl, 10), .value = "mpg") %&gt;%
    unite(".name", am, cyl, remove = FALSE)

mtcars_spec2</code></pre>
<pre><code>## # A tibble: 8 x 4
##   .name    am   cyl .value
##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 
## 1 0_4       0     4 mpg   
## 2 0_6       0     6 mpg   
## 3 0_8       0     8 mpg   
## 4 0_10      0    10 mpg   
## 5 1_4       1     4 mpg   
## 6 1_6       1     6 mpg   
## 7 1_8       1     8 mpg   
## 8 1_10      1    10 mpg</code></pre>
<pre class="r"><code>mtcars %&gt;% 
    pivot_wide(spec = mtcars_spec2) %&gt;% 
    select(matches("^0|1"), everything())</code></pre>
<pre><code>## # A tibble: 32 x 16
##    `0_4` `0_6` `0_8` `0_10` `1_4` `1_6` `1_8` `1_10`  disp    hp  drat
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  NA    NA    NA       NA  NA      21    NA     NA  160    110  3.9 
##  2  NA    NA    NA       NA  NA      21    NA     NA  160    110  3.9 
##  3  NA    NA    NA       NA  22.8    NA    NA     NA  108     93  3.85
##  4  NA    21.4  NA       NA  NA      NA    NA     NA  258    110  3.08
##  5  NA    NA    18.7     NA  NA      NA    NA     NA  360    175  3.15
##  6  NA    18.1  NA       NA  NA      NA    NA     NA  225    105  2.76
##  7  NA    NA    14.3     NA  NA      NA    NA     NA  360    245  3.21
##  8  24.4  NA    NA       NA  NA      NA    NA     NA  147.    62  3.69
##  9  22.8  NA    NA       NA  NA      NA    NA     NA  141.    95  3.92
## 10  NA    19.2  NA       NA  NA      NA    NA     NA  168.   123  3.92
## # … with 22 more rows, and 5 more variables: wt &lt;dbl&gt;, qsec &lt;dbl&gt;,
## #   vs &lt;dbl&gt;, gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>
As you can see, we now have two more columns have been added, and they are full of NA’s.
</p>
<p>
Now, let’s try to go from wide to long data sets, using <code>pivot_long()</code>:
</p>
<pre class="r"><code>mtcars_wide1 %&gt;% 
  pivot_long(cols = c(`1`, `0`), names_to = "am", values_to = "mpg") %&gt;% 
  select(am, mpg, everything())</code></pre>
<pre><code>## # A tibble: 64 x 11
##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1      21       6   160   110  3.9   2.62  16.5     0     4     4
##  2 0      NA       6   160   110  3.9   2.62  16.5     0     4     4
##  3 1      21       6   160   110  3.9   2.88  17.0     0     4     4
##  4 0      NA       6   160   110  3.9   2.88  17.0     0     4     4
##  5 1      22.8     4   108    93  3.85  2.32  18.6     1     4     1
##  6 0      NA       4   108    93  3.85  2.32  18.6     1     4     1
##  7 1      NA       6   258   110  3.08  3.22  19.4     1     3     1
##  8 0      21.4     6   258   110  3.08  3.22  19.4     1     3     1
##  9 1      NA       8   360   175  3.15  3.44  17.0     0     3     2
## 10 0      18.7     8   360   175  3.15  3.44  17.0     0     3     2
## # … with 54 more rows</code></pre>
<p>
The arguments of <code>pivot_long()</code> are quite explicit too, and similar to the ones in <code>pivot_wide()</code>. <code>cols =</code> is where the user specifies the columns that need to be pivoted. <code>names_to =</code> is where the user can specify the name of the new columns, whose levels will be exactly the ones specified to <code>cols =</code>. <code>values_to =</code> is where the user specifies the column name of the new column that will contain the values.
</p>
<p>
It is also possible to specify the columns that should not be transformed, by using <code>-</code>:
</p>
<pre class="r"><code>mtcars_wide1 %&gt;% 
  pivot_long(cols = -matches("^[[:alpha:]]"), names_to = "am", values_to = "mpg") %&gt;% 
  select(am, mpg, everything())</code></pre>
<pre><code>## # A tibble: 64 x 11
##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1      21       6   160   110  3.9   2.62  16.5     0     4     4
##  2 0      NA       6   160   110  3.9   2.62  16.5     0     4     4
##  3 1      21       6   160   110  3.9   2.88  17.0     0     4     4
##  4 0      NA       6   160   110  3.9   2.88  17.0     0     4     4
##  5 1      22.8     4   108    93  3.85  2.32  18.6     1     4     1
##  6 0      NA       4   108    93  3.85  2.32  18.6     1     4     1
##  7 1      NA       6   258   110  3.08  3.22  19.4     1     3     1
##  8 0      21.4     6   258   110  3.08  3.22  19.4     1     3     1
##  9 1      NA       8   360   175  3.15  3.44  17.0     0     3     2
## 10 0      18.7     8   360   175  3.15  3.44  17.0     0     3     2
## # … with 54 more rows</code></pre>
<p>
Here the columns that should not be modified are all those that start with a letter, hence the “<sup>1</sup>” regular expression. It is also possible to remove all the <code>NA</code>’s from the data frame, with <code>na.rm =</code>.
</p>
<pre class="r"><code>mtcars_wide1 %&gt;% 
  pivot_long(cols = c(`1`, `0`), names_to = "am", values_to = "mpg", na.rm = TRUE) %&gt;% 
  select(am, mpg, everything())</code></pre>
<pre><code>## # A tibble: 32 x 11
##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1      21       6  160    110  3.9   2.62  16.5     0     4     4
##  2 1      21       6  160    110  3.9   2.88  17.0     0     4     4
##  3 1      22.8     4  108     93  3.85  2.32  18.6     1     4     1
##  4 0      21.4     6  258    110  3.08  3.22  19.4     1     3     1
##  5 0      18.7     8  360    175  3.15  3.44  17.0     0     3     2
##  6 0      18.1     6  225    105  2.76  3.46  20.2     1     3     1
##  7 0      14.3     8  360    245  3.21  3.57  15.8     0     3     4
##  8 0      24.4     4  147.    62  3.69  3.19  20       1     4     2
##  9 0      22.8     4  141.    95  3.92  3.15  22.9     1     4     2
## 10 0      19.2     6  168.   123  3.92  3.44  18.3     1     4     4
## # … with 22 more rows</code></pre>
<p>
We can also pivot data frames where the names of the columns are made of two or more variables, for example in our <code>mtcars_wide2</code> data frame:
</p>
<pre class="r"><code>mtcars_wide2 %&gt;% 
    select(matches("^0|1"), everything())</code></pre>
<pre><code>## # A tibble: 32 x 14
##    `1_6` `1_4` `0_6` `0_8` `0_4` `1_8`  disp    hp  drat    wt  qsec    vs
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1    21  NA    NA    NA    NA      NA  160    110  3.9   2.62  16.5     0
##  2    21  NA    NA    NA    NA      NA  160    110  3.9   2.88  17.0     0
##  3    NA  22.8  NA    NA    NA      NA  108     93  3.85  2.32  18.6     1
##  4    NA  NA    21.4  NA    NA      NA  258    110  3.08  3.22  19.4     1
##  5    NA  NA    NA    18.7  NA      NA  360    175  3.15  3.44  17.0     0
##  6    NA  NA    18.1  NA    NA      NA  225    105  2.76  3.46  20.2     1
##  7    NA  NA    NA    14.3  NA      NA  360    245  3.21  3.57  15.8     0
##  8    NA  NA    NA    NA    24.4    NA  147.    62  3.69  3.19  20       1
##  9    NA  NA    NA    NA    22.8    NA  141.    95  3.92  3.15  22.9     1
## 10    NA  NA    19.2  NA    NA      NA  168.   123  3.92  3.44  18.3     1
## # … with 22 more rows, and 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>
All the columns that start with either “0” or “1” must be pivoted:
</p>
<pre class="r"><code>mtcars_wide2 %&gt;% 
  pivot_long(cols = matches("0|1"), names_to = "am_cyl", values_to = "mpg", na.rm = TRUE) %&gt;% 
  select(am_cyl, everything())</code></pre>
<pre><code>## # A tibble: 32 x 10
##    am_cyl  disp    hp  drat    wt  qsec    vs  gear  carb   mpg
##    &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1_6     160    110  3.9   2.62  16.5     0     4     4  21  
##  2 1_6     160    110  3.9   2.88  17.0     0     4     4  21  
##  3 1_4     108     93  3.85  2.32  18.6     1     4     1  22.8
##  4 0_6     258    110  3.08  3.22  19.4     1     3     1  21.4
##  5 0_8     360    175  3.15  3.44  17.0     0     3     2  18.7
##  6 0_6     225    105  2.76  3.46  20.2     1     3     1  18.1
##  7 0_8     360    245  3.21  3.57  15.8     0     3     4  14.3
##  8 0_4     147.    62  3.69  3.19  20       1     4     2  24.4
##  9 0_4     141.    95  3.92  3.15  22.9     1     4     2  22.8
## 10 0_6     168.   123  3.92  3.44  18.3     1     4     4  19.2
## # … with 22 more rows</code></pre>
<p>
Now, there is one new column, “am_cyl” which must still be transformed by separating “am_cyl” into two new columns:
</p>
<pre class="r"><code>mtcars_wide2 %&gt;% 
  pivot_long(cols = matches("0|1"), names_to = "am_cyl", values_to = "mpg", na.rm = TRUE) %&gt;% 
  separate(am_cyl, into = c("am", "cyl"), sep = "_") %&gt;% 
  select(am, cyl, everything())</code></pre>
<pre><code>## # A tibble: 32 x 11
##    am    cyl    disp    hp  drat    wt  qsec    vs  gear  carb   mpg
##    &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1     6      160    110  3.9   2.62  16.5     0     4     4  21  
##  2 1     6      160    110  3.9   2.88  17.0     0     4     4  21  
##  3 1     4      108     93  3.85  2.32  18.6     1     4     1  22.8
##  4 0     6      258    110  3.08  3.22  19.4     1     3     1  21.4
##  5 0     8      360    175  3.15  3.44  17.0     0     3     2  18.7
##  6 0     6      225    105  2.76  3.46  20.2     1     3     1  18.1
##  7 0     8      360    245  3.21  3.57  15.8     0     3     4  14.3
##  8 0     4      147.    62  3.69  3.19  20       1     4     2  24.4
##  9 0     4      141.    95  3.92  3.15  22.9     1     4     2  22.8
## 10 0     6      168.   123  3.92  3.44  18.3     1     4     4  19.2
## # … with 22 more rows</code></pre>
<p>
It is also possible to achieve this using a data frame with the specification of what you need:
</p>
<pre class="r"><code>mtcars_spec_long &lt;- mtcars_wide2 %&gt;% 
  pivot_long_spec(matches("0|1"), values_to = "mpg") %&gt;% 
  separate(name, c("am", "cyl"), sep = "_")

mtcars_spec_long</code></pre>
<pre><code>## # A tibble: 6 x 4
##   .name .value am    cyl  
##   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;
## 1 1_6   mpg    1     6    
## 2 1_4   mpg    1     4    
## 3 0_6   mpg    0     6    
## 4 0_8   mpg    0     8    
## 5 0_4   mpg    0     4    
## 6 1_8   mpg    1     8</code></pre>
<p>
Providing this spec to <code>pivot_long()</code> solves the issue:
</p>
<pre class="r"><code>mtcars_wide2 %&gt;% 
  pivot_long(spec = mtcars_spec_long, na.rm = TRUE) %&gt;% 
  select(am, cyl, everything())</code></pre>
<pre><code>## # A tibble: 32 x 11
##    am    cyl    disp    hp  drat    wt  qsec    vs  gear  carb   mpg
##    &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1     6      160    110  3.9   2.62  16.5     0     4     4  21  
##  2 1     6      160    110  3.9   2.88  17.0     0     4     4  21  
##  3 1     4      108     93  3.85  2.32  18.6     1     4     1  22.8
##  4 0     6      258    110  3.08  3.22  19.4     1     3     1  21.4
##  5 0     8      360    175  3.15  3.44  17.0     0     3     2  18.7
##  6 0     6      225    105  2.76  3.46  20.2     1     3     1  18.1
##  7 0     8      360    245  3.21  3.57  15.8     0     3     4  14.3
##  8 0     4      147.    62  3.69  3.19  20       1     4     2  24.4
##  9 0     4      141.    95  3.92  3.15  22.9     1     4     2  22.8
## 10 0     6      168.   123  3.92  3.44  18.3     1     4     4  19.2
## # … with 22 more rows</code></pre>
<p>
Stay tuned to Hadley Wickham’s <a href="https://twitter.com/hadleywickham">twitter</a> as there will definitely be announcements soon!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-03-20-pivot.html</guid>
  <pubDate>Wed, 20 Mar 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2</title>
  <link>https://b-rodrigues.github.io/posts/2019-03-05-historical_vowpal_part2.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://youtu.be/BilPXIt0R2w?t=41"> <img src="https://b-rodrigues.github.io/assets/img/wabbit_reading.jpg" title="Vowpal Wabbit is fast as heck"></a>
</p>
</div>
<p>
In <a href="../posts/2019-03-03-historical_vowpal.html">part 1</a> of this series I set up Vowpal Wabbit to classify newspapers content. Now, let’s use the model to make predictions and see how and if we can improve the model. Then, let’s train the model on the whole data.
</p>
<section id="step-1-prepare-the-data" class="level2">
<h2 class="anchored" data-anchor-id="step-1-prepare-the-data">
Step 1: prepare the data
</h2>
<p>
The first step consists in importing the test data and preparing it. The test data need not be large and thus can be imported and worked on in R.
</p>
<p>
I need to remove the target column from the test set, or else it will be used to make predictions. If you do not remove this column the accuracy of the model will be very high, but it will be wrong since, of course, you do not have the target column at running time… because it is the column that you want to predict!
</p>
<pre class="r"><code>library("tidyverse")
library("yardstick")

small_test &lt;- read_delim("data_split/small_test.txt", "|",
                      escape_double = FALSE, col_names = FALSE,
                      trim_ws = TRUE)

small_test %&gt;%
    mutate(X1= " ") %&gt;%
    write_delim("data_split/small_test2.txt", col_names = FALSE, delim = "|")</code></pre>
<p>
I wrote the data in a file called <code>small_test2.txt</code> and can now use my model to make predictions:
</p>
<pre class="r"><code>system2("/home/cbrunos/miniconda3/bin/vw", args = "-t -i vw_models/small_oaa.model data_split/small_test2.txt -p data_split/small_oaa.predict")</code></pre>
<p>
The predictions get saved in the file <code>small_oaa.predict</code>, which is a plain text file. Let’s add these predictions to the original test set:
</p>
<pre class="r"><code>small_predictions &lt;- read_delim("data_split/small_oaa.predict", "|",
                          escape_double = FALSE, col_names = FALSE,
                          trim_ws = TRUE)

small_test &lt;- small_test %&gt;%
    rename(truth = X1) %&gt;%
    mutate(truth = factor(truth, levels = c("1", "2", "3", "4", "5")))

small_predictions &lt;- small_predictions %&gt;%
    rename(predictions = X1) %&gt;%
    mutate(predictions = factor(predictions, levels = c("1", "2", "3", "4", "5")))

small_test &lt;- small_test %&gt;%
    bind_cols(small_predictions)</code></pre>
</section>
<section id="step-2-use-the-model-and-test-data-to-evaluate-performance" class="level2">
<h2 class="anchored" data-anchor-id="step-2-use-the-model-and-test-data-to-evaluate-performance">
Step 2: use the model and test data to evaluate performance
</h2>
<p>
We can use the several metrics included in <code>{yardstick}</code> to evaluate the model’s performance:
</p>
<pre class="r"><code>conf_mat(small_test, truth = truth, estimate = predictions)

accuracy(small_test, truth = truth, estimate = predictions)</code></pre>
<pre class="r"><code>          Truth
Prediction  1  2  3  4  5
         1 51 15  2 10  1
         2 11  6  3  1  0
         3  0  0  0  0  0
         4  0  0  0  0  0
         5  0  0  0  0  0</code></pre>
<pre class="r"><code># A tibble: 1 x 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy multiclass     0.570</code></pre>
<p>
We can see that the model never predicted class <code>3</code>, <code>4</code> or <code>5</code>. Can we improve by adding some regularization? Let’s find out!
</p>
</section>
<section id="step-3-adding-regularization" class="level2">
<h2 class="anchored" data-anchor-id="step-3-adding-regularization">
Step 3: adding regularization
</h2>
<p>
Before trying regularization, let’s try changing the cost function from the logistic function to the hinge function:
</p>
<pre class="r"><code># Train the model
hinge_oaa_fit &lt;- system2("/home/cbrunos/miniconda3/bin/vw", args = "--oaa 5 -d data_split/small_train.txt --loss_function hinge -f vw_models/hinge_oaa.model", stderr = TRUE)

system2("/home/cbrunos/miniconda3/bin/vw", args = "-i vw_models/hinge_oaa.model -t -d data_split/small_test2.txt -p data_split/hinge_oaa.predict")


predictions &lt;- read_delim("data_split/hinge_oaa.predict", "|",
                          escape_double = FALSE, col_names = FALSE,
                          trim_ws = TRUE)

test &lt;- test %&gt;%
    select(-predictions)

predictions &lt;- predictions %&gt;%
    rename(predictions = X1) %&gt;%
    mutate(predictions = factor(predictions, levels = c("1", "2", "3", "4", "5")))

test &lt;- test %&gt;%
    bind_cols(predictions)</code></pre>
<pre class="r"><code>conf_mat(test, truth = truth, estimate = predictions)

accuracy(test, truth = truth, estimate = predictions)</code></pre>
<pre class="r"><code>          Truth
Prediction   1   2   3   4   5
         1 411 120  45  92   1
         2 355 189  12  17   0
         3  11   2   0   0   0
         4  36   4   0   1   0
         5   3   0   3   0   0</code></pre>
<pre class="r"><code># A tibble: 1 x 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy multiclass     0.462</code></pre>
<p>
Well, didn’t work out so well, but at least we now know how to change the loss function. Let’s go back to the logistic loss and add some regularization. First, let’s train the model:
</p>
<pre class="r"><code>regul_oaa_fit &lt;- system2("/home/cbrunos/miniconda3/bin/vw", args = "--oaa 5 --l1 0.005 --l2 0.005 -d data_split/small_train.txt -f vw_models/small_regul_oaa.model", stderr = TRUE)</code></pre>
<p>
Now we can use it for prediction:
</p>
<pre class="r"><code>system2("/home/cbrunos/miniconda3/bin/vw", args = "-i vw_models/small_regul_oaa.model -t -d data_split/test2.txt -p data_split/small_regul_oaa.predict")


predictions &lt;- read_delim("data_split/small_regul_oaa.predict", "|",
                          escape_double = FALSE, col_names = FALSE,
                          trim_ws = TRUE)

test &lt;- test %&gt;%
    select(-predictions)

predictions &lt;- predictions %&gt;%
    rename(predictions = X1) %&gt;%
    mutate(predictions = factor(predictions, levels = c("1", "2", "3", "4", "5")))

test &lt;- test %&gt;%
    bind_cols(predictions)</code></pre>
<p>
We can now use it for predictions:
</p>
<pre class="r"><code>conf_mat(test, truth = truth, estimate = predictions)

accuracy(test, truth = truth, estimate = predictions)</code></pre>
<pre class="r"><code>          Truth
Prediction   1   2   3   4   5
         1 816 315  60 110   1
         2   0   0   0   0   0
         3   0   0   0   0   0
         4   0   0   0   0   0
         5   0   0   0   0   0</code></pre>
<pre class="r"><code># A tibble: 1 x 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy multiclass     0.627</code></pre>
<p>
So accuracy improved, but the model only predicts class 1 now… let’s try with other hyper-parameters values:
</p>
<pre class="r"><code>regul_oaa_fit &lt;- system2("/home/cbrunos/miniconda3/bin/vw", args = "--oaa 5 --l1 0.00015 --l2 0.00015 -d data_split/small_train.txt -f vw_models/small_regul_oaa.model", stderr = TRUE)</code></pre>
<pre class="r"><code>conf_mat(test, truth = truth, estimate = predictions)

accuracy(test, truth = truth, estimate = predictions)</code></pre>
<pre class="r"><code>          Truth
Prediction   1   2   3   4   5
         1 784 300  57 108   1
         2  32  14   3   2   0
         3   0   1   0   0   0
         4   0   0   0   0   0
         5   0   0   0   0   0</code></pre>
<pre class="r"><code># A tibble: 1 x 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy multiclass     0.613</code></pre>
<p>
So accuracy is lower than previously, but at least more categories get correctly predicted. Depending on your needs, you should consider different metrics. Especially for classification problems, you might not be interested in accuracy, in particular if the data is severely unbalanced.
</p>
<p>
Anyhow, to finish this blog post, let’s train the model on the whole data and measure the time it takes to run the full model.
</p>
</section>
<section id="step-4-training-on-the-whole-data" class="level2">
<h2 class="anchored" data-anchor-id="step-4-training-on-the-whole-data">
Step 4: Training on the whole data
</h2>
<p>
Let’s first split the whole data into a training and a testing set:
</p>
<pre class="r"><code>nb_lines &lt;- system2("cat", args = "text_fr.txt | wc -l", stdout = TRUE)

system2("split", args = paste0("-l", floor(as.numeric(nb_lines)*0.995), " text_fr.txt data_split/"))

system2("mv", args = "data_split/aa data_split/train.txt")
system2("mv", args = "data_split/ab data_split/test.txt")</code></pre>
<p>
The whole data contains 260247 lines, and the training set weighs 667MB, which is quite large. Let’s train the simple multiple classifier on the data and see how long it takes:
</p>
<pre class="r"><code>tic &lt;- Sys.time()
oaa_fit &lt;- system2("/home/cbrunos/miniconda3/bin/vw", args = "--oaa 5 -d data_split/train.txt -f vw_models/oaa.model", stderr = TRUE)
Sys.time() - tic</code></pre>
<pre class="r"><code>Time difference of 4.73266 secs</code></pre>
<p>
Yep, you read that right. Training the classifier on 667MB of data took less than 5 seconds!
</p>
<p>
Let’s take a look at the final object:
</p>
<pre class="r"><code>oaa_fit</code></pre>
<pre class="r"><code> [1] "final_regressor = vw_models/oaa.model"                                   
 [2] "Num weight bits = 18"                                                    
 [3] "learning rate = 0.5"                                                     
 [4] "initial_t = 0"                                                           
 [5] "power_t = 0.5"                                                           
 [6] "using no cache"                                                          
 [7] "Reading datafile = data_split/train.txt"                                 
 [8] "num sources = 1"                                                         
 [9] "average  since         example        example  current  current  current"
[10] "loss     last          counter         weight    label  predict features"
[11] "1.000000 1.000000            1            1.0        2        1      253"
[12] "0.500000 0.000000            2            2.0        2        2      499"
[13] "0.250000 0.000000            4            4.0        2        2        6"
[14] "0.250000 0.250000            8            8.0        1        1     2268"
[15] "0.312500 0.375000           16           16.0        1        1      237"
[16] "0.250000 0.187500           32           32.0        1        1      557"
[17] "0.171875 0.093750           64           64.0        1        1      689"
[18] "0.179688 0.187500          128          128.0        2        2      208"
[19] "0.144531 0.109375          256          256.0        1        1      856"
[20] "0.136719 0.128906          512          512.0        4        4        4"
[21] "0.122070 0.107422         1024         1024.0        1        1     1353"
[22] "0.106934 0.091797         2048         2048.0        1        1      571"
[23] "0.098633 0.090332         4096         4096.0        1        1       43"
[24] "0.080566 0.062500         8192         8192.0        1        1      885"
[25] "0.069336 0.058105        16384        16384.0        1        1      810"
[26] "0.062683 0.056030        32768        32768.0        2        2      467"
[27] "0.058167 0.053650        65536        65536.0        1        1       47"
[28] "0.056061 0.053955       131072       131072.0        1        1      495"
[29] ""                                                                        
[30] "finished run"                                                            
[31] "number of examples = 258945"                                             
[32] "weighted example sum = 258945.000000"                                    
[33] "weighted label sum = 0.000000"                                           
[34] "average loss = 0.054467"                                                 
[35] "total feature number = 116335486"  </code></pre>
<p>
Let’s use the test set and see how the model fares:
</p>
<pre class="r"><code>conf_mat(test, truth = truth, estimate = predictions)

accuracy(test, truth = truth, estimate = predictions)</code></pre>
<pre class="r"><code>          Truth
Prediction   1   2   3   4   5
         1 537 175  52 100   1
         2 271 140   8   9   0
         3   1   0   0   0   0
         4   7   0   0   1   0
         5   0   0   0   0   0</code></pre>
<pre class="r"><code># A tibble: 1 x 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy multiclass     0.521</code></pre>
<p>
Better accuracy can certainly be achieved with hyper-parameter tuning… maybe the subject for a future blog post? In any case I am very impressed with Vowpal Wabbit and am certainly looking forward to future developments of <code>{RVowpalWabbit}</code>!
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-03-05-historical_vowpal_part2.html</guid>
  <pubDate>Tue, 05 Mar 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 1</title>
  <link>https://b-rodrigues.github.io/posts/2019-03-03-historical_vowpal.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://youtu.be/BilPXIt0R2w?t=41"> <img src="https://b-rodrigues.github.io/assets/img/wabbit_reading.jpg" title="Vowpal Wabbit is fast as heck"></a>
</p>
</div>
<p>
Can I get enough of historical newspapers data? Seems like I don’t. I already wrote four (<a href="../posts/2019-01-04-newspapers.html">1</a>, <a href="../posts/2019-01-13-newspapers_mets_alto.html">2</a>, <a href="../posts/2019-01-31-newspapers_shiny_app.html">3</a> and <a href="../posts/2019-02-04-newspapers_shiny_app_tutorial.html">4</a>) blog posts, but there’s still a lot to explore. This blog post uses a new batch of data announced on twitter:
</p>
<div style="text-align:center" ;="">
<p><img src="https://b-rodrigues.github.io/assets/img/ralph_marschall_tweet.png" style="width:80%;"></p>
</div>
<p>
and this data could not have arrived at a better moment, since something else got announced via Twitter recently:
</p>
{{% tweet “1098941963527700480” %}}
<p>
I wanted to try using <a href="https://github.com/VowpalWabbit/vowpal_wabbit">Vowpal Wabbit</a> for a couple of weeks now because it seems to be the perfect tool for when you’re dealing with what I call <em>big-ish</em> data: data that is not big data, and might fit in your RAM, but is still a PITA to deal with. It can be data that is large enough to take 30 seconds to be imported into R, and then every operation on it lasts for minutes, and estimating/training a model on it might eat up all your RAM. Vowpal Wabbit avoids all this because it’s an online-learning system. Vowpal Wabbit is capable of training a model with data that it sees on the fly, which means VW can be used for real-time machine learning, but also for when the training data is very large. Each row of the data gets streamed into VW which updates the estimated parameters of the model (or weights) in real time. So no need to first import all the data into R!
</p>
<p>
The goal of this blog post is to get started with VW, and build a very simple logistic model to classify documents using the historical newspapers data from the National Library of Luxembourg, which you can download <a href="https://data.bnl.lu/data/historical-newspapers/">here</a> (scroll down and download the <em>Text Analysis Pack</em>). The goal is not to build the best model, but <em>a</em> model. Several steps are needed for this: prepare the data, install VW and train a model using <code>{RVowpalWabbit}</code>.
</p>
<section id="step-1-preparing-the-data" class="level2">
<h2 class="anchored" data-anchor-id="step-1-preparing-the-data">
Step 1: Preparing the data
</h2>
<p>
The data is in a neat <code>.xml</code> format, and extracting what I need will be easy. However, the input format for VW is a bit unusual; it resembles <em>.psv</em> files (<strong>P</strong>ipe <strong>S</strong>eparated <strong>V</strong>alues) but allows for more flexibility. I will not dwell much into it, but for our purposes, the file must look like this:
</p>
<pre><code>1 | this is the first observation, which in our case will be free text
2 | this is another observation, its label, or class, equals 2
4 | this is another observation, of class 4</code></pre>
<p>
The first column, before the “|” is the target class we want to predict, and the second column contains free text.
</p>
<p>
The raw data looks like this:
</p>
<details>
<p>
</p><summary>
Click if you want to see the raw data
</summary>
<p></p>
<pre><code>&lt;OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd"&gt;
&lt;responseDate&gt;2019-02-28T11:13:01&lt;/responseDate&gt;
&lt;request&gt;http://www.eluxemburgensia.lu/OAI&lt;/request&gt;
&lt;ListRecords&gt;
&lt;record&gt;
&lt;header&gt;
&lt;identifier&gt;digitool-publish:3026998-DTL45&lt;/identifier&gt;
&lt;datestamp&gt;2019-02-28T11:13:01Z&lt;/datestamp&gt;
&lt;/header&gt;
&lt;metadata&gt;
&lt;oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dcterms="http://purl.org/dc/terms/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd"&gt;
&lt;dc:identifier&gt;
https://persist.lu/ark:/70795/6gq1q1/articles/DTL45
&lt;/dc:identifier&gt;
&lt;dc:source&gt;newspaper/indeplux/1871-12-29_01&lt;/dc:source&gt;
&lt;dcterms:isPartOf&gt;L'indépendance luxembourgeoise&lt;/dcterms:isPartOf&gt;
&lt;dcterms:isReferencedBy&gt;
issue:newspaper/indeplux/1871-12-29_01/article:DTL45
&lt;/dcterms:isReferencedBy&gt;
&lt;dc:date&gt;1871-12-29&lt;/dc:date&gt;
&lt;dc:publisher&gt;Jean Joris&lt;/dc:publisher&gt;
&lt;dc:relation&gt;3026998&lt;/dc:relation&gt;
&lt;dcterms:hasVersion&gt;
http://www.eluxemburgensia.lu/webclient/DeliveryManager?pid=3026998#panel:pp|issue:3026998|article:DTL45
&lt;/dcterms:hasVersion&gt;
&lt;dc:description&gt;
CONSEIL COMMUNAL de la ville de Luxembourg. Séance du 23 décembre 1871. (Suite.) Art. 6. Glacière communale. M. le Bourgmcstr ¦ . Le collège échevinal propose un autro mode de se procurer de la glace. Nous avons dépensé 250 fr. cha- que année pour distribuer 30 kilos do glace; c’est une trop forte somme pour un résultat si minime. Nous aurions voulu nous aboucher avec des fabricants de bière ou autres industriels qui nous auraient fourni de la glace en cas de besoin. L’architecte qui été chargé de passer un contrat, a été trouver des négociants, mais ses démarches n’ont pas abouti. 
&lt;/dc:description&gt;
&lt;dc:title&gt;
CONSEIL COMMUNAL de la ville de Luxembourg. Séance du 23 décembre 1871. (Suite.)
&lt;/dc:title&gt;
&lt;dc:type&gt;ARTICLE&lt;/dc:type&gt;
&lt;dc:language&gt;fr&lt;/dc:language&gt;
&lt;dcterms:extent&gt;863&lt;/dcterms:extent&gt;
&lt;/oai_dc:dc&gt;
&lt;/metadata&gt;
&lt;/record&gt;
&lt;/ListRecords&gt;
&lt;/OAI-PMH&gt;</code></pre>
</details>
<p>
I need several things from this file:
</p>
<ul>
<li>
The title of the newspaper: <code>&lt;dcterms:isPartOf&gt;L’indépendance luxembourgeoise&lt;/dcterms:isPartOf&gt;</code>
</li>
<li>
The type of the article: <code>&lt;dc:type&gt;ARTICLE&lt;/dc:type&gt;</code>. Can be Article, Advertisement, Issue, Section or Other.
</li>
<li>
The contents: <code>&lt;dc:description&gt;CONSEIL COMMUNAL de la ville de Luxembourg. Séance du ….&lt;/dc:description&gt;</code>
</li>
</ul>
<p>
I will only focus on newspapers in French, even though newspapers in German also had articles in French. This is because the tag <code>&lt;dc:language&gt;fr&lt;/dc:language&gt;</code> is not always available. If it were, I could simply look for it and extract all the content in French easily, but unfortunately this is not the case.
</p>
<p>
First of all, let’s get the data into R:
</p>
<pre class="r"><code>library("tidyverse")
library("xml2")
library("furrr")

files &lt;- list.files(path = "export01-newspapers1841-1878/", all.files = TRUE, recursive = TRUE)</code></pre>
<p>
This results in a character vector with the path to all the files:
</p>
<pre class="r"><code>head(files)
[1] "000/1400000/1400000-ADVERTISEMENT-DTL78.xml"   "000/1400000/1400000-ADVERTISEMENT-DTL79.xml"  
[3] "000/1400000/1400000-ADVERTISEMENT-DTL80.xml"   "000/1400000/1400000-ADVERTISEMENT-DTL81.xml"  
[5] "000/1400000/1400000-MODSMD_ARTICLE1-DTL34.xml" "000/1400000/1400000-MODSMD_ARTICLE2-DTL35.xml"</code></pre>
<p>
Now I write a function that does the needed data preparation steps. I describe what the function does in the comments inside:
</p>
<pre class="r"><code>to_vw &lt;- function(xml_file){

    # read in the xml file
    file &lt;- read_xml(paste0("export01-newspapers1841-1878/", xml_file))

    # Get the newspaper
    newspaper &lt;- xml_find_all(file, ".//dcterms:isPartOf") %&gt;% xml_text()

    # Only keep the newspapers written in French
    if(!(newspaper %in% c("L'UNION.",
                          "L'indépendance luxembourgeoise",
                          "COURRIER DU GRAND-DUCHÉ DE LUXEMBOURG.",
                          "JOURNAL DE LUXEMBOURG.",
                          "L'AVENIR",
                          "L’Arlequin",
                          "La Gazette du Grand-Duché de Luxembourg",
                          "L'AVENIR DE LUXEMBOURG",
                          "L'AVENIR DU GRAND-DUCHE DE LUXEMBOURG.",
                          "L'AVENIR DU GRAND-DUCHÉ DE LUXEMBOURG.",
                          "Le gratis luxembourgeois",
                          "Luxemburger Zeitung – Journal de Luxembourg",
                          "Recueil des mémoires et des travaux publiés par la Société de Botanique du Grand-Duché de Luxembourg"))){
        return(NULL)
    } else {
        # Get the type of the content. Can be article, advert, issue, section or other
        type &lt;- xml_find_all(file, ".//dc:type") %&gt;% xml_text()

        type &lt;- case_when(type == "ARTICLE" ~ "1",
                          type == "ADVERTISEMENT" ~ "2",
                          type == "ISSUE" ~ "3",
                          type == "SECTION" ~ "4",
                          TRUE ~ "5"
        )

        # Get the content itself. Only keep alphanumeric characters, and remove any line returns or 
        # carriage returns
        description &lt;- xml_find_all(file, ".//dc:description") %&gt;%
            xml_text() %&gt;%
            str_replace_all(pattern = "[^[:alnum:][:space:]]", "") %&gt;%
            str_to_lower() %&gt;%
            str_replace_all("\r?\n|\r|\n", " ")

        # Return the final object: one line that looks like this
        # 1 | bla bla
        paste(type, "|", description)
    }

}</code></pre>
<p>
I can now run this code to parse all the files, and I do so in parallel, thanks to the <code>{furrr}</code> package:
</p>
<pre class="r"><code>plan(multiprocess, workers = 12)

text_fr &lt;- files %&gt;%
    future_map(to_vw)

text_fr &lt;- text_fr %&gt;%
    discard(is.null)

write_lines(text_fr, "text_fr.txt")</code></pre>
</section>
<section id="step-2-install-vowpal-wabbit" class="level2">
<h2 class="anchored" data-anchor-id="step-2-install-vowpal-wabbit">
Step 2: Install Vowpal Wabbit
</h2>
<p>
To easiest way to install VW must be using Anaconda, and more specifically the conda package manager. Anaconda is a Python (and R) distribution for scientific computing and it comes with a package manager called conda which makes installing Python (or R) packages very easy. While VW is a standalone piece of software, it can also be installed by conda or pip. Instead of installing the full Anaconda distribution, you can install Miniconda, which only comes with the bare minimum: a Python executable and the conda package manager. You can find Miniconda <a href="https://docs.conda.io/en/latest/miniconda.html">here</a> and once it’s installed, you can install VW with:
</p>
<pre><code>conda install -c gwerbin vowpal-wabbit </code></pre>
<p>
It is also possible to install VW with pip, as detailed <a href="https://pypi.org/project/vowpalwabbit/">here</a>, but in my experience, managing Python packages with pip is not super. It is better to manage your Python distribution through conda, because it creates environments in your home folder which are independent of the system’s Python installation, which is often out-of-date.
</p>
</section>
<section id="step-3-building-a-model" class="level2">
<h2 class="anchored" data-anchor-id="step-3-building-a-model">
Step 3: Building <em>a</em> model
</h2>
<p>
Vowpal Wabbit can be used from the command line, but there are interfaces for Python and since a few weeks, for R. The R interface is quite crude for now, as it’s still in very early stages. I’m sure it will evolve, and perhaps a Vowpal Wabbit engine will be added to <code>{parsnip}</code>, which would make modeling with VW really easy.
</p>
<p>
For now, let’s only use 10000 lines for prototyping purposes before running the model on the whole file. Because the data is quite large, I do not want to import it into R. So I use command line tools to manipulate this data directly from my hard drive:
</p>
<pre class="r"><code># Prepare data
system2("shuf", args = "-n 10000 text_fr.txt &gt; small.txt")</code></pre>
<p>
<code>shuf</code> is a Unix command, and as such the above code should work on GNU/Linux systems, and most likely macOS too. <code>shuf</code> generates random permutations of a given file to standard output. I use <code>&gt;</code> to direct this output to another file, which I called <code>small.txt</code>. The <code>-n 10000</code> options simply means that I want 10000 lines.
</p>
<p>
I then split this small file into a training and a testing set:
</p>
<pre class="r"><code># Adapted from http://bitsearch.blogspot.com/2009/03/bash-script-to-split-train-and-test.html

# The command below counts the lines in small.txt. This is not really needed, since I know that the 
# file only has 10000 lines, but I kept it here for future reference
# notice the stdout = TRUE option. This is needed because the output simply gets shown in R's
# command line and does get saved into a variable.
nb_lines &lt;- system2("cat", args = "small.txt | wc -l", stdout = TRUE)

system2("split", args = paste0("-l", as.numeric(nb_lines)*0.99, " small.txt data_split/"))</code></pre>
<p>
<code>split</code> is the Unix command that does the splitting. I keep 99% of the lines in the training set and 1% in the test set. This creates two files, <code>aa</code> and <code>ab</code>. I rename them using the <code>mv</code> Unix command:
</p>
<pre class="r"><code>system2("mv", args = "data_split/aa data_split/small_train.txt")
system2("mv", args = "data_split/ab data_split/small_test.txt")</code></pre>
<p>
Ok, now let’s run a model using the VW command line utility from R, using <code>system2()</code>:
</p>
<pre class="r"><code>oaa_fit &lt;- system2("~/miniconda3/bin/vw", args = "--oaa 5 -d data_split/small_train.txt -f small_oaa.model", stderr = TRUE)</code></pre>
<p>
I need to point <code>system2()</code> to the <code>vw</code> executable, and then add some options. <code>–oaa</code> stands for <em>one against all</em> and is a way of doing multiclass classification; first, one class gets classified by a logistic classifier against all the others, then the other class against all the others, then the other…. The <code>5</code> in the option means that there are 5 classes.
</p>
<p>
<code>-d data_split/train.txt</code> specifies the path to the training data. <code>-f</code> means “final regressor” and specifies where you want to save the trained model.
</p>
<p>
This is the output that get’s captured and saved into <code>oaa_fit</code>:
</p>
<pre><code> [1] "final_regressor = oaa.model"                                             
 [2] "Num weight bits = 18"                                                    
 [3] "learning rate = 0.5"                                                     
 [4] "initial_t = 0"                                                           
 [5] "power_t = 0.5"                                                           
 [6] "using no cache"                                                          
 [7] "Reading datafile = data_split/train.txt"                                 
 [8] "num sources = 1"                                                         
 [9] "average  since         example        example  current  current  current"
[10] "loss     last          counter         weight    label  predict features"
[11] "1.000000 1.000000            1            1.0        3        1       87"
[12] "1.000000 1.000000            2            2.0        1        3     2951"
[13] "1.000000 1.000000            4            4.0        1        3      506"
[14] "0.625000 0.250000            8            8.0        1        1      262"
[15] "0.625000 0.625000           16           16.0        1        2      926"
[16] "0.500000 0.375000           32           32.0        4        1        3"
[17] "0.375000 0.250000           64           64.0        1        1      436"
[18] "0.296875 0.218750          128          128.0        2        2      277"
[19] "0.238281 0.179688          256          256.0        2        2      118"
[20] "0.158203 0.078125          512          512.0        2        2       61"
[21] "0.125000 0.091797         1024         1024.0        2        2      258"
[22] "0.096191 0.067383         2048         2048.0        1        1       45"
[23] "0.085205 0.074219         4096         4096.0        1        1      318"
[24] "0.076172 0.067139         8192         8192.0        2        1      523"
[25] ""                                                                        
[26] "finished run"                                                            
[27] "number of examples = 9900"                                               
[28] "weighted example sum = 9900.000000"                                      
[29] "weighted label sum = 0.000000"                                           
[30] "average loss = 0.073434"                                                 
[31] "total feature number = 4456798"  </code></pre>
<p>
Now, when I try to run the same model using <code>RVowpalWabbit::vw()</code> I get the following error:
</p>
<pre class="r"><code>oaa_class &lt;- c("--oaa", "5",
               "-d", "data_split/small_train.txt",
               "-f", "vw_models/small_oaa.model")

result &lt;- vw(oaa_class)</code></pre>
<pre><code>Error in Rvw(args) : unrecognised option '--oaa'</code></pre>
<p>
I think the problem might be because I installed Vowpal Wabbit using conda, and the package cannot find the executable. I’ll open an issue with reproducible code and we’ll see.
</p>
<p>
In any case, that’s it for now! In the next blog post, we’ll see how to get the accuracy of this very simple model, and see how to improve it!
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-03-03-historical_vowpal.html</guid>
  <pubDate>Sun, 03 Mar 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Manipulating strings with the {stringr} package</title>
  <link>https://b-rodrigues.github.io/posts/2019-02-10-stringr_package.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://b-rodrigues.github.io/modern_R/descriptive-statistics-and-data-manipulation.html#manipulate-strings-with-stringr"> <img src="https://b-rodrigues.github.io/assets/img/string.jpg" title="Click here to go the ebook"></a>
</p>
</div>
<p>
This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free <a href="https://b-rodrigues.github.io/modern_R/">here</a>. This is taken from Chapter 4, in which I introduce the <code>{stringr}</code> package.
</p>
<section id="manipulate-strings-with-stringr" class="level2">
<h2 class="anchored" data-anchor-id="manipulate-strings-with-stringr">
Manipulate strings with <code>{stringr}</code>
</h2>
<p>
<code>{stringr}</code> contains functions to manipulate strings. In Chapter 10, I will teach you about regular expressions, but the functions contained in <code>{stringr}</code> allow you to already do a lot of work on strings, without needing to be a regular expression expert.
</p>
<p>
I will discuss the most common string operations: detecting, locating, matching, searching and replacing, and exctracting/removing strings.
</p>
<p>
To introduce these operations, let us use an ALTO file of an issue of <em>The Winchester News</em> from October 31, 1910, which you can find on this <a href="https://gist.githubusercontent.com/b-rodrigues/5139560e7d0f2ecebe5da1df3629e015/raw/e3031d894ffb97217ddbad1ade1b307c9937d2c8/gistfile1.txt">link</a> (to see how the newspaper looked like, <a href="https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/">click here</a>). I re-hosted the file on a public gist for archiving purposes. While working on the book, the original site went down several times…
</p>
<p>
ALTO is an XML schema for the description of text OCR and layout information of pages for digitzed material, such as newspapers (source: <a href="https://en.wikipedia.org/wiki/ALTO_(XML)">ALTO Wikipedia page</a>). For more details, you can read my <a href="https://www.brodrigues.co/blog/2019-01-13-newspapers_mets_alto/">blogpost</a> on the matter, but for our current purposes, it is enough to know that the file contains the text of newspaper articles. The file looks like this:
</p>
<pre><code>&lt;TextLine HEIGHT="138.0" WIDTH="2434.0" HPOS="4056.0" VPOS="5814.0"&gt;
&lt;String STYLEREFS="ID7" HEIGHT="108.0" WIDTH="393.0" HPOS="4056.0" VPOS="5838.0" CONTENT="timore" WC="0.82539684"&gt;
&lt;ALTERNATIVE&gt;timole&lt;/ALTERNATIVE&gt;
&lt;ALTERNATIVE&gt;tlnldre&lt;/ALTERNATIVE&gt;
&lt;ALTERNATIVE&gt;timor&lt;/ALTERNATIVE&gt;
&lt;ALTERNATIVE&gt;insole&lt;/ALTERNATIVE&gt;
&lt;ALTERNATIVE&gt;landed&lt;/ALTERNATIVE&gt;
&lt;/String&gt;
&lt;SP WIDTH="74.0" HPOS="4449.0" VPOS="5838.0"/&gt;
&lt;String STYLEREFS="ID7" HEIGHT="105.0" WIDTH="432.0" HPOS="4524.0" VPOS="5847.0" CONTENT="market" WC="0.95238096"/&gt;
&lt;SP WIDTH="116.0" HPOS="4956.0" VPOS="5847.0"/&gt;
&lt;String STYLEREFS="ID7" HEIGHT="69.0" WIDTH="138.0" HPOS="5073.0" VPOS="5883.0" CONTENT="as" WC="0.96825397"/&gt;
&lt;SP WIDTH="74.0" HPOS="5211.0" VPOS="5883.0"/&gt;
&lt;String STYLEREFS="ID7" HEIGHT="69.0" WIDTH="285.0" HPOS="5286.0" VPOS="5877.0" CONTENT="were" WC="1.0"&gt;
&lt;ALTERNATIVE&gt;verc&lt;/ALTERNATIVE&gt;
&lt;ALTERNATIVE&gt;veer&lt;/ALTERNATIVE&gt;
&lt;/String&gt;
&lt;SP WIDTH="68.0" HPOS="5571.0" VPOS="5877.0"/&gt;
&lt;String STYLEREFS="ID7" HEIGHT="111.0" WIDTH="147.0" HPOS="5640.0" VPOS="5838.0" CONTENT="all" WC="1.0"/&gt;
&lt;SP WIDTH="83.0" HPOS="5787.0" VPOS="5838.0"/&gt;
&lt;String STYLEREFS="ID7" HEIGHT="111.0" WIDTH="183.0" HPOS="5871.0" VPOS="5835.0" CONTENT="the" WC="0.95238096"&gt;
&lt;ALTERNATIVE&gt;tll&lt;/ALTERNATIVE&gt;
&lt;ALTERNATIVE&gt;Cu&lt;/ALTERNATIVE&gt;
&lt;ALTERNATIVE&gt;tall&lt;/ALTERNATIVE&gt;
&lt;/String&gt;
&lt;SP WIDTH="75.0" HPOS="6054.0" VPOS="5835.0"/&gt;
&lt;String STYLEREFS="ID3" HEIGHT="132.0" WIDTH="351.0" HPOS="6129.0" VPOS="5814.0" CONTENT="cattle" WC="0.95238096"/&gt;
&lt;/TextLine&gt;</code></pre>
<p>
We are interested in the strings after <code>CONTENT=</code>. We are going to use functions from the <code>{stringr}</code> package to get the strings after <code>CONTENT=</code>. In Chapter 10, we are going to explore this file again, but using complex regular expressions to get all the content in one go.
</p>
<section id="getting-text-data-into-rstudio" class="level3">
<h3 class="anchored" data-anchor-id="getting-text-data-into-rstudio">
Getting text data into Rstudio
</h3>
<p>
First of all, let us read in the file:
</p>
<pre class="r"><code>winchester &lt;- read_lines("https://gist.githubusercontent.com/b-rodrigues/5139560e7d0f2ecebe5da1df3629e015/raw/e3031d894ffb97217ddbad1ade1b307c9937d2c8/gistfile1.txt")</code></pre>
<p>
Even though the file is an XML file, I still read it in using <code>read_lines()</code> and not <code>read_xml()</code> from the <code>{xml2}</code> package. This is for the purposes of the current exercise, and also because I always have trouble with XML files, and prefer to treat them as simple text files, and use regular expressions to get what I need.
</p>
<p>
Now that the ALTO file is read in and saved in the <code>winchester</code> variable, you might want to print the whole thing in the console. Before that, take a look at the structure:
</p>
<pre class="r"><code>str(winchester)</code></pre>
<pre><code>##  chr [1:43] "" ...</code></pre>
<p>
So the <code>winchester</code> variable is a character atomic vector with 43 elements. So first, we need to understand what these elements are. Let’s start with the first one:
</p>
<pre class="r"><code>winchester[1]</code></pre>
<pre><code>## [1] ""</code></pre>
<p>
Ok, so it seems like the first element is part of the header of the file. What about the second one?
</p>
<pre class="r"><code>winchester[2]</code></pre>
<pre><code>## [1] "&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"&gt;&lt;base href=\"https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml\"&gt;&lt;style&gt;body{margin-left:0;margin-right:0;margin-top:0}#bN015htcoyT__google-cache-hdr{background:#f5f5f5;font:13px arial,sans-serif;text-align:left;color:#202020;border:0;margin:0;border-bottom:1px solid #cecece;line-height:16px;padding:16px 28px 24px 28px}#bN015htcoyT__google-cache-hdr *{display:inline;font:inherit;text-align:inherit;color:inherit;line-height:inherit;background:none;border:0;margin:0;padding:0;letter-spacing:0}#bN015htcoyT__google-cache-hdr a{text-decoration:none;color:#1a0dab}#bN015htcoyT__google-cache-hdr a:hover{text-decoration:underline}#bN015htcoyT__google-cache-hdr a:visited{color:#609}#bN015htcoyT__google-cache-hdr div{display:block;margin-top:4px}#bN015htcoyT__google-cache-hdr b{font-weight:bold;display:inline-block;direction:ltr}&lt;/style&gt;&lt;div id=\"bN015htcoyT__google-cache-hdr\"&gt;&lt;div&gt;&lt;span&gt;This is Google's cache of &lt;a href=\"https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml\"&gt;https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml&lt;/a&gt;.&lt;/span&gt;&amp;nbsp;&lt;span&gt;It is a snapshot of the page as it appeared on 21 Jan 2019 05:18:18 GMT.&lt;/span&gt;&amp;nbsp;&lt;span&gt;The &lt;a href=\"https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml\"&gt;current page&lt;/a&gt; could have changed in the meantime.&lt;/span&gt;&amp;nbsp;&lt;a href=\"http://support.google.com/websearch/bin/answer.py?hl=en&amp;amp;p=cached&amp;amp;answer=1687222\"&gt;&lt;span&gt;Learn more&lt;/span&gt;.&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=\"display:inline-block;margin-top:8px;margin-right:104px;white-space:nowrap\"&gt;&lt;span style=\"margin-right:28px\"&gt;&lt;span style=\"font-weight:bold\"&gt;Full version&lt;/span&gt;&lt;/span&gt;&lt;span style=\"margin-right:28px\"&gt;&lt;a href=\"http://webcache.googleusercontent.com/search?q=cache:2BVPV8QGj3oJ:https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml&amp;amp;hl=en&amp;amp;gl=lu&amp;amp;strip=1&amp;amp;vwsrc=0\"&gt;&lt;span&gt;Text-only version&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;span style=\"margin-right:28px\"&gt;&lt;a href=\"http://webcache.googleusercontent.com/search?q=cache:2BVPV8QGj3oJ:https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml&amp;amp;hl=en&amp;amp;gl=lu&amp;amp;strip=0&amp;amp;vwsrc=1\"&gt;&lt;span&gt;View source&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;span style=\"display:inline-block;margin-top:8px;color:#717171\"&gt;&lt;span&gt;Tip: To quickly find your search term on this page, press &lt;b&gt;Ctrl+F&lt;/b&gt; or &lt;b&gt;⌘-F&lt;/b&gt; (Mac) and use the find bar.&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div style=\"position:relative;\"&gt;&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;"</code></pre>
<p>
Same. So where is the content? The file is very large, so if you print it in the console, it will take quite some time to print, and you will not really be able to make out anything. The best way would be to try to detect the string <code>CONTENT</code> and work from there.
</p>
</section>
<section id="detecting-getting-the-position-and-locating-strings" class="level3">
<h3 class="anchored" data-anchor-id="detecting-getting-the-position-and-locating-strings">
Detecting, getting the position and locating strings
</h3>
<p>
When confronted to an atomic vector of strings, you might want to know inside which elements you can find certain strings. For example, to know which elements of <code>winchester</code> contain the string <code>CONTENT</code>, use <code>str_detect()</code>:
</p>
<pre class="r"><code>winchester %&gt;%
  str_detect("CONTENT")</code></pre>
<pre><code>##  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
## [12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
## [23] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
## [34] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE</code></pre>
<p>
This returns a boolean atomic vector of the same length as <code>winchester</code>. If the string <code>CONTENT</code> is nowhere to be found, the result will equal <code>FALSE</code>, if not it will equal <code>TRUE</code>. Here it is easy to see that the last element contains the string <code>CONTENT</code>. But what if instead of having 43 elements, the vector had 24192 elements? And hundreds would contain the string <code>CONTENT</code>? It would be easier to instead have the indices of the vector where one can find the word <code>CONTENT</code>. This is possible with <code>str_which()</code>:
</p>
<pre class="r"><code>winchester %&gt;%
  str_which("CONTENT")</code></pre>
<pre><code>## [1] 43</code></pre>
<p>
Here, the result is 43, meaning that the 43rd element of <code>winchester</code> contains the string <code>CONTENT</code> somewhere. If we need more precision, we can use <code>str_locate()</code> and <code>str_locate_all()</code>. To explain how both these functions work, let’s create a very small example:
</p>
<pre class="r"><code>ancient_philosophers &lt;- c("aristotle", "plato", "epictetus", "seneca the younger", "epicurus", "marcus aurelius")</code></pre>
<p>
Now suppose I am interested in philosophers whose name ends in <code>us</code>. Let us use <code>str_locate()</code> first:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_locate("us")</code></pre>
<pre><code>##      start end
## [1,]    NA  NA
## [2,]    NA  NA
## [3,]     8   9
## [4,]    NA  NA
## [5,]     7   8
## [6,]     5   6</code></pre>
<p>
You can interpret the result as follows: in the rows, the index of the vector where the string <code>us</code> is found. So the 3rd, 5th and 6th philosopher have <code>us</code> somewhere in their name. The result also has two columns: <code>start</code> and <code>end</code>. These give the position of the string. So the string <code>us</code> can be found starting at position 8 of the 3rd element of the vector, and ends at position 9. Same goes for the other philisophers. However, consider Marcus Aurelius. He has two names, both ending with <code>us</code>. However, <code>str_locate()</code> only shows the position of the <code>us</code> in <code>Marcus</code>.
</p>
<p>
To get both <code>us</code> strings, you need to use <code>str_locate_all()</code>:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_locate_all("us")</code></pre>
<pre><code>## [[1]]
##      start end
## 
## [[2]]
##      start end
## 
## [[3]]
##      start end
## [1,]     8   9
## 
## [[4]]
##      start end
## 
## [[5]]
##      start end
## [1,]     7   8
## 
## [[6]]
##      start end
## [1,]     5   6
## [2,]    14  15</code></pre>
<p>
Now we get the position of the two <code>us</code> in Marcus Aurelius. Doing this on the <code>winchester</code> vector will give use the position of the <code>CONTENT</code> string, but this is not really important right now. What matters is that you know how <code>str_locate()</code> and <code>str_locate_all()</code> work.
</p>
<p>
So now that we know what interests us in the 43nd element of <code>winchester</code>, let’s take a closer look at it:
</p>
<pre class="r"><code>winchester[43]</code></pre>
<p>
As you can see, it’s a mess:
</p>
<pre><code>&lt;TextLine HEIGHT=\"126.0\" WIDTH=\"1731.0\" HPOS=\"17160.0\" VPOS=\"21252.0\"&gt;&lt;String HEIGHT=\"114.0\" WIDTH=\"354.0\" HPOS=\"17160.0\" VPOS=\"21264.0\" CONTENT=\"0tV\" WC=\"0.8095238\"/&gt;&lt;SP WIDTH=\"131.0\" HPOS=\"17514.0\" VPOS=\"21264.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"111.0\" WIDTH=\"474.0\" HPOS=\"17646.0\" VPOS=\"21258.0\" CONTENT=\"BATES\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"140.0\" HPOS=\"18120.0\" VPOS=\"21258.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"114.0\" WIDTH=\"630.0\" HPOS=\"18261.0\" VPOS=\"21252.0\" CONTENT=\"President\" WC=\"1.0\"&gt;&lt;ALTERNATIVE&gt;Prcideht&lt;/ALTERNATIVE&gt;&lt;ALTERNATIVE&gt;Pride&lt;/ALTERNATIVE&gt;&lt;/String&gt;&lt;/TextLine&gt;&lt;TextLine HEIGHT=\"153.0\" WIDTH=\"1689.0\" HPOS=\"17145.0\" VPOS=\"21417.0\"&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"105.0\" WIDTH=\"258.0\" HPOS=\"17145.0\" VPOS=\"21439.0\" CONTENT=\"WM\" WC=\"0.82539684\"&gt;&lt;TextLine HEIGHT=\"120.0\" WIDTH=\"2211.0\" HPOS=\"16788.0\" VPOS=\"21870.0\"&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"96.0\" WIDTH=\"102.0\" HPOS=\"16788.0\" VPOS=\"21894.0\" CONTENT=\"It\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"72.0\" HPOS=\"16890.0\" VPOS=\"21894.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"96.0\" WIDTH=\"93.0\" HPOS=\"16962.0\" VPOS=\"21885.0\" CONTENT=\"is\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"80.0\" HPOS=\"17055.0\" VPOS=\"21885.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"102.0\" WIDTH=\"417.0\" HPOS=\"17136.0\" VPOS=\"21879.0\" CONTENT=\"seldom\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"80.0\" HPOS=\"17553.0\" VPOS=\"21879.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"96.0\" WIDTH=\"267.0\" HPOS=\"17634.0\" VPOS=\"21873.0\" CONTENT=\"hard\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"81.0\" HPOS=\"17901.0\" VPOS=\"21873.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"87.0\" WIDTH=\"111.0\" HPOS=\"17982.0\" VPOS=\"21879.0\" CONTENT=\"to\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"81.0\" HPOS=\"18093.0\" VPOS=\"21879.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"96.0\" WIDTH=\"219.0\" HPOS=\"18174.0\" VPOS=\"21870.0\" CONTENT=\"find\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"77.0\" HPOS=\"18393.0\" VPOS=\"21870.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"69.0\" WIDTH=\"66.0\" HPOS=\"18471.0\" VPOS=\"21894.0\" CONTENT=\"a\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"77.0\" HPOS=\"18537.0\" VPOS=\"21894.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"78.0\" WIDTH=\"384.0\" HPOS=\"18615.0\" VPOS=\"21888.0\" CONTENT=\"succes\" WC=\"0.82539684\"&gt;&lt;ALTERNATIVE&gt;success&lt;/ALTERNATIVE&gt;&lt;/String&gt;&lt;/TextLine&gt;&lt;TextLine HEIGHT=\"126.0\" WIDTH=\"2316.0\" HPOS=\"16662.0\" VPOS=\"22008.0\"&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"75.0\" WIDTH=\"183.0\" HPOS=\"16662.0\" VPOS=\"22059.0\" CONTENT=\"sor\" WC=\"1.0\"&gt;&lt;ALTERNATIVE&gt;soar&lt;/ALTERNATIVE&gt;&lt;/String&gt;&lt;SP WIDTH=\"72.0\" HPOS=\"16845.0\" VPOS=\"22059.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"90.0\" WIDTH=\"168.0\" HPOS=\"16917.0\" VPOS=\"22035.0\" CONTENT=\"for\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"72.0\" HPOS=\"17085.0\" VPOS=\"22035.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"69.0\" WIDTH=\"267.0\" HPOS=\"17157.0\" VPOS=\"22050.0\" CONTENT=\"even\" WC=\"1.0\"&gt;&lt;ALTERNATIVE&gt;cen&lt;/ALTERNATIVE&gt;&lt;ALTERNATIVE&gt;cent&lt;/ALTERNATIVE&gt;&lt;/String&gt;&lt;SP WIDTH=\"77.0\" HPOS=\"17434.0\" VPOS=\"22050.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"66.0\" WIDTH=\"63.0\" HPOS=\"17502.0\" VPOS=\"22044.0\"</code></pre>
<p>
The file was imported without any newlines. So we need to insert them ourselves, by splitting the string in a clever way.
</p>
</section>
<section id="splitting-strings" class="level3">
<h3 class="anchored" data-anchor-id="splitting-strings">
Splitting strings
</h3>
<p>
There are two functions included in <code>{stringr}</code> to split strings, <code>str_split()</code> and <code>str_split_fixed()</code>. Let’s go back to our ancient philosophers. Two of them, Seneca the Younger and Marcus Aurelius have something else in common than both being Roman Stoic philosophers. Their names are composed of several words. If we want to split their names at the space character, we can use <code>str_split()</code> like this:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_split(" ")</code></pre>
<pre><code>## [[1]]
## [1] "aristotle"
## 
## [[2]]
## [1] "plato"
## 
## [[3]]
## [1] "epictetus"
## 
## [[4]]
## [1] "seneca"  "the"     "younger"
## 
## [[5]]
## [1] "epicurus"
## 
## [[6]]
## [1] "marcus"   "aurelius"</code></pre>
<p>
<code>str_split()</code> also has a <code>simplify = TRUE</code> option:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_split(" ", simplify = TRUE)</code></pre>
<pre><code>##      [,1]        [,2]       [,3]     
## [1,] "aristotle" ""         ""       
## [2,] "plato"     ""         ""       
## [3,] "epictetus" ""         ""       
## [4,] "seneca"    "the"      "younger"
## [5,] "epicurus"  ""         ""       
## [6,] "marcus"    "aurelius" ""</code></pre>
<p>
This time, the returned object is a matrix.
</p>
<p>
What about <code>str_split_fixed()</code>? The difference is that here you can specify the number of pieces to return. For example, you could consider the name “Aurelius” to be the middle name of Marcus Aurelius, and the “the younger” to be the middle name of Seneca the younger. This means that you would want to split the name only at the first space character, and not at all of them. This is easily achieved with <code>str_split_fixed()</code>:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_split_fixed(" ", 2)</code></pre>
<pre><code>##      [,1]        [,2]         
## [1,] "aristotle" ""           
## [2,] "plato"     ""           
## [3,] "epictetus" ""           
## [4,] "seneca"    "the younger"
## [5,] "epicurus"  ""           
## [6,] "marcus"    "aurelius"</code></pre>
<p>
This gives the expected result.
</p>
<p>
So how does this help in our case? Well, if you look at how the ALTO file looks like, at the beginning of this section, you will notice that every line ends with the “&gt;” character. So let’s split at that character!
</p>
<pre class="r"><code>winchester_text &lt;- winchester[43] %&gt;%
  str_split("&gt;")</code></pre>
<p>
Let’s take a closer look at <code>winchester_text</code>:
</p>
<pre class="r"><code>str(winchester_text)</code></pre>
<pre><code>## List of 1
##  $ : chr [1:19706] "&lt;/processingStepSettings" "&lt;processingSoftware" "&lt;softwareCreator" "iArchives&lt;/softwareCreator" ...</code></pre>
<p>
So this is a list of length one, and the first, and only, element of that list is an atomic vector with 19706 elements. Since this is a list of only one element, we can simplify it by saving the atomic vector in a variable:
</p>
<pre class="r"><code>winchester_text &lt;- winchester_text[[1]]</code></pre>
<p>
Let’s now look at some lines:
</p>
<pre class="r"><code>winchester_text[1232:1245]</code></pre>
<pre><code>##  [1] "&lt;SP WIDTH=\"66.0\" HPOS=\"5763.0\" VPOS=\"9696.0\"/"                                                                         
##  [2] "&lt;String STYLEREFS=\"ID7\" HEIGHT=\"108.0\" WIDTH=\"612.0\" HPOS=\"5829.0\" VPOS=\"9693.0\" CONTENT=\"Louisville\" WC=\"1.0\""
##  [3] "&lt;ALTERNATIVE"                                                                                                                
##  [4] "Loniile&lt;/ALTERNATIVE"                                                                                                        
##  [5] "&lt;ALTERNATIVE"                                                                                                                
##  [6] "Lenities&lt;/ALTERNATIVE"                                                                                                       
##  [7] "&lt;/String"                                                                                                                    
##  [8] "&lt;/TextLine"                                                                                                                  
##  [9] "&lt;TextLine HEIGHT=\"150.0\" WIDTH=\"2520.0\" HPOS=\"4032.0\" VPOS=\"9849.0\""                                                 
## [10] "&lt;String STYLEREFS=\"ID7\" HEIGHT=\"108.0\" WIDTH=\"510.0\" HPOS=\"4032.0\" VPOS=\"9861.0\" CONTENT=\"Tobacco\" WC=\"1.0\"/"  
## [11] "&lt;SP WIDTH=\"113.0\" HPOS=\"4542.0\" VPOS=\"9861.0\"/"                                                                        
## [12] "&lt;String STYLEREFS=\"ID7\" HEIGHT=\"105.0\" WIDTH=\"696.0\" HPOS=\"4656.0\" VPOS=\"9861.0\" CONTENT=\"Warehouse\" WC=\"1.0\"" 
## [13] "&lt;ALTERNATIVE"                                                                                                                
## [14] "WHrchons&lt;/ALTERNATIVE"</code></pre>
<p>
This now looks easier to handle. We can narrow it down to the lines that only contain the string we are interested in, “CONTENT”. First, let’s get the indices:
</p>
<pre class="r"><code>content_winchester_index &lt;- winchester_text %&gt;%
  str_which("CONTENT")</code></pre>
<p>
How many lines contain the string “CONTENT”?
</p>
<pre class="r"><code>length(content_winchester_index)</code></pre>
<pre><code>## [1] 4462</code></pre>
<p>
As you can see, this reduces the amount of data we have to work with. Let us save this is a new variable:
</p>
<pre class="r"><code>content_winchester &lt;- winchester_text[content_winchester_index]</code></pre>
</section>
<section id="matching-strings" class="level3">
<h3 class="anchored" data-anchor-id="matching-strings">
Matching strings
</h3>
<p>
Matching strings is useful, but only in combination with regular expressions. As stated at the beginning of this section, we are going to learn about regular expressions in Chapter 10, but in order to make this section useful, we are going to learn the easiest, but perhaps the most useful regular expression: <code>.*</code>.
</p>
<p>
Let’s go back to our ancient philosophers, and use <code>str_match()</code> and see what happens. Let’s match the “us” string:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_match("us")</code></pre>
<pre><code>##      [,1]
## [1,] NA  
## [2,] NA  
## [3,] "us"
## [4,] NA  
## [5,] "us"
## [6,] "us"</code></pre>
<p>
Not very useful, but what about the regular expression <code>.*</code>? How could it help?
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_match(".*us")</code></pre>
<pre><code>##      [,1]             
## [1,] NA               
## [2,] NA               
## [3,] "epictetus"      
## [4,] NA               
## [5,] "epicurus"       
## [6,] "marcus aurelius"</code></pre>
<p>
That’s already very interesting! So how does <code>.*</code> work? To understand, let’s first start by using <code>.</code> alone:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_match(".us")</code></pre>
<pre><code>##      [,1] 
## [1,] NA   
## [2,] NA   
## [3,] "tus"
## [4,] NA   
## [5,] "rus"
## [6,] "cus"</code></pre>
<p>
This also matched whatever symbol comes just before the “u” from “us”. What if we use two <code>.</code> instead?
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_match("..us")</code></pre>
<pre><code>##      [,1]  
## [1,] NA    
## [2,] NA    
## [3,] "etus"
## [4,] NA    
## [5,] "urus"
## [6,] "rcus"</code></pre>
<p>
This time, we get the two symbols that immediately precede “us”. Instead of continuing like this we now use the <code><em></em></code><em>, which matches zero or more of <code>.</code>. So by combining <code></code></em> and <code>.</code>, we can match any symbol repeatedly, until there is nothing more to match. Note that there is also <code>+</code>, which works similarly to <code>*</code>, but it matches one or more symbols.
</p>
<p>
There is also a <code>str_match_all()</code>:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_match_all(".*us")</code></pre>
<pre><code>## [[1]]
##      [,1]
## 
## [[2]]
##      [,1]
## 
## [[3]]
##      [,1]       
## [1,] "epictetus"
## 
## [[4]]
##      [,1]
## 
## [[5]]
##      [,1]      
## [1,] "epicurus"
## 
## [[6]]
##      [,1]             
## [1,] "marcus aurelius"</code></pre>
<p>
In this particular case it does not change the end result, but keep it in mind for cases like this one:
</p>
<pre class="r"><code>c("haha", "huhu") %&gt;%
  str_match("ha")</code></pre>
<pre><code>##      [,1]
## [1,] "ha"
## [2,] NA</code></pre>
<p>
and:
</p>
<pre class="r"><code>c("haha", "huhu") %&gt;%
  str_match_all("ha")</code></pre>
<pre><code>## [[1]]
##      [,1]
## [1,] "ha"
## [2,] "ha"
## 
## [[2]]
##      [,1]</code></pre>
<p>
What if we want to match names containing the letter “t”? Easy:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_match(".*t.*")</code></pre>
<pre><code>##      [,1]                
## [1,] "aristotle"         
## [2,] "plato"             
## [3,] "epictetus"         
## [4,] "seneca the younger"
## [5,] NA                  
## [6,] NA</code></pre>
<p>
So how does this help us with our historical newspaper? Let’s try to get the strings that come after “CONTENT”:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_text %&gt;%
  str_match("CONTENT.*")</code></pre>
<p>
Let’s use our faithful <code>str()</code> function to take a look:
</p>
<pre class="r"><code>winchester_content %&gt;%
  str</code></pre>
<pre><code>##  chr [1:19706, 1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ...</code></pre>
<p>
Hum, there’s a lot of <code>NA</code> values! This is because a lot of the lines from the file did not have the string “CONTENT”, so there is no match possible. Let’s us remove all these <code>NA</code>s. Because the result is a matrix, we cannot use the <code>filter()</code> function from <code>{dplyr}</code>. So we need to convert it to a tibble first:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_content %&gt;%
  as.tibble() %&gt;%
  filter(!is.na(V1))</code></pre>
<pre><code>## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics).
## This warning is displayed once per session.</code></pre>
<p>
Because matrix columns do not have names, when a matrix gets converted into a tibble, the firt column gets automatically called <code>V1</code>. This is why I filter on this column. Let’s take a look at the data:
</p>
<pre class="r"><code>head(winchester_content)</code></pre>
<pre><code>## # A tibble: 6 x 1
##   V1                                  
##   &lt;chr&gt;                               
## 1 "CONTENT=\"J\" WC=\"0.8095238\"/"   
## 2 "CONTENT=\"a\" WC=\"0.8095238\"/"   
## 3 "CONTENT=\"Ira\" WC=\"0.95238096\"/"
## 4 "CONTENT=\"mj\" WC=\"0.8095238\"/"  
## 5 "CONTENT=\"iI\" WC=\"0.8095238\"/"  
## 6 "CONTENT=\"tE1r\" WC=\"0.8095238\"/"</code></pre>
</section>
<section id="searching-and-replacing-strings" class="level3">
<h3 class="anchored" data-anchor-id="searching-and-replacing-strings">
Searching and replacing strings
</h3>
<p>
We are getting close to the final result. We still need to do some cleaning however. Since our data is inside a nice tibble, we might as well stick with it. So let’s first rename the column and change all the strings to lowercase:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_content %&gt;% 
  mutate(content = tolower(V1)) %&gt;% 
  select(-V1)</code></pre>
<p>
Let’s take a look at the result:
</p>
<pre class="r"><code>head(winchester_content)</code></pre>
<pre><code>## # A tibble: 6 x 1
##   content                             
##   &lt;chr&gt;                               
## 1 "content=\"j\" wc=\"0.8095238\"/"   
## 2 "content=\"a\" wc=\"0.8095238\"/"   
## 3 "content=\"ira\" wc=\"0.95238096\"/"
## 4 "content=\"mj\" wc=\"0.8095238\"/"  
## 5 "content=\"ii\" wc=\"0.8095238\"/"  
## 6 "content=\"te1r\" wc=\"0.8095238\"/"</code></pre>
<p>
The second part of the string, “wc=….” is not really interesting. Let’s search and replace this with an empty string, using <code>str_replace()</code>:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_content %&gt;% 
  mutate(content = str_replace(content, "wc.*", ""))

head(winchester_content)</code></pre>
<pre><code>## # A tibble: 6 x 1
##   content            
##   &lt;chr&gt;              
## 1 "content=\"j\" "   
## 2 "content=\"a\" "   
## 3 "content=\"ira\" " 
## 4 "content=\"mj\" "  
## 5 "content=\"ii\" "  
## 6 "content=\"te1r\" "</code></pre>
<p>
We need to use the regular expression from before to replace “wc” and every character that follows. The same can be use to remove “content=”:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_content %&gt;% 
  mutate(content = str_replace(content, "content=", ""))

head(winchester_content)</code></pre>
<pre><code>## # A tibble: 6 x 1
##   content    
##   &lt;chr&gt;      
## 1 "\"j\" "   
## 2 "\"a\" "   
## 3 "\"ira\" " 
## 4 "\"mj\" "  
## 5 "\"ii\" "  
## 6 "\"te1r\" "</code></pre>
<p>
We are almost done, but some cleaning is still necessary:
</p>
</section>
<section id="exctracting-or-removing-strings" class="level3">
<h3 class="anchored" data-anchor-id="exctracting-or-removing-strings">
Exctracting or removing strings
</h3>
<p>
Now, because I now the ALTO spec, I know how to find words that are split between two sentences:
</p>
<pre class="r"><code>winchester_content %&gt;% 
  filter(str_detect(content, "hyppart"))</code></pre>
<pre><code>## # A tibble: 64 x 1
##    content                                                               
##    &lt;chr&gt;                                                                 
##  1 "\"aver\" subs_type=\"hyppart1\" subs_content=\"average\" "           
##  2 "\"age\" subs_type=\"hyppart2\" subs_content=\"average\" "            
##  3 "\"considera\" subs_type=\"hyppart1\" subs_content=\"consideration\" "
##  4 "\"tion\" subs_type=\"hyppart2\" subs_content=\"consideration\" "     
##  5 "\"re\" subs_type=\"hyppart1\" subs_content=\"resigned\" "            
##  6 "\"signed\" subs_type=\"hyppart2\" subs_content=\"resigned\" "        
##  7 "\"install\" subs_type=\"hyppart1\" subs_content=\"installed\" "      
##  8 "\"ed\" subs_type=\"hyppart2\" subs_content=\"installed\" "           
##  9 "\"be\" subs_type=\"hyppart1\" subs_content=\"before\" "              
## 10 "\"fore\" subs_type=\"hyppart2\" subs_content=\"before\" "            
## # … with 54 more rows</code></pre>
<p>
For instance, the word “average” was split over two lines, the first part of the word, “aver” on the first line, and the second part of the word, “age”, on the second line. We want to keep what comes after “subs_content”. Let’s extract the word “average” using <code>str_extract()</code>. However, because only some words were split between two lines, we first need to detect where the string “hyppart1” is located, and only then can we extract what comes after “subs_content”. Thus, we need to combine <code>str_detect()</code> to first detect the string, and then <code>str_extract()</code> to extract what comes after “subs_content”:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_content %&gt;% 
  mutate(content = if_else(str_detect(content, "hyppart1"), 
                           str_extract_all(content, "content=.*", simplify = TRUE), 
                           content))</code></pre>
<p>
Let’s take a look at the result:
</p>
<pre class="r"><code>winchester_content %&gt;% 
  filter(str_detect(content, "content"))</code></pre>
<pre><code>## # A tibble: 64 x 1
##    content                                                          
##    &lt;chr&gt;                                                            
##  1 "content=\"average\" "                                           
##  2 "\"age\" subs_type=\"hyppart2\" subs_content=\"average\" "       
##  3 "content=\"consideration\" "                                     
##  4 "\"tion\" subs_type=\"hyppart2\" subs_content=\"consideration\" "
##  5 "content=\"resigned\" "                                          
##  6 "\"signed\" subs_type=\"hyppart2\" subs_content=\"resigned\" "   
##  7 "content=\"installed\" "                                         
##  8 "\"ed\" subs_type=\"hyppart2\" subs_content=\"installed\" "      
##  9 "content=\"before\" "                                            
## 10 "\"fore\" subs_type=\"hyppart2\" subs_content=\"before\" "       
## # … with 54 more rows</code></pre>
<p>
We still need to get rid of the string “content=” and then of all the strings that contain “hyppart2”, which are not needed now:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_content %&gt;% 
  mutate(content = str_replace(content, "content=", "")) %&gt;% 
  mutate(content = if_else(str_detect(content, "hyppart2"), NA_character_, content))

head(winchester_content)</code></pre>
<pre><code>## # A tibble: 6 x 1
##   content    
##   &lt;chr&gt;      
## 1 "\"j\" "   
## 2 "\"a\" "   
## 3 "\"ira\" " 
## 4 "\"mj\" "  
## 5 "\"ii\" "  
## 6 "\"te1r\" "</code></pre>
<p>
Almost done! We only need to remove the <code>“</code> characters:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_content %&gt;% 
  mutate(content = str_replace_all(content, "\"", "")) 

head(winchester_content)</code></pre>
<pre><code>## # A tibble: 6 x 1
##   content
##   &lt;chr&gt;  
## 1 "j "   
## 2 "a "   
## 3 "ira " 
## 4 "mj "  
## 5 "ii "  
## 6 "te1r "</code></pre>
<p>
Let’s remove space characters with <code>str_trim()</code>:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_content %&gt;% 
  mutate(content = str_trim(content)) 

head(winchester_content)</code></pre>
<pre><code>## # A tibble: 6 x 1
##   content
##   &lt;chr&gt;  
## 1 j      
## 2 a      
## 3 ira    
## 4 mj     
## 5 ii     
## 6 te1r</code></pre>
<p>
To finish off this section, let’s remove stop words (words that do not add any meaning to a sentence, such as “as”, “and”…) and words that are composed of less than 3 characters. You can find a dataset with stopwords inside the <code>{stopwords}</code> package:
</p>
<pre class="r"><code>library(stopwords)

data(data_stopwords_stopwordsiso)

eng_stopwords &lt;- tibble("content" = data_stopwords_stopwordsiso$en)

winchester_content &lt;- winchester_content %&gt;% 
  anti_join(eng_stopwords) %&gt;% 
  filter(nchar(content) &gt; 3)</code></pre>
<pre><code>## Joining, by = "content"</code></pre>
<pre class="r"><code>head(winchester_content)</code></pre>
<pre><code>## # A tibble: 6 x 1
##   content   
##   &lt;chr&gt;     
## 1 te1r      
## 2 jilas     
## 3 edition   
## 4 winchester
## 5 news      
## 6 injuries</code></pre>
<p>
That’s it for this section! You now know how to work with strings, but in Chapter 10 we are going one step further by learning about regular expressions, which offer much more power.
</p>


</section>
</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-02-10-stringr_package.html</guid>
  <pubDate>Sun, 10 Feb 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Building a shiny app to explore historical newspapers: a step-by-step guide</title>
  <link>https://b-rodrigues.github.io/posts/2019-02-04-newspapers_shiny_app_tutorial.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://brodriguesco.shinyapps.io/newspapers_app/"> <img src="https://b-rodrigues.github.io/assets/img/tf_idf.png" title="Click here to go the app"></a>
</p>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">
Introduction
</h2>
<p>
I started off this year by exploring a world that was unknown to me, the world of historical newspapers. I did not know that historical newspapers data was a thing, and have been thoroughly enjoying myself exploring the different datasets published by the National Library of Luxembourg. You can find the data <a href="https://data.bnl.lu/data/historical-newspapers/">here</a>.
</p>
<p>
In my <a href="../posts/2019-01-04-newspapers.html">first blog post</a>, I analyzed data from <em>L’indépendence Luxembourgeoise</em>. I focused on the ads, which were for the most part in the 4th and last page of the newspaper. I did so by extracting the data from the ALTO files. ALTO files contain the content of the newspapers, (basically, the words that make up the article). For this first exercise, I disregarded the METS files, for two reasons. First, I simply wanted to have something quick, and get used to the data. And second, I did not know about ALTO and METS files enough to truly make something out of them. The problem of disregarding the METS file is that I only had a big dump of words, and did not know which words came from which article, or ad in this case.
</p>
<p>
In the <a href="../posts/2019-01-13-newspapers_mets_alto.html">second blog post</a>), I extracted data from the <em>L’Union</em> newspaper, this time by using the metadata from the METS files too. By combining the data from the ALTO files with the metadata from the METS files, I know which words came from which article, which would make further analysis much more interesting.
</p>
<p>
In the <a href="https://www.brodrigues.co/blog/2019-01-31-newspapers_shiny_app/">third blog post</a> of this series, I built a Shiny app which makes it easy to explore the 10 years of publications of <em>L’Union</em>. In this blog post, I will explain in great detail how I created this app.
</p>
</section>
<section id="part-1-getting-the-data-ready-for-the-shiny-app" class="level2">
<h2 class="anchored" data-anchor-id="part-1-getting-the-data-ready-for-the-shiny-app">
Part 1: Getting the data ready for the Shiny app
</h2>
<section id="step-1-extracting-the-needed-data" class="level3">
<h3 class="anchored" data-anchor-id="step-1-extracting-the-needed-data">
Step 1: Extracting the needed data
</h3>
<p>
If you want to follow along with a dataset from a single publication, you can download the following archive on <a href="https://www.dropbox.com/s/56ttqetz4cirsja/1533660_newspaper_lunion_1860-11-14.zip?dl=0">dropbox</a>. Extract this archive, and you will find the data exactly as you would get it from the the big archive you can download from the website of the National Library of Luxembourg. However, to keep the size of the archive small, I removed the .pdf and .jpeg scans.
</p>
<p>
In the <a href="https://www.brodrigues.co/blog/2019-01-13-newspapers_mets_alto/">second blog post</a>) I wrote some functions that made extracting the needed data from the files easy. However, after I wrote the article, I noticed that in some cases these functions were not working exactly as intended. I rewrote them a little bit to overcome these issues. You can find the code I used right below. I won’t explain it too much, because you can read the details in the previous blog post. However, should something be unclear, just drop me an email or a tweet!
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code># This functions will be used within the next functions to extract the relevant pieces

extractor &lt;- function(string, regex, all = FALSE){
    if(all) {
        string %&gt;%
            str_extract_all(regex) %&gt;%
            flatten_chr() %&gt;%
            str_remove_all("=|\\\"") %&gt;%
            #str_extract_all("[:alnum:]+|.|,|\\?|!", simplify = FALSE) %&gt;%
            map(paste, collapse = "") %&gt;%
            flatten_chr()
    } else {
        string %&gt;%
            str_extract(regex) %&gt;%
            str_remove_all("=|\\\"") %&gt;%
            #str_extract_all("[:alnum:]+|.|,|\\?|!", simplify = TRUE) %&gt;%
            paste(collapse = " ") %&gt;%
            tolower()
    }
}

# This function extracts the data from the METS files, and returns a tibble:

extract_mets &lt;- function(article){
    id &lt;- article %&gt;%
        extractor("(?&lt;=ID)(.*?)(?=LABEL)")

    label &lt;- article %&gt;%
        extractor("(?&lt;=LABEL)(.*?)(?=TYPE)")

    type &lt;- article %&gt;%
        extractor("(?&lt;=TYPE)(.*?)(?=&gt;)")

    begins &lt;- article %&gt;%
        extractor("(?&lt;=BEGIN)(.*?)(?=BETYPE)", all = TRUE)

    tibble::tribble(~label, ~type, ~begins, ~id,
                    label, type, begins, id) %&gt;%
        unnest()
}

# This function extracts the data from the ALTO files, and also returns a tibble:

extract_alto &lt;- function(article){
    begins &lt;- article[1] %&gt;%
        extractor("(?&lt;=^ID)(.*?)(?=HPOS)", all = TRUE)

    content &lt;- article %&gt;%
        extractor("(?&lt;=CONTENT)(.*?)(?=WC)", all = TRUE)

    tibble::tribble(~begins, ~content,
                    begins, content) %&gt;%
        unnest()
}

# This function takes the path to a page as an argument, and extracts the data from 
# each article using the function defined above. It then writes a flat CSV to disk.

alto_csv &lt;- function(page_path){

    page &lt;- read_file(page_path)

    doc_name &lt;- str_extract(page_path, "(?&lt;=/text/).*")

    alto_articles &lt;- page %&gt;%
        str_split("TextBlock ") %&gt;%
        flatten_chr()

    alto_df &lt;- map_df(alto_articles, extract_alto)

    alto_df &lt;- alto_df %&gt;%
        mutate(document = doc_name)

    write_csv(alto_df, paste0(page_path, ".csv"))
}

# Same as above, but for the METS file:

mets_csv &lt;- function(page_path){

    page &lt;- read_file(page_path)

    doc_name &lt;- str_extract(page_path, "(?&lt;=/).*")

    mets_articles &lt;- page %&gt;%
        str_split("DMDID") %&gt;%
        flatten_chr()

    mets_df &lt;- map_df(mets_articles, extract_mets)

    mets_df &lt;- mets_df %&gt;%
        mutate(document = doc_name)

    write_csv(mets_df, paste0(page_path, ".csv"))
}

# Time to use the above defined functions. First, let's save the path of all the ALTO files
# into a list:

pages_alto &lt;- str_match(list.files(path = "./", all.files = TRUE, recursive = TRUE), ".*/text/.*.xml") %&gt;%
    discard(is.na)

# I use the {furrr} library to do the extraction in parallel, using 8 cores:

library(furrr)

plan(multiprocess, workers = 8)

tic &lt;- Sys.time()
future_map(pages_alto, alto_csv)
toc &lt;- Sys.time()

toc - tic

#Time difference of 18.64776 mins


# Same for the METS files:

pages_mets &lt;- str_match(list.files(path = "./", all.files = TRUE, recursive = TRUE), ".*mets.xml") %&gt;%
    discard(is.na)


library(furrr)

plan(multiprocess, workers = 8)

tic &lt;- Sys.time()
future_map(pages_mets, mets_csv)
toc &lt;- Sys.time()

toc - tic

#Time difference of 18.64776 mins</code></pre>
</details>
<p>
If you want to try the above code for one ALTO and METS files, you can use the following lines (use the download link in the beginning of the blog post to get the required data):
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code>mets &lt;- read_file("1533660_newspaper_lunion_1860-11-14/1533660_newspaper_lunion_1860-11-14-mets.xml")

mets_articles2 &lt;- mets %&gt;%
    str_split("DMDID") %&gt;%
    flatten_chr()


alto &lt;- read_file("1533660_newspaper_lunion_1860-11-14/text/1860-11-14_01-00001.xml")

alto_articles &lt;- alto %&gt;%
    str_split("TextBlock ") %&gt;%
    flatten_chr()

mets_df2 &lt;- mets_articles2 %&gt;%
    map_df(extract_mets)

# Same exercice for ALTO

alto_df &lt;- alto_articles %&gt;%
    map_df(extract_alto)</code></pre>
</details>
</section>
<section id="step-2-joining-the-data-and-the-metadata" class="level3">
<h3 class="anchored" data-anchor-id="step-2-joining-the-data-and-the-metadata">
Step 2: Joining the data and the metadata
</h3>
<p>
Now that I extracted the data from the ALTO files, and the metadata from the METS files, I still need to join both data sets and do some cleaning. What is the goal of joining these two sources? Remember, by doing this I will know which words come from which article, which will make things much easier later on. I explain how the code works as comments in the code block below:
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code>library(tidyverse)
library(udpipe)
library(textrank)
library(tidytext)

# First, I need the path to each folder that contains the ALTO and METS files. Each newspaper
# data is inside its own folder, one folder per publication. Inside, there's `text` folder that
# contains the ALTO and METS files. This is also where I saved the .csv files from before.

pathdirs &lt;- list.dirs(recursive = FALSE) %&gt;%
    str_match(".*lunion.*") %&gt;%
    discard(is.na)

# The following function imports the METS and the ALTO csv files, joins them, and does some 
# basic cleaning. I used a trick to detect German articles (even though L'Union is a French publication
# some articles are in German) and then remove them.

tidy_papers &lt;- function(path){
    mets_path &lt;- paste0(path, "/", list.files(path, ".*.xml.csv"))
    mets_csv &lt;- data.table::fread(mets_path)

    alto_path &lt;- paste0(path, "/text/", list.files(paste0(path, "/text/"), ".*.csv"))
    alto_csv &lt;- map_dfr(alto_path, data.table::fread)

    final &lt;- full_join(alto_csv, mets_csv, by = "begins") %&gt;%
        mutate(content = tolower(content)) %&gt;%
        mutate(content = if_else(str_detect(content, "hyppart1"), str_extract_all(content, "(?&lt;=CONTENT_).*", simplify = TRUE), content)) %&gt;%
        mutate(content = if_else(str_detect(content, "hyppart2"), NA_character_, content)) %&gt;%
        # When words are separated by a hyphen and split over two lines, it looks like this in the data.
        # ex SUBS_TYPEHypPart1 SUBS_CONTENTexceptée
        # ceptée SUBS_TYPEHypPart2 SUBS_CONTENTexceptée
        # Here, the word `exceptée` is split over two lines, so using a regular expression, I keep
        # the string `exceptée`, which comes after the string `CONTENT`,  from the first line and 
        # replace the second line by an NA_character_
        mutate(content = if_else(str_detect(content, "superscript"), NA_character_, content)) %&gt;%
        mutate(content = if_else(str_detect(content, "subscript"), NA_character_, content)) %&gt;%
        filter(!is.na(content)) %&gt;%
        filter(type == "article") %&gt;%
        group_by(id) %&gt;%
        nest %&gt;%
        # Below I create a list column with all the content of the article in a single string.
        mutate(article_text = map(data, ~paste(.$content, collapse = " "))) %&gt;%
        mutate(article_text = as.character(article_text)) %&gt;%
        # Detecting and removing german articles
        mutate(german = str_detect(article_text, "wenn|wird|und")) %&gt;%
        filter(german == FALSE) %&gt;%
        select(-german) %&gt;%
        # Finally, creating the label of the article (the title), and removing things that are 
        # not articles, such as the daily feuilleton.
        mutate(label = map(data, ~`[`(.$label, 1))) %&gt;%
        filter(!str_detect(label, "embranchement|ligne|bourse|abonnés|feuilleton")) %&gt;%
        filter(label != "na")

    # Save the data in the rds format, as it is not a flat file
    saveRDS(final, paste0(path, "/", str_sub(path, 11, -1), ".rds"))
}

# Here again, I do this in parallel

library(furrr)

plan(multiprocess, workers = 8)

future_map(pathdirs, tidy_papers)</code></pre>
</details>
<p>
This is how one of these files looks like, after passing through this function:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/articles_rds.png"><!-- -->
</p>
<p>
One line is one article. The first column is the id of the article, the second column contains a data frame, the text of the article and finally the title of the article. Let’s take a look at the content of the first element of the <em>data</em> column:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/merged_alto_mets.png"><!-- -->
</p>
<p>
This is the result of the merger of the METS and ALTO csv files. The first column is the id of the article, the second column contains each individual word of the article, the <em>label</em> column the label, or title of the article.
</p>
</section>
<section id="step-3-part-of-speech-annotation" class="level3">
<h3 class="anchored" data-anchor-id="step-3-part-of-speech-annotation">
Step 3: Part-of-speech annotation
</h3>
<p>
Part-of-speech annotation is a technique with the aim of assigning to each word its part of speech. Basically, Pos annotation tells us whether a word is a verb, a noun, an adjective… This will be quite useful for the analysis. To perform Pos annotation, you need to install the <code>{udpipe}</code> package, and download the pre-trained model for the language you want to annotate, in my case French:
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code># Only run this once. This downloads the model for French
udpipe_download_model(language = "french")

# Load the model
udmodel_french &lt;- udpipe_load_model(file = 'french-gsd-ud-2.3-181115.udpipe')

# Save the path of the files to annotate in a list:
pathrds &lt;- list.files(path = "./", all.files = TRUE, recursive = TRUE) %&gt;% 
  str_match(".*.rds") %&gt;%
  discard(is.na)

annotate_rds &lt;- function(path, udmodel){

    newspaper &lt;- readRDS(path)

    s &lt;- udpipe_annotate(udmodel, newspaper$article_text, doc_id = newspaper$label)
    x &lt;- data.frame(s)

    saveRDS(x, str_replace(path, ".rds", "_annotated.rds"))
}

library(furrr)
plan(multiprocess, workers = 8)
tic &lt;- Sys.time()
future_map(pathrds, annotate_rds, udmodel = udmodel_french)
toc &lt;- Sys.time()
toc - tic</code></pre>
</details>
<p>
And here is the result:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/pos_article.png"><!-- -->
</p>
<p>
The <em>upos</em> column contains the tags. Now I know which words are nouns, verbs, adjectives, stopwords… Meaning that I can easily focus on the type of words that interest me. Plus, as an added benefit, I can focus on the lemma of the words. For example, the word <em>viennent</em>, is the <a href="https://en.wikipedia.org/wiki/French_conjugation">conjugated</a> form of the verb <em>venir</em>. <em>venir</em> is thus the lemma of <em>viennent</em>. This means that I can focus my analysis on lemmata. This is useful, because if I compute the frequency of words, <em>viennent</em> would be different from <em>venir</em>, which is not really what we want.
</p>
</section>
<section id="step-4-tf-idf" class="level3">
<h3 class="anchored" data-anchor-id="step-4-tf-idf">
Step 4: tf-idf
</h3>
<p>
Just like what I did in my <a href="https://www.brodrigues.co/blog/2019-01-04-newspapers/">first blog post</a>, I compute the tf-idf of words. The difference, is that here the “document” is the article. This means that I will get the most frequent words inside each article, but who are at the same time rare in the other articles. Doing this ensures that I will only get very relevant words for each article.
</p>
<p>
In the lines below, I prepare the data to then make the plots. The files that are created using the code below are available in the following <a href="https://github.com/b-rodrigues/newspapers_shinyapp/tree/master/tf_idf_data">Github link</a>.
</p>
<p>
In the Shiny app, I read the data directly from the repo. This way, I can keep the app small in size.
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code>path_annotatedrds &lt;- list.files(path = "./", all.files = TRUE, recursive = TRUE) %&gt;% str_match(".*_annotated.rds") %&gt;%
    discard(is.na)

prepare_tf_idf &lt;- function(path){

    annotated_newspaper &lt;- readRDS(path)

    tf_idf_data &lt;- annotated_newspaper %&gt;%
        filter(upos %in% c("NOUN", "VERB", "ADJ", "PROPN")) %&gt;%
        filter(nchar(lemma) &gt; 3) %&gt;%
        count(doc_id, lemma) %&gt;%
        bind_tf_idf(lemma, doc_id, n) %&gt;%
        arrange(desc(tf_idf)) %&gt;%
        group_by(doc_id)

    name_tf_idf_data &lt;- str_split(path, "/", simplify = 1)[1] %&gt;%
        paste0("_tf_idf_data.rds")  %&gt;%
        str_sub(start = 9, -1)

    saveRDS(tf_idf_data, paste0("tf_idf_data/", name_tf_idf_data))
}

library(furrr)
plan(multiprocess, workers = 8)

future_map(path_annotatedrds, prepare_tf_idf)</code></pre>
</details>
</section>
<section id="step-5-summarizing-articles-by-extracting-the-most-relevant-sentences-using-textrank" class="level3">
<h3 class="anchored" data-anchor-id="step-5-summarizing-articles-by-extracting-the-most-relevant-sentences-using-textrank">
Step 5: Summarizing articles by extracting the most relevant sentences, using <code>{textrank}</code>
</h3>
<p>
The last step in data preparation is to extract the most relevant sentences of each articles, using the <code>{textrank}</code> package. This packages implements the <em>PageRank</em> algorithm developed by Larry Page and Sergey Brin in 1995. This algorithm ranks pages by the number of links that point to the pages; the most popular and important pages are also the ones with more links to them. A similar approach is used by the implementation of <code>{textrank}</code>. The algorithm is explained in detail in the following <a href="https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf">paper</a>.
</p>
<p>
However, I cannot simply apply <code>{textrank}</code> to the annotated data frame as it is. Because I have several articles, I have to run the <code>textrank_sentences()</code> function, which extracts the relevant sentences, article by article. For this I still need to transform the data set and also need to prepare the data in a way that makes it digestible by the function. I will not explain the code below line by line, since the documentation of the package is quite straightforward. However, keep in mind that I have to run the <code>textrank_sentences()</code> function for each article, which explains that as some point I use the following:
</p>
<pre class="r"><code>group_by(doc_id) %&gt;%
    nest() %&gt;%</code></pre>
<p>
which then makes it easy to work by article (<em>doc_id</em> is the id of the articles). This part is definitely the most complex, so if you’re interested in the methodology described here, really take your time to understand this function. Let me know if I can clarify things!
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code>library(textrank)
library(brotools)

path_annotatedrds &lt;- list.files(path = "./", all.files = TRUE, recursive = TRUE) %&gt;% str_match(".*_annotated.rds") %&gt;%
    discard(is.na)

prepare_textrank &lt;- function(path){

    annotated_newspaper &lt;- readRDS(path)

    # sentences summary
    x_text_rank &lt;- annotated_newspaper %&gt;%
        group_by(doc_id) %&gt;%
        nest() %&gt;%
        mutate(textrank_id = map(data, ~unique_identifier(., c("paragraph_id", "sentence_id")))) %&gt;%
        mutate(cleaned = map2(.x = data, .y = textrank_id, ~cbind(.x, "textrank_id" = .y))) %&gt;%
        select(doc_id, cleaned)

    x_text_rank2 &lt;- x_text_rank %&gt;%
        mutate(sentences = map(cleaned, ~select(., textrank_id, sentence))) %&gt;%
        # one_row() is a function from my own package, which eliminates duplicates rows
        # from a data frame
        mutate(sentences = map(sentences, ~one_row(., c("textrank_id", "sentence"))))

    x_terminology &lt;- x_text_rank %&gt;%
        mutate(terminology = map(cleaned, ~filter(., upos %in% c("NOUN", "ADJ")))) %&gt;%
        mutate(terminology = map(terminology, ~select(., textrank_id, "lemma"))) %&gt;%
        select(terminology)

    x_final &lt;- bind_cols(x_text_rank2, x_terminology)

    possibly_textrank_sentences &lt;- possibly(textrank_sentences, otherwise = NULL)

    x_final &lt;- x_final %&gt;%
        mutate(summary = map2(sentences, terminology, possibly_textrank_sentences)) %&gt;%
        select(doc_id, summary)

    name_textrank_data &lt;- str_split(path, "/", simplify = 1)[1] %&gt;%
        paste0("_textrank_data.rds") %&gt;%
        str_sub(start = 9, -1)

    saveRDS(x_final, paste0("textrank_data/", name_textrank_data))
}

library(furrr)
plan(multiprocess, workers = 8)

future_map(path_annotatedrds, prepare_textrank)</code></pre>
</details>
<p>
You can download the annotated data sets from the following <a href="https://github.com/b-rodrigues/newspapers_shinyapp/tree/master/textrank_data">link</a>. This is how the data looks like:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/textrank_df.png"><!-- -->
</p>
<p>
Using the <code>summary()</code> function on an element of the <em>summary</em> column returns the 5 most relevant sentences as extracted by <code>{textrank}</code>.
</p>
</section>
</section>
<section id="part-2-building-the-shiny-app" class="level2">
<h2 class="anchored" data-anchor-id="part-2-building-the-shiny-app">
Part 2: Building the shiny app
</h2>
<p>
The most difficult parts are behind us! Building a dashboard is quite easy thanks to the <code>{flexdashboard}</code> package. You need to know Markdown and some Shiny, but it’s way easier than building a complete Shiny app. First of all, install the <code>{fleshdashboard}</code> package, and start from a template, or from <a href="https://rmarkdown.rstudio.com/flexdashboard/layouts.html">this list of layouts</a>.
</p>
<p>
I think that the only trick worth mentioning is that I put the data in a Github repo, and read it directly from the Shiny app. Users choose a date, which I save in a reactive variable. I then build the right url that points towards the right data set, and read it:
</p>
<pre class="r"><code>path_tf_idf &lt;- reactive({
    paste0("https://raw.githubusercontent.com/b-rodrigues/newspapers_shinyapp/master/tf_idf_data/newspaper_lunion_", as.character(input$date2), "_tf_idf_data.rds")
})

dfInput &lt;- reactive({
        read_rds(url(path_tf_idf())) %&gt;%
        top_n(as.numeric(input$tf_df_words), tf_idf) %&gt;%
        mutate(word = reorder(lemma, tf_idf)) 
})</code></pre>
<p>
Because I did all the computations beforehand, the app simply reads the data and creates the bar plots for the tf-idf data, or prints the sentences for the textrank data. To print the sentences correcly, I had to use some html tags, using the <code>{htmltools}</code> package. Below you can find the source code of the app:
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre><code>---
title: "Exploring 10 years of daily publications of the Luxembourguish newspaper, *L'Union*"
output: 
  flexdashboard::flex_dashboard:
    theme: yeti
    orientation: columns
    vertical_layout: fill
runtime: shiny

---

`` `{r setup, include=FALSE}
library(flexdashboard)
library(shiny)
library(tidyverse)
library(textrank)
library(tidytext)
library(udpipe)
library(plotly)
library(ggthemes)
`` `

Sidebar {.sidebar}
=====================================

`` `{r}
dateInput('date2',
      label = paste('Select date'),
      value = as.character(as.Date("1860-11-14")),
      min = as.Date("1860-11-12"), max = as.Date("1869-12-31"),
      format = "yyyy/mm/dd",
      startview = 'year', language = 'en-GB', weekstart = 1
    )
selectInput(inputId = "tf_df_words", 
            label = "Select number of unique words for tf-idf", 
            choices = seq(1:10),
            selected = 5)
selectInput(inputId = "textrank_n_sentences", 
            label = "Select the number of sentences for the summary of the article", 
            choices = seq(1:20), 
            selected = 5)
`` `

*The BnL has digitised over 800.000 pages of Luxembourg newspapers. From those, more than 700.000 
pages have rich metadata using international XML standards such as METS and ALTO. 
Multiple datasets are available for download. Each one is of different size and contains different
newspapers. All the digitised material can also be found on our search platform a-z.lu 
(Make sure to filter by “eluxemburgensia”). All datasets contain XML (METS + ALTO), PDF, original 
TIFF and PNG files for every newspaper issue.* 
Source: https://data.bnl.lu/data/historical-newspapers/

This Shiny app allows you to get summaries of the 10 years of daily issues of the "L'Union" newspaper.
In the first tab, a simple word frequency per article is shown, using the tf-idf method. In the 
second tab, summary sentences have been extracted using the `{textrank}` package.


Word frequency per article
===================================== 
Row
-----------------------------------------------------------------------

### Note: there might be days without any publication. In case of an error, select another date.
    
`` `{r}
path_tf_idf &lt;- reactive({
    paste0("https://raw.githubusercontent.com/b-rodrigues/newspapers_shinyapp/master/tf_idf_data/newspaper_lunion_", as.character(input$date2), "_tf_idf_data.rds")
})
dfInput &lt;- reactive({
        read_rds(url(path_tf_idf())) %&gt;%
        top_n(as.numeric(input$tf_df_words), tf_idf) %&gt;%
        mutate(word = reorder(lemma, tf_idf)) 
})
renderPlotly({
    df_tf_idf &lt;- dfInput()
    p1 &lt;- ggplot(df_tf_idf,
                 aes(word, tf_idf)) +
                 geom_col(show.legend = FALSE, fill = "#82518c") +
                 labs(x = NULL, y = "tf-doc_idf") +
                 facet_wrap(~doc_id, ncol = 2, scales = "free") +
                 coord_flip() +
                 theme_dark()
    ggplotly(p1)
})
`` `

Summary of articles {data-orientation=rows}
===================================== 
Row 
-----------------------------------------------------------------------

### The sentence in bold is the title of the article. You can show more sentences in the summary by using the input in the sidebar.
    
`` `{r}
print_summary_textrank &lt;- function(doc_id, summary, n_sentences){
    htmltools::HTML(paste0("&lt;b&gt;", doc_id, "&lt;/b&gt;"), paste("&lt;p&gt;", summary(summary, n_sentences), sep = "", collapse = "&lt;br/&gt;"), "&lt;/p&gt;")
}
path_textrank &lt;- reactive({
    paste0("https://raw.githubusercontent.com/b-rodrigues/newspapers_shinyapp/master/textrank_data/newspaper_lunion_", as.character(input$date2), "_textrank_data.rds")
})
dfInput2 &lt;- reactive({
        read_rds(url(path_textrank()))
})
renderUI({
    df_textrank &lt;- dfInput2()
    
df_textrank &lt;- df_textrank %&gt;% 
    mutate(to_print = map2(doc_id, summary, print_summary_textrank, n_sentences = as.numeric(input$textrank_n_sentences)))
df_textrank$to_print
})
`` `
</code></pre>
</details>
<p>
I host the app on Shinyapps.io, which is really easy to do from within Rstudio.
</p>
<p>
That was quite long, I’m not sure that anyone will read this blog post completely, but oh well. Better to put the code online, might help someone one day, that leave it to rot on my hard drive.
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-02-04-newspapers_shiny_app_tutorial.html</guid>
  <pubDate>Mon, 04 Feb 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Using Data Science to read 10 years of Luxembourguish newspapers from the 19th century</title>
  <link>https://b-rodrigues.github.io/posts/2019-01-31-newspapers_shiny_app.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://brodriguesco.shinyapps.io/newspapers_app/"> <img src="https://b-rodrigues.github.io/assets/img/tf_idf.png" title="Click here to go the app"></a>
</p>
</div>
<p>
I have been playing around with historical newspaper data (see <a href="../posts/2019-01-04-newspapers.html">here</a> and <a href="../posts/2019-01-13-newspapers_mets_alto.html">here</a>). I have extracted the data from the largest archive available, as described in the previous blog post, and now created a shiny dashboard where it is possible to visualize the most common words per article, as well as read a summary of each article. The summary was made using a method called <em>textrank</em>, using the <code>{textrank}</code> package, which extracts relevant sentences using the Pagerank (developed by Google) algorithm. You can read the scientific paper <a href="https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf">here</a> for more info.
</p>
<p>
You can play around with the app by clicking <a href="https://brodriguesco.shinyapps.io/newspapers_app/">here</a>. In the next blog post, I will explain how I created the app, step by step. It’s going to be a long blog post!
</p>
<p>
Using the app, I noticed that some war happened around November 1860 in China, which turned out to be the <a href="https://en.wikipedia.org/wiki/Second_Opium_War">Second Opium War</a>. The war actually ended in October 1860, but apparently the news took several months to travel to Europe.
</p>
<p>
I also learned that already in the 1861, there was public transportation between some Luxembourguish villages, and French villages that were by the border (see the publication from the 17th of December 1861).
</p>
<p>
Let me know if you find about historical events using my app!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-01-31-newspapers_shiny_app.html</guid>
  <pubDate>Thu, 31 Jan 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Making sense of the METS and ALTO XML standards</title>
  <link>https://b-rodrigues.github.io/posts/2019-01-13-newspapers_mets_alto.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=V1qpvpH26fo"> <img src="https://b-rodrigues.github.io/assets/img/union.png" title="The 19th century was a tough place"></a>
</p>
</div>
<p>
Last week I wrote a <a href="https://www.brodrigues.co/blog/2019-01-04-newspapers/">blog post</a> where I analyzed one year of newspapers ads from 19th century newspapers. The data is made available by the <a href="https://data.bnl.lu/data/historical-newspapers/">national library of Luxembourg</a>. In this blog post, which is part 1 of a 2 part series, I extract data from the 257gb archive, which contains 10 years of publications of the <em>L’Union</em>, another 19th century Luxembourguish newspaper written in French. As I explained in the previous post, to make life easier to data scientists, the national library also included ALTO and METS files (which are a XML files used to describe the layout and contents of physical text sources, such as pages of a book or newspaper) which can be easily parsed by R.
</p>
<p>
This is how a ALTO file looks like:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/alto.png"><!-- -->
</p>
<p>
Each page of the newspaper of a given day has one ALTO file. This is how a METS file looks like:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/mets.png"><!-- -->
</p>
<p>
For each daily issue of the newspaper, there is a METS file. So 1 METS file for 4 ALTO files.
</p>
<p>
In my last blog post, I only extracted the words from the ALTO file (red rectangles of the first screenshot) and did not touch the METS file. The problem of doing this is that I get all the words for each page, without knowing which come from the same article. If I want to know which words come from the same article, I need to use the info from the METS file. From the METS file I have the ID of the article, and some other metadata, such as the title of the article and the type of the article (which can be <em>article</em>, <em>advertisement</em>, etc). The information highlighted with the green rectangles in the METS file can be linked to the green rectangles from the ALTO files. My goal is to get the following data frame from the METS file:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/mets_df.png"><!-- -->
</p>
<p>
and this data frame from the ALTO files:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/alto_df.png"><!-- -->
</p>
<p>
As you can see, by combining both data frames I can know which words come from the same article, which will be helpful for further analysis. <a href="https://en.wikipedia.org/wiki/1860s">A lot of things happened in the 1860s.</a> I am really curious to see if and how these events where reported in a Luxembourguish newspaper. I am particularly curious about how long it took to report certain news from far away, such as the assassination of Abraham Lincoln. But before that I need to extract the data!
</p>
<p>
I will only focus on the METS file. The logic for the ALTO file is the same. All the source code will be in the appendix of this blog post.
</p>
<p>
First, let’s take a look at a METS file:
</p>
<pre class="r"><code>library(tidyverse)
mets &lt;- read_file("1533660_newspaper_lunion_1860-11-14/1533660_newspaper_lunion_1860-11-14-mets.xml")</code></pre>
<p>
This is how it looks like:
</p>
<pre><code>"&lt;?xml version='1.0' encoding='utf-8'?&gt;\r\n&lt;mets xmlns=\"http://www.loc.gov/METS/\" xmlns:mix=\"http://www.loc.gov/mix/v20\" xmlns:mods=\"http://www.loc.gov/mods/v3\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" LABEL=\"L'UNION. 1860-11-14_01\" OBJID=\"https://persist.lu/ark:/70795/m62fcm\" TYPE=\"Newspaper\" xsi:schemaLocation=\"http://www.loc.gov/METS/ http://www.loc.gov/standards/mets/mets.xsd http://www.loc.gov/mods/v3 http://www.loc.gov/standards/mods/v3/mods-3-6.xsd http://www.loc.gov/mix/v20 http://www.loc.gov/standards/mix/mix.xsd\"&gt;\r\n  &lt;metsHdr CREATEDATE=\"2010-12-03T20:35:05\" LASTMODDATE=\"2018-05-09T05:35:51Z\"&gt;\r\n    &lt;agent OTHERTYPE=\"SOFTWARE\" ROLE=\"CREATOR\" TYPE=\"OTHER\"&gt;\r\n      &lt;name&gt;CCS docWORKS/METAe Version 6.4-3&lt;/name&gt;\r\n      &lt;note&gt;docWORKS-ID: 101636&lt;/note&gt;\r\n    &lt;/agent&gt;\r\n  &lt;/metsHdr&gt;\r\n  &lt;dmdSec ID=\"MODSMD_COLLECTION\"&gt;\r\n    &lt;mdWrap LABEL=\"Bibliographic meta-data of the collection\" MDTYPE=\"MODS\" MIMETYPE=\"text/xml\"&gt;\r\n      &lt;xmlData&gt;\r\n        &lt;mods:mods&gt;\r\n          &lt;mods:identifier type=\"local\"&gt;lunion&lt;/mods:identifier&gt;\r\n          &lt;mods:titleInfo ID=\"MODSMD_COLLECTION_TI1\" xml:lang=\"fr\"&gt;\r\n            &lt;mods:title&gt;L'UNION.&lt;/mods:title&gt;\r\n          &lt;/mods:titleInfo&gt;\r\n        &lt;/mods:mods&gt;\r\n      &lt;/xmlData&gt;\r\n    &lt;/mdWrap&gt;\r\n  &lt;/dmdSec&gt;\r\n  &lt;dmdSec ID=\"MODSMD_SECTION1\"&gt;\r\n    &lt;mdWrap MDTYPE=\"MODS\" MIMETYPE=\"text/xml\"&gt;\r\n      &lt;xmlData&gt;\r\n        &lt;mods:mods&gt;\r\n          &lt;mods:titleInfo ID=\"MODSMD_SECTION1_TI1\" xml:lang=\"fr\"&gt;\r\n            &lt;mods:title&gt;Chemins de fer. — Service d'hiver.&lt;/mods:title&gt;\r\n          &lt;/mods:titleInfo&gt;\r\n          &lt;mods:language&gt;\r\n            &lt;mods:languageTerm authority=\"rfc3066\" type=\"code\"&gt;fr&lt;/mods:languageTerm&gt;\r\n ...."</code></pre>
<p>
As usual when you import text files like this, it’s always a good idea to split the file. I will split at the <code>“DMDID”</code> character. Take a look back at the second screenshot. The very first tag, first row, first word after <code>div</code> is <code>“DMDID”</code>. By splitting at this level, I will get back a list, where each element is the content of this <code>div DMDID</code> block. This is exactly what I need, since this block contains the information from the green rectangles. So let’s split the <code>mets</code> variable at this level:
</p>
<pre class="r"><code>mets_articles &lt;- mets %&gt;%
    str_split("DMDID") %&gt;%
    flatten_chr()</code></pre>
<p>
Let’s take a look at <code>mets_articles</code>:
</p>
<pre class="r"><code>str(mets_articles)</code></pre>
<pre><code> chr [1:25] "&lt;?xml version='1.0' encoding='utf-8'?&gt;\r\n&lt;mets xmlns=\"http://www.loc.gov/METS/\" xmlns:mix=\"http://www.loc.g"| __truncated__ ...</code></pre>
<p>
Doesn’t seem to be very helpful, but actually it is. We can see that <code>mets_articles</code> is a now a list of 25 elements.
</p>
<p>
This means that for each element of <code>mets_articles</code>, I need to get the identifier, the label, the type (the red rectangles from the screenshot), but also the information from the <code>“BEGIN”</code> element (the green rectangle).
</p>
<p>
To do this, I’ll be using regular expressions. In general, I start by experimenting in the console, and then when things start looking good, I write a function. Here is this function:
</p>
<pre class="r"><code>extractor &lt;- function(string, regex, all = FALSE){
    if(all) {
        string %&gt;%
            str_extract_all(regex) %&gt;%
            flatten_chr() %&gt;%
            str_extract_all("[:alnum:]+", simplify = FALSE) %&gt;%
            map(paste, collapse = "_") %&gt;%
            flatten_chr()
    } else {
        string %&gt;%
            str_extract(regex) %&gt;%
            str_extract_all("[:alnum:]+", simplify = TRUE) %&gt;%
            paste(collapse = " ") %&gt;%
            tolower()
    }
}</code></pre>
<p>
This function may seem complicated, but it simply encapsulates some pretty standard steps to get the data I need. I had to consider two cases. The first case is when I need to extract all the elements with <code>str_extract_all()</code>, or only the first occurrence, with <code>str_extract()</code>. Let’s test it on the first article of the <code>mets_articles</code> list:
</p>
<pre class="r"><code>mets_articles_1 &lt;- mets_articles[1]</code></pre>
<pre class="r"><code>extractor(mets_articles_1, "ID", all = FALSE)</code></pre>
<pre><code>## [1] "id"</code></pre>
<p>
Let’s see what happens with <code>all = TRUE</code>:
</p>
<pre class="r"><code>extractor(mets_articles_1, "ID", all = TRUE)</code></pre>
<pre><code>##   [1] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [15] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [29] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [43] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [57] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [71] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [85] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [99] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
## [113] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
## [127] "ID" "ID" "ID" "ID" "ID"</code></pre>
<p>
This seems to work as intended. Since I need to call this function several times, I’ll be writing another function that extracts all I need:
</p>
<pre class="r"><code>extract_mets &lt;- function(article){

    id &lt;- article %&gt;%
        extractor("(?&lt;=ID)(.*?)(?=LABEL)")

    label &lt;- article %&gt;%
        extractor("(?&lt;=LABEL)(.*?)(?=TYPE)")

    type &lt;- article %&gt;%
        extractor("(?&lt;=TYPE)(.*?)(?=&gt;)")

    begins &lt;- article %&gt;%
        extractor("(?&lt;=BEGIN)(.*?)(?=BETYPE)", all = TRUE)

    tibble::tribble(~label, ~type, ~begins, ~id,
                    label, type, begins, id) %&gt;%
        unnest()
}</code></pre>
<p>
This function uses complex regular expressions to extract the strings I need, and then puts the result into a data frame, with the <code>tibble()</code> function. I then use <code>unnest()</code>, because <code>label</code>, <code>type</code>, <code>begins</code> and <code>id</code> are not the same length. <code>label</code>, <code>type</code> and <code>id</code> are of length 1, while <code>begins</code> is longer. This means that when I put them into a data frame it looks like this:
</p>
<pre class="r"><code>tribble(~a, ~b,
"a", rep("b", 4))</code></pre>
<pre><code>## # A tibble: 1 x 2
##   a     b        
##   &lt;chr&gt; &lt;list&gt;   
## 1 a     &lt;chr [4]&gt;</code></pre>
<p>
With <code>unnest()</code>, I get a nice data frame:
</p>
<pre class="r"><code>tribble(~a, ~b,
"a", rep("b", 4)) %&gt;% 
  unnest()</code></pre>
<pre><code>## # A tibble: 4 x 2
##   a     b    
##   &lt;chr&gt; &lt;chr&gt;
## 1 a     b    
## 2 a     b    
## 3 a     b    
## 4 a     b</code></pre>
<p>
Now, I simply need to map this function to all the files and that’s it! For this, I will write yet another helper function:
</p>
<pre class="r"><code>mets_csv &lt;- function(page_path){
    
    page &lt;- read_file(page_path)
    
    doc_name &lt;- str_extract(page_path, "(?&lt;=/).*")
    
    mets_articles &lt;- page %&gt;%
        str_split("DMDID") %&gt;%
        flatten_chr()
    
    mets_df &lt;- map_df(mets_articles, extract_mets)
    
    mets_df &lt;- mets_df %&gt;%
        mutate(document = doc_name)
    
    write_csv(mets_df, paste0(page_path, ".csv"))
}</code></pre>
<p>
This function takes the path to a METS file as input, and processes it using the steps I explained above. The only difference is that I add a column containing the name of the file that was processed, and write the resulting data frame directly to disk as a data frame. Finally, I can map this function to all the METS files:
</p>
<pre class="r"><code># Extract content from METS files

pages_mets &lt;- str_match(list.files(path = "./", all.files = TRUE, recursive = TRUE), ".*mets.xml") %&gt;%
    discard(is.na)

library(furrr)

plan(multiprocess, workers = 8)

tic &lt;- Sys.time()
future_map(pages_mets, mets_csv)
toc &lt;- Sys.time()

toc - tic</code></pre>
<p>
I use <code>{furrr}</code> to extract the data from all the files in parallel, by putting 8 cores of my CPU to work. This took around 3 minutes and 20 seconds to finish.
</p>
<p>
That’s it for now, stay tuned for part 2 where I will analyze this fresh data!
</p>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">
Appendix
</h2>
<pre class="r"><code>extract_alto &lt;- function(article){
    begins &lt;- article[1] %&gt;%
        extractor("(?&lt;=^ID)(.*?)(?=HPOS)", all = TRUE)

    content &lt;- article %&gt;%
        extractor("(?&lt;=CONTENT)(.*?)(?=WC)", all = TRUE)

    tibble::tribble(~begins, ~content,
                    begins, content) %&gt;%
        unnest()
}

alto_csv &lt;- function(page_path){

    page &lt;- read_file(page_path)

    doc_name &lt;- str_extract(page_path, "(?&lt;=/text/).*")

    alto_articles &lt;- page %&gt;%
        str_split("TextBlock ") %&gt;%
        flatten_chr()

    alto_df &lt;- map_df(alto_articles, extract_alto)

    alto_df &lt;- alto_df %&gt;%
        mutate(document = doc_name)

    write_csv(alto_df, paste0(page_path, ".csv"))
}


alto &lt;- read_file("1533660_newspaper_lunion_1860-11-14/text/1860-11-14_01-00001.xml")


# Extract content from alto files

pages_alto &lt;- str_match(list.files(path = "./", all.files = TRUE, recursive = TRUE), ".*/text/.*.xml") %&gt;%
    discard(is.na)


library(furrr)

plan(multiprocess, workers = 8)

tic &lt;- Sys.time()
future_map(pages_alto, alto_csv)
toc &lt;- Sys.time()

toc - tic

#Time difference of 18.64776 mins</code></pre>
</section>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-01-13-newspapers_mets_alto.html</guid>
  <pubDate>Sun, 13 Jan 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Looking into 19th century ads from a Luxembourguish newspaper with R</title>
  <link>https://b-rodrigues.github.io/posts/2019-01-04-newspapers.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=0xzN6FM5x_E"> <img src="https://b-rodrigues.github.io/assets/img/Wales.jpg" title="Sometimes ads are better than this. Especially if it's Flex Tape ® ads."></a>
</p>
</div>
<p>
The <a href="https://data.bnl.lu/data/historical-newspapers/">national library of Luxembourg</a> published some very interesting data sets; scans of historical newspapers! There are several data sets that you can download, from 250mb up to 257gb. I decided to take a look at the 32gb “ML Starter Pack”. It contains high quality scans of one year of the <em>L’indépendence Luxembourgeoise</em> (Luxembourguish independence) from the year 1877. To make life easier to data scientists, the national library also included ALTO and METS files (which is a XML schema that is used to describe the layout and contents of physical text sources, such as pages of a book or newspaper) which can be easily parsed by R.
</p>
<p>
<em>L’indépendence Luxembourgeoise</em> is quite interesting in that it is a Luxembourguish newspaper written in French. Luxembourg always had 3 languages that were used in different situations, French, German and Luxembourguish. Luxembourguish is the language people used (and still use) for day to day life and to speak to their baker. Historically however, it was not used for the press or in politics. Instead it was German that was used for the press (or so I thought) and French in politics (only in <a href="http://legilux.public.lu/eli/etat/leg/loi/1984/02/24/n1/jo">1984</a> was Luxembourguish made an official Language of Luxembourg). It turns out however that <em>L’indépendence Luxembourgeoise</em>, a daily newspaper that does not exist anymore, was in French. This piqued my interest, and it also made analysis easier, for 2 reasons: I first started with the <em>Luxemburger Wort</em> (Luxembourg’s Word I guess would be a translation), which still exists today, but which is in German. And at that time, German was written using the Fraktur font, which makes it barely readable. Look at the alphabet in Fraktur:
</p>
<pre><code>𝕬 𝕭 𝕮 𝕯 𝕰 𝕱 𝕲 𝕳 𝕴 𝕵 𝕶 𝕷 𝕸 𝕹 𝕺 𝕻 𝕼 𝕽 𝕾 𝕿 𝖀 𝖁 𝖂 𝖃 𝖄 𝖅
𝖆 𝖇 𝖈 𝖉 𝖊 𝖋 𝖌 𝖍 𝖎 𝖏 𝖐 𝖑 𝖒 𝖓 𝖔 𝖕 𝖖 𝖗 𝖘 𝖙 𝖚 𝖛 𝖜 𝖝 𝖞 𝖟</code></pre>
<p>
It’s not like German is already hard enough, they had to invent the least readable font ever to write German in, to make extra sure it would be hell to decipher.
</p>
<p>
So basically I couldn’t be bothered to try to read a German newspaper in Fraktur. That’s when I noticed the <em>L’indépendence Luxembourgeoise</em>… A Luxembourguish newspaper? Written in French? Sounds interesting.
</p>
<p>
And oh boy. Interesting it was.
</p>
<p>
19th century newspapers articles were something else. There’s this article for instance:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/pray for senators.png"><!-- -->
</p>
<p>
For those of you that do not read French, this article relates that in France, the ministry of justice required priests to include prayers on the Sunday that follows the start of the new season of parliamentary discussions, in order for God to provide senators his help.
</p>
<p>
There this gem too:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/tallest_soldier.jpg"><!-- -->
</p>
<p>
This article presents the tallest soldier of the German army, called Emhke, and nominated by the German Emperor himself to accompany him during his visit to Palestine. Emhke was 2.08 meters tall and weighted 236 pounds (apparently at the time Luxembourg was not fully sold on the metric system).
</p>
<p>
Anyway, I decided to take a look at ads. The last paper of this 4 page newspaper always contained ads and other announcements. For example, there’s this ad for a pharmacy:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/pharmacy.png"><!-- -->
</p>
<p>
that sells tea, and mineral water. Yes, tea and mineral water. In a pharmacy. Or this one:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/upside_down.png"><!-- -->
</p>
<p>
which is literally upside down in the newspaper (the one from the 10th of April 1877). I don’t know if it’s a mistake or if it’s a marketing ploy, but it did catch my attention, 140 years later, so <em>bravo</em>. This is an announcement made by a shop owner that wants to sell all his merchandise for cheap, perhaps to make space for new stuff coming in?
</p>
<p>
So I decided brush up on my natural language processing skills with R and do topic modeling on these ads. The challenge here is that a single document, the 4th page of the newspaper, contains a lot of ads. So it will probably be difficult to clearly isolate topics. But let’s try nonetheless. First of all, let’s load all the <code>.xml</code> files that contain the data. These files look like this:
</p>
<pre><code>&lt;TextLine ID="LINE6" STYLEREFS="TS11" HEIGHT="42" WIDTH="449" HPOS="165" VPOS="493"&gt;
                                    &lt;String ID="S16" CONTENT="l’après-midi," WC="0.638" CC="0803367024653" HEIGHT="42" WIDTH="208" HPOS="165" VPOS="493"/&gt;
                                    &lt;SP ID="SP11" WIDTH="24" HPOS="373" VPOS="493"/&gt;
                                    &lt;String ID="S17" CONTENT="le" WC="0.8" CC="40" HEIGHT="30" WIDTH="29" HPOS="397" VPOS="497"/&gt;
                                    &lt;SP ID="SP12" WIDTH="14" HPOS="426" VPOS="497"/&gt;
                                    &lt;String ID="S18" CONTENT="Gouverne" WC="0.638" CC="72370460" HEIGHT="31" WIDTH="161" HPOS="440" VPOS="496" SUBS_TYPE="HypPart1" SUBS_CONTENT="Gouvernement"/&gt;
                                    &lt;HYP CONTENT="-" WIDTH="11" HPOS="603" VPOS="514"/&gt;
                                  &lt;/TextLine&gt;
                        &lt;TextLine ID="LINE7" STYLEREFS="TS11" HEIGHT="41" WIDTH="449" HPOS="166" VPOS="541"&gt;
                                    &lt;String ID="S19" CONTENT="ment" WC="0.725" CC="0074" HEIGHT="26" WIDTH="81" HPOS="166" VPOS="545" SUBS_TYPE="HypPart2" SUBS_CONTENT="Gouvernement"/&gt;
                                    &lt;SP ID="SP13" WIDTH="24" HPOS="247" VPOS="545"/&gt;
                                    &lt;String ID="S20" CONTENT="Royal" WC="0.62" CC="74503" HEIGHT="41" WIDTH="100" HPOS="271" VPOS="541"/&gt;
                                    &lt;SP ID="SP14" WIDTH="26" HPOS="371" VPOS="541"/&gt;
                                    &lt;String ID="S21" CONTENT="Grand-Ducal" WC="0.682" CC="75260334005" HEIGHT="32" WIDTH="218" HPOS="397" VPOS="541"/&gt;
                                  &lt;/TextLine&gt;</code></pre>
<p>
I’m interested in the “CONTENT” tag, which contains the words. Let’s first get that into R.
</p>
<p>
Load the packages, and the files:
</p>
<pre class="r"><code>library(tidyverse)
library(tidytext)
library(topicmodels)
library(brotools)

ad_pages &lt;- str_match(list.files(path = "./", all.files = TRUE, recursive = TRUE), ".*4-alto.xml") %&gt;%
    discard(is.na)</code></pre>
<p>
I save the path of all the pages at once into the <code>ad_pages</code> variables. To understand how and why this works, you must take a look at the hierarchy of the folder:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/layout.png"><!-- -->
</p>
<p>
Inside each of these folder, there is a <code>text</code> folder, and inside this folder there are the <code>.xml</code> files. Because this structure is bit complex, I use the <code>list.files()</code> function with the <code>all.files</code> and <code>recursive</code> argument set to <code>TRUE</code> which allow me to dig deep into the folder structure and list every single file. I am only interested into the 4th page though, so that’s why I use <code>str_match()</code> to only keep the 4th page using the <code>“.*4-alto.xml”</code> regular expression. This is the right regular expression, because the files are named like so:
</p>
<pre><code>1877-12-29_01-00004-alto.xml</code></pre>
<p>
So in the end, <code>ad_pages</code> is a list of all the paths to these files. I then write a function to extract the contents of the “CONTENT” tag. Here is the function.
</p>
<pre class="r"><code>get_words &lt;- function(page_path){
    
    page &lt;- read_file(page_path)
    
    page_name &lt;- str_extract(page_path, "1.*(?=-0000)") 
    
    page %&gt;%  
        str_split("\n", simplify = TRUE) %&gt;% 
        keep(str_detect(., "CONTENT")) %&gt;% 
        str_extract("(?&lt;=CONTENT)(.*?)(?=WC)") %&gt;% 
        discard(is.na) %&gt;% 
        str_extract("[:alpha:]+") %&gt;% 
        tolower %&gt;% 
        as_tibble %&gt;% 
        rename(tokens = value) %&gt;% 
        mutate(page = page_name)
}</code></pre>
<p>
This function takes the path to a page as argument, and returns a tibble with the two columns: one containing the words, which I called <code>tokens</code> and the second the name of the document this word was found. I uploaded on <code>.xml</code> file <a href="https://gist.github.com/b-rodrigues/a22d2aa63dff01d88acc2916c003489d">here</a> so that you can try the function yourself. The difficult part is <code>str_extract(“(?&lt;=CONTENT)(.*?)(?=WC)“)</code> which is were the words inside the “CONTENT” tag get extracted.
</p>
<p>
I then map this function to all the pages, and get a nice tibble with all the words:
</p>
<pre class="r"><code>ad_words &lt;- map_dfr(ad_pages, get_words)</code></pre>
<pre class="r"><code>ad_words</code></pre>
<pre><code>## # A tibble: 1,114,662 x 2
##    tokens     page                            
##    &lt;chr&gt;      &lt;chr&gt;                           
##  1 afin       1877-01-05_01/text/1877-01-05_01
##  2 de         1877-01-05_01/text/1877-01-05_01
##  3 mettre     1877-01-05_01/text/1877-01-05_01
##  4 mes        1877-01-05_01/text/1877-01-05_01
##  5 honorables 1877-01-05_01/text/1877-01-05_01
##  6 clients    1877-01-05_01/text/1877-01-05_01
##  7 à          1877-01-05_01/text/1877-01-05_01
##  8 même       1877-01-05_01/text/1877-01-05_01
##  9 d          1877-01-05_01/text/1877-01-05_01
## 10 avantages  1877-01-05_01/text/1877-01-05_01
## # … with 1,114,652 more rows</code></pre>
<p>
I then do some further cleaning, removing stop words (French and German, because there are some ads in German) and a bunch of garbage characters and words, which are probably when the OCR failed. I also remove some German words from the few German ads that are in the paper, because they have a very high tf-idf (I’ll explain below what that is). I also remove very common words in ads that were just like stopwords. Every ad of a shop mentioned their clients with <em>honorable clientèle</em>, or used the word <em>vente</em>, and so on. This is what you see below in the very long calls to <code>str_remove_all</code>. I also compute the <code>tf_idf</code> and I am grateful to ThinkR blog post on that, which you can read <a href="https://thinkr.fr/text-mining-et-topic-modeling-avec-r/">here</a>. It’s in French though, but the idea of the blog post is to present topic modeling with Wikipedia articles. You can also read the section on tf-idf from the Text Mining with R ebook, <a href="https://www.tidytextmining.com/tfidf.html">here</a>. tf-idf gives a measure of how common words are. Very common words, like stopwords, have a tf-idf of 0. So I use this to further remove very common words, by only keeping words with a tf-idf greater than 0.01. This is why I manually remove garbage words and German words below, because they are so uncommon that they have a very high tf-idf and mess up the rest of the analysis. To find these words I had to go back and forth between the tibble of cleaned words and my code, and manually add all these exceptions. It took some time, but definitely made the results of the next steps better.<br> I then use <code>cast_dtm</code> to cast the tibble into a DocumentTermMatrix object, which is needed for the <code>LDA()</code> function that does the topic modeling:
</p>
<pre class="r"><code>stopwords_fr &lt;- read_csv("https://raw.githubusercontent.com/stopwords-iso/stopwords-fr/master/stopwords-fr.txt",
                         col_names = FALSE)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   X1 = col_character()
## )</code></pre>
<pre class="r"><code>stopwords_de &lt;- read_csv("https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/stopwords-de.txt",
                         col_names = FALSE)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   X1 = col_character()
## )</code></pre>
<pre><code>## Warning: 1 parsing failure.
## row col  expected    actual                                                                                   file
## 157  -- 1 columns 2 columns 'https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/stopwords-de.txt'</code></pre>
<pre class="r"><code>ad_words2 &lt;- ad_words %&gt;% 
    filter(!is.na(tokens)) %&gt;% 
    mutate(tokens = str_remove_all(tokens, 
                                   '[|\\|!|"|#|$|%|&amp;|\\*|+|,|-|.|/|:|;|&lt;|=|&gt;|?|@|^|_|`|’|\'|‘|(|)|\\||~|=|]|°|&lt;|&gt;|«|»|\\d{1,100}|©|®|•|—|„|“|-|¦\\\\|”')) %&gt;%
    mutate(tokens = str_remove_all(tokens,
                                   "j'|j’|m’|m'|n’|n'|c’|c'|qu’|qu'|s’|s'|t’|t'|l’|l'|d’|d'|luxembourg|honneur|rue|prix|maison|frs|ber|adresser|unb|mois|vente|informer|sann|neben|rbudj|artringen|salz|eingetragen|ort|ftofjenb|groifdjen|ort|boch|chem|jahrgang|uoa|genannt|neuwahl|wechsel|sittroe|yerlorenkost|beichsmark|tttr|slpril|ofto|rbudj|felben|acferftücf|etr|eft|sbege|incl|estce|bes|franzosengrund|qne|nne|mme|qni|faire|id|kil")) %&gt;%
    anti_join(stopwords_de, by = c("tokens" = "X1")) %&gt;% 
    filter(!str_detect(tokens, "§")) %&gt;% 
    mutate(tokens = ifelse(tokens == "inédite", "inédit", tokens)) %&gt;% 
    filter(tokens != "") %&gt;% 
    anti_join(stopwords_fr, by = c("tokens" = "X1")) %&gt;% 
    count(page, tokens) %&gt;% 
    bind_tf_idf(tokens, page, n) %&gt;% 
    arrange(desc(tf_idf))

dtm_long &lt;- ad_words2 %&gt;% 
    filter(tf_idf &gt; 0.01) %&gt;% 
    cast_dtm(page, tokens, n)</code></pre>
<p>
To read more details on this, I suggest you take a look at the following section of the Text Mining with R ebook: <a href="https://www.tidytextmining.com/topicmodeling.html#latent-dirichlet-allocation">Latent Dirichlet Allocation</a>.
</p>
<p>
I choose to model 10 topics (<code>k = 10</code>), and set the <code>alpha</code> parameter to 5. This hyperparamater controls how many topics are present in one document. Since my ads are all in one page (one document), I increased it. Let’s fit the model, and plot the results:
</p>
<pre class="r"><code>lda_model_long &lt;- LDA(dtm_long, k = 10, control = list(alpha = 5))</code></pre>
<p>
I plot the per-topic-per-word probabilities, the “beta” from the model and plot the 5 words that contribute the most to each topic:
</p>
<pre class="r"><code>result &lt;- tidy(lda_model_long, "beta")

result %&gt;%
    group_by(topic) %&gt;%
    top_n(5, beta) %&gt;%
    ungroup() %&gt;%
    arrange(topic, -beta) %&gt;% 
    mutate(term = reorder(term, beta)) %&gt;%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    coord_flip() +
    theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/newspapers-13-1.png" width="672">
</p>
<p>
So some topics seem clear to me, other not at all. For example topic 4 seems to be about shoes made out of leather. The word <code>semelle</code>, sole, also appears. Then there’s a lot of topics that reference either music, bals, or instruments. I guess these are ads for local music festivals, or similar events. There’s also an ad for what seems to be bundles of sticks, topic 3: <code>chêne</code> is oak, <code>copeaux</code> is shavings and you know what <code>fagots</code> is. The first word <code>stère</code> which I did not know is a unit of volume equal to one cubic meter (see <a href="https://en.wikipedia.org/wiki/Stere">Wikipedia</a>). So they were likely selling bundle of oak sticks by the cubic meter. For the other topics, I either lack context or perhaps I just need to adjust <code>k</code>, the number of topics to model, and <code>alpha</code> to get better results. In the meantime, topic 1 is about shoes (<code>chaussures</code>), theatre, fuel (<code>combustible</code>) and farts (<code>pet</code>). Really wonder what they were selling in that shop.
</p>
<p>
In any case, this was quite an interesting project. I learned a lot about topic modeling and historical newspapers of my country! I do not know if I will continue exploring it myself, but I am really curious to see what others will do with it!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-01-04-newspapers.html</guid>
  <pubDate>Fri, 04 Jan 2019 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
