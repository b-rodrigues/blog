<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Econometrics and Free Software</title>
<link>https://b-rodrigues.github.io/</link>
<atom:link href="https://b-rodrigues.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.6.37</generator>
<lastBuildDate>Sat, 09 Nov 2019 00:00:00 GMT</lastBuildDate>
<item>
  <title>Intrumental variable regression and machine learning</title>
  <link>https://b-rodrigues.github.io/posts/2019-11-06-explainability_econometrics.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=-Bu654lDuBk"> <img src="https://b-rodrigues.github.io/assets/img/maybe.jpg" title="Every time I look at observational data and wonder if 
  this correlation could imply causation, at least this one time." width="80%" height="auto"> </a>
</p>
</div>
<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">
Intro
</h2>
<p>
Just like the question “what’s the difference between machine learning and statistics” has shed a lot of ink (since at least <a href="https://projecteuclid.org/euclid.ss/1009213726">Breiman (2001)</a>), the same question but where statistics is replaced by econometrics has led to a lot of discussion, as well. I like this presentation by <a href="https://web.stanford.edu/class/ee380/Abstracts/140129-slides-Machine-Learning-and-Econometrics.pdf">Hal Varian</a> from almost 6 years ago. There’s a slide called “What econometrics can learn from machine learning”, which summarises in a few bullet points <a href="https://www.aeaweb.org/articles?id=10.1257/jep.28.2.3">Varian (2014)</a> and the rest of the presentation discusses what machine learning can learn from econometrics. Varian argues that the difference between machine learning and econometrics is that machine learning focuses on prediction, while econometrics on inference and causality (and to a lesser extent prediction as well). Varian cites some methods that have been in the econometricians’ toolbox for decades (at least for some of them), such as regression discontinuity, difference in differences and instrumental variables regression. Another interesting paper is <a href="https://www.aeaweb.org/articles?id=10.1257/jep.31.2.87">Mullainathan and Spiess</a>, especially the section called <em>What Do We (Not) Learn from Machine Learning Output?</em>. The authors discuss the tempting idea of using LASSO to perform variable (feature) selection. Econometricians might be tempted to use LASSO to perform variable selection, and draw conclusions such as <em>The variable (feature) “Number of rooms” has not been selected by LASSO, thus it plays no role in the prediction of house prices</em>. However, when variables (features) are highly correlated, LASSO selects variables essentially randomly, without any meaningful impact on model performance (for prediction). I found this paragraph quite interesting (emphasis mine):
</p>
<p>
<em>This problem is ubiquitous in machine learning. The very appeal of these algorithms is that they can fit many different functions. But this creates an Achilles’ heel: more functions mean a greater chance that two functions with very different coefficients can produce similar prediction quality. As a result, how an algorithm chooses between two very different functions can effectively come down to the flip of a coin. In econometric terms, while the lack of <strong>standard errors</strong> illustrates the limitations to making inference after model selection, the challenge here is (uniform) model selection consistency itself.</em>
</p>
<p>
Assuming that we successfully dealt with model selection, we still have to content with significance of coefficients. There is recent research into this topic, such as <a href="https://arxiv.org/abs/1902.06021v1">Horel and Giesecke</a>, but I wonder to what extent explainability could help with this. I have been looking around for papers that discuss explainability in the context of the social sciences but have not found any. If any of the readers of this blog are aware of such papers, please let me know.
</p>
<p>
Just to wrap up Mullainathan and Spiess; the authors then suggest to use machine learning mainly for prediction tasks, such as using images taken using satellites to predict future harvest size (the authors cite <a href="https://www.sciencedirect.com/science/article/pii/S0378429012002754">Lobell (2013)</a>), or for tasks that have an <em>implicit</em> prediction component. For instance in the case of instrumental variables regression, two stages least squares is often used, and the first stage is a prediction task. Propensity score matching is another prediction task, where machine learning could be used. Other examples are presented as well. In this blog post, I’ll explore two stages least squares and see what happens when a random forest is used for the first step.
</p>
</section>
<section id="instrumental-variables-regression-using-two-stage-least-squares" class="level2">
<h2 class="anchored" data-anchor-id="instrumental-variables-regression-using-two-stage-least-squares">
Instrumental variables regression using two-stage least squares
</h2>
<p>
Let’s work out a textbook (literally) example of instrumental variable regression. The below example is taken from Wooldrige’s <em>Econometric analysis of cross section and panel data</em>, and is an exercise made using data from Mroz (1987) <em>The sensitivity of an empirical model of married women’s hours of work to economic and statistical assumptions</em>.
</p>
<p>
Let’s first load the needed packages and the data <code>"mroz"</code> included in the <code>{wooldridge}</code> package:
</p>
<pre class="r"><code>library(tidyverse)
library(randomForest)
library(wooldridge)
library(AER)
library(Metrics)

data("mroz")</code></pre>
<p>
Let’s only select the women that are in the labour force (<code>inlf == 1</code>), and let’s run a simple linear regression. The dependent variable, or target, is <code>lwage</code>, the logarithm of the wage, and the explanatory variables, or features are <code>exper</code>, <code>expersq</code> and <code>educ</code>. For a full description of the data, click below:
</p>
<p>
</p><details>
<summary>
Description of data
</summary>
<p></p>
<pre><code>
mroz {wooldridge}   R Documentation
mroz

Description

Wooldridge Source: T.A. Mroz (1987), “The Sensitivity of an Empirical Model of Married Women’s Hours of Work to Economic and Statistical Assumptions,” Econometrica 55, 765-799. Professor Ernst R. Berndt, of MIT, kindly provided the data, which he obtained from Professor Mroz. Data loads lazily.

Usage

data('mroz')
Format

A data.frame with 753 observations on 22 variables:

inlf: =1 if in lab frce, 1975

hours: hours worked, 1975

kidslt6: # kids &lt; 6 years

kidsge6: # kids 6-18

age: woman's age in yrs

educ: years of schooling

wage: est. wage from earn, hrs

repwage: rep. wage at interview in 1976

hushrs: hours worked by husband, 1975

husage: husband's age

huseduc: husband's years of schooling

huswage: husband's hourly wage, 1975

faminc: family income, 1975

mtr: fed. marg. tax rte facing woman

motheduc: mother's years of schooling

fatheduc: father's years of schooling

unem: unem. rate in county of resid.

city: =1 if live in SMSA

exper: actual labor mkt exper

nwifeinc: (faminc - wage*hours)/1000

lwage: log(wage)

expersq: exper^2

Used in Text

pages 249-251, 260, 294, 519-520, 530, 535, 535-536, 565-566, 578-579, 593- 595, 601-603, 619-620, 625

Source

https://www.cengage.com/cgi-wadsworth/course_products_wp.pl?fid=M20b&amp;product_isbn_issn=9781111531041
</code></pre>
<p>
</p></details>
<p></p>
<pre class="r"><code>working_w &lt;- mroz %&gt;% 
    filter(inlf == 1)

wage_lm &lt;- lm(lwage ~ exper + expersq + educ, 
              data = working_w)

summary(wage_lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = lwage ~ exper + expersq + educ, data = working_w)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.08404 -0.30627  0.04952  0.37498  2.37115 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.5220406  0.1986321  -2.628  0.00890 ** 
## exper        0.0415665  0.0131752   3.155  0.00172 ** 
## expersq     -0.0008112  0.0003932  -2.063  0.03974 *  
## educ         0.1074896  0.0141465   7.598 1.94e-13 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.6664 on 424 degrees of freedom
## Multiple R-squared:  0.1568, Adjusted R-squared:  0.1509 
## F-statistic: 26.29 on 3 and 424 DF,  p-value: 1.302e-15</code></pre>
<p>
Now, we see that education is statistically significant and the effect is quite high. The return to education is about 11%. Now, let’s add some more explanatory variables:
</p>
<pre class="r"><code>wage_lm2 &lt;- lm(lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage + city + educ, 
              data = working_w)

summary(wage_lm2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + 
##     huswage + city + educ, data = working_w)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.07431 -0.30500  0.05477  0.37871  2.31157 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.3853695  0.3163043  -1.218  0.22378    
## exper        0.0398817  0.0133651   2.984  0.00301 ** 
## expersq     -0.0007400  0.0003985  -1.857  0.06402 .  
## kidslt6     -0.0564071  0.0890759  -0.633  0.52692    
## kidsge6     -0.0143165  0.0276579  -0.518  0.60499    
## husage      -0.0028828  0.0049338  -0.584  0.55934    
## huswage      0.0177470  0.0102733   1.727  0.08482 .  
## city         0.0119960  0.0725595   0.165  0.86877    
## educ         0.0986810  0.0151589   6.510 2.16e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.6669 on 419 degrees of freedom
## Multiple R-squared:  0.1654, Adjusted R-squared:  0.1495 
## F-statistic: 10.38 on 8 and 419 DF,  p-value: 2.691e-13</code></pre>
<p>
The return to education lowers a bit, but is still significant. Now, the issue is that education is not exogenous (randomly assigned), and is thus correlated with the error term of the regression, due to an omitted variable for instance contained in the error term, that is correlated with education (for example work ethic).
</p>
<p>
To deal with this, econometricians use instrumental variables (IV) regression. I won’t go into detail here; just know that this method can deal with these types of issues. The <a href="https://en.wikipedia.org/wiki/Instrumental_variables_estimation">Wikipedia</a> page gives a good intro on what this is all about. This short <a href="https://wol.iza.org/uploads/articles/250/pdfs/using-instrumental-variables-to-establish-causality.pdf">paper</a> is also quite interesting in introducing instrumental variables.
</p>
<p>
In practice, IV is done in two steps. First, regress the endogenous variable, in our case education, on all the explanatory variables from before, plus so called instruments. Instruments are variables that are correlated with the endogenous variable, here education, but uncorrelated to the error term. They only affect the target variable through their correlation with the endogenous variable. We will be using the education level of the parents of the women, as well as the education levels of their husbands as intruments. The assumption is that the parents’, as well as the husband’s education are exogenous in the log wage of the woman. This assumption can of course be challenged, but let’s say that it holds.
</p>
<p>
To conclude stage 1, we obtain the predictions of education:
</p>
<pre class="r"><code>first_stage &lt;- lm(educ ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage 
                  + city + motheduc +  fatheduc + huseduc, data = working_w)

working_w$predictions_first_stage &lt;- predict(first_stage)</code></pre>
<p>
We are now ready for the second stage. In the regression from before:
</p>
<pre><code>wage_lm2 &lt;- lm(lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage + city + educ, 
              data = working_w)</code></pre>
<p>
we now replace <code>educ</code> with the predictions of stage 1:
</p>
<pre class="r"><code>second_stage &lt;- lm(lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage 
                   + city + predictions_first_stage,
                  data = working_w)

summary(second_stage)</code></pre>
<pre><code>## 
## Call:
## lm(formula = lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + 
##     huswage + city + predictions_first_stage, data = working_w)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.13493 -0.30004  0.03046  0.37142  2.27199 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)              0.1763588  0.4206911   0.419   0.6753   
## exper                    0.0419047  0.0139885   2.996   0.0029 **
## expersq                 -0.0007881  0.0004167  -1.891   0.0593 . 
## kidslt6                 -0.0255934  0.0941128  -0.272   0.7858   
## kidsge6                 -0.0234422  0.0291914  -0.803   0.4224   
## husage                  -0.0042628  0.0051919  -0.821   0.4121   
## huswage                  0.0263802  0.0114511   2.304   0.0217 * 
## city                     0.0215685  0.0759034   0.284   0.7764   
## predictions_first_stage  0.0531993  0.0263735   2.017   0.0443 * 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.6965 on 419 degrees of freedom
## Multiple R-squared:  0.08988,    Adjusted R-squared:  0.0725 
## F-statistic: 5.172 on 8 and 419 DF,  p-value: 3.581e-06</code></pre>
<p>
We see that education, now instrumented by the parents’ and the husband’s education is still significant, but the effect is much lower. The return to education is now about 5%. However, should our assumption hold, this effect is now <em>causal</em>. However there are some caveats. The IV estimate is a local average treatment effect, meaning that we only get the effect on those individuals that were affected by the treatment. In this case, it would mean that the effect we recovered is only for women who were not planning on, say, studying, but only did so under the influence of their parents (or vice-versa).
</p>
<p>
IV regression can also be achieved using the <code>ivreg()</code> function from the <code>{AER}</code> package:
</p>
<pre class="r"><code>inst_reg &lt;- ivreg(lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage + city + educ 
                  | .-educ + motheduc + fatheduc + huseduc,
                  data = working_w)

summary(inst_reg)</code></pre>
<pre><code>## 
## Call:
## ivreg(formula = lwage ~ exper + expersq + kidslt6 + kidsge6 + 
##     husage + huswage + city + educ | . - educ + motheduc + fatheduc + 
##     huseduc, data = working_w)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.10175 -0.30407  0.03379  0.35255  2.25107 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)  0.1763588  0.4071522   0.433   0.6651   
## exper        0.0419047  0.0135384   3.095   0.0021 **
## expersq     -0.0007881  0.0004033  -1.954   0.0514 . 
## kidslt6     -0.0255934  0.0910840  -0.281   0.7789   
## kidsge6     -0.0234422  0.0282519  -0.830   0.4071   
## husage      -0.0042628  0.0050249  -0.848   0.3967   
## huswage      0.0263802  0.0110826   2.380   0.0177 * 
## city         0.0215685  0.0734606   0.294   0.7692   
## educ         0.0531993  0.0255247   2.084   0.0377 * 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.6741 on 419 degrees of freedom
## Multiple R-Squared: 0.1475,  Adjusted R-squared: 0.1312 
## Wald test: 5.522 on 8 and 419 DF,  p-value: 1.191e-06</code></pre>
<p>
Ok, great, now let’s see how a machine learning practitioner who took an econometrics MOOC might tackle the issue. The first step will be to split the data into training and testing sets:
</p>
<pre class="r"><code>set.seed(42)
sample &lt;- sample.int(n = nrow(working_w), size = floor(.90*nrow(working_w)), replace = F)
train &lt;- working_w[sample, ]
test  &lt;- working_w[-sample, ]</code></pre>
<p>
Let’s now run the same analysis as above, but let’s compute the RMSE of the first stage regression on the testing data as well:
</p>
<pre class="r"><code>first_stage &lt;- lm(educ ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage 
                  + city + motheduc +  fatheduc + huseduc, data = train)

test$predictions_first_stage &lt;- predict(first_stage, newdata = test)

lm_rmse &lt;- rmse(predicted = test$predictions_first_stage, actual = test$educ)

train$predictions_first_stage &lt;- predict(first_stage)</code></pre>
<p>
The first stage is done, let’s go with the second stage:
</p>
<pre class="r"><code>second_stage &lt;- lm(lwage ~ exper + expersq + kidslt6 + kidsge6 + 
                       husage + huswage + city + predictions_first_stage,
                  data = train)

summary(second_stage)</code></pre>
<pre><code>## 
## Call:
## lm(formula = lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + 
##     huswage + city + predictions_first_stage, data = train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.09828 -0.28606  0.05248  0.37258  2.29947 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)             -0.0037711  0.4489252  -0.008  0.99330   
## exper                    0.0449370  0.0145632   3.086  0.00218 **
## expersq                 -0.0008394  0.0004344  -1.933  0.05404 . 
## kidslt6                 -0.0630522  0.0963953  -0.654  0.51345   
## kidsge6                 -0.0197164  0.0306834  -0.643  0.52089   
## husage                  -0.0034744  0.0054358  -0.639  0.52310   
## huswage                  0.0219622  0.0118602   1.852  0.06484 . 
## city                     0.0679668  0.0804317   0.845  0.39863   
## predictions_first_stage  0.0618777  0.0283253   2.185  0.02954 * 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.6952 on 376 degrees of freedom
## Multiple R-squared:  0.1035, Adjusted R-squared:  0.08438 
## F-statistic: 5.424 on 8 and 376 DF,  p-value: 1.764e-06</code></pre>
<p>
The coefficients here are a bit different due to the splitting, but that’s not an issue. Ok, great, but our machine learning engineer is in love with random forests, so he wants to use a random forest for the prediction task of the first stage:
</p>
<pre class="r"><code>library(randomForest)

first_stage_rf &lt;- randomForest(educ ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage 
                  + city + motheduc +  fatheduc + huseduc, 
                               data = train)

test$predictions_first_stage_rf &lt;- predict(first_stage_rf, newdata = test)

rf_rmse &lt;- rmse(predicted = test$predictions_first_stage_rf, actual = test$educ)

train$predictions_first_stage_rf &lt;- predict(first_stage_rf)</code></pre>
<p>
Let’s compare the RMSE’s of the two first stages. The RMSE of the first stage using linear regression was 2.0558723 and for the random forest 2.0000417. Our machine learning engineer is happy, because the random forest has better performance. Let’s now use the predictions for the second stage:
</p>
<pre class="r"><code>second_stage_rf_lm &lt;- lm(lwage ~ exper + expersq + kidslt6 + kidsge6 + 
                             husage + huswage + city + predictions_first_stage_rf,
                  data = train)

summary(second_stage_rf_lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + 
##     huswage + city + predictions_first_stage_rf, data = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0655 -0.3198  0.0376  0.3710  2.3277 
## 
## Coefficients:
##                              Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)                -0.0416945  0.4824998  -0.086  0.93118   
## exper                       0.0460311  0.0145543   3.163  0.00169 **
## expersq                    -0.0008594  0.0004344  -1.978  0.04863 * 
## kidslt6                    -0.0420827  0.0952030  -0.442  0.65872   
## kidsge6                    -0.0211208  0.0306490  -0.689  0.49117   
## husage                     -0.0033102  0.0054660  -0.606  0.54514   
## huswage                     0.0229111  0.0118142   1.939  0.05322 . 
## city                        0.0688384  0.0805209   0.855  0.39314   
## predictions_first_stage_rf  0.0629275  0.0306877   2.051  0.04100 * 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.6957 on 376 degrees of freedom
## Multiple R-squared:  0.1021, Adjusted R-squared:  0.08302 
## F-statistic: 5.346 on 8 and 376 DF,  p-value: 2.251e-06</code></pre>
<p>
The results are pretty similar. Now, why not go a bit further and use a random forest for the second stage as well?
</p>
</section> ]]></description>
  <category>R</category>
  <category>econometrics</category>
  <guid>https://b-rodrigues.github.io/posts/2019-11-06-explainability_econometrics.html</guid>
  <pubDate>Sat, 09 Nov 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Multiple data imputation and explainability</title>
  <link>https://b-rodrigues.github.io/posts/2019-11-02-mice_exp.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://xkcd.com/303/"> <img src="https://b-rodrigues.github.io/assets/img/kermit.png" title="It is always so tempting"></a>
</p>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">
Introduction
</h2>
<p>
Imputing missing values is quite an important task, but in my experience, very often, it is performed using very simplistic approaches. The basic approach is to impute missing values for numerical features using the average of each feature, or using the mode for categorical features. There are better ways of imputing missing values, for instance by predicting the values using a regression model, or KNN. However, imputing only once is not enough, because each imputed value carries with it a certain level of uncertainty. To account for this, it is better to perform multiple imputation. This means that if you impute your dataset 10 times, you’ll end up with 10 different datasets. Then, you should perform your analysis 10 times, for instance, if training a machine learning model, you should train it on the 10 datasets (and do a train/test split for each, even potentially tune a model for each). Finally, you should pool the results of these 10 analyses.
</p>
<p>
I have met this approach in the social sciences and statistical literature in general, but very rarely in machine learning. Usually, in the social sciences, explainability is the goal of fitting statistical models to data, and the approach I described above is very well suited for this. Fit 10 (linear) regressions to each imputed dataset, and then pool the estimated coefficients/weights together. Rubin’s rule is used to pool these estimates. You can read more about this rule <a href="https://bookdown.org/mwheymans/bookmi/rubins-rules.html">here</a>. In machine learning, the task is very often prediction; in this case, you should pool the predictions. Computing the average and other statistics of the predictions seem to work just fine in practice.
</p>
<p>
However, if you are mainly interested in explainability, how should you proceed? I’ve thought a bit about it, and the answer, is “exactly the same way”… I think. What I’m sure about, is you should impute m times, run the analysis m times (which in this case will include getting explanations) and then pool. So the idea is to be able to pool explanations.
</p>
</section>
<section id="explainability-in-the-standard-case-no-missing-values" class="level2">
<h2 class="anchored" data-anchor-id="explainability-in-the-standard-case-no-missing-values">
Explainability in the “standard” case (no missing values)
</h2>
<p>
To illustrate this idea, I’ll be using the <code>{mice}</code> package for multiple imputation, <code>{h2o}</code> for the machine learning bit and<code>{iml}</code> for explainability. Note that I could have used any other machine learning package instead of <code>{h2o}</code> as <code>{iml}</code> is totally <em>package-agnostic</em>. However, I have been experimenting with <code>{h2o}</code>’s automl implementation lately, so I happened to have code on hand. Let’s start with the “standard” case where the data does not have any missing values.
</p>
<p>
First let’s load the needed packages and initialize <code>h2o</code> functions with <code>h2o.init()</code>:
</p>
<pre class="r"><code>library(tidyverse)
library(Ecdat)
library(mice)
library(h2o)
library(iml)
h2o.init()</code></pre>
<p>
I’ll be using the <code>DoctorContacts</code> data. Here’s a description:
</p>
<details>
<p>
</p><summary>
Click to view the description of the data
</summary>
<p></p>
<pre><code>DoctorContacts              package:Ecdat              R Documentation

Contacts With Medical Doctor

Description:

     a cross-section from 1977-1978

     _number of observations_ : 20186

Usage:

     data(DoctorContacts)
     
Format:

     A time serie containing :

     mdu number of outpatient visits to a medical doctor

     lc log(coinsrate+1) where coinsurance rate is 0 to 100

     idp individual deductible plan ?

     lpi log(annual participation incentive payment) or 0 if no payment

     fmde log(max(medical deductible expenditure)) if IDP=1 and MDE&gt;1
          or 0 otherw

     physlim physical limitation ?

     ndisease number of chronic diseases

     health self-rate health (excellent,good,fair,poor)

     linc log of annual family income (in \$)

     lfam log of family size

     educdec years of schooling of household head

     age exact age

     sex sex (male,female)

     child age less than 18 ?

     black is household head black ?

Source:

     Deb, P.  and P.K.  Trivedi (2002) “The Structure of Demand for
     Medical Care: Latent Class versus Two-Part Models”, _Journal of
     Health Economics_, *21*, 601-625.

References:

     Cameron, A.C.  and P.K.  Trivedi (2005) _Microeconometrics :
     methods and applications_, Cambridge, pp. 553-556 and 565.</code></pre>
</details>
<p>
The task is to predict <code>"mdu"</code>, the number of outpatient visits to an MD. Let’s prepare the data and split it into 3; a training, validation and holdout set.
</p>
<pre class="r"><code>data("DoctorContacts")

contacts &lt;- as.h2o(DoctorContacts)</code></pre>
<pre class="r"><code>splits &lt;- h2o.splitFrame(data=contacts, ratios = c(0.7, 0.2))

original_train &lt;- splits[[1]]

validation &lt;- splits[[2]]

holdout &lt;- splits[[3]]

features_names &lt;- setdiff(colnames(original_train), "mdu")</code></pre>
<p>
As you see, the ratios argument <code>c(0.7, 0.2)</code> does not add up to 1. This means that the first of the splits will have 70% of the data, the second split 20% and the final 10% will be the holdout set.
</p>
<p>
Let’s first go with a poisson regression. To obtain the same results as with R’s built-in <code>glm()</code> function, I use the options below, as per H2o’s glm <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html#faq">faq</a>.
</p>
<p>
If you read Cameron and Trivedi’s <em>Microeconometrics</em>, where this data is presented in the context of count models, you’ll see that they also fit a negative binomial model 2 to this data, as it allows for overdispersion. Here, I’ll stick to a simple poisson regression, simply because the goal of this blog post is not to get the best model; as explained in the beginning, this is an attempt at pooling explanations when doing multiple imputation (and it’s also because GBMs, which I use below, do not handle the negative binomial model).
</p>
<pre class="r"><code>glm_model &lt;- h2o.glm(y = "mdu", x = features_names,
                     training_frame = original_train,
                     validation_frame = validation,
                     compute_p_values = TRUE,
                     solver = "IRLSM",
                     lambda = 0,
                     remove_collinear_columns = TRUE,
                     score_each_iteration = TRUE,
                     family = "poisson", 
                     link = "log")</code></pre>
<p>
Now that I have this simple model, which returns the (almost) same results as R’s <code>glm()</code> function, I can take a look at coefficients and see which are important, because GLMs are easily interpretable:
</p>
<details>
<p>
</p><summary>
Click to view <code>h2o.glm()</code>’s output
</summary>
<p></p>
<pre class="r"><code>summary(glm_model)</code></pre>
<pre><code>## Model Details:
## ==============
## 
## H2ORegressionModel: glm
## Model Key:  GLM_model_R_1572735931328_5 
## GLM Model: summary
##    family link regularization number_of_predictors_total
## 1 poisson  log           None                         16
##   number_of_active_predictors number_of_iterations  training_frame
## 1                          16                    5 RTMP_sid_8588_3
## 
## H2ORegressionMetrics: glm
## ** Reported on training data. **
## 
## MSE:  17.6446
## RMSE:  4.200547
## MAE:  2.504063
## RMSLE:  0.8359751
## Mean Residual Deviance :  3.88367
## R^2 :  0.1006768
## Null Deviance :64161.44
## Null D.o.F. :14131
## Residual Deviance :54884.02
## Residual D.o.F. :14115
## AIC :83474.52
## 
## 
## H2ORegressionMetrics: glm
## ** Reported on validation data. **
## 
## MSE:  20.85941
## RMSE:  4.56721
## MAE:  2.574582
## RMSLE:  0.8403465
## Mean Residual Deviance :  4.153042
## R^2 :  0.09933874
## Null Deviance :19667.55
## Null D.o.F. :4078
## Residual Deviance :16940.26
## Residual D.o.F. :4062
## AIC :25273.25
## 
## 
## 
## 
## Scoring History: 
##             timestamp   duration iterations negative_log_likelihood
## 1 2019-11-03 00:33:46  0.000 sec          0             64161.43611
## 2 2019-11-03 00:33:46  0.004 sec          1             56464.99004
## 3 2019-11-03 00:33:46  0.020 sec          2             54935.05581
## 4 2019-11-03 00:33:47  0.032 sec          3             54884.19756
## 5 2019-11-03 00:33:47  0.047 sec          4             54884.02255
## 6 2019-11-03 00:33:47  0.063 sec          5             54884.02255
##   objective
## 1   4.54015
## 2   3.99554
## 3   3.88728
## 4   3.88368
## 5   3.88367
## 6   3.88367
## 
## Variable Importances: (Extract with `h2o.varimp`) 
## =================================================
## 
##        variable relative_importance scaled_importance  percentage
## 1    black.TRUE          0.67756097        1.00000000 0.236627982
## 2   health.poor          0.48287163        0.71266152 0.168635657
## 3  physlim.TRUE          0.33962316        0.50124369 0.118608283
## 4   health.fair          0.25602066        0.37785627 0.089411366
## 5      sex.male          0.19542639        0.28842628 0.068249730
## 6      ndisease          0.16661902        0.24591001 0.058189190
## 7      idp.TRUE          0.15703578        0.23176627 0.054842384
## 8    child.TRUE          0.09988003        0.14741114 0.034881600
## 9          linc          0.09830075        0.14508030 0.034330059
## 10           lc          0.08126160        0.11993253 0.028379394
## 11         lfam          0.07234463        0.10677213 0.025265273
## 12         fmde          0.06622332        0.09773781 0.023127501
## 13      educdec          0.06416087        0.09469387 0.022407220
## 14  health.good          0.05501613        0.08119732 0.019213558
## 15          age          0.03167598        0.04675000 0.011062359
## 16          lpi          0.01938077        0.02860373 0.006768444</code></pre>
</details>
<p>
As a bonus, let’s see the output of the <code>glm()</code> function:
</p>
<details>
<p>
</p><summary>
Click to view <code>glm()</code>’s output
</summary>
<p></p>
<pre class="r"><code>train_tibble &lt;- as_tibble(original_train)

r_glm &lt;- glm(mdu ~ ., data = train_tibble,
            family = poisson(link = "log"))

summary(r_glm)</code></pre>
<pre><code>## 
## Call:
## glm(formula = mdu ~ ., family = poisson(link = "log"), data = train_tibble)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -5.7039  -1.7890  -0.8433   0.4816  18.4703  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.0005100  0.0585681   0.009   0.9931    
## lc          -0.0475077  0.0072280  -6.573 4.94e-11 ***
## idpTRUE     -0.1794563  0.0139749 -12.841  &lt; 2e-16 ***
## lpi          0.0129742  0.0022141   5.860 4.63e-09 ***
## fmde        -0.0166968  0.0042265  -3.951 7.80e-05 ***
## physlimTRUE  0.3182780  0.0126868  25.087  &lt; 2e-16 ***
## ndisease     0.0222300  0.0007215  30.811  &lt; 2e-16 ***
## healthfair   0.2434235  0.0192873  12.621  &lt; 2e-16 ***
## healthgood   0.0231824  0.0115398   2.009   0.0445 *  
## healthpoor   0.4608598  0.0329124  14.003  &lt; 2e-16 ***
## linc         0.0826053  0.0062208  13.279  &lt; 2e-16 ***
## lfam        -0.1194981  0.0106904 -11.178  &lt; 2e-16 ***
## educdec      0.0205582  0.0019404  10.595  &lt; 2e-16 ***
## age          0.0041397  0.0005152   8.035 9.39e-16 ***
## sexmale     -0.2096761  0.0104668 -20.032  &lt; 2e-16 ***
## childTRUE    0.1529588  0.0179179   8.537  &lt; 2e-16 ***
## blackTRUE   -0.6231230  0.0176758 -35.253  &lt; 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 64043  on 14096  degrees of freedom
## Residual deviance: 55529  on 14080  degrees of freedom
## AIC: 84052
## 
## Number of Fisher Scoring iterations: 6</code></pre>
</details>
<p>
I could also use the excellent <code>{ggeffects}</code> package to see the marginal effects of different variables, for instance <code>"linc"</code>:
</p>
<pre class="r"><code>library(ggeffects)

ggeffect(r_glm, "linc") %&gt;% 
    ggplot(aes(x, predicted)) +
    geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = "#0f4150") +
    geom_line(colour = "#82518c") +
    brotools::theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/mice_exp-10-1.png" width="672">
</p>
<p>
We can see that as “linc” (and other covariates are held constant), the target variable increases.
</p>
<p>
Let’s also take a look at the marginal effect of a categorical variable, namely <code>"sex"</code>:
</p>
<details>
<p>
</p><summary>
Click to view another example of marginal effects
</summary>
<p></p>
<pre class="r"><code>library(ggeffects)

ggeffect(r_glm, "sex") %&gt;% 
    ggplot(aes(x, predicted)) +
    geom_point(colour = "#82518c") +
    geom_errorbar(aes(x, ymin = conf.low, ymax = conf.high), colour = "#82518c") +
    brotools::theme_blog()</code></pre>
<img src="https://b-rodrigues.github.io/assets/img/mice_exp-11-1.png" width="672">
</details>
<p>
In the case of the <code>"sex"</code> variable, men have significantly less doctor contacts than women.
</p>
<p>
Now, let’s suppose that I&nbsp;want to train a model with a more complicated name, in order to justify my salary. Suppose I go with one of those nifty <em>black-box</em> models, for instance a GBM, which very likely will perform better than the GLM from before. GBMs are available in <code>{h2o}</code> via the <code>h2o.gbm()</code> function:
</p>
<pre class="r"><code>gbm_model &lt;- h2o.gbm(y = "mdu", x = features_names,
            training_frame = original_train,
            validation_frame = validation,
            distribution = "poisson",
            score_each_iteration = TRUE,
            ntrees = 110,
            max_depth = 20,
            sample_rate = 0.6,
            col_sample_rate = 0.8,
            col_sample_rate_per_tree = 0.9,
            learn_rate = 0.05)</code></pre>
<p>
To find a set of good hyper-parameter values, I actually used <code>h2o.automl()</code> and then used the returned parameter values from the leader model. Maybe I’ll write another blog post about <code>h2o.automl()</code> one day, it’s quite cool. Anyways, now, how do I get me some explainability out of this? The model does perform better than the GLM as indicated by all the different metrics, but now I cannot compute any marginal effects, or anything like that. I do get feature importance by default with:
</p>
<pre class="r"><code>h2o.varimp(gbm_model)</code></pre>
<pre><code>## Variable Importances: 
##    variable relative_importance scaled_importance percentage
## 1       age       380350.093750          1.000000   0.214908
## 2      linc       282274.343750          0.742143   0.159492
## 3  ndisease       245862.718750          0.646412   0.138919
## 4       lpi       173552.734375          0.456297   0.098062
## 5   educdec       148186.265625          0.389605   0.083729
## 6      lfam       139174.312500          0.365911   0.078637
## 7      fmde        94193.585938          0.247650   0.053222
## 8    health        86160.679688          0.226530   0.048683
## 9       sex        63502.667969          0.166958   0.035881
## 10       lc        50674.968750          0.133232   0.028633
## 11  physlim        45328.382812          0.119175   0.025612
## 12    black        26376.841797          0.069349   0.014904
## 13      idp        24809.185547          0.065227   0.014018
## 14    child         9382.916992          0.024669   0.005302</code></pre>
<p>
but that’s it. And had I chosen a different “black-box” model, not based on trees, then I would not even have that. Thankfully, there’s the amazing <code>{iml}</code> package that contains a lot of functions for model-agnostic explanations. If you are not familiar with this package and the methods it implements, I highly encourage you to read the free online <a href="https://christophm.github.io/interpretable-ml-book/">ebook</a> written by the packages author, Christoph Molnar (who you can follow on <a href="https://twitter.com/ChristophMolnar">Twitter</a>).
</p>
<p>
Out of the box, <code>{iml}</code> works with several machine learning frameworks, such as <code>{caret}</code> or <code>{mlr}</code> but not with <code>{h2o}</code>. However, this is not an issue; you only need to create a predict function which returns a data frame (<code>h2o.predict()</code> used for prediction with h2o models returns an h2o frame). I have found this interesting blog post from <a href="https://www.business-science.io/business/2018/08/13/iml-model-interpretability.html">business-science.io</a> which explains how to do this. I highly recommend you read this blog post, as it goes much deeper into the capabilities of <code>{iml}</code>.
</p>
<p>
So let’s write a predict function that <code>{iml}</code> can use:
</p>
<pre class="r"><code>#source: https://www.business-science.io/business/2018/08/13/iml-model-interpretability.html
predict_for_iml &lt;- function(model, newdata){
  as_tibble(h2o.predict(model, as.h2o(newdata)))
}</code></pre>
<p>
And let’s now create a <code>Predictor</code> object. These objects are used by <code>{iml}</code> to create explanations:
</p>
<pre class="r"><code>just_features &lt;- as_tibble(holdout[, 2:15])
actual_target &lt;- as_tibble(holdout[, 1])

predictor_original &lt;- Predictor$new(
  model = gbm_model, 
  data = just_features, 
  y = actual_target, 
  predict.fun = predict_for_iml
  )</code></pre>
<p>
<code>predictor_original</code> can now be used to compute all kinds of explanations. I won’t go into much detail here, as this blog post is already quite long (and I haven’t even reached what I actually want to write about yet) and you can read more on the before-mentioned blog post or directly from Christoph Molnar’s ebook linked above.
</p>
<p>
First, let’s compute a partial dependence plot, which shows the marginal effect of a variable on the outcome. This is to compare it to the one from the GLM model:
</p>
<pre class="r"><code>feature_effect_original &lt;- FeatureEffect$new(predictor_original, "linc", method = "pdp")

plot(feature_effect_original) +
    brotools::theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/mice_exp-19-1.png" width="672">
</p>
<pre class="r"><code>feature_effect_original &lt;- FeatureEffect$new(predictor_original, "linc", method = "pdp")

plot(feature_effect_original) +
    brotools::theme_blog()</code></pre>
<p>
Quite similar to the marginal effects from the GLM! Let’s now compute model-agnostic feature importances:
</p>
<pre class="r"><code>feature_importance_original &lt;- FeatureImp$new(predictor_original, loss = "mse")

plot(feature_importance_original)</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/mice_exp-23-1.png" width="672">
</p>
<p>
And finally, the interaction effect of the <code>sex</code> variable interacted with all the others:
</p>
<pre class="r"><code>interaction_sex_original &lt;- Interaction$new(predictor_original, feature = "sex")

plot(interaction_sex_original)</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/mice_exp-26-1.png" width="672">
</p>
<p>
Ok so let’s assume that I’m happy with these explanations, and do need or want to go further. This would be the end of it in an ideal world, but this is not an ideal world unfortunately, but it’s the best we’ve got. In the real world, it often happens that data comes with missing values.
</p>
</section>
<section id="missing-data-and-explainability" class="level2">
<h2 class="anchored" data-anchor-id="missing-data-and-explainability">
Missing data and explainability
</h2>
<p>
As explained in the beginning, I’ve been wondering how to deal with missing values when the goal of the analysis is explainability. How can the explanations be pooled? Let’s start with creating a data set with missing values, then perform multiple imputation, then perform the analysis.
</p>
<p>
First, let me create a <code>patterns</code> matrix, that I will pass to the <code>ampute()</code> function from the <code>{mice}</code> package. This function creates a dataset with missing values, and by using its <code>patterns</code> argument, I can decide which columns should have missing values:
</p>
<pre class="r"><code>patterns &lt;- -1*(diag(1, nrow = 15, ncol = 15) - 1)

patterns[ ,c(seq(1, 6), c(9, 13))] &lt;- 0

amputed_train &lt;- ampute(as_tibble(original_train), prop = 0.1, patterns = patterns, mech = "MNAR")</code></pre>
<pre><code>## Warning: Data is made numeric because the calculation of weights requires
## numeric data</code></pre>
<p>
Let’s take a look at the missingness pattern:
</p>
<pre class="r"><code>naniar::vis_miss(amputed_train$amp) + 
    brotools::theme_blog() + 
      theme(axis.text.x=element_text(angle=90, hjust=1))</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/mice_exp-28-1.png" width="672">
</p>
<p>
Ok, so now let’s suppose that this was the dataset I was given. As a serious data scientist, I decide to perform multiple imputation first:
</p>
<pre class="r"><code>imputed_train_data &lt;- mice(data = amputed_train$amp, m = 10)

long_train_data &lt;- complete(imputed_train_data, "long")</code></pre>
<p>
So because I performed multiple imputation 10 times, I now have 10 different datasets. I should now perform my analysis on these 10 datasets, which means I&nbsp;should run my GBM on each of them, and then get out the explanations for each of them. So let’s do just that. But first, let’s change the columns back to how they were; to perform amputation, the factor columns were converted to numbers:
</p>
<pre class="r"><code>long_train_data &lt;- long_train_data %&gt;% 
    mutate(idp = ifelse(idp == 1, FALSE, TRUE),
           physlim = ifelse(physlim == 1, FALSE, TRUE),
           health = as.factor(case_when(health == 1 ~ "excellent",
                              health == 2 ~ "fair",
                              health == 3 ~ "good", 
                              health == 4 ~  "poor")),
           sex = as.factor(ifelse(sex == 1, "female", "male")),
           child = ifelse(child == 1, FALSE, TRUE),
           black = ifelse(black == 1, FALSE, TRUE))</code></pre>
<p>
Ok, so now we’re ready to go. I will use the <code>h2o.gbm()</code> function on each imputed data set. For this, I’ll use the <code>group_by()</code>-<code>nest()</code> trick which consists in grouping the dataset by the <code>.imp</code> column, then nesting it, then mapping the <code>h2o.gbm()</code> function to each imputed dataset. If you are not familiar with this, you can read <a href="https://www.brodrigues.co/blog/2017-03-29-make-ggplot2-purrr/">this other</a> blog post, which explains the approach. I define a custom function, <code>train_on_imputed_data()</code> to run <code>h2o.gbm()</code> on each imputed data set:
</p>
<pre class="r"><code>train_on_imputed_data &lt;- function(long_data){
    long_data %&gt;% 
        group_by(.imp) %&gt;% 
        nest() %&gt;% 
        mutate(model = map(data, ~h2o.gbm(y = "mdu", x = features_names,
            training_frame = as.h2o(.),
            validation_frame = validation,
            distribution = "poisson",
            score_each_iteration = TRUE,
            ntrees = 110,
            max_depth = 20,
            sample_rate = 0.6,
            col_sample_rate = 0.8,
            col_sample_rate_per_tree = 0.9,
            learn_rate = 0.05)))
}</code></pre>
<p>
Now the training takes place:
</p>
<pre class="r"><code>imp_trained &lt;- train_on_imputed_data(long_train_data)</code></pre>
<p>
Let’s take a look at <code>imp_trained</code>:
</p>
<pre class="r"><code>imp_trained</code></pre>
<pre><code>## # A tibble: 10 x 3
## # Groups:   .imp [10]
##     .imp            data model     
##    &lt;int&gt; &lt;list&lt;df[,16]&gt;&gt; &lt;list&gt;    
##  1     1   [14,042 × 16] &lt;H2ORgrsM&gt;
##  2     2   [14,042 × 16] &lt;H2ORgrsM&gt;
##  3     3   [14,042 × 16] &lt;H2ORgrsM&gt;
##  4     4   [14,042 × 16] &lt;H2ORgrsM&gt;
##  5     5   [14,042 × 16] &lt;H2ORgrsM&gt;
##  6     6   [14,042 × 16] &lt;H2ORgrsM&gt;
##  7     7   [14,042 × 16] &lt;H2ORgrsM&gt;
##  8     8   [14,042 × 16] &lt;H2ORgrsM&gt;
##  9     9   [14,042 × 16] &lt;H2ORgrsM&gt;
## 10    10   [14,042 × 16] &lt;H2ORgrsM&gt;</code></pre>
<p>
We see that the column <code>model</code> contains one model for each imputed dataset. Now comes the part I wanted to write about (finally): getting explanations out of this. Getting the explanations from each model is not the hard part, that’s easily done using some <code>{tidyverse}</code> magic (if you’re following along, run this bit of code below, and go make dinner, have dinner, and wash the dishes, because it takes time to run):
</p>
<pre class="r"><code>make_predictors &lt;- function(model){
    Predictor$new(
        model = model, 
        data = just_features, 
        y = actual_target, 
        predict.fun = predict_for_iml
        )
}

make_effect &lt;- function(predictor_object, feature = "linc", method = "pdp"){
    FeatureEffect$new(predictor_object, feature, method)
}

make_feat_imp &lt;- function(predictor_object, loss = "mse"){
    FeatureImp$new(predictor_object, loss)
}

make_interactions &lt;- function(predictor_object, feature = "sex"){
    Interaction$new(predictor_object, feature = feature)
}

imp_trained &lt;- imp_trained %&gt;%
    mutate(predictors = map(model, make_predictors)) %&gt;% 
    mutate(effect_linc = map(predictors, make_effect)) %&gt;% 
    mutate(feat_imp = map(predictors, make_feat_imp)) %&gt;% 
    mutate(interactions_sex = map(predictors, make_interactions))</code></pre>
<p>
Ok so now that I’ve got these explanations, I am done with my analysis. This is the time to pool the results together. Remember, in the case of regression models as used in the social sciences, this means averaging the estimated model parameters and using Rubin’s rule to compute their standard errors. But in this case, this is not so obvious. Should the explanations be averaged? Should I instead analyse them one by one, and see if they differ? My gut feeling is that they shouldn’t differ much, but who knows? Perhaps the answer is doing a bit of both. I have checked online for a paper that would shed some light into this, but have not found any. So let’s take a closer look to the explanations. Let’s look at feature importance:
</p>
<details>
<p>
</p><summary>
Click to view the 10 feature importances
</summary>
<p></p>
<pre class="r"><code>imp_trained %&gt;% 
    pull(feat_imp)</code></pre>
<pre><code>## [[1]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##    feature importance.05 importance importance.95 permutation.error
## 1 ndisease     1.0421605   1.362672      1.467244          22.03037
## 2     fmde     0.8611917   1.142809      1.258692          18.47583
## 3      lpi     0.8706659   1.103367      1.196081          17.83817
## 4   health     0.8941010   1.098014      1.480508          17.75164
## 5       lc     0.8745229   1.024288      1.296668          16.55970
## 6    black     0.7537278   1.006294      1.095054          16.26879
## 
## [[2]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##    feature importance.05 importance importance.95 permutation.error
## 1      age      0.984304   1.365702      1.473146          22.52529
## 2     linc      1.102023   1.179169      1.457907          19.44869
## 3 ndisease      1.075821   1.173938      1.642938          19.36241
## 4     fmde      1.059303   1.150112      1.281291          18.96944
## 5       lc      0.837573   1.132719      1.200556          18.68257
## 6  physlim      0.763757   1.117635      1.644434          18.43379
## 
## [[3]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##    feature importance.05 importance importance.95 permutation.error
## 1      age     0.8641304   1.334382      1.821797          21.62554
## 2    black     1.0553001   1.301338      1.429119          21.09001
## 3     fmde     0.8965085   1.208761      1.360217          19.58967
## 4 ndisease     1.0577766   1.203418      1.651611          19.50309
## 5     linc     0.9299725   1.114041      1.298379          18.05460
## 6      sex     0.9854144   1.091391      1.361406          17.68754
## 
## [[4]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##   feature importance.05 importance importance.95 permutation.error
## 1 educdec     0.9469049   1.263961      1.358115          20.52909
## 2     age     1.0980269   1.197441      1.763202          19.44868
## 3  health     0.8539843   1.133338      1.343389          18.40753
## 4    linc     0.7608811   1.123423      1.328756          18.24649
## 5     lpi     0.8203850   1.103394      1.251688          17.92118
## 6   black     0.9476909   1.089861      1.328960          17.70139
## 
## [[5]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##   feature importance.05 importance importance.95 permutation.error
## 1     lpi     0.9897789   1.336405      1.601778          22.03791
## 2 educdec     0.8701162   1.236741      1.424602          20.39440
## 3     age     0.8537786   1.181242      1.261411          19.47920
## 4    lfam     1.0185313   1.133158      1.400151          18.68627
## 5     idp     0.9502284   1.069772      1.203147          17.64101
## 6    linc     0.8600586   1.042453      1.395231          17.19052
## 
## [[6]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##   feature importance.05 importance importance.95 permutation.error
## 1      lc     0.7707383   1.208190      1.379422          19.65436
## 2     sex     0.9309901   1.202629      1.479511          19.56391
## 3    linc     1.0549563   1.138404      1.624217          18.51912
## 4     lpi     0.9360817   1.135198      1.302084          18.46696
## 5 physlim     0.7357272   1.132525      1.312584          18.42349
## 6   child     1.0199964   1.109120      1.316306          18.04274
## 
## [[7]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##    feature importance.05 importance importance.95 permutation.error
## 1     linc     0.9403425   1.262994      1.511122          20.65942
## 2       lc     1.0481333   1.233136      1.602796          20.17103
## 3 ndisease     1.1612194   1.212454      1.320208          19.83272
## 4  educdec     0.7924637   1.197343      1.388218          19.58554
## 5     lfam     0.8423790   1.178545      1.349884          19.27805
## 6      age     0.9125829   1.168297      1.409525          19.11043
## 
## [[8]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##    feature importance.05 importance importance.95 permutation.error
## 1      age     1.1281736   1.261273      1.609524          20.55410
## 2   health     0.9134557   1.240597      1.432366          20.21716
## 3     lfam     0.7469043   1.182294      1.345910          19.26704
## 4      lpi     0.8088552   1.160863      1.491139          18.91779
## 5 ndisease     1.0756671   1.104357      1.517278          17.99695
## 6     fmde     0.6929092   1.093465      1.333544          17.81946
## 
## [[9]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##    feature importance.05 importance importance.95 permutation.error
## 1  educdec     1.0188109   1.287697      1.381982          20.92713
## 2      lpi     0.9853336   1.213095      1.479002          19.71473
## 3     linc     0.8354715   1.195344      1.254350          19.42625
## 4      age     0.9980451   1.179371      1.383545          19.16666
## 5 ndisease     1.0492685   1.176804      1.397398          19.12495
## 6     lfam     1.0814043   1.166626      1.264592          18.95953
## 
## [[10]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##    feature importance.05 importance importance.95 permutation.error
## 1      age     0.9538824   1.211869      1.621151          19.53671
## 2      sex     0.9148921   1.211253      1.298311          19.52678
## 3     lfam     0.8227355   1.093094      1.393815          17.62192
## 4 ndisease     0.8282127   1.090779      1.205994          17.58459
## 5       lc     0.7004401   1.060870      1.541697          17.10244
## 6   health     0.8137149   1.058324      1.183639          17.06138</code></pre>
</details>
<p>
As you can see, the feature importances are quite different from each other, but I don’t think this comes from the imputations, but rather from the fact that feature importance <em>depends on shuffling the feature, which adds randomness to the measurement</em> (source: <a href="https://christophm.github.io/interpretable-ml-book/feature-importance.html#disadvantages-9" class="uri">https://christophm.github.io/interpretable-ml-book/feature-importance.html#disadvantages-9</a>). To mitigate this, Christoph Molnar suggests repeating the the permutation and averaging the importance measures; I think that this would be my approach for <em>pooling</em> as well.
</p>
<p>
Let’s now take a look at interactions:
</p>
<details>
<p>
</p><summary>
Click to view the 10 interactions
</summary>
<p></p>
<pre class="r"><code>imp_trained %&gt;% 
    pull(interactions_sex)</code></pre>
<pre><code>## [[1]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.07635197
## 2      idp:sex   0.08172754
## 3      lpi:sex   0.10704357
## 4     fmde:sex   0.11267146
## 5  physlim:sex   0.04099073
## 6 ndisease:sex   0.16314524
## 
## [[2]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.10349820
## 2      idp:sex   0.07432519
## 3      lpi:sex   0.11651413
## 4     fmde:sex   0.18123926
## 5  physlim:sex   0.12952808
## 6 ndisease:sex   0.14528876
## 
## [[3]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.05919320
## 2      idp:sex   0.05586197
## 3      lpi:sex   0.24253335
## 4     fmde:sex   0.05240474
## 5  physlim:sex   0.06404969
## 6 ndisease:sex   0.14508072
## 
## [[4]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.02775529
## 2      idp:sex   0.02050390
## 3      lpi:sex   0.11781130
## 4     fmde:sex   0.11084240
## 5  physlim:sex   0.17932694
## 6 ndisease:sex   0.07181589
## 
## [[5]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.12873151
## 2      idp:sex   0.03681428
## 3      lpi:sex   0.15879389
## 4     fmde:sex   0.16952900
## 5  physlim:sex   0.07031520
## 6 ndisease:sex   0.10567463
## 
## [[6]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.15320481
## 2      idp:sex   0.08645037
## 3      lpi:sex   0.16674641
## 4     fmde:sex   0.14671054
## 5  physlim:sex   0.09236257
## 6 ndisease:sex   0.14605618
## 
## [[7]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.04072960
## 2      idp:sex   0.05641868
## 3      lpi:sex   0.19491959
## 4     fmde:sex   0.07119644
## 5  physlim:sex   0.05777469
## 6 ndisease:sex   0.16555363
## 
## [[8]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.04979709
## 2      idp:sex   0.06036898
## 3      lpi:sex   0.14009307
## 4     fmde:sex   0.10927688
## 5  physlim:sex   0.08761533
## 6 ndisease:sex   0.20544585
## 
## [[9]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.08572075
## 2      idp:sex   0.12254979
## 3      lpi:sex   0.17532347
## 4     fmde:sex   0.12557420
## 5  physlim:sex   0.05084209
## 6 ndisease:sex   0.13977328
## 
## [[10]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.08636490
## 2      idp:sex   0.04807331
## 3      lpi:sex   0.17922280
## 4     fmde:sex   0.05728403
## 5  physlim:sex   0.09392774
## 6 ndisease:sex   0.13408956</code></pre>
</details>
<p>
It would seem that interactions are a bit more stable. Let’s average the values; for this I need to access the <code>results</code> element of the interactions object and the result out:
</p>
<pre class="r"><code>interactions_sex_result &lt;- imp_trained %&gt;% 
    mutate(interactions_results = map(interactions_sex, function(x)(x$results))) %&gt;% 
    pull()</code></pre>
<p>
<code>interactions_sex_result</code> is a list of dataframes, which means I can bind the rows together and compute whatever I need:
</p>
<pre class="r"><code>interactions_sex_result %&gt;% 
    bind_rows() %&gt;% 
    group_by(.feature) %&gt;% 
    summarise_at(.vars = vars(.interaction), 
                 .funs = funs(mean, sd, low_ci = quantile(., 0.05), high_ci = quantile(., 0.95)))</code></pre>
<pre><code>## # A tibble: 13 x 5
##    .feature       mean     sd low_ci high_ci
##    &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
##  1 age:sex      0.294  0.0668 0.181    0.369
##  2 black:sex    0.117  0.0286 0.0763   0.148
##  3 child:sex    0.0817 0.0308 0.0408   0.125
##  4 educdec:sex  0.148  0.0411 0.104    0.220
##  5 fmde:sex     0.114  0.0443 0.0546   0.176
##  6 health:sex   0.130  0.0190 0.104    0.151
##  7 idp:sex      0.0643 0.0286 0.0278   0.106
##  8 lc:sex       0.0811 0.0394 0.0336   0.142
##  9 lfam:sex     0.149  0.0278 0.125    0.198
## 10 linc:sex     0.142  0.0277 0.104    0.179
## 11 lpi:sex      0.160  0.0416 0.111    0.221
## 12 ndisease:sex 0.142  0.0356 0.0871   0.187
## 13 physlim:sex  0.0867 0.0415 0.0454   0.157</code></pre>
<p>
That seems pretty good. Now, what about the partial dependence? Let’s take a closer look:
</p>
<details>
<p>
</p><summary>
Click to view the 10 pdps
</summary>
<p></p>
<pre class="r"><code>imp_trained %&gt;% 
    pull(effect_linc)</code></pre>
<pre><code>## [[1]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 1.652445   pdp
## 2 0.5312226 1.687522   pdp
## 3 1.0624453 1.687522   pdp
## 4 1.5936679 1.687522   pdp
## 5 2.1248905 1.685088   pdp
## 6 2.6561132 1.694112   pdp
## 
## [[2]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 1.813449   pdp
## 2 0.5312226 1.816195   pdp
## 3 1.0624453 1.816195   pdp
## 4 1.5936679 1.816195   pdp
## 5 2.1248905 1.804457   pdp
## 6 2.6561132 1.797238   pdp
## 
## [[3]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 1.906515   pdp
## 2 0.5312226 2.039318   pdp
## 3 1.0624453 2.039318   pdp
## 4 1.5936679 2.039318   pdp
## 5 2.1248905 2.002970   pdp
## 6 2.6561132 2.000922   pdp
## 
## [[4]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 1.799552   pdp
## 2 0.5312226 2.012634   pdp
## 3 1.0624453 2.012634   pdp
## 4 1.5936679 2.012634   pdp
## 5 2.1248905 1.982425   pdp
## 6 2.6561132 1.966392   pdp
## 
## [[5]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 1.929158   pdp
## 2 0.5312226 1.905171   pdp
## 3 1.0624453 1.905171   pdp
## 4 1.5936679 1.905171   pdp
## 5 2.1248905 1.879721   pdp
## 6 2.6561132 1.869113   pdp
## 
## [[6]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 2.147697   pdp
## 2 0.5312226 2.162393   pdp
## 3 1.0624453 2.162393   pdp
## 4 1.5936679 2.162393   pdp
## 5 2.1248905 2.119923   pdp
## 6 2.6561132 2.115131   pdp
## 
## [[7]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 1.776742   pdp
## 2 0.5312226 1.957938   pdp
## 3 1.0624453 1.957938   pdp
## 4 1.5936679 1.957938   pdp
## 5 2.1248905 1.933847   pdp
## 6 2.6561132 1.885287   pdp
## 
## [[8]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 2.020647   pdp
## 2 0.5312226 2.017981   pdp
## 3 1.0624453 2.017981   pdp
## 4 1.5936679 2.017981   pdp
## 5 2.1248905 1.981122   pdp
## 6 2.6561132 2.017604   pdp
## 
## [[9]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 1.811189   pdp
## 2 0.5312226 2.003053   pdp
## 3 1.0624453 2.003053   pdp
## 4 1.5936679 2.003053   pdp
## 5 2.1248905 1.938150   pdp
## 6 2.6561132 1.918518   pdp
## 
## [[10]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 1.780325   pdp
## 2 0.5312226 1.850203   pdp
## 3 1.0624453 1.850203   pdp
## 4 1.5936679 1.850203   pdp
## 5 2.1248905 1.880805   pdp
## 6 2.6561132 1.881305   pdp</code></pre>
</details>
<p>
As you can see, the values are quite similar. I think that in the case of plots, the best way to visualize the impact of the imputation is to simply plot all the lines in a single plot:
</p>
<pre class="r"><code>effect_linc_results &lt;- imp_trained %&gt;% 
    mutate(effect_linc_results = map(effect_linc, function(x)(x$results))) %&gt;% 
    select(.imp, effect_linc_results) %&gt;% 
    unnest(effect_linc_results)

effect_linc_results %&gt;% 
    bind_rows() %&gt;% 
    ggplot() + 
    geom_line(aes(y = .y.hat, x = linc, group = .imp), colour = "#82518c") + 
    brotools::theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/mice_exp-43-1.png" width="672">
</p>
<p>
Overall, the partial dependence plot seems to behave in a very similar way across the different imputed datasets!
</p>
<p>
To conclude, I think that the approach I suggest here is nothing revolutionary; it is consistent with the way one should conduct an analysis with multiple imputed datasets. However, the pooling step is non-trivial and there is no magic recipe; it really depends on the goal of the analysis and what you want or need to show.
</p>
<p>
Hope you enjoyed! If you found this blog post useful, you might want to follow me on <a href="https://www.twitter.com/brodriguesco">twitter</a> for blog post updates and <a href="https://www.buymeacoffee.com/brodriguesco">buy me an espresso</a> or <a href="https://www.paypal.me/brodriguesco">paypal.me</a>, or buy my ebook on <a href="https://leanpub.com/modern_tidyverse">Leanpub</a>.
</p>
<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}</style>
<p>
<link href="https://fonts.googleapis.com/css?family=Cookie" rel="stylesheet"><a class="bmc-button" target="_blank" href="https://www.buymeacoffee.com/brodriguesco"><img src="https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg" alt="Buy me an Espresso"><span style="margin-left:5px">Buy me an Espresso</span></a>
</p>
</section>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-11-02-mice_exp.html</guid>
  <pubDate>Sat, 02 Nov 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Cluster multiple time series using K-means</title>
  <link>https://b-rodrigues.github.io/posts/2019-10-12-cluster_ts.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)"> <img src="https://b-rodrigues.github.io/assets/img/deepfried_elbow.jpg" title="A life saving skill"></a>
</p>
</div>
<p>
I have been recently confronted to the issue of finding similarities among time-series and though about using k-means to cluster them. To illustrate the method, I’ll be using data from the Penn World Tables, readily available in R (inside the <code>{pwt9}</code> package):
</p>
<pre class="r"><code>library(tidyverse)
library(lubridate)
library(pwt9)
library(brotools)</code></pre>
<p>
First, of all, let’s only select the needed columns:
</p>
<pre class="r"><code>pwt &lt;- pwt9.0 %&gt;%
select(country, year, avh)</code></pre>
<p>
<code>avh</code> contains the average worked hours for a given country and year. The data looks like this:
</p>
<pre class="r"><code>head(pwt)</code></pre>
<pre><code>##          country year avh
## ABW-1950   Aruba 1950  NA
## ABW-1951   Aruba 1951  NA
## ABW-1952   Aruba 1952  NA
## ABW-1953   Aruba 1953  NA
## ABW-1954   Aruba 1954  NA
## ABW-1955   Aruba 1955  NA</code></pre>
<p>
For each country, there’s yearly data on the <code>avh</code> variable. The goal here is to cluster the different countries by looking at how similar they are on the <code>avh</code> variable. Let’s do some further cleaning. The k-means implementation in R expects a wide data frame (currently my data frame is in the long format) and no missing values. These could potentially be imputed, but I can’t be bothered:
</p>
<pre class="r"><code>pwt_wide &lt;- pwt %&gt;%
  pivot_wider(names_from = year, values_from = avh)  %&gt;%
  filter(!is.na(`1950`)) %&gt;%
  mutate_at(vars(-country), as.numeric)</code></pre>
<p>
To convert my data frame from long to wide, I use the fresh <code>pivot_wider()</code> function, instead of the less intuitive <code>spread()</code> function.
</p>
<p>
We’re ready to use the k-means algorithm. To know how many clusters I should aim for, I’ll be using the elbow method (if you’re not familiar with this method, click on the image at the very top of this post):
</p>
<pre class="r"><code>wss &lt;- map_dbl(1:5, ~{kmeans(select(pwt_wide, -country), ., nstart=50,iter.max = 15 )$tot.withinss})

n_clust &lt;- 1:5

elbow_df &lt;- as.data.frame(cbind("n_clust" = n_clust, "wss" = wss))


ggplot(elbow_df) +
geom_line(aes(y = wss, x = n_clust), colour = "#82518c") +
theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/cluster_ts-6-1.png" width="672">
</p>
<p>
Looks like 3 clusters is a good choice. Let’s now run the kmeans algorithm:
</p>
<pre class="r"><code>clusters &lt;- kmeans(select(pwt_wide, -country), centers = 3)</code></pre>
<p>
<code>clusters</code> is a list with several interesting items. The item <code>centers</code> contains the “average” time series:
</p>
<pre class="r"><code>(centers &lt;- rownames_to_column(as.data.frame(clusters$centers), "cluster"))</code></pre>
<pre><code>##   cluster     1950     1951     1952     1953     1954     1955     1956
## 1       1 2110.440 2101.273 2088.947 2074.273 2066.617 2053.391 2034.926
## 2       2 2086.509 2088.571 2084.433 2081.939 2078.756 2078.710 2074.175
## 3       3 2363.600 2350.774 2338.032 2325.375 2319.011 2312.083 2308.483
##       1957     1958     1959     1960     1961     1962     1963     1964
## 1 2021.855 2007.221 1995.038 1985.904 1978.024 1971.618 1963.780 1962.983
## 2 2068.807 2062.021 2063.687 2060.176 2052.070 2044.812 2038.939 2037.488
## 3 2301.355 2294.556 2287.556 2279.773 2272.899 2262.781 2255.690 2253.431
##       1965     1966     1967     1968     1969     1970     1971     1972
## 1 1952.945 1946.961 1928.445 1908.354 1887.624 1872.864 1855.165 1825.759
## 2 2027.958 2021.615 2015.523 2007.176 2001.289 1981.906 1967.323 1961.269
## 3 2242.775 2237.216 2228.943 2217.717 2207.037 2190.452 2178.955 2167.124
##       1973     1974     1975     1976     1977     1978     1979     1980
## 1 1801.370 1770.484 1737.071 1738.214 1713.395 1693.575 1684.215 1676.703
## 2 1956.755 1951.066 1933.527 1926.508 1920.668 1911.488 1904.316 1897.103
## 3 2156.304 2137.286 2125.298 2118.138 2104.382 2089.717 2083.036 2069.678
##       1981     1982     1983     1984     1985     1986     1987     1988
## 1 1658.894 1644.019 1636.909 1632.371 1623.901 1615.320 1603.383 1604.331
## 2 1883.376 1874.730 1867.266 1861.386 1856.947 1849.568 1848.748 1847.690
## 3 2055.658 2045.501 2041.428 2030.095 2040.210 2033.289 2028.345 2029.290
##       1989     1990     1991     1992     1993     1994     1995     1996
## 1 1593.225 1586.975 1573.084 1576.331 1569.725 1567.599 1567.113 1558.274
## 2 1842.079 1831.907 1823.552 1815.864 1823.824 1830.623 1831.815 1831.648
## 3 2031.741 2029.786 1991.807 1974.954 1973.737 1975.667 1980.278 1988.728
##       1997     1998     1999     2000     2001     2002     2003     2004
## 1 1555.079 1555.071 1557.103 1545.349 1530.207 1514.251 1509.647 1522.389
## 2 1835.372 1836.030 1839.857 1827.264 1813.477 1781.696 1786.047 1781.858
## 3 1985.076 1961.219 1966.310 1959.219 1946.954 1940.110 1924.799 1917.130
##       2005     2006     2007     2008     2009     2010     2011     2012
## 1 1514.492 1512.872 1515.299 1514.055 1493.875 1499.563 1503.049 1493.862
## 2 1775.167 1776.759 1773.587 1771.648 1734.559 1736.098 1742.143 1735.396
## 3 1923.496 1912.956 1902.156 1897.550 1858.657 1861.875 1861.608 1850.802
##       2013     2014
## 1 1485.589 1486.991
## 2 1729.973 1729.543
## 3 1848.158 1851.829</code></pre>
<p>
<code>clusters</code> also contains the <code>cluster</code> item, which tells me to which cluster the different countries belong to. I can easily add this to the original data frame:
</p>
<pre class="r"><code>pwt_wide &lt;- pwt_wide %&gt;% 
  mutate(cluster = clusters$cluster)</code></pre>
<p>
Now, let’s prepare the data for visualisation. I have to go back to a long data frame for this:
</p>
<pre class="r"><code>pwt_long &lt;- pwt_wide %&gt;%
  pivot_longer(cols=c(-country, -cluster), names_to = "year", values_to = "avh") %&gt;%
  mutate(year = ymd(paste0(year, "-01-01")))

centers_long &lt;- centers %&gt;%
  pivot_longer(cols = -cluster, names_to = "year", values_to = "avh") %&gt;%  
  mutate(year = ymd(paste0(year, "-01-01")))</code></pre>
<p>
And I can now plot the different time series, by cluster and highlight the “average” time series for each cluster as well (yellow line):
</p>
<pre class="r"><code>ggplot() +
  geom_line(data = pwt_long, aes(y = avh, x = year, group = country), colour = "#82518c") +
  facet_wrap(~cluster, nrow = 1) + 
  geom_line(data = centers_long, aes(y = avh, x = year, group = cluster), col = "#b58900", size = 2) +
  theme_blog() +
  labs(title = "Average hours worked in several countries", 
       caption = "The different time series have been clustered using k-means.
                 Cluster 1: Belgium, Switzerland, Germany, Denmark, France, Luxembourg, Netherlands,
                 Norway, Sweden.\nCluster 2: Australia, Colombia, Ireland, Iceland, Japan, Mexico,
                 Portugal, Turkey.\nCluster 3: Argentina, Austria, Brazil, Canada, Cyprus, Spain, Finland,
                 UK, Italy, New Zealand, Peru, USA, Venezuela") +
  theme(plot.caption = element_text(colour = "white"))</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/cluster_ts-11-1.png" width="672">
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-10-12-cluster_ts.html</guid>
  <pubDate>Sat, 12 Oct 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Split-apply-combine for Maximum Likelihood Estimation of a linear model</title>
  <link>https://b-rodrigues.github.io/posts/2019-10-05-parallel_maxlik.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="http://www.univ-orleans.fr/deg/masters/ESA/CH/Chapter2_MLE.pdf"> <img src="https://b-rodrigues.github.io/assets/img/hieforth.png" title="click with thy mouse hither to wot moe about maximum plausability estimation" width="80%" height="auto"></a>
</p>
</div>
<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">
Intro
</h2>
<p>
Maximum likelihood estimation is a very useful technique to fit a model to data used a lot in econometrics and other sciences, but seems, at least to my knowledge, to not be so well known by machine learning practitioners (but I may be wrong about that). Other useful techniques to confront models to data used in econometrics are the minimum distance family of techniques such as the general method of moments or Bayesian approaches, while machine learning practitioners seem to favor the minimization of a loss function (the mean squared error in the case of linear regression for instance).
</p>
<p>
When I taught at the university, students had often some problems to understand the technique. It is true that it is not as easy to understand as ordinary least squares, but I’ll try to explain to the best of my abilities.
</p>
<p>
Given a sample of data, what is the unknown probability distribution that <em>most likely</em> generated it? For instance, if your sample only contains 0’s and 1’s, and the proportion of 1’s is 80%, what do you think is the most likely distribution that generated it? The probability distribution that <em>most likely</em> generated such a dataset is a binomial distribution with probability of success equal to 80%. It <em>might have been</em> a binomial distribution with probability of success equal to, say, 60%, but the <em>most likely</em> one is one with probability of success equal to 80%.
</p>
<p>
To perform maximum likelihood estimation, one thus needs to assume a certain probability distribution, and then look for the parameters that maximize the likelihood that this distribution generated the observed data. So, now the question is, how to maximize this likelihood? And mathematically speaking, what is a <em>likelihood</em>?
</p>
</section>
<section id="some-theory" class="level2">
<h2 class="anchored" data-anchor-id="some-theory">
Some theory
</h2>
<p>
First of all, let’s assume that each observation from your dataset not only was generated from the same distribution, but that each observation is also independent from each other. For instance, if in your sample you have data on people’s wages and socio-economic background, it is safe to assume, under certain circumstances, that the observations are independent.
</p>
<p>
Let <img src="https://latex.codecogs.com/png.latex?(X_i)"> be random variables, and <img src="https://latex.codecogs.com/png.latex?(x_i)"> be their realizations (actual observed values). Let’s assume that the <img src="https://latex.codecogs.com/png.latex?(X_i)"> are distributed according to a certain probability distribution <img src="https://latex.codecogs.com/png.latex?(D)"> with density <img src="https://latex.codecogs.com/png.latex?(f())"> where <img src="https://latex.codecogs.com/png.latex?()"> is a parameter of said distribution. Because our sample is composed of i.i.d. random variables, the probability that it was generated by our distribution <img src="https://latex.codecogs.com/png.latex?(D())"> is:
</p>
<p>
<img src="https://latex.codecogs.com/png.latex?%5B_%7Bi=1%7D%5EN%20Pr(X_i%20=%20x_i)%5D">
</p>
<p>
It is customary to take the log of this expression:
</p>
<p>
<img src="https://latex.codecogs.com/png.latex?%5B(%7Bi=1%7D%5EN%20Pr(X_i%20=%20x_i))%20=%20%7Bi=1%7D%5EN%20(Pr(X_i%20=%20x_i))%5D">
</p>
<p>
The expression above is called the <em>log-likelihood</em>, <img src="https://latex.codecogs.com/png.latex?(logL(;%20x_1,%20%E2%80%A6,%20x_N))">. Maximizing this function yields <img src="https://latex.codecogs.com/png.latex?(%5E*)">, the value of the parameter that makes the sample the most probable. In the case of linear regression, the density function to use is the one from the Normal distribution.
</p>
</section>
<section id="maximum-likelihood-of-the-linear-model-as-an-example-of-the-split-apply-combine-strategy" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-of-the-linear-model-as-an-example-of-the-split-apply-combine-strategy">
Maximum likelihood of the linear model as an example of the split-apply-combine strategy
</h2>
<p>
Hadley Wickham’s seminal paper, <a href="https://www.jstatsoft.org/article/view/v040i01/v40i01.pdf">The Split-Apply-Combine Strategy for Data Analysis</a> presents the <em>split-apply-combine</em> strategy, which should remind the reader of the map-reduce framework from Google. The idea is to recognize that in some cases big problems are simply an aggregation of smaller problems. This is the case for Maximum Likelihood Estimation of the linear model as well. The picture below illustrates how Maximum Likelihood works, in the standard case:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/maxlik_1.png" width="80%" height="auto"><!-- -->
</p>
<p>
Let’s use R to do exactly this. Let’s first start by simulating some data:
</p>
<pre class="r"><code>library("tidyverse")
size &lt;- 500000

x1 &lt;- rnorm(size)
x2 &lt;- rnorm(size)
x3 &lt;- rnorm(size)

dep_y &lt;- 1.5 + 2*x1 + 3*x2 + 4*x3 + rnorm(size)

x_data &lt;- cbind(dep_y, 1, x1, x2, x3)

x_df &lt;- as.data.frame(x_data) %&gt;%
  rename(iota = V2)

head(x_df)</code></pre>
<pre><code>##       dep_y iota         x1          x2         x3
## 1  1.637044    1  0.2287198  0.91609653 -0.4006215
## 2 -1.684578    1  1.2780291 -0.02468559 -1.4020914
## 3  1.289595    1  1.0524842  0.30206515 -0.3553641
## 4 -3.769575    1 -2.5763576  0.13864796 -0.3181661
## 5 13.110239    1 -0.9376462  0.77965301  3.0351646
## 6  5.059152    1  0.7488792 -0.10049061  0.1307225</code></pre>
<p>
Now that this is done, let’s write a function to perform Maximum Likelihood Estimation:
</p>
<pre class="r"><code>loglik_linmod &lt;- function(parameters, x_data){
  sum_log_likelihood &lt;- x_data %&gt;%
    mutate(log_likelihood =
             dnorm(dep_y,
                   mean = iota*parameters[1] + x1*parameters[2] + x2*parameters[3] + x3*parameters[4],
                   sd = parameters[5],
                   log = TRUE)) %&gt;%
    summarise(sum(log_likelihood))

  -1 * sum_log_likelihood
}</code></pre>
<p>
The function returns minus the log likelihood, because <code>optim()</code> which I will be using to optimize the log-likelihood function minimizes functions by default (minimizing the opposite of a function is the same as maximizing a function). Let’s optimize the function and see if we’re able to find the parameters of the data generating process, <code>1.5, 2, 3, 4</code> and <code>1</code> (the standard deviation of the error term):
</p>
<pre class="r"><code>optim(c(1,1,1,1,1), loglik_linmod, x_data = x_df)</code></pre>
<p>
We successfully find the parameters of our data generating process!
</p>
<p>
Now, what if I’d like to distribute the computation of the contribution to the likelihood of each observations across my 12 cores? The goal is not necessarily to speed up the computations but to be able to handle larger than RAM data. If I have data that is too large to fit in memory, I could split it into chunks, compute the contributions to the likelihood of each chunk, sum everything again, and voila! This is illustrated below:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/maxlik_2.png" width="80%" height="auto"><!-- -->
</p>
<p>
To do this, I use the <code>{disk.frame}</code> package, and only need to change my <code>loglik_linmod()</code> function slightly:
</p>
<pre class="r"><code>library("disk.frame")
x_diskframe &lt;- as.disk.frame(x_df) #Convert the data frame to a disk.frame

loglik_linmod_df &lt;- function(parameters, x_data){
  sum_log_likelihood &lt;- x_data %&gt;%
    mutate(log_likelihood =
             dnorm(dep_y,
                   mean = iota*parameters[1] + x1*parameters[2] + x2*parameters[3] + x3*parameters[4],
                   sd = parameters[5],
                   log = TRUE)) %&gt;% 
    chunk_summarise(sum(log_likelihood))

  out &lt;- sum_log_likelihood %&gt;%
    collect() %&gt;%
    pull() %&gt;%
    sum()

  -out
}</code></pre>
<p>
The function is applied to each chunk, and <code>chunk_summarise()</code> computes the sum of the contributions inside each chunk. Thus, I first need to use <code>collect()</code> to transfer the chunk-wise sums in memory and then use <code>pull()</code> to convert it to an atomic vector, and finally sum them all again.
</p>
<p>
Let’s now optimize this function:
</p>
<pre class="r"><code>optim(rep(1, 5), loglik_linmod_df, x_data = x_diskframe)</code></pre>
<pre><code>## $par
## [1] 1.5351722 1.9566144 3.0067978 4.0202956 0.9889412
## 
## $value
## [1] 709977.2
## 
## $counts
## function gradient 
##      502       NA 
## 
## $convergence
## [1] 1
## 
## $message
## NULL</code></pre>
<p>
This is how you can use the split-apply-combine approach for maximum likelihood estimation of a linear model! This approach is quite powerful, and the familiar <code>map()</code> and <code>reduce()</code> functions included in <code>{purrr}</code> can also help with this task. However, this only works if you can split your problem into chunks, which is sometimes quite hard to achieve.
</p>
<p>
However, as usual, there is rarely a need to write your own functions, as <code>{disk.frame}</code> includes the <code>dfglm()</code> function which can be used to estimate any generalized linear model using <code>disk.frame</code> objects!
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>programming</category>
  <guid>https://b-rodrigues.github.io/posts/2019-10-05-parallel_maxlik.html</guid>
  <pubDate>Sat, 05 Oct 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>{disk.frame} is epic</title>
  <link>https://b-rodrigues.github.io/posts/2019-09-03-disk_frame.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://youtu.be/3XMTyi_H4q4"> <img src="https://b-rodrigues.github.io/assets/img/disk_frame.png" title="Zhuo Jia Dai's talk at useR!2019" width="80%" height="auto"></a>
</p>
</div>
<p>
Note: When I started writing this blog post, I encountered a bug and filed a <a href="https://github.com/xiaodaigh/disk.frame/issues/141">bug report</a> that I encourage you to read. The responsiveness of the developer was exemplary. Not only did Zhuo solve the issue in record time, he provided ample code snippets to illustrate the solutions. Hats off to him!
</p>
<p>
This blog post is a short presentation of <code>{disk.frame}</code> a package that makes it easy to work with data that is too large to fit on RAM, but not large enough that it could be called big data. Think data that is around 30GB for instance, or more, but nothing at the level of TBs.
</p>
<p>
I have already written a blog post about this topic, using Spark and the R library <code>{sparklyr}</code>, where I showed how to set up <code>{sparklyr}</code> to import 30GB of data. I will import the same file here, and run a very simple descriptive analysis. If you need context about the file I’ll be using, just read the <a href="../posts/2018-02-16-importing_30gb_of_data.html">previous blog post</a>.
</p>
<p>
The first step, as usual, is to load the needed packages:
</p>
<pre class="r"><code>library(tidyverse)
library(disk.frame)</code></pre>
<p>
The next step is to specify how many cores you want to dedicate to <code>{disk.frame}</code>; of course, the more cores you use, the faster the operations:
</p>
<pre class="r"><code>setup_disk.frame(workers = 6)
options(future.globals.maxSize = Inf)</code></pre>
<p>
<code>setup_disk.frame(workers = 6)</code> means that 6 cpu threads will be dedicated to importing and working on the data. The second line, <code>future.globals.maxSize = Inf</code> means that an <em>unlimited amount of data will be passed from worker to worker</em>, as described in the documentation.
</p>
<p>
Now comes the interesting bit. If you followed the previous blog post, you should have a 30GB csv file. This file was obtained by merging a lot of smaller sized csv files. In practice, you should keep the files separated, and NOT merge them. This makes things much easier. However, as I said before, I want to be in the situation, which already happened to me in the past, where I get a big-sized csv file and I am to provide an analysis on that data. So, let’s try to read in that big file, which I called <code>combined.csv</code>:
</p>
<pre class="r"><code>path_to_data &lt;- "path/to/data/"

flights.df &lt;- csv_to_disk.frame(
  paste0(path_to_data, "combined.csv"), 
  outdir = paste0(path_to_data, "combined.df"),
  in_chunk_size = 2e6,
  backend = "LaF")</code></pre>
<p>
Let’s go through these lines, one at a time. In the first line, I simply define the path to the folder that contains the data. The next chunk is where I read in the data using the <code>csv_to_disk_frame()</code> function. The first option is simply the path to the csv file. The second option <code>outdir =</code> is where you need to define the directory that will hold the intermediary files, which are in the fst format. This folder, that contains these fst files, is the <code>disk.frame</code>. fst files are created by the <code>{fst}</code> package, which <em>provides a fast, easy and flexible way to serialize data frames</em>. This means that files that are in that format can be read and written much much faster than by other means (see a benchmark of <code>{fst}</code> <a href="https://www.fstpackage.org/">here</a>). The next time you want to import the data, you can use the <code>disk.frame()</code> function and point it to the <code>combined.df</code> folder. <code>in_chunk_size =</code> specifies how many lines are to be read in one swoop, and <code>backend =</code> is the underlying engine that reads in the data, in this case the <code>{LaF}</code> package. The default backend is <code>{data.table}</code> and there is also a <code>{readr}</code> backend. As written in the note at the beginning of the blog post, I encourage you to read the github issue to learn why I am using the <code>LaF</code> backend (the <code>{data.table}</code> and <code>{readr}</code> backend work as well).
</p>
<p>
Now, let’s try to replicate what I did in my previous blog post, namely, computing the average delay in departures per day. With <code>{disk.frame}</code>, one has to be very careful about something important however; all the <code>group_by()</code> operations are performed <em>per chunk</em>. This means that a second <code>group_by()</code> call might be needed. For more details, I encourage you to read the <a href="http://diskframe.com/articles/intro-disk-frame.html#grouping">documentation</a>.
</p>
<p>
The code below is what I want to perform; group by day, and compute the average daily flight delay:
</p>
<pre class="r"><code>mean_dep_delay &lt;- flights.df %&gt;%
  group_by(YEAR, MONTH, DAY_OF_MONTH) %&gt;%
  summarise(mean_delay = mean(DEP_DELAY, na.rm = TRUE))</code></pre>
<p>
However, because with <code>{disk.frame}</code>, <code>group_by()</code> calls are performed per chunk, the code must now be changed. The first step is to compute the sum of delays within each chunk, and count the number of days within each chunk. This is the time consuming part:
</p>
<pre class="r"><code>tic &lt;- Sys.time()
mean_dep_delay &lt;- flights.df %&gt;%
  group_by(YEAR, MONTH, DAY_OF_MONTH) %&gt;%
  summarise(sum_delay = sum(DEP_DELAY, na.rm = TRUE), n = n()) %&gt;%
  collect()
(toc = Sys.time() - tic)
Time difference of 1.805515 mins</code></pre>
<p>
This is pretty impressive! It is much faster than with <code>{sparklyr}</code>. But we’re not done yet, we still need to compute the average:
</p>
<pre class="r"><code>mean_dep_delay &lt;- mean_dep_delay %&gt;%
  group_by(YEAR, MONTH, DAY_OF_MONTH) %&gt;%
  summarise(mean_delay = sum(sum_delay)/sum(n))</code></pre>
<p>
It is important to keep in mind that <code>group_by()</code> works by chunks when dealing with <code>disk.frame</code> objects.
</p>
<p>
To conclude, we can plot the data:
</p>
<pre class="r"><code>library(lubridate)
dep_delay &lt;- mean_dep_delay %&gt;%
  arrange(YEAR, MONTH, DAY_OF_MONTH) %&gt;%
  mutate(date = ymd(paste(YEAR, MONTH, DAY_OF_MONTH, sep = "-")))

ggplot(dep_delay, aes(date, mean_delay)) +
  geom_smooth(colour = "#82518c") + 
  brotools::theme_blog()</code></pre>
<pre><code>## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/disk_frame-9-1.png" width="80%" height="auto">
</p>
<p>
<code>{disk.frame}</code> is really promising, and I will monitor this package very closely. I might write another blog post about it, focusing this time on using machine learning with <code>disk.frame</code> objects.
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-09-03-disk_frame.html</guid>
  <pubDate>Tue, 03 Sep 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Modern R with the tidyverse is available on Leanpub</title>
  <link>https://b-rodrigues.github.io/posts/2019-08-17-modern_R.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://leanpub.com/modern_tidyverse"> <img src="https://b-rodrigues.github.io/assets/img/cover_modern.png" title="Click here to go to Leanpub" with="90%" height="auto" width="80%"></a>
</p>
</div>
<p>
Yesterday I released an ebook on <a href="https://leanpub.com/modern_tidyverse">Leanpub</a>, called <em>Modern R with the tidyverse</em>, which you can also read for free <a href="https://b-rodrigues.github.io/modern_R/">here</a>.
</p>
<p>
In this blog post, I want to give some context.
</p>
<p>
<em>Modern R with the tidyverse</em> is the second ebook I release on Leanpub. I released the first one, called <a href="https://leanpub.com/fput">Functional programming and unit testing for data munging with R</a> around Christmas 2016 (I’ve retired it on Leanpub, but you can still read it for free <a href="https://b-rodrigues.github.io/fput/">here</a>) . I just had moved back to my home country of Luxembourg and started a new job as a research assistant at the statistical national institute. Since then, lots of things happened; I’ve changed jobs and joined PwC Luxembourg as a data scientist, was promoted to manager, finished my PhD, and most importantly of all, I became a father.
</p>
<p>
Through all this, I continued blogging and working on a new ebook, called <em>Modern R with the tidyverse</em>. At first, this was supposed to be a separate book from the first one, but as I continued writing, I realized that updating and finishing the first one, would take a lot of effort, and also, that it wouldn’t make much sense in keeping both separated. So I decided to merge the content from the first ebook with the second, and update everything in one go.
</p>
<p>
My very first notes were around 50 pages if memory serves, and I used them to teach R at the University of Strasbourg while I employed there as a research and teaching assistant and working on my PhD. These notes were the basis of <em>Functional programming and unit testing for data munging with R</em> and now <em>Modern R</em>. Chapter 2 of <em>Modern R</em> is almost a simple copy and paste from these notes (with more sections added). These notes were first written around 2012-2013ish.
</p>
<p>
<em>Modern R</em> is the kind of text I would like to have had when I first started playing around with R, sometime around 2009-2010. It starts from the beginning, but also goes quite into details in the later chapters. For instance, the section on <a href="https://b-rodrigues.github.io/modern_R/functional-programming.html#modeling-with-functional-programming">modeling with functional programming</a> is quite advanced, but I believe that readers that read through all the book and reached that part would be armed with all the needed knowledge to follow. At least, this is my hope.
</p>
<p>
Now, the book is still not finished. Two chapters are missing, but it should not take me long to finish them as I already have drafts lying around. However, exercises might still be in wrong places, and more are required. Also, generally, more polishing is needed.
</p>
<p>
As written in the first paragraph of this section, the book is available on <a href="https://leanpub.com/modern_tidyverse">Leanpub</a>. Unlike my previous ebook, this one costs money; a minimum price of 4.99$ and a recommended price of 14.99$, but as mentioned you can read it for free <a href="https://b-rodrigues.github.io/modern_R/">online</a>. I’ve hesitated to give it a minimum price of 0$, but I figured that since the book can be read for free online, and that Leanpub has a 45 days return policy where readers can get 100% reimbursed, no questions asked (and keep the downloaded ebook), readers were not taking a lot of risks by buying it for 5 bucks. I sure hope however that readers will find that this ebook is worth at least 5 bucks!
</p>
<p>
Now why should you read it? There’s already a lot of books on learning how to use R. Well, I don’t really want to convince you to read it. But some people do seem to like my style of writing and my blog posts, so I guess these same people, or similar people, might like the ebook. Also, I think that this ebook covers a lot of different topics, enough of them to make you an efficient R user. But as I’ve written in the introduction of <em>Modern R</em>:
</p>
<p>
<em>So what you can expect from this book is that this book is not the only one you should read.</em>
</p>
<p>
Anyways, hope you’ll enjoy <em>Modern R</em>, suggestions, criticisms and reviews welcome!
</p>
<p>
By the way, the cover of the book is a painting by John William Waterhouse, depicting Diogenes of Sinope, an ancient Greek philosopher, an absolute mad lad. Read his Wikipedia page, it’s worth it.
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-08-17-modern_R.html</guid>
  <pubDate>Sat, 17 Aug 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Using linear models with binary dependent variables, a simulation study</title>
  <link>https://b-rodrigues.github.io/posts/2019-08-14-lpm.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://psyarxiv.com/4gmbv"> <img src="https://b-rodrigues.github.io/assets/img/illegal.png" title="Even psychologists are not safe" width="80%" height="auto"></a>
</p>
</div>
<p>
This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free <a href="https://b-rodrigues.github.io/modern_R/functional-programming.html#modeling-with-functional-programming">here</a>. This is taken from Chapter 8, in which I discuss advanced functional programming methods for modeling.
</p>
<p>
As written just above (note: as written above <em>in the book</em>), <code>map()</code> simply applies a function to a list of inputs, and in the previous section we mapped <code>ggplot()</code> to generate many plots at once. This approach can also be used to map any modeling functions, for instance <code>lm()</code> to a list of datasets.
</p>
<p>
For instance, suppose that you wish to perform a Monte Carlo simulation. Suppose that you are dealing with a binary choice problem; usually, you would use a logistic regression for this.
</p>
<p>
However, in certain disciplines, especially in the social sciences, the so-called Linear Probability Model is often used as well. The LPM is a simple linear regression, but unlike the standard setting of a linear regression, the dependent variable, or target, is a binary variable, and not a continuous variable. Before you yell “Wait, that’s illegal”, you should know that in practice LPMs do a good job of estimating marginal effects, which is what social scientists and econometricians are often interested in. Marginal effects are another way of interpreting models, giving how the outcome (or the target) changes given a change in a independent variable (or a feature). For instance, a marginal effect of 0.10 for age would mean that probability of success would increase by 10% for each added year of age.
</p>
<p>
There has been a lot of discussion on logistic regression vs LPMs, and there are pros and cons of using LPMs. Micro-econometricians are still fond of LPMs, even though the pros of LPMs are not really convincing. However, quoting Angrist and Pischke:
</p>
<p>
“While a nonlinear model may fit the CEF (population conditional expectation function) for LDVs (limited dependent variables) more closely than a linear model, when it comes to marginal effects, this probably matters little” (source: <em>Mostly Harmless Econometrics</em>)
</p>
<p>
so LPMs are still used for estimating marginal effects.
</p>
<p>
Let us check this assessment with one example. First, we simulate some data, then run a logistic regression and compute the marginal effects, and then compare with a LPM:
</p>
<pre class="r"><code>set.seed(1234)
x1 &lt;- rnorm(100)
x2 &lt;- rnorm(100)
  
z &lt;- .5 + 2*x1 + 4*x2

p &lt;- 1/(1 + exp(-z))

y &lt;- rbinom(100, 1, p)

df &lt;- tibble(y = y, x1 = x1, x2 = x2)</code></pre>
<p>
This data generating process generates data from a binary choice model. Fitting the model using a logistic regression allows us to recover the structural parameters:
</p>
<pre class="r"><code>logistic_regression &lt;- glm(y ~ ., data = df, family = binomial(link = "logit"))</code></pre>
<p>
Let’s see a summary of the model fit:
</p>
<pre class="r"><code>summary(logistic_regression)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ ., family = binomial(link = "logit"), data = df)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.91941  -0.44872   0.00038   0.42843   2.55426  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   0.0960     0.3293   0.292 0.770630    
## x1            1.6625     0.4628   3.592 0.000328 ***
## x2            3.6582     0.8059   4.539 5.64e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 138.629  on 99  degrees of freedom
## Residual deviance:  60.576  on 97  degrees of freedom
## AIC: 66.576
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<p>
We do recover the parameters that generated the data, but what about the marginal effects? We can get the marginal effects easily using the <code>{margins}</code> package:
</p>
<pre class="r"><code>library(margins)

margins(logistic_regression)</code></pre>
<pre><code>## Average marginal effects</code></pre>
<pre><code>## glm(formula = y ~ ., family = binomial(link = "logit"), data = df)</code></pre>
<pre><code>##      x1     x2
##  0.1598 0.3516</code></pre>
<p>
Or, even better, we can compute the <em>true</em> marginal effects, since we know the data generating process:
</p>
<pre class="r"><code>meffects &lt;- function(dataset, coefs){
  X &lt;- dataset %&gt;% 
  select(-y) %&gt;% 
  as.matrix()
  
  dydx_x1 &lt;- mean(dlogis(X%*%c(coefs[2], coefs[3]))*coefs[2])
  dydx_x2 &lt;- mean(dlogis(X%*%c(coefs[2], coefs[3]))*coefs[3])
  
  tribble(~term, ~true_effect,
          "x1", dydx_x1,
          "x2", dydx_x2)
}

(true_meffects &lt;- meffects(df, c(0.5, 2, 4)))</code></pre>
<pre><code>## # A tibble: 2 x 2
##   term  true_effect
##   &lt;chr&gt;       &lt;dbl&gt;
## 1 x1          0.175
## 2 x2          0.350</code></pre>
<p>
Ok, so now what about using this infamous Linear Probability Model to estimate the marginal effects?
</p>
<pre class="r"><code>lpm &lt;- lm(y ~ ., data = df)

summary(lpm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ ., data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.83953 -0.31588 -0.02885  0.28774  0.77407 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.51340    0.03587  14.314  &lt; 2e-16 ***
## x1           0.16771    0.03545   4.732 7.58e-06 ***
## x2           0.31250    0.03449   9.060 1.43e-14 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.3541 on 97 degrees of freedom
## Multiple R-squared:  0.5135, Adjusted R-squared:  0.5034 
## F-statistic: 51.18 on 2 and 97 DF,  p-value: 6.693e-16</code></pre>
<p>
It’s not too bad, but maybe it could have been better in other circumstances. Perhaps if we had more observations, or perhaps for a different set of structural parameters the results of the LPM would have been closer. The LPM estimates the marginal effect of <code>x1</code> to be 0.1677134 vs 0.1597956 for the logistic regression and for <code>x2</code>, the LPM estimation is 0.3124966 vs 0.351607. The <em>true</em> marginal effects are 0.1750963 and 0.3501926 for <code>x1</code> and <code>x2</code> respectively.
</p>
<p>
Just as to assess the accuracy of a model data scientists perform cross-validation, a Monte Carlo study can be performed to asses how close the estimation of the marginal effects using a LPM is to the marginal effects derived from a logistic regression. It will allow us to test with datasets of different sizes, and generated using different structural parameters.
</p>
<p>
First, let’s write a function that generates data. The function below generates 10 datasets of size 100 (the code is inspired by this <a href="https://stats.stackexchange.com/a/46525">StackExchange answer</a>):
</p>
<pre class="r"><code>generate_datasets &lt;- function(coefs = c(.5, 2, 4), sample_size = 100, repeats = 10){

  generate_one_dataset &lt;- function(coefs, sample_size){
  x1 &lt;- rnorm(sample_size)
  x2 &lt;- rnorm(sample_size)
  
  z &lt;- coefs[1] + coefs[2]*x1 + coefs[3]*x2

  p &lt;- 1/(1 + exp(-z))

  y &lt;- rbinom(sample_size, 1, p)

  df &lt;- tibble(y = y, x1 = x1, x2 = x2)
  }

  simulations &lt;- rerun(.n = repeats, generate_one_dataset(coefs, sample_size))
 
  tibble("coefs" = list(coefs), "sample_size" = sample_size, "repeats" = repeats, "simulations" = list(simulations))
}</code></pre>
<p>
Let’s first generate one dataset:
</p>
<pre class="r"><code>one_dataset &lt;- generate_datasets(repeats = 1)</code></pre>
<p>
Let’s take a look at <code>one_dataset</code>:
</p>
<pre class="r"><code>one_dataset</code></pre>
<pre><code>## # A tibble: 1 x 4
##   coefs     sample_size repeats simulations
##   &lt;list&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;     
## 1 &lt;dbl [3]&gt;         100       1 &lt;list [1]&gt;</code></pre>
<p>
As you can see, the tibble with the simulated data is inside a list-column called <code>simulations</code>. Let’s take a closer look:
</p>
<pre class="r"><code>str(one_dataset$simulations)</code></pre>
<pre><code>## List of 1
##  $ :List of 1
##   ..$ :Classes 'tbl_df', 'tbl' and 'data.frame': 100 obs. of  3 variables:
##   .. ..$ y : int [1:100] 0 1 1 1 0 1 1 0 0 1 ...
##   .. ..$ x1: num [1:100] 0.437 1.06 0.452 0.663 -1.136 ...
##   .. ..$ x2: num [1:100] -2.316 0.562 -0.784 -0.226 -1.587 ...</code></pre>
<p>
The structure is quite complex, and it’s important to understand this, because it will have an impact on the next lines of code; it is a list, containing a list, containing a dataset! No worries though, we can still map over the datasets directly, by using <code>modify_depth()</code> instead of <code>map()</code>.
</p>
<p>
Now, let’s fit a LPM and compare the estimation of the marginal effects with the <em>true</em> marginal effects. In order to have some confidence in our results, we will not simply run a linear regression on that single dataset, but will instead simulate hundreds, then thousands and ten of thousands of data sets, get the marginal effects and compare them to the true ones (but here I won’t simulate more than 500 datasets).
</p>
<p>
Let’s first generate 10 datasets:
</p>
<pre class="r"><code>many_datasets &lt;- generate_datasets()</code></pre>
<p>
Now comes the tricky part. I have this object, <code>many_datasets</code> looking like this:
</p>
<pre class="r"><code>many_datasets</code></pre>
<pre><code>## # A tibble: 1 x 4
##   coefs     sample_size repeats simulations
##   &lt;list&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;     
## 1 &lt;dbl [3]&gt;         100      10 &lt;list [10]&gt;</code></pre>
<p>
I would like to fit LPMs to the 10 datasets. For this, I will need to use all the power of functional programming and the <code>{tidyverse}</code>. I will be adding columns to this data frame using <code>mutate()</code> and mapping over the <code>simulations</code> list-column using <code>modify_depth()</code>. The list of data frames is at the second level (remember, it’s a list containing a list containing data frames).
</p>
<p>
I’ll start by fitting the LPMs, then using <code>broom::tidy()</code> I will get a nice data frame of the estimated parameters. I will then only select what I need, and then bind the rows of all the data frames. I will do the same for the <em>true</em> marginal effects.
</p>
<p>
I highly suggest that you run the following lines, one after another. It is complicated to understand what’s going on if you are not used to such workflows. However, I hope to convince you that once it will click, it’ll be much more intuitive than doing all this inside a loop. Here’s the code:
</p>
<pre class="r"><code>results &lt;- many_datasets %&gt;% 
  mutate(lpm = modify_depth(simulations, 2, ~lm(y ~ ., data = .x))) %&gt;% 
  mutate(lpm = modify_depth(lpm, 2, broom::tidy)) %&gt;% 
  mutate(lpm = modify_depth(lpm, 2, ~select(., term, estimate))) %&gt;% 
  mutate(lpm = modify_depth(lpm, 2, ~filter(., term != "(Intercept)"))) %&gt;% 
  mutate(lpm = map(lpm, bind_rows)) %&gt;% 
  mutate(true_effect = modify_depth(simulations, 2, ~meffects(., coefs = coefs[[1]]))) %&gt;% 
  mutate(true_effect = map(true_effect, bind_rows))</code></pre>
<p>
This is how results looks like:
</p>
<pre class="r"><code>results</code></pre>
<pre><code>## # A tibble: 1 x 6
##   coefs     sample_size repeats simulations lpm             true_effect    
##   &lt;list&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;      &lt;list&gt;          &lt;list&gt;         
## 1 &lt;dbl [3]&gt;         100      10 &lt;list [10]&gt; &lt;tibble [20 × … &lt;tibble [20 × …</code></pre>
<p>
Let’s take a closer look to the <code>lpm</code> and <code>true_effect</code> columns:
</p>
<pre class="r"><code>results$lpm</code></pre>
<pre><code>## [[1]]
## # A tibble: 20 x 2
##    term  estimate
##    &lt;chr&gt;    &lt;dbl&gt;
##  1 x1       0.228
##  2 x2       0.353
##  3 x1       0.180
##  4 x2       0.361
##  5 x1       0.165
##  6 x2       0.374
##  7 x1       0.182
##  8 x2       0.358
##  9 x1       0.125
## 10 x2       0.345
## 11 x1       0.171
## 12 x2       0.331
## 13 x1       0.122
## 14 x2       0.309
## 15 x1       0.129
## 16 x2       0.332
## 17 x1       0.102
## 18 x2       0.374
## 19 x1       0.176
## 20 x2       0.410</code></pre>
<pre class="r"><code>results$true_effect</code></pre>
<pre><code>## [[1]]
## # A tibble: 20 x 2
##    term  true_effect
##    &lt;chr&gt;       &lt;dbl&gt;
##  1 x1          0.183
##  2 x2          0.366
##  3 x1          0.166
##  4 x2          0.331
##  5 x1          0.174
##  6 x2          0.348
##  7 x1          0.169
##  8 x2          0.339
##  9 x1          0.167
## 10 x2          0.335
## 11 x1          0.173
## 12 x2          0.345
## 13 x1          0.157
## 14 x2          0.314
## 15 x1          0.170
## 16 x2          0.340
## 17 x1          0.182
## 18 x2          0.365
## 19 x1          0.161
## 20 x2          0.321</code></pre>
<p>
Let’s bind the columns, and compute the difference between the <em>true</em> and estimated marginal effects:
</p>
<pre class="r"><code>simulation_results &lt;- results %&gt;% 
  mutate(difference = map2(.x = lpm, .y = true_effect, bind_cols)) %&gt;% 
  mutate(difference = map(difference, ~mutate(., difference = true_effect - estimate))) %&gt;% 
  mutate(difference = map(difference, ~select(., term, difference))) %&gt;% 
  pull(difference) %&gt;% 
  .[[1]]</code></pre>
<p>
Let’s take a look at the simulation results:
</p>
<pre class="r"><code>simulation_results %&gt;% 
  group_by(term) %&gt;% 
  summarise(mean = mean(difference), 
            sd = sd(difference))</code></pre>
<pre><code>## # A tibble: 2 x 3
##   term     mean     sd
##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1 x1     0.0122 0.0370
## 2 x2    -0.0141 0.0306</code></pre>
<p>
Already with only 10 simulated datasets, the difference in means is not significant. Let’s rerun the analysis, but for difference sizes. In order to make things easier, we can put all the code into a nifty function:
</p>
<pre class="r"><code>monte_carlo &lt;- function(coefs, sample_size, repeats){
  many_datasets &lt;- generate_datasets(coefs, sample_size, repeats)
  
  results &lt;- many_datasets %&gt;% 
    mutate(lpm = modify_depth(simulations, 2, ~lm(y ~ ., data = .x))) %&gt;% 
    mutate(lpm = modify_depth(lpm, 2, broom::tidy)) %&gt;% 
    mutate(lpm = modify_depth(lpm, 2, ~select(., term, estimate))) %&gt;% 
    mutate(lpm = modify_depth(lpm, 2, ~filter(., term != "(Intercept)"))) %&gt;% 
    mutate(lpm = map(lpm, bind_rows)) %&gt;% 
    mutate(true_effect = modify_depth(simulations, 2, ~meffects(., coefs = coefs[[1]]))) %&gt;% 
    mutate(true_effect = map(true_effect, bind_rows))

  simulation_results &lt;- results %&gt;% 
    mutate(difference = map2(.x = lpm, .y = true_effect, bind_cols)) %&gt;% 
    mutate(difference = map(difference, ~mutate(., difference = true_effect - estimate))) %&gt;% 
    mutate(difference = map(difference, ~select(., term, difference))) %&gt;% 
    pull(difference) %&gt;% 
    .[[1]]

  simulation_results %&gt;% 
    group_by(term) %&gt;% 
    summarise(mean = mean(difference), 
              sd = sd(difference))
}</code></pre>
<p>
And now, let’s run the simulation for different parameters and sizes:
</p>
<pre class="r"><code>monte_carlo(c(.5, 2, 4), 100, 10)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   term      mean     sd
##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;
## 1 x1    -0.00826 0.0291
## 2 x2    -0.00732 0.0412</code></pre>
<pre class="r"><code>monte_carlo(c(.5, 2, 4), 100, 100)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   term     mean     sd
##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1 x1    0.00360 0.0392
## 2 x2    0.00517 0.0446</code></pre>
<pre class="r"><code>monte_carlo(c(.5, 2, 4), 100, 500)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   term       mean     sd
##   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;
## 1 x1    -0.00152  0.0371
## 2 x2    -0.000701 0.0423</code></pre>
<pre class="r"><code>monte_carlo(c(pi, 6, 9), 100, 10)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   term      mean     sd
##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;
## 1 x1    -0.00829 0.0546
## 2 x2     0.00178 0.0370</code></pre>
<pre class="r"><code>monte_carlo(c(pi, 6, 9), 100, 100)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   term     mean     sd
##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1 x1    0.0107  0.0608
## 2 x2    0.00831 0.0804</code></pre>
<pre class="r"><code>monte_carlo(c(pi, 6, 9), 100, 500)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   term     mean     sd
##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1 x1    0.00879 0.0522
## 2 x2    0.0113  0.0668</code></pre>
<p>
We see that, at least for this set of parameters, the LPM does a good job of estimating marginal effects.
</p>
<p>
Now, this study might in itself not be very interesting to you, but I believe the general approach is quite useful and flexible enough to be adapted to all kinds of use-cases.
</p>



 ]]></description>
  <category>R</category>
  <category>econometrics</category>
  <guid>https://b-rodrigues.github.io/posts/2019-08-14-lpm.html</guid>
  <pubDate>Wed, 14 Aug 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Statistical matching, or when one single data source is not enough</title>
  <link>https://b-rodrigues.github.io/posts/2019-07-19-statmatch.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Row_and_column_vectors"> <img src="https://b-rodrigues.github.io/assets/img/columns.jpg" title="Not that kind of columns" width="80%" height="auto"></a>
</p>
</div>
<p>
I was recently asked how to go about matching several datasets where different samples of individuals were interviewed. This sounds like a big problem; say that you have dataset A and B, and that A contain one sample of individuals, and B another sample of individuals, then how could you possibly match the datasets? Matching datasets requires a common identifier, for instance, suppose that A contains socio-demographic information on a sample of individuals I, while B, contains information on wages and hours worked on the same sample of individuals I, then yes, it will be possible to match/merge/join both datasets.
</p>
<p>
But that was not what I was asked about; I was asked about a situation where the same population gets sampled twice, and each sample answers to a different survey. For example the first survey is about labour market information and survey B is about family structure. Would it be possible to combine the information from both datasets?
</p>
<p>
To me, this sounded a bit like missing data imputation problem, but where all the information about the variables of interest was missing! I started digging a bit, and found that not only there was already quite some literature on it, there is even a package for this, called <code>{StatMatch}</code> with a very detailed <a href="https://cran.r-project.org/web/packages/StatMatch/vignettes/Statistical_Matching_with_StatMatch.pdf">vignette</a>. The vignette is so detailed, that I will not write any code, I just wanted to share this package!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-07-19-statmatch.html</guid>
  <pubDate>Fri, 19 Jul 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Curly-Curly, the successor of Bang-Bang</title>
  <link>https://b-rodrigues.github.io/posts/2019-06-20-tidy_eval_saga.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Row_and_column_vectors"> <img src="https://b-rodrigues.github.io/assets/img/curly.jpg" title="Not that kind of columns" width="80%" height="auto"></a>
</p>
</div>
<p>
Writing functions that take data frame columns as arguments is a problem that most R users have been confronted with at some point. There are different ways to tackle this issue, and this blog post will focus on the solution provided by the latest release of the <code>{rlang}</code> package. You can read the announcement <a href="https://www.tidyverse.org/articles/2019/06/rlang-0-4-0/">here</a>, which explains really well what was wrong with the old syntax, and how the new syntax works now.
</p>
<p>
I have written about the problem of writing functions that use data frame columns as arguments <a href="../posts/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions.html">three years ago</a> and <a href="../posts/2017-08-27-why_tidyeval.html">two year ago</a> too. <a href="../posts/2018-01-19-mapping_functions_with_any_cols.html">Last year</a>, I wrote a blog post that showed how to map a list of functions to a list of datasets with a list of columns as arguments that used the <code>!!quo(column_name)</code> syntax (the <code>!!</code> is pronounced <em>bang-bang</em>). Now, there is a new sheriff in town, <code>{{}}</code>, introduced in <code>{rlang}</code> version 0.4.0 that makes things even easier. The suggested pronunciation of <code>{{}}</code> is <em>curly-curly</em>, but there is no <a href="https://twitter.com/JonTheGeek/status/1144815369766547456">consensus yet</a>.
</p>
<p>
First, let’s load the <code>{tidyverse}</code>:
</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<p>
Let’s suppose that I need to write a function that takes a data frame, as well as a column from this data frame as arguments:
</p>
<pre class="r"><code>how_many_na &lt;- function(dataframe, column_name){
  dataframe %&gt;%
    filter(is.na(column_name)) %&gt;%
    count()
}</code></pre>
<p>
Let’s try this function out on the <code>starwars</code> data:
</p>
<pre class="r"><code>data(starwars)

head(starwars)</code></pre>
<pre><code>## # A tibble: 6 x 13
##   name  height  mass hair_color skin_color eye_color birth_year gender
##   &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; 
## 1 Luke…    172    77 blond      fair       blue            19   male  
## 2 C-3PO    167    75 &lt;NA&gt;       gold       yellow         112   &lt;NA&gt;  
## 3 R2-D2     96    32 &lt;NA&gt;       white, bl… red             33   &lt;NA&gt;  
## 4 Dart…    202   136 none       white      yellow          41.9 male  
## 5 Leia…    150    49 brown      light      brown           19   female
## 6 Owen…    178   120 brown, gr… light      blue            52   male  
## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,
## #   vehicles &lt;list&gt;, starships &lt;list&gt;</code></pre>
<p>
As you can see, there are missing values in the <code>hair_color</code> column. Let’s try to count how many missing values are in this column:
</p>
<pre class="r"><code>how_many_na(starwars, hair_color)</code></pre>
<pre><code>Error: object 'hair_color' not found</code></pre>
<p>
R cannot find the <code>hair_color</code> column, and yet it is in the data! Well, this is actually exactly the issue. The issue is that the column is inside the dataframe, but when calling the function with <code>hair_color</code> as the second argument, R is looking for a variable called <code>hair_color</code> that does not exist. What about trying with <code>"hair_color"</code>?
</p>
<pre class="r"><code>how_many_na(starwars, "hair_color")</code></pre>
<pre><code>## # A tibble: 1 x 1
##       n
##   &lt;int&gt;
## 1     0</code></pre>
<p>
Now we get something, but something wrong!
</p>
<p>
One way to solve this issue, is to not use the <code>filter()</code> function, and instead rely on base R:
</p>
<pre class="r"><code>how_many_na_base &lt;- function(dataframe, column_name){
  na_index &lt;- is.na(dataframe[, column_name])
  nrow(dataframe[na_index, column_name])
}

how_many_na_base(starwars, "hair_color")</code></pre>
<pre><code>## [1] 5</code></pre>
<p>
This works, but not using the <code>{tidyverse}</code> at all is not an option, at least for me. For instance, the next function, which uses a grouping variable, would be difficult to implement without the <code>{tidyverse}</code>:
</p>
<pre class="r"><code>summarise_groups &lt;- function(dataframe, grouping_var, column_name){
  dataframe %&gt;%
    group_by(grouping_var) %&gt;%  
    summarise(mean(column_name, na.rm = TRUE))
}</code></pre>
<p>
Calling this function results in the following error message:
</p>
<pre><code>Error: Column `grouping_var` is unknown</code></pre>
<p>
Before the release of <code>{rlang}</code> 0.4.0 this is was the solution:
</p>
<pre class="r"><code>summarise_groups &lt;- function(dataframe, grouping_var, column_name){

  grouping_var &lt;- enquo(grouping_var)
  column_name &lt;- enquo(column_name)
  mean_name &lt;- paste0("mean_", quo_name(column_name))

  dataframe %&gt;%
    group_by(!!grouping_var) %&gt;%  
    summarise(!!(mean_name) := mean(!!column_name, na.rm = TRUE))
}</code></pre>
<p>
The core of the function remained very similar to the version from before, but now one has to use the <code>enquo()</code>-<code>!!</code> syntax. While not overly difficult to use, it is cumbersome.
</p>
<p>
Now this can be simplified using the new <code>{{}}</code> syntax:
</p>
<pre class="r"><code>summarise_groups &lt;- function(dataframe, grouping_var, column_name){

  dataframe %&gt;%
    group_by({{grouping_var}}) %&gt;%  
    summarise({{column_name}} := mean({{column_name}}, na.rm = TRUE))
}</code></pre>
<p>
Much easier and cleaner! You still have to use the <code>:=</code> operator instead of <code>=</code> for the column name however. Also, from my understanding, if you want to modify the column names, for instance in this case return <code>"mean_height"</code> instead of <code>height</code> you have to keep using the <code>enquo()</code>-<code>!!</code> syntax.
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-06-20-tidy_eval_saga.html</guid>
  <pubDate>Sat, 29 Jun 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Intermittent demand, Croston and Die Hard</title>
  <link>https://b-rodrigues.github.io/posts/2019-06-12-intermittent.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/List_of_Christmas_films"> <img src="https://b-rodrigues.github.io/assets/img/diehard.jpg" title="Die Hard is the best Christmas movie" width="80%" height="auto"></a>
</p>
</div>
<p>
I have recently been confronted to a kind of data set and problem that I was not even aware existed: intermittent demand data. Intermittent demand arises when the demand for a certain good arrives sporadically. Let’s take a look at an example, by analyzing the number of downloads for the <code>{RDieHarder}</code> package:
</p>
<pre class="r"><code>library(tidyverse)
library(tsintermittent)
library(nnfor)
library(cranlogs)
library(brotools)</code></pre>
<pre class="r"><code>rdieharder &lt;- cran_downloads("RDieHarder", from = "2017-01-01")

ggplot(rdieharder) +
  geom_line(aes(y = count, x = date), colour = "#82518c") +
  theme_blog()</code></pre>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/intermittent-3-1.png" width="80%" height="auto">
</div>
<p>
Let’s take a look at just one month of data, because the above plot is not very clear, because of the outlier just before 2019… I wonder now, was that on Christmas day?
</p>
<pre class="r"><code>rdieharder %&gt;%
  filter(count == max(count))</code></pre>
<pre><code>##         date count    package
## 1 2018-12-21   373 RDieHarder</code></pre>
<p>
Not exactly on Christmas day, but almost! Anyways, let’s look at one month of data:
</p>
<pre class="r"><code>january_2018 &lt;- rdieharder %&gt;%
  filter(between(date, as.Date("2018-01-01"), as.Date("2018-02-01")))

ggplot(january_2018) +
  geom_line(aes(y = count, x = date), colour = "#82518c") +
  theme_blog()</code></pre>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/intermittent-5-1.png" width="80%" height="auto">
</div>
<p>
Now, it is clear that this will be tricky to forecast. There is no discernible pattern, no trend, no seasonality… nothing that would make it “easy” for a model to learn how to forecast such data.
</p>
<p>
This is typical intermittent demand data. Specific methods have been developed to forecast such data, the most well-known being Croston, as detailed in <a href="https://www.jstor.org/stable/3007885?seq=1#page_scan_tab_contents">this paper</a>. A function to estimate such models is available in the <code>{tsintermittent}</code> package, written by <a href="https://kourentzes.com/forecasting/2014/06/23/intermittent-demand-forecasting-package-for-r/">Nikolaos Kourentzes</a> who also wrote another package, <code>{nnfor}</code>, which uses Neural Networks to forecast time series data. I am going to use both to try to forecast the intermittent demand for the <code>{RDieHarder}</code> package for the year 2019.
</p>
<p>
Let’s first load these packages:
</p>
<pre class="r"><code>library(tsintermittent)
library(nnfor)</code></pre>
<p>
And as usual, split the data into training and testing sets:
</p>
<pre class="r"><code>train_data &lt;- rdieharder %&gt;%
  filter(date &lt; as.Date("2019-01-01")) %&gt;%
  pull(count) %&gt;%
  ts()

test_data &lt;- rdieharder %&gt;%
  filter(date &gt;= as.Date("2019-01-01"))</code></pre>
<p>
Let’s consider three models; a naive one, which simply uses the mean of the training set as the forecast for all future periods, Croston’s method, and finally a Neural Network from the <code>{nnfor}</code> package:
</p>
<pre class="r"><code>naive_model &lt;- mean(train_data)

croston_model &lt;- crost(train_data, h = 163)

nn_model &lt;- mlp(train_data, reps = 1, hd.auto.type = "cv")</code></pre>
<pre><code>## Warning in preprocess(y, m, lags, keep, difforder, sel.lag,
## allow.det.season, : No inputs left in the network after pre-selection,
## forcing AR(1).</code></pre>
<pre class="r"><code>nn_model_forecast &lt;- forecast(nn_model, h = 163)</code></pre>
<p>
The <code>crost()</code> function estimates Croston’s model, and the <code>h</code> argument produces the forecast for the next 163 days. <code>mlp()</code> trains a multilayer perceptron, and the <code>hd.auto.type = "cv"</code> argument means that 5-fold cross-validation will be used to find the best number of hidden nodes. I then obtain the forecast using the <code>forecast()</code> function. As you can read from the Warning message above, the Neural Network was replaced by an auto-regressive model, AR(1), because no inputs were left after pre-selection… I am not exactly sure what that means, but if I remove the big outlier from before, this warning message disappears, and a Neural Network is successfully trained.
</p>
<p>
In order to rank the models, I follow <a href="https://www.sciencedirect.com/science/article/pii/S0169207006000239">this paper</a> from Rob J. Hyndman, who wrote a very useful book titled <a href="https://otexts.com/fpp2/">Forecasting: Principles and Practice</a>, and use the Mean Absolute Scaled Error, or MASE. You can also read <a href="https://robjhyndman.com/papers/foresight.pdf">this shorter pdf</a> which also details how to use MASE to measure the accuracy for intermittent demand. Here is the function:
</p>
<pre class="r"><code>mase &lt;- function(train_ts, test_ts, outsample_forecast){

  naive_insample_forecast &lt;- stats::lag(train_ts)

  insample_mae &lt;- mean(abs(train_ts - naive_insample_forecast), na.rm = TRUE)
  error_outsample &lt;- test_ts - outsample_forecast

  ase &lt;- error_outsample / insample_mae
  mean(abs(ase), na.rm = TRUE)
}</code></pre>
<p>
It is now easy to compute the models’ accuracies:
</p>
<pre class="r"><code>mase(train_data, test_data$count, naive_model)</code></pre>
<pre><code>## [1] 1.764385</code></pre>
<pre class="r"><code>mase(train_data, test_data$count, croston_model$component$c.out[1])</code></pre>
<pre><code>## [1] 1.397611</code></pre>
<pre class="r"><code>mase(train_data, test_data$count, nn_model_forecast$mean)</code></pre>
<pre><code>## [1] 1.767357</code></pre>
<p>
Croston’s method is the one that performs best from the three. Maybe surprisingly, the naive method performs just as well as the Neural Network! (or rather, the AR(1) model) Let’s also plot the predictions with the true values from the test set:
</p>
<pre class="r"><code>test_data &lt;- test_data %&gt;%
  mutate(naive_model_forecast = naive_model,
         croston_model_forecast = croston_model$component$c.out[1],
         nn_model_forecast = nn_model_forecast$mean) %&gt;%
  select(-package) %&gt;%
  rename(actual_value = count)


test_data_longer &lt;- test_data %&gt;%
  gather(models, value,
         actual_value, naive_model_forecast, croston_model_forecast, nn_model_forecast)</code></pre>
<pre><code>## Warning: attributes are not identical across measure variables;
## they will be dropped</code></pre>
<pre class="r"><code>ggplot(test_data_longer) +
  geom_line(aes(y = value, x = date, colour = models)) +
  theme_blog()</code></pre>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/intermittent-13-1.png" width="80%" height="auto">
</div>
<p>
Just to make sure I didn’t make a mistake when writing the <code>mase()</code> function, let’s use the <code>accuracy()</code> function from the <code>{forecast}</code> package and compare the result for the Neural Network:
</p>
<pre class="r"><code>library(forecast)
accuracy(nn_model_forecast, x = test_data$actual_value)</code></pre>
<pre><code>##                       ME     RMSE      MAE  MPE MAPE      MASE       ACF1
## Training set 0.001929409 14.81196 4.109577  NaN  Inf 0.8437033 0.05425074
## Test set     8.211758227 12.40199 8.635563 -Inf  Inf 1.7673570         NA</code></pre>
<p>
The result is the same, so it does seem like the naive method is not that bad, actually! Now, in general, intermittent demand series have a lot of 0 values, which is not really the case here. I still think that the methodology fits to this particular data set.
</p>
<p>
How else would you have forecast this data? Let me know via twitter!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-06-12-intermittent.html</guid>
  <pubDate>Wed, 12 Jun 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Using cosine similarity to find matching documents: a tutorial using Seneca’s letters to his friend Lucilius</title>
  <link>https://b-rodrigues.github.io/posts/2019-06-04-cosine_sim.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Seneca_the_Younger"> <img src="https://b-rodrigues.github.io/assets/img/seneca.png" title="Seneca the Younger" width="80%" height="auto"></a>
</p>
</div>
<p>
Lately I’ve been interested in trying to cluster documents, and to find similar documents based on their contents. In this blog post, I will use <a href="https://en.wikisource.org/wiki/Moral_letters_to_Lucilius">Seneca’s <em>Moral letters to Lucilius</em></a> and compute the pairwise <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a> of his 124 letters. Computing the cosine similarity between two vectors returns how similar these vectors are. A cosine similarity of 1 means that the angle between the two vectors is 0, and thus both vectors have the same direction. Seneca’s Moral letters to Lucilius deal mostly with philosophical topics, as Seneca was, among many other things, a philosopher of the stoic school. The stoic school of philosophy is quite interesting, but it has been unfortunately misunderstood, especially in modern times. There is now a renewed interest for this school, see <a href="https://en.wikipedia.org/wiki/Modern_Stoicism">Modern Stoicism</a>.
</p>
<p>
The first step is to scrape the letters. The code below scrapes the letters, and saves them into a list. I first start by writing a function that gets the raw text. Note the <code>xpath</code> argument of the <code>html_nodes()</code> function. I obtained this complex expression by using the <a href="https://selectorgadget.com/">SelectorGadget</a> extension for Google Chrome, and then selecting the right element of the web page. See this <a href="https://i.imgur.com/2cntugt.png">screenshot</a> if my description was not very clear.
</p>
<p>
Then, the <code>extract_text()</code> function extracts the text from the letter. The only line that might be a bit complex is <code>discard(~<code>==</code>(., ""))</code> which removes every empty line.
</p>
<p>
Finally, there’s the <code>get_letter()</code> function that actually gets the letter by calling the first two functions. In the last line, I get all the letters into a list by mapping the list of urls to the <code>get_letter()</code> function.
</p>
<pre class="r"><code>library(tidyverse)
library(rvest)

base_url &lt;- "https://en.wikisource.org/wiki/Moral_letters_to_Lucilius/Letter_"

letter_numbers &lt;- seq(1, 124)

letter_urls &lt;- paste0(base_url, letter_numbers)

get_raw_text &lt;- function(base_url, letter_number){
  paste0(base_url, letter_number) %&gt;%
    read_html() %&gt;%
    html_nodes(xpath ='//*[contains(concat( " ", @class, " " ), concat( " ", "mw-parser-output", " " ))]') %&gt;%  
    html_text()
}


extract_text &lt;- function(raw_text, letter_number){
  raw_text &lt;- raw_text %&gt;%
    str_split("\n") %&gt;%  
    flatten_chr() %&gt;%  
    discard(~`==`(., ""))

  start &lt;- 5

  end &lt;- str_which(raw_text, "Footnotes*")

  raw_text[start:(end-1)] %&gt;%
    str_remove_all("\\[\\d{1,}\\]") %&gt;%
    str_remove_all("\\[edit\\]")
}

get_letter &lt;- function(base_url, letter_number){

  raw_text &lt;- get_raw_text(base_url, letter_number)

  extract_text(raw_text, letter_number)
}

letters_to_lucilius &lt;- map2(base_url, letter_numbers, get_letter)</code></pre>
<p>
Now that we have the letters saved in a list, we need to process the text a little bit. In order to compute the cosine similarity between the letters, I need to somehow represent them as vectors. There are several ways of doing this, and I am going to compute the tf-idf of each letter. The tf-idf will give me a vector for each letter, with zero and non-zero values. Zero values represent words that are common to all letters, and thus do not have any <em>predictive power</em>. Non-zero values are words that are not present in all letters, but maybe only a few. I expect that letters that discuss death for example, will have the word death in them, and letters that do not discuss death will not have this word. The word death thus has what I call <em>predictive power</em>, in that it helps us distinguish the letters discussing death from the other letters that do not discuss it. The same reasoning can be applied for any topic.
</p>
<p>
So, to get the tf-idf of each letter, I first need to put them in a tidy dataset. I will use the <code>{tidytext}</code> package for this. First, I load the required packages, convert each letter to a dataframe of one column that contains the text, and save the letter’s titles into another list:
</p>
<pre class="r"><code>library(tidytext)
library(SnowballC)
library(stopwords)
library(text2vec)

letters_to_lucilius_df &lt;- map(letters_to_lucilius, ~tibble("text" = .))

letter_titles &lt;- letters_to_lucilius_df %&gt;%
  map(~slice(., 1)) %&gt;%
  map(pull)</code></pre>
<p>
Now, I add this title to each dataframe as a new column, called title:
</p>
<pre class="r"><code>letters_to_lucilius_df &lt;-  map2(.x = letters_to_lucilius_df, .y = letter_titles,
                                ~mutate(.x, title = .y)) %&gt;%
  map(~slice(., -1))</code></pre>
<p>
I can now use <code>unnest_tokens()</code> to transform the datasets. Before, I had the whole text of the letter in one column. After using <code>unnest_tokens()</code> I now have a dataset with one row per word. This will make it easy to compute frequencies by letters, or what I am interested in, the tf-idf of each letter:
</p>
<pre class="r"><code>tokenized_letters &lt;- letters_to_lucilius_df %&gt;%
  bind_rows() %&gt;%
  group_by(title) %&gt;%
  unnest_tokens(word, text)</code></pre>
<p>
I can now remove stopwords, using the data containing in the <code>{stopwords}</code> package:
</p>
<pre class="r"><code>stopwords_en &lt;- tibble("word" = stopwords("en", source  = "smart"))

tokenized_letters &lt;- tokenized_letters %&gt;%
  anti_join(stopwords_en) %&gt;%
  filter(!str_detect(word, "\\d{1,}"))</code></pre>
<pre><code>## Joining, by = "word"</code></pre>
<p>
Next step, wordstemming, meaning, going from “dogs” to “dog”, or from “was” to “be”. If you do not do wordstemming, “dogs” and “dog” will be considered different words, even though they are not. <code>wordStem()</code> is a function from <code>{SnowballC}</code>.
</p>
<pre class="r"><code>tokenized_letters &lt;- tokenized_letters %&gt;%
  mutate(word = wordStem(word, language = "en"))</code></pre>
<p>
Finally, I can compute the tf-idf of each letter and cast the data as a sparse matrix:
</p>
<pre class="r"><code>tfidf_letters &lt;- tokenized_letters %&gt;%
  count(title, word, sort  = TRUE) %&gt;%
  bind_tf_idf(word, title, n)

sparse_matrix &lt;- tfidf_letters %&gt;%
  cast_sparse(title, word, tf)</code></pre>
<p>
Let’s take a look at the sparse matrix:
</p>
<pre class="r"><code>sparse_matrix[1:10, 1:4]</code></pre>
<pre><code>## 10 x 4 sparse Matrix of class "dgCMatrix"
##                                                                   thing
## CXIII. On the Vitality of the Soul and Its Attributes       0.084835631
## LXVI. On Various Aspects of Virtue                          0.017079890
## LXXXVII. Some Arguments in Favour of the Simple Life        0.014534884
## CXVII. On Real Ethics as Superior to Syllogistic Subtleties 0.025919732
## LXXVI. On Learning Wisdom in Old Age                        0.021588946
## CII. On the Intimations of Our Immortality                  0.014662757
## CXXIV. On the True Good as Attained by Reason               0.010139417
## XCIV. On the Value of Advice                                0.009266409
## LXXXI. On Benefits                                          0.007705479
## LXXXV. On Some Vain Syllogisms                              0.013254786
##                                                                     live
## CXIII. On the Vitality of the Soul and Its Attributes       0.0837751856
## LXVI. On Various Aspects of Virtue                          .           
## LXXXVII. Some Arguments in Favour of the Simple Life        0.0007267442
## CXVII. On Real Ethics as Superior to Syllogistic Subtleties 0.0050167224
## LXXVI. On Learning Wisdom in Old Age                        0.0025906736
## CII. On the Intimations of Our Immortality                  0.0019550342
## CXXIV. On the True Good as Attained by Reason               .           
## XCIV. On the Value of Advice                                0.0023166023
## LXXXI. On Benefits                                          0.0008561644
## LXXXV. On Some Vain Syllogisms                              0.0022091311
##                                                                   good
## CXIII. On the Vitality of the Soul and Its Attributes       0.01166490
## LXVI. On Various Aspects of Virtue                          0.04132231
## LXXXVII. Some Arguments in Favour of the Simple Life        0.04578488
## CXVII. On Real Ethics as Superior to Syllogistic Subtleties 0.04849498
## LXXVI. On Learning Wisdom in Old Age                        0.04663212
## CII. On the Intimations of Our Immortality                  0.05180841
## CXXIV. On the True Good as Attained by Reason               0.06717364
## XCIV. On the Value of Advice                                0.01081081
## LXXXI. On Benefits                                          0.01626712
## LXXXV. On Some Vain Syllogisms                              0.01472754
##                                                                 precept
## CXIII. On the Vitality of the Soul and Its Attributes       .          
## LXVI. On Various Aspects of Virtue                          .          
## LXXXVII. Some Arguments in Favour of the Simple Life        .          
## CXVII. On Real Ethics as Superior to Syllogistic Subtleties .          
## LXXVI. On Learning Wisdom in Old Age                        .          
## CII. On the Intimations of Our Immortality                  .          
## CXXIV. On the True Good as Attained by Reason               0.001267427
## XCIV. On the Value of Advice                                0.020463320
## LXXXI. On Benefits                                          .          
## LXXXV. On Some Vain Syllogisms                              .</code></pre>
<p>
We can consider each row of this matrix as the vector representing a letter, and thus compute the cosine similarity between letters. For this, I am using the <code>sim2()</code> function from the <code>{text2vec}</code> package. I then create the <code>get_similar_letters()</code> function that returns similar letters for a given reference letter:
</p>
<pre class="r"><code>similarities &lt;- sim2(sparse_matrix, method = "cosine", norm = "l2") 

get_similar_letters &lt;- function(similarities, reference_letter, n_recommendations = 3){
  sort(similarities[reference_letter, ], decreasing = TRUE)[1:(2 + n_recommendations)]
}</code></pre>
<pre class="r"><code>get_similar_letters(similarities, 19)</code></pre>
<pre><code>##          XXX. On Conquering the Conqueror 
##                                 1.0000000 
##                  XXIV. On Despising Death 
##                                 0.6781600 
##      LXXXII. On the Natural Fear of Death 
##                                 0.6639736 
## LXX. On the Proper Time to Slip the Cable 
##                                 0.5981706 
## LXXVIII. On the Healing Power of the Mind 
##                                 0.4709679</code></pre>
<pre class="r"><code>get_similar_letters(similarities, 99)</code></pre>
<pre><code>##                              LXI. On Meeting Death Cheerfully 
##                                                     1.0000000 
##                     LXX. On the Proper Time to Slip the Cable 
##                                                     0.5005015 
## XCIII. On the Quality, as Contrasted with the Length, of Life 
##                                                     0.4631796 
##                         CI. On the Futility of Planning Ahead 
##                                                     0.4503093 
##                              LXXVII. On Taking One's Own Life 
##                                                     0.4147019</code></pre>
<pre class="r"><code>get_similar_letters(similarities, 32)</code></pre>
<pre><code>##                                    LIX. On Pleasure and Joy 
##                                                   1.0000000 
##          XXIII. On the True Joy which Comes from Philosophy 
##                                                   0.4743672 
##                          CIX. On the Fellowship of Wise Men 
##                                                   0.4526835 
## XC. On the Part Played by Philosophy in the Progress of Man 
##                                                   0.4498278 
##         CXXIII. On the Conflict between Pleasure and Virtue 
##                                                   0.4469312</code></pre>
<pre class="r"><code>get_similar_letters(similarities, 101)</code></pre>
<pre><code>##                    X. On Living to Oneself 
##                                  1.0000000 
##          LXXIII. On Philosophers and Kings 
##                                  0.3842292 
##                  XLI. On the God within Us 
##                                  0.3465457 
##                       XXXI. On Siren Songs 
##                                  0.3451388 
## XCV. On the Usefulness of Basic Principles 
##                                  0.3302794</code></pre>
<p>
As we can see from these examples, this seems to be working quite well: the first title is the title of the reference letter, will the next 3 are the suggested letters. The problem is that my matrix is not in the right order, and thus reference letter 19 does not correspond to letter 19 of Seneca… I have to correct that, but not today.
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-06-04-cosine_sim.html</guid>
  <pubDate>Tue, 04 Jun 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The never-ending editor war (?)</title>
  <link>https://b-rodrigues.github.io/posts/2019-05-19-spacemacs.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Death_mask"> <img src="https://b-rodrigues.github.io/assets/img/typical_emacs_user.gif" title="typical emacs user working" width="80%" height="auto"></a>
</p>
</div>
<p>
The creation of this blog post was prompted by this tweet, asking an age-old question:
</p>
{{% tweet “1128981852558123008” %}}
<p>
This is actually a very important question, that I have been asking myself for a long time. An IDE, and plain text editors, are a very important tools to anyone writing code. Most working hours are spent within such a program, which means that one has to be careful about choosing the right one, and once a choice is made, one has, in my humble opinion, learn as many features of this program as possible to become as efficient as possible.
</p>
<p>
As you can notice from the tweet above, I suggested the use of <a href="http://spacemacs.org/">Spacemacs</a>… and my tweet did not get any likes or retweets (as of the 19th of May, sympathetic readers of this blog have liked the tweet). It is to set this great injustice straight that I decided to write this blog post.
</p>
<p>
Spacemacs is a strange beast; if vi and Emacs had a baby, it would certainly look like Spacemacs. So first of all, to understand what is Spacemacs, one has to know a bit about vi and Emacs.
</p>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/vim.png" width="100%" style="display: block; margin: auto;" height="auto">
</div>
<p>
vi is a text editor with 43 years of history now. You might have heard of Vim (Vi IMproved) which is a modern clone of vi, from 1991. More recently, another clone has been getting popular, Neovim, started in 2014. Whatever version of vi however, its basic way of functioning remains the same. vi is a modal editor, meaning that the user has to switch between different modes to work on a text file. When vi is first started, the program will be in <em>Normal</em> mode. In this mode, trying to type a word will likely result in nothing, or unexpected behaviour; unexpected, if you’re not familiar with vi. For instance, in <em>Normal</em> mode, typing <strong>j</strong> will not show the character <strong>j</strong> on your screen. Instead, this will move the cursor down one line. Typing <strong>p</strong> will paste, <strong>u</strong> will undo the last action, <strong>y</strong> will yank (copy) etc…
</p>
<p>
To type text, first, one has to enter <em>Insert</em> mode, by typing <strong>i</strong> while in <em>Normal</em> mode. Only then is it possible to write text. To go back to <em>Normal</em> mode, type <strong>ESC</strong>. Other modes are <em>Visual</em> mode (from <em>Normal</em> mode press <strong>v</strong>), which allows the user to select text and <em>Command-line</em> mode which can be entered by keying <strong>:</strong> from <em>Normal</em> mode and allows to enter commands.
</p>
<p>
Now you might be wondering why anyone would use such a convoluted way to type text. Well, this is because one can chain these commands quite easily to perform repetitive tasks very quickly. For instance, to delete a word, one types <strong>daw</strong> (in <em>Normal</em> mode), <strong>d</strong>elete <strong>a</strong> <strong>w</strong>ord. To delete the next 3 words, you can type <strong>3daw</strong>. To edit the text between, for instance, <strong>()</strong> you would type <strong>ci(</strong> (while in <em>Normal</em> mode and anywhere between the braces containing the text to edit), <strong>c</strong>hange <strong>i</strong>n <strong>(</strong>. Same logic applies for <strong>ci[</strong> for instance. Can you guess what <strong>ciw</strong> does? If you are in <em>Normal</em> mode, and you want to change the word the cursor is on, this command will erase the word and put you in <em>Insert</em> mode so that you can write the new word.
</p>
<p>
These are just basic reasons why vi (or its clones) are awesome. It is also possible to automate very long and complex tasks using macros. One starts a macro by typing <strong>q</strong> and then any letter of the alphabet to name it, for instance <strong>a</strong>. The user then performs the actions needed, types <strong>q</strong> again to stop the recording of the macro, and can then execute the macro with <strong><span class="citation"><span class="citation" data-cites="a">@a</span></span></strong>. If the user needs to execute the macro say, 10 times, <strong>10@‌‌a</strong> does the trick. It is possible to extend vi’s functionalities by using plugins, but more on that down below.
</p>
<p>
vi keybindings have inspired a lot of other programs. For instance, you can get extensions for popular web browsers that mimick vi keybindings, such as <a href="https://github.com/tridactyl/tridactyl">Tridayctl</a> for Firefox, or <a href="http://vimium.github.io/">Vivium</a> for Chromium (or Google Chrome). There are even browsers that are built from scratch with support for vi keybinds, such as my personal favorite, <a href="http://qutebrowser.org/">qutebrowser</a>. You can even go further and use a tiling window manager on GNU-Linux, for instance <a href="https://i3wm.org/">i3</a>, which I use, or <a href="https://xmonad.org/">xmonad</a>. You might need to configure those to behave more like vi, but it is possible. This means that by learning one set of keyboard shortcuts, (and the logic behind chaining the keystrokes to achieve what you want), you can master several different programs. This blog post only deals with the editor part, but as you can see, if you go down the rabbit hole enough, a new exciting world opens up.
</p>
<p>
I will show some common vi operations below, but before that let’s discuss Emacs.
</p>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/emacs.png" width="80%" style="display: block; margin: auto;" height="auto">
</div>
<p>
I am not really familiar with Emacs; I know that Emacs users only swear by it (just like vi users only swear by vi), and that Emacs is not a modal editor. However, it contains a lot of functions that you can use by pressing <strong>ESC</strong>, <strong>CTRL</strong>, <strong>ALT</strong> or <strong>META</strong> (<strong>META</strong> is the Windows key on a regular PC keyboard) followed by regular keys. So the approach is different, but it is widely accepted that productivity of proficient Emacs users is very high too. Emacs was started in 1985, and the most popular clone is GNU Emacs. Emacs also features modes, but not in the same sense as vi. There are major and minor modes. For instance, if you’re editing a Python script, Emacs will be in Python mode, or if editing a Markdown file Emacs will be in Markdown mode. This will change the available functions to the user, as well as provide other niceties, such as auto-completion. Emacs is also easily extensible, which is another reason why it is so popular. Users can install packages for Emacs, just like R users would do for R, to extend Emacs’ capabilities. For instance, a very important package if you plan to use Emacs for statistics or data science is <code>ESS</code>, <code>E</code>macs <code>S</code>peaks <code>S</code>tatistics. Emacs contains other very high quality packages, and it seems to me (but don’t quote me on that) that Emacs’ packages are more mature and feature-rich than vi’s plugins. However, vi keybindings are really awesome. This is, I believe, what <a href="https://twitter.com/syl20bnr">Sylvain Benner</a> was thinking when he developed Spacemacs.
</p>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/spacemacs.png" width="30%" style="display: block; margin: auto;" height="auto">
</div>
<p>
Spacemacs’ motto is that <em>The best editor is neither Emacs nor Vim, it’s Emacs and Vim!</em>. Spacemacs is a version, or distribution of Emacs, that has a very specific way of doing things. However, since it’s built on top of Emacs, all of Emacs’ packages are available to the user, notably <em>Evil</em>, which is a package that makes Emacs mimick vi’s modal mode and keybindings (the name of this package tells you everything you need to know about what Emacs users think of vi users 😀)
</p>
<p>
Not only does Spacemacs support Emacs packages, but Spacemacs also features so-called <em>layers</em>, which are configuration files that integrate one, or several packages, seamlessly into Spacemacs particular workflow. This particular workflow is what gave Spacemacs its name. Instead of relying on <strong>ESC</strong>, <strong>CTRL</strong>, <strong>ALT</strong> or <strong>META</strong> like Emacs, users can launch functions by typing <strong>Space</strong> in <em>Normal</em> mode and then a sequence of letters. For instance, <strong>Spaceqr</strong> restarts Spacemacs. And what’s more, you don’t actually need to learn these new key sequences. When you type <strong>Space</strong>, the minibuffer, a little popup window at the bottom of Spacemacs, appears and shows you all the options that you can type. For instance, typing <strong>b</strong> after <strong>Space</strong> opens up the buffer menu. Buffers are what could be called tabs in Rstudio. Here you can chose to <em>delete</em> a buffer, with <strong>d</strong>, create a new buffer with <strong>N</strong>, and many more options.
</p>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/minibuffer.png" width="100%" style="display: block; margin: auto;" height="auto">
</div>
<p>
Enough text, let’s get into the videos. But keep in mind the following: the videos below show the keystrokes I am typing to perform the actions. However, because I use the BÉPO keyboard layout, which is the french equivalent of the DVORAK layout, the keystrokes will be different than those in a regular vi guide, which are mainly written for the QWERTY layout. Also, to use Spacemacs for R, you need to enable the <strong>ESS</strong> layer, which I show how to do at the end. Enabling this layer will turn on auto-completion, as well as provide documentation in real time for your function in the minibuffer:
</p>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/spacemacs_autocompletion.png" style="display: block; margin: auto;" width="80%" height="auto">
</div>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/spacemacs_doc.png" style="display: block; margin: auto;" width="80%" height="auto">
</div>
<p>
The first video shows Spacemacs divided into two windows. On the left, I am navigating around code using the <strong>T</strong> (move down) and <strong>S</strong> (move up) keys. To execute a region that I select, I type <strong>Spacemrr</strong> (this stands for <strong>M</strong>ajor mode <strong>R</strong>un <strong>R</strong>egion). Then around second 5, I key <strong>O</strong> which switches to <em>Insert</em> mode one line below the line I was, type <code>head(mtcars)</code> and then <strong>ESC</strong> to switch back to <em>Normal</em> mode and run the line with <strong>Spacemrl</strong> (<strong>M</strong>ajor mode <strong>R</strong>un <strong>L</strong>ine).
</p>
<div style="text-align:center;">
<video width="80%" height="auto" controls="">
<source src="../assets/img/spacemacs_01_running_lines.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
In this video, I show you how to switch between windows. Type <strong>SpaceN</strong> to switch to window N. At the end, I key <strong>dd</strong> which deletes a whole line.
</p>
<div style="text-align:center;">
<video width="80%" height="auto" controls="">
<source src="../assets/img/spacemacs_02_switching_windows.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
In the video below, I show how to use the pipe operator with <strong>Spacemm</strong>. This is a keyboard shortcut that I have defined myself. You can also spot the auto-completion at work in this video. To run the code, I first select it with <strong>V</strong>, which selects the whole line the cursor is currently at and enters <em>Visual</em> mode. I then select the lines below with <strong>T</strong> and run the region with <strong>Spacemrr</strong>.
</p>
<div style="text-align:center;">
<video width="80%" height="auto" controls="">
<source src="../assets/img/spacemacs_03_pipe.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Here I show how plotting behaves. When a plot is created, a new window is opened with the plot. This is a major shortcoming of using Spacemacs for R programming; there is not a dedicated buffer for plots, and it only shows the very last one created, so there is no way to keep all the plots created in the current session in a neat, dedicated buffer. It seems to be possible using <a href="https://github.com/erikriverson/org-mode-R-tutorial/blob/master/org-mode-R-tutorial.org">Org-mode</a>, which is an Emacs mode for writing notes, todos, and authoring documents. But I haven’t explored this option yet, mainly because in my case, only looking at one plot at a time is ok.
</p>
<div style="text-align:center;">
<video width="80%" height="auto" controls="">
<source src="../assets/img/spacemacs_04_ggplot.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Here I show how to quickly add text to the top of the document when at the cursor is at the bottom: I try to use the <code>tabyl()</code> function found in the <code>{janitor}</code> package, which I forgot to load. I quickly go all the way up with <strong>gg</strong>, then key <strong>yy</strong> to copy the first line, then <strong>P</strong> to paste it on the line below (<strong>p</strong> would paste it on the same line), type <strong>fv</strong>, to <strong>f</strong>ind the letter v from the word “tidyverse”, then type <strong>liw</strong> (which is the BÉPO equivalent of <strong>ciw</strong> for <strong>C</strong>hange <strong>I</strong>n <strong>W</strong>ord) and finally change “tidyverse” to “janitor”. This seems overly complex, but once you get used to this way of working, you will wonder why you hadn’t tried vi sooner.
</p>
<div style="text-align:center;">
<video width="80%" height="auto" controls="">
<source src="../assets/img/spacemacs_05_janitor.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Here I show how to do block comment. <strong>8gg</strong> jumps to the 8th line, <strong>CTRLv</strong> starts block visual mode, which allows me to select a block of text. I select the first column of the text, <strong>G</strong> to jump all the way down, then <strong>A</strong> to enter insert mode at the end of the selection (actually, it would have been more logical to use <strong>I</strong>, which enters insert mode at the beginning of the selection) of the line and then add “#” to comment.
</p>
<div style="text-align:center;">
<video width="80%" height="auto" controls="">
<source src="../assets/img/spacemacs_06_block_comment.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Here I show how to delete a block of text:
</p>
<div style="text-align:center;">
<video width="80%" height="auto" controls="">
<source src="../assets/img/spacemacs_07_block_delete.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Search and replace, by entering <em>command-line</em> mode (look at the very bottom of the window):
</p>
<div style="text-align:center;">
<video width="80%" height="auto" controls="">
<source src="../assets/img/spacemacs_08_search_replace_undo.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
I forgot to add “,” characters on a bunch of lines. I add the first “,” to the first line, go down and press <strong>ESC</strong> to exit <em>Insert</em> mode. Now in <strong>Normal</strong> mode, I type <strong>.</strong> to execute the last command, which is <em>inserting a “,” character and going down a line</em>. This <em>dot command</em> is a feature of vi, and it will always redo the last performed change.
</p>
<div style="text-align:center;">
<video width="80%" height="auto" controls="">
<source src="../assets/img/spacemacs_09_dot.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
But instead of typing <strong>.</strong> six times, just type <strong>6.</strong> and be done with it:
</p>
<div style="text-align:center;">
<video width="80%" height="auto" controls="">
<source src="../assets/img/spacemacs_09b_repeated_dot.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
What if you want to do something more complex, involving several commands? Here the <em>dot command</em> won’t be enough, since it only replicates the last command, not more. For this you can define macros with **<span class="citation"><span class="citation" data-cites="*">@*</span></span>*. I look for the “,” character, twice, and put the rest of the characters in the next line with enter. I then repeat this operation by executing the macro using <strong>@‌‌a</strong> repeatedly (<strong>@‌‌a</strong> because I saved the actions in <strong>a</strong>, but it could have been any other letter). I then undo my changes and execute the macro 5 times with <strong>5@‌‌a</strong>.
</p>
<div style="text-align:center;">
<video width="80%" height="auto" controls="">
<source src="../assets/img/spacemacs_10_macros.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Here I show the undo tree (by typing <strong>Spaceua</strong>), which is a feature Spacemacs inherited from Emacs: it makes undoing changes and going back to a previous version of your script very easily:
</p>
<div style="text-align:center;">
<video width="80%" height="auto" controls="">
<source src="../assets/img/spacemacs_11_undo_tree.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Finally, I show my Spacemacs configuration file. I show where one needs to specify the layers one wishes to use. For R, the ESS layer (which is a configuration file for the ESS Emacs package) is mandatory. As I explained above, it is also possible to use Emacs packages for which no layer is available. These are the packages under <code>dotspacemacs-additional-packages</code>. In my case I use:
</p>
<pre><code>dotspacemacs-additional-packages '(polymode
                                  poly-R
                                  poly-noweb
                                  poly-markdown)</code></pre>
<p>
which makes working with RMarkdown possible. <code>polymode</code> enables simultaneous Major modes, which is needed for RMarkdown (because RMarkdown files mix Markdown and R).
</p>
<div style="text-align:center;">
<video width="80%" height="auto" controls="">
<source src="../assets/img/spacemacs_12_config.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
That’s the end of this long post. Spacemacs is really a joy to use, but the learning curve is quite steep. However, it is definitely worth it. There are so many packages available for Emacs (and hence Spacemacs) that allow you to browse the web, play games, listen to music, send and read emails… that a recurrent joke is that Emacs is <em>a very nice operating system, but it lacks a decent editor</em>. If that’s the case, Spacemacs is the perfect operating system, because it includes the greatest editor, vi.
</p>
<p>
If you’re interested and and want to learn more about vi, I advise you to read the following book <a href="https://www.ossblog.org/wp-content/uploads/2017/06/vim-recipes.pdf">Vim Recipes</a> (pdf warning, free) or <a href="https://pragprog.com/book/dnvim2/practical-vim-second-edition">Practical Vim, Edit Text at the Speed of thought</a> (not free, but worth every cent), and <a href="https://leanpub.com/VimLikeAPro">Use Vim Like a Pro</a>, which I have not read, but it looks quite good, and is free too if you want. Now this only covers the vi part, not the Emacs aspects of Spacemacs, but you don’t really need to know about Emacs to use Spacemacs. I had 0 experience with Emacs, and still have 0 experience with it. I only learned how to configure Spacemacs, which does not require any previous experience. To find the packages you need, as usual, use any search engine of your liking.
</p>
<p>
The last point I want to address is the built-in Vim mode of Rstudio. While it works, it does not work 100% as regular Vim, and worst of all, does not support, as far as I know, any other keyboard layout than QWERTY, which is a nogo for me.
</p>
<p>
In any case, if you’re looking to learn something new that you can use for many programs, including Rstudio, learn Vim, and then give Spacemacs a try. Chaining keystrokes to edit text gets addictive very quickly.
</p>
<p>
For reference, here is my <code>dotspacemacs/user-config</code>, which is where I defined the shortcut for the <code>%&gt;%</code> operator.
</p>
<pre><code>(defun dotspacemacs/user-config ()
  "Configuration for user code:
This function is called at the very end of Spacemacs startup, after layer
configuration.
Put your configuration code here, except for variables that should be set
before packages are loaded."
;;; R modes
  (add-to-list 'auto-mode-alist '("\\.md" . poly-markdown-mode))
  (add-to-list 'auto-mode-alist '("\\.Snw" . poly-noweb+r-mode))
  (add-to-list 'auto-mode-alist '("\\.Rnw" . poly-noweb+r-mode))
  (add-to-list 'auto-mode-alist '("\\.Rmd" . poly-markdown+r-mode))

  ;; (require 'poly-R)
  ;; (require 'poly-markdown)
  ;; (add-to-list 'auto-mode-alist '("\\.Rmd" . poly-markdown+r-mode))

  (global-company-mode t)
  (global-hl-line-mode 1) ; Enable/Disable current line highlight
  (setq-default fill-column 99)
  (setq-default auto-fill-mode t)
  ;; ESS shortcuts
  (spacemacs/set-leader-keys "mdt" 'ess-r-devtools-test-package)
  (spacemacs/set-leader-keys "mrl" 'ess-eval-line)
  (spacemacs/set-leader-keys "mrr" 'ess-eval-region)
  (spacemacs/set-leader-keys "mdb" 'ess-r-devtools-build-package)
  (spacemacs/set-leader-keys "mdd" 'ess-r-devtools-document-package)
  (spacemacs/set-leader-keys "mdl" 'ess-r-devtools-load-package)
  (spacemacs/set-leader-keys "mdc" 'ess-r-devtools-check-package)
  (spacemacs/set-leader-keys "mdp" 'ess-r-package-mode)
  (add-hook 'ess-mode-hook
            (lambda ()
              (ess-toggle-underscore nil)))
  (define-key evil-normal-state-map (kbd "SPC mm")
            (lambda ()
              (interactive)
              (insert " %&gt;% ")
              (evil-insert-state)
              ))
  ;; Move lines around
  (spacemacs/set-leader-keys "MS" 'move-text-line-up)
  (spacemacs/set-leader-keys "MT" 'move-text-line-down)
  (setq-default whitespace-mode t)
  (setq-default whitespace-style (quote (spaces tabs newline space-mark tab-mark newline-mark)))
  (setq-default whitespace-display-mappings
        ;; all numbers are Unicode codepoint in decimal. try (insert-char 182 ) to see it
        '(
          (space-mark 32 [183] [46]) ; 32 SPACE, 183 MIDDLE DOT 「·」, 46 FULL STOP 「.」
          (newline-mark 10 [9226 10]) ; 10 LINE FEED
          (tab-mark 9 [9655 9] [92 9]) ; 9 TAB, 9655 WHITE RIGHT-POINTING TRIANGLE 「▷」
          ))
  (setq-default TeX-view-program-selection
         '((output-pdf "PDF Viewer")))
  (setq-default TeX-view-program-list
        '(("PDF Viewer" "okular %o")))
  (setq-default indent-tabs-mode nil)
  (setq-default tab-width 2)
   ;; (setq org-default-notes-file (concat org-directory "/agenda/notes.org"))
   (add-hook 'prog-mode-hook 'spacemacs/toggle-fill-column-indicator-on)
   (add-hook 'text-mode-hook 'spacemacs/toggle-fill-column-indicator-on)
   (add-hook 'markdown-mode-hook 'spacemacs/toggle-fill-column-indicator-on)
  )</code></pre>


 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-05-19-spacemacs.html</guid>
  <pubDate>Sun, 19 May 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>For posterity: install {xml2} on GNU/Linux distros</title>
  <link>https://b-rodrigues.github.io/posts/2019-05-18-xml2.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Death_mask"> <img src="https://b-rodrigues.github.io/assets/img/napoleon_death_mask.jpg" title="I will probably be the only reader of this blog post" width="80%" height="auto"></a>
</p>
</div>
<p>
Today I’ve removed my system’s R package and installed MRO instead. While re-installing all packages, I’ve encountered one of the most frustrating error message for someone installing packages from source:
</p>
<pre><code>Error : /tmp/Rtmpw60aCp/R.INSTALL7819efef27e/xml2/man/read_xml.Rd:47: unable to load shared object
'/usr/lib64/R/library/xml2/libs/xml2.so': 
libicui18n.so.58: cannot open shared object file: No such file or directory ERROR: 
installing Rd objects failed for package ‘xml2’ </code></pre>
<p>
This library, <code>libicui18n.so.58</code> is a pain in the butt. However, you can easily install it if you install miniconda. After installing miniconda, you can look for it with:
</p>
<pre><code>[19-05-18 18:26] cbrunos in ~/ ➤ locate libicui18n.so.58

/home/cbrunos/miniconda3/lib/libicui18n.so.58
/home/cbrunos/miniconda3/lib/libicui18n.so.58.2
/home/cbrunos/miniconda3/pkgs/icu-58.2-h9c2bf20_1/lib/libicui18n.so.58
/home/cbrunos/miniconda3/pkgs/icu-58.2-h9c2bf20_1/lib/libicui18n.so.58.2
</code></pre>
<p>
So now you need to tell R where to look for this library. The <a href="https://stackoverflow.com/a/47851648">following Stackoverflow</a> answer saved the day. Add the following lines to <code>R_HOME/etc/ldpaths</code> (in my case, it was in <code>/opt/microsoft/ropen/3.5.2/lib64/R/etc/</code>):
</p>
<pre><code>LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/username/miniconda3/lib/
export LD_LIBRARY_PATH</code></pre>
<p>
and try to install <code>xml2</code> again, and it should work! If not, just abandon the idea of using R and switch to doing data science with VBA, it’ll be less frustrating.
</p>
<p>
Something else, if you install Microsoft R Open, you’ll be stuck with some older packages, because by default MRO uses a snapshot of CRAN from a given day as a mirror. To get the freshest packages, add the following line to your <code>.Rprofile</code> file (which should be located in your <code>HOME</code>):
</p>
<pre><code>options(repos = c(CRAN = "http://cran.rstudio.com/"))</code></pre>
<p>
And to finish this short blog post, add the following line to your <code>.Rprofile</code> if you get the following error messages when trying to install a package from github:
</p>
<pre><code>remotes::install_github('rstudio/DT') Downloading GitHub repo rstudio/DT@master tar: 
This does not look like a tar archive gzip: stdin: unexpected end of file tar: Child returned 
status 1 tar: Error is not recoverable: exiting now tar: This does not look like a tar archive 
gzip: stdin: unexpected end of file tar: Child returned status 1 tar: Error is not recoverable: 
exiting now Error in getrootdir(untar(src, list = TRUE)) : length(file_list) &gt; 0 is not TRUE Calls: 
&lt;Anonymous&gt; ... source_pkg -&gt; decompress -&gt; getrootdir -&gt; stopifnot In addition: Warning messages: 1: 
In utils::untar(tarfile, ...) : ‘tar -xf '/tmp/RtmpitCFRe/file2677442609b8.tar.gz' -C 
'/tmp/RtmpitCFRe/remotes267752f2629f'’ returned error code 2 2: 
In system(cmd, intern = TRUE) : running command 'tar -tf '/tmp/RtmpitCFRe/file2677442609b8.tar.gz'' 
had status 2 Execution halted</code></pre>
<p>
The solution, which can found <a href="https://github.com/r-lib/remotes/issues/350#issuecomment-493649792">here</a>
</p>
<pre><code>options("download.file.method" = "libcurl")</code></pre>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-05-18-xml2.html</guid>
  <pubDate>Sat, 18 May 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Fast food, causality and R packages, part 2</title>
  <link>https://b-rodrigues.github.io/posts/2019-05-04-diffindiff_part2.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Joke"> <img src="https://b-rodrigues.github.io/assets/img/distracted_economist.jpg" title="Soon, humanity will only communicate in memes" width="80%" height="auto"></a>
</p>
</div>
<p>
I am currently working on a package for the R programming language; its initial goal was to simply distribute the data used in the Card and Krueger 1994 paper that you can read <a href="http://davidcard.berkeley.edu/papers/njmin-aer.pdf">here</a> (PDF warning). However, I decided that I would add code to perform diff-in-diff.
</p>
<p>
In my <a href="https://www.brodrigues.co/blog/2019-04-28-diffindiff_part1/">previous blog post</a> I showed how to set up the structure of your new package. In this blog post, I will only focus on getting Card and Krueger’s data and prepare it for distribution. The next blog posts will focus on writing a function to perform difference-in-differences.
</p>
<p>
If you want to distribute data through a package, you first need to use the <code>usethis::use_data_raw()</code> function (as shown in part 1).
</p>
<p>
This creates a <code>data-raw</code> folder, and inside you will find the <code>DATASET.R</code> script. You can edit this script to prepare the data.
</p>
<p>
First, let’s download the data from Card’s website, unzip it and load the data into R. All these operations will be performed from R:
</p>
<pre class="r"><code>library(tidyverse)

tempfile_path &lt;- tempfile()

download.file("http://davidcard.berkeley.edu/data_sets/njmin.zip", destfile = tempfile_path)

tempdir_path &lt;- tempdir()

unzip(tempfile_path, exdir = tempdir_path)</code></pre>
<p>
To download and unzip a file from R, first, you need to define where you want to save the file. Because I am not interested in keeping the downloaded file, I use the <code>tempfile()</code> function to get a temporary file in my <code>/tmp/</code> folder (which is the folder that contains temporary files and folders in a GNU+Linux system). Then, using <code>download.file()</code> I download the file, and save it in my temporary file. I then create a temporary directory using <code>tempdir()</code> (the idea is the same as with <code>tempfile()</code>), and use this folder to save the files that I will unzip, using the <code>unzip()</code> function. This folder now contains several files:
</p>
<pre><code>check.sas
codebook
public.csv
read.me
survey1.nj
survey2.nj</code></pre>
<p>
<code>check.sas</code> is the SAS script Card and Krueger used. It’s interesting, because it is quite simple, quite short (170 lines long) and yet the impact of Card and Krueger’s research was and has been very important for the field of econometrics. This script will help me define my own functions. <code>codebook</code>, you guessed it, contains the variables’ descriptions. I will use this to name the columns of the data and to write the dataset’s documentation.
</p>
<p>
<code>public.csv</code> is the data. It does not contain any column names:
</p>
<pre><code> 46 1 0 0 0 0 0 1 0 0  0 30.00 15.00  3.00   .    19.0   .   1    .  2  6.50 16.50  1.03  1.03  0.52  3  3 1 1 111792  1  3.50 35.00  3.00  4.30  26.0  0.08 1 2  6.50 16.50  1.03   .    0.94  4  4    
 49 2 0 0 0 0 0 1 0 0  0  6.50  6.50  4.00   .    26.0   .   0    .  2 10.00 13.00  1.01  0.90  2.35  4  3 1 1 111292  .  0.00 15.00  4.00  4.45  13.0  0.05 0 2 10.00 13.00  1.01  0.89  2.35  4  4    
506 2 1 0 0 0 0 1 0 0  0  3.00  7.00  2.00   .    13.0  0.37 0  30.0 2 11.00 10.00  0.95  0.74  2.33  3  3 1 1 111292  .  3.00  7.00  4.00  5.00  19.0  0.25 . 1 11.00 11.00  0.95  0.74  2.33  4  3    
 56 4 1 0 0 0 0 1 0 0  0 20.00 20.00  4.00  5.00  26.0  0.10 1   0.0 2 10.00 12.00  0.87  0.82  1.79  2  2 1 1 111492  .  0.00 36.00  2.00  5.25  26.0  0.15 0 2 10.00 12.00  0.92  0.79  0.87  2  2    
 61 4 1 0 0 0 0 1 0 0  0  6.00 26.00  5.00  5.50  52.0  0.15 1   0.0 3 10.00 12.00  0.87  0.77  1.65  2  2 1 1 111492  . 28.00  3.00  6.00  4.75  13.0  0.15 0 2 10.00 12.00  1.01  0.84  0.95  2  2    
 62 4 1 0 0 0 0 1 0 0  2  0.00 31.00  5.00  5.00  26.0  0.07 0  45.0 2 10.00 12.00  0.87  0.77  0.95  2  2 1 1 111492  .   .     .     .     .    26.0   .   0 2 10.00 12.00   .    0.84  1.79  3  3    </code></pre>
<p>
Missing data is defined by <code>.</code> and the delimiter is the space character. <code>read.me</code> is a README file. Finally, <code>survey1.nj</code> and <code>survey2.nj</code> are the surveys that were administered to the fast food restaurants’ managers; one in February (before the raise) and the second one in November (after the minimum wage raise).
</p>
<p>
The next lines import the codebook:
</p>
<pre class="r"><code>codebook &lt;- read_lines(file = paste0(tempdir_path, "/codebook"))

variable_names &lt;- codebook %&gt;%
    `[`(8:59) %&gt;%
    `[`(-c(5, 6, 13, 14, 32, 33)) %&gt;%
    str_sub(1, 13) %&gt;%
    str_squish() %&gt;%
    str_to_lower()</code></pre>
<p>
Once I import the codebook, I select lines 8 to 59 using the <code><code>[</code>()</code> function. If you’re not familiar with this notation, try the following in a console:
</p>
<pre class="r"><code>seq(1, 100)[1:10]</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10</code></pre>
<p>
and compare:
</p>
<pre class="r"><code>seq(1, 100) %&gt;% 
  `[`(., 1:10)</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10</code></pre>
<p>
both are equivalent, as you can see. You can also try the following:
</p>
<pre class="r"><code>1 + 10</code></pre>
<pre><code>## [1] 11</code></pre>
<pre class="r"><code>1 %&gt;% 
  `+`(., 10)</code></pre>
<pre><code>## [1] 11</code></pre>
<p>
Using the same trick, I remove lines that I do not need, and then using <code>stringr::str_sub(1, 13)</code> I only keep the first 13 characters (which are the variable names, plus some white space characters) and then, to remove all the unneeded white space characters I use <code>stringr::squish()</code>, and then change the column names to lowercase.
</p>
<p>
I then load the data, and add the column names that I extracted before:
</p>
<pre class="r"><code>dataset &lt;- read_table2(paste0(tempdir_path, "/public.dat"),
                      col_names = FALSE)

dataset &lt;- dataset %&gt;%
    select(-X47) %&gt;%
    `colnames&lt;-`(., variable_names) %&gt;%
    mutate_all(as.numeric) %&gt;%
    mutate(sheet = as.character(sheet))</code></pre>
<p>
I use the same trick as before. I rename the 47th column, which is empty, I name the columns with <code><code>colnames&amp;lt;-</code>()</code>.
</p>
<p>
After this, I perform some data cleaning. It’s mostly renaming categories of categorical variables, and creating a “true” panel format. Several variables were measured at several points in time. Variables that were measured a second time have a “2” at the end of their name. I remove these variables, and add an observation data variable. So my data as twice as many rows as the original data, but that format makes it way easier to work with. Below you can read the full code:
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code>dataset &lt;- dataset %&gt;%
    mutate(chain = case_when(chain == 1 ~ "bk",
                             chain == 2 ~ "kfc",
                             chain == 3 ~ "roys",
                             chain == 4 ~ "wendys")) %&gt;%
    mutate(state = case_when(state == 1 ~ "New Jersey",
                             state == 0 ~ "Pennsylvania")) %&gt;%
    mutate(region = case_when(southj == 1 ~ "southj",
              centralj == 1 ~ "centralj",
              northj == 1 ~ "northj",
              shore == 1 ~ "shorej",
              pa1 == 1 ~ "pa1",
              pa2 == 1 ~ "pa2")) %&gt;%
    mutate(meals = case_when(meals == 0 ~ "None",
                             meals == 1 ~ "Free meals",
                             meals == 2 ~ "Reduced price meals",
                             meals == 3 ~ "Both free and reduced price meals")) %&gt;%
    mutate(meals2 = case_when(meals2 == 0 ~ "None",
                             meals2 == 1 ~ "Free meals",
                             meals2 == 2 ~ "Reduced price meals",
                             meals2 == 3 ~ "Both free and reduced price meals")) %&gt;%
    mutate(status2 = case_when(status2 == 0 ~ "Refused 2nd interview",
                               status2 == 1 ~ "Answered 2nd interview",
                               status2 == 2 ~ "Closed for renovations",
                               status2 == 3 ~ "Closed permanently",
                               status2 == 4 ~ "Closed for highway construction",
                               status2 == 5 ~ "Closed due to Mall fire")) %&gt;%
    mutate(co_owned = if_else(co_owned == 1, "Yes", "No")) %&gt;%
    mutate(bonus = if_else(bonus == 1, "Yes", "No")) %&gt;%
    mutate(special2 = if_else(special2 == 1, "Yes", "No")) %&gt;%
    mutate(type2 = if_else(type2 == 1, "Phone", "Personal")) %&gt;%
    select(sheet, chain, co_owned, state, region, everything()) %&gt;%
    select(-southj, -centralj, -northj, -shore, -pa1, -pa2) %&gt;%
    mutate(date2 = lubridate::mdy(date2)) %&gt;%
    rename(open2 = open2r) %&gt;%
    rename(firstinc2 = firstin2)

dataset1 &lt;- dataset %&gt;%
    select(-ends_with("2"), -sheet, -chain, -co_owned, -state, -region, -bonus) %&gt;%
    mutate(type = NA_character_,
           status = NA_character_,
           date = NA)

dataset2 &lt;- dataset %&gt;%
    select(ends_with("2")) %&gt;%
    #mutate(bonus = NA_character_) %&gt;%
    rename_all(~str_remove(., "2"))

other_cols &lt;- dataset %&gt;%
    select(sheet, chain, co_owned, state, region, bonus)

other_cols_1 &lt;- other_cols %&gt;%
    mutate(observation = "February 1992")

other_cols_2 &lt;- other_cols %&gt;%
    mutate(observation = "November 1992")

dataset1 &lt;- bind_cols(other_cols_1, dataset1)
dataset2 &lt;- bind_cols(other_cols_2, dataset2)

njmin &lt;- bind_rows(dataset1, dataset2) %&gt;%
    select(sheet, chain, state, region, observation, everything())</code></pre>
</details>
<p>
The line I would like to comment is the following:
</p>
<pre class="r"><code>dataset %&gt;%
    select(-ends_with("2"), -sheet, -chain, -co_owned, -state, -region, -bonus)</code></pre>
<p>
This select removes every column that ends with the character “2” (among others). I split the data in two, to then bind the rows together and thus create my long dataset. I then save the data into the <code>data/</code> folder:
</p>
<pre class="r"><code>usethis::use_data(njmin, overwrite = TRUE)</code></pre>
<p>
This saves the data as an <code>.rda</code> file. To enable users to read the data by typing <code>data(“njmin”)</code>, you need to create a <code>data.R</code> script in the <code>R/</code> folder. You can read my <code>data.R</code> script below:
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code>#' Data from the Card and Krueger 1994 paper *Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania*
#'
#' This dataset was downloaded and distributed with the permission of David Card. The original
#' data contains 410 observations and 46 variables. The data distributed in this package is
#' exactly the same, but was changed from a wide to a long dataset, which is better suited for
#' manipulation with *tidyverse* functions.
#'
#' @format A data frame with 820 rows and 28 variables:
#' \describe{
#'   \item{\code{sheet}}{Sheet number (unique store id).}
#'   \item{\code{chain}}{The fastfood chain: bk is Burger King, kfc is Kentucky Fried Chicken, wendys is Wendy's, roys is Roy Rogers.}
#'   \item{\code{state}}{State where the restaurant is located.}
#'   \item{\code{region}}{pa1 is northeast suburbs of Phila, pa2 is Easton etc, centralj is central NJ, northj is northern NJ, southj is south NJ.}
#'   \item{\code{observation}}{Date of first (February 1992) and second (November 1992) observation.}
#'   \item{\code{co_owned}}{"Yes" if company owned.}
#'   \item{\code{ncalls}}{Number of call-backs. Is 0 if contacted on first call.}
#'   \item{\code{empft}}{Number full-time employees.}
#'   \item{\code{emppt}}{Number part-time employees.}
#'   \item{\code{nmgrs}}{Number of managers/assistant managers.}
#'   \item{\code{wage_st}}{Starting wage ($/hr).}
#'   \item{\code{inctime}}{Months to usual first raise.}
#'   \item{\code{firstinc}}{Usual amount of first raise (\$/hr).}
#'   \item{\code{bonus}}{"Yes" if cash bounty for new workers.}
#'   \item{\code{pctaff}}{\% of employees affected by new minimum.}
#'   \item{\code{meals}}{Free/reduced priced code.}
#'   \item{\code{open}}{Hour of opening.}
#'   \item{\code{hrsopen}}{Number of hours open per day.}
#'   \item{\code{psode}}{Price of medium soda, including tax.}
#'   \item{\code{pfry}}{Price of small fries, including tax.}
#'   \item{\code{pentree}}{Price of entree, including tax.}
#'   \item{\code{nregs}}{Number of cash registers in store.}
#'   \item{\code{nregs11}}{Number of registers open at 11:00 pm.}
#'   \item{\code{type}}{Type of 2nd interview.}
#'   \item{\code{status}}{Status of 2nd interview.}
#'   \item{\code{date}}{Date of 2nd interview.}
#'   \item{\code{nregs11}}{"Yes" if special program for new workers.}
#' }
#' @source \url{http://davidcard.berkeley.edu/data_sets.html}
"njmin"</code></pre>
</details>
<p>
I have documented the data, and using <code>roxygen2::royxgenise()</code> to create the dataset’s documentation.
</p>
<p>
The data can now be used to create some nifty plots:
</p>
<pre class="r"><code>ggplot(njmin, aes(wage_st)) + geom_density(aes(fill = state), alpha = 0.3) +
    facet_wrap(vars(observation)) + theme_blog() +
    theme(legend.title = element_blank(), plot.caption = element_text(colour = "white")) +
    labs(title = "Distribution of starting wage rates in fast food restaurants",
         caption = "On April 1st, 1992, New Jersey's minimum wage rose from $4.25 to $5.05. Source: Card and Krueger (1994)")</code></pre>
<pre><code>## Warning: Removed 41 rows containing non-finite values (stat_density).</code></pre>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/diffindiff_part2-6-1.png" width="80%" height="auto">
</div>
<p>
In the next blog post, I am going to write a first function to perform diff and diff, and we will learn how to make it available to users, document and test it!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-05-04-diffindiff_part2.html</guid>
  <pubDate>Sat, 04 May 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Fast food, causality and R packages, part 1</title>
  <link>https://b-rodrigues.github.io/posts/2019-04-28-diffindiff_part1.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Joke"> <img src="https://b-rodrigues.github.io/assets/img/distracted_economist.jpg" title="Soon, humanity will only communicate in memes" width="80%" height="auto"></a>
</p>
</div>
<p>
I am currently working on a package for the R programming language; its initial goal was to simply distribute the data used in the Card and Krueger 1994 paper that you can read <a href="http://davidcard.berkeley.edu/papers/njmin-aer.pdf">here</a> (PDF warning).
</p>
<p>
The gist of the paper is to try to answer the following question: <em>Do increases in minimum wages reduce employment?</em> According to Card and Krueger’s paper from 1994, no. The authors studied a change in legislation in New Jersey which increased the minimum wage from $4.25 an hour to $5.05 an hour. The neighbourghing state of Pennsylvania did not introduce such an increase. The authors thus used the State of Pennsylvania as a control for the State of New Jersey and studied how the increase in minimum wage impacted the employment in fast food restaurants and found, against what economic theory predicted, an increase and not a decrease in employment. The authors used a method called difference-in-differences to asses the impact of the minimum wage increase.
</p>
<p>
This result was and still is controversial, with subsequent studies finding subtler results. For instance, showing that there is a reduction in employment following an increase in minimum wage, but only for large restaurants (see Ropponen and Olli, 2011).
</p>
<p>
Anyways, this blog post will discuss how to create a package using to distribute the data. In a future blog post, I will discuss preparing the data to make it available as a demo dataset inside the package, and then writing and documenting functions.
</p>
<p>
The first step to create a package, is to create a new project:
</p>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/package_01.png" width="80%" height="auto">
</div>
<p>
Select “New Directory”:
</p>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/package_02.png" width="80%" height="auto">
</div>
<p>
Then “R package”:
</p>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/package_03.png" width="80%" height="auto">
</div>
<p>
and on the window that appears, you can choose the name of the package, as well as already some starting source files:
</p>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/package_04.png" width="80%" height="auto">
</div>
<p>
Also, I’d highly recommend you click on the “Create a git repository” box and use git within your project for reproducibility and sharing your code more easily. If you do not know git, there’s a lot of online resources to get you started. It’s not super difficult, but it does require making some new habits, which can take some time.
</p>
<p>
I called my package <code>{diffindiff}</code>, and clicked on “Create Project”. This opens up a new project with a <code>hello.R</code> script, which gives you some pointers:
</p>
<pre><code># Hello, world!
#
# This is an example function named 'hello' 
# which prints 'Hello, world!'.
#
# You can learn more about package authoring with RStudio at:
#
#   http://r-pkgs.had.co.nz/
#
# Some useful keyboard shortcuts for package authoring:
#
#   Install Package:           'Ctrl + Shift + B'
#   Check Package:             'Ctrl + Shift + E'
#   Test Package:              'Ctrl + Shift + T'

hello &lt;- function() {
  print("Hello, world!")
}</code></pre>
<p>
Now, to simplify the creation of your package, I highly recommend you use the <code>{usethis}</code> package. <code>{usethis}</code> removes a lot of the pain involved in creating packages.
</p>
<p>
For instance, want to start by adding a README file? Simply run:
</p>
<pre class="r"><code>usethis::use_readme_md()</code></pre>
<pre class="r"><code>✔ Setting active project to '/path/to/your/package/diffindiff'
✔ Writing 'README.md'
● Modify 'README.md'</code></pre>
<p>
This creates a <code>README.md</code> file in the root directory of your package. Simply change that file, and that’s it.
</p>
<p>
The next step could be setting up your package to work with <code>{roxygen2}</code>, which is very useful for writing documentation:
</p>
<pre class="r"><code>usethis::use_roxygen_md()</code></pre>
<pre class="r"><code>✔ Setting Roxygen field in DESCRIPTION to 'list(markdown = TRUE)'
✔ Setting RoxygenNote field in DESCRIPTION to '6.1.1'
● Run `devtools::document()`</code></pre>
<p>
See how the output tells you to run <code>devtools::document()</code>? This function will document your package, transforming the comments you write to describe your functions to documentation and managing the NAMESPACE file. Let’s run this function too:
</p>
<pre class="r"><code>devtools::document()</code></pre>
<pre class="r"><code>Updating diffindiff documentation
First time using roxygen2. Upgrading automatically...
Loading diffindiff
Warning: The existing 'NAMESPACE' file was not generated by roxygen2, and will not be overwritten.</code></pre>
<p>
You might have a similar message than me, telling you that the NAMESPACE file was not generated by <code>{roxygen2}</code>, and will thus not be overwritten. Simply remove the file and run <code>devtools::document()</code> again:
</p>
<pre class="r"><code>devtools::document()</code></pre>
<pre class="r"><code>Updating diffindiff documentation
First time using roxygen2. Upgrading automatically...
Writing NAMESPACE
Loading diffindiff</code></pre>
<p>
But what is actually the NAMESPACE file? This file is quite important, as it details where your package’s functions have to look for in order to use other functions. This means that if your package needs function <code>foo()</code> from package <code>{bar}</code>, it will consistently look for <code>foo()</code> inside <code>{bar}</code> and not confuse it with, say, the <code>foo()</code> function from the <code>{barley}</code> package, even if you load <code>{barley}</code> after <code>{bar}</code> in your interactive session. This can seem confusing now, but in the next blog posts I will detail this, and you will see that it’s not that difficult. Just know that it is an important file, and that you do not have to edit it by hand.
</p>
<p>
Next, I like to run the following:
</p>
<pre class="r"><code>usethis::use_pipe()</code></pre>
<pre class="r"><code>✔ Adding 'magrittr' to Imports field in DESCRIPTION
✔ Writing 'R/utils-pipe.R'
● Run `devtools::document()`</code></pre>
<p>
This makes the now famous <code>%&gt;%</code> function available internally to your package (so you can use it to write the functions that will be included in your package) but also available to the users that will load the package.
</p>
<p>
Your package is still missing a license. If you plan on writing a package for your own personal use, for instance, a collection of functions, there is no need to think about licenses. But if you’re making your package available through CRAN, then you definitely need to think about it. For this package, I’ll be using the MIT license, because the package will distribute data which I do not own (I’ve got permission from Card to re-distribute it) and thus I think it would be better to use a permissive license (I don’t know if the GPL, another license, which is stricter in terms of redistribution, could be used in this case).
</p>
<pre class="r"><code>usethis::use_mit_license()</code></pre>
<pre class="r"><code>✔ Setting License field in DESCRIPTION to 'MIT + file LICENSE'
✔ Writing 'LICENSE.md'
✔ Adding '^LICENSE\\.md$' to '.Rbuildignore'
✔ Writing 'LICENSE'</code></pre>
<p>
We’re almost done setting up the structure of the package. If we forget something though, it’s not an issue, we’ll just have to run the right <code>use_*</code> function later on. Let’s finish by preparing the folder that will contains the script to prepare the data:
</p>
<pre class="r"><code>usethis::use_data_raw()</code></pre>
<pre class="r"><code>✔ Creating 'data-raw/'
✔ Adding '^data-raw$' to '.Rbuildignore'
✔ Writing 'data-raw/DATASET.R'
● Modify 'data-raw/DATASET.R'
● Finish the data preparation script in 'data-raw/DATASET.R'
● Use `usethis::use_data()` to add prepared data to package</code></pre>
<p>
This creates the <code>data-raw</code> folder with the <code>DATASET.R</code> script inside. This is the script that will contain the code to download and prepare datasets that you want to include in your package. This will be the subject of the next blog post.
</p>
<p>
Let’s now finish by documenting the package, and pushing everything to Github:
</p>
<pre class="r"><code>devtools::document()</code></pre>
<p>
The following lines will only work if you set up the Github repo:
</p>
<pre><code>git add .
git commit -am "first commit"
git push origin master</code></pre>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-04-28-diffindiff_part1.html</guid>
  <pubDate>Sun, 28 Apr 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Historical newspaper scraping with {tesseract} and R</title>
  <link>https://b-rodrigues.github.io/posts/2019-04-07-historical_newspaper_scraping_tesseract.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Cliometrics"> <img src="https://b-rodrigues.github.io/assets/img/clio.jpg" title="Historical newspapers as a source to practice cliometrics?" width="80%" height="auto"></a>
</p>
</div>
<p>
I have been playing around with historical newspapers data for some months now. The “obvious” type of analysis to do is NLP, but there is also a lot of numerical data inside historical newspapers. For instance, you can find these tables that show the market prices of the day in the <em>L’Indépendance Luxembourgeoise</em>:
</p>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/market_price_table.png" width="80%" height="auto">
</div>
<p>
I wanted to see how easy it was to extract these tables from the newspapers and then make it available. It was a bit more complicated than anticipated.
</p>
<section id="download-data" class="level2">
<h2 class="anchored" data-anchor-id="download-data">
Download data
</h2>
<p>
The first step is to download the data. For this, I have used the code <a href="https://twitter.com/yvesmaurer"><code><span class="citation" data-cites="yvesmaurer">@yvesmaurer</span></code></a> which you can find <a href="https://github.com/ymaurer/eluxemburgensia-opendata-ark">here</a>. This code makes it easy to download individual pages of certain newspapers, for instance <a href="https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F1drtkj%2Fpages%2F1/full/full/0/default.jpg">this one</a>. The pages I am interested in are pages 3, which contain the tables I need, for example <a href="https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F1drtkj%2Fpages%2F3/full/full/0/default.jpg">here</a>. <a href="https://twitter.com/yvesmaurer"><code><span class="citation" data-cites="yvesmaurer">@yvesmaurer</span></code></a>’s code makes it easy to find the download links, which look like this: <code>https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F1drtkj%2Fpages%2F3/full/full/0/default.jpg</code>. It is also possible to crop the image by changing some parameters <a href="https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fwsvhwh%2Fpages%2F3/pct:74,0,100,100/full/0/default.jpg">like so</a>. This is helpful, because it makes the image smaller. The tables I’m interested in are always in the last column, so I can can use this feature to get smaller images. However, not every issue contains these tables, and I only want to download the ones that have these tables. So I wrote the following code to download the images I’m interested in:
</p>
<pre class="r"><code>library(tidyverse)
library(magick)
library(tesseract)
library(furrr)

download_image &lt;- function(link){

    print(link)

    isok &lt;- image_read(link) %&gt;%
        ocr(engine = "fra") %&gt;%
        str_to_lower() %&gt;%
        str_detect("marché de luxembourg")

    if(isok){
        date_link &lt;- link %&gt;%
            str_replace("pages%2f3", "pages%2f1") %&gt;%
            str_replace("pct:74,0,100,100", "pct:76,1,17,5")

        paper_date &lt;- image_read(date_link) %&gt;%
            ocr(engine = "fra") %&gt;%
            str_squish() %&gt;%
            str_remove("%") %&gt;%
            str_remove("&amp;") %&gt;%
            str_remove("/")

        ark &lt;- link %&gt;%
            str_sub(53, 60)

        download.file(link, paste0("indep_pages/", ark, "-", paper_date, ".jpg"))
    } else {
        NULL
        }
}</code></pre>
<p>
This code only downloads an image if the <code>ocr()</code> from the {tesseract} (which does, you guessed it, OCR) detects the string “marché de luxembourg” which is the title of the tables. This is a bit extreme, because if a single letter cannot be correctly detected by the OCR, the page will not be downloaded. But I figured that if this string could not be easily recognized, this would be a canary telling me that the text inside the table would also not be easily recognized. So it might be extreme, but my hope was that it would make detecting the table itself easier. Turned out it wasn’t so easy, but more on this later.
</p>
</section>
<section id="preparing-images" class="level2">
<h2 class="anchored" data-anchor-id="preparing-images">
Preparing images
</h2>
<p>
Now that I have the images, I will prepare them to make character recognition easier. To do this, I’m using the <code>{magick}</code> package:
</p>
<pre class="r"><code>library(tidyverse)
library(magick)
library(tesseract)
library(furrr)

prepare_image &lt;- function(image_path){
    image &lt;- image_read(image_path)

    image &lt;- image %&gt;%
        image_modulate(brightness = 150) %&gt;%
        image_convolve('DoG:0,0,2', scaling = '1000, 100%') %&gt;%
        image_despeckle(times = 10)

    image_write(image, paste0(getwd(), "/edited/", str_remove(image_path, ".jpg"), "edited.jpg"))
}


image_paths &lt;- dir(path = "indep_pages", pattern = "*.jpg", full.names = TRUE)

plan(multiprocess, workers = 8)

image_paths %&gt;%
    future_map(prepare_image)</code></pre>
<p>
The picture below shows the result:
</p>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/table_and_edit.jpg" width="80%" height="auto">
</div>
<p>
Now comes the complicated part, which is going from the image above, to the dataset below:
</p>
<pre><code>good_fr,good_en,unit,market_date,price,source_url
Froment,Wheat,hectolitre,1875-08-28,23,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Métail,Meslin,hectolitre,1875-08-28,21,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Seigle,Rye,hectolitre,1875-08-28,15,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Orge,Barley,hectolitre,1875-08-28,16,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Orge mondé,Pot Barley,kilogram,1875-08-28,0.85,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Orge perlé,Pearl barley,kilogram,1875-08-28,0.8,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Avoine,Oats,hectolitre,1875-08-28,8.5,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Pois,Peas,hectolitre,1875-08-28,NA,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg</code></pre>
</section>
<section id="ocr-with-tesseract" class="level2">
<h2 class="anchored" data-anchor-id="ocr-with-tesseract">
OCR with {tesseract}
</h2>
<p>
The first step was to get the date. For this, I have used the following function, which will then be used inside another function, which will extract the data and prices.
</p>
<pre class="r"><code>library(tidyverse)
library(lubridate)
library(magick)
library(tesseract)
library(furrr)
library(janitor)

is_empty_line &lt;- function(line){
    ifelse(line == "", TRUE, FALSE)
}

Sys.setlocale('LC_TIME', "fr_FR")

get_date &lt;- function(string, annee){

    liste_mois &lt;- c("janvier", "février", "mars", "avril", "mai", "juin", "juillet",
                    "août", "septembre", "octobre", "novembre", "décembre")

    raw_date &lt;- string %&gt;%
      str_to_lower() %&gt;%
        str_remove_all("\\.") %&gt;%
        str_extract("\\d{1,2} .{3,9}(\\s+)?\\d{0,4}") %&gt;%
        str_split("\\s+", simplify = TRUE)

    if(ncol(raw_date) == 2){
        raw_date &lt;- cbind(raw_date, "annee")
    }

    raw_date[1, 3] &lt;- annee

    raw_date &lt;- str_to_lower(raw_date[1:1, 1:3])

    long_month &lt;- case_when(
      raw_date[2] == "janv" ~ "janvier",
      raw_date[2] == "févr" ~ "février",
      raw_date[2] == "sept" ~ "septembre",
      raw_date[2] == "oct" ~ "octobre",
      raw_date[2] == "nov" ~ "novembre",
      raw_date[2] == "dec" ~ "décembre",
      TRUE ~ as.character(raw_date[2]))

    raw_date[2] &lt;- long_month

    is_it_date &lt;- as.Date(paste0(raw_date, collapse = "-"), format = "%d-%b-%Y") %&gt;%
        is.na() %&gt;% `!`()

    if(is_it_date){
        return(as.Date(paste0(raw_date, collapse = "-"), format = "%d-%b-%Y"))
    } else {
        if(!(raw_date[2] %in% liste_mois)){
            raw_date[2] &lt;- liste_mois[stringdist::amatch(raw_date[2], liste_mois, maxDist = 2)]
            return(as.Date(paste0(raw_date, collapse = "-"), format = "%d-%b-%Y"))
        }
    }
}</code></pre>
<p>
This function is more complicated than I had hoped. This is because dates come in different formats. For example, there are dates written like this “21 Janvier 1872”, or “12 Septembre” or “12 sept.”. The biggest problem here is that sometimes the year is missing. I deal with this in the next function, which is again, more complicated than what I had hoped. I won’t go into details and explain every step of the function above, but the idea is to extract the data from the raw text, replace abbreviated months with the full month name if needed, and then check if I get a valid date. If not, I try my luck with <code>stringdist::amatch()</code>, to try to match, say “jonvier” with “janvier”. This is in case the OCR made a mistake. I am not very happy with this solution, because it is very approximative, but oh well.
</p>
<p>
The second step is to get the data. I noticed that the rows stay consistent, but do change after June 1st 1876. So I simply hardcoded the goods names, and was only concerned with extracting the prices. I also apply some manual corrections inside the function; mainly dates that were wrongly recognized by the OCR engine, and which were causing problems. Again, not an optimal solution, the other alternative was to simply drop this data, which I did not want to do. Here is the function:
</p>
<pre class="r"><code>extract_table &lt;- function(image_path){

  image &lt;- image_read(image_path)

  annee &lt;- image_path %&gt;%
    str_extract("187\\d")

  ark &lt;- image_path %&gt;%
    str_sub(22, 27)

  source_url &lt;- str_glue("https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F{ark}%2Fpages%2F1/full/full/0/default.jpg",
                         ark = ark)

  text &lt;- ocr(image, engine = "fra")

    text &lt;- text %&gt;%
      str_split("\n") %&gt;%
      unlist %&gt;%
      str_squish() %&gt;%
      str_remove_all("^.{1,10}$") %&gt;%
      discard(is_empty_line) %&gt;%
      str_replace("Mercuriale du \\+ Nov. 1831.", "Mercuriale du 4 Nov. 1831.") %&gt;%
      str_replace("….u .T juillet.", "du 7 juillet") %&gt;%
      str_replace("octobré", "octobre") %&gt;%
      str_replace("AT octobre", "17 octobre") %&gt;% # correction for "f8g6kq8-18  LUNDI 19 OCTOBRÉ 1874. BUREAUX de fa RÉDACTIGedited.jpg"
      str_replace("T norembre", "7 novembre") %&gt;%  # correction for fcrhrn5-LE 8  LUNDI 9 NOVEMBRE 1874 BUREAUX de la RÉDedited.jpg
      str_replace("À oc demain 5", "27 mai") %&gt;% # correction for fd61vzp-MARDI 50. MAI 1876 BUREAUX de la. RED, n VE DE L’ADMINISTRAedited.jpg
      str_replace("G", "6") %&gt;%
      str_replace("Hercariale du 80 nov. 1872,", "du 30 novembre 1872") %&gt;%
      str_replace("….u .T juillet.", "du 7 juillet") %&gt;%
      str_replace("Rs ne its du 28-octobré.: :!: :", "28 octobre") %&gt;%
      str_replace("De routes due 98-juilléle. à eat", "28 juillet") %&gt;%
      str_replace("\\| Mereariale dn 14 dre. 1872,", "14 décembre 1872")


  start &lt;- text %&gt;%
    str_which("MARCH(É|E).*D(E|É).*LUXEMBOUR(G|6)") + 2

  start &lt;- ifelse(is_empty(start), str_which(text, ".*D.*UXEM.*") + 2, start)

  end &lt;- start + 40

  pricing_date &lt;- text[start - 1] %&gt;%
    str_remove("%") %&gt;%
    str_remove("er") %&gt;%
    str_remove("\\.+") %&gt;%
    str_remove("\\*") %&gt;%
    str_remove("®") %&gt;%
    str_remove(":") %&gt;%
    str_remove("\\?") %&gt;%
    str_replace("\\$", "9") %&gt;%
    str_remove("°") %&gt;%
    str_replace("‘du 14août.. - ; En", "14 août") %&gt;%
    str_replace("OP PE CN AP PP", "du 28 juin") %&gt;%
    str_replace("‘ du 81 janvi Le", "31 janvier") %&gt;%
    str_replace("\\| \\| du AT août", "17 août") %&gt;%
    str_replace("Su”  du 81 juillet. L", "31 juillet") %&gt;%
    str_replace("0 du 29 avril \" \\|", "29 avril") %&gt;%
    str_replace("LU 0 du 28 ail", "28 avril") %&gt;%
    str_replace("Rs ne its du 28-octobre :!: :", "23 octobre") %&gt;%
    str_replace("7 F \\|  du 13 octobre LA LOTS", "13 octobre") %&gt;%
    str_replace("À. du 18 juin UT ET", "13 juin")


  market_date &lt;- get_date(pricing_date, annee)

  items &lt;- c("Froment", "Métail", "Seigle", "Orge", "Orge mondé", "Orge perlé", "Avoine", "Pois", "Haricots",
             "Lentilles", "Pommes de terre", "Bois de hêtre", "Bois de chêne", "Beurre", "Oeufs", "Foin",
             "Paille", "Viande de boeuf", "Viande de vache", "Viande de veau", "Viande de mouton",
             "Viande fraîche de cochon", "Viande fumée de cochon", "Haricots", "Pois", "Lentilles",
             "Farines de froment", "Farines de méteil", "Farines de seigle")

  items_en &lt;- c("Wheat", "Meslin", "Rye", "Barley", "Pot Barley", "Pearl barley", "Oats", "Peas", "Beans",
    "Lentils", "Potatoes", "Beech wood", "Oak wood", "Butter", "Eggs", "Hay", "Straw", "Beef meat",
    "Cow meat", "Veal meat", "Sheep meat", "Fresh pig meat", "Smoked pig meat", "Beans", "Peas",
    "Lentils", "Wheat flours", "Meslin flours", "Rye flours")


  unit &lt;- c("hectolitre", "hectolitre", "hectolitre", "hectolitre", "kilogram", "kilogram", "hectolitre",
            "hectolitre", "hectolitre", "hectolitre", "hectolitre", "stere", "stere", "kilogram", "dozen",
            "500 kilogram", "500 kilogram", "kilogram", "kilogram", "kilogram", "kilogram", "kilogram",
            "kilogram", "litre", "litre", "litre", "kilogram", "kilogram", "kilogram")

  # starting with june 1876, the order of the items changes
  items_06_1876 &lt;- c("Froment", "Métail", "Seigle", "Orge", "Avoine", "Pois", "Haricots", "Lentilles",
                     "Pommes de terre", "Farines de froment", "Farines de méteil", "Farines de seigle", "Orge mondé",
                     "Beurre", "Oeufs", "Foins", "Paille", "Bois de hêtre", "Bois de chêne", "Viande de boeuf", "Viande de vache",
                     "Viande de veau", "Viande de mouton", "Viande fraîche de cochon", "Viande fumée de cochon")

  items_06_1876_en &lt;- c("Wheat", "Meslin", "Rye", "Barley", "Oats", "Peas", "Beans", "Lentils",
                        "Potatoes", "Wheat flours", "Meslin flours", "Rye flours", "Pot barley",
                        "Butter", "Eggs", "Hay", "Straw", "Beechwood", "Oakwood", "Beef meat", "Cow meat",
                        "Veal meat", "Sheep meat", "Fresh pig meat", "Smoked pig meat")

  units_06_1876 &lt;- c(rep("hectolitre", 9), rep("kilogram", 5), "douzaine", rep("500 kilogram", 2),
                     "stere", "stere", rep("kilogram", 6))

  raw_data &lt;- text[start:end]

  prices &lt;- raw_data %&gt;%
    str_replace_all("©", "0") %&gt;%
    str_extract("\\d{1,2}\\s\\d{2}") %&gt;%
    str_replace("\\s", "\\.") %&gt;%
    as.numeric

  if(is.na(prices[1])){
    prices &lt;- tail(prices, -1)
  } else {
    prices &lt;- prices
  }

  if(market_date &lt; as.Date("01-06-1876", format = "%d-%m-%Y")){
    prices &lt;- prices[1:length(items)]
    tibble("good_fr" = items, "good_en" = items_en, "unit" = unit, "market_date" = market_date,
           "price" = prices, "source_url" = source_url)
  } else {
    prices &lt;- prices[1:length(items_06_1876_en)]
    tibble("good_fr" = items_06_1876, "good_en" = items_06_1876_en, "unit" = units_06_1876,
           "market_date" = market_date, "price" = prices, "source_url" = source_url)
  }
}</code></pre>
<p>
As I wrote previously, I had to deal with the missing year in the date inside this function. To do that, I extracted the year from the name of the file, and pasted it then into the date. The file name contains the data because the function in the function that downloads the files I also performed OCR on the first page, to get the date of the newspaper issue. The sole purpose of this was to get the year. Again, the function is more complex than what I hoped, but it did work well overall. There are still mistakes in the data, for example sometimes the prices are in the wrong order; meaning that they’re “shifted”, for example instead of the prices for eggs, I have the prices of the good that comes next. So obviously be careful if you decide to analyze the data, and double-check if something seems weird. I have made the data available on Luxembourg Open Data Portal, <a href="https://data.public.lu/fr/datasets/digitised-luxembourg-historical-newspapers-journaux-historiques-luxembourgeois-numerises/#resource-community-27293c42-22e5-4811-aee8-89d6f7fa9533">here</a>.
</p>
</section>
<section id="analyzing-the-data" class="level2">
<h2 class="anchored" data-anchor-id="analyzing-the-data">
Analyzing the data
</h2>
<p>
And now, to the fun part. I want to know what was the price of smoked pig meat, and how it varied through time:
</p>
<pre class="r"><code>library(tidyverse)
library(ggplot2)
library(brotools)</code></pre>
<pre class="r"><code>market_price &lt;- read_csv("https://download.data.public.lu/resources/digitised-luxembourg-historical-newspapers-journaux-historiques-luxembourgeois-numerises/20190407-183605/market-price.csv")</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   good_fr = col_character(),
##   good_en = col_character(),
##   unit = col_character(),
##   market_date = col_date(format = ""),
##   price = col_double(),
##   source_url = col_character()
## )</code></pre>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Smoked pig meat") %&gt;%
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of smoked pig meat at the Luxembourg-City market in the 19th century")</code></pre>
<pre><code>## Warning: Removed 2 rows containing missing values (geom_path).</code></pre>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-9-1.png" width="80%" height="auto">
</div>
<p>
As you can see, there is a huge spike somewhere in 1874. Maybe there was a very severe smoked pig meat shortage that caused the prices to increase dramatically, but the more likely explanation is that there was some sort of mistake, either in the OCR step, or when I extracted the prices, and somehow that particular price of smoked pig meat is actually the price of another, more expensive good.
</p>
<p>
So let’s only consider prices that are below, say, 20 franks, which is already very high:
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Smoked pig meat") %&gt;%
    filter(price &lt; 20) %&gt;% 
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of smoked pig meat at the Luxembourg-City market in the 1870s")</code></pre>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-10-1.png" width="80%" height="auto">
</div>
<p>
Now, some prices are very high. Let’s check if it’s a mistake:
</p>
<pre class="r"><code>market_price %&gt;% 
    filter(good_en == "Smoked pig meat") %&gt;% 
    filter(between(price, 5, 20)) %&gt;% 
    pull(source_url)</code></pre>
<pre><code>## [1] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fbs2fs6%2Fpages%2F1/full/full/0/default.jpg"
## [2] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fd61vzp%2Fpages%2F1/full/full/0/default.jpg"
## [3] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fjdwb6m%2Fpages%2F1/full/full/0/default.jpg"
## [4] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fng14m3%2Fpages%2F1/full/full/0/default.jpg"
## [5] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fw9jdrb%2Fpages%2F1/full/full/0/default.jpg"</code></pre>
<p>
If you go to the first url, you will land on the first page of the newspaper. To check the table, you need to check the third page, by changing this part of the url “pages%2F1” to this “pages%2F3”.
</p>
<p>
You will then find the following:
</p>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/price_smoked_pig.png" width="80%" height="auto">
</div>
<p>
As you can see, the price was 2.5, but the OCR returned 7.5. This is a problem that is unavoidable with OCR; there is no way of knowing a priori if characters were not well recognized. It is actually quite interesting how the price for smoked pig meat stayed constant through all these years. A density plot shows that most prices were around 2.5:
</p>
<pre class="r"><code>market_price %&gt;% 
    filter(good_en == "Smoked pig meat") %&gt;% 
    filter(price &lt; 20) %&gt;% 
    ggplot() + 
    geom_density(aes(price), colour = "#82518c") + 
    theme_blog()</code></pre>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-13-1.png" width="80%" height="auto">
</div>
<p>
What about another good, say, barley?
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Barley") %&gt;%
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of barley at the Luxembourg-City market in the 1870s")</code></pre>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-14-1.png" width="80%" height="auto">
</div>
<p>
Here again, we see some very high spikes, most likely due to errors. Let’s try to limit the prices to likely values:
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Barley") %&gt;%
    filter(between(price, 10, 40)) %&gt;%
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of barley at the Luxembourg-City market in the 1870s")</code></pre>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-15-1.png" width="80%" height="auto">
</div>
<pre class="r"><code>market_price %&gt;% 
    filter(good_en == "Barley") %&gt;% 
    ggplot() + 
    geom_density(aes(price), colour = "#82518c") + 
    theme_blog()</code></pre>
<pre><code>## Warning: Removed 39 rows containing non-finite values (stat_density).</code></pre>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-16-1.png" width="80%" height="auto">
</div>
<p>
Let’s finish this with one of my favourite legume, lentils:
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Lentils") %&gt;%
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of lentils at the Luxembourg-City market in the 1870s")</code></pre>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-17-1.png" width="80%" height="auto">
</div>
<pre class="r"><code>market_price %&gt;% 
    filter(good_en == "Lentils") %&gt;% 
    ggplot() + 
    geom_density(aes(price), colour = "#82518c") + 
    theme_blog()</code></pre>
<pre><code>## Warning: Removed 79 rows containing non-finite values (stat_density).</code></pre>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-18-1.png" width="80%" height="auto">
</div>
<p>
All these 0’s might be surprising, but in most cases, they are actually true zeros! For example, you can check this <a href="https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fwsvhwh%2Fpages%2F3/pct:74,0,100,100/full/0/default.jpg">issue</a>. This very likely means that no lentils were available that day at the market. Let’s get rid of the 0s and other extreme values:
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Lentils") %&gt;%
    filter(between(price, 1, 40)) %&gt;% 
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of lentils at the Luxembourg-City market in the 1870s")</code></pre>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-19-1.png" width="80%" height="auto">
</div>
<p>
I would like to see if the spikes above 30 are errors or not:
</p>
<pre class="r"><code>market_price %&gt;% 
    filter(good_en == "Lentils") %&gt;% 
    filter(between(price, 30, 40)) %&gt;% 
    pull(source_url)</code></pre>
<pre><code>## [1] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F04mb5t%2Fpages%2F1/full/full/0/default.jpg"
## [2] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fb8zp31%2Fpages%2F1/full/full/0/default.jpg"
## [3] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fkzrj53%2Fpages%2F1/full/full/0/default.jpg"
## [4] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fs8sw2v%2Fpages%2F1/full/full/0/default.jpg"
## [5] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fsjptsk%2Fpages%2F1/full/full/0/default.jpg"
## [6] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fwk65b6%2Fpages%2F1/full/full/0/default.jpg"</code></pre>
<p>
The price was recognized as being 35, and turns out it was correct as you can see <a href="https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F04mb5t%2Fpages%2F3/full/full/0/default.jpg">here</a>. This is quite interesting, because the average price was way lower than that:
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Lentils") %&gt;%
    filter(between(price, 1, 40)) %&gt;% 
    summarise(mean_price = mean(price), 
              sd_price = sd(price))</code></pre>
<pre><code>## # A tibble: 1 x 2
##   mean_price sd_price
##        &lt;dbl&gt;    &lt;dbl&gt;
## 1       20.8     5.82</code></pre>
<p>
I’m going to finish here; it was an interesting project, and I can’t wait for more newspapers to be digitized and OCR to work even better. There is a lot more historical data trapped in these newspapers that could provide a lot insights on Luxembourg’s society in the 19th century.
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-04-07-historical_newspaper_scraping_tesseract.html</guid>
  <pubDate>Sun, 07 Apr 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Get text from pdfs or images using OCR: a tutorial with {tesseract} and {magick}</title>
  <link>https://b-rodrigues.github.io/posts/2019-03-31-tesseract.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Michel_Rodange"> <img src="https://b-rodrigues.github.io/assets/img/michelrodange.jpg" title="The high school I attended was named after this gentleman" width="80%" height="auto"></a>
</p>
</div>
<p>
In this blog post I’m going to show you how you can extract text from scanned pdf files, or pdf files where no text recognition was performed. (For pdfs where text recognition was performed, you can read my <a href="../posts/2018-06-10-scraping_pdfs.html">other blog post</a>).
</p>
<p>
The pdf I’m going to use can be downloaded from <a href="http://www.luxemburgensia.bnl.lu/cgi/getPdf1_2.pl?mode=item&amp;id=7110">here</a>. It’s a poem titled, <em>D’Léierchen (Dem Léiweckerche säi Lidd)</em>, written by Michel Rodange, arguably Luxembourg’s most well known writer and poet. Michel Rodange is mostly known for his fable, <em>Renert oder De Fuuß am Frack an a Ma’nsgrëßt</em>, starring a central European <a href="https://en.wikipedia.org/wiki/Reynard_the_Fox">trickster anthropomorphic red fox</a>.
</p>
<div style="text-align: center;">
<img src="https://upload.wikimedia.org/wikipedia/commons/d/d4/Reynard-the-fox.jpg" width="80%" height="auto">
</div>
<p>
Anyway, back to the point of this blog post. How can we get data from a pdf where no text recognition was performed (or, how can we get text from an image)? The pdf we need the text from looks like this:
</p>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/dleierchen_03.png" width="80%" height="auto">
</div>
<p>
To get the text from the pdf, we can use the <code>{tesseract}</code> package, which provides bindings to the <code>tesseract</code> program. <code>tesseract</code> is an open source OCR engine developed by Google. This means that first you will need to install the <code>tesseract</code> program on your system. You can follow the intructions from <code>tesseract</code>’s github <a href="https://github.com/tesseract-ocr/tesseract">page</a>. <code>tesseract</code> is currently at version 4.
</p>
<p>
Before applying OCR to a pdf, let’s first use the <code>{pdftools}</code> package to convert the pdf to png. This is because <code>{tesseract}</code> requires images as input (if you provide a pdf file, it will converted on the fly). Let’s first load the needed packages:
</p>
<pre class="r"><code>library(tidyverse)
library(tesseract)
library(pdftools)
library(magick)</code></pre>
<p>
And now let’s convert the pdf to png files (in plural, because we’ll get one image per page of the pdf):
</p>
<pre class="r"><code>pngfile &lt;- pdftools::pdf_convert("path/to/pdf", dpi = 600)</code></pre>
<p>
This will generate 14 png files. I erase the ones that are not needed, such as the title page. Now, let’s read in all the image files:
</p>
<pre class="r"><code>path &lt;- dir(path = "path/to/pngs", pattern = "*.png", full.names = TRUE)

images &lt;- map(path, magick::image_read)</code></pre>
<p>
The <code>images</code> object is a list of <code>magick-image</code>s, which we can parse. BUUUUUT! There’s a problem. The text is laid out in two columns. Which means that the first line after performing OCR will be the first line of the first column, and the first line of the second column joined together. Same for the other lines of course. So ideally, I’d need to split the file in the middle, and then perform OCR. This is easily done with the <code>{magick}</code> package:
</p>
<pre class="r"><code>first_half &lt;- map(images, ~image_crop(., geometry = "2307x6462"))

second_half &lt;- map(images, ~image_crop(., geometry = "2307x6462+2307+0"))</code></pre>
<p>
Because the pngs are 4614 by 6962 pixels, I can get the first half of the png by cropping at “2307x6462” (I decrease the height a bit to get rid of the page number), and the second half by applying the same logic, but starting the cropping at the “2307+0” position. The result looks like this:
</p>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/dleierchen_cropped.png" width="80%" height="auto">
</div>
<p>
Much better! Now I need to join these two lists together. I cannot simply join them. Consider the following example:
</p>
<pre class="r"><code>one &lt;- list(1, 3, 5)

two &lt;- list(2, 4, 6)</code></pre>
<p>
This is the setup I currently have; <code>first_half</code> contains odd pages, and <code>second_half</code> contains even pages. The result I want would look like this:
</p>
<pre class="r"><code>list(1, 2, 3, 4, 5, 6)</code></pre>
<pre><code>## [[1]]
## [1] 1
## 
## [[2]]
## [1] 2
## 
## [[3]]
## [1] 3
## 
## [[4]]
## [1] 4
## 
## [[5]]
## [1] 5
## 
## [[6]]
## [1] 6</code></pre>
<p>
There is a very elegant solution, with <code>reduce2()</code> from the <code>{purrr}</code> package. <code>reduce()</code> takes one list and a function, and … <em>reduces</em> the list to a single element. For instance:
</p>
<pre class="r"><code>reduce(list(1, 2, 3), paste)</code></pre>
<pre><code>## [1] "1 2 3"</code></pre>
<p>
<code>reduce2()</code> is very similar, but takes in two lists, but the second list must be one element shorter:
</p>
<pre class="r"><code>reduce2(list(1, 2, 3), list("a", "b"), paste)</code></pre>
<pre><code>## [1] "1 2 a 3 b"</code></pre>
<p>
So we cannot simply use <code>reduce2()</code> on lists <code>one</code> and <code>two</code>, because they’re the same length. So let’s prepend a value to <code>one</code>, using the <code>prepend()</code> function of <code>{purrr}</code>:
</p>
<pre class="r"><code>prepend(one, 0) %&gt;% 
    reduce2(two, c)</code></pre>
<pre><code>## [1] 0 1 2 3 4 5 6</code></pre>
<p>
Exactly what we need! Let’s apply this trick to our lists:
</p>
<pre class="r"><code>merged_list &lt;- prepend(first_half, NA) %&gt;% 
    reduce2(second_half, c) %&gt;% 
    discard(is.na)</code></pre>
<p>
I’ve prepended <code>NA</code> to the first list, and then used <code>reduce2()</code> and then used <code>discard(is.na)</code> to remove the <code>NA</code> I’ve added at the start. Now, we can use OCR to get the text:
</p>
<pre class="r"><code>text_list &lt;- map(merged_list, ocr)</code></pre>
<p>
<code>ocr()</code> uses a model trained on English by default, and even though there is a model trained on Luxembourguish, the one trained on English works better! Very likely because the English model was trained on a lot more data than the Luxembourguish one. I was worried the English model was not going to recognize characters such as <code>é</code>, but no, it worked quite well.
</p>
<p>
This is how it looks like:
</p>
<pre class="r"><code>text_list

[[1]]
[1] "Lhe\n| Kaum huet d’Feld dat fréndlecht Feier\nVun der Aussentssonn gesunn\nAs mam Plou aus Stall a Scheier\n* D’lescht e Bauer ausgezunn.\nFir de Plou em nach ze dreiwen\nWar sai Jéngelchen alaert,\nDeen nét wéllt doheem méi bleiwen\n8 An esouz um viischte Paerd.\nOp der Schéllche stoung ze denken\nD’Léierche mam Hierz voll Lidder\nFir de Béifchen nach ze zanken\n12 Duckelt s’an de Som sech nidder.\nBis e laascht war, an du stémmt se\nUn e Liddchen, datt et kraacht\nOp der Nouteleder klémmt se\n16 Datt dem Béifchen d’Haerz alt laacht.\nAn du sot en: Papp, ech mengen\nBal de Vull dee kénnt och schwatzen.\nLauschter, sot de Papp zum Klengen,\n20 Ech kann d’Liddchen iwersetzen.\nI\nBas de do, mii léiwe Fréndchen\nMa de Wanter dee war laang!\nKuck, ech hat keng fréilech Sténnchen\n24 *T war fir dech a mech mer baang.\nAn du koum ech dech besichen\nWell du goungs nét méi eraus\nMann wat hues jo du eng Kichen\n28 Wat eng Scheier wat en Haus.\nWi zerguttster, a wat Saachen!\nAn déng Frache gouf mer Brout.\nAn déng Kanner, wi se laachen,\n32, An hir Backelcher, wi rout!\nJo, bei dir as Rot nét deier!\nJo a kuck mer wat eng Méscht.\nDat gét Saache fir an d’Scheier\n36 An och Sué fir an d’Késcht.\nMuerges waars de schuns um Dreschen\nIr der Daudes d’Schung sech stréckt\nBas am Do duurch Wis a Paschen\n40 Laascht all Waassergruef geschréckt.\n"
....
....</code></pre>
<p>
We still need to split at the <code>“”</code> character:
</p>
<pre class="r"><code>text_list &lt;- text_list %&gt;% 
    map(., ~str_split(., "\n"))</code></pre>
<p>
The end result:
</p>
<pre class="r"><code>text_list

[[1]]
[[1]][[1]]
 [1] "Lhe"                                      "| Kaum huet d’Feld dat fréndlecht Feier" 
 [3] "Vun der Aussentssonn gesunn"              "As mam Plou aus Stall a Scheier"         
 [5] "* D’lescht e Bauer ausgezunn."            "Fir de Plou em nach ze dreiwen"          
 [7] "War sai Jéngelchen alaert,"               "Deen nét wéllt doheem méi bleiwen"       
 [9] "8 An esouz um viischte Paerd."            "Op der Schéllche stoung ze denken"       
[11] "D’Léierche mam Hierz voll Lidder"         "Fir de Béifchen nach ze zanken"          
[13] "12 Duckelt s’an de Som sech nidder."      "Bis e laascht war, an du stémmt se"      
[15] "Un e Liddchen, datt et kraacht"           "Op der Nouteleder klémmt se"             
[17] "16 Datt dem Béifchen d’Haerz alt laacht." "An du sot en: Papp, ech mengen"          
[19] "Bal de Vull dee kénnt och schwatzen."     "Lauschter, sot de Papp zum Klengen,"     
[21] "20 Ech kann d’Liddchen iwersetzen."       "I"                                       
[23] "Bas de do, mii léiwe Fréndchen"           "Ma de Wanter dee war laang!"             
[25] "Kuck, ech hat keng fréilech Sténnchen"    "24 *T war fir dech a mech mer baang."    
[27] "An du koum ech dech besichen"             "Well du goungs nét méi eraus"            
[29] "Mann wat hues jo du eng Kichen"           "28 Wat eng Scheier wat en Haus."         
[31] "Wi zerguttster, a wat Saachen!"           "An déng Frache gouf mer Brout."          
[33] "An déng Kanner, wi se laachen,"           "32, An hir Backelcher, wi rout!"         
[35] "Jo, bei dir as Rot nét deier!"            "Jo a kuck mer wat eng Méscht."           
[37] "Dat gét Saache fir an d’Scheier"          "36 An och Sué fir an d’Késcht."          
[39] "Muerges waars de schuns um Dreschen"      "Ir der Daudes d’Schung sech stréckt"     
[41] "Bas am Do duurch Wis a Paschen"           "40 Laascht all Waassergruef geschréckt." 
[43] ""  
...
...</code></pre>
<p>
Perfect! Some more cleaning would be needed though. For example, I need to remove the little annotations that are included:
</p>
<div style="text-align: center;">
<img src="https://b-rodrigues.github.io/assets/img/dleierchen_anot.png" width="80%" height="auto">
</div>
<p>
I don’t know yet how I’m going to do that.I also need to remove the line numbers at the beginning of every fourth line, but this is easily done with a simple regular expression:
</p>
<pre class="r"><code>str_remove_all(c("12 bla", "blb", "123 blc"), "^\\d{1,}\\s+")</code></pre>
<pre><code>## [1] "bla" "blb" "blc"</code></pre>
<p>
But this will be left for a future blog post!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-03-31-tesseract.html</guid>
  <pubDate>Sun, 31 Mar 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Pivoting data frames just got easier thanks to pivot_wide() and pivot_long()</title>
  <link>https://b-rodrigues.github.io/posts/2019-03-20-pivot.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://youtu.be/R2u0sN9stbA?t=69"> <img src="https://b-rodrigues.github.io/assets/img/pivot.jpg" title="You know where this leads" width="80%" height="auto"></a>
</p>
</div>
<p>
Update: <code>pivot_wide()</code> and <code>pivot_long()</code> are now called <code>pivot_wider()</code> and <code>pivot_longer()</code>, so the code below needs to be updated accondingly.
</p>
<p>
There’s a lot going on in the development version of <code>{tidyr}</code>. New functions for pivoting data frames, <code>pivot_wide()</code> and <code>pivot_long()</code> are coming, and will replace the current functions, <code>spread()</code> and <code>gather()</code>. <code>spread()</code> and <code>gather()</code> will remain in the package though:
</p>
{{% tweet “1108107722128613377” %}}
<p>
If you want to try out these new functions, you need to install the development version of <code>{tidyr}</code>:
</p>
<pre class="r"><code>devtools::install_github("tidyverse/tidyr")</code></pre>
<p>
and you can read the vignette <a href="https://tidyr.tidyverse.org/dev/articles/pivot.html#many-variables-in-column-names">here</a>. Because these functions are still being developed, some more changes might be introduced, but I guess that the main functionality will not change much.
</p>
<p>
Let’s play around with these functions and the <code>mtcars</code> data set. First let’s load the packages and the data:
</p>
<pre class="r"><code>library(tidyverse)
data(mtcars)</code></pre>
<p>
First, let’s create a wide dataset, by <em>spreading</em> the levels of the “am” column to two new columns:
</p>
<pre class="r"><code>mtcars_wide1 &lt;- mtcars %&gt;% 
    pivot_wide(names_from = "am", values_from = "mpg") 

mtcars_wide1 %&gt;% 
    select(`0`, `1`, everything())</code></pre>
<pre><code>## # A tibble: 32 x 11
##      `0`   `1`   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  NA    21       6  160    110  3.9   2.62  16.5     0     4     4
##  2  NA    21       6  160    110  3.9   2.88  17.0     0     4     4
##  3  NA    22.8     4  108     93  3.85  2.32  18.6     1     4     1
##  4  21.4  NA       6  258    110  3.08  3.22  19.4     1     3     1
##  5  18.7  NA       8  360    175  3.15  3.44  17.0     0     3     2
##  6  18.1  NA       6  225    105  2.76  3.46  20.2     1     3     1
##  7  14.3  NA       8  360    245  3.21  3.57  15.8     0     3     4
##  8  24.4  NA       4  147.    62  3.69  3.19  20       1     4     2
##  9  22.8  NA       4  141.    95  3.92  3.15  22.9     1     4     2
## 10  19.2  NA       6  168.   123  3.92  3.44  18.3     1     4     4
## # … with 22 more rows</code></pre>
<p>
<code>pivot_wide()</code>’s arguments are quite explicit: <code>names_from =</code> is where you specify the column that will be spread across the data frame, meaning, the levels of this column will become new columns. <code>values_from =</code> is where you specify the column that will fill in the values of the new columns.
</p>
<p>
“0” and “1” are the new columns (“am” had two levels, <code>0</code> and <code>1</code>), which contain the miles per gallon for manual and automatic cars respectively. Let’s also take a look at the data frame itself:
</p>
<pre class="r"><code>mtcars_wide1 %&gt;% 
    select(`0`, `1`, everything())</code></pre>
<pre><code>## # A tibble: 32 x 11
##      `0`   `1`   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  NA    21       6  160    110  3.9   2.62  16.5     0     4     4
##  2  NA    21       6  160    110  3.9   2.88  17.0     0     4     4
##  3  NA    22.8     4  108     93  3.85  2.32  18.6     1     4     1
##  4  21.4  NA       6  258    110  3.08  3.22  19.4     1     3     1
##  5  18.7  NA       8  360    175  3.15  3.44  17.0     0     3     2
##  6  18.1  NA       6  225    105  2.76  3.46  20.2     1     3     1
##  7  14.3  NA       8  360    245  3.21  3.57  15.8     0     3     4
##  8  24.4  NA       4  147.    62  3.69  3.19  20       1     4     2
##  9  22.8  NA       4  141.    95  3.92  3.15  22.9     1     4     2
## 10  19.2  NA       6  168.   123  3.92  3.44  18.3     1     4     4
## # … with 22 more rows</code></pre>
<p>
Now suppose that we want to spread the values of “am” times “cyl”, and filling the data with the values of “mpg”:
</p>
<pre class="r"><code>mtcars_wide2 &lt;- mtcars %&gt;% 
    pivot_wide(names_from = c("am", "cyl"), values_from = "mpg") 

mtcars_wide2 %&gt;% 
    select(matches("^0|1"), everything())</code></pre>
<pre><code>## # A tibble: 32 x 14
##    `1_6` `1_4` `0_6` `0_8` `0_4` `1_8`  disp    hp  drat    wt  qsec    vs
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1    21  NA    NA    NA    NA      NA  160    110  3.9   2.62  16.5     0
##  2    21  NA    NA    NA    NA      NA  160    110  3.9   2.88  17.0     0
##  3    NA  22.8  NA    NA    NA      NA  108     93  3.85  2.32  18.6     1
##  4    NA  NA    21.4  NA    NA      NA  258    110  3.08  3.22  19.4     1
##  5    NA  NA    NA    18.7  NA      NA  360    175  3.15  3.44  17.0     0
##  6    NA  NA    18.1  NA    NA      NA  225    105  2.76  3.46  20.2     1
##  7    NA  NA    NA    14.3  NA      NA  360    245  3.21  3.57  15.8     0
##  8    NA  NA    NA    NA    24.4    NA  147.    62  3.69  3.19  20       1
##  9    NA  NA    NA    NA    22.8    NA  141.    95  3.92  3.15  22.9     1
## 10    NA  NA    19.2  NA    NA      NA  168.   123  3.92  3.44  18.3     1
## # … with 22 more rows, and 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>
As you can see, this is easily achieved by simply providing more columns to <code>names_from =</code>.
</p>
<p>
Finally, it is also possible to use an optional data set which contains the specifications of the new columns:
</p>
<pre class="r"><code>mtcars_spec &lt;- mtcars %&gt;% 
    expand(am, cyl, .value = "mpg") %&gt;%
    unite(".name", am, cyl, remove = FALSE)

mtcars_spec</code></pre>
<pre><code>## # A tibble: 6 x 4
##   .name    am   cyl .value
##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 
## 1 0_4       0     4 mpg   
## 2 0_6       0     6 mpg   
## 3 0_8       0     8 mpg   
## 4 1_4       1     4 mpg   
## 5 1_6       1     6 mpg   
## 6 1_8       1     8 mpg</code></pre>
<p>
This optional data set defines how the columns “0_4”, “0_6” etc are constructed, and also the value that shall be used to fill in the values. “am” and “cyl” will be used to create the “.name” and the “mpg” column will be used for the “.value”:
</p>
<pre class="r"><code>mtcars %&gt;% 
    pivot_wide(spec = mtcars_spec) %&gt;% 
    select(matches("^0|1"), everything())</code></pre>
<pre><code>## # A tibble: 32 x 14
##    `0_4` `0_6` `0_8` `1_4` `1_6` `1_8`  disp    hp  drat    wt  qsec    vs
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  NA    NA    NA    NA      21    NA  160    110  3.9   2.62  16.5     0
##  2  NA    NA    NA    NA      21    NA  160    110  3.9   2.88  17.0     0
##  3  NA    NA    NA    22.8    NA    NA  108     93  3.85  2.32  18.6     1
##  4  NA    21.4  NA    NA      NA    NA  258    110  3.08  3.22  19.4     1
##  5  NA    NA    18.7  NA      NA    NA  360    175  3.15  3.44  17.0     0
##  6  NA    18.1  NA    NA      NA    NA  225    105  2.76  3.46  20.2     1
##  7  NA    NA    14.3  NA      NA    NA  360    245  3.21  3.57  15.8     0
##  8  24.4  NA    NA    NA      NA    NA  147.    62  3.69  3.19  20       1
##  9  22.8  NA    NA    NA      NA    NA  141.    95  3.92  3.15  22.9     1
## 10  NA    19.2  NA    NA      NA    NA  168.   123  3.92  3.44  18.3     1
## # … with 22 more rows, and 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>
Using a spec is especially useful if you need to make new levels that are not in the data. For instance, suppose that there are actually 10-cylinder cars too, but they do not appear in our sample. We would like to make the fact that they’re missing explicit:
</p>
<pre class="r"><code>mtcars_spec2 &lt;- mtcars %&gt;% 
    expand(am, "cyl" = c(cyl, 10), .value = "mpg") %&gt;%
    unite(".name", am, cyl, remove = FALSE)

mtcars_spec2</code></pre>
<pre><code>## # A tibble: 8 x 4
##   .name    am   cyl .value
##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 
## 1 0_4       0     4 mpg   
## 2 0_6       0     6 mpg   
## 3 0_8       0     8 mpg   
## 4 0_10      0    10 mpg   
## 5 1_4       1     4 mpg   
## 6 1_6       1     6 mpg   
## 7 1_8       1     8 mpg   
## 8 1_10      1    10 mpg</code></pre>
<pre class="r"><code>mtcars %&gt;% 
    pivot_wide(spec = mtcars_spec2) %&gt;% 
    select(matches("^0|1"), everything())</code></pre>
<pre><code>## # A tibble: 32 x 16
##    `0_4` `0_6` `0_8` `0_10` `1_4` `1_6` `1_8` `1_10`  disp    hp  drat
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  NA    NA    NA       NA  NA      21    NA     NA  160    110  3.9 
##  2  NA    NA    NA       NA  NA      21    NA     NA  160    110  3.9 
##  3  NA    NA    NA       NA  22.8    NA    NA     NA  108     93  3.85
##  4  NA    21.4  NA       NA  NA      NA    NA     NA  258    110  3.08
##  5  NA    NA    18.7     NA  NA      NA    NA     NA  360    175  3.15
##  6  NA    18.1  NA       NA  NA      NA    NA     NA  225    105  2.76
##  7  NA    NA    14.3     NA  NA      NA    NA     NA  360    245  3.21
##  8  24.4  NA    NA       NA  NA      NA    NA     NA  147.    62  3.69
##  9  22.8  NA    NA       NA  NA      NA    NA     NA  141.    95  3.92
## 10  NA    19.2  NA       NA  NA      NA    NA     NA  168.   123  3.92
## # … with 22 more rows, and 5 more variables: wt &lt;dbl&gt;, qsec &lt;dbl&gt;,
## #   vs &lt;dbl&gt;, gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>
As you can see, we now have two more columns have been added, and they are full of NA’s.
</p>
<p>
Now, let’s try to go from wide to long data sets, using <code>pivot_long()</code>:
</p>
<pre class="r"><code>mtcars_wide1 %&gt;% 
  pivot_long(cols = c(`1`, `0`), names_to = "am", values_to = "mpg") %&gt;% 
  select(am, mpg, everything())</code></pre>
<pre><code>## # A tibble: 64 x 11
##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1      21       6   160   110  3.9   2.62  16.5     0     4     4
##  2 0      NA       6   160   110  3.9   2.62  16.5     0     4     4
##  3 1      21       6   160   110  3.9   2.88  17.0     0     4     4
##  4 0      NA       6   160   110  3.9   2.88  17.0     0     4     4
##  5 1      22.8     4   108    93  3.85  2.32  18.6     1     4     1
##  6 0      NA       4   108    93  3.85  2.32  18.6     1     4     1
##  7 1      NA       6   258   110  3.08  3.22  19.4     1     3     1
##  8 0      21.4     6   258   110  3.08  3.22  19.4     1     3     1
##  9 1      NA       8   360   175  3.15  3.44  17.0     0     3     2
## 10 0      18.7     8   360   175  3.15  3.44  17.0     0     3     2
## # … with 54 more rows</code></pre>
<p>
The arguments of <code>pivot_long()</code> are quite explicit too, and similar to the ones in <code>pivot_wide()</code>. <code>cols =</code> is where the user specifies the columns that need to be pivoted. <code>names_to =</code> is where the user can specify the name of the new columns, whose levels will be exactly the ones specified to <code>cols =</code>. <code>values_to =</code> is where the user specifies the column name of the new column that will contain the values.
</p>
<p>
It is also possible to specify the columns that should not be transformed, by using <code>-</code>:
</p>
<pre class="r"><code>mtcars_wide1 %&gt;% 
  pivot_long(cols = -matches("^[[:alpha:]]"), names_to = "am", values_to = "mpg") %&gt;% 
  select(am, mpg, everything())</code></pre>
<pre><code>## # A tibble: 64 x 11
##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1      21       6   160   110  3.9   2.62  16.5     0     4     4
##  2 0      NA       6   160   110  3.9   2.62  16.5     0     4     4
##  3 1      21       6   160   110  3.9   2.88  17.0     0     4     4
##  4 0      NA       6   160   110  3.9   2.88  17.0     0     4     4
##  5 1      22.8     4   108    93  3.85  2.32  18.6     1     4     1
##  6 0      NA       4   108    93  3.85  2.32  18.6     1     4     1
##  7 1      NA       6   258   110  3.08  3.22  19.4     1     3     1
##  8 0      21.4     6   258   110  3.08  3.22  19.4     1     3     1
##  9 1      NA       8   360   175  3.15  3.44  17.0     0     3     2
## 10 0      18.7     8   360   175  3.15  3.44  17.0     0     3     2
## # … with 54 more rows</code></pre>
<p>
Here the columns that should not be modified are all those that start with a letter, hence the “<sup>1</sup>” regular expression. It is also possible to remove all the <code>NA</code>’s from the data frame, with <code>na.rm =</code>.
</p>
<pre class="r"><code>mtcars_wide1 %&gt;% 
  pivot_long(cols = c(`1`, `0`), names_to = "am", values_to = "mpg", na.rm = TRUE) %&gt;% 
  select(am, mpg, everything())</code></pre>
<pre><code>## # A tibble: 32 x 11
##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1      21       6  160    110  3.9   2.62  16.5     0     4     4
##  2 1      21       6  160    110  3.9   2.88  17.0     0     4     4
##  3 1      22.8     4  108     93  3.85  2.32  18.6     1     4     1
##  4 0      21.4     6  258    110  3.08  3.22  19.4     1     3     1
##  5 0      18.7     8  360    175  3.15  3.44  17.0     0     3     2
##  6 0      18.1     6  225    105  2.76  3.46  20.2     1     3     1
##  7 0      14.3     8  360    245  3.21  3.57  15.8     0     3     4
##  8 0      24.4     4  147.    62  3.69  3.19  20       1     4     2
##  9 0      22.8     4  141.    95  3.92  3.15  22.9     1     4     2
## 10 0      19.2     6  168.   123  3.92  3.44  18.3     1     4     4
## # … with 22 more rows</code></pre>
<p>
We can also pivot data frames where the names of the columns are made of two or more variables, for example in our <code>mtcars_wide2</code> data frame:
</p>
<pre class="r"><code>mtcars_wide2 %&gt;% 
    select(matches("^0|1"), everything())</code></pre>
<pre><code>## # A tibble: 32 x 14
##    `1_6` `1_4` `0_6` `0_8` `0_4` `1_8`  disp    hp  drat    wt  qsec    vs
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1    21  NA    NA    NA    NA      NA  160    110  3.9   2.62  16.5     0
##  2    21  NA    NA    NA    NA      NA  160    110  3.9   2.88  17.0     0
##  3    NA  22.8  NA    NA    NA      NA  108     93  3.85  2.32  18.6     1
##  4    NA  NA    21.4  NA    NA      NA  258    110  3.08  3.22  19.4     1
##  5    NA  NA    NA    18.7  NA      NA  360    175  3.15  3.44  17.0     0
##  6    NA  NA    18.1  NA    NA      NA  225    105  2.76  3.46  20.2     1
##  7    NA  NA    NA    14.3  NA      NA  360    245  3.21  3.57  15.8     0
##  8    NA  NA    NA    NA    24.4    NA  147.    62  3.69  3.19  20       1
##  9    NA  NA    NA    NA    22.8    NA  141.    95  3.92  3.15  22.9     1
## 10    NA  NA    19.2  NA    NA      NA  168.   123  3.92  3.44  18.3     1
## # … with 22 more rows, and 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>
All the columns that start with either “0” or “1” must be pivoted:
</p>
<pre class="r"><code>mtcars_wide2 %&gt;% 
  pivot_long(cols = matches("0|1"), names_to = "am_cyl", values_to = "mpg", na.rm = TRUE) %&gt;% 
  select(am_cyl, everything())</code></pre>
<pre><code>## # A tibble: 32 x 10
##    am_cyl  disp    hp  drat    wt  qsec    vs  gear  carb   mpg
##    &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1_6     160    110  3.9   2.62  16.5     0     4     4  21  
##  2 1_6     160    110  3.9   2.88  17.0     0     4     4  21  
##  3 1_4     108     93  3.85  2.32  18.6     1     4     1  22.8
##  4 0_6     258    110  3.08  3.22  19.4     1     3     1  21.4
##  5 0_8     360    175  3.15  3.44  17.0     0     3     2  18.7
##  6 0_6     225    105  2.76  3.46  20.2     1     3     1  18.1
##  7 0_8     360    245  3.21  3.57  15.8     0     3     4  14.3
##  8 0_4     147.    62  3.69  3.19  20       1     4     2  24.4
##  9 0_4     141.    95  3.92  3.15  22.9     1     4     2  22.8
## 10 0_6     168.   123  3.92  3.44  18.3     1     4     4  19.2
## # … with 22 more rows</code></pre>
<p>
Now, there is one new column, “am_cyl” which must still be transformed by separating “am_cyl” into two new columns:
</p>
<pre class="r"><code>mtcars_wide2 %&gt;% 
  pivot_long(cols = matches("0|1"), names_to = "am_cyl", values_to = "mpg", na.rm = TRUE) %&gt;% 
  separate(am_cyl, into = c("am", "cyl"), sep = "_") %&gt;% 
  select(am, cyl, everything())</code></pre>
<pre><code>## # A tibble: 32 x 11
##    am    cyl    disp    hp  drat    wt  qsec    vs  gear  carb   mpg
##    &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1     6      160    110  3.9   2.62  16.5     0     4     4  21  
##  2 1     6      160    110  3.9   2.88  17.0     0     4     4  21  
##  3 1     4      108     93  3.85  2.32  18.6     1     4     1  22.8
##  4 0     6      258    110  3.08  3.22  19.4     1     3     1  21.4
##  5 0     8      360    175  3.15  3.44  17.0     0     3     2  18.7
##  6 0     6      225    105  2.76  3.46  20.2     1     3     1  18.1
##  7 0     8      360    245  3.21  3.57  15.8     0     3     4  14.3
##  8 0     4      147.    62  3.69  3.19  20       1     4     2  24.4
##  9 0     4      141.    95  3.92  3.15  22.9     1     4     2  22.8
## 10 0     6      168.   123  3.92  3.44  18.3     1     4     4  19.2
## # … with 22 more rows</code></pre>
<p>
It is also possible to achieve this using a data frame with the specification of what you need:
</p>
<pre class="r"><code>mtcars_spec_long &lt;- mtcars_wide2 %&gt;% 
  pivot_long_spec(matches("0|1"), values_to = "mpg") %&gt;% 
  separate(name, c("am", "cyl"), sep = "_")

mtcars_spec_long</code></pre>
<pre><code>## # A tibble: 6 x 4
##   .name .value am    cyl  
##   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;
## 1 1_6   mpg    1     6    
## 2 1_4   mpg    1     4    
## 3 0_6   mpg    0     6    
## 4 0_8   mpg    0     8    
## 5 0_4   mpg    0     4    
## 6 1_8   mpg    1     8</code></pre>
<p>
Providing this spec to <code>pivot_long()</code> solves the issue:
</p>
<pre class="r"><code>mtcars_wide2 %&gt;% 
  pivot_long(spec = mtcars_spec_long, na.rm = TRUE) %&gt;% 
  select(am, cyl, everything())</code></pre>
<pre><code>## # A tibble: 32 x 11
##    am    cyl    disp    hp  drat    wt  qsec    vs  gear  carb   mpg
##    &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1     6      160    110  3.9   2.62  16.5     0     4     4  21  
##  2 1     6      160    110  3.9   2.88  17.0     0     4     4  21  
##  3 1     4      108     93  3.85  2.32  18.6     1     4     1  22.8
##  4 0     6      258    110  3.08  3.22  19.4     1     3     1  21.4
##  5 0     8      360    175  3.15  3.44  17.0     0     3     2  18.7
##  6 0     6      225    105  2.76  3.46  20.2     1     3     1  18.1
##  7 0     8      360    245  3.21  3.57  15.8     0     3     4  14.3
##  8 0     4      147.    62  3.69  3.19  20       1     4     2  24.4
##  9 0     4      141.    95  3.92  3.15  22.9     1     4     2  22.8
## 10 0     6      168.   123  3.92  3.44  18.3     1     4     4  19.2
## # … with 22 more rows</code></pre>
<p>
Stay tuned to Hadley Wickham’s <a href="https://twitter.com/hadleywickham">twitter</a> as there will definitely be announcements soon!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-03-20-pivot.html</guid>
  <pubDate>Wed, 20 Mar 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2</title>
  <link>https://b-rodrigues.github.io/posts/2019-03-05-historical_vowpal_part2.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://youtu.be/BilPXIt0R2w?t=41"> <img src="https://b-rodrigues.github.io/assets/img/wabbit_reading.jpg" title="Vowpal Wabbit is fast as heck" width="80%" height="auto"></a>
</p>
</div>
<p>
In <a href="../posts/2019-03-03-historical_vowpal.html">part 1</a> of this series I set up Vowpal Wabbit to classify newspapers content. Now, let’s use the model to make predictions and see how and if we can improve the model. Then, let’s train the model on the whole data.
</p>
<section id="step-1-prepare-the-data" class="level2">
<h2 class="anchored" data-anchor-id="step-1-prepare-the-data">
Step 1: prepare the data
</h2>
<p>
The first step consists in importing the test data and preparing it. The test data need not be large and thus can be imported and worked on in R.
</p>
<p>
I need to remove the target column from the test set, or else it will be used to make predictions. If you do not remove this column the accuracy of the model will be very high, but it will be wrong since, of course, you do not have the target column at running time… because it is the column that you want to predict!
</p>
<pre class="r"><code>library("tidyverse")
library("yardstick")

small_test &lt;- read_delim("data_split/small_test.txt", "|",
                      escape_double = FALSE, col_names = FALSE,
                      trim_ws = TRUE)

small_test %&gt;%
    mutate(X1= " ") %&gt;%
    write_delim("data_split/small_test2.txt", col_names = FALSE, delim = "|")</code></pre>
<p>
I wrote the data in a file called <code>small_test2.txt</code> and can now use my model to make predictions:
</p>
<pre class="r"><code>system2("/home/cbrunos/miniconda3/bin/vw", args = "-t -i vw_models/small_oaa.model data_split/small_test2.txt -p data_split/small_oaa.predict")</code></pre>
<p>
The predictions get saved in the file <code>small_oaa.predict</code>, which is a plain text file. Let’s add these predictions to the original test set:
</p>
<pre class="r"><code>small_predictions &lt;- read_delim("data_split/small_oaa.predict", "|",
                          escape_double = FALSE, col_names = FALSE,
                          trim_ws = TRUE)

small_test &lt;- small_test %&gt;%
    rename(truth = X1) %&gt;%
    mutate(truth = factor(truth, levels = c("1", "2", "3", "4", "5")))

small_predictions &lt;- small_predictions %&gt;%
    rename(predictions = X1) %&gt;%
    mutate(predictions = factor(predictions, levels = c("1", "2", "3", "4", "5")))

small_test &lt;- small_test %&gt;%
    bind_cols(small_predictions)</code></pre>
</section>
<section id="step-2-use-the-model-and-test-data-to-evaluate-performance" class="level2">
<h2 class="anchored" data-anchor-id="step-2-use-the-model-and-test-data-to-evaluate-performance">
Step 2: use the model and test data to evaluate performance
</h2>
<p>
We can use the several metrics included in <code>{yardstick}</code> to evaluate the model’s performance:
</p>
<pre class="r"><code>conf_mat(small_test, truth = truth, estimate = predictions)

accuracy(small_test, truth = truth, estimate = predictions)</code></pre>
<pre class="r"><code>          Truth
Prediction  1  2  3  4  5
         1 51 15  2 10  1
         2 11  6  3  1  0
         3  0  0  0  0  0
         4  0  0  0  0  0
         5  0  0  0  0  0</code></pre>
<pre class="r"><code># A tibble: 1 x 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy multiclass     0.570</code></pre>
<p>
We can see that the model never predicted class <code>3</code>, <code>4</code> or <code>5</code>. Can we improve by adding some regularization? Let’s find out!
</p>
</section>
<section id="step-3-adding-regularization" class="level2">
<h2 class="anchored" data-anchor-id="step-3-adding-regularization">
Step 3: adding regularization
</h2>
<p>
Before trying regularization, let’s try changing the cost function from the logistic function to the hinge function:
</p>
<pre class="r"><code># Train the model
hinge_oaa_fit &lt;- system2("/home/cbrunos/miniconda3/bin/vw", args = "--oaa 5 -d data_split/small_train.txt --loss_function hinge -f vw_models/hinge_oaa.model", stderr = TRUE)

system2("/home/cbrunos/miniconda3/bin/vw", args = "-i vw_models/hinge_oaa.model -t -d data_split/small_test2.txt -p data_split/hinge_oaa.predict")


predictions &lt;- read_delim("data_split/hinge_oaa.predict", "|",
                          escape_double = FALSE, col_names = FALSE,
                          trim_ws = TRUE)

test &lt;- test %&gt;%
    select(-predictions)

predictions &lt;- predictions %&gt;%
    rename(predictions = X1) %&gt;%
    mutate(predictions = factor(predictions, levels = c("1", "2", "3", "4", "5")))

test &lt;- test %&gt;%
    bind_cols(predictions)</code></pre>
<pre class="r"><code>conf_mat(test, truth = truth, estimate = predictions)

accuracy(test, truth = truth, estimate = predictions)</code></pre>
<pre class="r"><code>          Truth
Prediction   1   2   3   4   5
         1 411 120  45  92   1
         2 355 189  12  17   0
         3  11   2   0   0   0
         4  36   4   0   1   0
         5   3   0   3   0   0</code></pre>
<pre class="r"><code># A tibble: 1 x 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy multiclass     0.462</code></pre>
<p>
Well, didn’t work out so well, but at least we now know how to change the loss function. Let’s go back to the logistic loss and add some regularization. First, let’s train the model:
</p>
<pre class="r"><code>regul_oaa_fit &lt;- system2("/home/cbrunos/miniconda3/bin/vw", args = "--oaa 5 --l1 0.005 --l2 0.005 -d data_split/small_train.txt -f vw_models/small_regul_oaa.model", stderr = TRUE)</code></pre>
<p>
Now we can use it for prediction:
</p>
<pre class="r"><code>system2("/home/cbrunos/miniconda3/bin/vw", args = "-i vw_models/small_regul_oaa.model -t -d data_split/test2.txt -p data_split/small_regul_oaa.predict")


predictions &lt;- read_delim("data_split/small_regul_oaa.predict", "|",
                          escape_double = FALSE, col_names = FALSE,
                          trim_ws = TRUE)

test &lt;- test %&gt;%
    select(-predictions)

predictions &lt;- predictions %&gt;%
    rename(predictions = X1) %&gt;%
    mutate(predictions = factor(predictions, levels = c("1", "2", "3", "4", "5")))

test &lt;- test %&gt;%
    bind_cols(predictions)</code></pre>
<p>
We can now use it for predictions:
</p>
<pre class="r"><code>conf_mat(test, truth = truth, estimate = predictions)

accuracy(test, truth = truth, estimate = predictions)</code></pre>
<pre class="r"><code>          Truth
Prediction   1   2   3   4   5
         1 816 315  60 110   1
         2   0   0   0   0   0
         3   0   0   0   0   0
         4   0   0   0   0   0
         5   0   0   0   0   0</code></pre>
<pre class="r"><code># A tibble: 1 x 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy multiclass     0.627</code></pre>
<p>
So accuracy improved, but the model only predicts class 1 now… let’s try with other hyper-parameters values:
</p>
<pre class="r"><code>regul_oaa_fit &lt;- system2("/home/cbrunos/miniconda3/bin/vw", args = "--oaa 5 --l1 0.00015 --l2 0.00015 -d data_split/small_train.txt -f vw_models/small_regul_oaa.model", stderr = TRUE)</code></pre>
<pre class="r"><code>conf_mat(test, truth = truth, estimate = predictions)

accuracy(test, truth = truth, estimate = predictions)</code></pre>
<pre class="r"><code>          Truth
Prediction   1   2   3   4   5
         1 784 300  57 108   1
         2  32  14   3   2   0
         3   0   1   0   0   0
         4   0   0   0   0   0
         5   0   0   0   0   0</code></pre>
<pre class="r"><code># A tibble: 1 x 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy multiclass     0.613</code></pre>
<p>
So accuracy is lower than previously, but at least more categories get correctly predicted. Depending on your needs, you should consider different metrics. Especially for classification problems, you might not be interested in accuracy, in particular if the data is severely unbalanced.
</p>
<p>
Anyhow, to finish this blog post, let’s train the model on the whole data and measure the time it takes to run the full model.
</p>
</section>
<section id="step-4-training-on-the-whole-data" class="level2">
<h2 class="anchored" data-anchor-id="step-4-training-on-the-whole-data">
Step 4: Training on the whole data
</h2>
<p>
Let’s first split the whole data into a training and a testing set:
</p>
<pre class="r"><code>nb_lines &lt;- system2("cat", args = "text_fr.txt | wc -l", stdout = TRUE)

system2("split", args = paste0("-l", floor(as.numeric(nb_lines)*0.995), " text_fr.txt data_split/"))

system2("mv", args = "data_split/aa data_split/train.txt")
system2("mv", args = "data_split/ab data_split/test.txt")</code></pre>
<p>
The whole data contains 260247 lines, and the training set weighs 667MB, which is quite large. Let’s train the simple multiple classifier on the data and see how long it takes:
</p>
<pre class="r"><code>tic &lt;- Sys.time()
oaa_fit &lt;- system2("/home/cbrunos/miniconda3/bin/vw", args = "--oaa 5 -d data_split/train.txt -f vw_models/oaa.model", stderr = TRUE)
Sys.time() - tic</code></pre>
<pre class="r"><code>Time difference of 4.73266 secs</code></pre>
<p>
Yep, you read that right. Training the classifier on 667MB of data took less than 5 seconds!
</p>
<p>
Let’s take a look at the final object:
</p>
<pre class="r"><code>oaa_fit</code></pre>
<pre class="r"><code> [1] "final_regressor = vw_models/oaa.model"                                   
 [2] "Num weight bits = 18"                                                    
 [3] "learning rate = 0.5"                                                     
 [4] "initial_t = 0"                                                           
 [5] "power_t = 0.5"                                                           
 [6] "using no cache"                                                          
 [7] "Reading datafile = data_split/train.txt"                                 
 [8] "num sources = 1"                                                         
 [9] "average  since         example        example  current  current  current"
[10] "loss     last          counter         weight    label  predict features"
[11] "1.000000 1.000000            1            1.0        2        1      253"
[12] "0.500000 0.000000            2            2.0        2        2      499"
[13] "0.250000 0.000000            4            4.0        2        2        6"
[14] "0.250000 0.250000            8            8.0        1        1     2268"
[15] "0.312500 0.375000           16           16.0        1        1      237"
[16] "0.250000 0.187500           32           32.0        1        1      557"
[17] "0.171875 0.093750           64           64.0        1        1      689"
[18] "0.179688 0.187500          128          128.0        2        2      208"
[19] "0.144531 0.109375          256          256.0        1        1      856"
[20] "0.136719 0.128906          512          512.0        4        4        4"
[21] "0.122070 0.107422         1024         1024.0        1        1     1353"
[22] "0.106934 0.091797         2048         2048.0        1        1      571"
[23] "0.098633 0.090332         4096         4096.0        1        1       43"
[24] "0.080566 0.062500         8192         8192.0        1        1      885"
[25] "0.069336 0.058105        16384        16384.0        1        1      810"
[26] "0.062683 0.056030        32768        32768.0        2        2      467"
[27] "0.058167 0.053650        65536        65536.0        1        1       47"
[28] "0.056061 0.053955       131072       131072.0        1        1      495"
[29] ""                                                                        
[30] "finished run"                                                            
[31] "number of examples = 258945"                                             
[32] "weighted example sum = 258945.000000"                                    
[33] "weighted label sum = 0.000000"                                           
[34] "average loss = 0.054467"                                                 
[35] "total feature number = 116335486"  </code></pre>
<p>
Let’s use the test set and see how the model fares:
</p>
<pre class="r"><code>conf_mat(test, truth = truth, estimate = predictions)

accuracy(test, truth = truth, estimate = predictions)</code></pre>
<pre class="r"><code>          Truth
Prediction   1   2   3   4   5
         1 537 175  52 100   1
         2 271 140   8   9   0
         3   1   0   0   0   0
         4   7   0   0   1   0
         5   0   0   0   0   0</code></pre>
<pre class="r"><code># A tibble: 1 x 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy multiclass     0.521</code></pre>
<p>
Better accuracy can certainly be achieved with hyper-parameter tuning… maybe the subject for a future blog post? In any case I am very impressed with Vowpal Wabbit and am certainly looking forward to future developments of <code>{RVowpalWabbit}</code>!
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-03-05-historical_vowpal_part2.html</guid>
  <pubDate>Tue, 05 Mar 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 1</title>
  <link>https://b-rodrigues.github.io/posts/2019-03-03-historical_vowpal.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://youtu.be/BilPXIt0R2w?t=41"> <img src="https://b-rodrigues.github.io/assets/img/wabbit_reading.jpg" title="Vowpal Wabbit is fast as heck" width="80%" height="auto"></a>
</p>
</div>
<p>
Can I get enough of historical newspapers data? Seems like I don’t. I already wrote four (<a href="../posts/2019-01-04-newspapers.html">1</a>, <a href="../posts/2019-01-13-newspapers_mets_alto.html">2</a>, <a href="../posts/2019-01-31-newspapers_shiny_app.html">3</a> and <a href="../posts/2019-02-04-newspapers_shiny_app_tutorial.html">4</a>) blog posts, but there’s still a lot to explore. This blog post uses a new batch of data announced on twitter:
</p>
<div style="text-align:center" ;="">
<div style="text-align: center;"><img src="https://b-rodrigues.github.io/assets/img/ralph_marschall_tweet.png" style="width:80%;" width="80%" height="auto"></div>
</div>
<p>
and this data could not have arrived at a better moment, since something else got announced via Twitter recently:
</p>
{{% tweet “1098941963527700480” %}}
<p>
I wanted to try using <a href="https://github.com/VowpalWabbit/vowpal_wabbit">Vowpal Wabbit</a> for a couple of weeks now because it seems to be the perfect tool for when you’re dealing with what I call <em>big-ish</em> data: data that is not big data, and might fit in your RAM, but is still a PITA to deal with. It can be data that is large enough to take 30 seconds to be imported into R, and then every operation on it lasts for minutes, and estimating/training a model on it might eat up all your RAM. Vowpal Wabbit avoids all this because it’s an online-learning system. Vowpal Wabbit is capable of training a model with data that it sees on the fly, which means VW can be used for real-time machine learning, but also for when the training data is very large. Each row of the data gets streamed into VW which updates the estimated parameters of the model (or weights) in real time. So no need to first import all the data into R!
</p>
<p>
The goal of this blog post is to get started with VW, and build a very simple logistic model to classify documents using the historical newspapers data from the National Library of Luxembourg, which you can download <a href="https://data.bnl.lu/data/historical-newspapers/">here</a> (scroll down and download the <em>Text Analysis Pack</em>). The goal is not to build the best model, but <em>a</em> model. Several steps are needed for this: prepare the data, install VW and train a model using <code>{RVowpalWabbit}</code>.
</p>
<section id="step-1-preparing-the-data" class="level2">
<h2 class="anchored" data-anchor-id="step-1-preparing-the-data">
Step 1: Preparing the data
</h2>
<p>
The data is in a neat <code>.xml</code> format, and extracting what I need will be easy. However, the input format for VW is a bit unusual; it resembles <em>.psv</em> files (<strong>P</strong>ipe <strong>S</strong>eparated <strong>V</strong>alues) but allows for more flexibility. I will not dwell much into it, but for our purposes, the file must look like this:
</p>
<pre><code>1 | this is the first observation, which in our case will be free text
2 | this is another observation, its label, or class, equals 2
4 | this is another observation, of class 4</code></pre>
<p>
The first column, before the “|” is the target class we want to predict, and the second column contains free text.
</p>
<p>
The raw data looks like this:
</p>
<details>
<p>
</p><summary>
Click if you want to see the raw data
</summary>
<p></p>
<pre><code>&lt;OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd"&gt;
&lt;responseDate&gt;2019-02-28T11:13:01&lt;/responseDate&gt;
&lt;request&gt;http://www.eluxemburgensia.lu/OAI&lt;/request&gt;
&lt;ListRecords&gt;
&lt;record&gt;
&lt;header&gt;
&lt;identifier&gt;digitool-publish:3026998-DTL45&lt;/identifier&gt;
&lt;datestamp&gt;2019-02-28T11:13:01Z&lt;/datestamp&gt;
&lt;/header&gt;
&lt;metadata&gt;
&lt;oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dcterms="http://purl.org/dc/terms/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd"&gt;
&lt;dc:identifier&gt;
https://persist.lu/ark:/70795/6gq1q1/articles/DTL45
&lt;/dc:identifier&gt;
&lt;dc:source&gt;newspaper/indeplux/1871-12-29_01&lt;/dc:source&gt;
&lt;dcterms:isPartOf&gt;L'indépendance luxembourgeoise&lt;/dcterms:isPartOf&gt;
&lt;dcterms:isReferencedBy&gt;
issue:newspaper/indeplux/1871-12-29_01/article:DTL45
&lt;/dcterms:isReferencedBy&gt;
&lt;dc:date&gt;1871-12-29&lt;/dc:date&gt;
&lt;dc:publisher&gt;Jean Joris&lt;/dc:publisher&gt;
&lt;dc:relation&gt;3026998&lt;/dc:relation&gt;
&lt;dcterms:hasVersion&gt;
http://www.eluxemburgensia.lu/webclient/DeliveryManager?pid=3026998#panel:pp|issue:3026998|article:DTL45
&lt;/dcterms:hasVersion&gt;
&lt;dc:description&gt;
CONSEIL COMMUNAL de la ville de Luxembourg. Séance du 23 décembre 1871. (Suite.) Art. 6. Glacière communale. M. le Bourgmcstr ¦ . Le collège échevinal propose un autro mode de se procurer de la glace. Nous avons dépensé 250 fr. cha- que année pour distribuer 30 kilos do glace; c’est une trop forte somme pour un résultat si minime. Nous aurions voulu nous aboucher avec des fabricants de bière ou autres industriels qui nous auraient fourni de la glace en cas de besoin. L’architecte qui été chargé de passer un contrat, a été trouver des négociants, mais ses démarches n’ont pas abouti. 
&lt;/dc:description&gt;
&lt;dc:title&gt;
CONSEIL COMMUNAL de la ville de Luxembourg. Séance du 23 décembre 1871. (Suite.)
&lt;/dc:title&gt;
&lt;dc:type&gt;ARTICLE&lt;/dc:type&gt;
&lt;dc:language&gt;fr&lt;/dc:language&gt;
&lt;dcterms:extent&gt;863&lt;/dcterms:extent&gt;
&lt;/oai_dc:dc&gt;
&lt;/metadata&gt;
&lt;/record&gt;
&lt;/ListRecords&gt;
&lt;/OAI-PMH&gt;</code></pre>
</details>
<p>
I need several things from this file:
</p>
<ul>
<li>
The title of the newspaper: <code>&lt;dcterms:isPartOf&gt;L’indépendance luxembourgeoise&lt;/dcterms:isPartOf&gt;</code>
</li>
<li>
The type of the article: <code>&lt;dc:type&gt;ARTICLE&lt;/dc:type&gt;</code>. Can be Article, Advertisement, Issue, Section or Other.
</li>
<li>
The contents: <code>&lt;dc:description&gt;CONSEIL COMMUNAL de la ville de Luxembourg. Séance du ….&lt;/dc:description&gt;</code>
</li>
</ul>
<p>
I will only focus on newspapers in French, even though newspapers in German also had articles in French. This is because the tag <code>&lt;dc:language&gt;fr&lt;/dc:language&gt;</code> is not always available. If it were, I could simply look for it and extract all the content in French easily, but unfortunately this is not the case.
</p>
<p>
First of all, let’s get the data into R:
</p>
<pre class="r"><code>library("tidyverse")
library("xml2")
library("furrr")

files &lt;- list.files(path = "export01-newspapers1841-1878/", all.files = TRUE, recursive = TRUE)</code></pre>
<p>
This results in a character vector with the path to all the files:
</p>
<pre class="r"><code>head(files)
[1] "000/1400000/1400000-ADVERTISEMENT-DTL78.xml"   "000/1400000/1400000-ADVERTISEMENT-DTL79.xml"  
[3] "000/1400000/1400000-ADVERTISEMENT-DTL80.xml"   "000/1400000/1400000-ADVERTISEMENT-DTL81.xml"  
[5] "000/1400000/1400000-MODSMD_ARTICLE1-DTL34.xml" "000/1400000/1400000-MODSMD_ARTICLE2-DTL35.xml"</code></pre>
<p>
Now I write a function that does the needed data preparation steps. I describe what the function does in the comments inside:
</p>
<pre class="r"><code>to_vw &lt;- function(xml_file){

    # read in the xml file
    file &lt;- read_xml(paste0("export01-newspapers1841-1878/", xml_file))

    # Get the newspaper
    newspaper &lt;- xml_find_all(file, ".//dcterms:isPartOf") %&gt;% xml_text()

    # Only keep the newspapers written in French
    if(!(newspaper %in% c("L'UNION.",
                          "L'indépendance luxembourgeoise",
                          "COURRIER DU GRAND-DUCHÉ DE LUXEMBOURG.",
                          "JOURNAL DE LUXEMBOURG.",
                          "L'AVENIR",
                          "L’Arlequin",
                          "La Gazette du Grand-Duché de Luxembourg",
                          "L'AVENIR DE LUXEMBOURG",
                          "L'AVENIR DU GRAND-DUCHE DE LUXEMBOURG.",
                          "L'AVENIR DU GRAND-DUCHÉ DE LUXEMBOURG.",
                          "Le gratis luxembourgeois",
                          "Luxemburger Zeitung – Journal de Luxembourg",
                          "Recueil des mémoires et des travaux publiés par la Société de Botanique du Grand-Duché de Luxembourg"))){
        return(NULL)
    } else {
        # Get the type of the content. Can be article, advert, issue, section or other
        type &lt;- xml_find_all(file, ".//dc:type") %&gt;% xml_text()

        type &lt;- case_when(type == "ARTICLE" ~ "1",
                          type == "ADVERTISEMENT" ~ "2",
                          type == "ISSUE" ~ "3",
                          type == "SECTION" ~ "4",
                          TRUE ~ "5"
        )

        # Get the content itself. Only keep alphanumeric characters, and remove any line returns or 
        # carriage returns
        description &lt;- xml_find_all(file, ".//dc:description") %&gt;%
            xml_text() %&gt;%
            str_replace_all(pattern = "[^[:alnum:][:space:]]", "") %&gt;%
            str_to_lower() %&gt;%
            str_replace_all("\r?\n|\r|\n", " ")

        # Return the final object: one line that looks like this
        # 1 | bla bla
        paste(type, "|", description)
    }

}</code></pre>
<p>
I can now run this code to parse all the files, and I do so in parallel, thanks to the <code>{furrr}</code> package:
</p>
<pre class="r"><code>plan(multiprocess, workers = 12)

text_fr &lt;- files %&gt;%
    future_map(to_vw)

text_fr &lt;- text_fr %&gt;%
    discard(is.null)

write_lines(text_fr, "text_fr.txt")</code></pre>
</section>
<section id="step-2-install-vowpal-wabbit" class="level2">
<h2 class="anchored" data-anchor-id="step-2-install-vowpal-wabbit">
Step 2: Install Vowpal Wabbit
</h2>
<p>
To easiest way to install VW must be using Anaconda, and more specifically the conda package manager. Anaconda is a Python (and R) distribution for scientific computing and it comes with a package manager called conda which makes installing Python (or R) packages very easy. While VW is a standalone piece of software, it can also be installed by conda or pip. Instead of installing the full Anaconda distribution, you can install Miniconda, which only comes with the bare minimum: a Python executable and the conda package manager. You can find Miniconda <a href="https://docs.conda.io/en/latest/miniconda.html">here</a> and once it’s installed, you can install VW with:
</p>
<pre><code>conda install -c gwerbin vowpal-wabbit </code></pre>
<p>
It is also possible to install VW with pip, as detailed <a href="https://pypi.org/project/vowpalwabbit/">here</a>, but in my experience, managing Python packages with pip is not super. It is better to manage your Python distribution through conda, because it creates environments in your home folder which are independent of the system’s Python installation, which is often out-of-date.
</p>
</section>
<section id="step-3-building-a-model" class="level2">
<h2 class="anchored" data-anchor-id="step-3-building-a-model">
Step 3: Building <em>a</em> model
</h2>
<p>
Vowpal Wabbit can be used from the command line, but there are interfaces for Python and since a few weeks, for R. The R interface is quite crude for now, as it’s still in very early stages. I’m sure it will evolve, and perhaps a Vowpal Wabbit engine will be added to <code>{parsnip}</code>, which would make modeling with VW really easy.
</p>
<p>
For now, let’s only use 10000 lines for prototyping purposes before running the model on the whole file. Because the data is quite large, I do not want to import it into R. So I use command line tools to manipulate this data directly from my hard drive:
</p>
<pre class="r"><code># Prepare data
system2("shuf", args = "-n 10000 text_fr.txt &gt; small.txt")</code></pre>
<p>
<code>shuf</code> is a Unix command, and as such the above code should work on GNU/Linux systems, and most likely macOS too. <code>shuf</code> generates random permutations of a given file to standard output. I use <code>&gt;</code> to direct this output to another file, which I called <code>small.txt</code>. The <code>-n 10000</code> options simply means that I want 10000 lines.
</p>
<p>
I then split this small file into a training and a testing set:
</p>
<pre class="r"><code># Adapted from http://bitsearch.blogspot.com/2009/03/bash-script-to-split-train-and-test.html

# The command below counts the lines in small.txt. This is not really needed, since I know that the 
# file only has 10000 lines, but I kept it here for future reference
# notice the stdout = TRUE option. This is needed because the output simply gets shown in R's
# command line and does get saved into a variable.
nb_lines &lt;- system2("cat", args = "small.txt | wc -l", stdout = TRUE)

system2("split", args = paste0("-l", as.numeric(nb_lines)*0.99, " small.txt data_split/"))</code></pre>
<p>
<code>split</code> is the Unix command that does the splitting. I keep 99% of the lines in the training set and 1% in the test set. This creates two files, <code>aa</code> and <code>ab</code>. I rename them using the <code>mv</code> Unix command:
</p>
<pre class="r"><code>system2("mv", args = "data_split/aa data_split/small_train.txt")
system2("mv", args = "data_split/ab data_split/small_test.txt")</code></pre>
<p>
Ok, now let’s run a model using the VW command line utility from R, using <code>system2()</code>:
</p>
<pre class="r"><code>oaa_fit &lt;- system2("~/miniconda3/bin/vw", args = "--oaa 5 -d data_split/small_train.txt -f small_oaa.model", stderr = TRUE)</code></pre>
<p>
I need to point <code>system2()</code> to the <code>vw</code> executable, and then add some options. <code>–oaa</code> stands for <em>one against all</em> and is a way of doing multiclass classification; first, one class gets classified by a logistic classifier against all the others, then the other class against all the others, then the other…. The <code>5</code> in the option means that there are 5 classes.
</p>
<p>
<code>-d data_split/train.txt</code> specifies the path to the training data. <code>-f</code> means “final regressor” and specifies where you want to save the trained model.
</p>
<p>
This is the output that get’s captured and saved into <code>oaa_fit</code>:
</p>
<pre><code> [1] "final_regressor = oaa.model"                                             
 [2] "Num weight bits = 18"                                                    
 [3] "learning rate = 0.5"                                                     
 [4] "initial_t = 0"                                                           
 [5] "power_t = 0.5"                                                           
 [6] "using no cache"                                                          
 [7] "Reading datafile = data_split/train.txt"                                 
 [8] "num sources = 1"                                                         
 [9] "average  since         example        example  current  current  current"
[10] "loss     last          counter         weight    label  predict features"
[11] "1.000000 1.000000            1            1.0        3        1       87"
[12] "1.000000 1.000000            2            2.0        1        3     2951"
[13] "1.000000 1.000000            4            4.0        1        3      506"
[14] "0.625000 0.250000            8            8.0        1        1      262"
[15] "0.625000 0.625000           16           16.0        1        2      926"
[16] "0.500000 0.375000           32           32.0        4        1        3"
[17] "0.375000 0.250000           64           64.0        1        1      436"
[18] "0.296875 0.218750          128          128.0        2        2      277"
[19] "0.238281 0.179688          256          256.0        2        2      118"
[20] "0.158203 0.078125          512          512.0        2        2       61"
[21] "0.125000 0.091797         1024         1024.0        2        2      258"
[22] "0.096191 0.067383         2048         2048.0        1        1       45"
[23] "0.085205 0.074219         4096         4096.0        1        1      318"
[24] "0.076172 0.067139         8192         8192.0        2        1      523"
[25] ""                                                                        
[26] "finished run"                                                            
[27] "number of examples = 9900"                                               
[28] "weighted example sum = 9900.000000"                                      
[29] "weighted label sum = 0.000000"                                           
[30] "average loss = 0.073434"                                                 
[31] "total feature number = 4456798"  </code></pre>
<p>
Now, when I try to run the same model using <code>RVowpalWabbit::vw()</code> I get the following error:
</p>
<pre class="r"><code>oaa_class &lt;- c("--oaa", "5",
               "-d", "data_split/small_train.txt",
               "-f", "vw_models/small_oaa.model")

result &lt;- vw(oaa_class)</code></pre>
<pre><code>Error in Rvw(args) : unrecognised option '--oaa'</code></pre>
<p>
I think the problem might be because I installed Vowpal Wabbit using conda, and the package cannot find the executable. I’ll open an issue with reproducible code and we’ll see.
</p>
<p>
In any case, that’s it for now! In the next blog post, we’ll see how to get the accuracy of this very simple model, and see how to improve it!
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-03-03-historical_vowpal.html</guid>
  <pubDate>Sun, 03 Mar 2019 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
