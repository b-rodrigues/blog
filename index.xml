<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Econometrics and Free Software</title>
<link>https://b-rodrigues.github.io/</link>
<atom:link href="https://b-rodrigues.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.6.37</generator>
<lastBuildDate>Tue, 19 Dec 2023 00:00:00 GMT</lastBuildDate>
<item>
  <title>Reproducible data science with Nix, part 8 – nixpkgs, a tale of the magic of free and open source software and a call for charity</title>
  <link>https://b-rodrigues.github.io/posts/2023-12-19-nix_for_r_part_8.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/santa_tux.jpg" width="100%">
</p>
</div>
<p>
<em>This is part 8 of a series of blog posts about Nix. Check out the other parts <a href="https://b-rodrigues.github.io/blog/index.html#category=nix">here</a>. TLDR: free and open source software is one of the most important common goods with enormous positive externalities: if you want to help funding it, keep reading!</em>
</p>
<p>
I wanted to quickly discuss about <code>nixpkgs</code>, which is the collection of packages that can be installed using Nix. Why is a project like Nix and <code>nixpkgs</code> important, even if you don’t use Nix? In actuality, you may not realise it, but you very much benefit from projects like Nix even if you don’t use it. Let me explain.
</p>
<p>
<code>nixpkgs</code> is “just” a Github repository containing thousands upon thousands of Nix expressions. When installing a package, these expressions get evaluated, and the package in question gets installed. What <em>installed</em> means can vary: sometimes the package gets built from source, sometimes a pre-compiled binary package for your operating system gets downloaded and installed.
</p>
<p>
For example, <a href="https://github.com/NixOS/nixpkgs/blob/dce218f4f35440622d2056f93ddc335351763bb4/pkgs/development/libraries/quarto/default.nix">here</a> is the Nix expression that downloads and installs Quarto. This is an example of an expression that downloads the pre-compiled Quarto package from Quarto’s own Github repository, and then <em>installs</em> it. The installation process in this case is essentially making sure that Quarto is able to find its dependencies, which also get installed from Nix, and some R and Python packages to make Quarto work well with both languages also get installed.
</p>
<p>
Because Nix packages are “nothing but” Nix expressions hosted on Github, contributing to Nix is as simple as opening a PR. For example, <a href="https://github.com/NixOS/nixpkgs/pull/263108">here</a> is a draft PR I opened to prepare for the imminent release of Quarto <code>1.4</code>. My goal when I opened this draft PR was to get used to contributing to <code>nixpkgs</code> (this was my second or third PR to <code>nixpkgs</code>, and I did some rookie mistakes when opening my first ones) and also to make the latest version of Quarto available on Nix as quickly as possible. But this PR had an unexpected consequence: through it, we found a bug in Quarto, which was then fixed before the actual release of the next version!
</p>
<p>
You see, how these things work is that when software gets released, operating system specific packages get built downstream. In the case of Quarto, this is not entirely true though: the developers of Quarto release many pre-compiled packages for Windows, macOS and several Linux distribution themselves. But they don’t do so for many other operating systems (which is entirely normal: there’s just too many! So releasing pre-built binaries for the main operating systems is more than enough), so the maintainers of these other operating systems (or package managers) have to package the software themselves. In the case of scientific software like Quarto, this usually means that it must get packaged for the Conda package manager (popular among Python users) and Nix (and there’s certainly other package managers out there that provide Quarto for other <em>exotic</em> systems) (Note: in the case of Quarto, I think the Quarto devs themselves also package it for Conda, though).
</p>
<p>
Turns out that when trying to package the pre-releases of Quarto for Nix, we discovered a regression in the upstream code that would not only affect packaging for Nix, but also for other package managers. We opened an issue on <a href="https://github.com/quarto-dev/quarto-cli/issues/7344">Quarto’s issue tracker</a> and after some discussion, the bug was identified and adressed in a matter of hours. And now everyone gets to enjoy a better version of Quarto!
</p>
<p>
This type of thing happens quite a lot in the background of open source development. My mind always gets blown when I think about the enormous amount of hours that get put by hobbyists and paid developers into open source and how well everything works. Truly a Christmas miracle (but one that happens all around the year)!
</p>
<p>
But it’s not all good and perfect. Some software is more complex to package, and requires much more work. For example the RStudio IDE is one of these. It’s a complex piece of software with many dependencies, and while it is available on Nix, it can only be installed on Windows and Linux. If you’re a Nix user on macOS, you won’t be able to install RStudio, unfortunately. And, unfortunately also, if you install RStudio using the usual macOS installer, it won’t be able to find any version of R and R packages installed with Nix. This is because RStudio needs to be patched to make it work nicely with Nix (just like we have to patch and prepare Quarto to play well with Nix). And packaging Rstudio for Nix on macOS requires some expertise and hardware that we R users/contributers to Nix don’t have all have access to.
</p>
<p>
This is where I appeal to your generosity: I have contacted a company called Numtide which offers a packaging service. You tell them which software you want on Nix, they write the expression and open a PR to <code>nixpkgs</code>. But this costs money: so I started a Gofundme which you can find <a href="https://www.gofundme.com/f/package-rstudio-for-nix-on-macos-platforms">here</a> to fund this. The goal is 4500€, which would cover the work, plus Gofundme fees and interest rate risk. I stated in the Gofundme that if the goal was not reached until the end of the year, I would donate all the money to the R foundation, but I might extend it to end of January 2024 instead.
</p>
<p>
So here is my ask: if you want to help make free and open source software better, consider donating to this Gofundme! As explained above, even if you don’t use Nix, everyone can benefit from work that is done by everyone, be it upstream or downstream. And if the goal is not met, your donation will go to the R foundation anyways!
</p>
<p>
The link to the Gofundme is <a href="https://www.gofundme.com/f/package-rstudio-for-nix-on-macos-platforms">here</a>.
</p>
<p>
I hope you can help out with this and make free and open source available and better for everyone.
</p>
<p>
Many thanks, merry Christmas and happy new year!
</p>



 ]]></description>
  <category>R</category>
  <category>nix</category>
  <guid>https://b-rodrigues.github.io/posts/2023-12-19-nix_for_r_part_8.html</guid>
  <pubDate>Tue, 19 Dec 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Reproducible data science with Nix, part 7 – Building a Quarto book using Nix on Github Actions</title>
  <link>https://b-rodrigues.github.io/posts/2023-10-20-nix_for_r_part7.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/nix_users_press_both_buttons.png" width="50%">
</p>
</div>
<p>
Back in June I self-published a book on Amazon’s Kindle Direct Publishing service and wrote a blog post detailling how you could achieve that using Quarto, which you can read <a href="../posts/2023-06-29-book_quarto.html">here</a>. The book is about <a href="https://b-rodrigues.github.io/blog/books.html">building reproducible analytical pipelines with R</a>. For the purposes of this post I made a <a href="https://github.com/b-rodrigues/kdp_quarto">template on Github</a> that you could fork and use as a starting point to write your own book. The book also gets built using Github Actions each time you push new changes: a website gets built, an E-book for e-ink devices and a Amazon KDP-ready PDF for print get also built. That template used dedicated actions to install the required version of R, Quarto, and R packages (using <code>{renv}</code>).
</p>
<p>
Let’s take a look at the workflow file:
</p>
<pre><code>on:
  push:
    branches: main

name: Render and Publish

jobs:
  build-deploy:
    runs-on: ubuntu-22.04
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Setup pandoc
        uses: r-lib/actions/setup-pandoc@v2

      - name: Setup R
        uses: r-lib/actions/setup-r@v2
        with:
          r-version: '4.3.1'

      - name: Setup renv
        uses: r-lib/actions/setup-renv@v2

      - name: Set up Quarto
        uses: quarto-dev/quarto-actions/setup@v2
        with:
          # To install LaTeX to build PDF book 
          tinytex: true 
          # uncomment below and fill to pin a version
          #version: 1.3.353

      - name: Publish to GitHub Pages (and render)
        uses: quarto-dev/quarto-actions/publish@v2
        with:
          target: gh-pages
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # this secret is always available for github actions</code></pre>
<p>
As you can see, there are a lot of different moving pieces to get this to work. Since then I discovered Nix (if you’ve not been following my adventures, there’s 6 other parts to this series as of today), and now I wrote another template that uses Nix to handle the book’s dependencies instead of dedicated actions and <code>{renv}</code>. You can find the repository <a href="https://github.com/b-rodrigues/quarto_book_nix">here</a>.
</p>
<p>
Here is what the workflow file looks like:
</p>
<pre><code>name: Build book using Nix

on:
  push:
    branches:
      - main
      - master

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout Code
      uses: actions/checkout@v3

    - name: Install Nix
      uses: DeterminateSystems/nix-installer-action@main
      with:
        logger: pretty
        log-directives: nix_installer=trace
        backtrace: full

    - name: Nix cache
      uses: DeterminateSystems/magic-nix-cache-action@main

    - name: Build development environment
      run: |
        nix-build

    - name: Publish to GitHub Pages (and render)
      uses: b-rodrigues/quarto-nix-actions/publish@main
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} </code></pre>
<p>
The first thing you should notice is that this file is much shorter.
</p>
<p>
The first step, <code>Checkout Code</code> makes the code available to the rest of the steps. I then install Nix on this runner using the Determinate Systems <code>nix-installer-action</code> and then I use another action from Determinate Systems, the <code>magic-nix-cache-action</code>. This action caches all the packages so that they don’t need to get re-built each time a change gets pushed, speeding up the process by a lot. The development environment gets then built using <code>nix-build</code>.
</p>
<p>
Finally, an action I defined runs, <code>quarto-nix-actions/publish</code>. This is a fork of the <code>quarto-actions/publish</code> action which you can find <a href="https://github.com/quarto-dev/quarto-actions/blob/main/publish/action.yml">here</a>. My fork simply makes sure that the <code>quarto render</code> and <code>quarto publish</code> commands run in the <a href="https://github.com/b-rodrigues/quarto-nix-actions/blob/f48f5a7813eb4978a2f557ff45bcc854526fb80b/publish/action.yml#L58">Nix environment defined for the project</a>.
</p>
<p>
You can see the book website <a href="https://b-rodrigues.github.io/quarto_book_nix/">here</a>; read it, it’s explains everything in much more details than this blog post! But if you’re busy, read continue reading this blog post instead.
</p>
<p>
The obvious next question is why bother with this second, Nix-centric, approach?
</p>
<p>
There are at least three reasons. The first is that it is possible to define so-called <code>default.nix</code> files that the Nix package manager then uses to build a fully reproducible development environment. This environment will contain all the packages that you require, and will not interfere with any other packages installed on your system. This essentially means that you can have project-specific <code>default.nix</code> files, each specifying the requirements for specific projects. This file can then be used as-is on any other platform to re-create your environment. The second reason is that when installing a package that requires system-level dependencies, <code>{rJava}</code> for example, all the lower-level dependencies get automatically installed as well. Forget about reading error messages of <code>install.packages()</code> to find which system development library you need to install first. The third reason is that you can pin a specific revision of <code>nixpkgs</code> to ensure reproducibility.
</p>
<p>
The <code>nixpkgs</code> mono-repository is “just” a Github repository which you can find here: <a href="https://github.com/NixOS/nixpkgs">https://github.com/NixOS/nixpkgs</a>. This repository contains Nix expressions to build and install more than 80’000 packages and you can search for installable Nix packages <a href="https://search.nixos.org/packages">here</a>.
</p>
<p>
Because <code>nixpkgs</code> is a “just” Github repository, it is possible to use a specific commit hash to install the packages as they were at a specific point in time. For example, if you use this commit, <code>7c9cc5a6e</code>, you’ll get the very latest packages as of the 19th of October 2023, but if you used this one instead: <code>976fa3369</code>, you’ll get packages from the 19th of August 2023.
</p>
<p>
This ability to deal with both underlying system-level dependencies and pin package versions at a specific commit is extremely useful on Git(Dev)Ops platforms like Github Actions. Debugging installation failures of packages can be quite frustrating, especially on Github Actions, and especially if you’re not already familiar with how Linux distributions work. Having a tool that handles all of that for you is amazing. The difficult part is writing these <code>default.nix</code> files that the Nix package manager requires to actually build these development environments. But don’t worry, with my co-author <a href="https://github.com/philipp-baumann">Philipp Baumann</a>, we developed an R package called <code>{rix}</code> which generates these <code>default.nix</code> files for you.
</p>
<p>
<code>{rix}</code> is an R package that makes it very easy to generate very complex <code>default.nix</code> files. These files can in turn be used by the Nix package manager to build project-specific environments. The book’s Github repository contains a file called <code>define_env.R</code> with the following content:
</p>
<pre class="r"><code>library(rix)

rix(r_ver = "4.3.1",
    r_pkgs = c("quarto"),
    system_pkgs = "quarto",
    tex_pkgs = c(
      "amsmath",
      "framed",
      "fvextra",
      "environ",
      "fontawesome5",
      "orcidlink",
      "pdfcol",
      "tcolorbox",
      "tikzfill"
    ),
    ide = "other",
    shell_hook = "",
    project_path = ".",
    overwrite = TRUE,
    print = TRUE)</code></pre>
<p>
<code>{rix}</code> ships the <code>rix()</code> function which takes several arguments. These arguments allow you to specify an R version, a list of R packages, a list of system packages, TeXLive packages and other options that allow you to specify your requirements. Running this code generates this <code>default.nix</code> file:
</p>
<pre><code># This file was generated by the {rix} R package v0.4.1 on 2023-10-19
# with following call:
# &gt;rix(r_ver = "976fa3369d722e76f37c77493d99829540d43845",
#  &gt; r_pkgs = c("quarto"),
#  &gt; system_pkgs = "quarto",
#  &gt; tex_pkgs = c("amsmath",
#  &gt; "framed",
#  &gt; "fvextra",
#  &gt; "environ",
#  &gt; "fontawesome5",
#  &gt; "orcidlink",
#  &gt; "pdfcol",
#  &gt; "tcolorbox",
#  &gt; "tikzfill"),
#  &gt; ide = "other",
#  &gt; project_path = ".",
#  &gt; overwrite = TRUE,
#  &gt; print = TRUE,
#  &gt; shell_hook = "")
# It uses nixpkgs' revision 976fa3369d722e76f37c77493d99829540d43845 for reproducibility purposes
# which will install R version 4.3.1
# Report any issues to https://github.com/b-rodrigues/rix
let
 pkgs = import (fetchTarball "https://github.com/NixOS/nixpkgs/archive/976fa3369d722e76f37c77493d99829540d43845.tar.gz") {};
 rpkgs = builtins.attrValues {
  inherit (pkgs.rPackages) quarto;
};
  tex = (pkgs.texlive.combine {
  inherit (pkgs.texlive) scheme-small amsmath framed fvextra environ fontawesome5 orcidlink pdfcol tcolorbox tikzfill;
});
 system_packages = builtins.attrValues {
  inherit (pkgs) R glibcLocalesUtf8 quarto;
};
  in
  pkgs.mkShell {
    LOCALE_ARCHIVE = if pkgs.system == "x86_64-linux" then  "${pkgs.glibcLocalesUtf8}/lib/locale/locale-archive" else "";
    LANG = "en_US.UTF-8";
    LC_ALL = "en_US.UTF-8";
    LC_TIME = "en_US.UTF-8";
    LC_MONETARY = "en_US.UTF-8";
    LC_PAPER = "en_US.UTF-8";
    LC_MEASUREMENT = "en_US.UTF-8";

    buildInputs = [  rpkgs tex system_packages  ];
  }</code></pre>
<p>
This file defines the environment that is needed to build your book: be it locally on your machine, or on a GitOps platform like Github Actions. All that matters is that you have the Nix package manager installed (thankfully, it’s available for Windows –through WSL2–, Linux and macOS).
</p>
<p>
Being able to work locally on a specific environment, defined through code, and use that environment on the cloud as well, is great. It doesn’t matter that the code runs on Ubuntu on the Github Actions runner, and if that operating system is not the one you use as well. Thanks to Nix, your code will run on exactly the same environment. Because of that, you can use <code>ubuntu-latest</code> as your runner, because exactly the same packages will always get installed. This is not the case with my first template that uses dedicated actions and <code>{renv}</code>: there, the runner uses <code>ubuntu-22.04</code>, a fixed version of the Ubuntu operating system. The risk here, is that once these runners get decommissioned (Ubuntu 22.04 is a <em>long-term support</em> release of Ubuntu, so it’ll stop getting updated sometime in 2027), my code won’t be able to run anymore. This is because there’s no guarantee that the required version of R, Quarto, and all the other packages I need will be installable on that new release of Ubuntu. So for example, suppose I have the package <code>{foo}</code> at version 1.0 that requires the system-level development library <code>bar-dev</code> at version 0.4 to be installed on Ubuntu. This is not an issue now, as Ubuntu 22.04 ships version 0.4 of <code>bar-dev</code>. But it is very unlikely that the future version of Ubuntu from 2027 will ship that version, and there’s no guarantee my package will successfully build and work as expected with a more recent version of <code>bar-dev</code>. With Nix, this is not an issue; because I pin a specific commit of <code>nixpkgs</code>, not only will <code>{foo}</code> at version 1.0 get installed, its dependency <code>bar-dev</code> at version 0.4 will get installed by Nix as well, and get used to build <code>{foo}</code>. It doesn’t matter that my underlying operating system ships a more recent version of <code>bar-dev</code>. I really insist on this point, because this is not something that you can easily deal with, even with Docker. This is because when you use Docker, you need to be able to rebuild the image as many times as you need (the alternative is to store, forever, the built image), and just like for Github Actions runners, the underlying Ubuntu image will be decommissioned and stop working one day.
</p>
<p>
In other words, if you need long-term reproducibility, you should really consider using Nix, and even if you don’t need long-term reproducibility, you should really consider using Nix. This is because Nix makes things much easier. But there is one point where Nix is at a huge disadvantage when compared to the alternatives: the entry cost is quite high, as I’ve discussed in my <a href="../posts/2023-10-05-repro_overview.html">previous blog post</a>. But I’m hoping that through my blog posts, this entry cost is getting lowered for R users!
</p>



 ]]></description>
  <category>R</category>
  <category>nix</category>
  <guid>https://b-rodrigues.github.io/posts/2023-10-20-nix_for_r_part7.html</guid>
  <pubDate>Fri, 20 Oct 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>An overview of what’s out there for reproducibility with R</title>
  <link>https://b-rodrigues.github.io/posts/2023-10-05-repro_overview.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/like_this.jpg" width="100%">
</p>
</div>
<p>
In this short blog post I’ll be summarizing what I learnt these past years about reproducibility with R. I’ll give some high-level explanations about different tools and then link to different blog posts of mine.
</p>
<p>
I see currently two main approaches with some commonalities, so let’s start with the commonalities.
</p>
<section id="commonalities" class="level2">
<h2 class="anchored" data-anchor-id="commonalities">
Commonalities
</h2>
<p>
These are aspects that I think will help you build reproducible projects, but that are not strictly necessary. These are:
</p>
<ul>
<li>
Git for code versioning;
</li>
<li>
unit tests (be it on your code or data);
</li>
<li>
literate programming;
</li>
<li>
packaging code;
</li>
<li>
build automation.
</li>
</ul>
<p>
I think that these aspects are really very important nice-to-haves, but depending on the project you might not have to use all these tools or techniques (but I would really recommend that you think very hard about these requirements and make sure that you actually, really, don’t need them).
</p>
<p>
What’s also important is how you organize the work if you’re in a team. Making sure that everyone is on the same page and uses the same tools and approaches is very important.
</p>
<p>
Now that we have the commonalities out of the way, let’s discuss the “two approaches”. Let’s start by the most popular one.
</p>
</section>
<section id="docker-and-something-else" class="level2">
<h2 class="anchored" data-anchor-id="docker-and-something-else">
Docker and “something else”
</h2>
<p>
Docker is a very popular containerisation solution. The idea is to build an <em>image</em> that contains everything needed to run and rebuild your project in a single command. You can add a specific version of R with the required packages in it, your project files and so on. You could even add the data directly into the image or provide the required data at run-time, it’s up to you.
</p>
<p>
The “something else” can be several things, but they all deal with the problem of providing the right packages for your analysis. You see, if you run an analysis today, you’ll be using certain versions of packages. The same versions of packages need to be made available inside that Docker image. To do so, a popular choice for R users is to use <a href="https://rstudio.github.io/renv/index.html">{renv}</a>, but there’s also <a href="https://groundhogr.com/">{groundhog}</a> and <a href="https://github.com/gesistsa/rang">{rang}</a>. You could also use CRAN snapshots from the <a href="https://packagemanager.posit.co/client/#/repos/cran/setup?snapshot=2023-09-25&amp;r_environment=other">Posit Public Package Manager</a>. Whatever you choose, Docker by itself is not enough: Docker provides a base where you can then add these other things on top.
</p>
<p>
To know more, read this:
</p>
<ul>
<li>
<a href="https://www.brodrigues.co/blog/2022-11-19-raps/">https://www.brodrigues.co/blog/2022-11-19-raps/</a>
</li>
<li>
<a href="https://www.brodrigues.co/blog/2022-11-30-pipelines-as/">https://www.brodrigues.co/blog/2022-11-30-pipelines-as/</a>
</li>
<li>
<a href="https://www.brodrigues.co/blog/2023-05-08-dock_dev_env/">https://www.brodrigues.co/blog/2023-05-08-dock_dev_env/</a>
</li>
<li>
<a href="https://www.brodrigues.co/blog/2023-01-12-repro_r/">https://www.brodrigues.co/blog/2023-01-12-repro_r/</a>
</li>
</ul>
<p>
By combining Docker plus any of the other packages listed above (or by using the PPPM) you can quite easily build reproducible projects, because what you end up doing, is essentially building something like a capsule that contains everything needed to run the project (this capsule is what is called an <em>image</em>). Then, you don’t run R and the scripts to build the project, you run the image, and within that image, R is executed on the provided scripts. This running instance of an image is called a <em>container</em>. This approach is by far the most popular and can even be used on Github Actions if your project is hosted on Github. On a scale from 1 to 10, I would say that the entry cost is about 3 if you already have some familiarity with Linux, but can go up to 7 if you’ve never touched Linux. What does Linux have to do with all this? Well, the Docker images that you are going to build will be based on Linux (most of the time the Ubuntu distribution) so familiarity with Linux or Ubuntu is a huge plus. You could use <code>{renv}</code>, <code>{rang}</code> or <code>{groundhog}</code> without Docker, directly on your PC, but the issue here is that your operating system and the version of R changes through time. And both of these can have an impact on the reproducibility of your project. Hence, why we use Docker to, in a sense, “freeze” both the underlying operating system and version of R inside that image, and then, every container executed from that image will have the required versions of software.
</p>
<p>
One issue with Docker is that if you build an image today, the underlying Linux distribution will get out of date at some point, and you won’t be able to rebuild the image. So you either need to build the image and store it forever, or you need to maintain your image and port your code to newer base Ubuntu images.
</p>
</section>
<section id="nix" class="level2">
<h2 class="anchored" data-anchor-id="nix">
Nix
</h2>
<p>
Nix is a package manager for Linux (and Windows through WSL) and macOS, but also a programming language that focuses on reproducibility of software builds, meaning that using Nix it’s possible to build software in a completely reproducible way. Nix is incredibly flexible, so it’s also possible to use it to build reproducible development environments, or run reproducible analytical pipelines. What Nix doesn’t easily allow, unlike <code>{renv}</code> for example, is to install a specific version of one specific package. But I also wrote a package called <a href="https://b-rodrigues.github.io/rix/">{rix}</a> (co-authored by Philipp Baumann) that makes it easier for R users to get started with Nix and also allows to install arbitrary versions of packages easily using the Nix package manager. So you can define an environment with any version of R, plus corresponding packages, and install specific versions of specific packages if needed as well. Packages that are hosted on Github can also get easily installed if needed. Let me make this clear: using Nix, you install both R and R packages so there’s no need to use <code>install.packages()</code> anymore. Everything is managed by Nix.
</p>
<p>
Using Nix, we can define our environment and build instructions as code, and have the build process always produce exactly the same result. This definition of the environment and build instructions are written using the Nix programming language inside a simple text file, which then gets used to actually realize the build. This means that regardless of “when” or “where” you rebuild your project, <em>exactly</em> the same packages (all the way down to the system libraries and compilers and all that stuff we typically never think about) will get installed to rebuild the project.
</p>
<p>
Essentially, using the Nix package manager, you can replace Docker + any of the other tools listed above to build reproducible projects. The issue with Nix however is that the entry cost is quite high: even if you’re already familiar with Linux and package managers, Nix is really an incredible deep tool. So I would say that the entry cost is around 9 out of 10…, but to bring this entry cost down, I have written 6 blog posts to get you started:
</p>
<ul>
<li>
<a href="https://www.brodrigues.co/blog/2023-07-13-nix_for_r_part1/">https://www.brodrigues.co/blog/2023-07-13-nix_for_r_part1/</a>
</li>
<li>
<a href="https://www.brodrigues.co/blog/2023-07-19-nix_for_r_part2/">https://www.brodrigues.co/blog/2023-07-19-nix_for_r_part2/</a>
</li>
<li>
<a href="https://www.brodrigues.co/blog/2023-07-30-nix_for_r_part3/">https://www.brodrigues.co/blog/2023-07-30-nix_for_r_part3/</a>
</li>
<li>
<a href="https://www.brodrigues.co/blog/2023-08-12-nix_for_r_part4/">https://www.brodrigues.co/blog/2023-08-12-nix_for_r_part4/</a>
</li>
<li>
<a href="https://www.brodrigues.co/blog/2023-09-15-nix_for_r_part5/">https://www.brodrigues.co/blog/2023-09-15-nix_for_r_part5/</a>
</li>
<li>
<a href="https://www.brodrigues.co/blog/2023-09-20-nix_for_r_part6/">https://www.brodrigues.co/blog/2023-09-20-nix_for_r_part6/</a>
</li>
</ul>
<p>
Also, by the way, it is entirely possible to build a Docker image based on Ubuntu, install the Nix package manager on it, and then use Nix inside Docker to install the right software to build a reproducible project. This approach is extremely flexible, as it uses the best of both worlds in my opinion: we can take advantage of the popularity of Docker so that we can run containers anywhere, but use Nix to truly have reproducible builds. This also solves the issue I discussed before: if you’re using Nix inside Docker, it doesn’t matter if the base image gets outdated: simply use a newer base image, and Nix will take care of always installing the right versions of the needed pieces of software for your project.
</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">
Conclusion
</h2>
<p>
So which should you learn, Docker or Nix? While Docker is certainly more popular these days, I think that Nix is very interesting and not that hard to use <strong>once</strong> you learnt the basics (which does take some time). But the entry costs for any of these tools is in the end quite high and, very annoyingly, building reproducible projects does not get enough recognition, even in science where reproducibility is supposedly one of its corner stones. However, I think that you should definitely invest time in learning the tools and best practices required for building reproducible projects, because by making sure that a project is reproducible you end up increasing its quality as well. Furthermore, you avoid stressful situations where you get asked “hey, where did that graph/result/etc come from?” and you have no idea why the script that supposedly built that output does not reproduce the same output again.
</p>
<p>
If you read all the blog posts above but still want to learn and know more about reproducibility you can get my <a href="https://leanpub.com/raps-with-r/c/blog_reader">ebook at a discount</a> or get a physical copy on <a href="https://www.amazon.com/Building-reproducible-analytical-pipelines-R/dp/B0C87H6MGF/ref=sr_1_1?keywords=building+reproducible+analytical+pipelines&amp;sr=8-1">Amazon</a> or you can <a href="https://raps-with-r.dev/">read it for free</a>. That book does not discuss Nix, but I will very certainly be writing another book focusing this time on Nix during 2024.
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>proramming</category>
  <guid>https://b-rodrigues.github.io/posts/2023-10-05-repro_overview.html</guid>
  <pubDate>Thu, 05 Oct 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>ZSA Voyager review</title>
  <link>https://b-rodrigues.github.io/posts/2023-09-29-voyager.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=NlgmH5q9uNk"> <img src="https://b-rodrigues.github.io/assets/img/travelling_with_my_keyboard.png" title="Click for banger" width="80%" height="auto"></a>
</p>
</div>
<p>
Now for something completely different than our usual programming: today I’m sharing my thoughts on the latest ZSA mechanical keyboard, the <a href="https://www.zsa.io/voyager/buy/">Voyager</a>. First things first: this is in no way shape or form sponsored by ZSA. But Erez, if you’d like to send me money you’re more than welcome.
</p>
<p>
Here’s what the keyboard looks like:
</p>
<div style="text-align:center;">
<video width="854" height="480" controls="" autoplay="" muted="" loop="">
<source src="../assets/img/voyager.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Yes, it comes with RGB LEDs. Why do mechanical keyboards come with RGB LEDs? No idea, I usually don’t care for them, but unlike other keyboards from ZSA, you cannot order the Voyager without them. So now my keyboard looks like a Christmas tree. And by the way, yes, you can get the good old regular QWERTY layout instead of the dots. I chose to get blank keys because I don’t look at my keyboard when typing.
</p>
<p>
It’s quite small and there aren’t many keys on it. But it’s very comfortable to use. I’ll try to explain why.
</p>
<p>
If you don’t know anything about mechanical keyboards, I think you might find this blog post useful. I’ll explain the basics and also why you might want to consider one if you’re a programmer.
</p>
<p>
First of all, let me just get this out of the way: typing on a mechanical keyboard will not make you type any faster. I think that people that buy mechanical keyboards also tend to be people that spend some time learning how to touch-type, so yeah, they’ll type faster than most people that never bother to learn to touch-type, but two touch-typists, one that use a mechanical keyboard and another that uses a normal keyboard, will roughly type at the same speed.
</p>
<p>
So if not for speed, why bother with mechanical keyboards?
</p>
<p>
In my opinion, the main advantage of mechanical keyboards is customization. You can customize absolutely everything: not just how the keyboard looks, but also how it works. Many mechanical keyboards come with a firmware called QMK which enables you to program each key. So for instance I have a key that types “&lt;-” and another that types “%&gt;%”, very useful for an R programmer like myself. You can configure such things at the level of you favourite text editor, but it’s nice to also have the option at the level of the hardware, because it means that you can now easily type these programming symbols anywhere: on social media, an email, a forum… Configuring this firmware on keyboards made by ZSA, like the Voyager, is incredibly easy: there’s a web-application called Oryx that you can use for all they keyboards. Simply select the keys, change what you must and flash the new firmware to your keyboard! For example here, I’m configuring a key to output “,” when pressed, but to output “_” when double-tapped:
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/oryx.png" width="80%" height="auto">
</p>
</div>
<p>
And for the flashing process you don’t even have to install anything on your computer: if you’re using a Chromium based browser like Google Chrome, you can flash it from the Web Browser. You can even browse other people’s configurations, for example here’s <a href="https://configure.zsa.io/voyager/layouts/l9eWG/ErJeQ/0">mine</a> (and you can even customize the RGB).
</p>
<p>
I use the French ergonomic BÉPO layout, the English equivalent would be Dvorak. You can add different layers, for example by holding one key, all the other keys now output something different when pressed (like holding down the SHIFT key produces capital letters), but you can make any key switch layers and then any other key output anything. For example I have a layer in which I configured keys to move my mouse and click. I don’t use that very often, but in case I forget my mouse if I’m traveling, I could also use my keyboard as a mouse now.
</p>
<p>
Hardware can also be customized: the color of the keyboard, but also the keycaps (I have the blank ones, as you’ve seen above) and also the switches. If you’re not into mechanical keyboard I guess this doesn’t mean anything. Keycaps are these:
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/caps.jpg" width="80%" height="auto">
</p>
</div>
<p>
and switches are these:
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/switches.jpg" width="80%" height="auto">
</p>
</div>
<p>
And you can change either the caps, the switches or both. The keyboard is <em>hot-swapable</em> meaning that you can actually replace the switches. Here is a switch with a keycap on it that I removed from my keyboard:
</p>
<div style="text-align:center;">
<video width="854" height="480" controls="" loop="">
<source src="../assets/img/pressing_switch.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Again, if you’re not into mechanical keyboard it’s difficult to see why this is really a nice thing: but being able to change caps and switches allows you to truly make the keyboard feel and sound just right for you.
</p>
<p>
Let me explain: there’s switches that make no sound and that are very easy to press: they’re called linear switches. Then there’s switches that make a nice clicky sound and that require more force to press, and there’s switches that make even more noise and that require a lot of force to press. The harder ones are so-called “clicky” switches and the intermediate ones “tactile”. There’s a lot more subtlety than that, but even I don’t know everything about switches. What matters is that you can swap these, and find the ones that are just right for you. My first mechanical keyboard, also one from ZSA, the Ergodox EZ (pictured below) came with red switches. At the time, I had no idea what switches I should get, so I bought the reds because they were silent, and I figured that I would prefer silent ones. Turns out that I absolutely hated them. It didn’t fill right because they were extremely light, and simply by resting my hands on the keyboard I would press keys by mistake. Then I bought clicky switches, and since then haven’t looked back. Clicky switches make a nice “click” sound when you press them, because there’s actually a little mechanism that produces this noise when you press them. It’s like pushing an actual button. Much more satisfying, and much better, in my opinion, for typing. So for this board I got the white ones, which are the clickiest. It’s also the one’s I had for my other mechanical keyboard, the Planck EZ, also by ZSA:
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/planck.jpg" width="80%" height="auto">
</p>
</div>
<p>
I also experimented with heavier ones on my other board (an Idobao ID75, a somewhat overgrown Planck, not by ZSA but also very customizable through <a href="https://get.vial.today/">VIAL</a>):
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/idobao.jpg" width="80%" height="auto">
</p>
</div>
<p>
The switches there are heavier, and I enjoy them a lot as well.
</p>
<p>
Now, this keyboard isn’t cheap, but it does come with a lot of nice stuff in the box. You get 3 usb cables, 4 more switches, several keycaps more, and a carrying bag.
</p>
<p>
And as you can see, it’s a so-called low profile keyboard:
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/low_voyager_1.jpg" width="80%" height="auto">
</p>
</div>
<p>
You can even remove these little feet from the keyboard (they’re magnetic):
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/magnetic_feet.jpg" width="80%" height="auto">
</p>
</div>
<p>
to get it even lower:
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/low_voyager_2.jpg" width="80%" height="auto">
</p>
</div>
<p>
I’ve never had such a keyboard in the past and I must say that it’s really comfortable to use. I don’t need to use any wrist rests anymore, which is kinda nice. Because it’s low-profile the switches and keycaps are different from the usual ones you get for other mechanical keyboards:
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/mx_vs_choc.jpg" width="80%" height="auto">
</p>
</div>
<p>
Anyways, I really enjoy this form factor, not just that it’s low profile, but also that it doesn’t have a lot of keys. I like this, because my hands don’t need to move at all. If I need numbers for example, I switch layers, and now the keys that would usually be directly under my fingers will output numbers when pressed. So instead of my fingers going to the keys, they keys go to my fingers. It gets some time to get used to this, but once you know how to do that, it’s just great.
</p>
<p>
So, should you buy a Voyager? I might not advise it to you for a first mechanical keyboard. There’s much cheaper ones that you can get and see if mechanical keyboards are for you. If you can, try some out in a store, I think it’s especially important to find the right switches for your style. As I’ve written above, I started with linear reds which I hated, thankfully I tried clicky whites before abandoning my mechanical keyboard adventure. If you’re already a hardened mechanical keyboard user, and are looking for a light keyboard that you can take with you on your travels, I think that it’s hard to overlook the Voyager. There are other nice, very transportable keyboards out there, but the build quality of ZSA and the firmware customization tool they provide, Oryx, is hard to beat.
</p>


 ]]></description>
  <category>R</category>
  <category>proramming</category>
  <guid>https://b-rodrigues.github.io/posts/2023-09-29-voyager.html</guid>
  <pubDate>Fri, 29 Sep 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Reproducible data science with Nix, part 6 – CI/CD has never been easier</title>
  <link>https://b-rodrigues.github.io/posts/2023-09-20-nix_for_r_part6.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/gymnastics.png" width="50%">
</p>
</div>
<p>
<em>Warning: I highly recommend you read this <a href="../posts/2023-07-19-nix_for_r_part2.html">blog post</a> first, which will explain how to run a pipeline inside Nix in detail. This blog post will assume that you’ve read that one, and it would also help if you’re familiar with Github Actions, if not, read this <a href="../posts/2022-11-19-raps.html">other blog post of mine as well</a></em>
</p>
<p>
This is getting ridiculous. The meme that I’m using as a header for this blog post perfectly summaries how I feel.
</p>
<p>
This will be a short blog post, because Nix makes things so easy that there’s not much to say. I wanted to try how I could use Nix on Github Actions to run a reproducible pipeline. This pipeline downloads some data, prepares it, and fits a machine learning model. It is code that I had laying around from an old video on the now deprecated <code>{drake}</code> package, <code>{targets}</code> predecessor.
</p>
<p>
You can find the pipeline <a href="https://github.com/b-rodrigues/nix_ml_cicd_demo/tree/main">here</a> and you can also take a look at the same pipeline but which uses Docker <a href="https://github.com/b-rodrigues/mlops_demo">here</a> for comparison purposes.
</p>
<p>
What I wanted to achieve was the following: I wanted to set up a reproducible environment with Nix on my computer, work on my pipeline locally, and then have it run on Github Actions as well. But I wanted my pipeline to run exactly on the same environment as the one I was using to develop it. In a world without Nix, this means using a mix of <code>{renv}</code> (or <code>{groundhog}</code> or <code>{rang}</code>) and a Docker image that ships the right version of R. I would then need to write a Github Actions workflow file that builds that Docker image, then runs it and saves the outputs as artifacts. Also, in practice that image would not be exactly the same as my local environment: I would have the same version of R and R packages, but every other system-level dependency would be a different version unless I use that Dockerized environment to develop locally, something I suggested you should do merely <a href="https://www.brodrigues.co/blog/2023-05-08-dock_dev_env/">4 months ago</a> (oooh, how blind was I!).
</p>
<p>
With Nix, not only can I take care of the version of R and R packages with one single tool but also every underlying system-level dependency gets handled by Nix. So if I use a package that requires, say, Java, or GDAL, or any other of these usual suspects that make installing their R bindings so tricky, Nix will handle this for me without any intervention on my part. I can also use this environment to develop locally, and then, once I’m done working locally, <em>exactly</em> this environment, <em>exactly</em> every bit of that environment, will get rebuilt and used to run my code on Github Actions.
</p>
<p>
So <a href="https://github.com/b-rodrigues/nix_ml_cicd_demo">this is the repository</a> where you can find the code. There’s a <code>{targets}</code> script that defines the pipeline and a <code>functions/</code> folder with some code that I wrote for said pipeline. What’s unfamiliar to you (unless you’ve been reading my Nix adventures since the beginning) is the <code>default.nix</code> file:
</p>
<pre><code>let
 pkgs = import (fetchTarball "https://github.com/NixOS/nixpkgs/archive/976fa3369d722e76f37c77493d99829540d43845.tar.gz") {};
 rpkgs = builtins.attrValues {
  inherit (pkgs.rPackages) tidymodels vetiver targets xgboost;
};
 system_packages = builtins.attrValues {
  inherit (pkgs) R;
};
in
 pkgs.mkShell {
  buildInputs = [  rpkgs system_packages  ];
 }</code></pre>
<p>
This few lines of code define an environment that pulls packages from revision <code>976fa3369d722e76f37c77493d99829540d43845</code> of <code>nixpkgs</code>. It installs the packages <code>{tidymodels}</code>, <code>{vetiver}</code>, <code>{targets}</code> and <code>{xgboost}</code> (actually, I’m not using <code>{vetiver}</code> for this <em>yet</em>, so it could even be removed). Then it also installs R. Because we’re using that specific revision of Nix, exactly the same packages (and their dependencies) will get installed, regardless of when we build this environment. I want to insist that this file is 12 lines long and it defines a complete environment. The equivalent <code>Dockerfile</code> is much longer, and not even completely reproducible, and I would have needed external tools like <code>{renv}</code> (or use the Posit CRAN mirror dated snapshots) as you can check out <a href="https://github.com/b-rodrigues/mlops_demo">here</a>.
</p>
<p>
Let’s now turn our attention to the workflow file:
</p>
<pre><code>name: train_model

on:
  push:
    branches: [main]

jobs:
  targets:
    runs-on: ubuntu-latest
    env:
      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}
    steps:

      - uses: actions/checkout@v3

      - name: Install Nix
        uses: DeterminateSystems/nix-installer-action@main
        with:
          logger: pretty
          log-directives: nix_installer=trace
          backtrace: full

      - name: Nix cache
        uses: DeterminateSystems/magic-nix-cache-action@main

      - name: Build development environment
        run: |
          nix-build

      - name: Check if previous runs exists
        id: runs-exist
        run: git ls-remote --exit-code --heads origin targets-runs
        continue-on-error: true

      - name: Checkout previous run
        if: steps.runs-exist.outcome == 'success'
        uses: actions/checkout@v2
        with:
          ref: targets-runs
          fetch-depth: 1
          path: .targets-runs

      - name: Restore output files from the previous run
        if: steps.runs-exist.outcome == 'success'
        run: |
          nix-shell default.nix --run "Rscript -e 'for (dest in scan(\".targets-runs/.targets-files\", what = character())) {
            source &lt;- file.path(\".targets-runs\", dest)
            if (!file.exists(dirname(dest))) dir.create(dirname(dest), recursive = TRUE)
            if (file.exists(source)) file.rename(source, dest)
          }'"

      - name: Run model
        run: |
          nix-shell default.nix --run "Rscript -e 'targets::tar_make()'"

      - name: Identify files that the targets pipeline produced
        run: git ls-files -mo --exclude=renv &gt; .targets-files

      - name: Create the runs branch if it does not already exist
        if: steps.runs-exist.outcome != 'success'
        run: git checkout --orphan targets-runs

      - name: Put the worktree in the runs branch if the latter already exists
        if: steps.runs-exist.outcome == 'success'
        run: |
          rm -r .git
          mv .targets-runs/.git .
          rm -r .targets-runs

      - name: Upload latest run
        run: |
          git config --local user.name "GitHub Actions"
          git config --local user.email "actions@github.com"
          rm -r .gitignore .github/workflows
          git add --all -- ':!renv'
          for file in $(git ls-files -mo --exclude=renv)
          do
            git add --force $file
          done
          git commit -am "Run pipeline"
          git push origin targets-runs

      - name: Prepare failure artifact
        if: failure()
        run: rm -rf .git .github .targets-files .targets-runs

      - name: Post failure artifact
        if: failure()
        uses: actions/upload-artifact@main
        with:
          name: ${{ runner.os }}-r${{ matrix.config.r }}-results
          path: .</code></pre>
<p>
The workflow file above is heavily inspired from the one you get when you run <code>targets::tar_github_actions()</code>. Running this puts the following <a href="https://github.com/ropensci/targets/blob/22103e19584ea15ae44328c07bc9d2699b004a47/inst/templates/github_actions.yaml">file</a> on the root of your <code>{targets}</code> project. This file is a Github Actions workflow file, which means that each time you push your code on Github, the pipeline will run in the cloud. However it needs you to use <code>{renv}</code> with the project so that the right packages get installed. You’ll also see a step called <code>Install Linux dependencies</code> which you will have to adapt to your project.
</p>
<p>
All of this can be skipped when using Nix. All that must be done is installing Nix itself, using the <code>nix-installer-action</code> from Determinate Systems, then using the <code>magic-nix-cache-action</code> which caches the downloaded packages so we don’t need to wait for the environment to build each time we push (unless we changed the environment of course) and that’s about it. We then build the environment on Github Actions using <code>nix-build</code> and then run the pipeline using <code>nix-shell default.nix –run “Rscript -e ‘targets::tar_make()’”</code>. All the other steps are copied almost verbatim from the linked file above and make sure that the computed targets only get recomputed if I edit anything that impacts them, and also that they get pushed into a branch called <code>targets-runs</code>. I say <em>copied almost verbatim</em> because some steps must run inside R, so we need to specify that we want to use the R that is available through the Nix environment we just built.
</p>
<p>
Now, each time we push, the following happens:
</p>
<ul>
<li>
if we didn’t change anything to <code>default.nix</code>, the environment gets retrieved from the cache. If we did change something, then environment gets rebuilt (or rather, only the parts that need to be rebuilt, the rest will still get retrieved from the cache)
</li>
<li>
if we didn’t change anything to the <code>_targets.R</code> pipeline itself, then every target will get skipped. If not, only the targets that need to get recomputed will get recomputed.
</li>
</ul>
<p>
One last thing that I didn’t mention: on line 9 you’ll see this:
</p>
<pre><code>runs-on: ubuntu-latest</code></pre>
<p>
this means that the Github Actions will run on the latest available version of Ubuntu, which is obviously not fixed. When the next LTS gets released in April 2024, this pipeline will be running on Ubuntu 24.04 instead of the current LTS, version 22.04. This is not good practice because we don’t want the underlying operating system to be changing, because this could have an impact on the reproducibility of our pipeline. But with Nix, this <strong>does not matter</strong>. Remember that we are using a specific revision of <code>nixpkgs</code> for our pipeline, so the <em>exact</em> same version of not only R and R packages gets installed, but every underlying piece of software that needs to be available will be installed as well. We could be running this in 50 years on Ubuntu LTS 74.04 and it would still install the same stuff and run the same code and produce exactly the same results.
</p>
<p>
This is really bonkers.
</p>
<p>
Nix is an incredibly powerful tool. I’ve been exploring and using it for 3 months now, but if something impresses me more than how useful it is, is how terribly unknown it still is. I hope that this series of blog posts will motivate other people to learn it.
</p>



 ]]></description>
  <category>R</category>
  <category>nix</category>
  <guid>https://b-rodrigues.github.io/posts/2023-09-20-nix_for_r_part6.html</guid>
  <pubDate>Wed, 20 Sep 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Reproducible data science with Nix, part 5 – Reproducible literate programming with Nix and Quarto</title>
  <link>https://b-rodrigues.github.io/posts/2023-09-15-nix_for_r_part5.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/ayylmao.png" max-width="100%">
</p>
</div>
<p>
<em>This blog post is a copy-paste from <a href="https://b-rodrigues.github.io/rix/articles/building-an-environment-for-literate-programming.html">this vignette</a></em>
</p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">
Introduction
</h2>
<p>
This vignette will walk you through setting up a development environment with <code>{rix}</code> that can be used to compile Quarto documents into PDFs. We are going to use the <a href="https://github.com/quarto-journals/jss">Quarto template for the JSS</a> to illustrate the process. The first section will show a simple way of achieving this, which will also be ideal for interactive development (writing the doc). The second section will discuss a way to build the document in a completely reproducible manner once it’s done.
</p>
</section>
<section id="starting-with-the-basics-simple-but-not-entirely-reproducible" class="level2">
<h2 class="anchored" data-anchor-id="starting-with-the-basics-simple-but-not-entirely-reproducible">
Starting with the basics (simple but not entirely reproducible)
</h2>
<p>
This approach will not be the most optimal, but it will be the simplest. We will start by building a development environment with all our dependencies, and we can then use it to compile our document interactively. But this approach is not quite reproducible and requires manual actions. In the next section we will show you to build a 100% reproducible document in a single command.
</p>
<p>
Since we need both the <code>{quarto}</code> R package as well as the <code>quarto</code> engine, we add both of them to the <code>r_pkgs</code> and <code>system_pkgs</code> of arguments of <code>{rix}</code>. Because we want to compile a PDF, we also need to have <code>texlive</code> installed, as well as some LaTeX packages. For this, we use the <code>tex_pkgs</code> argument:
</p>
<pre class="r"><code>library(rix)

path_default_nix &lt;- tempdir()

rix(r_ver = "4.3.1",
    r_pkgs = c("quarto"),
    system_pkgs = "quarto",
    tex_pkgs = c("amsmath"),
    ide = "other",
    shell_hook = "",
    project_path = path_default_nix,
    overwrite = TRUE,
    print = TRUE)</code></pre>
<pre><code>## # This file was generated by the {rix} R package v0.4.1 on 2023-12-19
## # with following call:
## # &gt;rix(r_ver = "976fa3369d722e76f37c77493d99829540d43845",
## #  &gt; r_pkgs = c("quarto"),
## #  &gt; system_pkgs = "quarto",
## #  &gt; tex_pkgs = c("amsmath"),
## #  &gt; ide = "other",
## #  &gt; project_path = path_default_nix,
## #  &gt; overwrite = TRUE,
## #  &gt; print = TRUE,
## #  &gt; shell_hook = "")
## # It uses nixpkgs' revision 976fa3369d722e76f37c77493d99829540d43845 for reproducibility purposes
## # which will install R version 4.3.1
## # Report any issues to https://github.com/b-rodrigues/rix
## let
##  pkgs = import (fetchTarball "https://github.com/NixOS/nixpkgs/archive/976fa3369d722e76f37c77493d99829540d43845.tar.gz") {};
##  rpkgs = builtins.attrValues {
##   inherit (pkgs.rPackages) quarto;
## };
##   tex = (pkgs.texlive.combine {
##   inherit (pkgs.texlive) scheme-small amsmath;
## });
##  system_packages = builtins.attrValues {
##   inherit (pkgs) R glibcLocalesUtf8 quarto;
## };
##   in
##   pkgs.mkShell {
##     LOCALE_ARCHIVE = if pkgs.system == "x86_64-linux" then  "${pkgs.glibcLocalesUtf8}/lib/locale/locale-archive" else "";
##     LANG = "en_US.UTF-8";
##     LC_ALL = "en_US.UTF-8";
##     LC_TIME = "en_US.UTF-8";
##     LC_MONETARY = "en_US.UTF-8";
##     LC_PAPER = "en_US.UTF-8";
##     LC_MEASUREMENT = "en_US.UTF-8";
## 
##     buildInputs = [  rpkgs tex system_packages  ];
##       
##   }</code></pre>
<p>
(Save these lines into a script called <code>build_env.R</code> for instance, and run the script into a new folder made for this project.)
</p>
<p>
By default, <code>{rix}</code> will install the “small” version of the <code>texlive</code> distribution available on Nix. To see which <code>texlive</code> packages get installed with this small version, you can click <a href="https://search.nixos.org/packages?channel=unstable&amp;show=texlive.combined.scheme-small&amp;from=0&amp;size=50&amp;sort=relevance&amp;type=packages&amp;query=scheme-small">here</a>. We start by adding the <code>amsmath</code> package then build the environment using:
</p>
<pre class="r"><code>nix_build()</code></pre>
<p>
Then, drop into the Nix shell with <code>nix-shell</code>, and run <code>quarto add quarto-journals/jss</code>. This will install the template linked above. Then, in the folder that contains <code>build_env.R</code>, the generated <code>default.nix</code> and <code>result</code> download the following files from <a href="https://github.com/quarto-journals/jss/">here</a>:
</p>
<ul>
<li>
article-visualization.pdf
</li>
<li>
bibliography.bib
</li>
<li>
template.qmd
</li>
</ul>
<p>
and try to compile <code>template.qmd</code> by running:
</p>
<pre><code>quarto render template.qmd --to jss-pdf</code></pre>
<p>
You should get the following error message:
</p>
<pre><code>Quitting from lines 99-101 [unnamed-chunk-1] (template.qmd)
Error in `find.package()`:
! there is no package called 'MASS'
Backtrace:
 1. utils::data("quine", package = "MASS")
 2. base::find.package(package, lib.loc, verbose = verbose)
Execution halted
</code></pre>
<p>
So there’s an R chunk in <code>template.qmd</code> that uses the <code>{MASS}</code> package. Change <code>build_env.R</code> to generate a new <code>default.nix</code> file that will now add <code>{MASS}</code> to the environment when built:
</p>
<pre class="r"><code>rix(r_ver = "4.3.1",
    r_pkgs = c("quarto", "MASS"),
    system_pkgs = "quarto",
    tex_pkgs = c("amsmath"),
    ide = "other",
    shell_hook = "",
    project_path = path_default_nix,
    overwrite = TRUE,
    print = TRUE)</code></pre>
<pre><code>## # This file was generated by the {rix} R package v0.4.1 on 2023-12-19
## # with following call:
## # &gt;rix(r_ver = "976fa3369d722e76f37c77493d99829540d43845",
## #  &gt; r_pkgs = c("quarto",
## #  &gt; "MASS"),
## #  &gt; system_pkgs = "quarto",
## #  &gt; tex_pkgs = c("amsmath"),
## #  &gt; ide = "other",
## #  &gt; project_path = path_default_nix,
## #  &gt; overwrite = TRUE,
## #  &gt; print = TRUE,
## #  &gt; shell_hook = "")
## # It uses nixpkgs' revision 976fa3369d722e76f37c77493d99829540d43845 for reproducibility purposes
## # which will install R version 4.3.1
## # Report any issues to https://github.com/b-rodrigues/rix
## let
##  pkgs = import (fetchTarball "https://github.com/NixOS/nixpkgs/archive/976fa3369d722e76f37c77493d99829540d43845.tar.gz") {};
##  rpkgs = builtins.attrValues {
##   inherit (pkgs.rPackages) quarto MASS;
## };
##   tex = (pkgs.texlive.combine {
##   inherit (pkgs.texlive) scheme-small amsmath;
## });
##  system_packages = builtins.attrValues {
##   inherit (pkgs) R glibcLocalesUtf8 quarto;
## };
##   in
##   pkgs.mkShell {
##     LOCALE_ARCHIVE = if pkgs.system == "x86_64-linux" then  "${pkgs.glibcLocalesUtf8}/lib/locale/locale-archive" else "";
##     LANG = "en_US.UTF-8";
##     LC_ALL = "en_US.UTF-8";
##     LC_TIME = "en_US.UTF-8";
##     LC_MONETARY = "en_US.UTF-8";
##     LC_PAPER = "en_US.UTF-8";
##     LC_MEASUREMENT = "en_US.UTF-8";
## 
##     buildInputs = [  rpkgs tex system_packages  ];
##       
##   }</code></pre>
<p>
Trying to compile the document results now in another error message:
</p>
<pre><code>compilation failed- no matching packages
LaTeX Error: File `orcidlink.sty' not found</code></pre>
<p>
This means that the LaTeX <code>orcidlink</code> package is missing, and we can solve the problem by adding <code>“orcidlink”</code> to the list of <code>tex_pkgs</code>. Rebuild the environment and try again to compile the template. Trying again yields a new error:
</p>
<pre><code>compilation failed- no matching packages
LaTeX Error: File `tcolorbox.sty' not found.</code></pre>
<p>
Just as before, add the <code>tcolorbox</code> package to the list of <code>tex_pkgs</code>. You will need to do this several times for some other packages. There is unfortunately no easier way to list the dependencies and requirements of a LaTeX document.
</p>
<p>
This is what the final script to build the environment looks like:
</p>
<pre class="r"><code>rix(r_ver = "4.3.1",
    r_pkgs = c("quarto", "MASS"),
    system_pkgs = "quarto",
    tex_pkgs = c(
      "amsmath",
      "environ",
      "fontawesome5",
      "orcidlink",
      "pdfcol",
      "tcolorbox",
      "tikzfill"
    ),
    ide = "other",
    shell_hook = "",
    project_path = path_default_nix,
    overwrite = TRUE,
    print = TRUE)</code></pre>
<pre><code>## # This file was generated by the {rix} R package v0.4.1 on 2023-12-19
## # with following call:
## # &gt;rix(r_ver = "976fa3369d722e76f37c77493d99829540d43845",
## #  &gt; r_pkgs = c("quarto",
## #  &gt; "MASS"),
## #  &gt; system_pkgs = "quarto",
## #  &gt; tex_pkgs = c("amsmath",
## #  &gt; "environ",
## #  &gt; "fontawesome5",
## #  &gt; "orcidlink",
## #  &gt; "pdfcol",
## #  &gt; "tcolorbox",
## #  &gt; "tikzfill"),
## #  &gt; ide = "other",
## #  &gt; project_path = path_default_nix,
## #  &gt; overwrite = TRUE,
## #  &gt; print = TRUE,
## #  &gt; shell_hook = "")
## # It uses nixpkgs' revision 976fa3369d722e76f37c77493d99829540d43845 for reproducibility purposes
## # which will install R version 4.3.1
## # Report any issues to https://github.com/b-rodrigues/rix
## let
##  pkgs = import (fetchTarball "https://github.com/NixOS/nixpkgs/archive/976fa3369d722e76f37c77493d99829540d43845.tar.gz") {};
##  rpkgs = builtins.attrValues {
##   inherit (pkgs.rPackages) quarto MASS;
## };
##   tex = (pkgs.texlive.combine {
##   inherit (pkgs.texlive) scheme-small amsmath environ fontawesome5 orcidlink pdfcol tcolorbox tikzfill;
## });
##  system_packages = builtins.attrValues {
##   inherit (pkgs) R glibcLocalesUtf8 quarto;
## };
##   in
##   pkgs.mkShell {
##     LOCALE_ARCHIVE = if pkgs.system == "x86_64-linux" then  "${pkgs.glibcLocalesUtf8}/lib/locale/locale-archive" else "";
##     LANG = "en_US.UTF-8";
##     LC_ALL = "en_US.UTF-8";
##     LC_TIME = "en_US.UTF-8";
##     LC_MONETARY = "en_US.UTF-8";
##     LC_PAPER = "en_US.UTF-8";
##     LC_MEASUREMENT = "en_US.UTF-8";
## 
##     buildInputs = [  rpkgs tex system_packages  ];
##       
##   }</code></pre>
<p>
The template will now compile with this environment. To look for a LaTeX package, you can use the <a href="https://ctan.org/pkg/orcidlink?lang=en">search engine on CTAN</a>.
</p>
<p>
As stated in the beginning of this section, this approach is not the most optimal, but it has its merits, especially if you’re still working on the document. Once the environment is set up, you can simply work on the doc and compile it as needed using <code>quarto render</code>. In the next section, we will explain how to build a 100% reproducible document.
</p>
</section>
<section id="reproducible-literate-programming" class="level2">
<h2 class="anchored" data-anchor-id="reproducible-literate-programming">
100% reproducible literate programming
</h2>
<p>
Let’s not forget that Nix is not just a package manager, but also a programming language. The <code>default.nix</code> files that <code>{rix}</code> generates are written in this language, which was made entirely for the purpose of building software. If you are not a developer, you may not realise it but the process of compiling a Quarto or LaTeX document is very similar to the process of building any piece of software. So we can use Nix to compile a document in a completely reproducible environment.
</p>
<p>
First, let’s fork the repo that contains the Quarto template we need. We will fork <a href="https://github.com/quarto-journals/jss">this repo</a>. This repo contains the <code>template.qmd</code> file that we can change (which is why we fork it, in practice we would replace this <code>template.qmd</code> by our own, finished, source <code>.qmd</code> file). Now we need to change our <code>default.nix</code>:
</p>
<pre><code>let
 pkgs = import (fetchTarball "https://github.com/NixOS/nixpkgs/archive/976fa3369d722e76f37c77493d99829540d43845.tar.gz") {};
 rpkgs = builtins.attrValues {
   inherit (pkgs.rPackages) quarto MASS;
 };
 tex = (pkgs.texlive.combine {
   inherit (pkgs.texlive) scheme-small amsmath environ fontawesome5 orcidlink pdfcol tcolorbox tikzfill;
 });
 system_packages = builtins.attrValues {
   inherit (pkgs) R quarto;
 };
 in
 pkgs.mkShell {
   buildInputs = [  rpkgs tex system_packages  ];
 }</code></pre>
<p>
to the following:
</p>
<pre><code>let
 pkgs = import (fetchTarball "https://github.com/NixOS/nixpkgs/archive/976fa3369d722e76f37c77493d99829540d43845.tar.gz") {};
 rpkgs = builtins.attrValues {
  inherit (pkgs.rPackages) quarto MASS;
 };
 tex = (pkgs.texlive.combine {
  inherit (pkgs.texlive) scheme-small amsmath environ fontawesome5 orcidlink pdfcol tcolorbox tikzfill;
 });
 system_packages = builtins.attrValues {
  inherit (pkgs) R quarto;
 };
 in
 pkgs.stdenv.mkDerivation {
   name = "my-paper";
   src = pkgs.fetchgit {
       url = "https://github.com/b-rodrigues/my_paper/";
       branchName = "main";
       rev = "715e9f007d104c23763cebaf03782b8e80cb5445";
       sha256 = "sha256-e8Xg7nJookKoIfiJVTGoJkvCuFNTT83YZ6SK3GT2T8g=";
     };
   buildInputs = [  rpkgs tex system_packages  ];
   buildPhase =
     ''
     # Deno needs to add stuff to $HOME/.cache
     # so we give it a home to do this
     mkdir home
     export HOME=$PWD/home
     quarto add --no-prompt $src
     quarto render $PWD/template.qmd --to jss-pdf
     '';
   installPhase =
     ''
     mkdir -p $out
     cp template.pdf $out/
     '';
 }</code></pre>
<p>
So we changed the second part of the file, we’re not building a shell anymore using <code>mkShell</code>, but a <em>derivation</em>. <em>Derivation</em> is Nix jargon for package, or software. So what is our derivation? First, we clone the repo we forked just before (I forked the repository and called it <code>my_paper</code>):
</p>
<pre><code>pkgs.stdenv.mkDerivation {
  name = "my-paper";
  src = pkgs.fetchgit {
      url = "https://github.com/b-rodrigues/my_paper/";
      branchName = "main";
      rev = "715e9f007d104c23763cebaf03782b8e80cb5445";
      sha256 = "sha256-e8Xg7nJookKoIfiJVTGoJkvCuFNTT83YZ6SK3GT2T8g=";
    };</code></pre>
<p>
This repo contains our quarto template, and because we’re using a specific commit, we will always use exactly this release of the template for our document. This is in contrast to before where we used <code>quarto add quarto-journals/jss</code> to install the template. Doing this interactively makes our project not reproducible because if we compile our Quarto doc today, we would be using the template as it is today, but if we compile the document in 6 months, then we would be using the template as it would be in 6 months (I should say that it is possible to install specific releases of Quarto templates using following notation: <code>quarto add quarto-journals/jss@v0.9.2</code> so this problem can be mitigated).
</p>
<p>
The next part of the file contains following lines:
</p>
<pre><code>buildInputs = [  rpkgs tex system_packages  ];
buildPhase =
  ''
  # Deno needs to add stuff to $HOME/.cache
  # so we give it a home to do this
  mkdir home
  export HOME=$PWD/home
  quarto add --no-prompt $src
  quarto render $PWD/template.qmd --to jss-pdf
  '';</code></pre>
<p>
The <code>buildInputs</code> are the same as before. What’s new is the <code>buildPhase</code>. This is actually the part in which the document gets compiled. The first step is to create a <code>home</code> directory. This is because Quarto needs to save the template we want to use in <code>/home/.cache/deno</code>. If you’re using <code>quarto</code> interactively, that’s not an issue, since your home directory will be used. But with Nix, things are different, so we need to create an empty directory and specify this as the home. This is what these two lines do:
</p>
<pre><code>mkdir home
export HOME=$PWD/home</code></pre>
<p>
(<code>$PWD</code> —Print Working Directory— is a shell variable referring to the current working directory.)
</p>
<p>
Now, we need to install the template that we cloned from Github. For this we can use <code>quarto add</code> just as before, but instead of installing it directly from Github, we install it from the repository that we cloned. We also add the <code>–no-prompt</code> flag so that the template gets installed without asking us for confirmation. This is similar to how when building a Docker image, we don’t want any interactive prompt to show up, or else the process will get stuck. <code>$src</code> refers to the path of our downloaded Github repository. Finally we can compile the document:
</p>
<pre><code>quarto render $PWD/template.qmd --to jss-pdf</code></pre>
<p>
This will compile the <code>template.qmd</code> (our finished paper). Finally, there’s the <code>installPhase</code>:
</p>
<pre><code>installPhase =
  ''
  mkdir -p $out
  cp template.pdf $out/
  '';</code></pre>
<p>
<code>$out</code> is a shell variable defined inside the build environment and refers to the path, so we can use it to create a directory that will contain our output (the compiled PDF file). So we use <code>mkdir -p</code> to recursively create all the directory structure, and then copy the compiled document to <code>$out/</code>. We can now build our document by running <code>nix_build()</code>. Now, you may be confused by the fact that you won’t see the PDF in your working directory. But remember that software built by Nix will always be stored in the Nix store, so our PDF is also in the store, since this is what we built. To find it, run:
</p>
<pre><code>readlink result</code></pre>
<p>
which will show the path to the PDF. You could use this to open the PDF in your PDF viewer application (on Linux at least):
</p>
<pre><code>xdg-open $(readlink result)/template.pdf</code></pre>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">
Conclusion
</h2>
<p>
This vignette showed two approaches, both have their merits: the first approach that is more interactive is useful while writing the document. You get access to a shell and can work on the document and compile it quickly. The second approach is more useful once the document is ready and you want to have a way of quickly rebuilding it for reproducibility purposes. This approach should also be quite useful in a CI/CD environment.
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>nix</category>
  <guid>https://b-rodrigues.github.io/posts/2023-09-15-nix_for_r_part5.html</guid>
  <pubDate>Fri, 15 Sep 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Reproducible data science with Nix, part 4 – So long, {renv} and Docker, and thanks for all the fish</title>
  <link>https://b-rodrigues.github.io/posts/2023-08-12-nix_for_r_part4.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/friendship ended with docker.png" width="100%">
</p>
</div>
<p>
For this blog post, I also made a youtube video that goes over roughly the same ideas, but the blog post is more detailed as I explain the contents of <code>default.nix</code> files, which I don’t do in the video. Watch the video <a href="https://www.youtube.com/watch?v=c1LhgeTTxaI">here</a>.
</p>
<p>
This is the fourth post in a series of posts about Nix. <em>Disclaimer:</em> I’m a super beginner with Nix. So this series of blog posts is more akin to notes that I’m taking while learning than a super detailed Nix tutorial. So if you’re a Nix expert and read something stupid in here, that’s normal. This post is going to focus on R (obviously) but the ideas are applicable to any programming language.
</p>
<p>
If you’ve never heard of Nix, take a look at <a href="../posts/2023-07-13-nix_for_r_part1.html">part 1</a>.
</p>
<p>
In this blog post I will go over many, nitty-gritty details and explain, line by line, what a Nix expression you can use to build an environment for your projects contains. In practice, building such an environment allows you to essentially replace <code>{renv}</code>+Docker, but writing the right expressions to achieve it is not easy. So this blog post will also go over the features of <code>{rix}</code>, an <a href="https://docs.ropensci.org/rix/">R package</a> by <a href="https://github.com/philipp-baumann">Philipp Baumann</a> and myself.
</p>
<p>
Let me also address the click-bait title directly. Yes, the title is click-bait and I got you. I don’t believe that <code>{renv}</code> and Docker are going away any time soon and you should not hesitate to invest the required time to get to know and use these tools (I wrote <a href="https://raps-with-r.dev/">something by the way</a>). But I am more and more convinced that Nix is an amazing alternative that offers many possibilities, albeit with a high entry cost. By writing <code>{rix}</code>, we aimed at decreasing this entry cost as much as possible. However, more documentation, examples, etc., need to be written and more testing is required. This series of blog posts is a first step to get the word out and get people interested in the package and more broadly in Nix. So if you’re interested or intrigued, don’t hesitate to get in touch!
</p>
<p>
This will be a long and boring post. Unless you really want to know how all of this works go watch the Youtube video, which is more practical instead. I needed to write this down, as it will likely serve as documentation. I’m essentially beta testing it with you, so if you do take the time to read, and even better, to try out the code, please let us know how it went! Was it clear, was it simple, was it useful? Many thanks in advance.
</p>
<section id="part-1-starting-a-new-project-with-nix" class="level2">
<h2 class="anchored" data-anchor-id="part-1-starting-a-new-project-with-nix">
Part 1: starting a new project with Nix
</h2>
<p>
Let’s suppose that you don’t even have R installed on your computer yet. Maybe you bought a new computer, or changed operating system, whatever. Maybe you even have R already, which you installed from the installer that you can download from the R project website. It doesn’t matter, as we are going to install a (somewhat) isolated version of R using Nix for the purposes of this blog post. If you don’t know where to start, it’s simple: first, use the <a href="https://zero-to-nix.com/start/install">installer from Determinate Systems</a>. This installer will make it easy to install Nix on Linux, macOS or Windows (with WSL2). Once you have Nix installed, you can use it to install R and <code>{rix}</code> to start building reproducible development environments. To help you get started, you can run this line here (as documented in <code>{rix}</code>’s Readme), which will <em>drop you into a Nix shell</em> with R and <code>{rix}</code> available. Run the line inside a terminal (if you’re running Windows, run this in a Linux distribution that you installed for WSL2):
</p>
<pre><code>nix-shell --expr "$(curl -sl https://raw.githubusercontent.com/b-rodrigues/rix/master/inst/extdata/default.nix)"</code></pre>
<p>
This will take a bit to run, and then you will be inside an R session. This environment is not suited for development, but is only provided as an easy way for you to start using <code>{rix}</code>. Using <code>{rix}</code>, you can now use it to create a more complex environment suited for a project that you would like to start. Let’s start by loading <code>{rix}</code>:
</p>
<pre class="r"><code>library(rix)</code></pre>
<p>
Now you can run the following command to create an environment with the latest version of R and some packages (change the R version and list of packages to suit your needs):
</p>
<pre class="r"><code>path_default_nix &lt;- "path/to/my/project"

rix(r_ver = "current",
    r_pkgs = c("dplyr", "ggplot2"),
    other_pkgs = NULL,
    git_pkgs = list(package_name = "housing",
                    repo_url = "https://github.com/rap4all/housing",
                    branch_name = "fusen",
                    commit = "1c860959310b80e67c41f7bbdc3e84cef00df18e"),
    ide = "rstudio",
    project_path = path_default_nix,
    overwrite = TRUE)</code></pre>
<p>
Running the code above will create the following <code>default.nix</code> file in <code>path/to/my/project</code>:
</p>
<pre><code># This file was generated by the {rix} R package on Sat Aug 12 22:18:55 2023
# with following call:
# &gt;rix(r_ver = "cf73a86c35a84de0e2f3ba494327cf6fb51c0dfd",
#  &gt; r_pkgs = c("dplyr",
#  &gt; "ggplot2"),
#  &gt; other_pkgs = NULL,
#  &gt; git_pkgs = list(package_name = "housing",
#  &gt; repo_url = "https://github.com/rap4all/housing",
#  &gt; branch_name = "fusen",
#  &gt; commit = "1c860959310b80e67c41f7bbdc3e84cef00df18e"),
#  &gt; ide = "rstudio",
#  &gt; project_path = path_default_nix,
#  &gt; overwrite = TRUE)
# It uses nixpkgs' revision cf73a86c35a84de0e2f3ba494327cf6fb51c0dfd for reproducibility purposes
# which will install R as it was as of nixpkgs revision: cf73a86c35a84de0e2f3ba494327cf6fb51c0dfd
# Report any issues to https://github.com/b-rodrigues/rix
{ pkgs ? import (fetchTarball "https://github.com/NixOS/nixpkgs/archive/cf73a86c35a84de0e2f3ba494327cf6fb51c0dfd.tar.gz") {} }:

with pkgs;

let
  my-r = rWrapper.override {
    packages = with rPackages; [
        dplyr
        ggplot2
        (buildRPackage {
          name = "housing";
          src = fetchgit {
          url = "https://github.com/rap4all/housing";
          branchName = "fusen";
          rev = "1c860959310b80e67c41f7bbdc3e84cef00df18e";
          sha256 = "sha256-s4KGtfKQ7hL0sfDhGb4BpBpspfefBN6hf+XlslqyEn4=";
          };
          propagatedBuildInputs = [
            dplyr
            ggplot2
            janitor
            purrr
            readxl
            rlang
            rvest
            stringr
            tidyr
            ];
          })
        ];
    };
  my-rstudio = rstudioWrapper.override {
    packages = with rPackages; [
        dplyr
        ggplot2
        (buildRPackage {
          name = "housing";
          src = fetchgit {
          url = "https://github.com/rap4all/housing";
          branchName = "fusen";
          rev = "1c860959310b80e67c41f7bbdc3e84cef00df18e";
          sha256 = "sha256-s4KGtfKQ7hL0sfDhGb4BpBpspfefBN6hf+XlslqyEn4=";
          };
          propagatedBuildInputs = [
            dplyr
            ggplot2
            janitor
            purrr
            readxl
            rlang
            rvest
            stringr
            tidyr
            ];
          })
        ];
    };
in
 mkShell {
   LOCALE_ARCHIVE = "${glibcLocales}/lib/locale/locale-archive";
     buildInputs = [
        my-r
        my-rstudio
      ];
 }</code></pre>
<p>
Let’s go through it. The first thing you will notice is that this file is written in a language that you might not know: this language is called Nix as well! So <em>Nix</em> can both refer to the package manager, but also to the programming language. The Nix programming language was designed for creating and composing <em>derivations</em>. A derivation is Nix jargon for a package (not necessarily an R package; any piece of software that you can install through Nix is a package). To know more about the language itself, you can <a href="https://nixos.org/manual/nix/stable/language/index.html">RTFM</a>.
</p>
<p>
Let’s go back to our <code>default.nix</code> file. The first lines state the revision of <code>nixpkgs</code> used that is being used in this expression, as well as which version of R gets installed through it. <code>nixpkgs</code> is Nix’s repository which contains all the software that we will be installing. This is important to understand: since all the expressions that build all the software available through <code>nixpkgs</code> are versioned on <a href="https://github.com/NixOS/nixpkgs/tree/master/pkgs">Github</a>, it is possible to choose a particular commit, or revision, and use that particular release of <code>nixpkgs</code>. So by judiciously choosing the right commit, it’s possible to install any version of R (well any version until 3.0.2). <code>{rix}</code> takes care of this for you: state the version of R that is needed, and the right revision will be returned (the list of R versions and revisions can be found <a href="https://lazamar.co.uk/nix-versions/?channel=nixpkgs-unstable&amp;package=r">here</a>).
</p>
<p>
The call that was used to generate the <code>default.nix</code> file is also saved, but if you look at the argument <code>r_ver</code>, the <code>nixpkgs</code> revision is specified instead of <code>“current”</code>. This is because if you re-run this call but keep <code>r_ver = “current”</code>, another, more recent <code>nixpkgs</code> revision will get used instead, which will break reproducibility. To avoid this, the expression gets changed, so if you re-run it, you’re sure to find the exact same environment.
</p>
<p>
Then comes this line:
</p>
<pre><code>{ pkgs ? import (fetchTarball "https://github.com/NixOS/nixpkgs/archive/cf73a86c35a84de0e2f3ba494327cf6fb51c0dfd.tar.gz") {} }:</code></pre>
<p>
This actually defines a function with argument <code>pkgs</code> that is optional (hence the <code>?</code>). All that follows, <code>import (fetchTarball … ) {}</code> is the default value for <code>pkgs</code> if no argument is provided when you run this (which will always be the case). So here, if I call this function without providing any <code>pkgs</code> argument, the release of <code>nixpkgs</code> at that commit will be used. Then comes:
</p>
<pre><code>with pkgs;

let
  my-pkgs = rWrapper.override {
    packages = with rPackages; [
      dplyr
      ggplot2</code></pre>
<p>
The <code>with pkgs</code> statement makes all the imported packages available in the scope of the function. So I can write <code>quarto</code> if I want to install Quarto (the program that compiles <code>.qmd</code> files, not the <code>{quarto}</code> R package that provides bindings to it) instead of <code>nixpkgs.quarto</code>. Actually, R also has <code>with()</code>, so you can write this:
</p>
<pre class="r"><code>with(mtcars, plot(mpg ~ hp))</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/nix_for_r_part4-3-1.png" width="672">
</p>
<p>
instead of this:
</p>
<pre class="r"><code>plot(mtcars$mpg ~ mtcars$hp)</code></pre>
<p>
Then follows a <code>let … in</code>. This is how a variable gets defined locally, for example, this is a valid Nix statement:
</p>
<pre><code>let x = 1; y = 2; in x + y</code></pre>
<p>
which will obviously return <code>3</code>. So here we are defining a series of packages that will ultimately be available in our environment. These packages are named <code>my-pkgs</code> and are a list of R packages. You can see that I use a wrapper called <code>rWrapper</code> which changes certain options to make R installed through Nix work well. This wrapper has a <code>packages</code> attribute which I override using its <code>.override</code> method, and then I redefine <code>packages</code> as a list of R packages. Just like before, I use <code>with rPackages</code> before listing them, which allows me to write <code>dplyr</code> instead of <code>rPackages.dplyr</code> to refer to the <code>{dplyr}</code> packages. R packages that have a <code>.</code> character in their name must be written using <code>_</code>, so if you need <code>{data.table}</code> you’ll need to write <code>data_table</code> (but <code>{rix}</code> does this for you as well, so don’t worry). Then follows the list of R packages available through <code>nixpkgs</code> (which is the entirety of CRAN:
</p>
<pre><code>packages = with rPackages; [
          dplyr
          ggplot2</code></pre>
<p>
Each time you need to add a package, add it here, and rebuild your environment, do not run <code>install.packages(blabla)</code> to install the <code>{blabla}</code> package, because it’s likely not going to work anyways, and it’s not reproducible. Your projects need to be entirely defined as code. This also means that packages that have helper functions that install something, for example <code>tinytex::install_tinytex()</code>, cannot be used anymore. Instead, you will need to install <code>texlive</code> (by putting it in <code>other_pkgs</code>) and rebuild the expression. We plan to write vignettes documenting all these use-cases. For example, my blog is still built using Hugo (and will likely stay like this forever). I’m using a very old version of Hugo to generate it (I don’t want to upgrade and have to deal with potential issues), so I install the right version I need using Nix, instead of using <code>blogdown::install_hugo()</code>.
</p>
<p>
Then comes the expression that installs a package from Github:
</p>
<pre><code>(buildRPackage {
  name = "housing";
  src = fetchgit {
  url = "https://github.com/rap4all/housing";
  branchName = "fusen";
  rev = "1c860959310b80e67c41f7bbdc3e84cef00df18e";
  sha256 = "sha256-s4KGtfKQ7hL0sfDhGb4BpBpspfefBN6hf+XlslqyEn4=";
  };
  propagatedBuildInputs = [
    dplyr
    ggplot2
    janitor
    purrr
    readxl
    rlang
    rvest
    stringr
    tidyr
    ];
})</code></pre>
<p>
As you can see it’s quite a mouthful, but it was generated from this R code only:
</p>
<pre class="r"><code>git_pkgs = list(package_name = "housing",
                repo_url = "https://github.com/rap4all/housing",
                branch_name = "fusen",
                commit = "1c860959310b80e67c41f7bbdc3e84cef00df18e"),</code></pre>
<p>
If you want to install more than one package, you can also provide a list of lists, for example:
</p>
<pre class="r"><code>git_pkgs = list(
  list(package_name = "housing",
       repo_url = "https://github.com/rap4all/housing/",
       branch_name = "fusen",
       commit = "1c860959310b80e67c41f7bbdc3e84cef00df18e"),
  list(package_name = "fusen",
       repo_url = "https://github.com/ThinkR-open/fusen",
       branch_name = "main",
       commit = "d617172447d2947efb20ad6a4463742b8a5d79dc")
),
...</code></pre>
<p>
and the right expressions will be generated. There’s actually a lot going on here, so let me explain. The first thing is the <code>sha256</code> field. This field contains a hash that gets generated by Nix, and that must be provided by the user. But users rarely, if ever, know this value, so instead what they do is they try to build the expression without providing it. An error message like this one gets returned:
</p>
<pre><code>error: hash mismatch in fixed-output derivation '/nix/store/449zx4p6x0yijym14q3jslg55kihzw66-housing-1c86095.drv':
         specified: sha256-AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
            got:    sha256-s4KGtfKQ7hL0sfDhGb4BpBpspfefBN6hf+XlslqyEn4=</code></pre>
<p>
The <code>sha256</code> can now get copy-and-pasted into the expression. This approach is called “Trust On First Use”, or TOFU for short. Because this is quite annoying, <code>{rix}</code> provides a “private” function, called <code>get_sri_hash_deps()</code> that generates this hash for you. The issue is that this hash cannot be computed easily if you don’t have Nix installed, and since I don’t want to force users to install Nix to use <code>{rix}</code>, what I did is that I set up a server with Nix installed and a <code>{plumber}</code> api. <code>get_sri_hash_deps()</code> makes a call to that api and gets back the <code>sha256</code>, and also a list of packages (more on this later).
</p>
<p>
You can try making a call to the api if you have <code>curl</code> installed on your system:
</p>
<pre><code>curl -X GET "http://git2nixsha.dev:1506/hash?repo_url=https://github.com/rap4all/housing/&amp;branchName=fusen&amp;commit=1c860959310b80e67c41f7bbdc3e84cef00df18e" -H "accept: */*"</code></pre>
<p>
This is what you will get back:
</p>
<pre><code>{
  "sri_hash" : ["sha256-s4KGtfKQ7hL0sfDhGb4BpBpspfefBN6hf+XlslqyEn4="],
  "deps"     : ["dplyr ggplot2 janitor purrr readxl rlang rvest stringr tidyr"]
}</code></pre>
<p>
The reason computing <code>sri_hash</code> is not easy is because it gets computed on the folder containing the source code (after having deleted the <code>.git</code> folder in the case of a Github repo) after it was <em>serialised</em>. You are certainly familiar with serialisations such as the ZIP or TAR serialisation (in other words, zipping a folder is “serialising” it). But these serialisation algorithms come with certain shortcomings that I won’t discuss here, but if you’re interested check out section <em>5.2. The Nix store</em> from Eelco Dolstra’s Phd thesis which you can find <a href="https://archive.is/S9meY">here</a>. Instead, a Nix-specific serialisation algorithm was developed, called NAR. So to compute this hash, I either had to implement this serialisation algorithm in R, or write an api that does that for me by using the implementation that ships with Nix. Since I’m not talented enough to implement such an algorithm in R, I went for the api. But who knows, maybe in the future this could be done. There are implementation of this algorithm in other programming languages like Rust, so maybe packaging the Rust binary could be an option.
</p>
<p>
This gets then further processed by <code>rix()</code>. The second thing that gets returned is a list of packages. These get scraped from the <code>Imports</code> and <code>LinkingTo</code> sections of the <code>DESCRIPTION</code> file from the package and are then provided as the <code>propagatedBuildInputs</code> in the Nix expression. These packages are dependencies that must be available to your package at build and run-time.
</p>
<p>
You should know that as of today (<code>{rix}</code> commit <code>15cadf7f</code>) GitHub packages that use the <code>Remotes</code> field (so that have dependencies that are also on Github) are not handled by <code>{rix}</code>, but supporting this is planned. What <code>{rix}</code> supports though is installing packages from the CRAN archives, so you can specify a version of a package and have that installed. For example:
</p>
<pre class="r"><code>rix(r_ver = "current",
    r_pkgs = c("dplyr@0.8.0", "ggplot2@3.1.1"),
    other_pkgs = NULL,
    git_pkgs = NULL,
    ide = "other",
    path = path_default_nix,
    overwrite = TRUE)</code></pre>
<p>
The difference with the <code>default.nix</code> file from before is that these packages get downloaded off the CRAN archives, so <code>fetchzip()</code> is used to download them instead of <code>fetchgit()</code> (both Nix functions). Here is what the generated Nix code looks like:
</p>
<pre><code>(buildRPackage {
  name = "dplyr";
  src = fetchzip {
  url = "https://cran.r-project.org/src/contrib/Archive/dplyr/dplyr_0.8.0.tar.gz";
  sha256 = "sha256-f30raalLd9KoZKZSxeTN71PG6BczXRIiP6g7EZeH09U=";
  };
  propagatedBuildInputs = [
    assertthat
    glue
    magrittr
    pkgconfig
    R6
    Rcpp
    rlang
    tibble
    tidyselect
    BH
    plogr
    Rcpp
    ];
})
(buildRPackage {
  name = "ggplot2";
  src = fetchzip {
  url = "https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_3.1.1.tar.gz";
  sha256 = "sha256-0Qv/5V/XMsFBcGEFy+3IAaBJIscRMTwGong6fiP5Op0=";
  };
  propagatedBuildInputs = [
    digest
    gtable
    lazyeval
    MASS
    mgcv
    plyr
    reshape2
    rlang
    scales
    tibble
    viridisLite
    withr
    ];
})</code></pre>
<p>
Here’s what this looks like:
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/new_r_old_pkgs.png" width="100%">
</p>
</div>
<p>
This feature should ideally be used sparingly. If you want to reconstruct an environment as it was around a specific date (for example to run an old project), use the version of R that was current at that time. This will ensure that every package that gets installed is at a version compatible with that version of R, which might not be the case if you need to install a very old version of one particular package. But this feature is quite useful if you want to install a package that is not available on CRAN anymore, but that is archived, like <a href="https://cran.r-project.org/web/packages/ZeligChoice/index.html">{ZeligChoice}</a>.
</p>
<p>
Then a second list of packages gets defined, this time using the <code>rstudioWrapper</code> wrapper. This is because I specified that I wanted to use RStudio, but RStudio is a bit peculiar. It redefines many paths and so if you have RStudio installed in your system, it won’t be able to “see” the R installed through Nix. So you have to install RStudio through Nix as well (this is not necessary for VS Code nor Emacs, and likely not for other editors as well). However, it is still necessary to provide each package, again, to the <code>rstudioWrapper</code>. This is because the RStudio installed through Nix is also not able to “see” the R installed through Nix as well. But don’t worry, this does not take twice the space, since the packages simply get symlinked.
</p>
<p>
The last part of the expression uses <code>mkShell</code> which builds a shell with the provided <code>buildInputs</code> (our list of packages). There is also a line to define the location of the locale archive, which should properly configure the locale of the shell (so language, time zone and units):
</p>
<pre><code>in
 mkShell {
   LOCALE_ARCHIVE = "${glibcLocales}/lib/locale/locale-archive";
     buildInputs = [
        my-r
        my-rstudio
      ];
 }</code></pre>
<p>
With this file in hand, we can now build the environment and use it.
</p>
</section>
<section id="part-2-using-your-environment" class="level2">
<h2 class="anchored" data-anchor-id="part-2-using-your-environment">
Part 2: using your environment
</h2>
<p>
So let’s suppose that you have a <code>default.nix</code> file and you wish to build the environment. To do so, you need to have Nix installed, and, thanks to the contributions of <a href="https://github.com/philipp-baumann">Philipp Baumann</a>, you can use <code>rix::nix_build()</code> to build the environment as well:
</p>
<pre class="r"><code>nix_build(project_path = path_default_nix, exec_mode = "blocking")</code></pre>
<p>
If you prefer, you can use Nix directly as well; navigate to the project folder containing the <code>default.nix</code> file and run the command line tool <code>nix-build</code> that gets installed with Nix:
</p>
<pre><code>nix-build</code></pre>
<p>
This will take some time to run, depending on whether cached binary packages can be pulled from <a href="https://cache.nixos.org/" class="uri">https://cache.nixos.org/</a> or not. Once the build process is done, you should see a file called <code>result</code> next to the <code>default.nix</code> file. You can now <em>drop</em> into the Nix shell by typing this into your operating system’s terminal (after you navigated to the folder containing the <code>default.nix</code> and <code>result</code> files):
</p>
<pre><code>nix-shell</code></pre>
<p>
(this time, you really have to leave your current R session! But Philipp and myself are thinking about how we could also streamline this part as well…).
</p>
<p>
The environment that you just built is not an entirely isolated environment: you can still interact with your computer, unlike with Docker. For example, you can still use programs that are installed on your computer. This means that you can run your usual editor as well, but starting it from the Nix shell will make your editor be able to “see” the R installed in that environment. You need to be careful with this, because sometimes this can lead to surprising behavior. For example, if you already have R installed with some packages, these packages could interfere with your Nix environment. There are two ways of dealing with this: you either only use Nix-based environments to work (which would be my primary recommendation, as there can be no interference between different Nix environments), or you call <code>nix-shell –pure</code> instead of just <code>nix-shell</code>. This will ensure that only whatever is available in the environment gets used, but be warned that Nix environments are very, very lean, so you might need to add some tools to have something completely functional.
</p>
<p>
We can take advantage of the fact that environments are not completely isolated to use our IDEs. For example, if you use VS Code or Emacs, you can use the one that is installed directly on your system, as explained before. As already explained, but to drive the point home, if you’re an RStudio user, you need to specify the <code>ide = “rstudio”</code> argument to <code>rix()</code>, because in the case of RStudio, it needs to be installed by Nix as well (the current available RStudio version installed by Nix is now out of date, but efforts are ongoing to update it). This is because RStudio looks for R runtimes in very specific paths, and these need to be patched to see Nix-provided R versions. Hence the version that gets installed by Nix gets patched so that RStudio is able to find the correct runtimes.
</p>
<p>
Once you dropped into the shell, simply type <code>rstudio</code> to launch RStudio in that environment (or <code>code</code> if you use VS Code or <code>other</code> if you use Emacs, or any other editor). On Linux, RStudio may fail to launch with this error message:
</p>
<pre><code>Could not initialize GLX
Aborted (core dumped)</code></pre>
<p>
change your <code>default.nix</code> file from this:
</p>
<pre><code>mkShell {
  LOCALE_ARCHIVE = "${glibcLocales}/lib/locale/locale-archive";
    buildInputs = [
       my-r
       my-rstudio
     ];
}</code></pre>
<p>
to this:
</p>
<pre><code>mkShell {
  LOCALE_ARCHIVE = "${glibcLocales}/lib/locale/locale-archive";
    buildInputs = [
       my-r
       my-rstudio
     ];
  shellHook = ''
    export QT_XCB_GL_INTEGRATION=none
  '';
}</code></pre>
<p>
which should solve the issue, which is related to hardware acceleration as far as I can tell.
</p>
<p>
<code>shellHook</code>s are a nice feature which I haven’t discussed a lot yet (I did so in part 2 of this series, to run a <code>{targets}</code> pipeline each time I dropped into the shell). Whatever goes into the <code>shellHook</code> gets executed as soon as one drops into the Nix shell. I personally have to add the <code>export QT_XCB_GL_INTEGRATION=none</code> line in on virtual machines and on my desktop computer as well, but I’ve had problems in the past with my graphics drivers, and I think it’s related. I’m planning also to add an option to <code>rix()</code> to add this automatically.
</p>
<p>
If you need to add packages, best is to call <code>rix::rix()</code> again, but this time, provide the <code>nixpkgs</code> revision as the argument to <code>r_ver</code>. Copy and paste the call from the generated <code>default.nix</code> to an R console and rerun it:
</p>
<pre class="r"><code>rix(r_ver = "cf73a86c35a84de0e2f3ba494327cf6fb51c0dfd",
    r_pkgs = c("dplyr", "ggplot2", "tidyr", "quarto"),
    other_pkgs = "quarto",
    git_pkgs = list(package_name = "housing",
                    repo_url = "https://github.com/rap4all/housing",
                    branch_name = "fusen",
                    commit = "1c860959310b80e67c41f7bbdc3e84cef00df18e"),
    ide = "rstudio",
    path = path_default_nix,
    overwrite = TRUE)</code></pre>
<p>
In the call above I’ve added the <code>{tidyr}</code> and <code>{quarto}</code> packages, as well as the <code>quarto</code> command line utility to generate <code>.qmd</code> files. For <code>r_ver</code> I’m this time using the <code>nixpkgs</code> revision from my original <code>default.nix</code> file. This will ensure that my environment stays the same.
</p>
<p>
So if you have read up until this point, let me first thank you, and secondly humbly ask you to test <code>{rix}</code>! I’m looking for testers, especially on Windows and macOS, and would be really grateful if you could provide some feedback on the package. To report anything, simply open issue <a href="https://github.com/b-rodrigues/rix/issues">here</a>.
</p>
<p>
<em>Thanks to Philipp for proof-reading this post.</em>
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>nix</category>
  <guid>https://b-rodrigues.github.io/posts/2023-08-12-nix_for_r_part4.html</guid>
  <pubDate>Sat, 12 Aug 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Reproducible data science with Nix, part 3 – frictionless {plumber} api deployments with Nix</title>
  <link>https://b-rodrigues.github.io/posts/2023-07-30-nix_for_r_part3.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/i_use_nix_for_all_my_package_management_needs.png" width="60%">
</p>
</div>
<p>
This is the third post in a series of posts about Nix. Disclaimer: I’m a super beginner with Nix. So this series of blog posts is more akin to notes that I’m taking while learning than a super detailed tutorial. So if you’re a Nix expert and read something stupid in here, that’s normal. This post is going to focus on R (obviously) but the ideas are applicable to any programming language.
</p>
<p>
This blog post is part tutorial on creating an api using the <code>{plumber}</code> R package, part an illustration of how Nix makes developing and deploying a breeze.
</p>
<section id="part-1-getting-it-to-work-locally" class="level2">
<h2 class="anchored" data-anchor-id="part-1-getting-it-to-work-locally">
Part 1: getting it to work locally
</h2>
<p>
So in <a href="../posts/2023-07-13-nix_for_r_part1.html">part 1</a> I explained what Nix was and how you could use it to build reproducible development environments. In <a href="../posts/2023-07-19-nix_for_r_part2.html">part 2</a> I talked about running a <code>{targets}</code> pipeline in a reproducible environment set up with Nix, and in this blog post I’ll talk about how I made an api using {plumber} and how Nix made going from my development environment to the production environment (on Digital Ocean) the simplest ever. Originally I wanted to focus on interactive work using Nix, but that’ll be very likely for part 4, maybe even part 5 (yes, I really have a lot to write about).
</p>
<p>
Let me just first explain what <code>{plumber}</code> is before continuing. I already talked about <code>{plumber}</code> <a href="../posts/2021-06-04-own_knit_server.html">here</a>, but in summary, <code>{plumber}</code> allows you to build an api. What is an api? Essentially a service that you can call in different ways and which returns something to you. For example, you could send a Word document to this api and get back the same document converted in PDF. Or you could send some English text and get back a translation. Or you could send some data and get a prediction from a machine learning model. It doesn’t matter: what’s important is that apis completely abstract the programming language that is being used to compute whatever should be computed. With <code>{plumber}</code>, you can create such services using R. This is pretty awesome, because it means that whatever it is you can make with R, you could build a service around it and make it available to anyone. Of course you need a server that actually has R installed and that gets and processes the requests it receives, and this is where the problems start. And by problems I mean THE single biggest problem that you have to deal with whenever you develop something on your computer, and then have to make it work somewhere else: deployment. If you’ve had to deal with deployments you might not understand why it’s so hard. I certainly didn’t really get it until I’ve wanted to deploy my first Shiny app, many moons ago. And this is especially true whenever you don’t want to use any “off the shelf” services like <em>shinyapps.io</em>. In the <a href="https://www.brodrigues.co/blog/2021-06-04-own_knit_server/">blog post I mentioned above</a>, I used Docker to deploy the api. But Docker, while an amazing tool, is also quite heavy to deal with. Nix offers an alternative to Docker which I think you should know and think about. Let me try to convince you.
</p>
<p>
So let’s make a little <code>{plumber}</code> api and deploy that in the cloud. For this, I’m using Digital Ocean, but any other service that allows you to spin a virtual machine (VM) with Ubuntu on it will do. If you don’t have a Digital Ocean account, you can use my <a href="https://m.do.co/c/b68adc727710">referral link</a> to get 200$ in credit for 60 days, more than enough to experiment. A VM&nbsp;serving a <code>{plumber}</code> api needs at least 1 gig of RAM, and the cheapest one with 1 gig of ram is 6$ a month (if you spend 25$ of that credit, I’ll get 25$ too, so don’t hesitate to experiment, you’ll be doing me a solid as well).
</p>
<p>
I won’t explain what my api does, this doesn’t really matter for this blog post. But I’ll have to explain it in a future blog post, because it’s related to a package I’m working on, called <a href="https://github.com/b-rodrigues/rix">{rix}</a> which I’m writing to ease the process of building reproducible environments for R using Nix. So for this blog post, let’s make something very simple: let’s take the classic machine learning task of predicting survival of the passengers of the Titanic (which was not that long ago in the news again…) and make a service out of it.
</p>
<p>
What’s going to happen is this: users will make a request to the api giving some basic info about themselves: a simple ML model (I’ll go with logistic regression and call it “machine learning” just to make the statisticians reading this seethe lmao), the machine learning model is going to use this to compute a prediction and the result will be returned to the user. Now to answer a question that comes up often when I explain this stuff: <em>why not use Shiny? Users can enter their data and get a prediction and there’s a nice UI and everything?!</em>. Well yes, but it depends on what it is you actually want to do. An api is useful mostly in situations where you need that request to be made by another machine and then that machine will do something else with that prediction it got back. It could be as simple as showing it in a nice interface, or maybe the machine that made the request will then use that prediction and insert it somewhere for archiving for example. So think of it this way: use an api when machines need to interact with other machines, a Shiny app for when humans need to interact with a machine.
</p>
<p>
Ok so first, because I’m using Nix, I’ll create an environment that will contain everything I need to build this api. I’m doing that in the most simple way possible, simply by specifying an R version and the packages I need inside a file called <code>default.nix</code>. Writing this file if you’re not familiar with Nix can be daunting, so I’ve developed a package, called <code>{rix}</code> to write these files for you. Calling this:
</p>
<pre class="r"><code>rix::rix(r_ver = "4.2.2",
         r_pkgs = c("plumber", "tidymodels"),
         other_pkgs = NULL,
         git_pkgs = NULL,
         ide = "other",
         path = "titanic_api/", # you might need to create this folder
         overwrite = TRUE)</code></pre>
<p>
generates this file for me:
</p>
<pre><code># This file was generated by the {rix} R package on Sat Jul 29 15:50:41 2023
# It uses nixpkgs' revision 8ad5e8132c5dcf977e308e7bf5517cc6cc0bf7d8 for reproducibility purposes
# which will install R version 4.2.2
# Report any issues to https://github.com/b-rodrigues/rix
{ pkgs ? import (fetchTarball "https://github.com/NixOS/nixpkgs/archive/8ad5e8132c5dcf977e308e7bf5517cc6cc0bf7d8.tar.gz") {} }:

  with pkgs;

  let
  my-r = rWrapper.override {
    packages = with rPackages; [
      plumber tidymodels
    ];
  };
  in
  mkShell {
    buildInputs = [
      my-r
      ];
  }</code></pre>
<p>
(for posterity’s sake: this is using <a href="https://github.com/b-rodrigues/rix/tree/935fb194b38adfb085a5bda9ebe5dc5bb504f2cb">this version of {rix}</a>. Also, if you want to learn more about <code>{rix}</code> take a look at its <a href="https://b-rodrigues.github.io/rix/">website</a>. It’s still in very early development, comments and PR more than welcome!)
</p>
<p>
To build my api I’ll have to have <code>{plumber}</code> installed. I also install the <code>{tidymodels}</code> package. I actually don’t need <code>{tidymodels}</code> for what I’m doing (base R can fit logistic regressions just fine), but the reason I’m installing it is to mimic a “real-word example” as closely as possible (a project with some dependencies).
</p>
<p>
When I called <code>rix::rix()</code> to generate the <code>default.nix</code> file, I specified that I wanted R version 4.2.2 (because let’s say that this is the version I need. It’s also possible to get the current version of R by passing “current” to <code>r_ver</code>). You don’t see any reference to this version of R in the <code>default.nix</code> file, but this is the version that will get installed because it’s the version that comes with that particular revision of the <code>nixpkgs</code> repository:
</p>
<pre><code>"https://github.com/NixOS/nixpkgs/archive/8ad5e8132c5dcf977e308e7bf5517cc6cc0bf7d8.tar.gz"</code></pre>
<p>
This url downloads that particular revision on <code>nixpkgs</code> containing R version 4.2.2. <code>{rix}</code> finds the right revision for you (using <a href="https://lazamar.co.uk/nix-versions/?channel=nixpkgs-unstable&amp;package=r">this handy service</a>).
</p>
<p>
While <code>{rix}</code> doesn’t require your system to have Nix installed, if you want to continue you’ll have to install Nix. To install Nix, I recommend you don’t use the official installer, even if it’s quite simple to use. Instead, the <a href="https://zero-to-nix.com/start/install">Determinate Systems</a> installer seems better to me. On Windows, you will need to enable WSL2. An alternative is to run all of this inside a Docker container (but more on this later if you’re thinking something along the lines of <em>isn’t the purpose of Nix to not have to use Docker?</em> then see you in the conclusion). Once you have Nix up and running, go inside the <code>titanic_api/</code> folder (which contains the <code>default.nix</code> file above) and run the following command inside a terminal:
</p>
<pre><code>nix-build</code></pre>
<p>
This will build the environment according to the instructions in the <code>default.nix</code> file. Depending on what you want/need, this can take some time. Once the environment is done building, you can “enter” into it by typing:
</p>
<pre><code>nix-shell</code></pre>
<p>
Now this is where you would use this environment to work on your api. As I stated above, I’ll discuss interactive work using a Nix environment in a future blog post. Leave the terminal with this Nix shell open and create an empty text wile next to <code>default.nix</code> and call it <code>titanic_api.R</code> and put this in there using any text editor of your choice:
</p>
<pre class="r"><code>#* Would you have survived the Titanic sinking?
#* @param sex Character. "male" or "female"
#* @param age Integer. Your age.
#* @get /prediction
function(sex, age) {

  trained_logreg &lt;- readRDS("trained_logreg.rds")

  dataset &lt;- data.frame(sex = sex, age = as.numeric(age))

  parsnip::predict.model_fit(trained_logreg,
                             new_data = dataset)

}</code></pre>
<p>
This script is a <code>{plumber}</code> api. It’s a simple function that uses an already <em>trained</em> logistic regression (lol) by loading it into its scope using the <code>readRDS()</code> function. It then returns a prediction. The script that I wrote to train the model is this one:
</p>
<pre class="r"><code>library(parsnip)

titanic_raw &lt;- read.csv("https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv")

titanic &lt;- titanic_raw |&gt;
  subset(select = c(Survived,
                    Sex,
                    Age))

names(titanic) &lt;- c("survived", "sex", "age")

titanic$survived = as.factor(titanic$survived)

logreg_spec &lt;- logistic_reg() |&gt;
  set_engine("glm")

trained_logreg &lt;- logreg_spec |&gt;
  fit(survived ~ ., data = titanic)

saveRDS(trained_logreg, "trained_logreg.rds")</code></pre>
<p>
If you’re familiar with this Titanic prediction task, you will have noticed that the script above is completely stupid. I only kept two variables to fit the logistic regression. But the reason I did this is because this blog post is not about fitting models, but about apis. So bear with me. Anyways, once you’re run the script above to generate the file <code>trained_logreg.rds</code> containing the trained model, you can locally test the api using <code>{plumber}</code>. Go back to the terminal that is running your Nix shell, and now type <code>R</code> to start R in that session. You can then run your api inside that session using:
</p>
<pre class="r"><code>plumber::pr("titanic_api.R") |&gt;
  plumber::pr_run(port = "8000")</code></pre>
<p>
Open your web browser and visit <a href="http://localhost:8000/__docs__/">http://localhost:8000/<strong>docs</strong>/</a> to see the Swagger interface to your api (Swagger is a nice little tool that makes testing your apis way easier).
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/swagger_plumber.png" width="60%">
</p>
</div>
<p>
Using Swagger you can try out your api, click on (1) then on (2). You can enter some mock data in (3) and (4) and then run the computation by clicking on “Execute” (5). You’ll see the result in (7). (6) gives you a <code>curl</code> command to run exactly this example from a terminal. Congrats, your <code>{plumber}</code> api is running on your computer! Now we need to deploy it online and make it available to the world.
</p>
</section>
<section id="deploying-your-api" class="level2">
<h2 class="anchored" data-anchor-id="deploying-your-api">
Deploying your api
</h2>
<p>
So if you have a Digital Ocean account log in (and if you don’t, use my <a href="https://m.do.co/c/b68adc727710">referral link</a> to get 200$ to test things out) and click on the top-right corner on the “Create” button, and then select “Droplet” (a fancy name for a VM):
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/digital_ocean_1.png" width="60%">
</p>
</div>
<p>
In the next screen, select the region closest to you and then select Ubuntu as the operating system, “Regular” for the CPU options, and then the 4$ (or the 6<img src="https://latex.codecogs.com/png.latex?(,%20it%20doesn't%20matter%20at%20this%20stage)%20a%20month%20Droplet.%20We%20will%20need%20to%20upgrade%20it%20immediately%20after%20having%20created%20it%20in%20order%20to%20actually%20build%20the%20environment.%20This%20is%20because%20building%20the%20environment%20requires%20some%20more%20RAM%20than%20what%20the%206)"> option offers, but starting from the cheapest option ensures that we will then be able to downsize back to it, after the build process is done.
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/digital_ocean_2.png" width="60%">
</p>
</div>
<p>
Next comes how you want to authenticate to your VM. There are two options, one using an SSH key, another using a password. If you’re already using Git, you can use the same SSH key. Click on “New SSH Key” and paste the public key in the box (you should find the key under <code>~/.ssh/id_rsa.pub</code> if you’re using Linux). If you’re not using Git and have no idea what SSH keys are, my first piece of advice is to start using Git and then to generate an SSH key and login using it. This is much more secure than a password. Finally, click on “Create Droplet”. This will start building your VM. Once the Droplet is done building, you can check out its IP address:
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/digital_ocean_3.png" width="100%">
</p>
</div>
<p>
Let’s immediately resize the Droplet to a larger size. As I said before, this is only required to build our production environment using Nix. Once the build is done, we can downsize again to the cheapest Droplet:
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/digital_ocean_4.png" width="100%">
</p>
</div>
<p>
Choose a Droplet with 2 gigs of RAM to be on the safe side, and also enable the reserved IP option (this is a static IP that will never change):
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/digital_ocean_5.png" width="80%">
</p>
</div>
<p>
Finally, turn on your Droplet, it’s time to log in to it using SSH.
</p>
<p>
Open a terminal on your computer and connect to your Droplet using SSH (starting now, <code>user@local_computer</code> refers to a terminal opened on your computer and <code>root@droplet</code> to an active ssh session inside your Droplet):
</p>
<pre><code>user@local_computer &gt; ssh root@IP_ADDRESS_OF_YOUR_DROPLET</code></pre>
<p>
and add a folder that will contain the project’s files:
</p>
<pre><code>root@droplet &gt; mkdir titanic_api</code></pre>
<p>
Great, let’s now copy our files to the Droplet using <code>scp</code>. Open a terminal on your computer, and navigate to where the <code>default.nix</code> file is. If you prefer doing this graphically, you can use Filezilla. Run the following command to copy the <code>default.nix</code> file to the Droplet:
</p>
<pre><code>user@local_computer &gt; scp default.nix root@IP_ADDRESS_OF_YOUR_DROPLET:/root/titanic_api/</code></pre>
<p>
Now go back to the terminal that is logged into your Droplet. We now need to install Nix. For this, follow the instructions from the <a href="https://zero-to-nix.com/start/install">Determinate Systems</a> installer, and run this line in the Droplet:
</p>
<pre><code>root@droplet &gt; curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install</code></pre>
<p>
Pay attention to the final message once the installation is done:
</p>
<pre><code>Nix was installed successfully!
To get started using Nix, open a new shell or run `. /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh`</code></pre>
<p>
So run <code>. /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh</code> to start the Nix daemon. Ok so now comes the magic of Nix. You can now build the exact same environment that you used to build the pipeline on your computer in this Droplet. Simply run <code>nix-build</code> for the build process to start. I don’t really know how to describe how easy and awesome this is. You may be thinking <em>well installing R and a couple of packages is not that hard</em>, but let me remind you that we are using a Droplet that is running Ubuntu, which is likely NOT the operating system that you are running. Maybe you are on Windows, maybe you are on macOS, or maybe you’re running another Linux distribution. Whatever it is you’re using, it will be different from that Droplet. Even if you’re running Ubuntu on your computer, chances are that you’ve changed the CRAN repositories from the default Ubuntu ones to the Posit ones, or maybe you’re using <a href="https://github.com/eddelbuettel/r2u">r2u</a>. Basically, the chances that you will have the exact same environment in that Droplet than the one running on your computer is basically 0. And if you’re already familiar with Docker, I think that you will admit that this is much, much easier than dockerizing your <code>{plumber}</code> api. If you don’t agree, please shoot me an <a href="mailto:bruno@brodrigues.co">email</a> and tell me why, I’m honestly curious. Also, let me stress again that if you needed to install a package like <code>{xlsx}</code> that requires Java to be installed, Nix would install the right version of Java for you.
</p>
<p>
Once the environment is done building, you can downsize your Droplet. Go back to your Digital Ocean account, select that Droplet and choose “Resize Droplet”, and go back to the 6$ a month plan.
</p>
<p>
SSH back into the Droplet and copy the trained model <code>trained_logreg.rds</code> and the api file, <code>titanic_api.R</code> to the Droplet using <code>scp</code> or Filezilla. It’s time to run the api. To do so, the obvious way would be simply to start an R session and to execute the code to run the api. However, if something happens and the R session dies, the api won’t restart. Instead, I’m using a CRON job and an utility called <code>run-one</code>. This utility, pre-installed in Ubuntu, runs one (1) script at a time, and ensures that only one instance of said script is running. So by putting this in a CRON job (CRON is a scheduler, so it executes a script as often as you specify), <code>run-one</code> will try to run the script. If it’s still running, nothing happens, if the script is not running, it runs it.
</p>
<p>
So go back to your local computer, and create a new text file, call it <code>run_api.sh</code> and write the following text in it:
</p>
<pre><code>#!/bin/bash
while true
do
nix-shell /root/titanic_api/default.nix --run "Rscript -e 'plumber::pr_run(plumber::pr(\"/root/titanic_api/titanic_api.R\"), host = \"0.0.0.0\", port=80)'"
 sleep 10
done</code></pre>
<p>
then copy this to your VM using <code>scp</code> or Filezilla, to <code>/root/titanic_api/run_api.sh</code>. Then SSH back into your Droplet, go to where the script is using <code>cd</code>:
</p>
<pre><code>root@droplet &gt; cd /root/titanic_api/</code></pre>
<p>
and make the script executable:
</p>
<pre><code>root@droplet &gt; chmod +x run_api.sh</code></pre>
<p>
We’re almost done. Now, let’s edit the <code>crontab</code>, to specify that we want this script to be executed every hour using <code>run-one</code> (so if it’s running, nothing happens, if it died, it gets restarted). To edit the <code>crontab</code>, type <code>crontab -e</code> and select the editor you’re most comfortable with. If you have no idea, select the first option, <code>nano</code>. Using your keyboard keys, navigate all the way down and type:
</p>
<pre><code>*/60 * * * * run-one /root/titanic_api/run_api.sh</code></pre>
<p>
save the file by typing <code>CTRL-X</code>, and then type <code>Y</code> when asked <code>Save modified buffer?</code>, and then type the <code>ENTER</code> key when prompted for <code>File name to write</code>.
</p>
<p>
We are now ready to start the api. Make sure CRON restarts by running:
</p>
<pre><code>root@droplet &gt; service cron reload</code></pre>
<p>
and then run the script using <code>nohup</code> followed by <code>run-one</code>:
</p>
<pre><code>root@droplet &gt; nohup run-one /root/titanic_api/run_api.sh &amp;</code></pre>
<p>
<code>run-one</code> will now run the script and will ensure that only one instance of the script is running (the <code>&amp;</code> character at the end means “run this in the background” an <code>nohup</code>, which stands for “no hang-up”, ensures the command will continue running even when you close the terminal). If for any reason the process dies, CRON will restart an instance of the script. We can now call our api using this <code>curl</code> command:
</p>
<pre><code>user@local_computer &gt; curl -X GET "http://IP_ADDRESS_OF_YOUR_DROPLET/prediction?sex=female&amp;age=45" -H "accept: */*"</code></pre>
<p>
If you don’t have <code>curl</code> installed, you can use <a href="https://reqbin.com/curl">this webservice</a>. You should see this answer:
</p>
<pre><code>[{
    ".pred_class": "1"
}]</code></pre>
<p>
I’ll leave my Droplet running for a few days after I post this, so if you want you can try it out run this:
</p>
<pre><code>curl -X GET "http://142.93.164.182/prediction?sex=female&amp;age=45" -H "accept: */*"</code></pre>
<p>
The answer is in the JSON format, and can now be ingested by some other script which can now process it further.
</p>
</section>
<section id="conclusion" class="level1">
<h1>
Conclusion
</h1>
<p>
This was a long blog post. While it is part of my Nix series of blog posts, I almost didn’t talk about it, and this is actually the neat part. Nix made something that is usually difficult to solve trivially simple. Without Nix, the alternative would be to bundle the api with all its dependencies and an R interpreter using Docker or install everything by hand on the server. But the issue with Docker is that it’s not necessarily much easier than Nix, and you still have to make sure building the image is reproducible. So you have to make sure to use an image that ships with the right version of R and use <code>{renv}</code> to restore your packages. If you have system-level dependencies that are required, you also have to deal with those. Nix takes care of all of this for you, so that you can focus on all the other aspects of deployment, which take the bulk of the effort and time.
</p>
<p>
In the post I mentioned that you could also run Nix inside a Docker container. If you are already invested in Docker, Nix is still useful because you can use base NixOS images (NixOS is a Linux distribution that uses Nix as its package manager) or you could install Nix inside an Ubuntu image and then benefit from the reproducibility offered by Nix. Simply add <code>RUN nix-build</code> to your Dockerfile, and everything you need gets installed. You can even use Nix to build Docker images instead of writing a Dockerfile. The possibilities are endless!
</p>
<p>
Now, before you start building apis using R, you may want to read this blog post <a href="https://matthewrkaye.com/posts/2023-06-29-lessons-learned-from-running-r-in-production/lessons-learned-from-running-r-in-production.html">here</a> as well. I found it quite interesting: it discusses the shortcomings of using R to build apis like I showed you here, which I think you need to know. If you have needs like the author of this blog post, then maybe R and <code>{plumber}</code> is not the right solution for you.
</p>
<p>
Next time, in part 4, I’ll either finally discuss how to do interactive work using a Nix environment, or I’ll discuss my package, <code>{rix}</code> in more detail. We’ll see!
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>nix</category>
  <guid>https://b-rodrigues.github.io/posts/2023-07-30-nix_for_r_part3.html</guid>
  <pubDate>Sun, 30 Jul 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Reproducible data science with Nix, part 2 – running {targets} pipelines with Nix</title>
  <link>https://b-rodrigues.github.io/posts/2023-07-19-nix_for_r_part2.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/pipeline_nix.jpg" width="100%">
</p>
</div>
<p>
This is the second post in a series of posts about Nix. Disclaimer: I’m a super beginner with Nix. So this series of blog posts is more akin to notes that I’m taking while learning than a super detailed tutorial. So if you’re a Nix expert and read something stupid in here, that’s normal. This post is going to focus on R (obviously) but the ideas are applicable to any programming language.
</p>
<p>
So in <a href="../posts/2023-07-13-nix_for_r_part1.html">part 1</a> I explained what Nix was and how you could use it to build reproducible development environments. Now, let’s go into more details and actually set up some environments and run a <code>{targets}</code> pipeline using it.
</p>
<p>
Obviously the first thing you should do is install Nix. A lot of what I’m showing here comes from the <a href="https://nix.dev/tutorials/">Nix.dev</a> so if you want to install Nix, then look at the instructions <a href="https://nix.dev/tutorials/install-nix">here</a>. If you’re using Windows, you’ll have to have WSL2 installed. If you don’t want to install Nix just yet, you can also play around with a NixOS Docker image. NixOS is a Linux distribution that uses the concepts of Nix for managing the whole operating system, and obviously comes with the Nix package manager installed. But if you’re using Nix inside Docker you won’t be able to work interactively with graphical applications like RStudio, due to how Docker works (but more on working interactively with IDEs in part 3 of this series, which I’m already drafting).
</p>
<p>
Assuming you have Nix installed, you should be able to run the following command in a terminal:
</p>
<pre><code>nix-shell -p sl</code></pre>
<p>
This will launch a Nix shell with the <code>sl</code> package installed. Because <code>sl</code> is not available, it’ll get installed on the fly, and you will get “dropped” into a Nix shell:
</p>
<pre><code>[nix-shell:~]$</code></pre>
<p>
You can now run <code>sl</code> and marvel at what it does (I won’t spoil you). You can quit the Nix shell by typing <code>exit</code> and you’ll go back to your usual terminal. If you try now to run <code>sl</code> it won’t work (unless you installed on your daily machine). So if you need to go back to that Nix shell and rerun <code>sl</code>, simply rerun:
</p>
<pre><code>nix-shell -p sl</code></pre>
<p>
This time you’ll be dropped into the Nix shell immediately and can run <code>sl</code>. So if you need to use R, simply run the following:
</p>
<pre><code>nix-shell -p R</code></pre>
<p>
and you’ll be dropped in a Nix shell with R. This version of R will be different than the one potentially already installed on your system, and it won’t have access to any R packages that you might have installed. This is because Nix environment are isolated from the rest of your system (well, not quite, but again, more on this in part 3). So you’d need to add packages as well (exit the Nix shell and run this command to add packages):
</p>
<pre><code>nix-shell -p R rPackages.dplyr rPackages.janitor</code></pre>
<p>
You can now start R in that Nix shell and load the <code>{dplyr}</code> and <code>{janitor}</code> packages. You might be wondering how I knew that I needed to type <code>rPackages.dplyr</code> to install <code>{dplyr}</code>. You can look for this information <a href="https://search.nixos.org/packages">online</a>. By the way, if a package uses the <code>.</code> character in its name, you should replace that <code>.</code> character by <code>_</code> so to install <code>{data.table}</code> write <code>rPackages.data_table</code>.
</p>
<p>
So that’s nice and dandy, but not quite what we want. Instead, what we want is to be able to declare what we need in terms of packages, dependencies, etc, inside a file, and have Nix build an environment according to these specifications which we can then use for our daily needs. To do so, we need to write a so-called <code>default.nix</code> file. This is what such a file looks like:
</p>
<pre><code>{ pkgs ? import (fetchTarball "https://github.com/NixOS/nixpkgs/archive/e11142026e2cef35ea52c9205703823df225c947.tar.gz") {} }:

with pkgs;

let
  my-pkgs = rWrapper.override {
    packages = with rPackages; [dplyr ggplot2 R];
  };
in
mkShell {
  buildInputs = [my-pkgs];
}</code></pre>
<p>
I wont discuss the intricate details of writing such a file just yet, because it’ll take too much time and I’ll be repeating what you can find on the <a href="https://nix.dev/">Nix.dev</a> website. I’ll give some pointers though. But for now, let’s assume that we already have such a <code>default.nix</code> file that we defined for our project, and see how we can use it to run a <code>{targets}</code> pipeline. I’ll explain how I write such files.
</p>
<section id="running-a-targets-pipeline-using-nix" class="level2">
<h2 class="anchored" data-anchor-id="running-a-targets-pipeline-using-nix">
Running a {targets} pipeline using Nix
</h2>
<p>
Let’s say I have this, more complex, <code>default.nix</code> file:
</p>
<pre><code>{ pkgs ? import (fetchTarball "https://github.com/NixOS/nixpkgs/archive/8ad5e8132c5dcf977e308e7bf5517cc6cc0bf7d8.tar.gz") {} }:

with pkgs;

let
  my-pkgs = rWrapper.override {
    packages = with rPackages; [
      targets
      tarchetypes
      rmarkdown
    (buildRPackage {
      name = "housing";
      src = fetchgit {
        url = "https://github.com/rap4all/housing/";
        branchName = "fusen";
        rev = "1c860959310b80e67c41f7bbdc3e84cef00df18e";
        sha256 = "sha256-s4KGtfKQ7hL0sfDhGb4BpBpspfefBN6hf+XlslqyEn4=";
      };
    propagatedBuildInputs = [
        dplyr
        ggplot2
        janitor
        purrr
        readxl
        rlang
        rvest
        stringr
        tidyr
        ];
      })
    ];
  };
in
mkShell {
  buildInputs = [my-pkgs];
}</code></pre>
<p>
So the file above defines an environment that contains all the required packages to run a pipeline that you can find on <a href="https://github.com/b-rodrigues/nix_targets_pipeline">this Github repository</a>. What’s interesting is that I need to install a package that’s only been released on Github, the <code>{housing}</code> package that I wrote for the <a href="https://raps-with-r.dev/packages.html">purposes of my book</a>, and I can do so in that file as well, using the <code>fetchgit()</code> function. Nix has many such functions, called <em>fetchers</em> that simplify the process of downloading files from the internet, see <a href="https://ryantm.github.io/nixpkgs/builders/fetchers/">here</a>. This function takes some self-explanatory inputs as arguments, and two other arguments that might not be that self-explanatory: <code>rev</code> and <code>sha256</code>. <code>rev</code> is actually the commit on the Github repository. This commit is the one that I want to use for this particular project. So if I keep working on this package, then building an environment with this <code>default.nix</code> will always pull the source code as it was at that particular commit. <code>sha256</code> is the hash of the downloaded repository. It makes sure that the files weren’t tampered with. How did I obtain that? Well, the simplest way is to set it to the empty string <code>““</code> and then try to build the environment. This error message will pop-up:
</p>
<pre><code>error: hash mismatch in fixed-output derivation '/nix/store/449zx4p6x0yijym14q3jslg55kihzw66-housing-1c86095.drv':
         specified: sha256-AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
            got:    sha256-s4KGtfKQ7hL0sfDhGb4BpBpspfefBN6hf+XlslqyEn4=</code></pre>
<p>
So simply copy the hash from the last line, and rebuild! Then if in the future something happens to the files, you’ll know. Another interesting input is <code>propagatedBuildInputs</code>. These are simply the dependencies of the <code>{housing}</code> package. To find them, see the <code>Imports:</code> section of the <a href="https://github.com/rap4all/housing/blob/fusen/DESCRIPTION">DESCRIPTION</a> file. There’s also the <code>fetchFromGithub</code> fetcher that I could have used, but unlike <code>fetchgit</code>, it is not possible to specify the branch name we want to use. Since here I wanted to get the code from the branch called <code>fusen</code>, I had to use <code>fetchgit</code>. The last thing I want to explain is the very first line:
</p>
<pre><code>{ pkgs ? import (fetchTarball "https://github.com/NixOS/nixpkgs/archive/8ad5e8132c5dcf977e308e7bf5517cc6cc0bf7d8.tar.gz") {} }:</code></pre>
<p>
In particular the url. This url points to a specific release of <code>nixpkgs</code>, that ships the required version of R for this project, R version 4.2.2. How did I find this release of <code>nixpkgs</code>? There’s a handy service for that <a href="https://lazamar.co.uk/nix-versions/?channel=nixpkgs-unstable&amp;package=r">here</a>. So using this service, I get the right commit hash for the release that install R version 4.2.2.
</p>
<p>
Ok, but before building the environment defined by this file, let me just say that I know what you’re thinking. Probably something along the lines of: <em>damn it Bruno, this looks complicated and why should I care? Let me just use {renv}!!</em> and I’m not going to lie, writing the above file from scratch didn’t take me long in typing, but it took me long in reading. I had to read quite a lot (look at <a href="https://www.brodrigues.co/blog/2023-07-13-nix_for_r_part1/">part 1</a> for some nice references) before being comfortable enough to write it. But I’ll just say this:
</p>
<ul>
<li>
continue reading, because I hope to convince you that Nix is really worth the effort
</li>
<li>
I’m working on a package that will help R users generate <code>default.nix</code> files like the one from above with minimal effort (more on this at the end of the blog post)
</li>
</ul>
<p>
If you’re following along, instead of typing this file, you can clone this <a href="https://github.com/b-rodrigues/nix_targets_pipeline">repository</a>. This repository contains the <code>default.nix</code> file from above, and a <code>{targets}</code> pipeline that I will run in that environment.
</p>
<p>
Ok, so now let’s build the environment by running <code>nix-build</code> inside a terminal in the folder that contains this file. It should take a bit of time, because many of the packages will need to be built from source. But they <strong>will</strong> get built. Then, you can drop into a Nix shell using <code>nix-shell</code> and then type R, which will start the R session in that environment. You can then simply run <code>targets::tar_make()</code>, and you’ll see the file <code>analyse.html</code> appear, which is the output of the <code>{targets}</code> pipeline.
</p>
<p>
Before continuing, let me just make you realize three things:
</p>
<ul>
<li>
we just ran a targets pipeline with all the needed dependencies which include not only package dependencies, but the right version of R (version 4.2.2) as well, and all required system dependencies;
</li>
<li>
we did so WITHOUT using any containerization tool like Docker;
</li>
<li>
the whole thing is <strong>completely</strong> reproducible; the exact same packages will forever be installed, regardless of <em>when</em> we build this environment, because I’m using a particular release of <code>nixpkgs</code> (8ad5e8132c5dcf977e308e7bf5517cc6cc0bf7d8) so each piece of software this release of Nix installs is going to stay constant.
</li>
</ul>
<p>
And I need to stress <em>completely reproducible</em>. Because using {renv}+Docker, while providing a very nice solution, still has some issues. First of all, with Docker, the underlying operating system (often Ubuntu) evolves and changes through time. So lower level dependencies might change. And at some point in the future, that version of Ubuntu will not be supported anymore. So it won’t be possible to rebuild the image, because it won’t be possible to download any software into it. So either we build our Docker image and really need to make sure to keep it forever, or we need to port our pipeline to newer versions of Ubuntu, without any guarantee that it’s going to work exactly the same. Also, by defining <code>Dockerfile</code>s that build upon <code>Dockerfile</code>s that build upon <code>Dockerfile</code>s, it’s difficult to know what is actually installed in a particular image. This situation can of course be avoided by writing <code>Dockerfile</code>s in such a way that it doesn’t rely on any other <code>Dockerfile</code>, but that’s also a lot of effort. Now don’t get me wrong: I’m not saying Docker should be canceled. I still think that it has its place and that its perfectly fine to use it (I’ll take a project that uses <code>{renv}</code>+Docker any day over one that doesn’t!). But you should be aware of alternative ways of running pipelines in a reproducible way, and Nix is such a way.
</p>
<p>
Going back to our pipeline, we could also run the pipeline with this command:
</p>
<pre><code>nix-shell /path/to/default.nix --run "Rscript -e 'setwd(\"/path/to\");targets::tar_make()'"</code></pre>
<p>
but it’s a bit of a mouthful. What you could do instead is running the pipeline each time you drop into the nix shell by adding a so-called <code>shellHook</code>. For this, we need to change the <code>default.nix</code> file again. Add these lines in the <code>mkShell</code> function:
</p>
<pre><code>...
mkShell {
  buildInputs = [my-pkgs];
  shellHook = ''
     Rscript -e "targets::tar_make()"
  '';
}</code></pre>
<p>
Now, each time you drop into the Nix shell in the folder containing that <code>default.nix</code> file, <code>targets::tar_make()</code> get automatically executed. You can then inspect the results.
</p>
<p>
In the next blog post, I’ll show how we can use that environment with IDEs like RStudio, VS Code and Emacs to work interactively. But first, let me quickly talk about a package I’ve been working on to ease the process of writing <code>default.nix</code> files.
</p>
</section>
<section id="rix-reproducible-environments-with-nix" class="level2">
<h2 class="anchored" data-anchor-id="rix-reproducible-environments-with-nix">
Rix: Reproducible Environments with Nix
</h2>
<p>
I wrote a very early, experimental package called <code>{rix}</code> which will help write these <code>default.nix</code> files for us. <code>{rix}</code> is an R package that hopefully will make R users want to try out Nix for their development purposes. It aims to mimic the workflow of <code>{renv}</code>, or to be more exact, the workflow of what Python users do when starting a new project. Usually what they do is create a completely fresh environment using <code>pyenv</code> (or another similar tool). Using <code>pyenv</code>, Python developers can install a per project version of Python and Python packages, but unlike Nix, won’t install system-level dependencies as well.
</p>
<p>
If you want to install <code>{rix}</code>, run the following line in an R session:
</p>
<pre class="r"><code>devtools::install_github("b-rodrigues/rix")</code></pre>
<p>
You can then using the <code>rix()</code> function to create a <code>default.nix</code> file like so:
</p>
<pre class="r"><code>rix::rix(r_ver = "current",
         pkgs = c("dplyr", "janitor"),
         ide = "rstudio",
         path = ".")</code></pre>
<p>
This will create a <code>default.nix</code> file that Nix can use to build an environment that includes the current versions of R, <code>{dplyr}</code> and <code>{janitor}</code>, and RStudio as well. Yes you read that right: you need to have a per-project RStudio installation. The reason is that RStudio modifies environment variables and so your “locally” installed RStudio would not find the R version installed with Nix. This is not the case with other IDEs like VS Code or Emacs. If you want to have an environment with another version of R, simply run:
</p>
<pre class="r"><code>rix::rix(r_ver = "4.2.1",
         pkgs = c("dplyr", "janitor"),
         ide = "rstudio",
         path = ".")</code></pre>
<p>
and you’ll get an environment with R version 4.2.1. To see which versions are available, you can run <code>rix::available_r()</code>. Learn more about <code>{rix}</code> on its <a href="https://b-rodrigues.github.io/rix/">website</a>. It’s in very early stages, and doesn’t handle packages that have only been released on Github, yet. And the interface might change. I’m thinking of making it possible to list the packages in a yaml file and then have <code>rix()</code> generate the <code>default.nix</code> file from the yaml file. This might be cleaner. There is already something like this called <a href="https://github.com/luispedro/nixml/tree/main">Nixml</a>, so maybe I don’t even need to rewrite anything!
</p>
<p>
But I’ll discuss this is more detail next time, where I’ll explain how you can use development environments built with Nix using an IDE.
</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">
References
</h2>
<ul>
<li>
The great <a href="https://nix.dev/tutorials/install-nix">Nix.dev</a> tutorials.
</li>
<li>
This <a href="https://rgoswami.me/posts/rethinking-r-nix/">blog post: Statistical Rethinking and Nix</a> I referenced in part 1 as well, it helped me install my <code>{housing}</code> package from Github.
</li>
<li>
<a href="https://github.com/luispedro/nixml/tree/main">Nixml</a>.
</li>
</ul>


</section>

 ]]></description>
  <category>R</category>
  <category>nix</category>
  <guid>https://b-rodrigues.github.io/posts/2023-07-19-nix_for_r_part2.html</guid>
  <pubDate>Wed, 19 Jul 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Reproducible data science with Nix, part 1 – what is Nix</title>
  <link>https://b-rodrigues.github.io/posts/2023-07-13-nix_for_r_part1.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/nix.jpg" width="100%">
</p>
</div>
<p>
This is the first of a (hopefully) series of posts about Nix. Disclaimer: I’m a super beginner with Nix. So this series of blog posts is more akin to notes that I’m taking while learning than a super detailed tutorial. So if you’re a Nix expert and read something stupid in here, that’s normal. This post is going to focus on R (obviously) but the ideas are applicable to any programming language.
</p>
<p>
To ensure that a project is reproducible you need to deal with at least four things:
</p>
<ul>
<li>
Make sure that the required/correct version of R (or any other language) is installed;
</li>
<li>
Make sure that the required versions of packages are installed;
</li>
<li>
Make sure that system dependencies are installed (for example, you’d need a working Java installation to install the <code>{rJava}</code> R package on Linux);
</li>
<li>
Make sure that you can install all of this for the hardware you have on hand.
</li>
</ul>
<p>
For the three first bullet points, the consensus seems to be a mixture of Docker to deal with system dependencies, <code>{renv}</code> for the packages (or <code>{groundhog}</code>, or a fixed CRAN snapshot like those <a href="https://packagemanager.posit.co/__docs__/user/get-repo-url/#ui-frozen-urls">Posit provides</a>) and the <a href="https://github.com/r-lib/rig">R installation manager</a> to install the correct version of R (unless you use a Docker image as base that already ships the required version by default). As for the last point, the only way out is to be able to compile the software for the target architecture. There’s a lot of moving pieces, and knowledge that you need to know and I even wrote a whole 522 pages <a href="https://raps-with-r.dev/">book about all of this</a>.
</p>
<p>
But it turns out that this is not the only solution. Docker + <code>{renv}</code> (or some other way to deal with packages) is likely the most popular way to ensure reproducibility of your projects, but there are other tools to achieve this. One such tool is called Nix.
</p>
<p>
Nix is a package manager for Linux distributions, macOS and apparently it even works on Windows if you enable WSL2. What’s a package manager? If you’re not a Linux user, you may not be aware. Let me explain it this way: in R, if you want to install a package to provide some functionality not included with a vanilla installation of R, you’d run this:
</p>
<pre><code>install.packages("dplyr")</code></pre>
<p>
It turns out that Linux distributions, like Ubuntu for example, work in a similar way, but for software that you’d usually install using an installer (at least on Windows). For example you could install Firefox on Ubuntu using:
</p>
<pre><code>sudo apt-get install firefox</code></pre>
<p>
(there’s also graphical interfaces that make this process “more user-friendly”). In Linux jargon, <code>packages</code> are simply what normies call software (or I guess it’s all “apps” these days). These packages get downloaded from so-called repositories (think of CRAN, the repository of R packages) but for any type of software that you might need to make your computer work: web browsers, office suites, multimedia software and so on.
</p>
<p>
So Nix is just another package manager that you can use to install software.
</p>
<p>
But what interests us is not using Nix to install Firefox, but instead to install R and the R packages that we require for our analysis (or any other programming language that we need). But why use Nix instead of the usual ways to install software on our operating systems?
</p>
<p>
The first thing that you should know is that Nix’s repository, <code>nixpkgs</code>, is huge. Humongously huge. As I’m writing these lines, <a href="https://search.nixos.org/packages">there’s more than 80’000 pieces of software available</a>, and the <em>entirety of CRAN</em> is also available through <code>nixpkgs</code>. So instead of installing R as you usually do and then use <code>install.packages()</code> to install packages, you could use Nix to handle everything. But still, why use Nix at all?
</p>
<p>
Nix has an interesting feature: using Nix, it is possible to install software in (relatively) isolated environments. So using Nix, you can install as many versions of R and R packages that you need. Suppose that you start working on a new project. As you start the project, with Nix, you would install a project-specific version of R and R packages that you would only use for that particular project. If you switch projects, you’d switch versions of R and R packages. If you are familiar with <code>{renv}</code>, you should see that this is exactly the same thing: the difference is that not only will you have a project-specific library of R packages, you will also have a project-specific R version. So if you start a project now, you’d have R version 4.2.3 installed (the latest version available in <code>nixpkgs</code> but not the latest version available, more on this later), with the accompagnying versions of R packages, for as long as the project lives (which can be a long time). If you start a project next year, then that project will have its own R, maybe R version 4.4.2 or something like that, and the set of required R packages that would be current at that time. This is because Nix always installs the software that you need in separate, (isolated) environments on your computer. So you can define an environment for one specific project.
</p>
<p>
But Nix even goes even further: not only can you install R and R packages using Nix (in isolated) project-specific environments, Nix even installs the required system dependencies. So for example if I need <code>{rJava}</code>, Nix will make sure to install the correct version of Java as well, always in that project-specific environment (so if you already some Java version installed on your system, there won’t be any interference).
</p>
<p>
What’s also pretty awesome, is that you can use a specific version of <code>nixpkgs</code> to <em>always</em> get <em>exactly</em> the same versions of <strong>all</strong> the software whenever you build that environment to run your project’s code. The environment gets defined in a simple plain-text file, and anyone using that file to build the environment will get exactly, byte by byte, the same environment as you when you initially started the project. And this also regardless of the operating system that is used.
</p>
<p>
So let me illustrate this. After <a href="https://nix.dev/tutorials/install-nix">installing Nix</a>, I can define an environment by writing a file called <code>default.nix</code> that looks like this:
</p>
<pre><code>{ pkgs ? import (fetchTarball "https://github.com/NixOS/nixpkgs/archive/e11142026e2cef35ea52c9205703823df225c947.tar.gz") {} }:

with pkgs;

let
  my-pkgs = rWrapper.override {
    packages = with rPackages; [ dplyr ggplot2 R];
  };
in
mkShell {
  buildInputs = [my-pkgs];
}</code></pre>
<p>
Now this certainly looks complicated! And it is. The entry cost to Nix is quite high, because, actually, Nix is more than a package manager. It is also a programming language, and this programming language gets used to configure environments. I won’t go too much into detail, but you’ll see in the first line that I’m using a specific version of <code>nixpkgs</code> that gets downloaded directly from Github. This means that all the software that I will install with that specific version of <code>nixpkgs</code> will always install the same software. This is what ensures that R and R packages are versioned. Basically, by using a specific version of <code>nixpkgs</code>, I pin all the versions of all the software that this particular version of Nix will <em>ever</em> install. I then define a variable called <code>my-pkgs</code> which lists the packages I want to install (<code>{dplyr}</code>, <code>{ggplot2}</code> and <code>R</code>).
</p>
<p>
By the way, this may look like it would take a lot of time to install because, after all, you need to install R, R packages and underlying system dependencies, but thankfully there is an online cache of binaries that gets automatically used by Nix (<a href="https://cache.nixos.org/">cache.nixos.org</a>) for fast installations. If binaries are not available, sources get compiled.
</p>
<p>
I can now create an environment with these exact specifications using (in the directory where <code>default.nix</code> is):
</p>
<pre><code>nix-build</code></pre>
<p>
or I could use the R version from this environment to run some arbitrary code:
</p>
<pre><code>nix-shell /home/renv/default.nix --run "Rscript -e 'sessionInfo()'" &gt;&gt; /home/renv/sessionInfo.txt</code></pre>
<p>
(assuming my <code>default.nix</code> file is available in the <code>/home/renv/</code> directory). This would build the environment on the fly and run <code>sessionInfo()</code> inside of it. Here are the contents of this <code>sessionInfo.txt</code> file:
</p>
<pre><code>R version 4.2.3 (2023-03-15)
Platform: x86_64-pc-linux-gnu (64-bit)

Matrix products: default
BLAS/LAPACK: /nix/store/pbfs53rcnrzgjiaajf7xvwrfqq385ykv-blas-3/lib/libblas.so.3

locale:
[1] C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

loaded via a namespace (and not attached):
[1] compiler_4.2.3</code></pre>
<p>
This looks like any other output of the <code>sessionInfo()</code> function, but there is something quite unusual: the <code>BLAS/LAPACK</code> line:
</p>
<pre><code>BLAS/LAPACK: /nix/store/pbfs53rcnrzgjiaajf7xvwrfqq385ykv-blas-3/lib/libblas.so.3</code></pre>
<p>
BLAS is a library that R uses for linear algebra, matrix multiplication and vector operations. R usually ships with its own version of BLAS and LAPACK, but it’s also possible to use external ones. Here, we see that the path to the shared object <code>libblas.so.3</code> is somewhere in <code>/nix/store/….</code>. <code>/nix/store/</code> is where all the software gets installed. The long chain of seemingly random characters is a hash, essentially the unique identifier of that particular version of BLAS. This means that unlike Docker, if you’re using Nix you are also certain than these types of dependencies, that may have an impact on your results, also get handled properly, and that the exact same version you used will keep getting installed in the future. Docker images also evolve, and even if you use an LTS release of Ubuntu as a base, the underlying system packages will evolve through time as well. And there will be a point in time where this release will be abandoned (LTS releases receive 5 years of support), so if you need to rebuild a Docker images based on an LTS that doesn’t get supported anymore, you’re out of luck.
</p>
<p>
If you don’t want to install Nix just yet on your computer, you should know that there’s also a complete operating system called NixOS, that uses Nix as its package manager, and that there are Docker images that use NixOS as a base. So this means that you could use such an image and then build the environment (that is 100% completely reproducible) inside and run a container that will always produce the same output. To see an example of this, check out this <a href="https://github.com/b-rodrigues/nix_experiments/tree/master">Github repo</a>. I’m writing a Dockerfile as I usually do, but actually I could even use Nix to define the Docker image for me, it’s that powerful!
</p>
<p>
Nix seems like a very powerful tool to me. But there are some “issues”:
</p>
<ul>
<li>
As I stated above, the entry cost is quite high, because Nix is not “just a tool”, it’s a complete programming language that can even run pipelines, so you could technically even replace something like <code>{targets}</code> with it;
</li>
<li>
If you need to install specific versions of R packages, that are not pinned to dates, then Nix is not for you. Nix will always create a coherent environment with R and R packages that go together for a particular release of <code>nixpkgs</code>. If for some reason you need a very old version of <code>{ggplot2}</code> but a much more recent version of <code>{dplyr}</code>, using Nix won’t make this any easier than other methods;
</li>
<li>
There is no easy way (afaik) to find the version of <code>nixpkgs</code> that you need to download to find the version of R that you may need; <strong>UPDATE</strong>: turns out that there is such a <a href="https://lazamar.co.uk/nix-versions/?channel=nixpkgs-unstable&amp;package=r">simple tool</a>, thanks to <span class="citation"><span class="citation" data-cites="shane">@shane</span></span><span class="citation"><span class="citation" data-cites="hachyderm.io">@hachyderm.io</span></span> for the telling me!
</li>
<li>
R packages (and I guess others for other programming languages as well) that are available on the stable channel of <code>nixpkgs</code> lag a bit behind their counterparts on CRAN. These usually all get updated whenever there’s a new release of R. Currently however, R is at version 4.2.3, but R should be at version 4.3.1 on the stable branch of <code>nixpkgs</code>. This can sometimes happen due to various reasons (there are actual human beings behind this that volunteer their time and they also have a life). There is however an “unstable” <code>nixpkgs</code> channel that contains bleeding edge versions of R packages (and R itself) if you really need the latest versions of packages (don’t worry about the “unstable” label, from my understanding this simply means that package have not been thoroughly tested yet, but is still pretty much rock-solid);
</li>
<li>
If you need something that is not on CRAN (or Bioconductor) then it’s still possible to use Nix to install these packages, but you’ll have to perform some manual configuration.
</li>
</ul>
<p>
I will keep exploring Nix, and this is essentially my todo:
</p>
<ul>
<li>
using my environment that I installed with Nix to work interactively;
</li>
<li>
write some tool that lets me specify an R version, a list of packages and it generates a <code>default.nix</code> file automagically (ideally it should also deal with packages only available on Github);
</li>
<li>
????
</li>
<li>
Profit!
</li>
</ul>
<section id="resources" class="level3">
<h3 class="anchored" data-anchor-id="resources">
Resources
</h3>
<p>
Here are some of the resources I’ve been using:
</p>
<ul>
<li>
<a href="https://nix.dev/tutorials/first-steps/towards-reproducibility-pinning-nixpkgs#pinning-nixpkgs">nix.dev tutorials</a>
</li>
<li>
<a href="https://nix-tutorial.gitlabpages.inria.fr/nix-tutorial/installation.html">INRIA’s Nix tutorial</a>
</li>
<li>
<a href="https://nixos.org/guides/nix-pills/">Nix pills</a>
</li>
<li>
<a href="https://github.com/nix-community/nix-data-science">Nix for Data Science</a>
</li>
<li>
<a href="https://christitus.com/nixos-explained/">NixOS explained</a>: NixOS is an entire Linux distribution that uses Nix as its package manager.
</li>
<li>
<a href="https://rgoswami.me/posts/nix-r-devtools/">Blog post: Nix with R and devtools</a>
</li>
<li>
<a href="https://rgoswami.me/posts/rethinking-r-nix/">Blog post: Statistical Rethinking and Nix</a>
</li>
<li>
<a href="https://lazamar.github.io/download-specific-package-version-with-nix/">Blog post: Searching and installing old versions of Nix packages</a>
</li>
</ul>
</section>
<section id="thanks" class="level3">
<h3 class="anchored" data-anchor-id="thanks">
Thanks
</h3>
<p>
Many thanks to <a href="https://github.com/jbedo">Justin Bedő</a>, maintainer of the R package for Nix, for answering all my questions on Nix!
</p>
<p>
</p>

</section>

 ]]></description>
  <category>R</category>
  <category>nix</category>
  <guid>https://b-rodrigues.github.io/posts/2023-07-13-nix_for_r_part1.html</guid>
  <pubDate>Thu, 13 Jul 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>How to self-publish a technical book on Leanpub and Amazon using Quarto</title>
  <link>https://b-rodrigues.github.io/posts/2023-06-29-book_quarto.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/gosling.jpg" width="100%">
</p>
</div>
<p>
UPDATE: I’ve update this blog post on the 30 of June 2023. I corrected a statement where I said that the <code>_quarto.yml</code> file is where you can choose the version of R to compile the book. That’s wrong, choosing the version of R to compile the book is done on the Github Actions workflow. I’ve also added some answers to questions I got on social media.
</p>
<p>
So I’ve recently self-published a book on both <a href="https://leanpub.com/raps-with-r/">Leanpub</a> as an Ebook and <a href="https://www.amazon.com/dp/B0C87H6MGF">Amazon</a> as a <em>physical</em> book and thought that it would be worth it to write down how to do it. I’ve wasted some time getting everything to work flawlessly so if you’re looking for a guide on how to create both an ebook and a print-ready PDF for Amazon’s Kindle Direct Publishing service using Quarto, you’ve come to the right place.
</p>
<p>
If you don’t want to waste time reading, <a href="https://github.com/b-rodrigues/kdp_quarto">just fork this repo</a> and use that as a starting point. Each time you push a change to the repo, a website, Epub and PDF get generated using Github Actions. If you want to understand the details, read on.
</p>
<section id="your-books-dependencies" class="level2">
<h2 class="anchored" data-anchor-id="your-books-dependencies">
Your book’s dependencies
</h2>
<p>
You should start by creating an <code>renv.lock</code> file. This file will list all the dependencies of your book. For example, if you’re using <code>{ggplot2}</code> to make graphs or <code>{flextable}</code> for tables, the <code>renv.lock</code> file will list them and then this file will be used to download the required packages by Github Actions (more on Github Actions later) to create the book. The template comes with one such <code>renv.lock</code> file, but you should generate one specific to your project. Simply install <code>{renv}</code> and run:
</p>
<pre class="r"><code>renv::init()</code></pre>
<p>
Answer “Y” to the question and wait a little. The <code>renv.lock</code> file should appear alongside the source of your book now. If you need to install more packages to keep writing your book, install them as usual (using <code>install.packages(“package”)</code>) but then don’t forget to create a new <code>renv.lock</code> file using <code>renv::snapshot()</code>.
</p>
</section>
<section id="quarto.yml" class="level2">
<h2 class="anchored" data-anchor-id="quarto.yml">
_quarto.yml
</h2>
<p>
Whatever output format, everything gets defined in the <code>_quarto.yml</code> file in the root directory of your book. This file is where you can choose which themes to use for your website for example, which output formats you want to compile your book into, etc. I’ll discuss each option for each format below.
</p>
</section>
<section id="generating-the-website" class="level2">
<h2 class="anchored" data-anchor-id="generating-the-website">
Generating the website
</h2>
<p>
I’m using Github Actions to generate each format of the book. Github Actions is essentially a computer hosted by Github that you can use to run arbitrary commands. These commands must be written in a specific file which must be put in a specific place in your repository. <a href="https://github.com/b-rodrigues/kdp_quarto/blob/main/.github/workflows/build_book.yml">Here</a> is that file for the repo I linked above. I won’t go into details because I’ve explained how Github Actions works <a href="https://www.brodrigues.co/blog/2022-11-19-raps/">here</a> already, but you’ll notice that you can choose a version of R to compile your document and also a different version of Quarto. This can be useful for reproducibility.
</p>
<p>
Create a new branch called <code>gh-pages</code> and then go to settings, then on the side-bar on the left choose “Actions”, and scroll down. Then, in “Workflow persmissions”, check “Read and Write permissions” and “Allow Github Actions to create and approve pull requests”. Then go to “Pages” which you can find on the side-bar on the left, and choose “Deploy from a branch” under “Build and deployment” and choose “gh-pages” and “root”:
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/github_pages.png" width="100%">
</p>
</div>
<p>
Each time you push, the website to your book will get updated. Here are the options I chose for my website, which you can find in the <code>_quarto.yml</code> file:
</p>
<pre><code>html:
   theme:
     light: flatly
     dark: solar
   css:
     epub.css</code></pre>
<p>
So my website is available in two themes, a dark and light one. I highly recommend you also provide two themes. You can also provide a <code>.css</code> file to customize the appearance of you website, and of your ebook (that’s because an Epub is actually a bunch of html files). The <code>.css</code> I’m using is quite simple, the only thing it’s doing is making sure that images won’t be wider than the web-page. You can view the website of this template book <a href="https://b-rodrigues.github.io/kdp_quarto/">here</a>.
</p>
</section>
<section id="generating-an-ebook" class="level2">
<h2 class="anchored" data-anchor-id="generating-an-ebook">
Generating an Ebook
</h2>
<p>
Let’s continue with the <code>.epub</code> format. This is an Ebook format that can be read on E-readers such as the Kindle from Amazon. If you want to sell an Ebook on Leanpub (or send it to your Kindle), it needs to pass a tool called <code>epubcheck</code>.
</p>
<p>
I’ve already written about generating ebooks in the past (<a href="https://www.brodrigues.co/blog/2023-03-03-quarto_books/">here</a>). However, the advice in that blog post was only valid because there were bugs in the version of Quarto that was current at the time and so I showed some workarounds. With the current version, no workarounds are needed anymore, Epubs generated by Quarto now pass <code>epubcheck</code>. Check the source, specifically <code>index.qmd</code> to see how I include graphics.
</p>
</section>
<section id="generating-a-print-ready-pdf" class="level2">
<h2 class="anchored" data-anchor-id="generating-a-print-ready-pdf">
Generating a print-ready PDF
</h2>
<p>
This was the hardest part: I’m using Amazon’s KDP service to sell physical copies of the book and the PDF needs to be in a specific format. I’m using the 6 x 9 format without bleed, which seems to be the most common. If you look again at the <code>_quarto.yml</code> file, you should see this:
</p>
<pre><code>  pdf:
    keep-tex: true
    documentclass: scrbook
    classoption: [paper=6in:9in,pagesize=pdftex,headinclude=on,footinclude=on,12pt]
    include-in-header:
      text: |
        \usepackage{fvextra}
        \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
        \areaset[0.50in]{4.5in}{8in}
    include-before-body:
      text: |
        \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
           showspaces = false,
           showtabs = false,
           breaksymbolleft={},
           breaklines
           % Note: setting commandchars=\\\{\} here will cause an error 
        }
    fig-pos: 'H'</code></pre>
<p>
What’s important is ‘classoption’:
</p>
<pre><code>classoption: [paper=6in:9in,pagesize=pdftex,headinclude=on,footinclude=on,12pt]</code></pre>
<p>
This is where I can choose the dimensions of the book (6 x 9) and the size of the font (12pt). Then:
</p>
<pre><code>include-in-header:
  text: |
    \usepackage{fvextra}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
    \areaset[0.50in]{4.5in}{8in}</code></pre>
<p>
With <code>fvextra</code> and the call to <code></code> I make sure that long lines of code get wrapped in the PDF. Without this, long lines of code would simply continue outside the margins of the PDF, and Amazon’s KDP doesn’t accept a PDF like this.
</p>
<p>
<code></code>: this is the area that well be used for writing. These are the correct dimensions for a 6 by 9 book without bleed. Then, I keep customizing the <code>verbatim</code> environment:
</p>
<pre><code>include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
       showspaces = false,
       showtabs = false,
       breaksymbolleft={},
       breaklines
       % Note: setting commandchars=\\\{\} here will cause an error 
    }</code></pre>
<p>
Finally, the last option:
</p>
<pre><code>fig-pos: 'H'</code></pre>
<p>
This ensures that the figures are placed EXACTLY where you say they should be in the final PDF. For those of you that use LaTeX, you know that LaTeX sometimes takes some liberties with figure placement. I’ve been told the lie that LaTeX knows where to place the figures perfectly well many times but I don’t buy it. So use <code>fig-pos: ‘H’</code> to avoid lots of frustration.
</p>
<p>
That’s it! You should now be able to generate a book that is print-ready, and an Epub that passes <code>epubcheck</code> as well as website. You can now just focus on writing. Also check the source of <code>index.qmd</code> for to see how to include text in the PDF only (or not show text in the PDF).
</p>
</section>
<section id="my-personal-experience-and-some-faq" class="level2">
<h2 class="anchored" data-anchor-id="my-personal-experience-and-some-faq">
My personal experience and some FAQ
</h2>
<p>
Overall, I enjoyed using both Leanpub and Amazon to publish my book. Leanpub is really nice, because they really offer a very nice deal: you get 80% of the sales price as royalties, which is the highest share I’ve seen. Also the people working there are really nice, I’ve had the chance to discuss with Len Epp, Leanpub’s cofounder, in his <a href="https://youtu.be/aXfjhf2cDo0">Frontmatter podcast</a> and would definitely continue using their platform in the future. Regarding Amazon I must say that the experience was quite good as well; the only friction I had was getting the PDF in the right format for printing, but that’s hardly something that Amazon can be blamed for. After all if the PDF’s formatting is bad, the books will look like crap as well. One thing you should know though is that you need to get a VAT number to sell books on Amazon. I live in Luxembourg and getting one is trivial, but in other countries this may be more complex. You should know as well that Leanpub can sell the physical copies of your book, through Amazon, for you. They essentially act as a publisher then. This way, you don’t need to get a VAT number, if that’s difficult for you. But you’ll need to share the Amazon royalties with Leanpub.
</p>
<p>
When publishing a book through Amazon’s KDP service, you also get access to a basic book cover editor that you can use to create a cover for your book. You can provide an image and then use the editor to create the cover, but you can also provide a ready-made cover if you have the talent to make one using an image editing software. Once you’re done, and click “Publish” on Amazon, the book will get reviewed by a human. This process can take up to three days, but in my case it took only 4 to 6 hours. However, my book was rejected, twice. One time was because one of the images I used in the book had colour bars in the bottom right corner, that I needed to remove, and the other time was because the “g” in my name, “Rodrigues” was cut by the cover editor and it was hard to tell if it was a “g” or a “q”. Once I corrected both issues, the book was available for order on Amazon.com within the day. The other marketplaces, like France and Germany took one day more, and the UK marketplace took 4 days.
</p>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">
References
</h3>
<p>
I’m sorry, I have no idea where I found all of this stuff. I looked for it for some time, and if memory serves most of this came from stackoverflow.
</p>


</section>
</section>

 ]]></description>
  <category>R</category>
  <category>proramming</category>
  <guid>https://b-rodrigues.github.io/posts/2023-06-29-book_quarto.html</guid>
  <pubDate>Thu, 29 Jun 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Why you should consider working on a dockerized development environment</title>
  <link>https://b-rodrigues.github.io/posts/2023-05-08-dock_dev_env.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/ship_pc.png" width="100%">
</p>
</div>
<p>
Last year I wrote a post about dockerizing <code>{targets}</code>’s pipelines (<a href="../posts/2022-11-19-raps.html">link to post</a>) and between that blog post and this one, I’ve written a whole book on building reproducible analytical pipelines that you can read <a href="https://raps-with-r.dev/">here</a> (for free!). In the book I explain how you can build projects that will stay reproducible thanks to Docker. With Docker, you don’t only ship the code to your project, but ship <em>a whole computer</em> with it, and your project will be executed inside that computer. By <em>whole computer</em> I mean the whole computational environment: so a version of R, the required packages your project depends on, all of it running on a Linux distribution (usually Ubuntu). The whole project can then be executed like any program from your computer (whether you’re running Windows, macOS or Linux) or even on the cloud with a single command.
</p>
<p>
In this blog post, I’ll discuss something that I’ve been trying for some time now: working directly from a dockerized environment. The idea is to have a Docker image that comes with R, all the usual packages I use for development, Quarto, a LaTeX distribution (that I installed with <code>{tinytex}</code>) and finally, my IDE of choice, Spacemacs (if you use RStudio, just read on, I’ll explain how you can achieve the same thing but with RStudio instead). Why do this? Well because this way I can deploy the exact same environment anywhere. If I get a new computer, I’m only one command line away from a functioning environment. If I want to dockerize a <code>{targets}</code> pipeline, I can write a new Dockerfile that builds upon this image which ensures that there are no discrepancies between the development environment and the production environment. And because I’m building the image on top of a Rocker image, everything just work. If I need to install a package that might be tricky to install (for example, a package that depends on <code>{rJava}</code>, using Docker might be the simplest way to get it to work.
</p>
<p>
So, here’s the Dockerfile:
</p>
<pre><code># This builds upon the Rocker project's versioned image for R version 4.3
FROM rocker/r-ver:4.3

# install `gpg-agent` and `software-properties-common` which are needed to add an Ubuntu ppa to install Emacs
RUN apt-get update \
    &amp;&amp; apt-get install -y --no-install-recommends \
    gpg-agent software-properties-common

# add the ppa which includes the latest version of Emacs
RUN add-apt-repository ppa:kelleyk/emacs

# Install `git`, `wget` and the latest `Emacs`
RUN apt-get update \
    &amp;&amp; apt-get install -y --no-install-recommends \
    git \
    wget \
    emacs28-nox

# Install spacemacs by cloning its repository
RUN git clone -b develop https://github.com/syl20bnr/spacemacs ~/.emacs.d

# Download my .spacemacs config file
RUN wget https://raw.githubusercontent.com/b-rodrigues/dotfiles/master/dotfiles/.spacemacs -O ~/.spacemacs

# This launches emacs in daemon mode. This is needed to initialize spacemacs.
# Running it in daemon mode is required because a Dockerfile must be setup non-interactively
RUN emacs --daemon

# Install a bunch of Ubuntu dependencies. These are typical dependencies required to use some
# R packages on Linux.
RUN apt-get update \
   &amp;&amp; apt-get install -y --no-install-recommends \
   aspell \
   aspell-en \
   aspell-fr \
   aspell-pt-pt \
   libfontconfig1-dev \
   libglpk-dev \
   libxml2-dev \
   libcairo2-dev \
   libgit2-dev \
   default-libmysqlclient-dev \
   libpq-dev \
   libsasl2-dev \
   libsqlite3-dev \
   libssh2-1-dev \
   libxtst6 \
   libcurl4-openssl-dev \
   libharfbuzz-dev \
   libfribidi-dev \
   libfreetype6-dev \
   libpng-dev \
   libtiff5-dev \
   libjpeg-dev \
   libxt-dev \
   unixodbc-dev \
   pandoc

# Download the latest version of Quarto
RUN wget https://github.com/quarto-dev/quarto-cli/releases/download/v1.3.340/quarto-1.3.340-linux-amd64.deb -O ~/quarto.deb

# Install the latest version of Quarto
RUN apt-get install --yes ~/quarto.deb

# Remove the installer
RUN rm ~/quarto.deb

# Create a directory to host my projects
RUN mkdir /root/projects/

# Write a bunch of lines to the .Rprofile
# This makes sure that the httpgd server runs on localhost and on the port 8888
RUN echo 'options(httpgd.host = "0.0.0.0", httpgd.port = 8888, httpgd.token = "aaaaaaaa")' &gt;&gt; /root/.Rprofile

# This option clones renv cache folders inside the root folder of the projects. This makes
# sure that they stay persistent across reboots.
RUN echo 'options(renv.config.cache.symlinks = FALSE)' &gt;&gt; /root/.Rprofile

# Serve shiny apps through localhost and port 8888
RUN echo 'options(shiny.host = "0.0.0.0", shiny.port = 8888)' &gt;&gt; /root/.Rprofile

# Set the CRAN package repositories to the PPPM at the 28th of April
RUN echo 'options(repos = c(REPO_NAME = "https://packagemanager.rstudio.com/cran/__linux__/jammy/2023-04-28"))' &gt;&gt; /root/.Rprofile

# Install the usual packages I use
RUN R -e "install.packages(c('quarto', 'remotes', 'tinytex', 'tidyverse', 'arrow', 'chronicler', 'janitor', 'targets', 'tarchetypes', 'openxlsx', 'shiny', 'flexdashboard', 'data.table', 'httpgd', 'blogdown', 'bookdown'))" 

# Install the g2r package (not yet available on CRAN)
RUN R -e "remotes::install_github('devOpifex/g2r')"

# Install a LaTeX distro using tinytex
RUN R -e "tinytex::install_tinytex()"

# Install hugo for blogging
RUN R -e "blogdown::install_hugo()"

# Expose port 8888
EXPOSE 8888</code></pre>
<p>
(and <a href="https://github.com/b-rodrigues/ess_dev_env">here’s</a> the repository where you can find it).
</p>
<p>
I’ve explained each line of the Dockerfile using comments in the Dockerfile itself. But before explaining it in more detail, here’s a word from this blog post’s sponsor: me, I’m this post’s sponsor.
</p>
<p>
If you have read until here dear reader, let me express my gratitude by offering you a <a href="https://leanpub.com/raps-with-r/c/blog_reader">discount code</a> to purchase a DRM-free Epub and PDF version of my book, Building reproducible analytical pipelines with R (that you can also read for free <a href="https://raps-with-r.dev/">here</a> by the way). Using the <a href="https://leanpub.com/raps-with-r/c/blog_reader">discount code</a> you can get a DRM-free epub and PDF version of the book for 14.99 instead of 19.99! If you want a good old physical book instead, you’ll need to wait some more, I still need to get the formatting right before making it available through Amazon self-publishing service.
</p>
<p>
Now back to our Dockerfile. There are several decisions that I took that I need explain: first, why use a versioned image, and why use the PPPM at a specific date? I did this so that it doesn’t matter <em>when</em> I build the image, I always know which version of R and packages I get. Then, what’s with all the options that I write to the <code>.Rprofile</code>? Well, don’t forget that when I will be running the Docker container defined by this image, I will be using Emacs inside a terminal. So if I want to see plots for example, I need to use the <code>{httpgd}</code> package. This package provides a graphics device that runs on a web server, so if I tell <code>{httpgd}</code> to serve the images over port <code>8888</code>, and then expose this port in the Dockerfile, I can access <code>{httpgd}</code> from my web browser by pointing it to <code>http://localhost:8888</code>. Here’s how this looks like:
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/dev_env_httpgd.png" width="100%">
</p>
</div>
<p>
The terminal on top of the image is running my dockerized environment, and below you see my web browser on to the <code>http://localhost:8888/live?token=aaaaaaaa</code> url to access the <code>{httpgd}</code> web server <em>that is running inside the Docker container</em>. And it’s the same logic with Shiny: if I’m working on a Shiny app from inside the container, I can access it by going to <code>http://localhost:8888/</code>. Now, I have to do all of this because I’m running Emacs, but if you’re developing with RStudio, you could instead run RStudio server, access it on <code>http://localhost:8888/</code>, and then no need to think about configuring on which ports <code>{httpgd}</code> serves images, or on which port Shiny apps should run. Everything will be directly visible from within RStudio. <a href="https://github.com/rocker-org/rocker-versioned2/blob/master/dockerfiles/rstudio_4.3.0.Dockerfile">Here is the Dockerfile</a> to run R version 4.3 with RStudio as the IDE. If you want to use this, you could simply start from the above Dockerfile and then add the stuff you need, for example:
</p>
<pre><code>FROM rocker/rstudio:4.3.0

# and add what you want below like installing R packages and whatnot</code></pre>
<p>
There is still one important thing that you should know before using a dockerized development environment: a running Docker container can be changed (for example, you could install new R packages), but once you shut it down and restart it, any change will be lost. So how do you save your work? Well, you need to run the Docker image with a volume. A volume is nothing more than a folder on your computer that is linked to a folder inside the Docker container. Anything that gets saved there from the Docker container will be available on your computer, and vice-versa. Here is the command that I use to run my container:
</p>
<pre><code>docker run --rm -it --name ess_dev_env -p 8888:8888 -v /home/path_to_volume/folder:/root/projects:rw brodriguesco/ess_dev_env:main-cdcb1719d emacs</code></pre>
<p>
Take note of the <code>-v</code> flag, especially what comes after: <code>/home/path_to_volume/folder:/root/projects:rw</code>. <code>/home/path_to_volume/folder</code> is the folder on my computer, and it is linked to the <code>/root/projects</code> folder inside the Docker container. When I run the above command inside a terminal, Spacemacs starts and I can get to work! If you build a development environment based on RStudio, you would essentially use the same command, you would only need to set a password to login first (read the instructions <a href="https://rocker-project.org/images/versioned/rstudio.html#how-to-use">here</a>).
</p>
<p>
Also, if you forgot to add a package and want to install it and make this change permanent, the best way is to add it to the Dockerfile and rebuild the image. I’ve streamlined this process by using Github Actions to build images and push them to Docker Hub. Take a look at the <a href="https://github.com/b-rodrigues/ess_dev_env">Github repository where my Dockerfile is hosted</a>, and if you are familiar with Github Actions, take a look at my workflow file. You’ll see that I’ve set up Github Actions to build the Docker image and push it to Docker Hub each time I commit, and name the Docker image <code>ess_dev_env:main-xxxxx</code> where <code>xxxxx</code> is the corresponding commit hash on Github (so I can easily know which image was built with which commit).
</p>
<p>
I’ll be using this dockerized image for some time, and see how it feels. For now, it works quite well!
</p>



 ]]></description>
  <category>R</category>
  <category>proramming</category>
  <guid>https://b-rodrigues.github.io/posts/2023-05-08-dock_dev_env.html</guid>
  <pubDate>Mon, 08 May 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>I’ve been blogging for 10 years</title>
  <link>https://b-rodrigues.github.io/posts/2023-04-25-10_years.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/10_years.jpg">
</p>
</div>
<p>
I’ve been blogging for 10 years, actually even a bit more than that, my very first blog is not online anymore and if I remember correctly was in French. I think it was around 2010 when I was a Master’s student.
</p>
<p>
Anyways, here are my thoughts on why I think blogging is a very nice activity if you’re interested in technology or science.
</p>
<p>
The primary reason I started my blog was to have a repository of code snippets that I could re-use. Anytime I had to do something for my thesis or for work, I would write instructions around the code that I’ve used to explain how and why things worked out. But I needed a spot to save these scripts, and it turns out that a blog was the best solution for this: it doesn’t require any subscription to a (very often proprietary) service to store my notes for me, and I need 0 discipline to maintain a blog. Simply write a post, push to Github, website gets updated. If I would store the notes myself on my computer instead, this would mean a lot of work, and I would need to think about how to make them available across devices.
</p>
<p>
The other reason is that I thought that this would be a good way for me to contribute to the wider free software and open source ecosystem. I’m not a programmer, so contributing code would be quite difficult for me. I’ve recently published a <a href="https://b-rodrigues.github.io/chronicler/">package</a>, so in the end I ended up contributing code, but that was more due to “luck” finding an actual problem that hadn’t been solved (well, that’s not really the case, logging in R had been solved, but not using a monad and for some reason I had become obsessed with monads in 2022) and also thanks to the help of much better programmers than myself. So writing and posting these blog posts would be my way to contribute to the community. I think that this was the right decision, as I’ve had many people throughout the years thank me for some of my blog posts that helped them with some of their tasks.
</p>
<p>
Now, not all blog posts were about problems that I encountered throughout my career. There were also some blog posts about topics that piqued my interest, but purely out of curiosity. For example the ones about NLP, like this <a href="https://www.brodrigues.co/blog/2019-04-07-historical_newspaper_scraping_tesseract/">one</a>, among others. I’m lucky enough that I find enjoyment in programming and exploring data for fun, so I do write a lot of code. But how did I manage to consistently write blog posts for 10 years+?
</p>
<p>
I think that part of the reason is that I have literally 0 commitment to this blog. I don’t force myself to write on a schedule and sometimes don’t write for months like in 2020 where I didn’t write for 5 months because I was busy renovating my home and taking care of my baby daughter. I didn’t even miss writing. I think that one of the reasons this works is also because I have absolutely 0 trackers on this website. For all I know, this post will get read by 3 people, me, you and my mom. The only clue I have that one particular post was successful is if people reshare the announcement I make on social media, or if they contact me with questions or praise, or if it gets picked up for the R weekly highlights. By not having any trackers and not having really a clue of what people like, I avoid falling into the “recommendation engine” or “SEO” trap. If I did use trackers and knew exactly what my <em>audience</em> wanted, I’d be very incentivized to just continue writing what would generate the most traffic. And the issue with that, is that it would feel like a job, and I very likely would have abandoned this endeavour a long time ago. What generates a lot of traffic are posts mostly aimed at beginners, tutorials that explain how to do a t-test or make a bar graph with two y-axes, but sorry, I’m not interested. I don’t <em>hate</em> beginners, but I don’t only want to write tutorials to serve ads to my readers and tell them to subscribe to newsletters, bla bla bla. I have no qualms with people doing this, but that’s simply not my thing. Not interested (and don’t get me wrong, I have nothing against tutorial blog post, quite the contrary, especially when they present some lesser-known features of a package).
</p>
<p>
Also, the first thing I install on my web-browser is an ad-blocker, and I would be a huge hypocrite if I tracked my blog’s visits. To be fully transparent, I do use Goatcounter for my latest book <a href="https://raps-with-r.dev/">here</a>, but I don’t adjust the book’s content to the audience. It’s just to know if people read it, because writing a book takes some effort and I was curious. I might remove it in the future though. Again, nothing against people trying to live off the internet, I myself accept <a href="https://www.buymeacoffee.com/brodriguesco">donations</a>, but if I started to look into what drives people to click and donate, I’d turn this into a job, and that would be the fastest way for me to hate blogging.
</p>
<p>
So the reason this has been working is simply because I avoided considering this as an obligation, a duty or a job, and in order to do this, I avoid collecting data.
</p>
<p>
So blogging is a nice way to store code snippets and quickly find them. These code snippets can in turn help other people facing similar issues. But what are some other benefits of blogging? First, by turning your code into a full-fledged blog post, it also forces you to think more carefully about the solution you found. Very often, while writing the blog post to a problem I’ve solved, I often find a simpler, more elegant solution, and then use that solution instead to solve my problem. It’s a similar idea to rubber duck debugging, or writing a Minimal Reproducible Example when opening an issue because (you think) you found a nasty bug somewhere in that package you use daily and never bothered reading the documentation for carefully. Blogging is also a way to get some feedback by other people, and sometimes people show me other ways of doing things, like <a href="https://www.brodrigues.co/blog/2021-09-05-speedrunning_rows/">here</a>. Blogging also helped me meet people in the real world, and discuss with them about certain of the topics I’ve blogged about. I think that’s really neat.
</p>
Blogging also helps (at least it helped me) realize what I really enjoy about statistics, programming, and data science, and once you have a nice collection of blog posts, you could turn them easily into a book. That’s another way of contributing to the community. I’ve written two books, the latest one seems to really interest people:
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
My book is done. I need to write a conclusion to the last chapter and will likely rewrite some paragraphs, but I won’t make major changes anymore. So if you’re interested in building reproducible analytical pipelines with <a href="https://twitter.com/hashtag/RStats?src=hash&amp;ref_src=twsrc%5Etfw">#RStats</a>, take a look it’s free: <a href="https://t.co/Vx8LGpddwR">https://t.co/Vx8LGpddwR</a> <a href="https://t.co/PKFb2P4pJK">pic.twitter.com/PKFb2P4pJK</a>
</p>
— Bruno Rodrigues (<span class="citation"><span class="citation" data-cites="brodriguesco">@brodriguesco</span></span><span class="citation"><span class="citation" data-cites="fosstodon.org">@fosstodon.org</span></span>) (<span class="citation"><span class="citation" data-cites="brodriguesco">@brodriguesco</span></span>) <a href="https://twitter.com/brodriguesco/status/1650229398988095489?ref_src=twsrc%5Etfw">April 23, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>
That’s my most “viral” tweet! Before it was this classic:
</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
“doing science” <a href="https://t.co/T0TJQ6vY6q">pic.twitter.com/T0TJQ6vY6q</a>
</p>
— Bruno Rodrigues (<span class="citation"><span class="citation" data-cites="brodriguesco">@brodriguesco</span></span><span class="citation"><span class="citation" data-cites="fosstodon.org">@fosstodon.org</span></span>) (<span class="citation"><span class="citation" data-cites="brodriguesco">@brodriguesco</span></span>) <a href="https://twitter.com/brodriguesco/status/1443538925474222080?ref_src=twsrc%5Etfw">September 30, 2021</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>
I’m sure that many people, maybe even you!, work in an industry that tackles many interesting problems, and could share that with the world either through blog posts or books, or videos (yes, I also do that <a href="https://www.youtube.com/channel/UCTZXht1RTL2Duc3eU8MYGzQ">sometimes</a>), and many people would find that interesting. I’ve had people tell me that they have nothing interesting to write about, so they don’t even want to start. Who cares, just write.
</p>
<p>
So it’d be cool if you blogged. I like it, so maybe you will as well?
</p>
<p>
Anyways, thanks for reading, especially if you’ve been reading my blog for years.
</p>
<p>
Here’s to 10 more years!
</p>



 ]]></description>
  <category>R</category>
  <category>proramming</category>
  <guid>https://b-rodrigues.github.io/posts/2023-04-25-10_years.html</guid>
  <pubDate>Tue, 25 Apr 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Automating checks of handcrafted Word tables with {docxtractr}</title>
  <link>https://b-rodrigues.github.io/posts/2023-03-18-docxtractr.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/excel_nightmare.png" width="80%" height="auto">
</p>
</div>
<p>
Unfortunately not everyone knows about literate programming so many tables in Word documents are “generated” by hand (generated is really too strong a word) and what very often happens is that these handcrafted tables have typos. Usually it’s totals that are wrong. Checking the totals in these tables by hand with a pocket calculator is a tedious, long and boring job.
</p>
<p>
So as my job’s <em>statistician but also kinda automation dude that types all day in a weird black box</em>, I’ve been asked if it were possible to automate these checks on tables in a Word document. And of course, the answer is yes, because whatever you need to get done, there’s an R package for that!
</p>
<p>
There are, to my knowledge, 2 packages that we can use to get tables from a Word document into R (an activity that I will now refer to as <em>office-scraping</em>).
</p>
<p>
These packages are <code>{officer}</code> and <code>{docxtractr}</code>. For his particular task I’ve used <code>{docxtractr}</code>. The reason is that <code>{docxtractr}</code> returns the table “as-is”, while <code>{officer}</code> returns a tibble where each cell of the table gets a row in the tibble. <code>{officer}</code>’s representation of the scraped tables might be useful in some cases, but in this particular case, <code>{docxtractr}</code>’s approach was exactly what I needed.
</p>
<p>
First of all, we need a Word document with some tables.<a href="https://github.com/rbind/b-rodrigues.github.com/raw/master/content/blog/report.docx">Here’s one</a> I’ve prepared that contains two tables that look close to the ones I had to deal with. In the actual document, there were hundreds of such tables. Here’s a picture of the tables in case you can’t open the document:
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/report_tables.png" width="80%" height="auto">
</p>
</div>
<p>
The first table is divided by departments of the company, and each section of the table has its own sub-total. As stated in the beginning, the goal is to check for typos by recomputing the sub-totals and totals and then comparing the original tables with the ones where the totals were recomputed.
</p>
<p>
The problem we will face with each table are the merged cells; if there were no merged cells, scraping them with <code>{docxtractr}</code> would be trivially simple, but because of these merged cells, we will have to write quite a lot of code to get them in a format that we can actually use.
</p>
<section id="extracting-the-tables-using-docxtractr" class="level2">
<h2 class="anchored" data-anchor-id="extracting-the-tables-using-docxtractr">
Extracting the tables using {docxtractr}
</h2>
<p>
<code>{docxtractr}</code> has a very handy function that gets all the tables from a Word document and puts them into a list (it’s also possible to extract other stuff like comments). Let’s start by loading <code>{dplyr}</code> (for the rest of the functions, I’ll use the <code>package::function()</code> notation to make it clear where the functions come from):
</p>
<pre class="r"><code>library(dplyr)</code></pre>
<p>
Let’s now read the document using <code>{docxtractr}</code>:
</p>
<pre class="r"><code>doc_raw &lt;- docxtractr::read_docx("report.docx")</code></pre>
<p>
And let’s get all the tables:
</p>
<pre class="r"><code>list_tables &lt;- docxtractr::docx_extract_all_tbls(doc_raw)</code></pre>
<p>
Let’s now take a look at the second element of the list, which corresponds to the second table (I’m starting with the second table because it’s the smallest of the two):
</p>
<pre class="r"><code>table_1 &lt;- list_tables[[1]] %&gt;%
  janitor::clean_names()

table_2 &lt;- list_tables[[2]] %&gt;%
  janitor::clean_names()

table_2</code></pre>
<pre><code>## # A tibble: 8 × 11
##   company            x2020 x2021 x2022 na    na_2  na_3  na_4  na_5  na_6  na_7 
##   &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;
## 1 ""                 M     F     Total M     F     Total M     F     Total &lt;NA&gt; 
## 2 "Luxembourg branc… Work… 92    124   210   87    129   216   99    129   228  
## 3 ""                 Mana… 22    81    103   28    81    109   26    85    112  
## 4 ""                 Assi… 1     0     10    1     0     1     1     0     1    
## 5 ""                 Dire… 3     1     4     0     0     0     0     1     0    
## 6 "External consult… 38    55    95    35    64    99    42    70    112   &lt;NA&gt; 
## 7 "Statisticians"    0     0     0     0     0     0     0     0     0     &lt;NA&gt; 
## 8 "Total"            156   263   419   151   274   425   168   285   453   &lt;NA&gt;</code></pre>
<p>
As you can see, because of the merged cells, the rows are not all aligned with the columns. So we need to split the table, and treat the two parts separately. I’m starting with the part of the table where the rows are correctly aligned with the columns. This is just a matter of renaming some columns, and converting the numbers (that are represented as characters) into <code>numeric</code>s:
</p>
<pre class="r"><code>table_2_1 &lt;- table_2 %&gt;%
  select(-company) %&gt;%
  filter(!is.na(na_7)) %&gt;%
  purrr::set_names(
    c("worker_type",
      "m_2020",
      "f_2020",
      "t_2020",
      "m_2021",
      "f_2021",
      "t_2021",
      "m_2022",
      "f_2022",
      "t_2022"
      )
    ) %&gt;%
  mutate(across(!starts_with("worker"),
                as.numeric))

table_2_1</code></pre>
<pre><code>## # A tibble: 4 × 10
##   worker_type m_2020 f_2020 t_2020 m_2021 f_2021 t_2021 m_2022 f_2022 t_2022
##   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1 Workers         92    124    210     87    129    216     99    129    228
## 2 Managers        22     81    103     28     81    109     26     85    112
## 3 Assistants       1      0     10      1      0      1      1      0      1
## 4 Directors        3      1      4      0      0      0      0      1      0</code></pre>
<p>
Let’s now deal with the second part of the table. This is the part of the table where the rows were not aligned with the columns, due to the merged cells. The operations are essentially the same as before, the difference is that we need to remove a different column (here we remove <code>na_7</code>, before it was <code>company</code>):
</p>
<pre class="r"><code>table_2_2 &lt;- table_2 %&gt;%
  filter(is.na(na_7)) %&gt;%
  select(-na_7) %&gt;%
  rename(worker_type = company) %&gt;%
  filter(worker_type != "") %&gt;%
  purrr::set_names(
           c("worker_type",
             "m_2020",
             "f_2020",
             "t_2020",
             "m_2021",
             "f_2021",
             "t_2021",
             "m_2022",
             "f_2022",
             "t_2022"
             )
         ) %&gt;%
  mutate(across(!starts_with("worker"),
                as.numeric))

table_2_2</code></pre>
<pre><code>## # A tibble: 3 × 10
##   worker_type     m_2020 f_2020 t_2020 m_2021 f_2021 t_2021 m_2022 f_2022 t_2022
##   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1 External consu…     38     55     95     35     64     99     42     70    112
## 2 Statisticians        0      0      0      0      0      0      0      0      0
## 3 Total              156    263    419    151    274    425    168    285    453</code></pre>
<p>
I didn’t comment the operations, but if you’re following along, take some time to see what each line does.
</p>
<p>
Now we can bind the rows and we end up with the table from the Word document as a flat and easy to manipulate data frame:
</p>
<pre class="r"><code>table_2_clean &lt;- bind_rows(
  table_2_1,
  table_2_2
)

table_2_clean</code></pre>
<pre><code>## # A tibble: 7 × 10
##   worker_type     m_2020 f_2020 t_2020 m_2021 f_2021 t_2021 m_2022 f_2022 t_2022
##   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1 Workers             92    124    210     87    129    216     99    129    228
## 2 Managers            22     81    103     28     81    109     26     85    112
## 3 Assistants           1      0     10      1      0      1      1      0      1
## 4 Directors            3      1      4      0      0      0      0      1      0
## 5 External consu…     38     55     95     35     64     99     42     70    112
## 6 Statisticians        0      0      0      0      0      0      0      0      0
## 7 Total              156    263    419    151    274    425    168    285    453</code></pre>
<p>
All of this because of these merged cells! This may seem like a lot of work, but imagine that you need to check 50 such tables. You could put all the previous operations into a function and then simply apply that function over all the tables (which is exactly what I did at my job). So you end up with 50 cleaned tables in a matter of seconds. Now let’s not forget our original objective, we wanted to recompute the totals to check if everything was alright. In the operations below I remove the columns that represent the totals and remove the row with the grand totals as well. I then simply recompute the totals:
</p>
<pre class="r"><code>table_2_totals &lt;- table_2_clean %&gt;%
  select(-starts_with("t_")) %&gt;%
  filter(worker_type != "Total") %&gt;%
  mutate(
    t_2020 = m_2020 + f_2020,
    t_2021 = m_2021 + f_2021,
    t_2022 = m_2022 + f_2022,
    ) %&gt;%
  select(
    worker_type,
    m_2020,
    f_2020,
    t_2020,
    m_2021,
    f_2021,
    t_2021,
    m_2022,
    f_2022,
    t_2022,
    ) %&gt;%
  janitor::adorn_totals()</code></pre>
<p>
We can now compare both data frames and see if there were mistakes:
</p>
<pre class="r"><code>table_2_clean == table_2_totals</code></pre>
<pre><code>##      worker_type m_2020 f_2020 t_2020 m_2021 f_2021 t_2021 m_2022 f_2022 t_2022
## [1,]        TRUE   TRUE   TRUE  FALSE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE
## [2,]        TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE  FALSE
## [3,]        TRUE   TRUE   TRUE  FALSE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE
## [4,]        TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE  FALSE
## [5,]        TRUE   TRUE   TRUE  FALSE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE
## [6,]        TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE
## [7,]        TRUE   TRUE  FALSE  FALSE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE</code></pre>
<p>
We do see a bunch of <code>FALSE</code> statements, so we need to check those! This is where some typos where found.
</p>
<p>
Let’s now deal with table 1. The way we will handle this one will be very similar to the one before. It’s just that we have subtotals to deal with as well.
</p>
<pre class="r"><code>table_1</code></pre>
<pre><code>## # A tibble: 32 × 8
##    by_department   fte        persons na    na_2  na_3  na_4  na_5 
##    &lt;chr&gt;           &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;
##  1 ""              M          F       Total M     F     Total &lt;NA&gt; 
##  2 "Dep A"         Workers    12,30   33,40 55,70 13    36    59   
##  3 ""              Managers   3,80    19,90 15,70 4     19    19   
##  4 ""              Assistants 0,00    0,00  0,00  0     0     0    
##  5 ""              Directors  0,20    0,00  0,20  1     0     1    
##  6 "Total – Dep A" 26,30      45,30   71,60 28,00 51,00 79,00 &lt;NA&gt; 
##  7 "Dep B"         Workers    31,80   39,60 71,40 32    41    73   
##  8 ""              Managers   3,00    13,50 16,50 3     15    18   
##  9 ""              Assistants 0,00    0,00  0,00  0     0     0    
## 10 ""              Directors  0,20    0,20  0,40  1     1     2    
## # … with 22 more rows</code></pre>
<p>
Here as well, we have a problem with merged cells. But only the rows with the totals are affected. So just like before, we can split that into two tables and deal with the two parts separately:
</p>
<pre class="r"><code>table_1_1 &lt;- table_1 %&gt;%
  filter(!grepl("(t|T)otal", by_department),
         fte != "M") %&gt;%
  purrr::set_names(
           c("department",
             "worker_type",
             "m_fte",
             "f_fte",
             "t_fte",
             "m_hc",
             "f_hc",
             "t_hc"
             )
         ) %&gt;%
  mutate(department = ifelse(department == "",
                              NA,
                              department)) %&gt;%
  tidyr::fill(department, .direction = "down") %&gt;%
  mutate(across(contains("fte"),
                \(x)(gsub(pattern = ",", replacement = ".", x = x))),
         across(-c("department", "worker_type"),
                as.numeric)) %&gt;%
  as.data.frame()</code></pre>
<p>
Here again, it’s really worth it to take your time going through all the different commands.
</p>
<p>
Let’s now clean the totals:
</p>
<pre class="r"><code>table_1_2 &lt;- table_1 %&gt;%
  filter(grepl("(t|T)otal", by_department),
         fte != "M") %&gt;%
  select(by_department, na_5, everything()) %&gt;%
  purrr::set_names(
           c("department",
             "worker_type",
             "m_fte",
             "f_fte",
             "t_fte",
             "m_hc",
             "f_hc",
             "t_hc"
             )
         ) %&gt;%
  tidyr::fill(department, .direction = "down") %&gt;%
  mutate(across(-c("department", "worker_type"),
                \(x)(gsub(pattern = ",", replacement = ".", x = x))),
         across(-c("department", "worker_type"),
                as.numeric)) %&gt;%
  as.data.frame()</code></pre>
<p>
Finally, we can bind the rows and we end up with a clean data frame:
</p>
<pre class="r"><code>table_1 &lt;- bind_rows(
  table_1_1,
  table_1_2
)

table_1</code></pre>
<pre><code>##       department worker_type m_fte  f_fte  t_fte m_hc f_hc t_hc
## 1          Dep A     Workers  12.3  33.40  55.70   13   36   59
## 2          Dep A    Managers   3.8  19.90  15.70    4   19   19
## 3          Dep A  Assistants   0.0   0.00   0.00    0    0    0
## 4          Dep A   Directors   0.2   0.00   0.20    1    0    1
## 5          Dep B     Workers  31.8  39.60  71.40   32   41   73
## 6          Dep B    Managers   3.0  13.50  16.50    3   15   18
## 7          Dep B  Assistants   0.0   0.00   0.00    0    0    0
## 8          Dep B   Directors   0.2   0.20   0.40    1    1    2
## 9          Dep C     Workers  19.0  24.20  43.20   20   26   46
## 10         Dep C    Managers   1.0   8.95   9.95    1   11   12
## 11         Dep C  Assistants   0.0   0.00   0.00    0    0    0
## 12         Dep C   Directors   0.0   0.00   0.00    0    0    0
## 13         Dep D     Workers   7.5   5.00  12.50    8    5   13
## 14         Dep D    Managers   0.5   1.60   2.10    1    2    3
## 15         Dep D  Assistants   1.0   0.00   1.60    1    0    1
## 16         Dep D   Directors   0.4   0.00   0.40    1    0    1
## 17         Dep E     Workers  11.8  13.75  27.55   14   16   30
## 18         Dep E    Managers  16.0  38.20  54.20   17   42   59
## 19         Dep E  Assistants   0.0   0.00   0.00    0    0    0
## 20         Dep E   Directors   0.0   0.00   0.00    0    0    0
## 21         Dep F     Workers   0.2   0.00   0.20    1    0    1
## 22         Dep F    Managers   0.0   0.00   0.00    0    0    0
## 23         Dep F  Assistants   0.0   0.00   0.00    0    0    0
## 24         Dep F   Directors   0.2   0.00   0.20    1    0    1
## 25 Total – Dep A        &lt;NA&gt;  26.3  45.30  71.60   28   51   79
## 26 Total – Dep B        &lt;NA&gt;  35.0  53.30  98.30   36   57   93
## 27 Total – Dep C        &lt;NA&gt;  20.0  33.15  53.15   21   37   58
## 28 Total – Dep D        &lt;NA&gt;   9.4   6.60  16.00   11    7   18
## 29 Total – Dep E        &lt;NA&gt;  29.8  51.95  81.75   31   58   89
## 30 Total – Dep F        &lt;NA&gt;   1.0   1.00   0.20    1    1    1
## 31   Grand total        &lt;NA&gt; 101.5 195.40 316.90  129  216  345</code></pre>
<p>
Again, let’s not forget our objective: recomputing the totals to see if everything is alright. So because we need each sub-total, one per department, we will simply group by departments and use <code>janitor::adorn_totals()</code>. But <code>janitor::adorn_totals()</code> does not work on grouped data frames. So instead I use <code>group_nest()</code> to create a tibble with a list column, and then map <code>janitor::adorn_totals</code>:
</p>
<pre class="r"><code>table_1_subtotals &lt;- table_1 %&gt;%
  filter(!grepl("(t|T)otal", department)) %&gt;%
  group_nest(department) %&gt;%
  mutate(data = purrr::map(data, janitor::adorn_totals)) %&gt;%
  tidyr::unnest(cols = data) %&gt;%
  arrange(department) %&gt;%
  as.data.frame()</code></pre>
<p>
Ok so in the table above I have the subtotals per department. Now, I need to compute the grand total:
</p>
<pre class="r"><code>table_1_total &lt;- table_1_subtotals %&gt;%
  filter(grepl("Total", worker_type)) %&gt;%
  janitor::adorn_totals()</code></pre>
<p>
Now I just need to bind the grand total to the table from before:
</p>
<pre class="r"><code>table_1_clean &lt;- bind_rows(
  table_1_subtotals,
  filter(
    table_1_total,
    department == "Total")
)

table_1_clean</code></pre>
<pre><code>##    department worker_type m_fte  f_fte  t_fte m_hc f_hc t_hc
## 1       Dep A     Workers  12.3  33.40  55.70   13   36   59
## 2       Dep A    Managers   3.8  19.90  15.70    4   19   19
## 3       Dep A  Assistants   0.0   0.00   0.00    0    0    0
## 4       Dep A   Directors   0.2   0.00   0.20    1    0    1
## 5       Dep A       Total  16.3  53.30  71.60   18   55   79
## 6       Dep B     Workers  31.8  39.60  71.40   32   41   73
## 7       Dep B    Managers   3.0  13.50  16.50    3   15   18
## 8       Dep B  Assistants   0.0   0.00   0.00    0    0    0
## 9       Dep B   Directors   0.2   0.20   0.40    1    1    2
## 10      Dep B       Total  35.0  53.30  88.30   36   57   93
## 11      Dep C     Workers  19.0  24.20  43.20   20   26   46
## 12      Dep C    Managers   1.0   8.95   9.95    1   11   12
## 13      Dep C  Assistants   0.0   0.00   0.00    0    0    0
## 14      Dep C   Directors   0.0   0.00   0.00    0    0    0
## 15      Dep C       Total  20.0  33.15  53.15   21   37   58
## 16      Dep D     Workers   7.5   5.00  12.50    8    5   13
## 17      Dep D    Managers   0.5   1.60   2.10    1    2    3
## 18      Dep D  Assistants   1.0   0.00   1.60    1    0    1
## 19      Dep D   Directors   0.4   0.00   0.40    1    0    1
## 20      Dep D       Total   9.4   6.60  16.60   11    7   18
## 21      Dep E     Workers  11.8  13.75  27.55   14   16   30
## 22      Dep E    Managers  16.0  38.20  54.20   17   42   59
## 23      Dep E  Assistants   0.0   0.00   0.00    0    0    0
## 24      Dep E   Directors   0.0   0.00   0.00    0    0    0
## 25      Dep E       Total  27.8  51.95  81.75   31   58   89
## 26      Dep F     Workers   0.2   0.00   0.20    1    0    1
## 27      Dep F    Managers   0.0   0.00   0.00    0    0    0
## 28      Dep F  Assistants   0.0   0.00   0.00    0    0    0
## 29      Dep F   Directors   0.2   0.00   0.20    1    0    1
## 30      Dep F       Total   0.4   0.00   0.40    2    0    2
## 31      Total           - 108.9 198.30 311.80  119  214  339</code></pre>
<p>
We’re almost done! We now need to make sure that the rows are in the same order across the two tables. So we need to transform the original table from the Word document a little bit:
</p>
<pre class="r"><code>table_1 &lt;- table_1 %&gt;%
  mutate(worker_type = ifelse(is.na(worker_type),
                              "Total",
                              worker_type)) %&gt;%
  mutate(department = stringr::str_remove_all(department, "Total – "),
         worker_type = ifelse(department == "Grand total",
                              "-",
                              worker_type),
         department = ifelse(department == "Grand total",
                             "Total",
                             department))</code></pre>
<p>
We can now order them the same way, and finally compare them!
</p>
<pre class="r"><code>arrange(table_1, worker_type) == arrange(table_1_clean, worker_type)</code></pre>
<pre><code>##       department worker_type m_fte f_fte t_fte  m_hc  f_hc  t_hc
##  [1,]       TRUE        TRUE FALSE FALSE FALSE FALSE FALSE FALSE
##  [2,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [3,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [4,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [5,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [6,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [7,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [8,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [9,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [10,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [11,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [12,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [13,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [14,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [15,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [16,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [17,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [18,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [19,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [20,]       TRUE        TRUE FALSE FALSE FALSE FALSE FALSE  TRUE
## [21,]       TRUE        TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE
## [22,]       TRUE        TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE
## [23,]       TRUE        TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE
## [24,]       TRUE        TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
## [25,]       TRUE        TRUE FALSE FALSE FALSE FALSE FALSE FALSE
## [26,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [27,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [28,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [29,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [30,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [31,]       TRUE        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE</code></pre>
<p>
Let’s see where all the <code>FALSE</code>s are:
</p>
<pre class="r"><code>arrange(table_1, worker_type)</code></pre>
<pre><code>##    department worker_type m_fte  f_fte  t_fte m_hc f_hc t_hc
## 1       Total           - 101.5 195.40 316.90  129  216  345
## 2       Dep A  Assistants   0.0   0.00   0.00    0    0    0
## 3       Dep B  Assistants   0.0   0.00   0.00    0    0    0
## 4       Dep C  Assistants   0.0   0.00   0.00    0    0    0
## 5       Dep D  Assistants   1.0   0.00   1.60    1    0    1
## 6       Dep E  Assistants   0.0   0.00   0.00    0    0    0
## 7       Dep F  Assistants   0.0   0.00   0.00    0    0    0
## 8       Dep A   Directors   0.2   0.00   0.20    1    0    1
## 9       Dep B   Directors   0.2   0.20   0.40    1    1    2
## 10      Dep C   Directors   0.0   0.00   0.00    0    0    0
## 11      Dep D   Directors   0.4   0.00   0.40    1    0    1
## 12      Dep E   Directors   0.0   0.00   0.00    0    0    0
## 13      Dep F   Directors   0.2   0.00   0.20    1    0    1
## 14      Dep A    Managers   3.8  19.90  15.70    4   19   19
## 15      Dep B    Managers   3.0  13.50  16.50    3   15   18
## 16      Dep C    Managers   1.0   8.95   9.95    1   11   12
## 17      Dep D    Managers   0.5   1.60   2.10    1    2    3
## 18      Dep E    Managers  16.0  38.20  54.20   17   42   59
## 19      Dep F    Managers   0.0   0.00   0.00    0    0    0
## 20      Dep A       Total  26.3  45.30  71.60   28   51   79
## 21      Dep B       Total  35.0  53.30  98.30   36   57   93
## 22      Dep C       Total  20.0  33.15  53.15   21   37   58
## 23      Dep D       Total   9.4   6.60  16.00   11    7   18
## 24      Dep E       Total  29.8  51.95  81.75   31   58   89
## 25      Dep F       Total   1.0   1.00   0.20    1    1    1
## 26      Dep A     Workers  12.3  33.40  55.70   13   36   59
## 27      Dep B     Workers  31.8  39.60  71.40   32   41   73
## 28      Dep C     Workers  19.0  24.20  43.20   20   26   46
## 29      Dep D     Workers   7.5   5.00  12.50    8    5   13
## 30      Dep E     Workers  11.8  13.75  27.55   14   16   30
## 31      Dep F     Workers   0.2   0.00   0.20    1    0    1</code></pre>
<p>
We see that the totals for department A and F are all wrong, and some others for other departments as well. Obviously the grand total is this completely wrong!
</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">
Conclusion
</h2>
<p>
If this looked complicated, let me assure you that, yes, it was. That’s quite typical with tasks like these: if the data is not in a tidy format, you really have to type a lot of code to make it tidy. But the advantage now is that I could put all this code into two functions, and apply them to as many tables as I need. This is what I did, and what I will be doing in the future as well. Now that the code is written, I can simply keep applying it to future reports that use the same table format.
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>proramming</category>
  <guid>https://b-rodrigues.github.io/posts/2023-03-18-docxtractr.html</guid>
  <pubDate>Sat, 18 Mar 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Software engineering techniques that non-programmers who write a lot of code can benefit from — the DRY WIT approach</title>
  <link>https://b-rodrigues.github.io/posts/2023-03-07-dry_wit.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/archetypical_data_scientist.png" width="80%" height="auto">
</p>
</div>
<p>
Data scientists, statisticians, analysts, researchers, and many other professionals write <em>a lot of code</em>.
</p>
<p>
Not only do they write a lot of code, but they must also read and review a lot of code as well. They either work in teams and need to review each other’s code, or need to be able to reproduce results from past projects, be it for peer review or auditing purposes. And yet, they never, or very rarely, get taught the tools and techniques that would make the process of writing, collaborating, reviewing and reproducing projects possible.
</p>
<p>
Which is truly unfortunate because software engineers face the same challenges and solved them decades ago. Software engineers developed a set of project management techniques and tools that non-programmers who write a lot of code could benefit from as well.
</p>
<p>
These tools and techniques can be used right from the start of a project at a minimal cost, such that the analysis is well-tested, well-documented, trustworthy and reproducible <em>by design</em>. Projects are going to be reproducible simply because they were engineered, from the start, to be reproducible.
</p>
<p>
But all these tools, frameworks and techniques boil down to two acronyms that I like to keep in my head at all times:
</p>
<ul>
<li>
DRY: Don’t Repeat Yourself;
</li>
<li>
WIT: Write IT down.
</li>
</ul>
<p>
DRY WIT: by systematically avoiding not to repeat yourself and by writing everything down, projects become well-tested, well-documented, trustworthy and reproducible by design. Why is that?
</p>
<section id="dry-dont-repeat-yourself" class="level2">
<h2 class="anchored" data-anchor-id="dry-dont-repeat-yourself">
DRY: Don’t Repeat Yourself
</h2>
<p>
Let’s start with DRY: what does it mean not having to repeat oneself? It means:
</p>
<ul>
<li>
using functions instead of copy-and-pasting bits of code here and there;
</li>
<li>
using literate programming, to avoid having to copy and paste graphs and tables into word or pdf documents;
</li>
<li>
treating code as data and making use of templating.
</li>
</ul>
<p>
The most widely used programming languages for data science/statistics, Python and R, both have first-class functions. This means that functions can be manipulated like any other object. So something like:
</p>
<pre class="r"><code>Reduce(`+`, seq(1:100))</code></pre>
<pre><code>## [1] 5050</code></pre>
<p>
where the function <code>+</code>() gets used as an argument of the higher-order <code>Reduce()</code> function is absolutely valid (and so is Python’s equivalent <code>reduce</code> from <code>functools</code>) and avoids having to use a for-loop which can lead to other issues. Generally speaking, the functional programming paradigm lends itself very naturally to data analysis tasks, and in my opinion data scientists and statisticians would benefit a lot from adopting this paradigm.
</p>
<p>
Literate programming is another tool that needs to be in the toolbox of any person analysing data. This is because at the end of the day, the results of an analysis need to be in some form of document. Without literate programming, this is how you would draft reports:
</p>
<div style="text-align:center;">
<p>
<img src="https://github.com/b-rodrigues/rap4all/blob/master/images/report_draft_loop.png?raw=true" width="100%">
</p>
</div>
<p>
But with literate programming, this is how this loop would look like:
</p>
<div style="text-align:center;">
<p>
<img src="https://github.com/b-rodrigues/rap4all/blob/master/images/md_draft_loop.png?raw=true" width="100%">
</p>
</div>
<p>
<a href="https://quarto.org/">Quarto</a> is the latest open-source scientific and technical publishing system that leverages Pandoc and supports R, Python, Julia and ObservableJs right out of the box.
</p>
<p>
Below is a little Quarto Hello World:
</p>
<pre class="default"><code>---
output: pdf

---

In this example we embed parts of the examples from the
\texttt{kruskal.test} help page into a LaTeX document:



::: {.cell}

```{.r .cell-code}
data (airquality)
kruskal.test(Ozone ~ Month, data = airquality)
```

::: {.cell-output .cell-output-stdout}

```

    Kruskal-Wallis rank sum test

data:  Ozone by Month
Kruskal-Wallis chi-squared = 29.267, df = 4, p-value = 6.901e-06
```


:::
:::



which shows that the location parameter of the Ozone
distribution varies significantly from month to month.
Finally we include a boxplot of the data:



::: {.cell}
::: {.cell-output-display}
![](2023-03-07-dry_wit_files/figure-html/unnamed-chunk-2-1.png){width=672}
:::
:::


</code></pre>
<p>
Compiling this document results in the following:
</p>
<div style="text-align:center;">
<img src="https://raw.githubusercontent.com/b-rodrigues/rap4all/master/images/hello_rmd.PNG" width="100%">
<figcaption>
Example from Leisch’s 2002 paper.
</figcaption>
</div>
<p>
Of course, you could use Python code chunks instead of R, you could also compile this document to Word, or HTML, or anything else really. By combining code and prose, the process of data analysis gets streamlined and we don’t need to repeat ourselves copy and pasting images and tables into Word documents.
</p>
<p>
Finally, treating code as data is also quite useful. This means that it is possible to compute on the language itself. This is a more advanced topic, but definitely worth the effort. As an illustration, consider the following R toy example:
</p>
<pre class="r"><code>show_and_eval &lt;- function(f, ...){
  f &lt;- deparse(substitute(f))
  dots &lt;- list(...)
  message("Evaluating: ", f, "() with arguments: ", deparse(dots))
  do.call(f, dots)
}</code></pre>
<p>
Running this function does the following:
</p>
<pre class="r"><code>show_and_eval(sqrt, 2)</code></pre>
<pre><code>## Evaluating: sqrt() with arguments: list(2)</code></pre>
<pre><code>## [1] 1.414214</code></pre>
<pre class="r"><code>show_and_eval(mean, x = c(NA, 1, 2))</code></pre>
<pre><code>## Evaluating: mean() with arguments: list(x = c(NA, 1, 2))</code></pre>
<pre><code>## [1] NA</code></pre>
<pre class="r"><code>show_and_eval(mean, x = c(NA, 1, 2), na.rm = TRUE)</code></pre>
<pre><code>## Evaluating: mean() with arguments: list(x = c(NA, 1, 2), na.rm = TRUE)</code></pre>
<pre><code>## [1] 1.5</code></pre>
<p>
This is incredibly useful when writing packages (to know more about these techniques in the R programming language, read the chapter <em>Metaprogramming</em> from <a href="https://adv-r.hadley.nz/metaprogramming.html">Advanced R</a>).
</p>
</section>
<section id="wit-write-it-down" class="level2">
<h2 class="anchored" data-anchor-id="wit-write-it-down">
WIT: Write IT down
</h2>
<p>
Now on the WIT bit: <em>write it down</em>. You’ve just written a function. To see if it works correctly, you test it in the interactive console. You execute the test, see that it works, and move on. But wait! What you just did is called a unit test. Instead of writing that in the console and then never use it ever again, write it down in a script. Now you’ve got a unit test for that function that you can execute each time you update that function’s code, and make sure that it keeps working as expected. There are many unit testing frameworks that can help you how to write unit tests consistently and run them automatically.
</p>
<p>
Documentation: write it down! How does the function work? What are its inputs? Its outputs? What else should the user know to make it work? Very often, documentation is but a series of comments in your scripts. That’s already nice, but using literate programming, you could also turn these comments into proper documentation. You could use <em>docstrings</em> in Python or <code>{roxygen2}</code> style comments in R.
</p>
<p>
Another classic: you correct some data manually in the raw dataset (very often a <code>.csv</code> or <code>.xlsx</code> file). For example, when dealing with data on people, sex is sometimes “M” or “F”, sometimes “Male” or “Female”, sometimes “1” or “0”. You spot a couple of inconsistencies and decide to <em>quickly</em> correct them by hand. Maybe only 3 men were coded as “Male” so you simply erase the “ale” and go on with your project. Stop!
</p>
<p>
Write IT down!
</p>
<p>
Write a couple of lines of code that does the replacement for you. Not only will this leave a trace, it will ensure that when you get an update to that data in the future you don’t have to remember to have to change it by hand.
</p>
<p>
You should aim at completely eliminating any required manual intervention when building your project. A project that can be fully run by a machine is easier to debug, its execution can be scheduled and can be iterated over very quickly.
</p>
<p>
Something else that you should write down, or rather, let another tool do it for you: how you collaborate with your teammates. For this, you should be using Git. Who changed what part of what function when? If the project’s code is versioned, Git writes it down for you. You want to experiment with a new feature? Write it down by creating a new branch and going nuts. There’s something wrong in the code? Write it down as an issue on your versioning platform (usually Github).
</p>
<p>
There are many more topics that us disciplines of the data could learn from software engineers. I’m currently working on a free ebook that you can read <a href="https://raps-with-r.dev/">here</a> that teaches these techniques. If this post opened your appetite, give the book a go!
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>proramming</category>
  <guid>https://b-rodrigues.github.io/posts/2023-03-07-dry_wit.html</guid>
  <pubDate>Tue, 07 Mar 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>What I’ve learned making an .epub Ebook with Quarto</title>
  <link>https://b-rodrigues.github.io/posts/2023-03-03-quarto_books.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/epubcheck.png" width="50%">
</p>
</div>
<p>
I’ve been working on an ebook (that you can read over <a href="https://raps-with-r.dev/">here</a>) made using Quarto. Since I’m also selling a DRM-free Epub and PDF on <a href="https://leanpub.com/raps-with-r/">Leanpub</a> I wanted to share some tips and tricks I’ve learned to generate an Epub that passes <code>epubcheck</code> using Quarto.
</p>
<div style="text-align:center;">
<p>
<iframe width="160" height="400" src="https://leanpub.com/raps-with-r/embed" frameborder="0" allowtransparency="true">
</iframe>
</p>
</div>
<p>
Quarto is a tool made by Posit and is an <em>open-source scientific and technical</em> publishing tool. If you know what LaTeX is, then it should be easy for you to grok Quarto. The idea of Quarto is that you write documents using Markdown, and then compile these source files into either PDFs, Word documents, but also books, web-sites, ebooks (in the Epub format) and so on… It’s quite powerful, and you can also use programming language code chunks for literate programming. Quarto support R, Python, Julia and ObsevableJS chunks.
</p>
<p>
So, as I said, I’ve been using Quarto to write an ebook, and from a single set of Markdown source files I can generate the website (linked above), the PDF of the book and the Epub of the book. But you see, if you want to sell an Epub on platforms like Leanpub, the generated Epub must pass <code>epubcheck</code>. <code>epubcheck</code> is a command line application that verifies that your Epub satisfies certain quality checks. If these quality standards are not satisfied, there is no guarantee that Epub readers can successfully open your Epub. Leanpub actually allows you to upload an Epub that does not pass <code>epubcheck</code>, but they warn you that you really should strive for publishing an Epub without any errors or warnings raised by <code>epubcheck</code>. For example, the first version of my Epub did not pass <code>epubcheck</code> and I couldn’t upload it to my Kindle.
</p>
<p>
In this blog post I’ll show you what you should do to generate an Epub that passes <code>epubcheck</code> using Quarto.
</p>
<section id="starting-from-the-default-template" class="level2">
<h2 class="anchored" data-anchor-id="starting-from-the-default-template">
Starting from the default template
</h2>
<p>
Start by installing Quarto by downloading the right package for your operating system <a href="https://quarto.org/docs/get-started/">here</a>. To start from a book template open a terminal and run:
</p>
<pre><code>quarto create-project example_book --type book</code></pre>
<p>
Let’s open the <code>_quarto.yml</code> file that is inside the newly created <code>example_book/</code>. This is your book’s configuration file. It should look like this:
</p>
<pre><code>project:
  type: book

book:
  title: "example_book"
  author: "Jane Doe"
  date: "3/3/2023"
  chapters:
    - index.qmd
    - intro.qmd
    - summary.qmd
    - references.qmd

bibliography: references.bib

format:
  html:
    theme: cosmo
  pdf:
    documentclass: scrreprt</code></pre>
<p>
You can change whatever you like, but for our purposes, we are going to add the <code>epub</code> output format all the way at the bottom of the file. So change these lines:
</p>
<pre><code>format:
  html:
    theme: cosmo
  pdf:
    documentclass: scrreprt</code></pre>
<p>
into these lines:
</p>
<pre><code>format:
  html:
    theme: cosmo
  epub:
    toc: true</code></pre>
<p>
I’ve added the <code>epub</code> format as an output, as well as the <code>toc: true</code> option, which builds a table of contents. I’ve also removed the <code>pdf</code> output because you need to have a LaTeX distribution installed for this, and the point of this blog post is not to talk about the PDF output (which works flawlessly by the way). Before compiling, let’s open one of the <code>.qmd</code> files. These files are the Markdown source files that we need to edit in order to fill our book with content. Let’s open <code>intro.qmd</code> and change these lines from:
</p>
<pre><code># Introduction

This is a book created from markdown and executable code.

See @knuth84 for additional discussion of literate programming.</code></pre>
<p>
to:
</p>
<pre><code># Introduction

This is a book created from markdown and executable code.

See @knuth84 for additional discussion of literate programming.

![By Boaworm - Own work, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=10649477](images/640px-Eyjafjallajokull_Gigjokull_in_ash.jpg)
</code></pre>
<p>
Download the image from this <a href="https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Eyjafjallajokull_Gigjokull_in_ash.jpg/640px-Eyjafjallajokull_Gigjokull_in_ash.jpg?download">link</a> and create the <code>images/</code> folder at the root of the book, right next to the <code>.qmd</code> files.
</p>
<p>
This syntax is the default syntax for adding pictures in a Markdown document. If you’re an R user, you could also use an R code chunk and the function <code>knitr::include_graphics()</code> to achieve the same thing.
</p>
<p>
Let’s compile our little example book, and then use <code>epubcheck</code> to see what’s wrong! Use these commands to render the book in all the formats:
</p>
<pre><code>cd example_book/

quarto render</code></pre>
<p>
You should see a folder called <code>_book</code> appear on the root of your project. Inside this folder, you will see a bunch of <code>.html</code> files: these constitute the web-site of your book. You can right click on <code>index.html</code> and open it with a web browser and see how your book, as a web-site, looks like. You could host this book on Github pages for free!
</p>
<p>
But what interests us is the <code>.epub</code> file. If your PDF reader supports this format, you can open it and see how it looks. On Windows you could use SumatraPDF. I use Okular on Linux to open PDF and Epub documents. Anyways, there doesn’t seem to be anything wrong with it. You can open it, you can read it, it seems to be working just fine. But let’s see if <code>epubcheck</code> thinks the same. You can download <code>epubcheck</code> from <a href="https://www.w3.org/publishing/epubcheck/">here</a>. Save the downloaded file on the root directory of the book and decompress it. Inside the decompressed folder, you’ll see a file called <code>epubcheck.jar</code>. Put your epub file right next to it, in the same folder. Now, open a terminal and navigate to the right folder and run the following command to check the epub file:
</p>
<pre><code>cd epubcheck-5.0.0 # or whatever version it is you downloaded

java -jar epubcheck.jar example_book.epub</code></pre>
<p>
You should see this output:
</p>
<pre><code>Validating using EPUB version 3.3 rules.
ERROR(RSC-005): example_book.epub/EPUB/content.opf(6,39): Error while parsing file: character content of element "dc:date" invalid; must 
be a string with length at least 1 (actual length was 0)
WARNING(OPF-053): example_book.epub/EPUB/content.opf(6,39): Date value "" does not follow recommended syntax as per http://www.w3.org/TR/NOTE-datetime:zero-length string.
ERROR(RSC-005): example_book.epub/EPUB/text/ch002.xhtml(354,16): Error while parsing file: element "figcaption" not allowed here; expected the element end-tag, text, element "a", "abbr", "area", "audio", "b", "bdi", "bdo", "br", "button", "canvas", "cite", "code", "data", "datalist", "del", "dfn", "em", "embed", "epub:switch", "i", "iframe", "img", "input", "ins", "kbd", "label", "link", "map", "mark", "meta", "meter", "ns1:math", "ns2:svg", "object", "output", "picture", "progress", "q", "ruby", "s", "samp", "script", "select", "small", "span", "strong", "sub", "sup", "template", "textarea", "time", "u", "var", "video" or "wbr" (with xmlns:ns1="http://www.w3.org/1998/Math/MathML" xmlns:ns2="http://www.w3.org/2000/svg") or an element from another namespace

Check finished with errors
Messages: 0 fatals / 2 errors / 1 warning / 0 infos

EPUBCheck completed</code></pre>
<p>
So we get 2 errors and 1 warning! Let’s look at the first error:
</p>
<pre><code>ERROR(RSC-005): example_book.epub/EPUB/content.opf(6,39): Error while parsing file: character content of element "dc:date" invalid; must 
be a string with length at least 1 (actual length was 0)</code></pre>
<p>
The first error message states that our epub does not have a valid <code>dc:date</code> attribute. The warning is also related to this. We can correct this by adding this attribute in the <code>_quarto.yml</code> file:
</p>
<pre><code>format:
  epub:
    toc:
      true
    date:
      "2023-03-01"</code></pre>
<p>
However this is not enough. There is a bug in the current release of Quarto that prevents this from working, even though we did what we should. However, this bug <a href="https://github.com/quarto-dev/quarto-cli/issues/4615#issuecomment-1453921865">is already corrected in the development version of the next release</a>. But until the next version of Quarto, 1.3, gets released, here is the workaround; you need to also specify the language of the book:
</p>
<pre><code>format:
  html:
    theme: cosmo
  epub:
    toc:
      true
    lang:
      en-GB
    date:
      "2023-03-01"</code></pre>
<p>
And now <code>epubcheck</code> does not complain about the date anymore!
</p>
<p>
The next error:
</p>
<pre><code>ERROR(RSC-005): example_book.epub/EPUB/text/ch002.xhtml(354,16): Error while parsing file: element "figcaption" not allowed here; expected the element end-tag, text, element "a", "abbr", "area", "audio", "b", "bdi", "bdo", "br", "button", "canvas", "cite", "code", "data", "datalist", "del", "dfn", "em", "embed", "epub:switch", "i", "iframe", "img", "input", "ins", "kbd", "label", "link", "map", "mark", "meta", "meter", "ns1:math", "ns2:svg", "object", "output", "picture", "progress", "q", "ruby", "s", "samp", "script", "select", "small", "span", "strong", "sub", "sup", "template", "textarea", "time", "u", "var", "video" or "wbr" (with xmlns:ns1="http://www.w3.org/1998/Math/MathML" xmlns:ns2="http://www.w3.org/2000/svg") or an element from another namespace</code></pre>
<p>
is related to the image. It turns out that including the image like we did generates code that is not quite correct from the point of view of the standard that Epubs should follow. You should know that Epubs are actually a collection of HTML files, so you can include images by using HTML code in the source Markdown files.
</p>
<p>
If you insert the image like so, the error should disappear:
</p>
<pre><code>&lt;figure&gt;
    &lt;img src="images/640px-Eyjafjallajokull_Gigjokull_in_ash.jpg"
         alt="By Boaworm - Own work, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=10649477"&gt;&lt;/img&gt;
    &lt;figcaption&gt;By Boaworm - Own work, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=10649477&lt;/figcaption&gt;
&lt;/figure&gt;</code></pre>
<p>
If you re-render the Epub, and try <code>epubcheck</code> again, you should see this:
</p>
<pre><code>java -jar epubcheck.jar example_book.epub</code></pre>
<pre><code>Validating using EPUB version 3.3 rules.
No errors or warnings detected.
Messages: 0 fatals / 0 errors / 0 warnings / 0 infos</code></pre>
</section>
<section id="using-github-actions-to-build-the-book" class="level2">
<h2 class="anchored" data-anchor-id="using-github-actions-to-build-the-book">
Using Github Actions to build the book
</h2>
<p>
Finally, as a bonus, if you’re using Github, you can also use Github Actions to generate the web-site, as well as the Epub (and the PDF if you want). If you go to <a href="https://github.com/b-rodrigues/epubcheck_quarto_test">this repository</a>, which contains the example book from this post, you can find the workflow to automatically build the Epub and web-site from your Quarto source in the <code>.github/workflows/</code> folder. Create the same folder structure in your repository and copy the <code>.yml</code> file that is in it to these folders. You should then create a <code>gh-pages</code> branch and make sure that Github Actions has the required permissions to push. For this, go to the <em>Settings</em> menu of your repository, then <em>Actions</em> (listed in the menu on the left), then <em>General</em>, and then under <em>Workflow permissions</em> make sure that <em>Read and write permissions</em> is checked.
</p>
<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/ga_permissions.png" width="100%">
</p>
</div>
<p>
Now, each time you push, you should see your Epub get built in the <code>gh-pages</code> branch! If you use R code chunks, you also need to set up an action to set up R. Take a look at the <a href="https://github.com/b-rodrigues/rap4all/blob/master/.github/workflows/quarto-publish.yml">repo</a> of my book for an example.
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>nix</category>
  <guid>https://b-rodrigues.github.io/posts/2023-03-03-quarto_books.html</guid>
  <pubDate>Fri, 03 Mar 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>MRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?</title>
  <link>https://b-rodrigues.github.io/posts/2023-01-12-repro_R.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/sisyphus.jpg" width="80%" height="auto">
</p>
</div>
<section id="you-expect-me-to-read-this-long-ass-blog-post" class="level2">
<h2 class="anchored" data-anchor-id="you-expect-me-to-read-this-long-ass-blog-post">
You expect me to read this long ass blog post?
</h2>
<p>
<em>If you’re too busy to read this blog post, know that I respect your time. The table below summarizes this blog post:</em>
</p>
<table class="table">
<colgroup>
<col width="70%">
<col width="29%">
</colgroup>
<thead>
<tr class="header">
<th>
Need
</th>
<th>
Solution
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>
I want to start a project and make it reproducible.
</td>
<td>
{renv} and Docker
</td>
</tr>
<tr class="even">
<td>
There’s an old script laying around that I want to run.
</td>
<td>
{groundhog} and Docker
</td>
</tr>
<tr class="odd">
<td>
I want to work inside an environment that
</td>
<td>
Docker and the Posit
</td>
</tr>
<tr class="even">
<td>
enables me to run code in a reproducible way.
</td>
<td>
CRAN mirror.
</td>
</tr>
</tbody>
</table>
<p>
<em>But this table doesn’t show the whole picture, especially the issues with relying so much on Docker. So if you’re interesting in making science and your work reproducible and robust, my advice is that you stop doomscrolling on social media and keep on reading. If at the end of the blog post you think that this was a waste of time, just sent an insult my way, that’s ok.</em>
</p>
<p>
I learnt last week that MRAN is going to get shutdown this year. For those of you that don’t know, MRAN was a CRAN mirror managed by Microsoft. What made MRAN stand out was the fact that Microsoft took daily snapshots of CRAN and it was thus possible to quite easily install old packages using the <code>{checkpoint}</code> package. This was a good thing for reproducibility, and for Windows and macOS, it was even possible to install binary packages, so no need to compile them from source.
</p>
<p>
Last year I had the opportunity to teach a course on building reproducible analytical pipelines at the University of Luxembourg, and made my teaching material available as an ebook that you can find <a href="https://rap4mads.eu/">here</a>. I also wrote some blog posts about reproducibility and it looks like I will be continuing this trend for the forseeable future.
</p>
<p>
So in this blog post I’m going to show what you, as an R user, can do to make your code reproducible now that MRAN is getting shutdown. MRAN is not the only option for reproducibility, and I’m going to present in this blog post everything I know about other options. So if you happen to know of some solution that I don’t discuss here, please leave a comment <a href="https://github.com/rbind/b-rodrigues.github.com/issues/5">here</a>. But this blog post is not just a tutorial. I will also discuss what I think is a major risk that is coming and what, maybe, we can do to avoid it.
</p>
</section>
<section id="reproducibility-is-on-a-continuum" class="level2">
<h2 class="anchored" data-anchor-id="reproducibility-is-on-a-continuum">
Reproducibility is on a continuum
</h2>
<p>
Reproducibility is on a continuum, and depending on the nature of your project different tools are needed. First, let’s understand what I mean by “reproducibility is on a continuum”. Let’s suppose that you have written a script that produces some output. Here is the list of everything that can influence the output (other than the mathematical algorithm running under the hood):
</p>
<ul>
<li>
Version of the programming language used;
</li>
<li>
Versions of the packages/libraries of said programming language used;
</li>
<li>
Operating System, and its version;
</li>
<li>
Versions of the underlying system libraries (which often go hand in hand with OS version, but not necessarily).
</li>
<li>
Hardware that you run all that software stack on.
</li>
</ul>
<p>
So by “reproducibility is on a continuum”, what I mean is that you could set up your project in a way that none, one, two, three, four or all of the preceding items are taken into consideration when making your project reproducible.
</p>
<p>
There is, however, something else to consider. Before, I wrote “let’s suppose you have written a script”, which means that you actually have a script laying around that was written in a particular programming language, and which makes use of a known set of packages/libraries. So for example, if my script uses the following R packages:
</p>
<ul>
<li>
dplyr
</li>
<li>
tidyr
</li>
<li>
ggplot2
</li>
</ul>
<p>
I, obviously, know this list and if I want to make my script reproducible, I should take note of the versions of these 3 packages (and potentially of their own dependencies). However, what if you don’t know this set of packages that are used? This happens when you want to set up an environment that is frozen, and then distribute this environment. Developers will then all work on the same base environment, but you cannot possibly list all the packages that are going to be used because you have no idea what the developers will end up using (and remember that future you is included in these developers, and you should always try to be nice to future you).
</p>
<p>
So these means that we have two scenarios:
</p>
<ul>
<li>
Scenario 1: I have a script (or several), and want to make sure that it will always produce the same output;
</li>
<li>
Scenario 2: I don’t know what I (or my colleagues) will develop, but we want to use the same environment across the organization to develop and deploy data products.
</li>
</ul>
<p>
Turns out that the solutions to these two scenarios are different, but available in R, even though we won’t soon be able to count on MRAN anymore. HOWEVER! R won’t suffice for this job.
</p>
</section>
<section id="scenario-1-making-a-script-reproducible" class="level2">
<h2 class="anchored" data-anchor-id="scenario-1-making-a-script-reproducible">
Scenario 1: making a script reproducible
</h2>
<p>
Ok, so let’s suppose that I want to make a script reproducible, or at least increase the probability that others (including future me) will be able to run this script and get the exact same output as I originally did. If you want to minimize the amount of time spent doing it, the minimum thing you should do is use <a href="https://rstudio.github.io/renv/articles/renv.html">{renv}</a>. <code>{renv}</code> allows you to create a file called <code>renv.lock</code>. You can then distribute this file to others alongside the code of your project, or the paper, data, whatever. Nothing else is required from you; if people have this file, they should be able to set up an environment that would closely mimic the one that was used to get the results originally (but ideally, you’d invest a bit more time to make your script run anywhere, for example by avoiding setting absolute paths that only exist on your machine).
</p>
<p>
Let’s take a look at such an <code>renv.lock</code> file:
</p>
<details>
<summary>
Click me to see lock file
</summary>
<pre><code>{
  "R": {
    "Version": "4.2.1",
    "Repositories": [
      {
        "Name": "CRAN",
        "URL": "http://cran.rstudio.com"
      }
    ]
  },
  "Packages": {
    "MASS": {
      "Package": "MASS",
      "Version": "7.3-58.1",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "762e1804143a332333c054759f89a706",
      "Requirements": []
    },
    "Matrix": {
      "Package": "Matrix",
      "Version": "1.5-1",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "539dc0c0c05636812f1080f473d2c177",
      "Requirements": [
        "lattice"
      ]
    },
    "R6": {
      "Package": "R6",
      "Version": "2.5.1",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "470851b6d5d0ac559e9d01bb352b4021",
      "Requirements": []
    },
    "RColorBrewer": {
      "Package": "RColorBrewer",
      "Version": "1.1-3",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "45f0398006e83a5b10b72a90663d8d8c",
      "Requirements": []
    },
    "cli": {
      "Package": "cli",
      "Version": "3.4.1",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "0d297d01734d2bcea40197bd4971a764",
      "Requirements": []
    },
    "colorspace": {
      "Package": "colorspace",
      "Version": "2.0-3",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "bb4341986bc8b914f0f0acf2e4a3f2f7",
      "Requirements": []
    },
    "digest": {
      "Package": "digest",
      "Version": "0.6.29",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "cf6b206a045a684728c3267ef7596190",
      "Requirements": []
    },
    "fansi": {
      "Package": "fansi",
      "Version": "1.0.3",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "83a8afdbe71839506baa9f90eebad7ec",
      "Requirements": []
    },
    "farver": {
      "Package": "farver",
      "Version": "2.1.1",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "8106d78941f34855c440ddb946b8f7a5",
      "Requirements": []
    },
    "ggplot2": {
      "Package": "ggplot2",
      "Version": "3.3.6",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "0fb26d0674c82705c6b701d1a61e02ea",
      "Requirements": [
        "MASS",
        "digest",
        "glue",
        "gtable",
        "isoband",
        "mgcv",
        "rlang",
        "scales",
        "tibble",
        "withr"
      ]
    },
    "glue": {
      "Package": "glue",
      "Version": "1.6.2",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "4f2596dfb05dac67b9dc558e5c6fba2e",
      "Requirements": []
    },
    "gtable": {
      "Package": "gtable",
      "Version": "0.3.1",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "36b4265fb818f6a342bed217549cd896",
      "Requirements": []
    },
    "isoband": {
      "Package": "isoband",
      "Version": "0.2.5",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "7ab57a6de7f48a8dc84910d1eca42883",
      "Requirements": []
    },
    "labeling": {
      "Package": "labeling",
      "Version": "0.4.2",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "3d5108641f47470611a32d0bdf357a72",
      "Requirements": []
    },
    "lattice": {
      "Package": "lattice",
      "Version": "0.20-45",
      "Source": "Repository",
      "Repository": "CRAN",
      "Hash": "b64cdbb2b340437c4ee047a1f4c4377b",
      "Requirements": []
    },
    "lifecycle": {
      "Package": "lifecycle",
      "Version": "1.0.3",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "001cecbeac1cff9301bdc3775ee46a86",
      "Requirements": [
        "cli",
        "glue",
        "rlang"
      ]
    },
    "magrittr": {
      "Package": "magrittr",
      "Version": "2.0.3",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "7ce2733a9826b3aeb1775d56fd305472",
      "Requirements": []
    },
    "mgcv": {
      "Package": "mgcv",
      "Version": "1.8-40",
      "Source": "Repository",
      "Repository": "CRAN",
      "Hash": "c6b2fdb18cf68ab613bd564363e1ba0d",
      "Requirements": [
        "Matrix",
        "nlme"
      ]
    },
    "munsell": {
      "Package": "munsell",
      "Version": "0.5.0",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "6dfe8bf774944bd5595785e3229d8771",
      "Requirements": [
        "colorspace"
      ]
    },
    "nlme": {
      "Package": "nlme",
      "Version": "3.1-159",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "4a0b3a68f846cb999ff0e8e519524fbb",
      "Requirements": [
        "lattice"
      ]
    },
    "pillar": {
      "Package": "pillar",
      "Version": "1.8.1",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "f2316df30902c81729ae9de95ad5a608",
      "Requirements": [
        "cli",
        "fansi",
        "glue",
        "lifecycle",
        "rlang",
        "utf8",
        "vctrs"
      ]
    },
    "pkgconfig": {
      "Package": "pkgconfig",
      "Version": "2.0.3",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "01f28d4278f15c76cddbea05899c5d6f",
      "Requirements": []
    },
    "purrr": {
      "Package": "purrr",
      "Version": "0.3.5",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "54842a2443c76267152eface28d9e90a",
      "Requirements": [
        "magrittr",
        "rlang"
      ]
    },
    "renv": {
      "Package": "renv",
      "Version": "0.16.0",
      "Source": "Repository",
      "Repository": "CRAN",
      "Hash": "c9e8442ab69bc21c9697ecf856c1e6c7",
      "Requirements": []
    },
    "rlang": {
      "Package": "rlang",
      "Version": "1.0.6",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "4ed1f8336c8d52c3e750adcdc57228a7",
      "Requirements": []
    },
    "scales": {
      "Package": "scales",
      "Version": "1.2.1",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "906cb23d2f1c5680b8ce439b44c6fa63",
      "Requirements": [
        "R6",
        "RColorBrewer",
        "farver",
        "labeling",
        "lifecycle",
        "munsell",
        "rlang",
        "viridisLite"
      ]
    },
    "tibble": {
      "Package": "tibble",
      "Version": "3.1.8",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "56b6934ef0f8c68225949a8672fe1a8f",
      "Requirements": [
        "fansi",
        "lifecycle",
        "magrittr",
        "pillar",
        "pkgconfig",
        "rlang",
        "vctrs"
      ]
    },
    "utf8": {
      "Package": "utf8",
      "Version": "1.2.2",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "c9c462b759a5cc844ae25b5942654d13",
      "Requirements": []
    },
    "vctrs": {
      "Package": "vctrs",
      "Version": "0.5.1",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "970324f6572b4fd81db507b5d4062cb0",
      "Requirements": [
        "cli",
        "glue",
        "lifecycle",
        "rlang"
      ]
    },
    "viridisLite": {
      "Package": "viridisLite",
      "Version": "0.4.1",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "62f4b5da3e08d8e5bcba6cac15603f70",
      "Requirements": []
    },
    "withr": {
      "Package": "withr",
      "Version": "2.5.0",
      "Source": "Repository",
      "Repository": "RSPM",
      "Hash": "c0e49a9760983e81e55cdd9be92e7182",
      "Requirements": []
    }
  }
}
</code></pre>
</details>
<p>
as you can see this file lists the R version that was used as well as the different libraries for a project. Versions of the libraries are also listed, where they came from (CRAN or Github for example) and these libraries’ requirements as well. So someone who wants to run the original script in a similar environment has all the info needed to do it. <code>{renv}</code> also provides a simple way to install all of this. Simply put the <code>renv.lock</code> file in the same folder as the original script and run <code>renv::restore()</code>, and the right packages with the right versions get automagically installed (and without interfering with your system-wide, already installed library of packages). The “only” difficulty that you might have is installing the right version of R. If this is a recent enough version of R, this shouldn’t be a problem, but installing old software might be difficult. For example installing R version 2.5 might, or might not, be possible depending on your operating system (I don’t like microsoft windows, but generally speaking it is quite easy to install very old software on it, which in the case of Linux distros can be quite difficult. So I guess on windows this could work more easily). Then there’s also the system libraries that your script might need, and it might also be difficult to install these older versions. So that’s why I said that providing the <code>renv.lock</code> file is the bare minimum. But before seeing how we can deal with that, let’s discuss a scenario 1bis, which is the case where you want to run an old script (say, from 5 years ago), but there’s no <code>renv.lock</code> file to be found.
</p>
</section>
<section id="scenario-1bis-old-script-no-renv.lock-file" class="level2">
<h2 class="anchored" data-anchor-id="scenario-1bis-old-script-no-renv.lock-file">
Scenario 1bis: old script, no renv.lock file
</h2>
<p>
For these cases I would have used <code>{checkpoint}</code>, but as explained in the intro MRAN is getting shutdown, and with it out of the picture <code>{checkpoint}</code> will cease to work.
</p>
<p>
The way <code>{checkpoint}</code> worked is that you would simply add the following line at the very top of the script in question:
</p>
<pre class="r"><code>checkpoint::checkpoint("2018-12-12")</code></pre>
<p>
and this would download the packages needed for the script from that specific date and run your script. Really, really useful. But unfortunately, Microsoft decided that MRAN should get the axe. So what else is there? While researching for this blog post, I learned about <code>{groundhog}</code> which supposedly provides the same functionality. Suppose I have a script from 5 years ago that loads the following libraries:
</p>
<pre class="r"><code>library(purrr)
library(ggplot2)</code></pre>
<p>
By rewriting this like so (and installing <code>{groundhog}</code> beforehand):
</p>
<pre class="r"><code>groundhog.library("
    library(purrr)
    library(ggplot2)",
    "2017-10-04",
    tolerate.R.version = "4.2.2")</code></pre>
<p>
<code>{purrr}</code> and <code>{ggplot2}</code> get downloaded as they were on the date I provided. If you want to know what I had to use the “tolerate.R.version” option, this is because if you try to run it without it, you get the following very useful message:
</p>
<pre><code>---------------------------------------------------------------------------
|IMPORTANT.
|    Groundhog says: you are using R-4.2.2, but the version of R current 
|    for the entered date, '2017-10-04', is R-3.4.x. It is recommended 
|    that you either keep this date and switch to that version of R, or 
|    you keep the version of R you are using but switch the date to 
|    between '2022-04-22' and '2023-01-08'. 
|
|    You may bypass this R-version check by adding: 
|    `tolerate.R.version='4.2.2'`as an option in your groundhog.library() 
|    call. Please type 'OK' to confirm you have read this message. 
|   &gt;ok</code></pre>
<p>
That’s is pretty neat, as it tells you “hey, getting the right packages is good, but if your R version is not the same, you’re not guaranteed to get the same results back, and this might not even work at all”.
</p>
<p>
So, here’s what happens when I try to install these packages (on my windows laptop, as most people would do), without installing the right version of R as suggested by <code>{groundhog}</code>:
</p>
<details>
<summary>
Click me to see lock file
</summary>
<pre><code>+ Will now attempt installing 5 packages from source.

groundhog says: Installing 'magrittr_1.5', package #1 (from source) out of 5 needed
&gt; As of 16:12, the best guess is that all 5 packages will install around 16:14
trying URL 'https://packagemanager.rstudio.com/all/latest/src/contrib/Archive/magrittr/magrittr_1.5.tar.gz'
Content type 'application/x-tar' length 200957 bytes (196 KB)
downloaded 196 KB


groundhog says: Installing 'rlang_0.1.2', package #2 (from source) out of 5 needed
&gt; As of 16:12, the best guess is that all 5 packages will install around 16:14
trying URL 'https://packagemanager.rstudio.com/all/latest/src/contrib/Archive/rlang/rlang_0.1.2.tar.gz'
Content type 'application/x-tar' length 200867 bytes (196 KB)
downloaded 196 KB


groundhog says: Installing 'Rcpp_0.12.13', package #3 (from source) out of 5 needed
&gt; As of 16:13, the best guess is that all 5 packages will install around 16:14
trying URL 'https://packagemanager.rstudio.com/all/latest/src/contrib/Archive/Rcpp/Rcpp_0.12.13.tar.gz'
Content type 'application/x-tar' length 3752364 bytes (3.6 MB)
downloaded 3.6 MB

Will try again, now showing all installation output.
trying URL 'https://packagemanager.rstudio.com/all/latest/src/contrib/Archive/Rcpp/Rcpp_0.12.13.tar.gz'
Content type 'application/x-tar' length 3752364 bytes (3.6 MB)
downloaded 3.6 MB

* installing *source* package 'Rcpp' ...
** package 'Rcpp' successfully unpacked and MD5 sums checked
staged installation is only possible with locking
** using non-staged installation
** libs
g++ -std=gnu++11  -I"c:/Users/Bruno/AppData/Roaming/R-42~1.2/include" -DNDEBUG -I../inst/include/    -I"c:/rtools42/x86_64-w64-mingw32.static.posix/include"     -O2 -Wall  -mfpmath=sse -msse2 -mstackrealign  -c Date.cpp -o Date.o
In file included from ../inst/include/RcppCommon.h:67,
                 from ../inst/include/Rcpp.h:27,
                 from Date.cpp:31:
../inst/include/Rcpp/sprintf.h: In function 'std::string Rcpp::sprintf(const char*, ...)':
../inst/include/Rcpp/sprintf.h:30:12: warning: unnecessary parentheses in declaration of 'ap' [-Wparentheses]
   30 |     va_list(ap);
      |            ^~~~
../inst/include/Rcpp/sprintf.h:30:12: note: remove parentheses
   30 |     va_list(ap);
      |            ^~~~
      |            -  -
In file included from ../inst/include/Rcpp.h:77,
                 from Date.cpp:31:
../inst/include/Rcpp/Rmath.h: In function 'double R::pythag(double, double)':
../inst/include/Rcpp/Rmath.h:222:60: error: '::Rf_pythag' has not been declared; did you mean 'pythag'?
  222 |     inline double pythag(double a, double b)    { return ::Rf_pythag(a, b); }
      |                                                            ^~~~~~~~~
      |                                                            pythag
make: *** [c:/Users/Bruno/AppData/Roaming/R-42~1.2/etc/x64/Makeconf:260: Date.o] Error 1
ERROR: compilation failed for package 'Rcpp'
* removing 'C:/Users/Bruno/Documents/R_groundhog/groundhog_library/R-4.2/Rcpp_0.12.13/Rcpp'
The package 'Rcpp_0.12.13' failed to install!
groundhog says:
The package may have failed to install because you are using R-4.2.2
which is at least one major update after the date you entered '2017-10-04'.
You can try using a more recent date in your groundhog.library() command, 
or run it with the same date using 'R-3.4.4'
Instructions for running older versions of R: 
    http://groundhogr.com/many

----------------   The package purrr_0.2.3 did NOT install.  Read above for details  -----------------

Warning message:
In utils::install.packages(url, repos = NULL, lib = snowball$installation.path[k],  :
  installation of package 'C:/Users/Bruno/AppData/Local/Temp/RtmpyKYXFd/downloaded_packages/Rcpp_0.12.13.tar.gz' had non-zero exit status
&gt; </code></pre>
</details>
<p>
As you can see it failed, very likely because I don’t have the right development libraries installed on my windows laptop, due to the version mismatch that <code>{groundhog}</code> complained about. I also tried on my Linux workstation, and got the same outcome. In any case, I want to stress that this is not <code>{groundhog}</code>’s fault, but this due to the fact that I was here only concerned with packages; as I said multiple times now, reproducibility is on an continuum, and you also need to deal with OS and system libraries. So for now, we only got part of the solution.
</p>
<p>
By the way, you should know 3 more things about <code>{groundhog}</code>:
</p>
<ul>
<li>
the earliest available date is, in theory, any date. However, according to its author, <code>{groundhog}</code> should work reliably with a date as early as “2015-04-16”. That’s because the oldest R version <code>{groundhog}</code> is compatible with is R 3.2. However, again according to its author, it should be possible to patch <code>{groundhog}</code> to work with any earlier versions of R.
</li>
<li>
<code>{groundhog}</code>’s developer is <a href="https://github.com/CredibilityLab/groundhog/issues/83#issuecomment-1379932166">planning</a> to save the binary packages off MRAN so that <code>{groundhog}</code> will continue offering binary packages once MRAN is out of the picture, which will make installing these packages more reliable.
</li>
<li>
On Windows and macOS, <code>{groundhog}</code> installs binary packages if they’re available (which basically is always the case, in the example above it was not the case because I was using Posit’s CRAN mirror, and I don’t think they have binary packages for Windows that are that old. But if using another mirror, that should not be a problem). So if you install the right version of R, you’re almost guaranteed that it’s going to work. But, there is always a but, this also depends on hardware now. I’ll explain in the last part of this blog post, so read on.
</li>
</ul>
</section>
<section id="docker-to-the-rescue" class="level2">
<h2 class="anchored" data-anchor-id="docker-to-the-rescue">
Docker to the rescue
</h2>
<p>
The full solution in both scenarios involves Docker. If you are totally unfamiliar with Docker, you can imagine that Docker makes it easy to set up a Linux virtual machine and run it. In Docker, you use Dockerfiles (which are configuration files) to define Docker <em>images</em> and you can then run <em>containers</em> (your VMs, if you wish) based on that image. Inside that Dockerfile you can declare which operating system you want to use and what you want to run inside of it. For example, here’s a very simple Dockerfile that prints “Hello from Docker” on the Ubuntu operating system (a popular Linux distribution):
</p>
<pre><code>FROM ubuntu:latest

RUN echo "Hello, World!"</code></pre>
<p>
You then need to build the image as defined from this Dockerfile. (Don’t try to follow along for now with your own computer; I’ll link to resources below so that you can get started if you’re interested. What matters is that you understand <em>why</em> Docker is needed).
</p>
<p>
Building the image can be achieved by running this command where the Dockerfile is located:
</p>
<pre><code>docker build -t hello .</code></pre>
<p>
and then run a container from this image:
</p>
<pre><code>docker run --rm -d --name hello_container hello</code></pre>
<pre><code>Sending build context to Docker daemon  2.048kB
Step 1/2 : FROM ubuntu:latest
 ---&gt; 6b7dfa7e8fdb
Step 2/2 : RUN echo "Hello, World!"
 ---&gt; Running in 5dfbff5463cf
Hello, World!
Removing intermediate container 5dfbff5463cf
 ---&gt; c14004cd1801
Successfully built c14004cd1801
Successfully tagged hello:latest</code></pre>
<p>
You should see “Hello, World!” inside your terminal. Ok so this is the very basics. Now why is that useful? It turns out that there’s the so-called Rocker project, and this project provides a collection of Dockerfiles for current, but also older versions of R. So if we go back to our <code>renv.lock</code> file from before, we can see which R version was used (it was R 4.2.1) and define a new Dockerfile that builds upon the <a href="https://github.com/rocker-org/rocker-versioned2/blob/master/dockerfiles/r-ver_4.2.1.Dockerfile">Dockerfile from the Rocker project for R version 4.2.1</a>.
</p>
<p>
Let’s first start by writing a very simple script. Suppose that this is the script that we want to make reproducible:
</p>
<pre class="r"><code>library(purrr)
library(ggplot2)

data(mtcars)

myplot &lt;- ggplot(mtcars) +
  geom_line(aes(y = hp, x = mpg))

ggsave("/home/project/output/myplot.pdf", myplot)</code></pre>
<p>
First, let’s assume that I did my homework and that the <code>renv.lock</code> file from before is actually the one that was generated at the time this script was written. In that case, you could write a Dockerfile with the correct version of R and use the <code>renv.lock</code> file to install the right packages. This Dockerfile would look like this:
</p>
<pre><code>FROM rocker/r-ver:4.2.1

RUN mkdir /home/project

RUN mkdir /home/project/output

COPY renv.lock /home/project/renv.lock

COPY script.R /home/project/script.R

RUN R -e "install.packages('renv')"

RUN R -e "setwd('/home/project/');renv::restore(confirm = FALSE)"

CMD R -e "source('/home/project/script.R')"</code></pre>
<p>
We need to put the <code>renv.lock</code> file, as well as the script <code>script.R</code> in the same folder as the Dockerfile, and then build and run the image:
</p>
<pre><code># Build it with
docker build -t project .</code></pre>
<p>
run the container (and mount a volume to get the image back – don’t worry if you don’t know what volumes are, I’ll link resources at the end):
</p>
<pre><code>docker run --rm -d --name project_container -v /path/to/your/project/output:/home/project/output:rw project</code></pre>
<p>
Even if you’ve never seen a Dockerfile in your life, you likely understand what is going on here: the first line pulls a Docker image that contains R version 4.2.1 pre-installed on Ubuntu. Then, we create a directory to hold our files, we copy said files in the directory, and then run several R commands to install the packages as defined in the <code>renv.lock</code> file and run our script in an environment that not only has the right versions of the packages but also the right version of R. This script then saves the plot in a folder called <code>output/</code>, which we link to a folder also called <code>output/</code> but on our machine, so that we can then look at the generated plot (this is what <code>-v /path/to/your/project/output:/home/project/output:rw</code> does). Just as this script saves a plot, it could be doing any arbitrarily complex thing, like compiling an Rmarkdown file, running a model, etc, etc.
</p>
<p>
Here’s a short video of this process in action:
</p>
<p>
<a href="http://www.youtube.com/watch?v=E3E0yd6aFss" title="renv Docker demo"><img src="http://img.youtube.com/vi/E3E0yd6aFss/0.jpg" alt="Link to renv and Docker demo"></a>
</p>
<p>
Now, let’s do the same thing but for our scenario 1bis that relied on <code>{groundhog}</code>. Before writing the Dockerfile down, here’s how you should change the script. Add these lines at the very top:
</p>
<pre class="r"><code>groundhog::set.groundhog.folder("/home/groundhog_folder")

groundhog::groundhog.library("
    library(purrr)
    library(ggplot2)",
    "2017-10-04"
    )

data(mtcars)

myplot &lt;- ggplot(mtcars) +
  geom_line(aes(y = hp, x = mpg))

ggsave("/home/project/output/myplot.pdf", myplot)</code></pre>
<p>
I also created a new script that installs the dependencies of my script when building my Dockerfile. This way, when I run the container, nothing gets installed anymore. Here’s what this script looks like:
</p>
<pre class="r"><code>groundhog::set.groundhog.folder("/home/groundhog_folder")

groundhog::groundhog.library("
    library(purrr)
    library(ggplot2)",
    "2017-10-04"
    )</code></pre>
<p>
It’s exactly the beginning from the main script. Now here comes the Dockerfile, and this time it’s going to be a bit more complicated:
</p>
<pre><code>FROM rocker/r-ver:3.4.4

RUN echo "options(repos = c(CRAN='https://packagemanager.rstudio.com/cran/latest'), download.file.method = 'libcurl')" &gt;&gt; /usr/local/lib/R/etc/Rprofile.site

RUN mkdir /home/project

RUN mkdir /home/groundhog_folder

RUN mkdir /home/project/output

COPY script.R /home/project/script.R

COPY install_deps.R /home/project/install_deps.R

RUN R -e "install.packages('groundhog');source('/home/project/install_deps.R')"

CMD R -e "source('/home/project/script.R')"</code></pre>
<p>
As you can see from the first line, this time we’re pulling an image that comes with R 3.4.4. This is because that version of R was the current version as of 2017-10-04, the date we assumed this script was written on. Because this is now quite old, we need to add some more stuff to the Dockerfile to make it work. First, I change the repositories to the current mirror from <a href="https://packagemanager.rstudio.com/client/#/repos/2/overview">Posit</a>. This is because the repositories from this image are set to MRAN at a fixed date. This was done at the time for reproducibility, but now MRAN is getting shutdown, so we need to change the repositories or else our container will not be able to download packages. Also, <code>{groundhog}</code> will take care of installing the right package versions. Then I create the necessary folders and run the <code>install_deps.R</code> script which is the one that will install the packages. This way, the packages get installed when building the Docker image, and not when running the container, which is preferable. Finally, the main script gets run, and an output gets produced. Here’s a video showing this process:
</p>
<p>
<a href="http://www.youtube.com/watch?v=g0eG1OvXl9s" title="renv Docker demo"><img src="http://img.youtube.com/vi/g0eG1OvXl9s/0.jpg" alt="Link to the groundhog and Docker demo"></a>
</p>
<p>
Now all of this may seem complicated, and to be honest it is. Reproducibility is no easy task, but I hope that I’ve convinced you that by combining <code>{renv}</code> and Docker, or <code>{groundhog}</code> and Docker it is possible to rerun any analysis. But you do have to be familiar with these tools, and there’s also another issue by using Docker. Docker works on Windows, macOS and Linux, but the container that runs must be a Linux distribution, usually Ubuntu. But what if the original analysis was done on Windows and macOS? This can be a problem if the script relies on some Windows or macOS specific things, which even for a language available on all platforms like R can happen. For example, I’ve recently noticed that the <code>tar()</code> function in R, which is used to decompress <code>tar.gz</code> files, behaves differently on Windows than on Linux. So ideally, even if you’re running your analysis on Windows, you should then try to distribute a working Dockerfile alongside your paper (if you’re a researcher, or if you’re working in the private sector, you should do the same for each project). Of course, that is quite demanding, and you would need to learn about these tools, or hire someone to do that for you… But a natural question is then, well, “why use Docker at all? Since it’s easy to install older versions of R on Windows and macOS, wouldn’t an <code>renv.lock</code> file suffice? Or even just <code>{groundhog}</code> which is arguably even easier to use?”
</p>
<p>
Well, more on this later. I still need to discuss scenario 2 first.
</p>
</section>
<section id="scenario-2-offering-an-environment-that-is-made-for-reproducibility" class="level2">
<h2 class="anchored" data-anchor-id="scenario-2-offering-an-environment-that-is-made-for-reproducibility">
Scenario 2: offering an environment that is made for reproducibility
</h2>
<p>
Ok, so this one is easier. In this scenario, you have no idea what people are going to use, so you cannot generate an <code>renv.lock</code> file beforehand, and <code>{groundhog}</code> is of no help either, because, well, there’s no scripts yet to actually make reproducible. This is the situation I’ve had for this project that I’ve discussed at the end of last year, on <a href="https://www.brodrigues.co/blog/2022-12-21-longevity/">code longevity</a> of the R programming language. The solution is to write a Dockerfile that people can modify and run; this in turn produces some results that can then be shared. This Dockerfile pulls from another Dockerfile, and that other Dockerfile is made for reproducibility. How? Because that other Dockerfile is based on Ubuntu 22.04, compiles R 4.2.2 from source, and sets the repositories to <a href="https://packagemanager.rstudio.com/cran/__linux__/jammy/2022-11-21" class="uri">https://packagemanager.rstudio.com/cran/<strong>linux</strong>/jammy/2022-11-21</a> . This way, the packages get downloaded exactly as they were on November 21st 2022. So every time this image defined from this Dockerfile gets built, we get exactly the same environment.
</p>
<p>
It should also be noted that this solution can be used in the case of scenario 1bis. Let’s say I have a script from August 2018; by using a Docker image that ships the current version of R at that time (which should be R version 3.5.x) and Ubuntu (which at the time was 18.04, codenamed Bionic Beaver) and then using the Posit package manager at a frozen date, for example <a href="https://packagemanager.rstudio.com/cran/__linux__/bionic/2018-08-16" class="uri">https://packagemanager.rstudio.com/cran/<strong>linux</strong>/bionic/2018-08-16</a> I should be able to reproduce an environment that is close enough. However the problem is that Posit’s package manager earliest available date is Octobre 2017, so anything before that would not be possible to reproduce.
</p>
<p>
Ok, great, here are the solutions for reproducibility. But there are still problems.
</p>
</section>
<section id="a-single-point-of-failure-docker" class="level2">
<h2 class="anchored" data-anchor-id="a-single-point-of-failure-docker">
A single point of failure: Docker
</h2>
<p>
Let’s be blunt: having Docker as the common denominator in all these solutions is a problem. This is because Docker represents a single point of failure. But the problem is not Docker itself, but the infrastructure.
</p>
<p>
Let me explain: Docker is based on many different open source parts, and that’s great. There’s also Podman, which is basically a drop-in replacement (when combined with other tools) made by Red Hat, which is completely open source as well. So the risk does not come from there, because even if for some reason Docker would disappear, or get abandoned or whatever, we could still work with Podman, and it would likely be possible to create a fork from Docker.
</p>
<p>
But the issue is the infrastructure. For now, using Docker and more importantly hosting images is free for personal use, education, open source communities and small businesses. So this means that a project like Rocker likely pays nothing for hosting all the images they produce (but who knows, I may be wrong on this). And Rocker makes a lot of images. See, at the top of the Dockerfiles I’ve used in this blog post, there’s always a statement like:
</p>
<pre><code>FROM rocker/r-ver:4.2.1</code></pre>
<p>
as explained before, this states that a pre-built image that ships R version 4.2.1 on Ubuntu gets downloaded. But from where? This image gets downloaded from Docker Hub, see <a href="https://hub.docker.com/layers/rocker/r-ver/4.2.1/images/sha256-3636493af7028d899a6598ee4aabe70d231fb0ff60f61a70f8ea0ea24a51c3e6?context=explore">here</a>.
</p>
<p>
This means that you can download this pre-built image and don’t need to build it each time you need to work with it. You can simply use that as a base for your work, like the image built for reproducibility described in scenario 2. But what happens if at some point in the future Docker changes its licensing model? What if they still have a free tier, but massively limit the amount of images that get hosted for free? What if they get rid of the free tier entirely? This is a massive risk that needs to be managed in my opinion. There is the option of the Rocker project hosting the images themselves. It is possible to create your own, self-hosted Docker registry and not use Docker Hub, after all. But this is costly not only in terms of storage, but also of manpower to maintain all this. But maybe worse than that is: what if at some point in the future you cannot rebuild these images, at all? You would need to make sure that these pre-built images do not get lost. And this is already happening because of MRAN getting shutdown. In this blog post I’ve used the <code>rocker/r-ver:3.4.4</code> image to run code from 2017. The problem is that if you look at its Dockerfile, you see that building this image <a href="https://github.com/rocker-org/rocker-versioned/blob/1920e7cfc757bad02d041a0bddec1a18b1ebc4c1/r-ver/3.4.4.Dockerfile#L118">requires MRAN</a>. So in other words, once MRAN is offline, it won’t be possible to rebuild this image, and the Rocker project will need to make sure that the pre-built image that is currently available on Docker Hub stays available forever. Because if not, it would be quite hard to rerun code from, say, 2017. Same goes for Posit’s package manager. Posit’s package manager could be used as a drop-in replacement for MRAN, but for how long? Even though Posit is a very responsible company, I believe that it is dangerous that such a crucial service is managed by only one company.
</p>
<p>
And rebuilding old images will be necessary. This is now the part where I answer the question from above:
</p>
<p>
“why use Docker at all? Since it’s easy to install older versions of R on Windows and macOS, wouldn’t an <code>renv.lock</code> file suffice? Or even just <code>{groundhog}</code> which is arguably even easier to use?”
</p>
<p>
The problem is hardware. You see, Apple has changed hardware architecture recently, their new computers switched from Intel based hardware to their own proprietary architecture (Apple Silicon) based on the ARM specification. And what does that mean concretely? It means that all the binary packages that were built for Intel based Apple computers cannot work on their new computers. Which means that if you have a recent M1 Macbook and need to install old CRAN packages (for example, by using <code>{groundhog}</code>), these need to be compiled to work on M1. You cannot even install older versions of R, unless you also compile those from source! Now I have read about a compatibility layer called Rosetta which enables to run binaries compiled for the Intel architecture on the ARM architecture, and maybe this works well with R and CRAN binaries compiled for Intel architecture. Maybe, I don’t know. But my point is that you never know what might come in the future, and thus needing to be able to compile from source is important, because compiling from source is what requires the least amount of dependencies that are outside of your control. Relying on binaries is not future-proof.
</p>
<p>
And for you Windows users, don’t think that the preceding paragraph does not concern you. I think that it is very likely that Microsoft will push in the future for OEM manufacturers to develop more ARM based computers. There is already an ARM version of Windows after all, and it has been around for quite some time, and I think that Microsoft will not kill that version any time in the future. This is because ARM is much more energy efficient than other architectures, and any manufacturer can build its own ARM cpus by purchasing a license, which can be quite interesting. For example in the case of Apple silicon cpus, Apple can now get exactly the cpus they want for their machines and make their software work seamlessly with it. I doubt that others will pass the chance to do the same.
</p>
<p>
Also, something else that might happen is that we might move towards more and more cloud based computing, but I think that this scenario is less likely than the one from before. But who knows. And in that case it is quite likely that the actual code will be running on Linux servers that will likely be ARM based because of energy costs. Here again, if you want to run your historical code, you’ll have to compile old packages and R versions from source.
</p>
<p>
Basically, binary packages are in my opinion not a future-proof option, so that’s why something like Docker will stay, and become ever more relevant. But as I argued before, that’s a single point of failure.
</p>
<p>
But there might be a way we can solve this and not have to rely on Docker at all.
</p>
</section>
<section id="guix-toward-practical-transparent-verifiable-and-long-term-reproducible-research" class="level2">
<h2 class="anchored" data-anchor-id="guix-toward-practical-transparent-verifiable-and-long-term-reproducible-research">
Guix: <em>toward practical transparent verifiable and long-term reproducible research</em>
</h2>
<p>
The title of this section is the same as the title from a research paper published in 2022 that you can read <a href="https://www.nature.com/articles/s41597-022-01720-9#Abs1">here</a>.
</p>
<p>
This paper presents and shows how to use Guix, which is a tool to build, from scratch and in a reproducible manner, the computational environment that was used to run some code for research. Very importantly, Guix doesn’t rely on containers, virtual machines or anything like that. From my, albeit still limited, understanding of how it works, Guix requires some recipes telling it how it should build software. Guix integrates with CRAN, so it’s possible to tell Guix “hey, could you build ggplot2” and Guix does as instructed. It builds <code>{ggplot2}</code> and all the required dependencies. And what’s great about Guix is that it’s also possible to build older versions of software.
</p>
<p>
The authors illustrate this by reproducing results from a 2019 paper, by recreating the environment used at the time, 3 years later.
</p>
<p>
This could be a great solution, because it would always allow the recreation of computational environments from source. So architecture changes would not be a problem, making Guix quite future proof. The issue I’ve found though, is that Guix only works on Linux. So if you’re working on Windows or macOS, you would need Docker to recreate a computational environment with Guix. So you could think that we’re back to square one, but actually no. Because you could always have a Linux machine or server that you would use for reproducibility, on which Linux is installed, thus eliminating the need for Docker, thus removing that risk entirely.
</p>
<p>
I’m currently exploring Guix and will report on it in greater detail in the future. In the meantime, I hope to have convinced you that, while reproducibility is no easy task, the tools that are currently available can help you set up reproducible project. However, for something to be and stay truly reproducible, some long term maintenance is also required.
</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">
Conclusion
</h2>
<p>
So here we are, these are, as far as I know, the options to make your code reproducible. But it is no easy task and it takes time. Unfortunately, many scientists are not really concerned with making their code reproducible, simply because there is no real incentive for them to do it. And it’s a task that is becoming more and more complex as there are other risks that need to be managed, like the transition from Intel based architectures for cpus towards ARM. I’m pretty sure there are many scientists who have absolutely no idea what Intel based cpus or ARM cpus or Rosetta or whatever are. So telling them they need to make their code reproducible is one thing, telling them they need to make it so future proof that architecture changes won’t matter is like asking for the Moon.
</p>
<p>
So research software engineers will become more and more crucial, and should be integrated to research teams to deal with this question (and this also holds true for the private sector; there should be someone whose job is make code reproducible across the organization).
</p>
<p>
Anyways, if you read until here, I appreciate it. This was a long blog post. If you want to know more, you can read this <a href="https://www.brodrigues.co/blog/2022-11-19-raps/">other blog post</a> of mine that explains how to use Docker, and also this <a href="https://www.brodrigues.co/blog/2022-11-30-pipelines-as/">other blog post</a> that explains why Docker <em>is not</em> optional (but now that I’ve discovered Guix, maybe it’s Guix that is not optional).
</p>
<p>
By the way, if you want to grab the scripts and Dockerfiles from this blog post, you can get them <a href="https://github.com/b-rodrigues/repro_r">here</a>.
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>proramming</category>
  <guid>https://b-rodrigues.github.io/posts/2023-01-12-repro_R.html</guid>
  <pubDate>Thu, 12 Jan 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Code longevity of the R programming language</title>
  <link>https://b-rodrigues.github.io/posts/2022-12-21-longevity.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/spelunky.jpg" width="80%" height="auto">
</p>
</div>
<p>
I’ve been working on a way to evaluate how old R code runs on the current version of R, and am now ready to share some results. It all started with this tweet:
</p>
<div style="text-align:center;">
<p>
<a href="https://twitter.com/brodriguesco/status/1588088437655093250?s=20&amp;t=-8DPAVEpMEcAuxy8Q2sAQw"> <img src="https://b-rodrigues.github.io/assets/img/tweet_old_code.png" width="80%" height="auto"> </a>
</p>
</div>
<p>
The problem is that you have to find old code laying around. Some people have found old code they wrote a decade or more ago and tried to rerun it; there’s <a href="https://notstatschat.rbind.io/2022/10/14/code-archaeology-polynomial-distributed-lags/">this blog post</a> by Thomas Lumley and <a href="https://www.jumpingrivers.com/blog/r-from-the-turn-of-the-century/">this other one</a> by Colin Gillespie that I find fascinating, but ideally we’d have more than a handful of old scripts laying around. This is when Dirk Eddelbuettel suggested this:
</p>
<div style="text-align:center;">
<p>
<a href="https://twitter.com/eddelbuettel/status/1588149491772923907?s=20&amp;t=-8DPAVEpMEcAuxy8Q2sAQw"> <img src="https://b-rodrigues.github.io/assets/img/tweet_dirk.png" width="80%" height="auto"> </a>
</p>
</div>
<p>
And this is what I did. I wrote a lot of code to achieve this graph here:
</p>
<div style="text-align:center;">
<p>
<a href="https://github.com/b-rodrigues/code_longevity"> <img src="https://b-rodrigues.github.io/assets/img/r_longevity.png" width="80%" height="auto"> </a>
</p>
</div>
<p>
This graph shows the following: for each version of R, starting with R version 0.6.0 (released in 1997), how well the examples that came with a standard installation of R run on the current version of R (version 4.2.2 as of writing). These are the examples from the default packages like <code>{base}</code>, <code>{stats}</code>, <code>{stats4}</code>, and so on. Turns out that more than 75% of the example code from version 0.6.0 still works on the current version of R. A small fraction output a message (which doesn’t mean the code doesn’t work), some 5% raise a warning, which again doesn’t necessarily mean that the code doesn’t work, and finally around 20% or so errors. As you can see, the closer we get to the current release, the less errors get raised.
</p>
<p>
(But something important should be noted: just because some old piece of code runs without error, doesn’t mean that the result is exactly the same. There might be cases where the same function returns different results on different versions of R.)
</p>
<p>
Then, once I had this graph, I had to continue with packages. How well do old examples from any given package run on the current version of the same package?
</p>
<p>
What I came up with is a Docker image that runs this for you, and even starts a Shiny app to let you explore the results. All you have to do is edit one line in the Dockerfile. This Docker image uses a lot of code from other projects, and I even had to write a package for this, called <code>{wontrun}</code>.
</p>
<section id="the-wontrun-package" class="level2">
<h2 class="anchored" data-anchor-id="the-wontrun-package">
The {wontrun} package
</h2>
<p>
The problem I needed to solve was how to easily run examples from archived packages. So I needed to first have an easy way to download them, then extract the examples, and then run them. So to help me with this I wrote the <code>{wontrun}</code> package (thanks again to <a href="https://fediscience.org/@dmi3kno/109296599193965025">Deemah</a> for suggesting the name and making the hex logo!). To be honest, the quality of this package could be improved. Documentation is still lacking, and the package only seems to work on Linux (but that’s not an issue, since it really only makes sense to use it within Docker). In any case, this package has a function to download the archived source code for a given package, using the <code>get_archived_sources()</code> function. This function takes the name of a package as an input and returns a data frame with the archived sources and the download links to them. To actually download the source packages, the <code>get_examples()</code> function is used. This function extracts the examples from the <code>man/</code> folder included in source packages, and converts the examples into scripts. Remember that example files are in the <code>.Rd</code> format, which is some kind of markup language format. Thankfully, there’s a function called <code>Rd2ex()</code> from the <code>{tools}</code> package which I use to convert <code>.Rd</code> files into <code>.R</code> scripts.
</p>
<p>
Then, all that there is to do is to run these scripts. But that’s not as easy as one might think. This is becuse I first need to make sure that the latest version of the package is installed, and ideally, I don’t want to pollute my library with packages that I never use but only wanted to assess for their code longevity. I also need to make sure that I’m running all these scripts <em>all else being equal</em>: so same version of R, same version of the current packages and same operating system. That why I needed to use Docker for this. Also, all the required dependencies to run the examples should get installed as well. Sometimes, some examples load data from another package. So for this, I’m using the <code>renv::dependencies()</code> function which scans a file for calls to <code>library()</code> or <code>package::function()</code> to list the dependencies and then install them. This all happens automatically.
</p>
<p>
To conclude this section: I cannot stress how much I’m relying on work by other people for this. This is the NAMESPACE file of the <code>{wontrun}</code> package (I’m only showing the import statements):
</p>
<pre><code>importFrom(callr,r_vanilla)
importFrom(ctv,ctv)
importFrom(dplyr,filter)
importFrom(dplyr,group_by)
importFrom(dplyr,mutate)
importFrom(dplyr,rename)
importFrom(dplyr,select)
importFrom(dplyr,ungroup)
importFrom(furrr,future_map2)
importFrom(future,multisession)
importFrom(future,plan)
importFrom(janitor,clean_names)
importFrom(lubridate,year)
importFrom(lubridate,ymd)
importFrom(lubridate,ymd_hm)
importFrom(magrittr,"%&gt;%")
importFrom(pacman,p_load)
importFrom(pkgsearch,cran_package)
importFrom(purrr,keep)
importFrom(purrr,map)
importFrom(purrr,map_chr)
importFrom(purrr,map_lgl)
importFrom(purrr,pluck)
importFrom(purrr,pmap_chr)
importFrom(purrr,possibly)
importFrom(renv,dependencies)
importFrom(rlang,`!!`)
importFrom(rlang,cnd_message)
importFrom(rlang,quo)
importFrom(rlang,try_fetch)
importFrom(rvest,html_nodes)
importFrom(rvest,html_table)
importFrom(rvest,read_html)
importFrom(stringr,str_extract)
importFrom(stringr,str_remove_all)
importFrom(stringr,str_replace)
importFrom(stringr,str_trim)
importFrom(tibble,as_tibble)
importFrom(tidyr,unnest)
importFrom(tools,Rd2ex)
importFrom(withr,with_package)</code></pre>
<p>
That’s a lot of packages, most of them from Posit. What can I say, these packages are great! Even if I could reduce the number of dependencies from <code>{wontrun}</code>, I honestly cannot be bothered, I’ve been spoilt by the quality of Posit packages.
</p>
</section>
<section id="docker-for-reproducibility" class="level2">
<h2 class="anchored" data-anchor-id="docker-for-reproducibility">
Docker for reproducibility
</h2>
<p>
The Dockerfile I wrote is based on Ubuntu 22.04, compiles R 4.2.2 from source, and sets the repositories to <a href="https://packagemanager.rstudio.com/cran/__linux__/jammy/2022-11-21" class="uri">https://packagemanager.rstudio.com/cran/<strong>linux</strong>/jammy/2022-11-21</a> . This way, the packages get downloaded exactly as they were on November 21st 2022. This ensures that if readers of this blog post want to run this to assess the code longevity of some R packages, we can compare results and be certain that any conditions raised are not specific to any difference in R or package version. It should be noted that this Dockerfile is based on the work of the Rocker project, and more specifically their <a href="https://rocker-project.org/images/versioned/r-ver.html">versioned images</a> which are recommended when reproducibility is needed. Becuse the code runs inside Docker, it doesn’t matter if the <code>{wontrun}</code> package only runs on Linux (I think that this is the case because of the <code>untar()</code> function which I use to decompress the downloaded compressed archives from CRAN, and which seems to have a different behaviour on Linux vs Windows. No idea how this function behaves on macOS).
</p>
<p>
The image defined by this Dockerfile is quite heavy, because I also installed all possible dependencies required to run R packages smoothly. This is because even though the Posit repositories install compiled packages on Linux, shared libraries are still needed for the packages to run.
</p>
<p>
Here is what the Dockerfile looks like:
</p>
<pre><code>FROM brodriguesco/wontrun:r4.2.2

# This gets the shiny app
RUN git clone https://github.com/b-rodrigues/longevity_app.git

# These are needed for the Shiny app
RUN R -e "install.packages(c('dplyr', 'forcats', 'ggplot2', 'shiny', 'shinyWidgets', 'DT'))"

RUN mkdir /home/intermediary_output/
RUN mkdir /home/output/

COPY wontrun.R /home/wontrun.R

# Add one line per package you want to asses
RUN Rscript '/home/wontrun.R' dplyr 6
RUN Rscript '/home/wontrun.R' haven 6

CMD mv /home/intermediary_output/* /home/output/ &amp;&amp; R -e 'shiny::runApp("/longevity_app", port = 1506, host = "0.0.0.0")'</code></pre>
<p>
As you can see it starts by pulling an image from Docker Hub called <code>wontrun:r4.2.2</code>. This is the image based on Ubuntu 22.04 with R compiled from source and all dependencies pre-installed. (This Dockerfile is available <a href="https://github.com/b-rodrigues/code_longevity/tree/master/wontrun_dockerfile">here</a>.)
</p>
<p>
Then my Shiny app gets cloned, the required packages for the app to run get installed, and some needed directories get made. Now comes the interesting part; a script called <code>wontrun.R</code> gets copied. This is what the script looks like:
</p>
<pre class="r"><code>#!/usr/bin/env Rscript
args &lt;- commandArgs(trailingOnly = TRUE)

library(wontrun)

packages_sources &lt;- get_archived_sources(args[1])

out &lt;- wontrun(packages_sources ,
               ncpus = args[2],
               setup = TRUE,
               wontrun_lib = "/usr/local/lib/R/site-library/")

saveRDS(object = out,
        file = paste0("/home/intermediary_output/", args[1], ".rds"))</code></pre>
<p>
This script uses the <code>{wontrun}</code> package to get the archived sources of a package of interest, and the examples get executed and results tallied using the <code>wontrun()</code> function. The results then get saved into an <code>.rds</code> file.
</p>
<p>
Calling this script is done with this line in the Dockerfile:
</p>
<pre><code>RUN Rscript '/home/wontrun.R' dplyr 6</code></pre>
<p>
The <code>dplyr</code> and <code>6</code> get passed down to the <code>wontrun.R</code> script as a list called <code>args</code>. So <code>args[1]</code> is the “dplyr” string, and <code>args[2]</code> is 6. This means that the examples from archived versions of the <code>{dplyr}</code> package will get assessed on the current version of <code>{dplyr}</code> using 6 cores. You can add as many lines as you want and thus assess as many packages as you want. Once you’re done with editing the Dockerfile, you can build the image; this will actually run the code, so depending on how many packages you want to assess and the complexity of the examples, this may take some hours. To build the image run this in a console:
</p>
<pre><code>docker build -t code_longevity_packages .</code></pre>
<p>
Now, you still need to actually run a container based on this image. Running the container will move the <code>.rds</code> files from the container to your machine so you can actually get to the results, and it will also start a Shiny app in which you will be able to upload the <code>.rds</code> file and explore the results. Run the container with (and don’t forget to change <code>path/to/repository/</code> with the correct path on your machine):
</p>
<pre><code>docker run --rm --name code_longevity_packages_container -v /path/to/repository/code_longevity_packages/output:/home/output:rw -p 1506:1506 code_longevity_packages</code></pre>
<p>
Go over to <code>http://localhost:1506/</code> to start the Shiny app and explore the results:
</p>
<div style="text-align:center;">
<p>
<video width="640" height="480" controls="">
<source src="../assets/img/code_longevity.mp4" type="video/mp4">
</video>
</p>
</div>
</section> ]]></description>
  <category>R</category>
  <category>proramming</category>
  <guid>https://b-rodrigues.github.io/posts/2022-12-21-longevity.html</guid>
  <pubDate>Wed, 21 Dec 2022 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Functional programming explains why containerization is needed for reproducibility</title>
  <link>https://b-rodrigues.github.io/posts/2022-11-30-pipelines-as.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/dino.png" width="80%" height="auto">
</p>
</div>
<p>
I’ve had some discussions online and in the real world about <a href="../posts/2022-11-16-open_source_repro.html">this blog post</a> and I’d like to restate why containerization is needed for reproducibility, and do so from the lens of functional programming.
</p>
<p>
When setting up a pipeline, wether you’re a functional programming enthusiast or not, you’re aiming at setting it up in a way that this pipeline is the composition of (potentially) many referentially transparent and pure functions.
</p>
<p>
As a reminder:
</p>
<ul>
<li>
<p>
referentially transparent functions are functions that always return the same output for the same given input. So for example <code>f(x, y):=x+y</code> is referentially transparent, but <code>h(x):=x+y</code> is not. Because <code>y</code> is not an input of <code>h</code>, <code>h</code> will look for <code>y</code> in the global environment. Depending on the value of y, <code>h(1)</code> might equal 10 one day, but 100 the next. Let’s say that <code>f(1, 10)</code> is always equal to 11. Because this is true, you could replace <code>f(1, 10)</code> everywhere it appears with 11. But consider the following example of a function that is not referentially transparent, <code>rnorm()</code>. Try <code>rnorm(1)</code> several times… It will always give a different result! This is because <code>rnorm()</code> looks for the seed in the global environment and uses that to generate a random number.
</p>
</li>
<li>
<p>
pure functions are functions without side effects. So a function just does its thing, and does not interact with anything else; doesn’t change anything in the global environment, doesn’t print anything on screen, doesn’t write anything to disk. Basically, pure functions are functions that do nothing else but computing stuff. Now this may seem limiting, and to some extent it is, so we will need to relax this a bit: we’ll be ok with functions that output stuff, but only the very last function in the pipeline will be allowed to do it.
</p>
</li>
</ul>
<p>
To be pure, a function needs to be referentially transparent.
</p>
<p>
Ok so now that we know what referentially transparent and pure functions are, let’s explain why we want a pipeline to be a composition of such functions. Function composition is an operation that takes two functions <em>g</em> and <em>f</em> and returns a new function <em>h</em> such that <code>h(x) = g(f(x))</code>. Formally:
</p>
<pre><code>h = g ∘ f such that h(x) = g(f(x))</code></pre>
<p>
<code>∘</code> is the composition operator. You can read <code>g ∘ f</code> as <em>g after f</em>. In R, you can compose functions very easily, simply by using |&gt; or %&gt;%:
</p>
<pre><code>h &lt;- f |&gt; g</code></pre>
<p>
<code>f |&gt; g</code> can be read as <em>f then g</em>, which is equivalent to <em>g after f</em> (ok, using <code>|&gt;</code> is chaining rather than composing functions, but the net effect is the same).
</p>
<p>
So <code>h</code> would be our complete pipeline, which would be the composition, or chaining, of as many functions as needed:
</p>
<pre><code>h &lt;- a |&gt; b |&gt; c |&gt; d ... |&gt; z</code></pre>
<p>
If all the functions are pure (and referentially transparent) then we’re assured that <code>h</code> will always produce the same outputs for the same inputs. As stated above, <code>z</code> will be allowed to not be pure an actually output something (like a rendered Quarto document) to disk. Ok so that’s great, and all, but why does the title of this blog post say that containerization is needed?
</p>
<p>
The problem is that all the functions we use have “hidden” inputs, and are never truly referentially transparent. These inputs are the following:
</p>
<ul>
<li>
Version of R (or whatever programming language you’re using)
</li>
<li>
Versions of the packages you’re using
</li>
<li>
Operating system and its version (and all the different operating system dependencies that get used at run- or compile time)
</li>
</ul>
<p>
For example, let’s take a look at this function:
</p>
<pre class="r"><code>f &lt;- function(x){
  if (c(TRUE, FALSE)) x 
}</code></pre>
<p>
which will return the following on R 4.1 (which was released on May 2021):
</p>
<pre class="r"><code>f(1)</code></pre>
<pre class="r"><code>[1] 1
Warning message:
In if (c(TRUE, FALSE)) 1 :
  the condition has length &gt; 1 and only the first element will be used</code></pre>
<p>
So a result 1 and a warning. On R 4.2.2 (the current version as of writing), the exact same call returns:
</p>
<pre class="r"><code>Error in if (c(TRUE, FALSE)) 1 : the condition has length &gt; 1</code></pre>
<p>
These types of breaking changes are rare in R, at least to my knowledge (I’m actually looking into this in greater detail, 2023 will likely be the year I show my findings), but in this case it illustrates my point: code that was behaving in a certain way started behaving in another way, even though nothing changed. What changed was the version of R, even though the function itself was pure. This wouldn’t be so surprising if instead of <code>f(x)</code>, the function was something like <code>f(x, r_version)</code>. In this case, the calls above would be something like:
</p>
<pre class="r"><code>f(1, r_version = "4.1")</code></pre>
<p>
and this would always return:
</p>
<pre class="r"><code>[1] 1
Warning message:
In if (c(TRUE, FALSE)) 1 :
  the condition has length &gt; 1 and only the first element will be used</code></pre>
<p>
but changing the call to this:
</p>
<pre class="r"><code>f(1, r_version = "4.2.2")</code></pre>
<p>
would return the error:
</p>
<pre class="r"><code>Error in if (c(TRUE, FALSE)) 1 : the condition has length &gt; 1</code></pre>
<p>
regardless of the version of R we’re running, so our function would be referentially transparent.
</p>
<p>
Alas, this is not possible, at least not like this.
</p>
<p>
Hence why tools like Docker, Podman (a Docker alternative) or Guix (which I learned about recently but never used, yet, and as far as I know, not a containerization solution, but a solution actually based on functional programming) are crucial to ensure that your pipeline is truly reproducible. Basically, using Docker you turn the hidden inputs defined before (versions of tools and OS) explicit. Take a look at this Dockerfile:
</p>
<pre class="r"><code>FROM rocker/r-ver:4.1.0

RUN R -e "f &lt;- function(x){if (c(TRUE, FALSE)) x};f(1)"

CMD ["R"]</code></pre>
<p>
here’s what happens when you build it:
</p>
<pre><code>➤ docker build -t my_pipeline .</code></pre>
<pre><code>Sending build context to Docker daemon  2.048kB
Step 1/3 : FROM rocker/r-ver:4.1.0
4.1.0: Pulling from rocker/r-ver

eaead16dc43b: Already exists 
35eac095fa03: Pulling fs layer
c0088a79f8ab: Pulling fs layer
28e8d0ade0c0: Pulling fs layer
Digest: sha256:860c56970de1d37e9c376ca390617d50a127b58c56fbb807152c2e976ce02002
Status: Downloaded newer image for rocker/r-ver:4.1.0
 ---&gt; d83268fb6cda
Step 2/3 : RUN R -e "f &lt;- function(x){if (c(TRUE, FALSE)) x};f(1)"
 ---&gt; Running in a158e4ab474f

R version 4.1.0 (2021-05-18) -- "Camp Pontanezen"
Copyright (C) 2021 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

&gt; f &lt;- function(x){if (c(TRUE, FALSE)) x};f(1)
[1] 1
Warning message:
In if (c(TRUE, FALSE)) x :&gt; 
&gt; 

  the condition has length &gt; 1 and only the first element will be used
Removing intermediate container a158e4ab474f
 ---&gt; 49e2eb20a535
Step 3/3 : CMD ["R"]
 ---&gt; Running in ccda657c4d95
Removing intermediate container ccda657c4d95
 ---&gt; 5a432adbe6ff
Successfully built 5a432adbe6ff
Successfully tagged my_package:latest</code></pre>
<p>
as you can read from above, this starts the container with R version 4.1.0 and runs the code in it. We get back our result with the warning (it should be noted that in practice, you would structure your Dockerfile differently for running an actual pipeline).
</p>
<p>
This Dockerfile starts by using rocker/r-ver:4.1 as a basis. You can find this image in the <a href="https://github.com/rocker-org/rocker-versioned2/blob/master/dockerfiles/r-ver_4.1.0.Dockerfile">versioned</a> repository from the Rocker Project. This base image starts off from Ubuntu Focal Fossa so (Ubuntu version 20.04), uses R version 4.1.0 and even uses frozen CRAN repository as of 2021-08-09. It then runs our pipeline (or in this case, our simple function) in this, fixed environment. Our function essentially became <code>f(x, os_version, r_version, packages_version)</code> instead of just <code>f(x)</code>. By changing the first statement of the Dockerfile:
</p>
<pre class="r"><code>FROM rocker/r-ver:4.1.0</code></pre>
<p>
to this:
</p>
<pre class="r"><code>FROM rocker/r-ver:3.5.0</code></pre>
<p>
we can even do some archaeology and run the pipeline on R version 3.5.0! This has great potential and hopefully one day Docker or similar solution will become just another tool in scientists/analysts toolbox.
</p>
<p>
If you want to start using Docker for your projects, I’ve written this <a href="../posts/2022-11-19-raps.html">tutorial</a> and even a whole <a href="https://rap4mads.eu/">ebook</a>.
</p>



 ]]></description>
  <category>R</category>
  <category>proramming</category>
  <guid>https://b-rodrigues.github.io/posts/2022-11-30-pipelines-as.html</guid>
  <pubDate>Wed, 30 Nov 2022 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Reproducibility with Docker and Github Actions for the average R enjoyer</title>
  <link>https://b-rodrigues.github.io/posts/2022-11-19-raps.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<img src="https://b-rodrigues.github.io/assets/img/raps.png" width="80%" height="auto">
</p>
</div>
<p>
<em>This blog post is a summary of Chapters 9 and 10 of this <a href="https://rap4mads.eu/self-contained-raps-with-docker.html">ebook</a> I wrote for a course</em>
</p>
<p>
The goal is the following: we want to write a pipeline that produces some plots. We want the code to be executed inside a Docker container for reproducibility, and we want this container to get executed on Github Actions. Github Actions is a <em>Continuous Integration and Continuous Delivery</em> service from Github that allows you to execute arbitrary code on events (like pushing code to a repo). It’s pretty neat. For example, you could be writing a paper using Latex and get the pdf compiled on Github Actions each time you push, without needing to have to do it yourself. Or if you are developing an R package, unit tests could get executed each time you push code, so you don’t have to do it manually.
</p>
<p>
This blog post will assume that you are familiar with R and are comfortable with it, as well as Git and Github.
</p>
<p>
It will also assume that you’ve at least heard of Docker and have it already installed on your computer, but ideally, you’ve already played a bit around with Docker. If you’re a total Docker beginner, this tutorial might be a bit too esoteric.
</p>
<p>
Let’s start by writing a pipeline that works on our machines using the <code>{targets}</code> package.
</p>
<section id="getting-something-working-on-your-machine" class="level2">
<h2 class="anchored" data-anchor-id="getting-something-working-on-your-machine">
Getting something working on your machine
</h2>
<p>
So, let’s say that you got some nice code that you need to rerun every month, week, day, or even hour. Or let’s say that you’re a researcher that is concerned with reproducibility. Let’s also say that you want to make sure that this code always produces the same result (let’s say it’s some plots that need to get remade once some data is refreshed).
</p>
<p>
Ok, so first of all, you really want your workflow to be defined using the <code>{targets}</code> package. If you’re not familiar with <code>{targets}</code>, this will serve as a micro introduction, but you really should read the <code>{targets}</code> manual, at least the <a href="https://books.ropensci.org/targets/walkthrough.html">walkthrough</a> (watch the 4 minute video). <code>{targets}</code> is a build automation tool that you should definitely add to your toolbox.
</p>
<p>
Let’s define a workflow that does the following: data gets read, data gets filtered, data gets plotted. What’s the data about? Unemployment in Luxembourg. Luxembourg is a little Western European country that looks like a shoe and is <a href="https://raw.githubusercontent.com/rbind/b-rodrigues.github.com/master/static/img/rhode_island.png" width="80%" height="auto">about the size of .98 Rhode Islands</a> from which yours truly hails from. Did you know that Luxembourg was a monarchy, and the last Grand-Duchy in the World? I bet you did not know that. Also, what you should know to understand the script below is that the country of Luxembourg is divided into Cantons, and each Cantons into Communes. Basically, if Luxembourg was the USA, Cantons would be States and Communes would be Counties (or Parishes or Boroughs). What’s confusing is that “Luxembourg” is also the name of a Canton, and of a Commune (which also has the status of a city).
</p>
<p>
Anyways, here’s how my script looks like:
</p>
<pre class="r"><code>library(targets)
library(dplyr)
library(ggplot2)
source("functions.R")


list(
    tar_target(
        unemp_data,
        get_data()
    ),

    tar_target(
        lux_data,
        clean_unemp(unemp_data,
                    place_name_of_interest = "Luxembourg",
                    level_of_interest = "Country",
                    col_of_interest = active_population)
    ),

    tar_target(
        canton_data,
        clean_unemp(unemp_data,
                    level_of_interest = "Canton",
                    col_of_interest = active_population)
    ),

    tar_target(
        commune_data,
        clean_unemp(unemp_data,
                    place_name_of_interest = c("Luxembourg",
                                               "Dippach",
                                               "Wiltz",
                                               "Esch/Alzette",
                                               "Mersch"),
                    col_of_interest = active_population)
    ),

    tar_target(
        lux_plot,
        make_plot(lux_data)
    ),

    tar_target(
        canton_plot,
        make_plot(canton_data)
    ),

    tar_target(
        commune_plot,
        make_plot(commune_data)
    ),

    tar_target(
        luxembourg_saved_plot,
        save_plot("fig/luxembourg.png", lux_plot),
        format = "file"
    ),

    tar_target(
        canton_saved_plot,
        save_plot("fig/canton.png", canton_plot),
        format = "file"
    ),

    tar_target(
        commune_saved_plot,
        save_plot("fig/commune.png", commune_plot),
        format = "file"
    )


)</code></pre>
<p>
Because this is a <code>{targets}</code> script, this needs to be saved inside a file called <code>_targets.R</code>. Each <code>tar_target()</code> object defines a target that will get built once we run the pipeline. The first element of <code>tar_target()</code> is the name of the target, the second line a call to a function that returns the first element and in the last three targets <code>format = “file”</code> is used to indicate that this target saves an output to disk (as a file).
</p>
<p>
The fourth line of the script sources a script called <code>functions.R</code>. This script should be placed next to the <code>_targets.R</code> script and should look like this:
</p>
<pre><code># clean_unemp() is a function inside a package I made. Because I don't want you to install
# the package if you're following along, I'm simply sourcing it:

source("https://raw.githubusercontent.com/b-rodrigues/myPackage/main/R/functions.R")

# The cleaned data is also available in that same package. But again, because I don't want you
# to install a package just for a blog post, here is the script to clean it.
# Don't waste time trying to understand it, it's very specific to the data I'm using
# to illustrate the concept of reproducible analytical pipelines. Just accept this data 
# as given.

# This is a helper function to clean the data
clean_data &lt;- function(x){
  x %&gt;%
    janitor::clean_names() %&gt;%
    mutate(level = case_when(
             grepl("Grand-D.*", commune) ~ "Country",
             grepl("Canton", commune) ~ "Canton",
             !grepl("(Canton|Grand-D.*)", commune) ~ "Commune"
           ),
           commune = ifelse(grepl("Canton", commune),
                            stringr::str_remove_all(commune, "Canton "),
                            commune),
           commune = ifelse(grepl("Grand-D.*", commune),
                            stringr::str_remove_all(commune, "Grand-Duche de "),
                            commune),
           ) %&gt;%
    select(year,
           place_name = commune,
           level,
           everything())
}

# This reads in the data.
get_data &lt;- function(){
  list(
    "https://raw.githubusercontent.com/b-rodrigues/modern_R/master/datasets/unemployment/unemp_2013.csv",
    "https://raw.githubusercontent.com/b-rodrigues/modern_R/master/datasets/unemployment/unemp_2014.csv",
    "https://raw.githubusercontent.com/b-rodrigues/modern_R/master/datasets/unemployment/unemp_2015.csv",
    "https://raw.githubusercontent.com/b-rodrigues/modern_R/master/datasets/unemployment/unemp_2016.csv",
  ) |&gt;
    purrr::map_dfr(readr::read_csv) %&gt;%
    purrr::map_dfr(clean_data)
}

# This plots the data
make_plot &lt;- function(data){
  ggplot(data) +
    geom_col(
      aes(
        y = active_population,
        x = year,
        fill = place_name
      )
    ) +
    theme(legend.position = "bottom",
          legend.title = element_blank())
}

# This saves plots to disk
save_plot &lt;- function(save_path, plot){
  ggsave(save_path, plot)
  save_path
}</code></pre>
<p>
What you could do instead of having a <code>functions.R</code> script that you source like this, is put everything inside a package that you then host on Github. But that’s outside the scope of this blog post. Put these scripts inside a folder, open an R session inside that folder, and run the pipeline using <code>targets::tar_make()</code>:
</p>
<pre class="r"><code>targets::tar_make()</code></pre>
<pre><code>/
Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

• start target unemp_data
• built target unemp_data [1.826 seconds]
• start target canton_data
• built target canton_data [0.038 seconds]
• start target lux_data
• built target lux_data [0.034 seconds]
• start target commune_data
• built target commune_data [0.043 seconds]
• start target canton_plot
• built target canton_plot [0.007 seconds]
• start target lux_plot
• built target lux_plot [0.006 seconds]
• start target commune_plot
• built target commune_plot [0.003 seconds]
• start target canton_saved_plot
Saving 7 x 7 in image
• built target canton_saved_plot [0.425 seconds]
• start target luxembourg_saved_plot
Saving 7 x 7 in image
• built target luxembourg_saved_plot [0.285 seconds]
• start target commune_saved_plot
Saving 7 x 7 in image
• built target commune_saved_plot [0.291 seconds]
• end pipeline [3.128 seconds]</code></pre>
<p>
You can now see a <code>fig/</code> folder in the root of your project with the plots. Sweet.
</p>
</section>
<section id="making-sure-this-is-reproducible" class="level2">
<h2 class="anchored" data-anchor-id="making-sure-this-is-reproducible">
Making sure this is reproducible
</h2>
<p>
Now what we would like to do is make sure that this pipeline will, for the same inputs, returns the same outputs FOREVER. If I’m running this in 10 years on R version 6.9, I want the exact same plots back. So the idea is to actually never run this on whatever version of R will be available in 10 years, but keep rerunning it, <em>ad vitam æternam</em> on whatever environment I’m using now to type this blog post. So for this, I’m going to use Docker.
</p>
<p>
(If, like me, you’re an average functional programming enjoyer, then this means getting rid of the hidden state of our pipeline. The hidden global state is the version of R and packages used to run the pipeline.)
</p>
<p>
What’s Docker? Docker is a way to run a Linux computer inside your computer (Linux or not). That computer is not real, but real enough for our purposes. Ever heard of virtual machines? Basically the same thing, but without the overhead of actually setting up and running a virtual machine.
</p>
<p>
You can write a simple text file that defines what your machine is, and what it should run. Thankfully, we don’t need to start from scratch and can use the amazing <a href="https://rocker-project.org/">Rocker project</a> that provides many, many, images for us to start playing with Docker. What’s a Docker image? A definition of a computer/machine. Which is a text file. Don’t ask why it’s called an image. Turns out the Rocker project has a page specifically on <a href="https://rocker-project.org/use/reproducibility.html">reproducibility</a>. Their advice can be summarised as follows: if you’re aiming at setting up a reproducible pipeline, use a version-stable image. This means that if you start from such an image, the exact same R version will always be used to run your pipeline. Plus, the RStudio Public Package Manager (RSPM), frozen at a specific date, will be used to fetch the packages needed for your pipeline. So, not only is the R version frozen, but the exact same packages will always get installed (as long as the RSPM exists, hopefully for a long time).
</p>
<p>
Now, I’ve been talking about a script that defines an image for some time. This script is called a <code>Dockerfile</code>, and you can find the versioned <code>Dockerfiles</code> <a href="https://github.com/rocker-org/rocker-versioned2/tree/master/dockerfiles">here</a>. As you can see there are many <code>Dockerfile</code>s, each defining a Linux machine and with several things pre-installed. Let’s take a look at the image <a href="https://github.com/rocker-org/rocker-versioned2/blob/master/dockerfiles/r-ver_4.2.1.Dockerfile">r-ver_4.2.1.Dockerfile</a>. What’s interesting here are the following lines (let’s ignore the others):
</p>
<pre><code>8 ENV R_VERSION=4.2.1

16 ENV CRAN=https://packagemanager.rstudio.com/cran/__linux__/focal/2022-10-28</code></pre>
<p>
The last characters of that link are a date. This means that if you use this for your project, packages will be downloaded as they were on the October 28th, 2022, and the R version used will always be version 4.2.1.
</p>
<p>
Ok so, how do we use this?
</p>
<p>
Let’s add a <code>Dockerfile</code> to our project. Simply create a text file called <code>Dockerfile</code> and add the following lines in it:
</p>
<pre><code>FROM rocker/r-ver:4.2.1

RUN R -e "install.packages(c('dplyr', 'purrr', 'readr', 'stringr', 'ggplot2', 'janitor', 'targets'))"

RUN mkdir /home/fig

COPY _targets.R /_targets.R

COPY functions.R /functions.R

CMD R -e "targets::tar_make()"</code></pre>
<p>
Before continuing, I should explain what the first line does:
</p>
<pre><code>FROM rocker/r-ver:4.2.1</code></pre>
<p>
This simply means that we are using the image <a href="https://github.com/rocker-org/rocker-versioned2/blob/master/dockerfiles/r-ver_4.2.1.Dockerfile">from before</a> as a base. This image is itself based on <em>Ubuntu Focal</em>, see its first line:
</p>
<pre><code>FROM ubuntu:focal</code></pre>
<p>
Ubuntu is a very popular, likely the most popular, Linux distribution. So the versioned image is built on top of Ubuntu 20.04 codenamed Focal Fossa (which is a long term support release), and our image is built on top of that. To make sense of all this, you can take a look at the table <a href="https://github.com/rocker-org/rocker-versioned2/wiki/Versions">here</a>.
</p>
<p>
So now that we’ve written this <code>Dockerfile</code>, we need to build the image. This can be done inside a terminal with the following line:
</p>
<pre><code>docker build -t my_pipeline .</code></pre>
<p>
This tells Docker to build an image called <code>my_pipeline</code> using the Dockerfile in the current directory (hence the <code>.</code>).
</p>
<p>
But, here’s what happens when we try to run the pipeline (I’ll be showing the command to run the pipeline below):
</p>
<pre><code>&gt; targets::tar_make()
Error in dyn.load(file, DLLpath = DLLpath, ...) : 
  unable to load shared object '/usr/local/lib/R/site-library/igraph/libs/igraph.so':
  libxml2.so.2: cannot open shared object file: No such file or directory
Calls: loadNamespace ... asNamespace -&gt; loadNamespace -&gt; library.dynam -&gt; dyn.load
Execution halted</code></pre>
<p>
We get a nasty error message; apparently some library, <code>libxml2.so</code> cannot be found. So we need to change our <code>Dockerfile</code>, and add the following lines:
</p>
<pre><code>FROM rocker/r-ver:4.2.1

RUN apt-get update &amp;&amp; apt-get install -y \
    libxml2-dev \
    libglpk-dev \
    libxt-dev

RUN R -e "install.packages(c('dplyr', 'purrr', 'readr', 'stringr', 'ggplot2', 'janitor', 'targets'))"

RUN mkdir /home/fig

COPY _targets.R /_targets.R

COPY functions.R /functions.R

CMD R -e "targets::tar_make()"</code></pre>
<p>
I’ve added these lines:
</p>
<pre><code>RUN apt-get update &amp;&amp; apt-get install -y \
    libxml2-dev \
    libglpk-dev \
    libxt-dev</code></pre>
<p>
this runs the <code>apt-get update</code> and <code>apt-get install</code> commands. Aptitude is Ubuntu’s package manager and is used to install software. The three pieces of software I installed will avoid further issues. <code>libxml2-dev</code> is for the error message I’ve pasted here, while the other two avoid further error messages. One last thing before we rebuild th image: we actually need to change the <code>_targets.R</code> file a bit. Let’s take a look at our <code>Dockerfile</code> again, there’s three lines I haven’t commented:
</p>
<pre><code>RUN mkdir /home/fig

COPY _targets.R /_targets.R

COPY functions.R /functions.R</code></pre>
<p>
The first line creates the <code>fig/</code> folder in the <code>home/</code> directory, and the <code>COPY</code> statements copy the files into the Docker image, so that they’re actually available inside the Docker. I also need to tell <code>_targets</code> to save the figures into the <code>home/fig</code> folder. So simply change the last three targets from this:
</p>
<pre><code>tar_target(
        luxembourg_saved_plot,
        save_plot("fig/luxembourg.png", lux_plot),
        format = "file"
    ),

    tar_target(
        canton_saved_plot,
        save_plot("fig/canton.png", canton_plot),
        format = "file"
    ),

    tar_target(
        commune_saved_plot,
        save_plot("fig/commune.png", commune_plot),
        format = "file"
    )</code></pre>
<p>
to this:
</p>
<pre><code>tar_target(
        luxembourg_saved_plot,
        save_plot("/home/fig/luxembourg.png", lux_plot),
        format = "file"
    ),

    tar_target(
        canton_saved_plot,
        save_plot("/home/fig/canton.png", canton_plot),
        format = "file"
    ),

    tar_target(
        commune_saved_plot,
        save_plot("/home/fig/commune.png", commune_plot),
        format = "file"
    )</code></pre>
<p>
Ok, so now we’re ready to rebuild the image:
</p>
<pre><code>docker build -t my_pipeline .</code></pre>
<p>
and we can now run it:
</p>
<pre><code>docker run --rm --name my_pipeline_container -v /path/to/fig:/home/fig my_pipeline</code></pre>
<p>
<code>docker run</code> runs a container based on the image you defined. <code>–rm</code> means that the container should be removed once it stops, <code>–name</code> gives it a name, here <code>my_pipeline_container</code> (this is not really needed here, because the container stops and gets removed once it’s done running), and <code>-v</code> mounts a volume, which is a fancy way of saying that the folder <code>/path/to/fig/</code>, which is a real folder on your computer, is a portal to the folder <code>/home/fig/</code> (which we created in the <code>Dockerfile</code>). This means that whatever gets saved inside <code>home/fig/</code> inside the Docker container gets also saved inside <code>/path/to/fig</code> on your computer. The last argument <code>my_pipeline</code> is simply the Docker image you built before. You should see the three plots magically appearing in <code>/path/to/fig</code> once the container is done running. The other neat thing is that you can upload this image to Docker Hub, for free (to know how to do this, check out this <a href="https://rap4mads.eu/self-contained-raps-with-docker.html#building-a-truly-reproducible-pipeline">section</a> of the course I teach on this). This way, if other people want to run it, they could do so by running the same command as above, but replacing <code>my_pipeline</code> by <code>your_username_on_docker_hub/image_name_on_docker_hub</code>. People could even create new images based on this image, by using <code>FROM your_username_on_docker_hub/image_name_on_docker_hub</code> at the beginning of their <code>Dockerfile</code>s. If you want an example of a pipeline that starts off from such an image, you can check out this <a href="https://github.com/b-rodrigues/dockerized_pipeline_demo/tree/main">repository</a>. This repository tells you how can run a reproducible pipeline by simply cloning it, building the image (which only takes a few seconds because all software is already installed in the image that I start from) and then running it.
</p>
</section>
<section id="running-this-on-github-actions" class="level2">
<h2 class="anchored" data-anchor-id="running-this-on-github-actions">
Running this on Github Actions
</h2>
<p>
Ok, so now, let’s suppose that we got an image on Docker Hub that contains all the dependencies required for our pipeline, and let’s say that we create a Github repository containing a <code>Dockerfile</code> that pulls from this image, as well as the required scripts for our pipeline. Basically, this is what I did <a href="https://github.com/b-rodrigues/dockerized_pipeline_demo/tree/main">here</a> (the same repository that I linked above already). If you take a look at the first line of the <code>Dockerfile</code> in it, you will see this:
</p>
<pre><code>FROM brodriguesco/r421_rap:version1</code></pre>
<p>
This means that the image that gets built from this <code>Dockerfile</code> starts off from <a href="https://hub.docker.com/layers/brodriguesco/r421_rap/version1/images/sha256-9b8cdaaaf14828468f6c3136c6e2916d3a6efe9c654a97a2a0d12d5d9e5b9ccc?context=repo">this image I’ve uploaded on Docker Hub</a>, this way each time the image gets rebuilt, because the dependencies are already installed, it’s going to be fast. Ok, so now what I want is the following: each time I change a file, be it the <code>Dockerfile</code>, or the <code>_targets.R</code> script, commit my changes and push them, I want Github Actions to rebuild the image, run the container, and give me the plots back.
</p>
<p>
This means that I can focus on coding, Github Actions will take care of the boring stuff.
</p>
<p>
To do this, start by creating a <code>.github/</code> directory on the root of your Github repo, and inside of it, add a <code>.workflows</code> directory, and add a file in it called something like <code>docker-build-run.yml</code>. What matters is that this file ends in <code>.yml</code>. This is what the file I use to define the actions I’ve described above looks like:
</p>
<pre><code>name: Docker Image CI

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:

  build:

    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3
    - name: Build the Docker image
      run: docker build -t my-image-name .
    - name: Docker Run Action
      run: docker run --rm --name my_pipeline_container -v /github/workspace/fig/:/home/graphs/:rw my-image-name
    - uses: actions/upload-artifact@v3
      with:
        name: my-figures
        path: /github/workspace/fig/</code></pre>
<p>
The first line defines the name of the job, here <code>Docker Image CI</code>. The lines state when this should get executed: whenever there’s a push on or pull request on <code>main</code>. The job itself runs on an Ubuntu VM (so Github Actions starts an Ubuntu VM that will pull a Docker image itself running Ubuntu…). Then, there’s the <code>steps</code> statement. For now, let’s focus on the <code>run</code> statements inside <code>steps</code>, because these should be familiar:
</p>
<pre><code>run: docker build -t my-image-name .</code></pre>
<p>
and:
</p>
<pre><code>run: docker run --rm --name my_pipeline_container -v /github/workspace/fig/:/home/graphs/:rw my-image-name</code></pre>
<p>
The only new thing here, is that the path “on our machine” has been changed to <code>/github/workspace/</code>. This is the home directory of your repository, so to speak. Now there’s the <code>uses</code> keyword that’s new:
</p>
<pre><code>uses: actions/checkout@v3</code></pre>
<p>
This action checkouts your repository inside the VM, so the files in the repo are available inside the VM. Then, there’s this action here:
</p>
<pre><code>- uses: actions/upload-artifact@v3
  with:
    name: my-figures
    path: /github/workspace/fig/</code></pre>
<p>
This action takes what’s inside <code>/github/workspace/fig/</code> (which will be the output of our pipeline) and makes the contents available as so-called “artifacts”. Artifacts are the outputs of your workflow, and will be made available as <code>zip</code> files for download. In our case, as stated, the output of the pipeline. It is thus possible to rerun our workflow in the cloud. This has the advantage that we can now focus on simply changing the code, and not have to bother with useless manual steps. For example, let’s change this target in the <code>_targets.R</code> file:
</p>
<pre><code>tar_target(
    commune_data,
    clean_unemp(unemp_data,
                place_name_of_interest = c("Luxembourg", "Dippach", 
                                           "Wiltz", "Esch/Alzette", 
                                           "Mersch", "Dudelange"),
                col_of_interest = active_population)
)
</code></pre>
<p>
I’ve added “Dudelange” to the list of communes to plot. Let me push this change to the repo now, and let’s take a look at the artifacts. The video below summarises the process:
</p>
<div style="text-align:center;">
<p>
<video width="640" height="480" controls="">
<source src="../assets/img/ga_3.mp4" type="video/mp4">
</video>
</p>
</div>
<p>
As you can see in the video, the <code>_targets.R</code> script was changed, and the changes pushed to Github. This triggered the action we’ve defined before. The plots (artifacts) get refreshed, and we can download them. We see then that Dudelange was added in the <code>communes.png</code> plot!
</p>
<p>
If you enjoyed this blog post and want more of this, I wrote a whole <a href="https://rap4mads.eu/">ebook on it</a>.
</p>
<p>
</p>

</section> ]]></description>
  <category>R</category>
  <category>proramming</category>
  <guid>https://b-rodrigues.github.io/posts/2022-11-19-raps.html</guid>
  <pubDate>Sat, 19 Nov 2022 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
