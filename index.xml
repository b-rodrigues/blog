<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Econometrics and Free Software</title>
<link>https://b-rodrigues.github.io/</link>
<atom:link href="https://b-rodrigues.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.6.37</generator>
<lastBuildDate>Sun, 19 May 2019 00:00:00 GMT</lastBuildDate>
<item>
  <title>The never-ending editor war (?)</title>
  <link>https://b-rodrigues.github.io/posts/2019-05-19-spacemacs.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Death_mask"> <img src="https://b-rodrigues.github.io/assets/img/typical_emacs_user.gif" title="typical emacs user working"></a>
</p>
</div>
<p>
The creation of this blog post was prompted by this tweet, asking an age-old question:
</p>
{{% tweet ‚Äú1128981852558123008‚Äù %}}
<p>
This is actually a very important question, that I have been asking myself for a long time. An IDE, and plain text editors, are a very important tools to anyone writing code. Most working hours are spent within such a program, which means that one has to be careful about choosing the right one, and once a choice is made, one has, in my humble opinion, learn as many features of this program as possible to become as efficient as possible.
</p>
<p>
As you can notice from the tweet above, I suggested the use of <a href="http://spacemacs.org/">Spacemacs</a>‚Ä¶ and my tweet did not get any likes or retweets (as of the 19th of May, sympathetic readers of this blog have liked the tweet). It is to set this great injustice straight that I decided to write this blog post.
</p>
<p>
Spacemacs is a strange beast; if vi and Emacs had a baby, it would certainly look like Spacemacs. So first of all, to understand what is Spacemacs, one has to know a bit about vi and Emacs.
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/vim.png" width="100%" style="display: block; margin: auto;">
</p>
<p>
vi is a text editor with 43 years of history now. You might have heard of Vim (Vi IMproved) which is a modern clone of vi, from 1991. More recently, another clone has been getting popular, Neovim, started in 2014. Whatever version of vi however, its basic way of functioning remains the same. vi is a modal editor, meaning that the user has to switch between different modes to work on a text file. When vi is first started, the program will be in <em>Normal</em> mode. In this mode, trying to type a word will likely result in nothing, or unexpected behaviour; unexpected, if you‚Äôre not familiar with vi. For instance, in <em>Normal</em> mode, typing <strong>j</strong> will not show the character <strong>j</strong> on your screen. Instead, this will move the cursor down one line. Typing <strong>p</strong> will paste, <strong>u</strong> will undo the last action, <strong>y</strong> will yank (copy) etc‚Ä¶
</p>
<p>
To type text, first, one has to enter <em>Insert</em> mode, by typing <strong>i</strong> while in <em>Normal</em> mode. Only then is it possible to write text. To go back to <em>Normal</em> mode, type <strong>ESC</strong>. Other modes are <em>Visual</em> mode (from <em>Normal</em> mode press <strong>v</strong>), which allows the user to select text and <em>Command-line</em> mode which can be entered by keying <strong>:</strong> from <em>Normal</em> mode and allows to enter commands.
</p>
<p>
Now you might be wondering why anyone would use such a convoluted way to type text. Well, this is because one can chain these commands quite easily to perform repetitive tasks very quickly. For instance, to delete a word, one types <strong>daw</strong> (in <em>Normal</em> mode), <strong>d</strong>elete <strong>a</strong> <strong>w</strong>ord. To delete the next 3 words, you can type <strong>3daw</strong>. To edit the text between, for instance, <strong>()</strong> you would type <strong>ci(</strong> (while in <em>Normal</em> mode and anywhere between the braces containing the text to edit), <strong>c</strong>hange <strong>i</strong>n <strong>(</strong>. Same logic applies for <strong>ci[</strong> for instance. Can you guess what <strong>ciw</strong> does? If you are in <em>Normal</em> mode, and you want to change the word the cursor is on, this command will erase the word and put you in <em>Insert</em> mode so that you can write the new word.
</p>
<p>
These are just basic reasons why vi (or its clones) are awesome. It is also possible to automate very long and complex tasks using macros. One starts a macro by typing <strong>q</strong> and then any letter of the alphabet to name it, for instance <strong>a</strong>. The user then performs the actions needed, types <strong>q</strong> again to stop the recording of the macro, and can then execute the macro with <strong><span class="citation"><span class="citation" data-cites="a">@a</span></span></strong>. If the user needs to execute the macro say, 10 times, <strong>10@‚Äå‚Äåa</strong> does the trick. It is possible to extend vi‚Äôs functionalities by using plugins, but more on that down below.
</p>
<p>
vi keybindings have inspired a lot of other programs. For instance, you can get extensions for popular web browsers that mimick vi keybindings, such as <a href="https://github.com/tridactyl/tridactyl">Tridayctl</a> for Firefox, or <a href="http://vimium.github.io/">Vivium</a> for Chromium (or Google Chrome). There are even browsers that are built from scratch with support for vi keybinds, such as my personal favorite, <a href="http://qutebrowser.org/">qutebrowser</a>. You can even go further and use a tiling window manager on GNU-Linux, for instance <a href="https://i3wm.org/">i3</a>, which I use, or <a href="https://xmonad.org/">xmonad</a>. You might need to configure those to behave more like vi, but it is possible. This means that by learning one set of keyboard shortcuts, (and the logic behind chaining the keystrokes to achieve what you want), you can master several different programs. This blog post only deals with the editor part, but as you can see, if you go down the rabbit hole enough, a new exciting world opens up.
</p>
<p>
I will show some common vi operations below, but before that let‚Äôs discuss Emacs.
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/emacs.png" width="80%" style="display: block; margin: auto;">
</p>
<p>
I am not really familiar with Emacs; I know that Emacs users only swear by it (just like vi users only swear by vi), and that Emacs is not a modal editor. However, it contains a lot of functions that you can use by pressing <strong>ESC</strong>, <strong>CTRL</strong>, <strong>ALT</strong> or <strong>META</strong> (<strong>META</strong> is the Windows key on a regular PC keyboard) followed by regular keys. So the approach is different, but it is widely accepted that productivity of proficient Emacs users is very high too. Emacs was started in 1985, and the most popular clone is GNU Emacs. Emacs also features modes, but not in the same sense as vi. There are major and minor modes. For instance, if you‚Äôre editing a Python script, Emacs will be in Python mode, or if editing a Markdown file Emacs will be in Markdown mode. This will change the available functions to the user, as well as provide other niceties, such as auto-completion. Emacs is also easily extensible, which is another reason why it is so popular. Users can install packages for Emacs, just like R users would do for R, to extend Emacs‚Äô capabilities. For instance, a very important package if you plan to use Emacs for statistics or data science is <code>ESS</code>, <code>E</code>macs <code>S</code>peaks <code>S</code>tatistics. Emacs contains other very high quality packages, and it seems to me (but don‚Äôt quote me on that) that Emacs‚Äô packages are more mature and feature-rich than vi‚Äôs plugins. However, vi keybindings are really awesome. This is, I believe, what <a href="https://twitter.com/syl20bnr">Sylvain Benner</a> was thinking when he developed Spacemacs.
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/spacemacs.png" width="30%" style="display: block; margin: auto;">
</p>
<p>
Spacemacs‚Äô motto is that <em>The best editor is neither Emacs nor Vim, it‚Äôs Emacs and Vim!</em>. Spacemacs is a version, or distribution of Emacs, that has a very specific way of doing things. However, since it‚Äôs built on top of Emacs, all of Emacs‚Äô packages are available to the user, notably <em>Evil</em>, which is a package that makes Emacs mimick vi‚Äôs modal mode and keybindings (the name of this package tells you everything you need to know about what Emacs users think of vi users üòÄ)
</p>
<p>
Not only does Spacemacs support Emacs packages, but Spacemacs also features so-called <em>layers</em>, which are configuration files that integrate one, or several packages, seamlessly into Spacemacs particular workflow. This particular workflow is what gave Spacemacs its name. Instead of relying on <strong>ESC</strong>, <strong>CTRL</strong>, <strong>ALT</strong> or <strong>META</strong> like Emacs, users can launch functions by typing <strong>Space</strong> in <em>Normal</em> mode and then a sequence of letters. For instance, <strong>Spaceqr</strong> restarts Spacemacs. And what‚Äôs more, you don‚Äôt actually need to learn these new key sequences. When you type <strong>Space</strong>, the minibuffer, a little popup window at the bottom of Spacemacs, appears and shows you all the options that you can type. For instance, typing <strong>b</strong> after <strong>Space</strong> opens up the buffer menu. Buffers are what could be called tabs in Rstudio. Here you can chose to <em>delete</em> a buffer, with <strong>d</strong>, create a new buffer with <strong>N</strong>, and many more options.
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/minibuffer.png" width="100%" style="display: block; margin: auto;">
</p>
<p>
Enough text, let‚Äôs get into the videos. But keep in mind the following: the videos below show the keystrokes I am typing to perform the actions. However, because I use the B√âPO keyboard layout, which is the french equivalent of the DVORAK layout, the keystrokes will be different than those in a regular vi guide, which are mainly written for the QWERTY layout. Also, to use Spacemacs for R, you need to enable the <strong>ESS</strong> layer, which I show how to do at the end. Enabling this layer will turn on auto-completion, as well as provide documentation in real time for your function in the minibuffer:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/spacemacs_autocompletion.png" style="display: block; margin: auto;">
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/spacemacs_doc.png" style="display: block; margin: auto;">
</p>
<p>
The first video shows Spacemacs divided into two windows. On the left, I am navigating around code using the <strong>T</strong> (move down) and <strong>S</strong> (move up) keys. To execute a region that I select, I type <strong>Spacemrr</strong> (this stands for <strong>M</strong>ajor mode <strong>R</strong>un <strong>R</strong>egion). Then around second 5, I key <strong>O</strong> which switches to <em>Insert</em> mode one line below the line I was, type <code>head(mtcars)</code> and then <strong>ESC</strong> to switch back to <em>Normal</em> mode and run the line with <strong>Spacemrl</strong> (<strong>M</strong>ajor mode <strong>R</strong>un <strong>L</strong>ine).
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_01_running_lines.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
In this video, I show you how to switch between windows. Type <strong>SpaceN</strong> to switch to window N. At the end, I key <strong>dd</strong> which deletes a whole line.
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_02_switching_windows.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
In the video below, I show how to use the pipe operator with <strong>Spacemm</strong>. This is a keyboard shortcut that I have defined myself. You can also spot the auto-completion at work in this video. To run the code, I first select it with <strong>V</strong>, which selects the whole line the cursor is currently at and enters <em>Visual</em> mode. I then select the lines below with <strong>T</strong> and run the region with <strong>Spacemrr</strong>.
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_03_pipe.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Here I show how plotting behaves. When a plot is created, a new window is opened with the plot. This is a major shortcoming of using Spacemacs for R programming; there is not a dedicated buffer for plots, and it only shows the very last one created, so there is no way to keep all the plots created in the current session in a neat, dedicated buffer. It seems to be possible using <a href="https://github.com/erikriverson/org-mode-R-tutorial/blob/master/org-mode-R-tutorial.org">Org-mode</a>, which is an Emacs mode for writing notes, todos, and authoring documents. But I haven‚Äôt explored this option yet, mainly because in my case, only looking at one plot at a time is ok.
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_04_ggplot.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Here I show how to quickly add text to the top of the document when at the cursor is at the bottom: I try to use the <code>tabyl()</code> function found in the <code>{janitor}</code> package, which I forgot to load. I quickly go all the way up with <strong>gg</strong>, then key <strong>yy</strong> to copy the first line, then <strong>P</strong> to paste it on the line below (<strong>p</strong> would paste it on the same line), type <strong>fv</strong>, to <strong>f</strong>ind the letter v from the word ‚Äútidyverse‚Äù, then type <strong>liw</strong> (which is the B√âPO equivalent of <strong>ciw</strong> for <strong>C</strong>hange <strong>I</strong>n <strong>W</strong>ord) and finally change ‚Äútidyverse‚Äù to ‚Äújanitor‚Äù. This seems overly complex, but once you get used to this way of working, you will wonder why you hadn‚Äôt tried vi sooner.
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_05_janitor.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Here I show how to do block comment. <strong>8gg</strong> jumps to the 8th line, <strong>CTRLv</strong> starts block visual mode, which allows me to select a block of text. I select the first column of the text, <strong>G</strong> to jump all the way down, then <strong>A</strong> to enter insert mode at the end of the selection (actually, it would have been more logical to use <strong>I</strong>, which enters insert mode at the beginning of the selection) of the line and then add ‚Äú#‚Äù to comment.
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_06_block_comment.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Here I show how to delete a block of text:
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_07_block_delete.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Search and replace, by entering <em>command-line</em> mode (look at the very bottom of the window):
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_08_search_replace_undo.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
I forgot to add ‚Äú,‚Äù characters on a bunch of lines. I add the first ‚Äú,‚Äù to the first line, go down and press <strong>ESC</strong> to exit <em>Insert</em> mode. Now in <strong>Normal</strong> mode, I type <strong>.</strong> to execute the last command, which is <em>inserting a ‚Äú,‚Äù character and going down a line</em>. This <em>dot command</em> is a feature of vi, and it will always redo the last performed change.
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_09_dot.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
But instead of typing <strong>.</strong> six times, just type <strong>6.</strong> and be done with it:
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_09b_repeated_dot.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
What if you want to do something more complex, involving several commands? Here the <em>dot command</em> won‚Äôt be enough, since it only replicates the last command, not more. For this you can define macros with **<span class="citation"><span class="citation" data-cites="*">@*</span></span>*. I look for the ‚Äú,‚Äù character, twice, and put the rest of the characters in the next line with enter. I then repeat this operation by executing the macro using <strong>@‚Äå‚Äåa</strong> repeatedly (<strong>@‚Äå‚Äåa</strong> because I saved the actions in <strong>a</strong>, but it could have been any other letter). I then undo my changes and execute the macro 5 times with <strong>5@‚Äå‚Äåa</strong>.
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_10_macros.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Here I show the undo tree (by typing <strong>Spaceua</strong>), which is a feature Spacemacs inherited from Emacs: it makes undoing changes and going back to a previous version of your script very easily:
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_11_undo_tree.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
Finally, I show my Spacemacs configuration file. I show where one needs to specify the layers one wishes to use. For R, the ESS layer (which is a configuration file for the ESS Emacs package) is mandatory. As I explained above, it is also possible to use Emacs packages for which no layer is available. These are the packages under <code>dotspacemacs-additional-packages</code>. In my case I use:
</p>
<pre><code>dotspacemacs-additional-packages '(polymode
                                  poly-R
                                  poly-noweb
                                  poly-markdown)</code></pre>
<p>
which makes working with RMarkdown possible. <code>polymode</code> enables simultaneous Major modes, which is needed for RMarkdown (because RMarkdown files mix Markdown and R).
</p>
<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/spacemacs_12_config.mp4" type="video/mp4">
Your browser does not support the video tag. </video>
</div>
<p>
That‚Äôs the end of this long post. Spacemacs is really a joy to use, but the learning curve is quite steep. However, it is definitely worth it. There are so many packages available for Emacs (and hence Spacemacs) that allow you to browse the web, play games, listen to music, send and read emails‚Ä¶ that a recurrent joke is that Emacs is <em>a very nice operating system, but it lacks a decent editor</em>. If that‚Äôs the case, Spacemacs is the perfect operating system, because it includes the greatest editor, vi.
</p>
<p>
If you‚Äôre interested and and want to learn more about vi, I advise you to read the following book <a href="https://www.ossblog.org/wp-content/uploads/2017/06/vim-recipes.pdf">Vim Recipes</a> (pdf warning, free) or <a href="https://pragprog.com/book/dnvim2/practical-vim-second-edition">Practical Vim, Edit Text at the Speed of thought</a> (not free, but worth every cent), and <a href="https://leanpub.com/VimLikeAPro">Use Vim Like a Pro</a>, which I have not read, but it looks quite good, and is free too if you want. Now this only covers the vi part, not the Emacs aspects of Spacemacs, but you don‚Äôt really need to know about Emacs to use Spacemacs. I had 0 experience with Emacs, and still have 0 experience with it. I only learned how to configure Spacemacs, which does not require any previous experience. To find the packages you need, as usual, use any search engine of your liking.
</p>
<p>
The last point I want to address is the built-in Vim mode of Rstudio. While it works, it does not work 100% as regular Vim, and worst of all, does not support, as far as I know, any other keyboard layout than QWERTY, which is a nogo for me.
</p>
<p>
In any case, if you‚Äôre looking to learn something new that you can use for many programs, including Rstudio, learn Vim, and then give Spacemacs a try. Chaining keystrokes to edit text gets addictive very quickly.
</p>
<p>
For reference, here is my <code>dotspacemacs/user-config</code>, which is where I defined the shortcut for the <code>%&gt;%</code> operator.
</p>
<pre><code>(defun dotspacemacs/user-config ()
  "Configuration for user code:
This function is called at the very end of Spacemacs startup, after layer
configuration.
Put your configuration code here, except for variables that should be set
before packages are loaded."
;;; R modes
  (add-to-list 'auto-mode-alist '("\\.md" . poly-markdown-mode))
  (add-to-list 'auto-mode-alist '("\\.Snw" . poly-noweb+r-mode))
  (add-to-list 'auto-mode-alist '("\\.Rnw" . poly-noweb+r-mode))
  (add-to-list 'auto-mode-alist '("\\.Rmd" . poly-markdown+r-mode))

  ;; (require 'poly-R)
  ;; (require 'poly-markdown)
  ;; (add-to-list 'auto-mode-alist '("\\.Rmd" . poly-markdown+r-mode))

  (global-company-mode t)
  (global-hl-line-mode 1) ; Enable/Disable current line highlight
  (setq-default fill-column 99)
  (setq-default auto-fill-mode t)
  ;; ESS shortcuts
  (spacemacs/set-leader-keys "mdt" 'ess-r-devtools-test-package)
  (spacemacs/set-leader-keys "mrl" 'ess-eval-line)
  (spacemacs/set-leader-keys "mrr" 'ess-eval-region)
  (spacemacs/set-leader-keys "mdb" 'ess-r-devtools-build-package)
  (spacemacs/set-leader-keys "mdd" 'ess-r-devtools-document-package)
  (spacemacs/set-leader-keys "mdl" 'ess-r-devtools-load-package)
  (spacemacs/set-leader-keys "mdc" 'ess-r-devtools-check-package)
  (spacemacs/set-leader-keys "mdp" 'ess-r-package-mode)
  (add-hook 'ess-mode-hook
            (lambda ()
              (ess-toggle-underscore nil)))
  (define-key evil-normal-state-map (kbd "SPC mm")
            (lambda ()
              (interactive)
              (insert " %&gt;% ")
              (evil-insert-state)
              ))
  ;; Move lines around
  (spacemacs/set-leader-keys "MS" 'move-text-line-up)
  (spacemacs/set-leader-keys "MT" 'move-text-line-down)
  (setq-default whitespace-mode t)
  (setq-default whitespace-style (quote (spaces tabs newline space-mark tab-mark newline-mark)))
  (setq-default whitespace-display-mappings
        ;; all numbers are Unicode codepoint in decimal. try (insert-char 182 ) to see it
        '(
          (space-mark 32 [183] [46]) ; 32 SPACE, 183 MIDDLE DOT „Äå¬∑„Äç, 46 FULL STOP „Äå.„Äç
          (newline-mark 10 [9226 10]) ; 10 LINE FEED
          (tab-mark 9 [9655 9] [92 9]) ; 9 TAB, 9655 WHITE RIGHT-POINTING TRIANGLE „Äå‚ñ∑„Äç
          ))
  (setq-default TeX-view-program-selection
         '((output-pdf "PDF Viewer")))
  (setq-default TeX-view-program-list
        '(("PDF Viewer" "okular %o")))
  (setq-default indent-tabs-mode nil)
  (setq-default tab-width 2)
   ;; (setq org-default-notes-file (concat org-directory "/agenda/notes.org"))
   (add-hook 'prog-mode-hook 'spacemacs/toggle-fill-column-indicator-on)
   (add-hook 'text-mode-hook 'spacemacs/toggle-fill-column-indicator-on)
   (add-hook 'markdown-mode-hook 'spacemacs/toggle-fill-column-indicator-on)
  )</code></pre>


 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-05-19-spacemacs.html</guid>
  <pubDate>Sun, 19 May 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>For posterity: install {xml2} on GNU/Linux distros</title>
  <link>https://b-rodrigues.github.io/posts/2019-05-18-xml2.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Death_mask"> <img src="https://b-rodrigues.github.io/assets/img/napoleon_death_mask.jpg" title="I will probably be the only reader of this blog post"></a>
</p>
</div>
<p>
Today I‚Äôve removed my system‚Äôs R package and installed MRO instead. While re-installing all packages, I‚Äôve encountered one of the most frustrating error message for someone installing packages from source:
</p>
<pre><code>Error : /tmp/Rtmpw60aCp/R.INSTALL7819efef27e/xml2/man/read_xml.Rd:47: unable to load shared object
'/usr/lib64/R/library/xml2/libs/xml2.so': 
libicui18n.so.58: cannot open shared object file: No such file or directory ERROR: 
installing Rd objects failed for package ‚Äòxml2‚Äô </code></pre>
<p>
This library, <code>libicui18n.so.58</code> is a pain in the butt. However, you can easily install it if you install miniconda. After installing miniconda, you can look for it with:
</p>
<pre><code>[19-05-18 18:26] cbrunos in ~/ ‚û§ locate libicui18n.so.58

/home/cbrunos/miniconda3/lib/libicui18n.so.58
/home/cbrunos/miniconda3/lib/libicui18n.so.58.2
/home/cbrunos/miniconda3/pkgs/icu-58.2-h9c2bf20_1/lib/libicui18n.so.58
/home/cbrunos/miniconda3/pkgs/icu-58.2-h9c2bf20_1/lib/libicui18n.so.58.2
</code></pre>
<p>
So now you need to tell R where to look for this library. The <a href="https://stackoverflow.com/a/47851648">following Stackoverflow</a> answer saved the day. Add the following lines to <code>R_HOME/etc/ldpaths</code> (in my case, it was in <code>/opt/microsoft/ropen/3.5.2/lib64/R/etc/</code>):
</p>
<pre><code>LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/username/miniconda3/lib/
export LD_LIBRARY_PATH</code></pre>
<p>
and try to install <code>xml2</code> again, and it should work! If not, just abandon the idea of using R and switch to doing data science with VBA, it‚Äôll be less frustrating.
</p>
<p>
Something else, if you install Microsoft R Open, you‚Äôll be stuck with some older packages, because by default MRO uses a snapshot of CRAN from a given day as a mirror. To get the freshest packages, add the following line to your <code>.Rprofile</code> file (which should be located in your <code>HOME</code>):
</p>
<pre><code>options(repos = c(CRAN = "http://cran.rstudio.com/"))</code></pre>
<p>
And to finish this short blog post, add the following line to your <code>.Rprofile</code> if you get the following error messages when trying to install a package from github:
</p>
<pre><code>remotes::install_github('rstudio/DT') Downloading GitHub repo rstudio/DT@master tar: 
This does not look like a tar archive gzip: stdin: unexpected end of file tar: Child returned 
status 1 tar: Error is not recoverable: exiting now tar: This does not look like a tar archive 
gzip: stdin: unexpected end of file tar: Child returned status 1 tar: Error is not recoverable: 
exiting now Error in getrootdir(untar(src, list = TRUE)) : length(file_list) &gt; 0 is not TRUE Calls: 
&lt;Anonymous&gt; ... source_pkg -&gt; decompress -&gt; getrootdir -&gt; stopifnot In addition: Warning messages: 1: 
In utils::untar(tarfile, ...) : ‚Äòtar -xf '/tmp/RtmpitCFRe/file2677442609b8.tar.gz' -C 
'/tmp/RtmpitCFRe/remotes267752f2629f'‚Äô returned error code 2 2: 
In system(cmd, intern = TRUE) : running command 'tar -tf '/tmp/RtmpitCFRe/file2677442609b8.tar.gz'' 
had status 2 Execution halted</code></pre>
<p>
The solution, which can found <a href="https://github.com/r-lib/remotes/issues/350#issuecomment-493649792">here</a>
</p>
<pre><code>options("download.file.method" = "libcurl")</code></pre>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-05-18-xml2.html</guid>
  <pubDate>Sat, 18 May 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Fast food, causality and R packages, part 2</title>
  <link>https://b-rodrigues.github.io/posts/2019-05-04-diffindiff_part2.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Joke"> <img src="https://b-rodrigues.github.io/assets/img/distracted_economist.jpg" title="Soon, humanity will only communicate in memes"></a>
</p>
</div>
<p>
I am currently working on a package for the R programming language; its initial goal was to simply distribute the data used in the Card and Krueger 1994 paper that you can read <a href="http://davidcard.berkeley.edu/papers/njmin-aer.pdf">here</a> (PDF warning). However, I decided that I would add code to perform diff-in-diff.
</p>
<p>
In my <a href="https://www.brodrigues.co/blog/2019-04-28-diffindiff_part1/">previous blog post</a> I showed how to set up the structure of your new package. In this blog post, I will only focus on getting Card and Krueger‚Äôs data and prepare it for distribution. The next blog posts will focus on writing a function to perform difference-in-differences.
</p>
<p>
If you want to distribute data through a package, you first need to use the <code>usethis::use_data_raw()</code> function (as shown in part 1).
</p>
<p>
This creates a <code>data-raw</code> folder, and inside you will find the <code>DATASET.R</code> script. You can edit this script to prepare the data.
</p>
<p>
First, let‚Äôs download the data from Card‚Äôs website, unzip it and load the data into R. All these operations will be performed from R:
</p>
<pre class="r"><code>library(tidyverse)

tempfile_path &lt;- tempfile()

download.file("http://davidcard.berkeley.edu/data_sets/njmin.zip", destfile = tempfile_path)

tempdir_path &lt;- tempdir()

unzip(tempfile_path, exdir = tempdir_path)</code></pre>
<p>
To download and unzip a file from R, first, you need to define where you want to save the file. Because I am not interested in keeping the downloaded file, I use the <code>tempfile()</code> function to get a temporary file in my <code>/tmp/</code> folder (which is the folder that contains temporary files and folders in a GNU+Linux system). Then, using <code>download.file()</code> I download the file, and save it in my temporary file. I then create a temporary directory using <code>tempdir()</code> (the idea is the same as with <code>tempfile()</code>), and use this folder to save the files that I will unzip, using the <code>unzip()</code> function. This folder now contains several files:
</p>
<pre><code>check.sas
codebook
public.csv
read.me
survey1.nj
survey2.nj</code></pre>
<p>
<code>check.sas</code> is the SAS script Card and Krueger used. It‚Äôs interesting, because it is quite simple, quite short (170 lines long) and yet the impact of Card and Krueger‚Äôs research was and has been very important for the field of econometrics. This script will help me define my own functions. <code>codebook</code>, you guessed it, contains the variables‚Äô descriptions. I will use this to name the columns of the data and to write the dataset‚Äôs documentation.
</p>
<p>
<code>public.csv</code> is the data. It does not contain any column names:
</p>
<pre><code> 46 1 0 0 0 0 0 1 0 0  0 30.00 15.00  3.00   .    19.0   .   1    .  2  6.50 16.50  1.03  1.03  0.52  3  3 1 1 111792  1  3.50 35.00  3.00  4.30  26.0  0.08 1 2  6.50 16.50  1.03   .    0.94  4  4    
 49 2 0 0 0 0 0 1 0 0  0  6.50  6.50  4.00   .    26.0   .   0    .  2 10.00 13.00  1.01  0.90  2.35  4  3 1 1 111292  .  0.00 15.00  4.00  4.45  13.0  0.05 0 2 10.00 13.00  1.01  0.89  2.35  4  4    
506 2 1 0 0 0 0 1 0 0  0  3.00  7.00  2.00   .    13.0  0.37 0  30.0 2 11.00 10.00  0.95  0.74  2.33  3  3 1 1 111292  .  3.00  7.00  4.00  5.00  19.0  0.25 . 1 11.00 11.00  0.95  0.74  2.33  4  3    
 56 4 1 0 0 0 0 1 0 0  0 20.00 20.00  4.00  5.00  26.0  0.10 1   0.0 2 10.00 12.00  0.87  0.82  1.79  2  2 1 1 111492  .  0.00 36.00  2.00  5.25  26.0  0.15 0 2 10.00 12.00  0.92  0.79  0.87  2  2    
 61 4 1 0 0 0 0 1 0 0  0  6.00 26.00  5.00  5.50  52.0  0.15 1   0.0 3 10.00 12.00  0.87  0.77  1.65  2  2 1 1 111492  . 28.00  3.00  6.00  4.75  13.0  0.15 0 2 10.00 12.00  1.01  0.84  0.95  2  2    
 62 4 1 0 0 0 0 1 0 0  2  0.00 31.00  5.00  5.00  26.0  0.07 0  45.0 2 10.00 12.00  0.87  0.77  0.95  2  2 1 1 111492  .   .     .     .     .    26.0   .   0 2 10.00 12.00   .    0.84  1.79  3  3    </code></pre>
<p>
Missing data is defined by <code>.</code> and the delimiter is the space character. <code>read.me</code> is a README file. Finally, <code>survey1.nj</code> and <code>survey2.nj</code> are the surveys that were administered to the fast food restaurants‚Äô managers; one in February (before the raise) and the second one in November (after the minimum wage raise).
</p>
<p>
The next lines import the codebook:
</p>
<pre class="r"><code>codebook &lt;- read_lines(file = paste0(tempdir_path, "/codebook"))

variable_names &lt;- codebook %&gt;%
    `[`(8:59) %&gt;%
    `[`(-c(5, 6, 13, 14, 32, 33)) %&gt;%
    str_sub(1, 13) %&gt;%
    str_squish() %&gt;%
    str_to_lower()</code></pre>
<p>
Once I import the codebook, I select lines 8 to 59 using the <code><code>[</code>()</code> function. If you‚Äôre not familiar with this notation, try the following in a console:
</p>
<pre class="r"><code>seq(1, 100)[1:10]</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10</code></pre>
<p>
and compare:
</p>
<pre class="r"><code>seq(1, 100) %&gt;% 
  `[`(., 1:10)</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10</code></pre>
<p>
both are equivalent, as you can see. You can also try the following:
</p>
<pre class="r"><code>1 + 10</code></pre>
<pre><code>## [1] 11</code></pre>
<pre class="r"><code>1 %&gt;% 
  `+`(., 10)</code></pre>
<pre><code>## [1] 11</code></pre>
<p>
Using the same trick, I remove lines that I do not need, and then using <code>stringr::str_sub(1, 13)</code> I only keep the first 13 characters (which are the variable names, plus some white space characters) and then, to remove all the unneeded white space characters I use <code>stringr::squish()</code>, and then change the column names to lowercase.
</p>
<p>
I then load the data, and add the column names that I extracted before:
</p>
<pre class="r"><code>dataset &lt;- read_table2(paste0(tempdir_path, "/public.dat"),
                      col_names = FALSE)

dataset &lt;- dataset %&gt;%
    select(-X47) %&gt;%
    `colnames&lt;-`(., variable_names) %&gt;%
    mutate_all(as.numeric) %&gt;%
    mutate(sheet = as.character(sheet))</code></pre>
<p>
I use the same trick as before. I rename the 47th column, which is empty, I name the columns with <code><code>colnames&amp;lt;-</code>()</code>.
</p>
<p>
After this, I perform some data cleaning. It‚Äôs mostly renaming categories of categorical variables, and creating a ‚Äútrue‚Äù panel format. Several variables were measured at several points in time. Variables that were measured a second time have a ‚Äú2‚Äù at the end of their name. I remove these variables, and add an observation data variable. So my data as twice as many rows as the original data, but that format makes it way easier to work with. Below you can read the full code:
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code>dataset &lt;- dataset %&gt;%
    mutate(chain = case_when(chain == 1 ~ "bk",
                             chain == 2 ~ "kfc",
                             chain == 3 ~ "roys",
                             chain == 4 ~ "wendys")) %&gt;%
    mutate(state = case_when(state == 1 ~ "New Jersey",
                             state == 0 ~ "Pennsylvania")) %&gt;%
    mutate(region = case_when(southj == 1 ~ "southj",
              centralj == 1 ~ "centralj",
              northj == 1 ~ "northj",
              shore == 1 ~ "shorej",
              pa1 == 1 ~ "pa1",
              pa2 == 1 ~ "pa2")) %&gt;%
    mutate(meals = case_when(meals == 0 ~ "None",
                             meals == 1 ~ "Free meals",
                             meals == 2 ~ "Reduced price meals",
                             meals == 3 ~ "Both free and reduced price meals")) %&gt;%
    mutate(meals2 = case_when(meals2 == 0 ~ "None",
                             meals2 == 1 ~ "Free meals",
                             meals2 == 2 ~ "Reduced price meals",
                             meals2 == 3 ~ "Both free and reduced price meals")) %&gt;%
    mutate(status2 = case_when(status2 == 0 ~ "Refused 2nd interview",
                               status2 == 1 ~ "Answered 2nd interview",
                               status2 == 2 ~ "Closed for renovations",
                               status2 == 3 ~ "Closed permanently",
                               status2 == 4 ~ "Closed for highway construction",
                               status2 == 5 ~ "Closed due to Mall fire")) %&gt;%
    mutate(co_owned = if_else(co_owned == 1, "Yes", "No")) %&gt;%
    mutate(bonus = if_else(bonus == 1, "Yes", "No")) %&gt;%
    mutate(special2 = if_else(special2 == 1, "Yes", "No")) %&gt;%
    mutate(type2 = if_else(type2 == 1, "Phone", "Personal")) %&gt;%
    select(sheet, chain, co_owned, state, region, everything()) %&gt;%
    select(-southj, -centralj, -northj, -shore, -pa1, -pa2) %&gt;%
    mutate(date2 = lubridate::mdy(date2)) %&gt;%
    rename(open2 = open2r) %&gt;%
    rename(firstinc2 = firstin2)

dataset1 &lt;- dataset %&gt;%
    select(-ends_with("2"), -sheet, -chain, -co_owned, -state, -region, -bonus) %&gt;%
    mutate(type = NA_character_,
           status = NA_character_,
           date = NA)

dataset2 &lt;- dataset %&gt;%
    select(ends_with("2")) %&gt;%
    #mutate(bonus = NA_character_) %&gt;%
    rename_all(~str_remove(., "2"))

other_cols &lt;- dataset %&gt;%
    select(sheet, chain, co_owned, state, region, bonus)

other_cols_1 &lt;- other_cols %&gt;%
    mutate(observation = "February 1992")

other_cols_2 &lt;- other_cols %&gt;%
    mutate(observation = "November 1992")

dataset1 &lt;- bind_cols(other_cols_1, dataset1)
dataset2 &lt;- bind_cols(other_cols_2, dataset2)

njmin &lt;- bind_rows(dataset1, dataset2) %&gt;%
    select(sheet, chain, state, region, observation, everything())</code></pre>
</details>
<p>
The line I would like to comment is the following:
</p>
<pre class="r"><code>dataset %&gt;%
    select(-ends_with("2"), -sheet, -chain, -co_owned, -state, -region, -bonus)</code></pre>
<p>
This select removes every column that ends with the character ‚Äú2‚Äù (among others). I split the data in two, to then bind the rows together and thus create my long dataset. I then save the data into the <code>data/</code> folder:
</p>
<pre class="r"><code>usethis::use_data(njmin, overwrite = TRUE)</code></pre>
<p>
This saves the data as an <code>.rda</code> file. To enable users to read the data by typing <code>data(‚Äúnjmin‚Äù)</code>, you need to create a <code>data.R</code> script in the <code>R/</code> folder. You can read my <code>data.R</code> script below:
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code>#' Data from the Card and Krueger 1994 paper *Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania*
#'
#' This dataset was downloaded and distributed with the permission of David Card. The original
#' data contains 410 observations and 46 variables. The data distributed in this package is
#' exactly the same, but was changed from a wide to a long dataset, which is better suited for
#' manipulation with *tidyverse* functions.
#'
#' @format A data frame with 820 rows and 28 variables:
#' \describe{
#'   \item{\code{sheet}}{Sheet number (unique store id).}
#'   \item{\code{chain}}{The fastfood chain: bk is Burger King, kfc is Kentucky Fried Chicken, wendys is Wendy's, roys is Roy Rogers.}
#'   \item{\code{state}}{State where the restaurant is located.}
#'   \item{\code{region}}{pa1 is northeast suburbs of Phila, pa2 is Easton etc, centralj is central NJ, northj is northern NJ, southj is south NJ.}
#'   \item{\code{observation}}{Date of first (February 1992) and second (November 1992) observation.}
#'   \item{\code{co_owned}}{"Yes" if company owned.}
#'   \item{\code{ncalls}}{Number of call-backs. Is 0 if contacted on first call.}
#'   \item{\code{empft}}{Number full-time employees.}
#'   \item{\code{emppt}}{Number part-time employees.}
#'   \item{\code{nmgrs}}{Number of managers/assistant managers.}
#'   \item{\code{wage_st}}{Starting wage ($/hr).}
#'   \item{\code{inctime}}{Months to usual first raise.}
#'   \item{\code{firstinc}}{Usual amount of first raise (\$/hr).}
#'   \item{\code{bonus}}{"Yes" if cash bounty for new workers.}
#'   \item{\code{pctaff}}{\% of employees affected by new minimum.}
#'   \item{\code{meals}}{Free/reduced priced code.}
#'   \item{\code{open}}{Hour of opening.}
#'   \item{\code{hrsopen}}{Number of hours open per day.}
#'   \item{\code{psode}}{Price of medium soda, including tax.}
#'   \item{\code{pfry}}{Price of small fries, including tax.}
#'   \item{\code{pentree}}{Price of entree, including tax.}
#'   \item{\code{nregs}}{Number of cash registers in store.}
#'   \item{\code{nregs11}}{Number of registers open at 11:00 pm.}
#'   \item{\code{type}}{Type of 2nd interview.}
#'   \item{\code{status}}{Status of 2nd interview.}
#'   \item{\code{date}}{Date of 2nd interview.}
#'   \item{\code{nregs11}}{"Yes" if special program for new workers.}
#' }
#' @source \url{http://davidcard.berkeley.edu/data_sets.html}
"njmin"</code></pre>
</details>
<p>
I have documented the data, and using <code>roxygen2::royxgenise()</code> to create the dataset‚Äôs documentation.
</p>
<p>
The data can now be used to create some nifty plots:
</p>
<pre class="r"><code>ggplot(njmin, aes(wage_st)) + geom_density(aes(fill = state), alpha = 0.3) +
    facet_wrap(vars(observation)) + theme_blog() +
    theme(legend.title = element_blank(), plot.caption = element_text(colour = "white")) +
    labs(title = "Distribution of starting wage rates in fast food restaurants",
         caption = "On April 1st, 1992, New Jersey's minimum wage rose from $4.25 to $5.05. Source: Card and Krueger (1994)")</code></pre>
<pre><code>## Warning: Removed 41 rows containing non-finite values (stat_density).</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/diffindiff_part2-6-1.png" width="672">
</p>
<p>
In the next blog post, I am going to write a first function to perform diff and diff, and we will learn how to make it available to users, document and test it!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-05-04-diffindiff_part2.html</guid>
  <pubDate>Sat, 04 May 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Fast food, causality and R packages, part 1</title>
  <link>https://b-rodrigues.github.io/posts/2019-04-28-diffindiff_part1.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Joke"> <img src="https://b-rodrigues.github.io/assets/img/distracted_economist.jpg" title="Soon, humanity will only communicate in memes"></a>
</p>
</div>
<p>
I am currently working on a package for the R programming language; its initial goal was to simply distribute the data used in the Card and Krueger 1994 paper that you can read <a href="http://davidcard.berkeley.edu/papers/njmin-aer.pdf">here</a> (PDF warning).
</p>
<p>
The gist of the paper is to try to answer the following question: <em>Do increases in minimum wages reduce employment?</em> According to Card and Krueger‚Äôs paper from 1994, no. The authors studied a change in legislation in New Jersey which increased the minimum wage from $4.25 an hour to $5.05 an hour. The neighbourghing state of Pennsylvania did not introduce such an increase. The authors thus used the State of Pennsylvania as a control for the State of New Jersey and studied how the increase in minimum wage impacted the employment in fast food restaurants and found, against what economic theory predicted, an increase and not a decrease in employment. The authors used a method called difference-in-differences to asses the impact of the minimum wage increase.
</p>
<p>
This result was and still is controversial, with subsequent studies finding subtler results. For instance, showing that there is a reduction in employment following an increase in minimum wage, but only for large restaurants (see Ropponen and Olli, 2011).
</p>
<p>
Anyways, this blog post will discuss how to create a package using to distribute the data. In a future blog post, I will discuss preparing the data to make it available as a demo dataset inside the package, and then writing and documenting functions.
</p>
<p>
The first step to create a package, is to create a new project:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/package_01.png"><!-- -->
</p>
<p>
Select ‚ÄúNew Directory‚Äù:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/package_02.png"><!-- -->
</p>
<p>
Then ‚ÄúR package‚Äù:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/package_03.png"><!-- -->
</p>
<p>
and on the window that appears, you can choose the name of the package, as well as already some starting source files:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/package_04.png"><!-- -->
</p>
<p>
Also, I‚Äôd highly recommend you click on the ‚ÄúCreate a git repository‚Äù box and use git within your project for reproducibility and sharing your code more easily. If you do not know git, there‚Äôs a lot of online resources to get you started. It‚Äôs not super difficult, but it does require making some new habits, which can take some time.
</p>
<p>
I called my package <code>{diffindiff}</code>, and clicked on ‚ÄúCreate Project‚Äù. This opens up a new project with a <code>hello.R</code> script, which gives you some pointers:
</p>
<pre><code># Hello, world!
#
# This is an example function named 'hello' 
# which prints 'Hello, world!'.
#
# You can learn more about package authoring with RStudio at:
#
#   http://r-pkgs.had.co.nz/
#
# Some useful keyboard shortcuts for package authoring:
#
#   Install Package:           'Ctrl + Shift + B'
#   Check Package:             'Ctrl + Shift + E'
#   Test Package:              'Ctrl + Shift + T'

hello &lt;- function() {
  print("Hello, world!")
}</code></pre>
<p>
Now, to simplify the creation of your package, I highly recommend you use the <code>{usethis}</code> package. <code>{usethis}</code> removes a lot of the pain involved in creating packages.
</p>
<p>
For instance, want to start by adding a README file? Simply run:
</p>
<pre class="r"><code>usethis::use_readme_md()</code></pre>
<pre class="r"><code>‚úî Setting active project to '/path/to/your/package/diffindiff'
‚úî Writing 'README.md'
‚óè Modify 'README.md'</code></pre>
<p>
This creates a <code>README.md</code> file in the root directory of your package. Simply change that file, and that‚Äôs it.
</p>
<p>
The next step could be setting up your package to work with <code>{roxygen2}</code>, which is very useful for writing documentation:
</p>
<pre class="r"><code>usethis::use_roxygen_md()</code></pre>
<pre class="r"><code>‚úî Setting Roxygen field in DESCRIPTION to 'list(markdown = TRUE)'
‚úî Setting RoxygenNote field in DESCRIPTION to '6.1.1'
‚óè Run `devtools::document()`</code></pre>
<p>
See how the output tells you to run <code>devtools::document()</code>? This function will document your package, transforming the comments you write to describe your functions to documentation and managing the NAMESPACE file. Let‚Äôs run this function too:
</p>
<pre class="r"><code>devtools::document()</code></pre>
<pre class="r"><code>Updating diffindiff documentation
First time using roxygen2. Upgrading automatically...
Loading diffindiff
Warning: The existing 'NAMESPACE' file was not generated by roxygen2, and will not be overwritten.</code></pre>
<p>
You might have a similar message than me, telling you that the NAMESPACE file was not generated by <code>{roxygen2}</code>, and will thus not be overwritten. Simply remove the file and run <code>devtools::document()</code> again:
</p>
<pre class="r"><code>devtools::document()</code></pre>
<pre class="r"><code>Updating diffindiff documentation
First time using roxygen2. Upgrading automatically...
Writing NAMESPACE
Loading diffindiff</code></pre>
<p>
But what is actually the NAMESPACE file? This file is quite important, as it details where your package‚Äôs functions have to look for in order to use other functions. This means that if your package needs function <code>foo()</code> from package <code>{bar}</code>, it will consistently look for <code>foo()</code> inside <code>{bar}</code> and not confuse it with, say, the <code>foo()</code> function from the <code>{barley}</code> package, even if you load <code>{barley}</code> after <code>{bar}</code> in your interactive session. This can seem confusing now, but in the next blog posts I will detail this, and you will see that it‚Äôs not that difficult. Just know that it is an important file, and that you do not have to edit it by hand.
</p>
<p>
Next, I like to run the following:
</p>
<pre class="r"><code>usethis::use_pipe()</code></pre>
<pre class="r"><code>‚úî Adding 'magrittr' to Imports field in DESCRIPTION
‚úî Writing 'R/utils-pipe.R'
‚óè Run `devtools::document()`</code></pre>
<p>
This makes the now famous <code>%&gt;%</code> function available internally to your package (so you can use it to write the functions that will be included in your package) but also available to the users that will load the package.
</p>
<p>
Your package is still missing a license. If you plan on writing a package for your own personal use, for instance, a collection of functions, there is no need to think about licenses. But if you‚Äôre making your package available through CRAN, then you definitely need to think about it. For this package, I‚Äôll be using the MIT license, because the package will distribute data which I do not own (I‚Äôve got permission from Card to re-distribute it) and thus I think it would be better to use a permissive license (I don‚Äôt know if the GPL, another license, which is stricter in terms of redistribution, could be used in this case).
</p>
<pre class="r"><code>usethis::use_mit_license()</code></pre>
<pre class="r"><code>‚úî Setting License field in DESCRIPTION to 'MIT + file LICENSE'
‚úî Writing 'LICENSE.md'
‚úî Adding '^LICENSE\\.md$' to '.Rbuildignore'
‚úî Writing 'LICENSE'</code></pre>
<p>
We‚Äôre almost done setting up the structure of the package. If we forget something though, it‚Äôs not an issue, we‚Äôll just have to run the right <code>use_*</code> function later on. Let‚Äôs finish by preparing the folder that will contains the script to prepare the data:
</p>
<pre class="r"><code>usethis::use_data_raw()</code></pre>
<pre class="r"><code>‚úî Creating 'data-raw/'
‚úî Adding '^data-raw$' to '.Rbuildignore'
‚úî Writing 'data-raw/DATASET.R'
‚óè Modify 'data-raw/DATASET.R'
‚óè Finish the data preparation script in 'data-raw/DATASET.R'
‚óè Use `usethis::use_data()` to add prepared data to package</code></pre>
<p>
This creates the <code>data-raw</code> folder with the <code>DATASET.R</code> script inside. This is the script that will contain the code to download and prepare datasets that you want to include in your package. This will be the subject of the next blog post.
</p>
<p>
Let‚Äôs now finish by documenting the package, and pushing everything to Github:
</p>
<pre class="r"><code>devtools::document()</code></pre>
<p>
The following lines will only work if you set up the Github repo:
</p>
<pre><code>git add .
git commit -am "first commit"
git push origin master</code></pre>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-04-28-diffindiff_part1.html</guid>
  <pubDate>Sun, 28 Apr 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Historical newspaper scraping with {tesseract} and R</title>
  <link>https://b-rodrigues.github.io/posts/2019-04-07-historical_newspaper_scraping_tesseract.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Cliometrics"> <img src="https://b-rodrigues.github.io/assets/img/clio.jpg" title="Historical newspapers as a source to practice cliometrics?"></a>
</p>
</div>
<p>
I have been playing around with historical newspapers data for some months now. The ‚Äúobvious‚Äù type of analysis to do is NLP, but there is also a lot of numerical data inside historical newspapers. For instance, you can find these tables that show the market prices of the day in the <em>L‚ÄôInd√©pendance Luxembourgeoise</em>:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/market_price_table.png"><!-- -->
</p>
<p>
I wanted to see how easy it was to extract these tables from the newspapers and then make it available. It was a bit more complicated than anticipated.
</p>
<section id="download-data" class="level2">
<h2 class="anchored" data-anchor-id="download-data">
Download data
</h2>
<p>
The first step is to download the data. For this, I have used the code <a href="https://twitter.com/yvesmaurer"><code><span class="citation" data-cites="yvesmaurer">@yvesmaurer</span></code></a> which you can find <a href="https://github.com/ymaurer/eluxemburgensia-opendata-ark">here</a>. This code makes it easy to download individual pages of certain newspapers, for instance <a href="https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F1drtkj%2Fpages%2F1/full/full/0/default.jpg">this one</a>. The pages I am interested in are pages 3, which contain the tables I need, for example <a href="https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F1drtkj%2Fpages%2F3/full/full/0/default.jpg">here</a>. <a href="https://twitter.com/yvesmaurer"><code><span class="citation" data-cites="yvesmaurer">@yvesmaurer</span></code></a>‚Äôs code makes it easy to find the download links, which look like this: <code>https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F1drtkj%2Fpages%2F3/full/full/0/default.jpg</code>. It is also possible to crop the image by changing some parameters <a href="https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fwsvhwh%2Fpages%2F3/pct:74,0,100,100/full/0/default.jpg">like so</a>. This is helpful, because it makes the image smaller. The tables I‚Äôm interested in are always in the last column, so I can can use this feature to get smaller images. However, not every issue contains these tables, and I only want to download the ones that have these tables. So I wrote the following code to download the images I‚Äôm interested in:
</p>
<pre class="r"><code>library(tidyverse)
library(magick)
library(tesseract)
library(furrr)

download_image &lt;- function(link){

    print(link)

    isok &lt;- image_read(link) %&gt;%
        ocr(engine = "fra") %&gt;%
        str_to_lower() %&gt;%
        str_detect("march√© de luxembourg")

    if(isok){
        date_link &lt;- link %&gt;%
            str_replace("pages%2f3", "pages%2f1") %&gt;%
            str_replace("pct:74,0,100,100", "pct:76,1,17,5")

        paper_date &lt;- image_read(date_link) %&gt;%
            ocr(engine = "fra") %&gt;%
            str_squish() %&gt;%
            str_remove("%") %&gt;%
            str_remove("&amp;") %&gt;%
            str_remove("/")

        ark &lt;- link %&gt;%
            str_sub(53, 60)

        download.file(link, paste0("indep_pages/", ark, "-", paper_date, ".jpg"))
    } else {
        NULL
        }
}</code></pre>
<p>
This code only downloads an image if the <code>ocr()</code> from the {tesseract} (which does, you guessed it, OCR) detects the string ‚Äúmarch√© de luxembourg‚Äù which is the title of the tables. This is a bit extreme, because if a single letter cannot be correctly detected by the OCR, the page will not be downloaded. But I figured that if this string could not be easily recognized, this would be a canary telling me that the text inside the table would also not be easily recognized. So it might be extreme, but my hope was that it would make detecting the table itself easier. Turned out it wasn‚Äôt so easy, but more on this later.
</p>
</section>
<section id="preparing-images" class="level2">
<h2 class="anchored" data-anchor-id="preparing-images">
Preparing images
</h2>
<p>
Now that I have the images, I will prepare them to make character recognition easier. To do this, I‚Äôm using the <code>{magick}</code> package:
</p>
<pre class="r"><code>library(tidyverse)
library(magick)
library(tesseract)
library(furrr)

prepare_image &lt;- function(image_path){
    image &lt;- image_read(image_path)

    image &lt;- image %&gt;%
        image_modulate(brightness = 150) %&gt;%
        image_convolve('DoG:0,0,2', scaling = '1000, 100%') %&gt;%
        image_despeckle(times = 10)

    image_write(image, paste0(getwd(), "/edited/", str_remove(image_path, ".jpg"), "edited.jpg"))
}


image_paths &lt;- dir(path = "indep_pages", pattern = "*.jpg", full.names = TRUE)

plan(multiprocess, workers = 8)

image_paths %&gt;%
    future_map(prepare_image)</code></pre>
<p>
The picture below shows the result:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/table_and_edit.jpg"><!-- -->
</p>
<p>
Now comes the complicated part, which is going from the image above, to the dataset below:
</p>
<pre><code>good_fr,good_en,unit,market_date,price,source_url
Froment,Wheat,hectolitre,1875-08-28,23,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
M√©tail,Meslin,hectolitre,1875-08-28,21,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Seigle,Rye,hectolitre,1875-08-28,15,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Orge,Barley,hectolitre,1875-08-28,16,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Orge mond√©,Pot Barley,kilogram,1875-08-28,0.85,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Orge perl√©,Pearl barley,kilogram,1875-08-28,0.8,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Avoine,Oats,hectolitre,1875-08-28,8.5,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Pois,Peas,hectolitre,1875-08-28,NA,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg</code></pre>
</section>
<section id="ocr-with-tesseract" class="level2">
<h2 class="anchored" data-anchor-id="ocr-with-tesseract">
OCR with {tesseract}
</h2>
<p>
The first step was to get the date. For this, I have used the following function, which will then be used inside another function, which will extract the data and prices.
</p>
<pre class="r"><code>library(tidyverse)
library(lubridate)
library(magick)
library(tesseract)
library(furrr)
library(janitor)

is_empty_line &lt;- function(line){
    ifelse(line == "", TRUE, FALSE)
}

Sys.setlocale('LC_TIME', "fr_FR")

get_date &lt;- function(string, annee){

    liste_mois &lt;- c("janvier", "f√©vrier", "mars", "avril", "mai", "juin", "juillet",
                    "ao√ªt", "septembre", "octobre", "novembre", "d√©cembre")

    raw_date &lt;- string %&gt;%
      str_to_lower() %&gt;%
        str_remove_all("\\.") %&gt;%
        str_extract("\\d{1,2} .{3,9}(\\s+)?\\d{0,4}") %&gt;%
        str_split("\\s+", simplify = TRUE)

    if(ncol(raw_date) == 2){
        raw_date &lt;- cbind(raw_date, "annee")
    }

    raw_date[1, 3] &lt;- annee

    raw_date &lt;- str_to_lower(raw_date[1:1, 1:3])

    long_month &lt;- case_when(
      raw_date[2] == "janv" ~ "janvier",
      raw_date[2] == "f√©vr" ~ "f√©vrier",
      raw_date[2] == "sept" ~ "septembre",
      raw_date[2] == "oct" ~ "octobre",
      raw_date[2] == "nov" ~ "novembre",
      raw_date[2] == "dec" ~ "d√©cembre",
      TRUE ~ as.character(raw_date[2]))

    raw_date[2] &lt;- long_month

    is_it_date &lt;- as.Date(paste0(raw_date, collapse = "-"), format = "%d-%b-%Y") %&gt;%
        is.na() %&gt;% `!`()

    if(is_it_date){
        return(as.Date(paste0(raw_date, collapse = "-"), format = "%d-%b-%Y"))
    } else {
        if(!(raw_date[2] %in% liste_mois)){
            raw_date[2] &lt;- liste_mois[stringdist::amatch(raw_date[2], liste_mois, maxDist = 2)]
            return(as.Date(paste0(raw_date, collapse = "-"), format = "%d-%b-%Y"))
        }
    }
}</code></pre>
<p>
This function is more complicated than I had hoped. This is because dates come in different formats. For example, there are dates written like this ‚Äú21 Janvier 1872‚Äù, or ‚Äú12 Septembre‚Äù or ‚Äú12 sept.‚Äù. The biggest problem here is that sometimes the year is missing. I deal with this in the next function, which is again, more complicated than what I had hoped. I won‚Äôt go into details and explain every step of the function above, but the idea is to extract the data from the raw text, replace abbreviated months with the full month name if needed, and then check if I get a valid date. If not, I try my luck with <code>stringdist::amatch()</code>, to try to match, say ‚Äújonvier‚Äù with ‚Äújanvier‚Äù. This is in case the OCR made a mistake. I am not very happy with this solution, because it is very approximative, but oh well.
</p>
<p>
The second step is to get the data. I noticed that the rows stay consistent, but do change after June 1st 1876. So I simply hardcoded the goods names, and was only concerned with extracting the prices. I also apply some manual corrections inside the function; mainly dates that were wrongly recognized by the OCR engine, and which were causing problems. Again, not an optimal solution, the other alternative was to simply drop this data, which I did not want to do. Here is the function:
</p>
<pre class="r"><code>extract_table &lt;- function(image_path){

  image &lt;- image_read(image_path)

  annee &lt;- image_path %&gt;%
    str_extract("187\\d")

  ark &lt;- image_path %&gt;%
    str_sub(22, 27)

  source_url &lt;- str_glue("https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F{ark}%2Fpages%2F1/full/full/0/default.jpg",
                         ark = ark)

  text &lt;- ocr(image, engine = "fra")

    text &lt;- text %&gt;%
      str_split("\n") %&gt;%
      unlist %&gt;%
      str_squish() %&gt;%
      str_remove_all("^.{1,10}$") %&gt;%
      discard(is_empty_line) %&gt;%
      str_replace("Mercuriale du \\+ Nov. 1831.", "Mercuriale du 4 Nov. 1831.") %&gt;%
      str_replace("‚Ä¶.u .T juillet.", "du 7 juillet") %&gt;%
      str_replace("octobr√©", "octobre") %&gt;%
      str_replace("AT octobre", "17 octobre") %&gt;% # correction for "f8g6kq8-18  LUNDI 19 OCTOBR√â 1874. BUREAUX de fa R√âDACTIGedited.jpg"
      str_replace("T norembre", "7 novembre") %&gt;%  # correction for fcrhrn5-LE 8  LUNDI 9 NOVEMBRE 1874 BUREAUX de la R√âDedited.jpg
      str_replace("√Ä oc demain 5", "27 mai") %&gt;% # correction for fd61vzp-MARDI 50. MAI 1876 BUREAUX de la. RED, n VE DE L‚ÄôADMINISTRAedited.jpg
      str_replace("G", "6") %&gt;%
      str_replace("Hercariale du 80 nov. 1872,", "du 30 novembre 1872") %&gt;%
      str_replace("‚Ä¶.u .T juillet.", "du 7 juillet") %&gt;%
      str_replace("Rs ne its du 28-octobr√©.: :!: :", "28 octobre") %&gt;%
      str_replace("De routes due 98-juill√©le. √† eat", "28 juillet") %&gt;%
      str_replace("\\| Mereariale dn 14 dre. 1872,", "14 d√©cembre 1872")


  start &lt;- text %&gt;%
    str_which("MARCH(√â|E).*D(E|√â).*LUXEMBOUR(G|6)") + 2

  start &lt;- ifelse(is_empty(start), str_which(text, ".*D.*UXEM.*") + 2, start)

  end &lt;- start + 40

  pricing_date &lt;- text[start - 1] %&gt;%
    str_remove("%") %&gt;%
    str_remove("er") %&gt;%
    str_remove("\\.+") %&gt;%
    str_remove("\\*") %&gt;%
    str_remove("¬Æ") %&gt;%
    str_remove(":") %&gt;%
    str_remove("\\?") %&gt;%
    str_replace("\\$", "9") %&gt;%
    str_remove("¬∞") %&gt;%
    str_replace("‚Äòdu 14ao√ªt.. - ; En", "14 ao√ªt") %&gt;%
    str_replace("OP PE CN AP PP", "du 28 juin") %&gt;%
    str_replace("‚Äò du 81 janvi Le", "31 janvier") %&gt;%
    str_replace("\\| \\| du AT ao√ªt", "17 ao√ªt") %&gt;%
    str_replace("Su‚Äù  du 81 juillet. L", "31 juillet") %&gt;%
    str_replace("0 du 29 avril \" \\|", "29 avril") %&gt;%
    str_replace("LU 0 du 28 ail", "28 avril") %&gt;%
    str_replace("Rs ne its du 28-octobre :!: :", "23 octobre") %&gt;%
    str_replace("7 F \\|  du 13 octobre LA LOTS", "13 octobre") %&gt;%
    str_replace("√Ä. du 18 juin UT ET", "13 juin")


  market_date &lt;- get_date(pricing_date, annee)

  items &lt;- c("Froment", "M√©tail", "Seigle", "Orge", "Orge mond√©", "Orge perl√©", "Avoine", "Pois", "Haricots",
             "Lentilles", "Pommes de terre", "Bois de h√™tre", "Bois de ch√™ne", "Beurre", "Oeufs", "Foin",
             "Paille", "Viande de boeuf", "Viande de vache", "Viande de veau", "Viande de mouton",
             "Viande fra√Æche de cochon", "Viande fum√©e de cochon", "Haricots", "Pois", "Lentilles",
             "Farines de froment", "Farines de m√©teil", "Farines de seigle")

  items_en &lt;- c("Wheat", "Meslin", "Rye", "Barley", "Pot Barley", "Pearl barley", "Oats", "Peas", "Beans",
    "Lentils", "Potatoes", "Beech wood", "Oak wood", "Butter", "Eggs", "Hay", "Straw", "Beef meat",
    "Cow meat", "Veal meat", "Sheep meat", "Fresh pig meat", "Smoked pig meat", "Beans", "Peas",
    "Lentils", "Wheat flours", "Meslin flours", "Rye flours")


  unit &lt;- c("hectolitre", "hectolitre", "hectolitre", "hectolitre", "kilogram", "kilogram", "hectolitre",
            "hectolitre", "hectolitre", "hectolitre", "hectolitre", "stere", "stere", "kilogram", "dozen",
            "500 kilogram", "500 kilogram", "kilogram", "kilogram", "kilogram", "kilogram", "kilogram",
            "kilogram", "litre", "litre", "litre", "kilogram", "kilogram", "kilogram")

  # starting with june 1876, the order of the items changes
  items_06_1876 &lt;- c("Froment", "M√©tail", "Seigle", "Orge", "Avoine", "Pois", "Haricots", "Lentilles",
                     "Pommes de terre", "Farines de froment", "Farines de m√©teil", "Farines de seigle", "Orge mond√©",
                     "Beurre", "Oeufs", "Foins", "Paille", "Bois de h√™tre", "Bois de ch√™ne", "Viande de boeuf", "Viande de vache",
                     "Viande de veau", "Viande de mouton", "Viande fra√Æche de cochon", "Viande fum√©e de cochon")

  items_06_1876_en &lt;- c("Wheat", "Meslin", "Rye", "Barley", "Oats", "Peas", "Beans", "Lentils",
                        "Potatoes", "Wheat flours", "Meslin flours", "Rye flours", "Pot barley",
                        "Butter", "Eggs", "Hay", "Straw", "Beechwood", "Oakwood", "Beef meat", "Cow meat",
                        "Veal meat", "Sheep meat", "Fresh pig meat", "Smoked pig meat")

  units_06_1876 &lt;- c(rep("hectolitre", 9), rep("kilogram", 5), "douzaine", rep("500 kilogram", 2),
                     "stere", "stere", rep("kilogram", 6))

  raw_data &lt;- text[start:end]

  prices &lt;- raw_data %&gt;%
    str_replace_all("¬©", "0") %&gt;%
    str_extract("\\d{1,2}\\s\\d{2}") %&gt;%
    str_replace("\\s", "\\.") %&gt;%
    as.numeric

  if(is.na(prices[1])){
    prices &lt;- tail(prices, -1)
  } else {
    prices &lt;- prices
  }

  if(market_date &lt; as.Date("01-06-1876", format = "%d-%m-%Y")){
    prices &lt;- prices[1:length(items)]
    tibble("good_fr" = items, "good_en" = items_en, "unit" = unit, "market_date" = market_date,
           "price" = prices, "source_url" = source_url)
  } else {
    prices &lt;- prices[1:length(items_06_1876_en)]
    tibble("good_fr" = items_06_1876, "good_en" = items_06_1876_en, "unit" = units_06_1876,
           "market_date" = market_date, "price" = prices, "source_url" = source_url)
  }
}</code></pre>
<p>
As I wrote previously, I had to deal with the missing year in the date inside this function. To do that, I extracted the year from the name of the file, and pasted it then into the date. The file name contains the data because the function in the function that downloads the files I also performed OCR on the first page, to get the date of the newspaper issue. The sole purpose of this was to get the year. Again, the function is more complex than what I hoped, but it did work well overall. There are still mistakes in the data, for example sometimes the prices are in the wrong order; meaning that they‚Äôre ‚Äúshifted‚Äù, for example instead of the prices for eggs, I have the prices of the good that comes next. So obviously be careful if you decide to analyze the data, and double-check if something seems weird. I have made the data available on Luxembourg Open Data Portal, <a href="https://data.public.lu/fr/datasets/digitised-luxembourg-historical-newspapers-journaux-historiques-luxembourgeois-numerises/#resource-community-27293c42-22e5-4811-aee8-89d6f7fa9533">here</a>.
</p>
</section>
<section id="analyzing-the-data" class="level2">
<h2 class="anchored" data-anchor-id="analyzing-the-data">
Analyzing the data
</h2>
<p>
And now, to the fun part. I want to know what was the price of smoked pig meat, and how it varied through time:
</p>
<pre class="r"><code>library(tidyverse)
library(ggplot2)
library(brotools)</code></pre>
<pre class="r"><code>market_price &lt;- read_csv("https://download.data.public.lu/resources/digitised-luxembourg-historical-newspapers-journaux-historiques-luxembourgeois-numerises/20190407-183605/market-price.csv")</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   good_fr = col_character(),
##   good_en = col_character(),
##   unit = col_character(),
##   market_date = col_date(format = ""),
##   price = col_double(),
##   source_url = col_character()
## )</code></pre>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Smoked pig meat") %&gt;%
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of smoked pig meat at the Luxembourg-City market in the 19th century")</code></pre>
<pre><code>## Warning: Removed 2 rows containing missing values (geom_path).</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-9-1.png" width="672">
</p>
<p>
As you can see, there is a huge spike somewhere in 1874. Maybe there was a very severe smoked pig meat shortage that caused the prices to increase dramatically, but the more likely explanation is that there was some sort of mistake, either in the OCR step, or when I extracted the prices, and somehow that particular price of smoked pig meat is actually the price of another, more expensive good.
</p>
<p>
So let‚Äôs only consider prices that are below, say, 20 franks, which is already very high:
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Smoked pig meat") %&gt;%
    filter(price &lt; 20) %&gt;% 
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of smoked pig meat at the Luxembourg-City market in the 1870s")</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-10-1.png" width="672">
</p>
<p>
Now, some prices are very high. Let‚Äôs check if it‚Äôs a mistake:
</p>
<pre class="r"><code>market_price %&gt;% 
    filter(good_en == "Smoked pig meat") %&gt;% 
    filter(between(price, 5, 20)) %&gt;% 
    pull(source_url)</code></pre>
<pre><code>## [1] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fbs2fs6%2Fpages%2F1/full/full/0/default.jpg"
## [2] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fd61vzp%2Fpages%2F1/full/full/0/default.jpg"
## [3] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fjdwb6m%2Fpages%2F1/full/full/0/default.jpg"
## [4] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fng14m3%2Fpages%2F1/full/full/0/default.jpg"
## [5] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fw9jdrb%2Fpages%2F1/full/full/0/default.jpg"</code></pre>
<p>
If you go to the first url, you will land on the first page of the newspaper. To check the table, you need to check the third page, by changing this part of the url ‚Äúpages%2F1‚Äù to this ‚Äúpages%2F3‚Äù.
</p>
<p>
You will then find the following:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/price_smoked_pig.png"><!-- -->
</p>
<p>
As you can see, the price was 2.5, but the OCR returned 7.5. This is a problem that is unavoidable with OCR; there is no way of knowing a priori if characters were not well recognized. It is actually quite interesting how the price for smoked pig meat stayed constant through all these years. A density plot shows that most prices were around 2.5:
</p>
<pre class="r"><code>market_price %&gt;% 
    filter(good_en == "Smoked pig meat") %&gt;% 
    filter(price &lt; 20) %&gt;% 
    ggplot() + 
    geom_density(aes(price), colour = "#82518c") + 
    theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-13-1.png" width="672">
</p>
<p>
What about another good, say, barley?
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Barley") %&gt;%
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of barley at the Luxembourg-City market in the 1870s")</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-14-1.png" width="672">
</p>
<p>
Here again, we see some very high spikes, most likely due to errors. Let‚Äôs try to limit the prices to likely values:
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Barley") %&gt;%
    filter(between(price, 10, 40)) %&gt;%
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of barley at the Luxembourg-City market in the 1870s")</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-15-1.png" width="672">
</p>
<pre class="r"><code>market_price %&gt;% 
    filter(good_en == "Barley") %&gt;% 
    ggplot() + 
    geom_density(aes(price), colour = "#82518c") + 
    theme_blog()</code></pre>
<pre><code>## Warning: Removed 39 rows containing non-finite values (stat_density).</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-16-1.png" width="672">
</p>
<p>
Let‚Äôs finish this with one of my favourite legume, lentils:
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Lentils") %&gt;%
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of lentils at the Luxembourg-City market in the 1870s")</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-17-1.png" width="672">
</p>
<pre class="r"><code>market_price %&gt;% 
    filter(good_en == "Lentils") %&gt;% 
    ggplot() + 
    geom_density(aes(price), colour = "#82518c") + 
    theme_blog()</code></pre>
<pre><code>## Warning: Removed 79 rows containing non-finite values (stat_density).</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-18-1.png" width="672">
</p>
<p>
All these 0‚Äôs might be surprising, but in most cases, they are actually true zeros! For example, you can check this <a href="https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fwsvhwh%2Fpages%2F3/pct:74,0,100,100/full/0/default.jpg">issue</a>. This very likely means that no lentils were available that day at the market. Let‚Äôs get rid of the 0s and other extreme values:
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Lentils") %&gt;%
    filter(between(price, 1, 40)) %&gt;% 
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = "#82518c") + 
    theme_blog() + 
    labs(title = "Prices of lentils at the Luxembourg-City market in the 1870s")</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/historical_newspaper_scraping_tesseract-19-1.png" width="672">
</p>
<p>
I would like to see if the spikes above 30 are errors or not:
</p>
<pre class="r"><code>market_price %&gt;% 
    filter(good_en == "Lentils") %&gt;% 
    filter(between(price, 30, 40)) %&gt;% 
    pull(source_url)</code></pre>
<pre><code>## [1] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F04mb5t%2Fpages%2F1/full/full/0/default.jpg"
## [2] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fb8zp31%2Fpages%2F1/full/full/0/default.jpg"
## [3] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fkzrj53%2Fpages%2F1/full/full/0/default.jpg"
## [4] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fs8sw2v%2Fpages%2F1/full/full/0/default.jpg"
## [5] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fsjptsk%2Fpages%2F1/full/full/0/default.jpg"
## [6] "https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fwk65b6%2Fpages%2F1/full/full/0/default.jpg"</code></pre>
<p>
The price was recognized as being 35, and turns out it was correct as you can see <a href="https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F04mb5t%2Fpages%2F3/full/full/0/default.jpg">here</a>. This is quite interesting, because the average price was way lower than that:
</p>
<pre class="r"><code>market_price %&gt;%
    filter(good_en == "Lentils") %&gt;%
    filter(between(price, 1, 40)) %&gt;% 
    summarise(mean_price = mean(price), 
              sd_price = sd(price))</code></pre>
<pre><code>## # A tibble: 1 x 2
##   mean_price sd_price
##        &lt;dbl&gt;    &lt;dbl&gt;
## 1       20.8     5.82</code></pre>
<p>
I‚Äôm going to finish here; it was an interesting project, and I can‚Äôt wait for more newspapers to be digitized and OCR to work even better. There is a lot more historical data trapped in these newspapers that could provide a lot insights on Luxembourg‚Äôs society in the 19th century.
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-04-07-historical_newspaper_scraping_tesseract.html</guid>
  <pubDate>Sun, 07 Apr 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Get text from pdfs or images using OCR: a tutorial with {tesseract} and {magick}</title>
  <link>https://b-rodrigues.github.io/posts/2019-03-31-tesseract.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://en.wikipedia.org/wiki/Michel_Rodange"> <img src="https://b-rodrigues.github.io/assets/img/michelrodange.jpg" title="The high school I attended was named after this gentleman"></a>
</p>
</div>
<p>
In this blog post I‚Äôm going to show you how you can extract text from scanned pdf files, or pdf files where no text recognition was performed. (For pdfs where text recognition was performed, you can read my <a href="../posts/2018-06-10-scraping_pdfs.html">other blog post</a>).
</p>
<p>
The pdf I‚Äôm going to use can be downloaded from <a href="http://www.luxemburgensia.bnl.lu/cgi/getPdf1_2.pl?mode=item&amp;id=7110">here</a>. It‚Äôs a poem titled, <em>D‚ÄôL√©ierchen (Dem L√©iweckerche s√§i Lidd)</em>, written by Michel Rodange, arguably Luxembourg‚Äôs most well known writer and poet. Michel Rodange is mostly known for his fable, <em>Renert oder De Fuu√ü am Frack an a Ma‚Äônsgr√´√üt</em>, starring a central European <a href="https://en.wikipedia.org/wiki/Reynard_the_Fox">trickster anthropomorphic red fox</a>.
</p>
<p>
<img src="https://upload.wikimedia.org/wikipedia/commons/d/d4/Reynard-the-fox.jpg"><!-- -->
</p>
<p>
Anyway, back to the point of this blog post. How can we get data from a pdf where no text recognition was performed (or, how can we get text from an image)? The pdf we need the text from looks like this:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/dleierchen_03.png"><!-- -->
</p>
<p>
To get the text from the pdf, we can use the <code>{tesseract}</code> package, which provides bindings to the <code>tesseract</code> program. <code>tesseract</code> is an open source OCR engine developed by Google. This means that first you will need to install the <code>tesseract</code> program on your system. You can follow the intructions from <code>tesseract</code>‚Äôs github <a href="https://github.com/tesseract-ocr/tesseract">page</a>. <code>tesseract</code> is currently at version 4.
</p>
<p>
Before applying OCR to a pdf, let‚Äôs first use the <code>{pdftools}</code> package to convert the pdf to png. This is because <code>{tesseract}</code> requires images as input (if you provide a pdf file, it will converted on the fly). Let‚Äôs first load the needed packages:
</p>
<pre class="r"><code>library(tidyverse)
library(tesseract)
library(pdftools)
library(magick)</code></pre>
<p>
And now let‚Äôs convert the pdf to png files (in plural, because we‚Äôll get one image per page of the pdf):
</p>
<pre class="r"><code>pngfile &lt;- pdftools::pdf_convert("path/to/pdf", dpi = 600)</code></pre>
<p>
This will generate 14 png files. I erase the ones that are not needed, such as the title page. Now, let‚Äôs read in all the image files:
</p>
<pre class="r"><code>path &lt;- dir(path = "path/to/pngs", pattern = "*.png", full.names = TRUE)

images &lt;- map(path, magick::image_read)</code></pre>
<p>
The <code>images</code> object is a list of <code>magick-image</code>s, which we can parse. BUUUUUT! There‚Äôs a problem. The text is laid out in two columns. Which means that the first line after performing OCR will be the first line of the first column, and the first line of the second column joined together. Same for the other lines of course. So ideally, I‚Äôd need to split the file in the middle, and then perform OCR. This is easily done with the <code>{magick}</code> package:
</p>
<pre class="r"><code>first_half &lt;- map(images, ~image_crop(., geometry = "2307x6462"))

second_half &lt;- map(images, ~image_crop(., geometry = "2307x6462+2307+0"))</code></pre>
<p>
Because the pngs are 4614 by 6962 pixels, I can get the first half of the png by cropping at ‚Äú2307x6462‚Äù (I decrease the height a bit to get rid of the page number), and the second half by applying the same logic, but starting the cropping at the ‚Äú2307+0‚Äù position. The result looks like this:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/dleierchen_cropped.png"><!-- -->
</p>
<p>
Much better! Now I need to join these two lists together. I cannot simply join them. Consider the following example:
</p>
<pre class="r"><code>one &lt;- list(1, 3, 5)

two &lt;- list(2, 4, 6)</code></pre>
<p>
This is the setup I currently have; <code>first_half</code> contains odd pages, and <code>second_half</code> contains even pages. The result I want would look like this:
</p>
<pre class="r"><code>list(1, 2, 3, 4, 5, 6)</code></pre>
<pre><code>## [[1]]
## [1] 1
## 
## [[2]]
## [1] 2
## 
## [[3]]
## [1] 3
## 
## [[4]]
## [1] 4
## 
## [[5]]
## [1] 5
## 
## [[6]]
## [1] 6</code></pre>
<p>
There is a very elegant solution, with <code>reduce2()</code> from the <code>{purrr}</code> package. <code>reduce()</code> takes one list and a function, and ‚Ä¶ <em>reduces</em> the list to a single element. For instance:
</p>
<pre class="r"><code>reduce(list(1, 2, 3), paste)</code></pre>
<pre><code>## [1] "1 2 3"</code></pre>
<p>
<code>reduce2()</code> is very similar, but takes in two lists, but the second list must be one element shorter:
</p>
<pre class="r"><code>reduce2(list(1, 2, 3), list("a", "b"), paste)</code></pre>
<pre><code>## [1] "1 2 a 3 b"</code></pre>
<p>
So we cannot simply use <code>reduce2()</code> on lists <code>one</code> and <code>two</code>, because they‚Äôre the same length. So let‚Äôs prepend a value to <code>one</code>, using the <code>prepend()</code> function of <code>{purrr}</code>:
</p>
<pre class="r"><code>prepend(one, 0) %&gt;% 
    reduce2(two, c)</code></pre>
<pre><code>## [1] 0 1 2 3 4 5 6</code></pre>
<p>
Exactly what we need! Let‚Äôs apply this trick to our lists:
</p>
<pre class="r"><code>merged_list &lt;- prepend(first_half, NA) %&gt;% 
    reduce2(second_half, c) %&gt;% 
    discard(is.na)</code></pre>
<p>
I‚Äôve prepended <code>NA</code> to the first list, and then used <code>reduce2()</code> and then used <code>discard(is.na)</code> to remove the <code>NA</code> I‚Äôve added at the start. Now, we can use OCR to get the text:
</p>
<pre class="r"><code>text_list &lt;- map(merged_list, ocr)</code></pre>
<p>
<code>ocr()</code> uses a model trained on English by default, and even though there is a model trained on Luxembourguish, the one trained on English works better! Very likely because the English model was trained on a lot more data than the Luxembourguish one. I was worried the English model was not going to recognize characters such as <code>√©</code>, but no, it worked quite well.
</p>
<p>
This is how it looks like:
</p>
<pre class="r"><code>text_list

[[1]]
[1] "Lhe\n| Kaum huet d‚ÄôFeld dat fr√©ndlecht Feier\nVun der Aussentssonn gesunn\nAs mam Plou aus Stall a Scheier\n* D‚Äôlescht e Bauer ausgezunn.\nFir de Plou em nach ze dreiwen\nWar sai J√©ngelchen alaert,\nDeen n√©t w√©llt doheem m√©i bleiwen\n8 An esouz um viischte Paerd.\nOp der Sch√©llche stoung ze denken\nD‚ÄôL√©ierche mam Hierz voll Lidder\nFir de B√©ifchen nach ze zanken\n12 Duckelt s‚Äôan de Som sech nidder.\nBis e laascht war, an du st√©mmt se\nUn e Liddchen, datt et kraacht\nOp der Nouteleder kl√©mmt se\n16 Datt dem B√©ifchen d‚ÄôHaerz alt laacht.\nAn du sot en: Papp, ech mengen\nBal de Vull dee k√©nnt och schwatzen.\nLauschter, sot de Papp zum Klengen,\n20 Ech kann d‚ÄôLiddchen iwersetzen.\nI\nBas de do, mii l√©iwe Fr√©ndchen\nMa de Wanter dee war laang!\nKuck, ech hat keng fr√©ilech St√©nnchen\n24 *T war fir dech a mech mer baang.\nAn du koum ech dech besichen\nWell du goungs n√©t m√©i eraus\nMann wat hues jo du eng Kichen\n28 Wat eng Scheier wat en Haus.\nWi zerguttster, a wat Saachen!\nAn d√©ng Frache gouf mer Brout.\nAn d√©ng Kanner, wi se laachen,\n32, An hir Backelcher, wi rout!\nJo, bei dir as Rot n√©t deier!\nJo a kuck mer wat eng M√©scht.\nDat g√©t Saache fir an d‚ÄôScheier\n36 An och Su√© fir an d‚ÄôK√©scht.\nMuerges waars de schuns um Dreschen\nIr der Daudes d‚ÄôSchung sech str√©ckt\nBas am Do duurch Wis a Paschen\n40 Laascht all Waassergruef geschr√©ckt.\n"
....
....</code></pre>
<p>
We still need to split at the <code>‚Äú‚Äù</code> character:
</p>
<pre class="r"><code>text_list &lt;- text_list %&gt;% 
    map(., ~str_split(., "\n"))</code></pre>
<p>
The end result:
</p>
<pre class="r"><code>text_list

[[1]]
[[1]][[1]]
 [1] "Lhe"                                      "| Kaum huet d‚ÄôFeld dat fr√©ndlecht Feier" 
 [3] "Vun der Aussentssonn gesunn"              "As mam Plou aus Stall a Scheier"         
 [5] "* D‚Äôlescht e Bauer ausgezunn."            "Fir de Plou em nach ze dreiwen"          
 [7] "War sai J√©ngelchen alaert,"               "Deen n√©t w√©llt doheem m√©i bleiwen"       
 [9] "8 An esouz um viischte Paerd."            "Op der Sch√©llche stoung ze denken"       
[11] "D‚ÄôL√©ierche mam Hierz voll Lidder"         "Fir de B√©ifchen nach ze zanken"          
[13] "12 Duckelt s‚Äôan de Som sech nidder."      "Bis e laascht war, an du st√©mmt se"      
[15] "Un e Liddchen, datt et kraacht"           "Op der Nouteleder kl√©mmt se"             
[17] "16 Datt dem B√©ifchen d‚ÄôHaerz alt laacht." "An du sot en: Papp, ech mengen"          
[19] "Bal de Vull dee k√©nnt och schwatzen."     "Lauschter, sot de Papp zum Klengen,"     
[21] "20 Ech kann d‚ÄôLiddchen iwersetzen."       "I"                                       
[23] "Bas de do, mii l√©iwe Fr√©ndchen"           "Ma de Wanter dee war laang!"             
[25] "Kuck, ech hat keng fr√©ilech St√©nnchen"    "24 *T war fir dech a mech mer baang."    
[27] "An du koum ech dech besichen"             "Well du goungs n√©t m√©i eraus"            
[29] "Mann wat hues jo du eng Kichen"           "28 Wat eng Scheier wat en Haus."         
[31] "Wi zerguttster, a wat Saachen!"           "An d√©ng Frache gouf mer Brout."          
[33] "An d√©ng Kanner, wi se laachen,"           "32, An hir Backelcher, wi rout!"         
[35] "Jo, bei dir as Rot n√©t deier!"            "Jo a kuck mer wat eng M√©scht."           
[37] "Dat g√©t Saache fir an d‚ÄôScheier"          "36 An och Su√© fir an d‚ÄôK√©scht."          
[39] "Muerges waars de schuns um Dreschen"      "Ir der Daudes d‚ÄôSchung sech str√©ckt"     
[41] "Bas am Do duurch Wis a Paschen"           "40 Laascht all Waassergruef geschr√©ckt." 
[43] ""  
...
...</code></pre>
<p>
Perfect! Some more cleaning would be needed though. For example, I need to remove the little annotations that are included:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/dleierchen_anot.png"><!-- -->
</p>
<p>
I don‚Äôt know yet how I‚Äôm going to do that.I also need to remove the line numbers at the beginning of every fourth line, but this is easily done with a simple regular expression:
</p>
<pre class="r"><code>str_remove_all(c("12 bla", "blb", "123 blc"), "^\\d{1,}\\s+")</code></pre>
<pre><code>## [1] "bla" "blb" "blc"</code></pre>
<p>
But this will be left for a future blog post!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-03-31-tesseract.html</guid>
  <pubDate>Sun, 31 Mar 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Pivoting data frames just got easier thanks to pivot_wide() and pivot_long()</title>
  <link>https://b-rodrigues.github.io/posts/2019-03-20-pivot.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://youtu.be/R2u0sN9stbA?t=69"> <img src="https://b-rodrigues.github.io/assets/img/pivot.jpg" title="You know where this leads"></a>
</p>
</div>
<p>
Update: <code>pivot_wide()</code> and <code>pivot_long()</code> are now called <code>pivot_wider()</code> and <code>pivot_longer()</code>, so the code below needs to be updated accondingly.
</p>
<p>
There‚Äôs a lot going on in the development version of <code>{tidyr}</code>. New functions for pivoting data frames, <code>pivot_wide()</code> and <code>pivot_long()</code> are coming, and will replace the current functions, <code>spread()</code> and <code>gather()</code>. <code>spread()</code> and <code>gather()</code> will remain in the package though:
</p>
{{% tweet ‚Äú1108107722128613377‚Äù %}}
<p>
If you want to try out these new functions, you need to install the development version of <code>{tidyr}</code>:
</p>
<pre class="r"><code>devtools::install_github("tidyverse/tidyr")</code></pre>
<p>
and you can read the vignette <a href="https://tidyr.tidyverse.org/dev/articles/pivot.html#many-variables-in-column-names">here</a>. Because these functions are still being developed, some more changes might be introduced, but I guess that the main functionality will not change much.
</p>
<p>
Let‚Äôs play around with these functions and the <code>mtcars</code> data set. First let‚Äôs load the packages and the data:
</p>
<pre class="r"><code>library(tidyverse)
data(mtcars)</code></pre>
<p>
First, let‚Äôs create a wide dataset, by <em>spreading</em> the levels of the ‚Äúam‚Äù column to two new columns:
</p>
<pre class="r"><code>mtcars_wide1 &lt;- mtcars %&gt;% 
    pivot_wide(names_from = "am", values_from = "mpg") 

mtcars_wide1 %&gt;% 
    select(`0`, `1`, everything())</code></pre>
<pre><code>## # A tibble: 32 x 11
##      `0`   `1`   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  NA    21       6  160    110  3.9   2.62  16.5     0     4     4
##  2  NA    21       6  160    110  3.9   2.88  17.0     0     4     4
##  3  NA    22.8     4  108     93  3.85  2.32  18.6     1     4     1
##  4  21.4  NA       6  258    110  3.08  3.22  19.4     1     3     1
##  5  18.7  NA       8  360    175  3.15  3.44  17.0     0     3     2
##  6  18.1  NA       6  225    105  2.76  3.46  20.2     1     3     1
##  7  14.3  NA       8  360    245  3.21  3.57  15.8     0     3     4
##  8  24.4  NA       4  147.    62  3.69  3.19  20       1     4     2
##  9  22.8  NA       4  141.    95  3.92  3.15  22.9     1     4     2
## 10  19.2  NA       6  168.   123  3.92  3.44  18.3     1     4     4
## # ‚Ä¶ with 22 more rows</code></pre>
<p>
<code>pivot_wide()</code>‚Äôs arguments are quite explicit: <code>names_from =</code> is where you specify the column that will be spread across the data frame, meaning, the levels of this column will become new columns. <code>values_from =</code> is where you specify the column that will fill in the values of the new columns.
</p>
<p>
‚Äú0‚Äù and ‚Äú1‚Äù are the new columns (‚Äúam‚Äù had two levels, <code>0</code> and <code>1</code>), which contain the miles per gallon for manual and automatic cars respectively. Let‚Äôs also take a look at the data frame itself:
</p>
<pre class="r"><code>mtcars_wide1 %&gt;% 
    select(`0`, `1`, everything())</code></pre>
<pre><code>## # A tibble: 32 x 11
##      `0`   `1`   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  NA    21       6  160    110  3.9   2.62  16.5     0     4     4
##  2  NA    21       6  160    110  3.9   2.88  17.0     0     4     4
##  3  NA    22.8     4  108     93  3.85  2.32  18.6     1     4     1
##  4  21.4  NA       6  258    110  3.08  3.22  19.4     1     3     1
##  5  18.7  NA       8  360    175  3.15  3.44  17.0     0     3     2
##  6  18.1  NA       6  225    105  2.76  3.46  20.2     1     3     1
##  7  14.3  NA       8  360    245  3.21  3.57  15.8     0     3     4
##  8  24.4  NA       4  147.    62  3.69  3.19  20       1     4     2
##  9  22.8  NA       4  141.    95  3.92  3.15  22.9     1     4     2
## 10  19.2  NA       6  168.   123  3.92  3.44  18.3     1     4     4
## # ‚Ä¶ with 22 more rows</code></pre>
<p>
Now suppose that we want to spread the values of ‚Äúam‚Äù times ‚Äúcyl‚Äù, and filling the data with the values of ‚Äúmpg‚Äù:
</p>
<pre class="r"><code>mtcars_wide2 &lt;- mtcars %&gt;% 
    pivot_wide(names_from = c("am", "cyl"), values_from = "mpg") 

mtcars_wide2 %&gt;% 
    select(matches("^0|1"), everything())</code></pre>
<pre><code>## # A tibble: 32 x 14
##    `1_6` `1_4` `0_6` `0_8` `0_4` `1_8`  disp    hp  drat    wt  qsec    vs
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1    21  NA    NA    NA    NA      NA  160    110  3.9   2.62  16.5     0
##  2    21  NA    NA    NA    NA      NA  160    110  3.9   2.88  17.0     0
##  3    NA  22.8  NA    NA    NA      NA  108     93  3.85  2.32  18.6     1
##  4    NA  NA    21.4  NA    NA      NA  258    110  3.08  3.22  19.4     1
##  5    NA  NA    NA    18.7  NA      NA  360    175  3.15  3.44  17.0     0
##  6    NA  NA    18.1  NA    NA      NA  225    105  2.76  3.46  20.2     1
##  7    NA  NA    NA    14.3  NA      NA  360    245  3.21  3.57  15.8     0
##  8    NA  NA    NA    NA    24.4    NA  147.    62  3.69  3.19  20       1
##  9    NA  NA    NA    NA    22.8    NA  141.    95  3.92  3.15  22.9     1
## 10    NA  NA    19.2  NA    NA      NA  168.   123  3.92  3.44  18.3     1
## # ‚Ä¶ with 22 more rows, and 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>
As you can see, this is easily achieved by simply providing more columns to <code>names_from =</code>.
</p>
<p>
Finally, it is also possible to use an optional data set which contains the specifications of the new columns:
</p>
<pre class="r"><code>mtcars_spec &lt;- mtcars %&gt;% 
    expand(am, cyl, .value = "mpg") %&gt;%
    unite(".name", am, cyl, remove = FALSE)

mtcars_spec</code></pre>
<pre><code>## # A tibble: 6 x 4
##   .name    am   cyl .value
##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 
## 1 0_4       0     4 mpg   
## 2 0_6       0     6 mpg   
## 3 0_8       0     8 mpg   
## 4 1_4       1     4 mpg   
## 5 1_6       1     6 mpg   
## 6 1_8       1     8 mpg</code></pre>
<p>
This optional data set defines how the columns ‚Äú0_4‚Äù, ‚Äú0_6‚Äù etc are constructed, and also the value that shall be used to fill in the values. ‚Äúam‚Äù and ‚Äúcyl‚Äù will be used to create the ‚Äú.name‚Äù and the ‚Äúmpg‚Äù column will be used for the ‚Äú.value‚Äù:
</p>
<pre class="r"><code>mtcars %&gt;% 
    pivot_wide(spec = mtcars_spec) %&gt;% 
    select(matches("^0|1"), everything())</code></pre>
<pre><code>## # A tibble: 32 x 14
##    `0_4` `0_6` `0_8` `1_4` `1_6` `1_8`  disp    hp  drat    wt  qsec    vs
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  NA    NA    NA    NA      21    NA  160    110  3.9   2.62  16.5     0
##  2  NA    NA    NA    NA      21    NA  160    110  3.9   2.88  17.0     0
##  3  NA    NA    NA    22.8    NA    NA  108     93  3.85  2.32  18.6     1
##  4  NA    21.4  NA    NA      NA    NA  258    110  3.08  3.22  19.4     1
##  5  NA    NA    18.7  NA      NA    NA  360    175  3.15  3.44  17.0     0
##  6  NA    18.1  NA    NA      NA    NA  225    105  2.76  3.46  20.2     1
##  7  NA    NA    14.3  NA      NA    NA  360    245  3.21  3.57  15.8     0
##  8  24.4  NA    NA    NA      NA    NA  147.    62  3.69  3.19  20       1
##  9  22.8  NA    NA    NA      NA    NA  141.    95  3.92  3.15  22.9     1
## 10  NA    19.2  NA    NA      NA    NA  168.   123  3.92  3.44  18.3     1
## # ‚Ä¶ with 22 more rows, and 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>
Using a spec is especially useful if you need to make new levels that are not in the data. For instance, suppose that there are actually 10-cylinder cars too, but they do not appear in our sample. We would like to make the fact that they‚Äôre missing explicit:
</p>
<pre class="r"><code>mtcars_spec2 &lt;- mtcars %&gt;% 
    expand(am, "cyl" = c(cyl, 10), .value = "mpg") %&gt;%
    unite(".name", am, cyl, remove = FALSE)

mtcars_spec2</code></pre>
<pre><code>## # A tibble: 8 x 4
##   .name    am   cyl .value
##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 
## 1 0_4       0     4 mpg   
## 2 0_6       0     6 mpg   
## 3 0_8       0     8 mpg   
## 4 0_10      0    10 mpg   
## 5 1_4       1     4 mpg   
## 6 1_6       1     6 mpg   
## 7 1_8       1     8 mpg   
## 8 1_10      1    10 mpg</code></pre>
<pre class="r"><code>mtcars %&gt;% 
    pivot_wide(spec = mtcars_spec2) %&gt;% 
    select(matches("^0|1"), everything())</code></pre>
<pre><code>## # A tibble: 32 x 16
##    `0_4` `0_6` `0_8` `0_10` `1_4` `1_6` `1_8` `1_10`  disp    hp  drat
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  NA    NA    NA       NA  NA      21    NA     NA  160    110  3.9 
##  2  NA    NA    NA       NA  NA      21    NA     NA  160    110  3.9 
##  3  NA    NA    NA       NA  22.8    NA    NA     NA  108     93  3.85
##  4  NA    21.4  NA       NA  NA      NA    NA     NA  258    110  3.08
##  5  NA    NA    18.7     NA  NA      NA    NA     NA  360    175  3.15
##  6  NA    18.1  NA       NA  NA      NA    NA     NA  225    105  2.76
##  7  NA    NA    14.3     NA  NA      NA    NA     NA  360    245  3.21
##  8  24.4  NA    NA       NA  NA      NA    NA     NA  147.    62  3.69
##  9  22.8  NA    NA       NA  NA      NA    NA     NA  141.    95  3.92
## 10  NA    19.2  NA       NA  NA      NA    NA     NA  168.   123  3.92
## # ‚Ä¶ with 22 more rows, and 5 more variables: wt &lt;dbl&gt;, qsec &lt;dbl&gt;,
## #   vs &lt;dbl&gt;, gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>
As you can see, we now have two more columns have been added, and they are full of NA‚Äôs.
</p>
<p>
Now, let‚Äôs try to go from wide to long data sets, using <code>pivot_long()</code>:
</p>
<pre class="r"><code>mtcars_wide1 %&gt;% 
  pivot_long(cols = c(`1`, `0`), names_to = "am", values_to = "mpg") %&gt;% 
  select(am, mpg, everything())</code></pre>
<pre><code>## # A tibble: 64 x 11
##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1      21       6   160   110  3.9   2.62  16.5     0     4     4
##  2 0      NA       6   160   110  3.9   2.62  16.5     0     4     4
##  3 1      21       6   160   110  3.9   2.88  17.0     0     4     4
##  4 0      NA       6   160   110  3.9   2.88  17.0     0     4     4
##  5 1      22.8     4   108    93  3.85  2.32  18.6     1     4     1
##  6 0      NA       4   108    93  3.85  2.32  18.6     1     4     1
##  7 1      NA       6   258   110  3.08  3.22  19.4     1     3     1
##  8 0      21.4     6   258   110  3.08  3.22  19.4     1     3     1
##  9 1      NA       8   360   175  3.15  3.44  17.0     0     3     2
## 10 0      18.7     8   360   175  3.15  3.44  17.0     0     3     2
## # ‚Ä¶ with 54 more rows</code></pre>
<p>
The arguments of <code>pivot_long()</code> are quite explicit too, and similar to the ones in <code>pivot_wide()</code>. <code>cols =</code> is where the user specifies the columns that need to be pivoted. <code>names_to =</code> is where the user can specify the name of the new columns, whose levels will be exactly the ones specified to <code>cols =</code>. <code>values_to =</code> is where the user specifies the column name of the new column that will contain the values.
</p>
<p>
It is also possible to specify the columns that should not be transformed, by using <code>-</code>:
</p>
<pre class="r"><code>mtcars_wide1 %&gt;% 
  pivot_long(cols = -matches("^[[:alpha:]]"), names_to = "am", values_to = "mpg") %&gt;% 
  select(am, mpg, everything())</code></pre>
<pre><code>## # A tibble: 64 x 11
##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1      21       6   160   110  3.9   2.62  16.5     0     4     4
##  2 0      NA       6   160   110  3.9   2.62  16.5     0     4     4
##  3 1      21       6   160   110  3.9   2.88  17.0     0     4     4
##  4 0      NA       6   160   110  3.9   2.88  17.0     0     4     4
##  5 1      22.8     4   108    93  3.85  2.32  18.6     1     4     1
##  6 0      NA       4   108    93  3.85  2.32  18.6     1     4     1
##  7 1      NA       6   258   110  3.08  3.22  19.4     1     3     1
##  8 0      21.4     6   258   110  3.08  3.22  19.4     1     3     1
##  9 1      NA       8   360   175  3.15  3.44  17.0     0     3     2
## 10 0      18.7     8   360   175  3.15  3.44  17.0     0     3     2
## # ‚Ä¶ with 54 more rows</code></pre>
<p>
Here the columns that should not be modified are all those that start with a letter, hence the ‚Äú<sup>1</sup>‚Äù regular expression. It is also possible to remove all the <code>NA</code>‚Äôs from the data frame, with <code>na.rm =</code>.
</p>
<pre class="r"><code>mtcars_wide1 %&gt;% 
  pivot_long(cols = c(`1`, `0`), names_to = "am", values_to = "mpg", na.rm = TRUE) %&gt;% 
  select(am, mpg, everything())</code></pre>
<pre><code>## # A tibble: 32 x 11
##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1      21       6  160    110  3.9   2.62  16.5     0     4     4
##  2 1      21       6  160    110  3.9   2.88  17.0     0     4     4
##  3 1      22.8     4  108     93  3.85  2.32  18.6     1     4     1
##  4 0      21.4     6  258    110  3.08  3.22  19.4     1     3     1
##  5 0      18.7     8  360    175  3.15  3.44  17.0     0     3     2
##  6 0      18.1     6  225    105  2.76  3.46  20.2     1     3     1
##  7 0      14.3     8  360    245  3.21  3.57  15.8     0     3     4
##  8 0      24.4     4  147.    62  3.69  3.19  20       1     4     2
##  9 0      22.8     4  141.    95  3.92  3.15  22.9     1     4     2
## 10 0      19.2     6  168.   123  3.92  3.44  18.3     1     4     4
## # ‚Ä¶ with 22 more rows</code></pre>
<p>
We can also pivot data frames where the names of the columns are made of two or more variables, for example in our <code>mtcars_wide2</code> data frame:
</p>
<pre class="r"><code>mtcars_wide2 %&gt;% 
    select(matches("^0|1"), everything())</code></pre>
<pre><code>## # A tibble: 32 x 14
##    `1_6` `1_4` `0_6` `0_8` `0_4` `1_8`  disp    hp  drat    wt  qsec    vs
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1    21  NA    NA    NA    NA      NA  160    110  3.9   2.62  16.5     0
##  2    21  NA    NA    NA    NA      NA  160    110  3.9   2.88  17.0     0
##  3    NA  22.8  NA    NA    NA      NA  108     93  3.85  2.32  18.6     1
##  4    NA  NA    21.4  NA    NA      NA  258    110  3.08  3.22  19.4     1
##  5    NA  NA    NA    18.7  NA      NA  360    175  3.15  3.44  17.0     0
##  6    NA  NA    18.1  NA    NA      NA  225    105  2.76  3.46  20.2     1
##  7    NA  NA    NA    14.3  NA      NA  360    245  3.21  3.57  15.8     0
##  8    NA  NA    NA    NA    24.4    NA  147.    62  3.69  3.19  20       1
##  9    NA  NA    NA    NA    22.8    NA  141.    95  3.92  3.15  22.9     1
## 10    NA  NA    19.2  NA    NA      NA  168.   123  3.92  3.44  18.3     1
## # ‚Ä¶ with 22 more rows, and 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>
All the columns that start with either ‚Äú0‚Äù or ‚Äú1‚Äù must be pivoted:
</p>
<pre class="r"><code>mtcars_wide2 %&gt;% 
  pivot_long(cols = matches("0|1"), names_to = "am_cyl", values_to = "mpg", na.rm = TRUE) %&gt;% 
  select(am_cyl, everything())</code></pre>
<pre><code>## # A tibble: 32 x 10
##    am_cyl  disp    hp  drat    wt  qsec    vs  gear  carb   mpg
##    &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1_6     160    110  3.9   2.62  16.5     0     4     4  21  
##  2 1_6     160    110  3.9   2.88  17.0     0     4     4  21  
##  3 1_4     108     93  3.85  2.32  18.6     1     4     1  22.8
##  4 0_6     258    110  3.08  3.22  19.4     1     3     1  21.4
##  5 0_8     360    175  3.15  3.44  17.0     0     3     2  18.7
##  6 0_6     225    105  2.76  3.46  20.2     1     3     1  18.1
##  7 0_8     360    245  3.21  3.57  15.8     0     3     4  14.3
##  8 0_4     147.    62  3.69  3.19  20       1     4     2  24.4
##  9 0_4     141.    95  3.92  3.15  22.9     1     4     2  22.8
## 10 0_6     168.   123  3.92  3.44  18.3     1     4     4  19.2
## # ‚Ä¶ with 22 more rows</code></pre>
<p>
Now, there is one new column, ‚Äúam_cyl‚Äù which must still be transformed by separating ‚Äúam_cyl‚Äù into two new columns:
</p>
<pre class="r"><code>mtcars_wide2 %&gt;% 
  pivot_long(cols = matches("0|1"), names_to = "am_cyl", values_to = "mpg", na.rm = TRUE) %&gt;% 
  separate(am_cyl, into = c("am", "cyl"), sep = "_") %&gt;% 
  select(am, cyl, everything())</code></pre>
<pre><code>## # A tibble: 32 x 11
##    am    cyl    disp    hp  drat    wt  qsec    vs  gear  carb   mpg
##    &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1     6      160    110  3.9   2.62  16.5     0     4     4  21  
##  2 1     6      160    110  3.9   2.88  17.0     0     4     4  21  
##  3 1     4      108     93  3.85  2.32  18.6     1     4     1  22.8
##  4 0     6      258    110  3.08  3.22  19.4     1     3     1  21.4
##  5 0     8      360    175  3.15  3.44  17.0     0     3     2  18.7
##  6 0     6      225    105  2.76  3.46  20.2     1     3     1  18.1
##  7 0     8      360    245  3.21  3.57  15.8     0     3     4  14.3
##  8 0     4      147.    62  3.69  3.19  20       1     4     2  24.4
##  9 0     4      141.    95  3.92  3.15  22.9     1     4     2  22.8
## 10 0     6      168.   123  3.92  3.44  18.3     1     4     4  19.2
## # ‚Ä¶ with 22 more rows</code></pre>
<p>
It is also possible to achieve this using a data frame with the specification of what you need:
</p>
<pre class="r"><code>mtcars_spec_long &lt;- mtcars_wide2 %&gt;% 
  pivot_long_spec(matches("0|1"), values_to = "mpg") %&gt;% 
  separate(name, c("am", "cyl"), sep = "_")

mtcars_spec_long</code></pre>
<pre><code>## # A tibble: 6 x 4
##   .name .value am    cyl  
##   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;
## 1 1_6   mpg    1     6    
## 2 1_4   mpg    1     4    
## 3 0_6   mpg    0     6    
## 4 0_8   mpg    0     8    
## 5 0_4   mpg    0     4    
## 6 1_8   mpg    1     8</code></pre>
<p>
Providing this spec to <code>pivot_long()</code> solves the issue:
</p>
<pre class="r"><code>mtcars_wide2 %&gt;% 
  pivot_long(spec = mtcars_spec_long, na.rm = TRUE) %&gt;% 
  select(am, cyl, everything())</code></pre>
<pre><code>## # A tibble: 32 x 11
##    am    cyl    disp    hp  drat    wt  qsec    vs  gear  carb   mpg
##    &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 1     6      160    110  3.9   2.62  16.5     0     4     4  21  
##  2 1     6      160    110  3.9   2.88  17.0     0     4     4  21  
##  3 1     4      108     93  3.85  2.32  18.6     1     4     1  22.8
##  4 0     6      258    110  3.08  3.22  19.4     1     3     1  21.4
##  5 0     8      360    175  3.15  3.44  17.0     0     3     2  18.7
##  6 0     6      225    105  2.76  3.46  20.2     1     3     1  18.1
##  7 0     8      360    245  3.21  3.57  15.8     0     3     4  14.3
##  8 0     4      147.    62  3.69  3.19  20       1     4     2  24.4
##  9 0     4      141.    95  3.92  3.15  22.9     1     4     2  22.8
## 10 0     6      168.   123  3.92  3.44  18.3     1     4     4  19.2
## # ‚Ä¶ with 22 more rows</code></pre>
<p>
Stay tuned to Hadley Wickham‚Äôs <a href="https://twitter.com/hadleywickham">twitter</a> as there will definitely be announcements soon!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-03-20-pivot.html</guid>
  <pubDate>Wed, 20 Mar 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2</title>
  <link>https://b-rodrigues.github.io/posts/2019-03-05-historical_vowpal_part2.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://youtu.be/BilPXIt0R2w?t=41"> <img src="https://b-rodrigues.github.io/assets/img/wabbit_reading.jpg" title="Vowpal Wabbit is fast as heck"></a>
</p>
</div>
<p>
In <a href="../posts/2019-03-03-historical_vowpal.html">part 1</a> of this series I set up Vowpal Wabbit to classify newspapers content. Now, let‚Äôs use the model to make predictions and see how and if we can improve the model. Then, let‚Äôs train the model on the whole data.
</p>
<section id="step-1-prepare-the-data" class="level2">
<h2 class="anchored" data-anchor-id="step-1-prepare-the-data">
Step 1: prepare the data
</h2>
<p>
The first step consists in importing the test data and preparing it. The test data need not be large and thus can be imported and worked on in R.
</p>
<p>
I need to remove the target column from the test set, or else it will be used to make predictions. If you do not remove this column the accuracy of the model will be very high, but it will be wrong since, of course, you do not have the target column at running time‚Ä¶ because it is the column that you want to predict!
</p>
<pre class="r"><code>library("tidyverse")
library("yardstick")

small_test &lt;- read_delim("data_split/small_test.txt", "|",
                      escape_double = FALSE, col_names = FALSE,
                      trim_ws = TRUE)

small_test %&gt;%
    mutate(X1= " ") %&gt;%
    write_delim("data_split/small_test2.txt", col_names = FALSE, delim = "|")</code></pre>
<p>
I wrote the data in a file called <code>small_test2.txt</code> and can now use my model to make predictions:
</p>
<pre class="r"><code>system2("/home/cbrunos/miniconda3/bin/vw", args = "-t -i vw_models/small_oaa.model data_split/small_test2.txt -p data_split/small_oaa.predict")</code></pre>
<p>
The predictions get saved in the file <code>small_oaa.predict</code>, which is a plain text file. Let‚Äôs add these predictions to the original test set:
</p>
<pre class="r"><code>small_predictions &lt;- read_delim("data_split/small_oaa.predict", "|",
                          escape_double = FALSE, col_names = FALSE,
                          trim_ws = TRUE)

small_test &lt;- small_test %&gt;%
    rename(truth = X1) %&gt;%
    mutate(truth = factor(truth, levels = c("1", "2", "3", "4", "5")))

small_predictions &lt;- small_predictions %&gt;%
    rename(predictions = X1) %&gt;%
    mutate(predictions = factor(predictions, levels = c("1", "2", "3", "4", "5")))

small_test &lt;- small_test %&gt;%
    bind_cols(small_predictions)</code></pre>
</section>
<section id="step-2-use-the-model-and-test-data-to-evaluate-performance" class="level2">
<h2 class="anchored" data-anchor-id="step-2-use-the-model-and-test-data-to-evaluate-performance">
Step 2: use the model and test data to evaluate performance
</h2>
<p>
We can use the several metrics included in <code>{yardstick}</code> to evaluate the model‚Äôs performance:
</p>
<pre class="r"><code>conf_mat(small_test, truth = truth, estimate = predictions)

accuracy(small_test, truth = truth, estimate = predictions)</code></pre>
<pre class="r"><code>          Truth
Prediction  1  2  3  4  5
         1 51 15  2 10  1
         2 11  6  3  1  0
         3  0  0  0  0  0
         4  0  0  0  0  0
         5  0  0  0  0  0</code></pre>
<pre class="r"><code># A tibble: 1 x 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy multiclass     0.570</code></pre>
<p>
We can see that the model never predicted class <code>3</code>, <code>4</code> or <code>5</code>. Can we improve by adding some regularization? Let‚Äôs find out!
</p>
</section>
<section id="step-3-adding-regularization" class="level2">
<h2 class="anchored" data-anchor-id="step-3-adding-regularization">
Step 3: adding regularization
</h2>
<p>
Before trying regularization, let‚Äôs try changing the cost function from the logistic function to the hinge function:
</p>
<pre class="r"><code># Train the model
hinge_oaa_fit &lt;- system2("/home/cbrunos/miniconda3/bin/vw", args = "--oaa 5 -d data_split/small_train.txt --loss_function hinge -f vw_models/hinge_oaa.model", stderr = TRUE)

system2("/home/cbrunos/miniconda3/bin/vw", args = "-i vw_models/hinge_oaa.model -t -d data_split/small_test2.txt -p data_split/hinge_oaa.predict")


predictions &lt;- read_delim("data_split/hinge_oaa.predict", "|",
                          escape_double = FALSE, col_names = FALSE,
                          trim_ws = TRUE)

test &lt;- test %&gt;%
    select(-predictions)

predictions &lt;- predictions %&gt;%
    rename(predictions = X1) %&gt;%
    mutate(predictions = factor(predictions, levels = c("1", "2", "3", "4", "5")))

test &lt;- test %&gt;%
    bind_cols(predictions)</code></pre>
<pre class="r"><code>conf_mat(test, truth = truth, estimate = predictions)

accuracy(test, truth = truth, estimate = predictions)</code></pre>
<pre class="r"><code>          Truth
Prediction   1   2   3   4   5
         1 411 120  45  92   1
         2 355 189  12  17   0
         3  11   2   0   0   0
         4  36   4   0   1   0
         5   3   0   3   0   0</code></pre>
<pre class="r"><code># A tibble: 1 x 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy multiclass     0.462</code></pre>
<p>
Well, didn‚Äôt work out so well, but at least we now know how to change the loss function. Let‚Äôs go back to the logistic loss and add some regularization. First, let‚Äôs train the model:
</p>
<pre class="r"><code>regul_oaa_fit &lt;- system2("/home/cbrunos/miniconda3/bin/vw", args = "--oaa 5 --l1 0.005 --l2 0.005 -d data_split/small_train.txt -f vw_models/small_regul_oaa.model", stderr = TRUE)</code></pre>
<p>
Now we can use it for prediction:
</p>
<pre class="r"><code>system2("/home/cbrunos/miniconda3/bin/vw", args = "-i vw_models/small_regul_oaa.model -t -d data_split/test2.txt -p data_split/small_regul_oaa.predict")


predictions &lt;- read_delim("data_split/small_regul_oaa.predict", "|",
                          escape_double = FALSE, col_names = FALSE,
                          trim_ws = TRUE)

test &lt;- test %&gt;%
    select(-predictions)

predictions &lt;- predictions %&gt;%
    rename(predictions = X1) %&gt;%
    mutate(predictions = factor(predictions, levels = c("1", "2", "3", "4", "5")))

test &lt;- test %&gt;%
    bind_cols(predictions)</code></pre>
<p>
We can now use it for predictions:
</p>
<pre class="r"><code>conf_mat(test, truth = truth, estimate = predictions)

accuracy(test, truth = truth, estimate = predictions)</code></pre>
<pre class="r"><code>          Truth
Prediction   1   2   3   4   5
         1 816 315  60 110   1
         2   0   0   0   0   0
         3   0   0   0   0   0
         4   0   0   0   0   0
         5   0   0   0   0   0</code></pre>
<pre class="r"><code># A tibble: 1 x 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy multiclass     0.627</code></pre>
<p>
So accuracy improved, but the model only predicts class 1 now‚Ä¶ let‚Äôs try with other hyper-parameters values:
</p>
<pre class="r"><code>regul_oaa_fit &lt;- system2("/home/cbrunos/miniconda3/bin/vw", args = "--oaa 5 --l1 0.00015 --l2 0.00015 -d data_split/small_train.txt -f vw_models/small_regul_oaa.model", stderr = TRUE)</code></pre>
<pre class="r"><code>conf_mat(test, truth = truth, estimate = predictions)

accuracy(test, truth = truth, estimate = predictions)</code></pre>
<pre class="r"><code>          Truth
Prediction   1   2   3   4   5
         1 784 300  57 108   1
         2  32  14   3   2   0
         3   0   1   0   0   0
         4   0   0   0   0   0
         5   0   0   0   0   0</code></pre>
<pre class="r"><code># A tibble: 1 x 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy multiclass     0.613</code></pre>
<p>
So accuracy is lower than previously, but at least more categories get correctly predicted. Depending on your needs, you should consider different metrics. Especially for classification problems, you might not be interested in accuracy, in particular if the data is severely unbalanced.
</p>
<p>
Anyhow, to finish this blog post, let‚Äôs train the model on the whole data and measure the time it takes to run the full model.
</p>
</section>
<section id="step-4-training-on-the-whole-data" class="level2">
<h2 class="anchored" data-anchor-id="step-4-training-on-the-whole-data">
Step 4: Training on the whole data
</h2>
<p>
Let‚Äôs first split the whole data into a training and a testing set:
</p>
<pre class="r"><code>nb_lines &lt;- system2("cat", args = "text_fr.txt | wc -l", stdout = TRUE)

system2("split", args = paste0("-l", floor(as.numeric(nb_lines)*0.995), " text_fr.txt data_split/"))

system2("mv", args = "data_split/aa data_split/train.txt")
system2("mv", args = "data_split/ab data_split/test.txt")</code></pre>
<p>
The whole data contains 260247 lines, and the training set weighs 667MB, which is quite large. Let‚Äôs train the simple multiple classifier on the data and see how long it takes:
</p>
<pre class="r"><code>tic &lt;- Sys.time()
oaa_fit &lt;- system2("/home/cbrunos/miniconda3/bin/vw", args = "--oaa 5 -d data_split/train.txt -f vw_models/oaa.model", stderr = TRUE)
Sys.time() - tic</code></pre>
<pre class="r"><code>Time difference of 4.73266 secs</code></pre>
<p>
Yep, you read that right. Training the classifier on 667MB of data took less than 5 seconds!
</p>
<p>
Let‚Äôs take a look at the final object:
</p>
<pre class="r"><code>oaa_fit</code></pre>
<pre class="r"><code> [1] "final_regressor = vw_models/oaa.model"                                   
 [2] "Num weight bits = 18"                                                    
 [3] "learning rate = 0.5"                                                     
 [4] "initial_t = 0"                                                           
 [5] "power_t = 0.5"                                                           
 [6] "using no cache"                                                          
 [7] "Reading datafile = data_split/train.txt"                                 
 [8] "num sources = 1"                                                         
 [9] "average  since         example        example  current  current  current"
[10] "loss     last          counter         weight    label  predict features"
[11] "1.000000 1.000000            1            1.0        2        1      253"
[12] "0.500000 0.000000            2            2.0        2        2      499"
[13] "0.250000 0.000000            4            4.0        2        2        6"
[14] "0.250000 0.250000            8            8.0        1        1     2268"
[15] "0.312500 0.375000           16           16.0        1        1      237"
[16] "0.250000 0.187500           32           32.0        1        1      557"
[17] "0.171875 0.093750           64           64.0        1        1      689"
[18] "0.179688 0.187500          128          128.0        2        2      208"
[19] "0.144531 0.109375          256          256.0        1        1      856"
[20] "0.136719 0.128906          512          512.0        4        4        4"
[21] "0.122070 0.107422         1024         1024.0        1        1     1353"
[22] "0.106934 0.091797         2048         2048.0        1        1      571"
[23] "0.098633 0.090332         4096         4096.0        1        1       43"
[24] "0.080566 0.062500         8192         8192.0        1        1      885"
[25] "0.069336 0.058105        16384        16384.0        1        1      810"
[26] "0.062683 0.056030        32768        32768.0        2        2      467"
[27] "0.058167 0.053650        65536        65536.0        1        1       47"
[28] "0.056061 0.053955       131072       131072.0        1        1      495"
[29] ""                                                                        
[30] "finished run"                                                            
[31] "number of examples = 258945"                                             
[32] "weighted example sum = 258945.000000"                                    
[33] "weighted label sum = 0.000000"                                           
[34] "average loss = 0.054467"                                                 
[35] "total feature number = 116335486"  </code></pre>
<p>
Let‚Äôs use the test set and see how the model fares:
</p>
<pre class="r"><code>conf_mat(test, truth = truth, estimate = predictions)

accuracy(test, truth = truth, estimate = predictions)</code></pre>
<pre class="r"><code>          Truth
Prediction   1   2   3   4   5
         1 537 175  52 100   1
         2 271 140   8   9   0
         3   1   0   0   0   0
         4   7   0   0   1   0
         5   0   0   0   0   0</code></pre>
<pre class="r"><code># A tibble: 1 x 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy multiclass     0.521</code></pre>
<p>
Better accuracy can certainly be achieved with hyper-parameter tuning‚Ä¶ maybe the subject for a future blog post? In any case I am very impressed with Vowpal Wabbit and am certainly looking forward to future developments of <code>{RVowpalWabbit}</code>!
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-03-05-historical_vowpal_part2.html</guid>
  <pubDate>Tue, 05 Mar 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 1</title>
  <link>https://b-rodrigues.github.io/posts/2019-03-03-historical_vowpal.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://youtu.be/BilPXIt0R2w?t=41"> <img src="https://b-rodrigues.github.io/assets/img/wabbit_reading.jpg" title="Vowpal Wabbit is fast as heck"></a>
</p>
</div>
<p>
Can I get enough of historical newspapers data? Seems like I don‚Äôt. I already wrote four (<a href="../posts/2019-01-04-newspapers.html">1</a>, <a href="../posts/2019-01-13-newspapers_mets_alto.html">2</a>, <a href="../posts/2019-01-31-newspapers_shiny_app.html">3</a> and <a href="../posts/2019-02-04-newspapers_shiny_app_tutorial.html">4</a>) blog posts, but there‚Äôs still a lot to explore. This blog post uses a new batch of data announced on twitter:
</p>
<div style="text-align:center" ;="">
<p><img src="https://b-rodrigues.github.io/assets/img/ralph_marschall_tweet.png" style="width:80%;"></p>
</div>
<p>
and this data could not have arrived at a better moment, since something else got announced via Twitter recently:
</p>
{{% tweet ‚Äú1098941963527700480‚Äù %}}
<p>
I wanted to try using <a href="https://github.com/VowpalWabbit/vowpal_wabbit">Vowpal Wabbit</a> for a couple of weeks now because it seems to be the perfect tool for when you‚Äôre dealing with what I call <em>big-ish</em> data: data that is not big data, and might fit in your RAM, but is still a PITA to deal with. It can be data that is large enough to take 30 seconds to be imported into R, and then every operation on it lasts for minutes, and estimating/training a model on it might eat up all your RAM. Vowpal Wabbit avoids all this because it‚Äôs an online-learning system. Vowpal Wabbit is capable of training a model with data that it sees on the fly, which means VW can be used for real-time machine learning, but also for when the training data is very large. Each row of the data gets streamed into VW which updates the estimated parameters of the model (or weights) in real time. So no need to first import all the data into R!
</p>
<p>
The goal of this blog post is to get started with VW, and build a very simple logistic model to classify documents using the historical newspapers data from the National Library of Luxembourg, which you can download <a href="https://data.bnl.lu/data/historical-newspapers/">here</a> (scroll down and download the <em>Text Analysis Pack</em>). The goal is not to build the best model, but <em>a</em> model. Several steps are needed for this: prepare the data, install VW and train a model using <code>{RVowpalWabbit}</code>.
</p>
<section id="step-1-preparing-the-data" class="level2">
<h2 class="anchored" data-anchor-id="step-1-preparing-the-data">
Step 1: Preparing the data
</h2>
<p>
The data is in a neat <code>.xml</code> format, and extracting what I need will be easy. However, the input format for VW is a bit unusual; it resembles <em>.psv</em> files (<strong>P</strong>ipe <strong>S</strong>eparated <strong>V</strong>alues) but allows for more flexibility. I will not dwell much into it, but for our purposes, the file must look like this:
</p>
<pre><code>1 | this is the first observation, which in our case will be free text
2 | this is another observation, its label, or class, equals 2
4 | this is another observation, of class 4</code></pre>
<p>
The first column, before the ‚Äú|‚Äù is the target class we want to predict, and the second column contains free text.
</p>
<p>
The raw data looks like this:
</p>
<details>
<p>
</p><summary>
Click if you want to see the raw data
</summary>
<p></p>
<pre><code>&lt;OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd"&gt;
&lt;responseDate&gt;2019-02-28T11:13:01&lt;/responseDate&gt;
&lt;request&gt;http://www.eluxemburgensia.lu/OAI&lt;/request&gt;
&lt;ListRecords&gt;
&lt;record&gt;
&lt;header&gt;
&lt;identifier&gt;digitool-publish:3026998-DTL45&lt;/identifier&gt;
&lt;datestamp&gt;2019-02-28T11:13:01Z&lt;/datestamp&gt;
&lt;/header&gt;
&lt;metadata&gt;
&lt;oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dcterms="http://purl.org/dc/terms/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd"&gt;
&lt;dc:identifier&gt;
https://persist.lu/ark:/70795/6gq1q1/articles/DTL45
&lt;/dc:identifier&gt;
&lt;dc:source&gt;newspaper/indeplux/1871-12-29_01&lt;/dc:source&gt;
&lt;dcterms:isPartOf&gt;L'ind√©pendance luxembourgeoise&lt;/dcterms:isPartOf&gt;
&lt;dcterms:isReferencedBy&gt;
issue:newspaper/indeplux/1871-12-29_01/article:DTL45
&lt;/dcterms:isReferencedBy&gt;
&lt;dc:date&gt;1871-12-29&lt;/dc:date&gt;
&lt;dc:publisher&gt;Jean Joris&lt;/dc:publisher&gt;
&lt;dc:relation&gt;3026998&lt;/dc:relation&gt;
&lt;dcterms:hasVersion&gt;
http://www.eluxemburgensia.lu/webclient/DeliveryManager?pid=3026998#panel:pp|issue:3026998|article:DTL45
&lt;/dcterms:hasVersion&gt;
&lt;dc:description&gt;
CONSEIL COMMUNAL de la ville de Luxembourg. S√©ance du 23 d√©cembre 1871. (Suite.) Art. 6. Glaci√®re communale. M. le Bourgmcstr ¬¶ . Le coll√®ge √©chevinal propose un autro mode de se procurer de la glace. Nous avons d√©pens√© 250 fr. cha- que ann√©e pour distribuer 30 kilos do glace; c‚Äôest une trop forte somme pour un r√©sultat si minime. Nous aurions voulu nous aboucher avec des fabricants de bi√®re ou autres industriels qui nous auraient fourni de la glace en cas de besoin. L‚Äôarchitecte qui √©t√© charg√© de passer un contrat, a √©t√© trouver des n√©gociants, mais ses d√©marches n‚Äôont pas abouti. 
&lt;/dc:description&gt;
&lt;dc:title&gt;
CONSEIL COMMUNAL de la ville de Luxembourg. S√©ance du 23 d√©cembre 1871. (Suite.)
&lt;/dc:title&gt;
&lt;dc:type&gt;ARTICLE&lt;/dc:type&gt;
&lt;dc:language&gt;fr&lt;/dc:language&gt;
&lt;dcterms:extent&gt;863&lt;/dcterms:extent&gt;
&lt;/oai_dc:dc&gt;
&lt;/metadata&gt;
&lt;/record&gt;
&lt;/ListRecords&gt;
&lt;/OAI-PMH&gt;</code></pre>
</details>
<p>
I need several things from this file:
</p>
<ul>
<li>
The title of the newspaper: <code>&lt;dcterms:isPartOf&gt;L‚Äôind√©pendance luxembourgeoise&lt;/dcterms:isPartOf&gt;</code>
</li>
<li>
The type of the article: <code>&lt;dc:type&gt;ARTICLE&lt;/dc:type&gt;</code>. Can be Article, Advertisement, Issue, Section or Other.
</li>
<li>
The contents: <code>&lt;dc:description&gt;CONSEIL COMMUNAL de la ville de Luxembourg. S√©ance du ‚Ä¶.&lt;/dc:description&gt;</code>
</li>
</ul>
<p>
I will only focus on newspapers in French, even though newspapers in German also had articles in French. This is because the tag <code>&lt;dc:language&gt;fr&lt;/dc:language&gt;</code> is not always available. If it were, I could simply look for it and extract all the content in French easily, but unfortunately this is not the case.
</p>
<p>
First of all, let‚Äôs get the data into R:
</p>
<pre class="r"><code>library("tidyverse")
library("xml2")
library("furrr")

files &lt;- list.files(path = "export01-newspapers1841-1878/", all.files = TRUE, recursive = TRUE)</code></pre>
<p>
This results in a character vector with the path to all the files:
</p>
<pre class="r"><code>head(files)
[1] "000/1400000/1400000-ADVERTISEMENT-DTL78.xml"   "000/1400000/1400000-ADVERTISEMENT-DTL79.xml"  
[3] "000/1400000/1400000-ADVERTISEMENT-DTL80.xml"   "000/1400000/1400000-ADVERTISEMENT-DTL81.xml"  
[5] "000/1400000/1400000-MODSMD_ARTICLE1-DTL34.xml" "000/1400000/1400000-MODSMD_ARTICLE2-DTL35.xml"</code></pre>
<p>
Now I write a function that does the needed data preparation steps. I describe what the function does in the comments inside:
</p>
<pre class="r"><code>to_vw &lt;- function(xml_file){

    # read in the xml file
    file &lt;- read_xml(paste0("export01-newspapers1841-1878/", xml_file))

    # Get the newspaper
    newspaper &lt;- xml_find_all(file, ".//dcterms:isPartOf") %&gt;% xml_text()

    # Only keep the newspapers written in French
    if(!(newspaper %in% c("L'UNION.",
                          "L'ind√©pendance luxembourgeoise",
                          "COURRIER DU GRAND-DUCH√â DE LUXEMBOURG.",
                          "JOURNAL DE LUXEMBOURG.",
                          "L'AVENIR",
                          "L‚ÄôArlequin",
                          "La Gazette du Grand-Duch√© de Luxembourg",
                          "L'AVENIR DE LUXEMBOURG",
                          "L'AVENIR DU GRAND-DUCHE DE LUXEMBOURG.",
                          "L'AVENIR DU GRAND-DUCH√â DE LUXEMBOURG.",
                          "Le gratis luxembourgeois",
                          "Luxemburger Zeitung ‚Äì Journal de Luxembourg",
                          "Recueil des m√©moires et des travaux publi√©s par la Soci√©t√© de Botanique du Grand-Duch√© de Luxembourg"))){
        return(NULL)
    } else {
        # Get the type of the content. Can be article, advert, issue, section or other
        type &lt;- xml_find_all(file, ".//dc:type") %&gt;% xml_text()

        type &lt;- case_when(type == "ARTICLE" ~ "1",
                          type == "ADVERTISEMENT" ~ "2",
                          type == "ISSUE" ~ "3",
                          type == "SECTION" ~ "4",
                          TRUE ~ "5"
        )

        # Get the content itself. Only keep alphanumeric characters, and remove any line returns or 
        # carriage returns
        description &lt;- xml_find_all(file, ".//dc:description") %&gt;%
            xml_text() %&gt;%
            str_replace_all(pattern = "[^[:alnum:][:space:]]", "") %&gt;%
            str_to_lower() %&gt;%
            str_replace_all("\r?\n|\r|\n", " ")

        # Return the final object: one line that looks like this
        # 1 | bla bla
        paste(type, "|", description)
    }

}</code></pre>
<p>
I can now run this code to parse all the files, and I do so in parallel, thanks to the <code>{furrr}</code> package:
</p>
<pre class="r"><code>plan(multiprocess, workers = 12)

text_fr &lt;- files %&gt;%
    future_map(to_vw)

text_fr &lt;- text_fr %&gt;%
    discard(is.null)

write_lines(text_fr, "text_fr.txt")</code></pre>
</section>
<section id="step-2-install-vowpal-wabbit" class="level2">
<h2 class="anchored" data-anchor-id="step-2-install-vowpal-wabbit">
Step 2: Install Vowpal Wabbit
</h2>
<p>
To easiest way to install VW must be using Anaconda, and more specifically the conda package manager. Anaconda is a Python (and R) distribution for scientific computing and it comes with a package manager called conda which makes installing Python (or R) packages very easy. While VW is a standalone piece of software, it can also be installed by conda or pip. Instead of installing the full Anaconda distribution, you can install Miniconda, which only comes with the bare minimum: a Python executable and the conda package manager. You can find Miniconda <a href="https://docs.conda.io/en/latest/miniconda.html">here</a> and once it‚Äôs installed, you can install VW with:
</p>
<pre><code>conda install -c gwerbin vowpal-wabbit </code></pre>
<p>
It is also possible to install VW with pip, as detailed <a href="https://pypi.org/project/vowpalwabbit/">here</a>, but in my experience, managing Python packages with pip is not super. It is better to manage your Python distribution through conda, because it creates environments in your home folder which are independent of the system‚Äôs Python installation, which is often out-of-date.
</p>
</section>
<section id="step-3-building-a-model" class="level2">
<h2 class="anchored" data-anchor-id="step-3-building-a-model">
Step 3: Building <em>a</em> model
</h2>
<p>
Vowpal Wabbit can be used from the command line, but there are interfaces for Python and since a few weeks, for R. The R interface is quite crude for now, as it‚Äôs still in very early stages. I‚Äôm sure it will evolve, and perhaps a Vowpal Wabbit engine will be added to <code>{parsnip}</code>, which would make modeling with VW really easy.
</p>
<p>
For now, let‚Äôs only use 10000 lines for prototyping purposes before running the model on the whole file. Because the data is quite large, I do not want to import it into R. So I use command line tools to manipulate this data directly from my hard drive:
</p>
<pre class="r"><code># Prepare data
system2("shuf", args = "-n 10000 text_fr.txt &gt; small.txt")</code></pre>
<p>
<code>shuf</code> is a Unix command, and as such the above code should work on GNU/Linux systems, and most likely macOS too. <code>shuf</code> generates random permutations of a given file to standard output. I use <code>&gt;</code> to direct this output to another file, which I called <code>small.txt</code>. The <code>-n 10000</code> options simply means that I want 10000 lines.
</p>
<p>
I then split this small file into a training and a testing set:
</p>
<pre class="r"><code># Adapted from http://bitsearch.blogspot.com/2009/03/bash-script-to-split-train-and-test.html

# The command below counts the lines in small.txt. This is not really needed, since I know that the 
# file only has 10000 lines, but I kept it here for future reference
# notice the stdout = TRUE option. This is needed because the output simply gets shown in R's
# command line and does get saved into a variable.
nb_lines &lt;- system2("cat", args = "small.txt | wc -l", stdout = TRUE)

system2("split", args = paste0("-l", as.numeric(nb_lines)*0.99, " small.txt data_split/"))</code></pre>
<p>
<code>split</code> is the Unix command that does the splitting. I keep 99% of the lines in the training set and 1% in the test set. This creates two files, <code>aa</code> and <code>ab</code>. I rename them using the <code>mv</code> Unix command:
</p>
<pre class="r"><code>system2("mv", args = "data_split/aa data_split/small_train.txt")
system2("mv", args = "data_split/ab data_split/small_test.txt")</code></pre>
<p>
Ok, now let‚Äôs run a model using the VW command line utility from R, using <code>system2()</code>:
</p>
<pre class="r"><code>oaa_fit &lt;- system2("~/miniconda3/bin/vw", args = "--oaa 5 -d data_split/small_train.txt -f small_oaa.model", stderr = TRUE)</code></pre>
<p>
I need to point <code>system2()</code> to the <code>vw</code> executable, and then add some options. <code>‚Äìoaa</code> stands for <em>one against all</em> and is a way of doing multiclass classification; first, one class gets classified by a logistic classifier against all the others, then the other class against all the others, then the other‚Ä¶. The <code>5</code> in the option means that there are 5 classes.
</p>
<p>
<code>-d data_split/train.txt</code> specifies the path to the training data. <code>-f</code> means ‚Äúfinal regressor‚Äù and specifies where you want to save the trained model.
</p>
<p>
This is the output that get‚Äôs captured and saved into <code>oaa_fit</code>:
</p>
<pre><code> [1] "final_regressor = oaa.model"                                             
 [2] "Num weight bits = 18"                                                    
 [3] "learning rate = 0.5"                                                     
 [4] "initial_t = 0"                                                           
 [5] "power_t = 0.5"                                                           
 [6] "using no cache"                                                          
 [7] "Reading datafile = data_split/train.txt"                                 
 [8] "num sources = 1"                                                         
 [9] "average  since         example        example  current  current  current"
[10] "loss     last          counter         weight    label  predict features"
[11] "1.000000 1.000000            1            1.0        3        1       87"
[12] "1.000000 1.000000            2            2.0        1        3     2951"
[13] "1.000000 1.000000            4            4.0        1        3      506"
[14] "0.625000 0.250000            8            8.0        1        1      262"
[15] "0.625000 0.625000           16           16.0        1        2      926"
[16] "0.500000 0.375000           32           32.0        4        1        3"
[17] "0.375000 0.250000           64           64.0        1        1      436"
[18] "0.296875 0.218750          128          128.0        2        2      277"
[19] "0.238281 0.179688          256          256.0        2        2      118"
[20] "0.158203 0.078125          512          512.0        2        2       61"
[21] "0.125000 0.091797         1024         1024.0        2        2      258"
[22] "0.096191 0.067383         2048         2048.0        1        1       45"
[23] "0.085205 0.074219         4096         4096.0        1        1      318"
[24] "0.076172 0.067139         8192         8192.0        2        1      523"
[25] ""                                                                        
[26] "finished run"                                                            
[27] "number of examples = 9900"                                               
[28] "weighted example sum = 9900.000000"                                      
[29] "weighted label sum = 0.000000"                                           
[30] "average loss = 0.073434"                                                 
[31] "total feature number = 4456798"  </code></pre>
<p>
Now, when I try to run the same model using <code>RVowpalWabbit::vw()</code> I get the following error:
</p>
<pre class="r"><code>oaa_class &lt;- c("--oaa", "5",
               "-d", "data_split/small_train.txt",
               "-f", "vw_models/small_oaa.model")

result &lt;- vw(oaa_class)</code></pre>
<pre><code>Error in Rvw(args) : unrecognised option '--oaa'</code></pre>
<p>
I think the problem might be because I installed Vowpal Wabbit using conda, and the package cannot find the executable. I‚Äôll open an issue with reproducible code and we‚Äôll see.
</p>
<p>
In any case, that‚Äôs it for now! In the next blog post, we‚Äôll see how to get the accuracy of this very simple model, and see how to improve it!
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-03-03-historical_vowpal.html</guid>
  <pubDate>Sun, 03 Mar 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Manipulating strings with the {stringr} package</title>
  <link>https://b-rodrigues.github.io/posts/2019-02-10-stringr_package.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://b-rodrigues.github.io/modern_R/descriptive-statistics-and-data-manipulation.html#manipulate-strings-with-stringr"> <img src="https://b-rodrigues.github.io/assets/img/string.jpg" title="Click here to go the ebook"></a>
</p>
</div>
<p>
This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free <a href="https://b-rodrigues.github.io/modern_R/">here</a>. This is taken from Chapter 4, in which I introduce the <code>{stringr}</code> package.
</p>
<section id="manipulate-strings-with-stringr" class="level2">
<h2 class="anchored" data-anchor-id="manipulate-strings-with-stringr">
Manipulate strings with <code>{stringr}</code>
</h2>
<p>
<code>{stringr}</code> contains functions to manipulate strings. In Chapter 10, I will teach you about regular expressions, but the functions contained in <code>{stringr}</code> allow you to already do a lot of work on strings, without needing to be a regular expression expert.
</p>
<p>
I will discuss the most common string operations: detecting, locating, matching, searching and replacing, and exctracting/removing strings.
</p>
<p>
To introduce these operations, let us use an ALTO file of an issue of <em>The Winchester News</em> from October 31, 1910, which you can find on this <a href="https://gist.githubusercontent.com/b-rodrigues/5139560e7d0f2ecebe5da1df3629e015/raw/e3031d894ffb97217ddbad1ade1b307c9937d2c8/gistfile1.txt">link</a> (to see how the newspaper looked like, <a href="https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/">click here</a>). I re-hosted the file on a public gist for archiving purposes. While working on the book, the original site went down several times‚Ä¶
</p>
<p>
ALTO is an XML schema for the description of text OCR and layout information of pages for digitzed material, such as newspapers (source: <a href="https://en.wikipedia.org/wiki/ALTO_(XML)">ALTO Wikipedia page</a>). For more details, you can read my <a href="https://www.brodrigues.co/blog/2019-01-13-newspapers_mets_alto/">blogpost</a> on the matter, but for our current purposes, it is enough to know that the file contains the text of newspaper articles. The file looks like this:
</p>
<pre><code>&lt;TextLine HEIGHT="138.0" WIDTH="2434.0" HPOS="4056.0" VPOS="5814.0"&gt;
&lt;String STYLEREFS="ID7" HEIGHT="108.0" WIDTH="393.0" HPOS="4056.0" VPOS="5838.0" CONTENT="timore" WC="0.82539684"&gt;
&lt;ALTERNATIVE&gt;timole&lt;/ALTERNATIVE&gt;
&lt;ALTERNATIVE&gt;tlnldre&lt;/ALTERNATIVE&gt;
&lt;ALTERNATIVE&gt;timor&lt;/ALTERNATIVE&gt;
&lt;ALTERNATIVE&gt;insole&lt;/ALTERNATIVE&gt;
&lt;ALTERNATIVE&gt;landed&lt;/ALTERNATIVE&gt;
&lt;/String&gt;
&lt;SP WIDTH="74.0" HPOS="4449.0" VPOS="5838.0"/&gt;
&lt;String STYLEREFS="ID7" HEIGHT="105.0" WIDTH="432.0" HPOS="4524.0" VPOS="5847.0" CONTENT="market" WC="0.95238096"/&gt;
&lt;SP WIDTH="116.0" HPOS="4956.0" VPOS="5847.0"/&gt;
&lt;String STYLEREFS="ID7" HEIGHT="69.0" WIDTH="138.0" HPOS="5073.0" VPOS="5883.0" CONTENT="as" WC="0.96825397"/&gt;
&lt;SP WIDTH="74.0" HPOS="5211.0" VPOS="5883.0"/&gt;
&lt;String STYLEREFS="ID7" HEIGHT="69.0" WIDTH="285.0" HPOS="5286.0" VPOS="5877.0" CONTENT="were" WC="1.0"&gt;
&lt;ALTERNATIVE&gt;verc&lt;/ALTERNATIVE&gt;
&lt;ALTERNATIVE&gt;veer&lt;/ALTERNATIVE&gt;
&lt;/String&gt;
&lt;SP WIDTH="68.0" HPOS="5571.0" VPOS="5877.0"/&gt;
&lt;String STYLEREFS="ID7" HEIGHT="111.0" WIDTH="147.0" HPOS="5640.0" VPOS="5838.0" CONTENT="all" WC="1.0"/&gt;
&lt;SP WIDTH="83.0" HPOS="5787.0" VPOS="5838.0"/&gt;
&lt;String STYLEREFS="ID7" HEIGHT="111.0" WIDTH="183.0" HPOS="5871.0" VPOS="5835.0" CONTENT="the" WC="0.95238096"&gt;
&lt;ALTERNATIVE&gt;tll&lt;/ALTERNATIVE&gt;
&lt;ALTERNATIVE&gt;Cu&lt;/ALTERNATIVE&gt;
&lt;ALTERNATIVE&gt;tall&lt;/ALTERNATIVE&gt;
&lt;/String&gt;
&lt;SP WIDTH="75.0" HPOS="6054.0" VPOS="5835.0"/&gt;
&lt;String STYLEREFS="ID3" HEIGHT="132.0" WIDTH="351.0" HPOS="6129.0" VPOS="5814.0" CONTENT="cattle" WC="0.95238096"/&gt;
&lt;/TextLine&gt;</code></pre>
<p>
We are interested in the strings after <code>CONTENT=</code>. We are going to use functions from the <code>{stringr}</code> package to get the strings after <code>CONTENT=</code>. In Chapter 10, we are going to explore this file again, but using complex regular expressions to get all the content in one go.
</p>
<section id="getting-text-data-into-rstudio" class="level3">
<h3 class="anchored" data-anchor-id="getting-text-data-into-rstudio">
Getting text data into Rstudio
</h3>
<p>
First of all, let us read in the file:
</p>
<pre class="r"><code>winchester &lt;- read_lines("https://gist.githubusercontent.com/b-rodrigues/5139560e7d0f2ecebe5da1df3629e015/raw/e3031d894ffb97217ddbad1ade1b307c9937d2c8/gistfile1.txt")</code></pre>
<p>
Even though the file is an XML file, I still read it in using <code>read_lines()</code> and not <code>read_xml()</code> from the <code>{xml2}</code> package. This is for the purposes of the current exercise, and also because I always have trouble with XML files, and prefer to treat them as simple text files, and use regular expressions to get what I need.
</p>
<p>
Now that the ALTO file is read in and saved in the <code>winchester</code> variable, you might want to print the whole thing in the console. Before that, take a look at the structure:
</p>
<pre class="r"><code>str(winchester)</code></pre>
<pre><code>##  chr [1:43] "" ...</code></pre>
<p>
So the <code>winchester</code> variable is a character atomic vector with 43 elements. So first, we need to understand what these elements are. Let‚Äôs start with the first one:
</p>
<pre class="r"><code>winchester[1]</code></pre>
<pre><code>## [1] ""</code></pre>
<p>
Ok, so it seems like the first element is part of the header of the file. What about the second one?
</p>
<pre class="r"><code>winchester[2]</code></pre>
<pre><code>## [1] "&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"&gt;&lt;base href=\"https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml\"&gt;&lt;style&gt;body{margin-left:0;margin-right:0;margin-top:0}#bN015htcoyT__google-cache-hdr{background:#f5f5f5;font:13px arial,sans-serif;text-align:left;color:#202020;border:0;margin:0;border-bottom:1px solid #cecece;line-height:16px;padding:16px 28px 24px 28px}#bN015htcoyT__google-cache-hdr *{display:inline;font:inherit;text-align:inherit;color:inherit;line-height:inherit;background:none;border:0;margin:0;padding:0;letter-spacing:0}#bN015htcoyT__google-cache-hdr a{text-decoration:none;color:#1a0dab}#bN015htcoyT__google-cache-hdr a:hover{text-decoration:underline}#bN015htcoyT__google-cache-hdr a:visited{color:#609}#bN015htcoyT__google-cache-hdr div{display:block;margin-top:4px}#bN015htcoyT__google-cache-hdr b{font-weight:bold;display:inline-block;direction:ltr}&lt;/style&gt;&lt;div id=\"bN015htcoyT__google-cache-hdr\"&gt;&lt;div&gt;&lt;span&gt;This is Google's cache of &lt;a href=\"https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml\"&gt;https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml&lt;/a&gt;.&lt;/span&gt;&amp;nbsp;&lt;span&gt;It is a snapshot of the page as it appeared on 21 Jan 2019 05:18:18 GMT.&lt;/span&gt;&amp;nbsp;&lt;span&gt;The &lt;a href=\"https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml\"&gt;current page&lt;/a&gt; could have changed in the meantime.&lt;/span&gt;&amp;nbsp;&lt;a href=\"http://support.google.com/websearch/bin/answer.py?hl=en&amp;amp;p=cached&amp;amp;answer=1687222\"&gt;&lt;span&gt;Learn more&lt;/span&gt;.&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=\"display:inline-block;margin-top:8px;margin-right:104px;white-space:nowrap\"&gt;&lt;span style=\"margin-right:28px\"&gt;&lt;span style=\"font-weight:bold\"&gt;Full version&lt;/span&gt;&lt;/span&gt;&lt;span style=\"margin-right:28px\"&gt;&lt;a href=\"http://webcache.googleusercontent.com/search?q=cache:2BVPV8QGj3oJ:https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml&amp;amp;hl=en&amp;amp;gl=lu&amp;amp;strip=1&amp;amp;vwsrc=0\"&gt;&lt;span&gt;Text-only version&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;span style=\"margin-right:28px\"&gt;&lt;a href=\"http://webcache.googleusercontent.com/search?q=cache:2BVPV8QGj3oJ:https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml&amp;amp;hl=en&amp;amp;gl=lu&amp;amp;strip=0&amp;amp;vwsrc=1\"&gt;&lt;span&gt;View source&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;span style=\"display:inline-block;margin-top:8px;color:#717171\"&gt;&lt;span&gt;Tip: To quickly find your search term on this page, press &lt;b&gt;Ctrl+F&lt;/b&gt; or &lt;b&gt;‚åò-F&lt;/b&gt; (Mac) and use the find bar.&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div style=\"position:relative;\"&gt;&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;"</code></pre>
<p>
Same. So where is the content? The file is very large, so if you print it in the console, it will take quite some time to print, and you will not really be able to make out anything. The best way would be to try to detect the string <code>CONTENT</code> and work from there.
</p>
</section>
<section id="detecting-getting-the-position-and-locating-strings" class="level3">
<h3 class="anchored" data-anchor-id="detecting-getting-the-position-and-locating-strings">
Detecting, getting the position and locating strings
</h3>
<p>
When confronted to an atomic vector of strings, you might want to know inside which elements you can find certain strings. For example, to know which elements of <code>winchester</code> contain the string <code>CONTENT</code>, use <code>str_detect()</code>:
</p>
<pre class="r"><code>winchester %&gt;%
  str_detect("CONTENT")</code></pre>
<pre><code>##  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
## [12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
## [23] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
## [34] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE</code></pre>
<p>
This returns a boolean atomic vector of the same length as <code>winchester</code>. If the string <code>CONTENT</code> is nowhere to be found, the result will equal <code>FALSE</code>, if not it will equal <code>TRUE</code>. Here it is easy to see that the last element contains the string <code>CONTENT</code>. But what if instead of having 43 elements, the vector had 24192 elements? And hundreds would contain the string <code>CONTENT</code>? It would be easier to instead have the indices of the vector where one can find the word <code>CONTENT</code>. This is possible with <code>str_which()</code>:
</p>
<pre class="r"><code>winchester %&gt;%
  str_which("CONTENT")</code></pre>
<pre><code>## [1] 43</code></pre>
<p>
Here, the result is 43, meaning that the 43rd element of <code>winchester</code> contains the string <code>CONTENT</code> somewhere. If we need more precision, we can use <code>str_locate()</code> and <code>str_locate_all()</code>. To explain how both these functions work, let‚Äôs create a very small example:
</p>
<pre class="r"><code>ancient_philosophers &lt;- c("aristotle", "plato", "epictetus", "seneca the younger", "epicurus", "marcus aurelius")</code></pre>
<p>
Now suppose I am interested in philosophers whose name ends in <code>us</code>. Let us use <code>str_locate()</code> first:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_locate("us")</code></pre>
<pre><code>##      start end
## [1,]    NA  NA
## [2,]    NA  NA
## [3,]     8   9
## [4,]    NA  NA
## [5,]     7   8
## [6,]     5   6</code></pre>
<p>
You can interpret the result as follows: in the rows, the index of the vector where the string <code>us</code> is found. So the 3rd, 5th and 6th philosopher have <code>us</code> somewhere in their name. The result also has two columns: <code>start</code> and <code>end</code>. These give the position of the string. So the string <code>us</code> can be found starting at position 8 of the 3rd element of the vector, and ends at position 9. Same goes for the other philisophers. However, consider Marcus Aurelius. He has two names, both ending with <code>us</code>. However, <code>str_locate()</code> only shows the position of the <code>us</code> in <code>Marcus</code>.
</p>
<p>
To get both <code>us</code> strings, you need to use <code>str_locate_all()</code>:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_locate_all("us")</code></pre>
<pre><code>## [[1]]
##      start end
## 
## [[2]]
##      start end
## 
## [[3]]
##      start end
## [1,]     8   9
## 
## [[4]]
##      start end
## 
## [[5]]
##      start end
## [1,]     7   8
## 
## [[6]]
##      start end
## [1,]     5   6
## [2,]    14  15</code></pre>
<p>
Now we get the position of the two <code>us</code> in Marcus Aurelius. Doing this on the <code>winchester</code> vector will give use the position of the <code>CONTENT</code> string, but this is not really important right now. What matters is that you know how <code>str_locate()</code> and <code>str_locate_all()</code> work.
</p>
<p>
So now that we know what interests us in the 43nd element of <code>winchester</code>, let‚Äôs take a closer look at it:
</p>
<pre class="r"><code>winchester[43]</code></pre>
<p>
As you can see, it‚Äôs a mess:
</p>
<pre><code>&lt;TextLine HEIGHT=\"126.0\" WIDTH=\"1731.0\" HPOS=\"17160.0\" VPOS=\"21252.0\"&gt;&lt;String HEIGHT=\"114.0\" WIDTH=\"354.0\" HPOS=\"17160.0\" VPOS=\"21264.0\" CONTENT=\"0tV\" WC=\"0.8095238\"/&gt;&lt;SP WIDTH=\"131.0\" HPOS=\"17514.0\" VPOS=\"21264.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"111.0\" WIDTH=\"474.0\" HPOS=\"17646.0\" VPOS=\"21258.0\" CONTENT=\"BATES\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"140.0\" HPOS=\"18120.0\" VPOS=\"21258.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"114.0\" WIDTH=\"630.0\" HPOS=\"18261.0\" VPOS=\"21252.0\" CONTENT=\"President\" WC=\"1.0\"&gt;&lt;ALTERNATIVE&gt;Prcideht&lt;/ALTERNATIVE&gt;&lt;ALTERNATIVE&gt;Pride&lt;/ALTERNATIVE&gt;&lt;/String&gt;&lt;/TextLine&gt;&lt;TextLine HEIGHT=\"153.0\" WIDTH=\"1689.0\" HPOS=\"17145.0\" VPOS=\"21417.0\"&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"105.0\" WIDTH=\"258.0\" HPOS=\"17145.0\" VPOS=\"21439.0\" CONTENT=\"WM\" WC=\"0.82539684\"&gt;&lt;TextLine HEIGHT=\"120.0\" WIDTH=\"2211.0\" HPOS=\"16788.0\" VPOS=\"21870.0\"&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"96.0\" WIDTH=\"102.0\" HPOS=\"16788.0\" VPOS=\"21894.0\" CONTENT=\"It\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"72.0\" HPOS=\"16890.0\" VPOS=\"21894.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"96.0\" WIDTH=\"93.0\" HPOS=\"16962.0\" VPOS=\"21885.0\" CONTENT=\"is\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"80.0\" HPOS=\"17055.0\" VPOS=\"21885.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"102.0\" WIDTH=\"417.0\" HPOS=\"17136.0\" VPOS=\"21879.0\" CONTENT=\"seldom\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"80.0\" HPOS=\"17553.0\" VPOS=\"21879.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"96.0\" WIDTH=\"267.0\" HPOS=\"17634.0\" VPOS=\"21873.0\" CONTENT=\"hard\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"81.0\" HPOS=\"17901.0\" VPOS=\"21873.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"87.0\" WIDTH=\"111.0\" HPOS=\"17982.0\" VPOS=\"21879.0\" CONTENT=\"to\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"81.0\" HPOS=\"18093.0\" VPOS=\"21879.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"96.0\" WIDTH=\"219.0\" HPOS=\"18174.0\" VPOS=\"21870.0\" CONTENT=\"find\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"77.0\" HPOS=\"18393.0\" VPOS=\"21870.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"69.0\" WIDTH=\"66.0\" HPOS=\"18471.0\" VPOS=\"21894.0\" CONTENT=\"a\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"77.0\" HPOS=\"18537.0\" VPOS=\"21894.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"78.0\" WIDTH=\"384.0\" HPOS=\"18615.0\" VPOS=\"21888.0\" CONTENT=\"succes\" WC=\"0.82539684\"&gt;&lt;ALTERNATIVE&gt;success&lt;/ALTERNATIVE&gt;&lt;/String&gt;&lt;/TextLine&gt;&lt;TextLine HEIGHT=\"126.0\" WIDTH=\"2316.0\" HPOS=\"16662.0\" VPOS=\"22008.0\"&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"75.0\" WIDTH=\"183.0\" HPOS=\"16662.0\" VPOS=\"22059.0\" CONTENT=\"sor\" WC=\"1.0\"&gt;&lt;ALTERNATIVE&gt;soar&lt;/ALTERNATIVE&gt;&lt;/String&gt;&lt;SP WIDTH=\"72.0\" HPOS=\"16845.0\" VPOS=\"22059.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"90.0\" WIDTH=\"168.0\" HPOS=\"16917.0\" VPOS=\"22035.0\" CONTENT=\"for\" WC=\"1.0\"/&gt;&lt;SP WIDTH=\"72.0\" HPOS=\"17085.0\" VPOS=\"22035.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"69.0\" WIDTH=\"267.0\" HPOS=\"17157.0\" VPOS=\"22050.0\" CONTENT=\"even\" WC=\"1.0\"&gt;&lt;ALTERNATIVE&gt;cen&lt;/ALTERNATIVE&gt;&lt;ALTERNATIVE&gt;cent&lt;/ALTERNATIVE&gt;&lt;/String&gt;&lt;SP WIDTH=\"77.0\" HPOS=\"17434.0\" VPOS=\"22050.0\"/&gt;&lt;String STYLEREFS=\"ID7\" HEIGHT=\"66.0\" WIDTH=\"63.0\" HPOS=\"17502.0\" VPOS=\"22044.0\"</code></pre>
<p>
The file was imported without any newlines. So we need to insert them ourselves, by splitting the string in a clever way.
</p>
</section>
<section id="splitting-strings" class="level3">
<h3 class="anchored" data-anchor-id="splitting-strings">
Splitting strings
</h3>
<p>
There are two functions included in <code>{stringr}</code> to split strings, <code>str_split()</code> and <code>str_split_fixed()</code>. Let‚Äôs go back to our ancient philosophers. Two of them, Seneca the Younger and Marcus Aurelius have something else in common than both being Roman Stoic philosophers. Their names are composed of several words. If we want to split their names at the space character, we can use <code>str_split()</code> like this:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_split(" ")</code></pre>
<pre><code>## [[1]]
## [1] "aristotle"
## 
## [[2]]
## [1] "plato"
## 
## [[3]]
## [1] "epictetus"
## 
## [[4]]
## [1] "seneca"  "the"     "younger"
## 
## [[5]]
## [1] "epicurus"
## 
## [[6]]
## [1] "marcus"   "aurelius"</code></pre>
<p>
<code>str_split()</code> also has a <code>simplify = TRUE</code> option:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_split(" ", simplify = TRUE)</code></pre>
<pre><code>##      [,1]        [,2]       [,3]     
## [1,] "aristotle" ""         ""       
## [2,] "plato"     ""         ""       
## [3,] "epictetus" ""         ""       
## [4,] "seneca"    "the"      "younger"
## [5,] "epicurus"  ""         ""       
## [6,] "marcus"    "aurelius" ""</code></pre>
<p>
This time, the returned object is a matrix.
</p>
<p>
What about <code>str_split_fixed()</code>? The difference is that here you can specify the number of pieces to return. For example, you could consider the name ‚ÄúAurelius‚Äù to be the middle name of Marcus Aurelius, and the ‚Äúthe younger‚Äù to be the middle name of Seneca the younger. This means that you would want to split the name only at the first space character, and not at all of them. This is easily achieved with <code>str_split_fixed()</code>:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_split_fixed(" ", 2)</code></pre>
<pre><code>##      [,1]        [,2]         
## [1,] "aristotle" ""           
## [2,] "plato"     ""           
## [3,] "epictetus" ""           
## [4,] "seneca"    "the younger"
## [5,] "epicurus"  ""           
## [6,] "marcus"    "aurelius"</code></pre>
<p>
This gives the expected result.
</p>
<p>
So how does this help in our case? Well, if you look at how the ALTO file looks like, at the beginning of this section, you will notice that every line ends with the ‚Äú&gt;‚Äù character. So let‚Äôs split at that character!
</p>
<pre class="r"><code>winchester_text &lt;- winchester[43] %&gt;%
  str_split("&gt;")</code></pre>
<p>
Let‚Äôs take a closer look at <code>winchester_text</code>:
</p>
<pre class="r"><code>str(winchester_text)</code></pre>
<pre><code>## List of 1
##  $ : chr [1:19706] "&lt;/processingStepSettings" "&lt;processingSoftware" "&lt;softwareCreator" "iArchives&lt;/softwareCreator" ...</code></pre>
<p>
So this is a list of length one, and the first, and only, element of that list is an atomic vector with 19706 elements. Since this is a list of only one element, we can simplify it by saving the atomic vector in a variable:
</p>
<pre class="r"><code>winchester_text &lt;- winchester_text[[1]]</code></pre>
<p>
Let‚Äôs now look at some lines:
</p>
<pre class="r"><code>winchester_text[1232:1245]</code></pre>
<pre><code>##  [1] "&lt;SP WIDTH=\"66.0\" HPOS=\"5763.0\" VPOS=\"9696.0\"/"                                                                         
##  [2] "&lt;String STYLEREFS=\"ID7\" HEIGHT=\"108.0\" WIDTH=\"612.0\" HPOS=\"5829.0\" VPOS=\"9693.0\" CONTENT=\"Louisville\" WC=\"1.0\""
##  [3] "&lt;ALTERNATIVE"                                                                                                                
##  [4] "Loniile&lt;/ALTERNATIVE"                                                                                                        
##  [5] "&lt;ALTERNATIVE"                                                                                                                
##  [6] "Lenities&lt;/ALTERNATIVE"                                                                                                       
##  [7] "&lt;/String"                                                                                                                    
##  [8] "&lt;/TextLine"                                                                                                                  
##  [9] "&lt;TextLine HEIGHT=\"150.0\" WIDTH=\"2520.0\" HPOS=\"4032.0\" VPOS=\"9849.0\""                                                 
## [10] "&lt;String STYLEREFS=\"ID7\" HEIGHT=\"108.0\" WIDTH=\"510.0\" HPOS=\"4032.0\" VPOS=\"9861.0\" CONTENT=\"Tobacco\" WC=\"1.0\"/"  
## [11] "&lt;SP WIDTH=\"113.0\" HPOS=\"4542.0\" VPOS=\"9861.0\"/"                                                                        
## [12] "&lt;String STYLEREFS=\"ID7\" HEIGHT=\"105.0\" WIDTH=\"696.0\" HPOS=\"4656.0\" VPOS=\"9861.0\" CONTENT=\"Warehouse\" WC=\"1.0\"" 
## [13] "&lt;ALTERNATIVE"                                                                                                                
## [14] "WHrchons&lt;/ALTERNATIVE"</code></pre>
<p>
This now looks easier to handle. We can narrow it down to the lines that only contain the string we are interested in, ‚ÄúCONTENT‚Äù. First, let‚Äôs get the indices:
</p>
<pre class="r"><code>content_winchester_index &lt;- winchester_text %&gt;%
  str_which("CONTENT")</code></pre>
<p>
How many lines contain the string ‚ÄúCONTENT‚Äù?
</p>
<pre class="r"><code>length(content_winchester_index)</code></pre>
<pre><code>## [1] 4462</code></pre>
<p>
As you can see, this reduces the amount of data we have to work with. Let us save this is a new variable:
</p>
<pre class="r"><code>content_winchester &lt;- winchester_text[content_winchester_index]</code></pre>
</section>
<section id="matching-strings" class="level3">
<h3 class="anchored" data-anchor-id="matching-strings">
Matching strings
</h3>
<p>
Matching strings is useful, but only in combination with regular expressions. As stated at the beginning of this section, we are going to learn about regular expressions in Chapter 10, but in order to make this section useful, we are going to learn the easiest, but perhaps the most useful regular expression: <code>.*</code>.
</p>
<p>
Let‚Äôs go back to our ancient philosophers, and use <code>str_match()</code> and see what happens. Let‚Äôs match the ‚Äúus‚Äù string:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_match("us")</code></pre>
<pre><code>##      [,1]
## [1,] NA  
## [2,] NA  
## [3,] "us"
## [4,] NA  
## [5,] "us"
## [6,] "us"</code></pre>
<p>
Not very useful, but what about the regular expression <code>.*</code>? How could it help?
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_match(".*us")</code></pre>
<pre><code>##      [,1]             
## [1,] NA               
## [2,] NA               
## [3,] "epictetus"      
## [4,] NA               
## [5,] "epicurus"       
## [6,] "marcus aurelius"</code></pre>
<p>
That‚Äôs already very interesting! So how does <code>.*</code> work? To understand, let‚Äôs first start by using <code>.</code> alone:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_match(".us")</code></pre>
<pre><code>##      [,1] 
## [1,] NA   
## [2,] NA   
## [3,] "tus"
## [4,] NA   
## [5,] "rus"
## [6,] "cus"</code></pre>
<p>
This also matched whatever symbol comes just before the ‚Äúu‚Äù from ‚Äúus‚Äù. What if we use two <code>.</code> instead?
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_match("..us")</code></pre>
<pre><code>##      [,1]  
## [1,] NA    
## [2,] NA    
## [3,] "etus"
## [4,] NA    
## [5,] "urus"
## [6,] "rcus"</code></pre>
<p>
This time, we get the two symbols that immediately precede ‚Äúus‚Äù. Instead of continuing like this we now use the <code><em></em></code><em>, which matches zero or more of <code>.</code>. So by combining <code></code></em> and <code>.</code>, we can match any symbol repeatedly, until there is nothing more to match. Note that there is also <code>+</code>, which works similarly to <code>*</code>, but it matches one or more symbols.
</p>
<p>
There is also a <code>str_match_all()</code>:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_match_all(".*us")</code></pre>
<pre><code>## [[1]]
##      [,1]
## 
## [[2]]
##      [,1]
## 
## [[3]]
##      [,1]       
## [1,] "epictetus"
## 
## [[4]]
##      [,1]
## 
## [[5]]
##      [,1]      
## [1,] "epicurus"
## 
## [[6]]
##      [,1]             
## [1,] "marcus aurelius"</code></pre>
<p>
In this particular case it does not change the end result, but keep it in mind for cases like this one:
</p>
<pre class="r"><code>c("haha", "huhu") %&gt;%
  str_match("ha")</code></pre>
<pre><code>##      [,1]
## [1,] "ha"
## [2,] NA</code></pre>
<p>
and:
</p>
<pre class="r"><code>c("haha", "huhu") %&gt;%
  str_match_all("ha")</code></pre>
<pre><code>## [[1]]
##      [,1]
## [1,] "ha"
## [2,] "ha"
## 
## [[2]]
##      [,1]</code></pre>
<p>
What if we want to match names containing the letter ‚Äút‚Äù? Easy:
</p>
<pre class="r"><code>ancient_philosophers %&gt;%
  str_match(".*t.*")</code></pre>
<pre><code>##      [,1]                
## [1,] "aristotle"         
## [2,] "plato"             
## [3,] "epictetus"         
## [4,] "seneca the younger"
## [5,] NA                  
## [6,] NA</code></pre>
<p>
So how does this help us with our historical newspaper? Let‚Äôs try to get the strings that come after ‚ÄúCONTENT‚Äù:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_text %&gt;%
  str_match("CONTENT.*")</code></pre>
<p>
Let‚Äôs use our faithful <code>str()</code> function to take a look:
</p>
<pre class="r"><code>winchester_content %&gt;%
  str</code></pre>
<pre><code>##  chr [1:19706, 1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ...</code></pre>
<p>
Hum, there‚Äôs a lot of <code>NA</code> values! This is because a lot of the lines from the file did not have the string ‚ÄúCONTENT‚Äù, so there is no match possible. Let‚Äôs us remove all these <code>NA</code>s. Because the result is a matrix, we cannot use the <code>filter()</code> function from <code>{dplyr}</code>. So we need to convert it to a tibble first:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_content %&gt;%
  as.tibble() %&gt;%
  filter(!is.na(V1))</code></pre>
<pre><code>## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics).
## This warning is displayed once per session.</code></pre>
<p>
Because matrix columns do not have names, when a matrix gets converted into a tibble, the firt column gets automatically called <code>V1</code>. This is why I filter on this column. Let‚Äôs take a look at the data:
</p>
<pre class="r"><code>head(winchester_content)</code></pre>
<pre><code>## # A tibble: 6 x 1
##   V1                                  
##   &lt;chr&gt;                               
## 1 "CONTENT=\"J\" WC=\"0.8095238\"/"   
## 2 "CONTENT=\"a\" WC=\"0.8095238\"/"   
## 3 "CONTENT=\"Ira\" WC=\"0.95238096\"/"
## 4 "CONTENT=\"mj\" WC=\"0.8095238\"/"  
## 5 "CONTENT=\"iI\" WC=\"0.8095238\"/"  
## 6 "CONTENT=\"tE1r\" WC=\"0.8095238\"/"</code></pre>
</section>
<section id="searching-and-replacing-strings" class="level3">
<h3 class="anchored" data-anchor-id="searching-and-replacing-strings">
Searching and replacing strings
</h3>
<p>
We are getting close to the final result. We still need to do some cleaning however. Since our data is inside a nice tibble, we might as well stick with it. So let‚Äôs first rename the column and change all the strings to lowercase:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_content %&gt;% 
  mutate(content = tolower(V1)) %&gt;% 
  select(-V1)</code></pre>
<p>
Let‚Äôs take a look at the result:
</p>
<pre class="r"><code>head(winchester_content)</code></pre>
<pre><code>## # A tibble: 6 x 1
##   content                             
##   &lt;chr&gt;                               
## 1 "content=\"j\" wc=\"0.8095238\"/"   
## 2 "content=\"a\" wc=\"0.8095238\"/"   
## 3 "content=\"ira\" wc=\"0.95238096\"/"
## 4 "content=\"mj\" wc=\"0.8095238\"/"  
## 5 "content=\"ii\" wc=\"0.8095238\"/"  
## 6 "content=\"te1r\" wc=\"0.8095238\"/"</code></pre>
<p>
The second part of the string, ‚Äúwc=‚Ä¶.‚Äù is not really interesting. Let‚Äôs search and replace this with an empty string, using <code>str_replace()</code>:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_content %&gt;% 
  mutate(content = str_replace(content, "wc.*", ""))

head(winchester_content)</code></pre>
<pre><code>## # A tibble: 6 x 1
##   content            
##   &lt;chr&gt;              
## 1 "content=\"j\" "   
## 2 "content=\"a\" "   
## 3 "content=\"ira\" " 
## 4 "content=\"mj\" "  
## 5 "content=\"ii\" "  
## 6 "content=\"te1r\" "</code></pre>
<p>
We need to use the regular expression from before to replace ‚Äúwc‚Äù and every character that follows. The same can be use to remove ‚Äúcontent=‚Äù:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_content %&gt;% 
  mutate(content = str_replace(content, "content=", ""))

head(winchester_content)</code></pre>
<pre><code>## # A tibble: 6 x 1
##   content    
##   &lt;chr&gt;      
## 1 "\"j\" "   
## 2 "\"a\" "   
## 3 "\"ira\" " 
## 4 "\"mj\" "  
## 5 "\"ii\" "  
## 6 "\"te1r\" "</code></pre>
<p>
We are almost done, but some cleaning is still necessary:
</p>
</section>
<section id="exctracting-or-removing-strings" class="level3">
<h3 class="anchored" data-anchor-id="exctracting-or-removing-strings">
Exctracting or removing strings
</h3>
<p>
Now, because I now the ALTO spec, I know how to find words that are split between two sentences:
</p>
<pre class="r"><code>winchester_content %&gt;% 
  filter(str_detect(content, "hyppart"))</code></pre>
<pre><code>## # A tibble: 64 x 1
##    content                                                               
##    &lt;chr&gt;                                                                 
##  1 "\"aver\" subs_type=\"hyppart1\" subs_content=\"average\" "           
##  2 "\"age\" subs_type=\"hyppart2\" subs_content=\"average\" "            
##  3 "\"considera\" subs_type=\"hyppart1\" subs_content=\"consideration\" "
##  4 "\"tion\" subs_type=\"hyppart2\" subs_content=\"consideration\" "     
##  5 "\"re\" subs_type=\"hyppart1\" subs_content=\"resigned\" "            
##  6 "\"signed\" subs_type=\"hyppart2\" subs_content=\"resigned\" "        
##  7 "\"install\" subs_type=\"hyppart1\" subs_content=\"installed\" "      
##  8 "\"ed\" subs_type=\"hyppart2\" subs_content=\"installed\" "           
##  9 "\"be\" subs_type=\"hyppart1\" subs_content=\"before\" "              
## 10 "\"fore\" subs_type=\"hyppart2\" subs_content=\"before\" "            
## # ‚Ä¶ with 54 more rows</code></pre>
<p>
For instance, the word ‚Äúaverage‚Äù was split over two lines, the first part of the word, ‚Äúaver‚Äù on the first line, and the second part of the word, ‚Äúage‚Äù, on the second line. We want to keep what comes after ‚Äúsubs_content‚Äù. Let‚Äôs extract the word ‚Äúaverage‚Äù using <code>str_extract()</code>. However, because only some words were split between two lines, we first need to detect where the string ‚Äúhyppart1‚Äù is located, and only then can we extract what comes after ‚Äúsubs_content‚Äù. Thus, we need to combine <code>str_detect()</code> to first detect the string, and then <code>str_extract()</code> to extract what comes after ‚Äúsubs_content‚Äù:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_content %&gt;% 
  mutate(content = if_else(str_detect(content, "hyppart1"), 
                           str_extract_all(content, "content=.*", simplify = TRUE), 
                           content))</code></pre>
<p>
Let‚Äôs take a look at the result:
</p>
<pre class="r"><code>winchester_content %&gt;% 
  filter(str_detect(content, "content"))</code></pre>
<pre><code>## # A tibble: 64 x 1
##    content                                                          
##    &lt;chr&gt;                                                            
##  1 "content=\"average\" "                                           
##  2 "\"age\" subs_type=\"hyppart2\" subs_content=\"average\" "       
##  3 "content=\"consideration\" "                                     
##  4 "\"tion\" subs_type=\"hyppart2\" subs_content=\"consideration\" "
##  5 "content=\"resigned\" "                                          
##  6 "\"signed\" subs_type=\"hyppart2\" subs_content=\"resigned\" "   
##  7 "content=\"installed\" "                                         
##  8 "\"ed\" subs_type=\"hyppart2\" subs_content=\"installed\" "      
##  9 "content=\"before\" "                                            
## 10 "\"fore\" subs_type=\"hyppart2\" subs_content=\"before\" "       
## # ‚Ä¶ with 54 more rows</code></pre>
<p>
We still need to get rid of the string ‚Äúcontent=‚Äù and then of all the strings that contain ‚Äúhyppart2‚Äù, which are not needed now:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_content %&gt;% 
  mutate(content = str_replace(content, "content=", "")) %&gt;% 
  mutate(content = if_else(str_detect(content, "hyppart2"), NA_character_, content))

head(winchester_content)</code></pre>
<pre><code>## # A tibble: 6 x 1
##   content    
##   &lt;chr&gt;      
## 1 "\"j\" "   
## 2 "\"a\" "   
## 3 "\"ira\" " 
## 4 "\"mj\" "  
## 5 "\"ii\" "  
## 6 "\"te1r\" "</code></pre>
<p>
Almost done! We only need to remove the <code>‚Äú</code> characters:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_content %&gt;% 
  mutate(content = str_replace_all(content, "\"", "")) 

head(winchester_content)</code></pre>
<pre><code>## # A tibble: 6 x 1
##   content
##   &lt;chr&gt;  
## 1 "j "   
## 2 "a "   
## 3 "ira " 
## 4 "mj "  
## 5 "ii "  
## 6 "te1r "</code></pre>
<p>
Let‚Äôs remove space characters with <code>str_trim()</code>:
</p>
<pre class="r"><code>winchester_content &lt;- winchester_content %&gt;% 
  mutate(content = str_trim(content)) 

head(winchester_content)</code></pre>
<pre><code>## # A tibble: 6 x 1
##   content
##   &lt;chr&gt;  
## 1 j      
## 2 a      
## 3 ira    
## 4 mj     
## 5 ii     
## 6 te1r</code></pre>
<p>
To finish off this section, let‚Äôs remove stop words (words that do not add any meaning to a sentence, such as ‚Äúas‚Äù, ‚Äúand‚Äù‚Ä¶) and words that are composed of less than 3 characters. You can find a dataset with stopwords inside the <code>{stopwords}</code> package:
</p>
<pre class="r"><code>library(stopwords)

data(data_stopwords_stopwordsiso)

eng_stopwords &lt;- tibble("content" = data_stopwords_stopwordsiso$en)

winchester_content &lt;- winchester_content %&gt;% 
  anti_join(eng_stopwords) %&gt;% 
  filter(nchar(content) &gt; 3)</code></pre>
<pre><code>## Joining, by = "content"</code></pre>
<pre class="r"><code>head(winchester_content)</code></pre>
<pre><code>## # A tibble: 6 x 1
##   content   
##   &lt;chr&gt;     
## 1 te1r      
## 2 jilas     
## 3 edition   
## 4 winchester
## 5 news      
## 6 injuries</code></pre>
<p>
That‚Äôs it for this section! You now know how to work with strings, but in Chapter 10 we are going one step further by learning about regular expressions, which offer much more power.
</p>


</section>
</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-02-10-stringr_package.html</guid>
  <pubDate>Sun, 10 Feb 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Building a shiny app to explore historical newspapers: a step-by-step guide</title>
  <link>https://b-rodrigues.github.io/posts/2019-02-04-newspapers_shiny_app_tutorial.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://brodriguesco.shinyapps.io/newspapers_app/"> <img src="https://b-rodrigues.github.io/assets/img/tf_idf.png" title="Click here to go the app"></a>
</p>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">
Introduction
</h2>
<p>
I started off this year by exploring a world that was unknown to me, the world of historical newspapers. I did not know that historical newspapers data was a thing, and have been thoroughly enjoying myself exploring the different datasets published by the National Library of Luxembourg. You can find the data <a href="https://data.bnl.lu/data/historical-newspapers/">here</a>.
</p>
<p>
In my <a href="../posts/2019-01-04-newspapers.html">first blog post</a>, I analyzed data from <em>L‚Äôind√©pendence Luxembourgeoise</em>. I focused on the ads, which were for the most part in the 4th and last page of the newspaper. I did so by extracting the data from the ALTO files. ALTO files contain the content of the newspapers, (basically, the words that make up the article). For this first exercise, I disregarded the METS files, for two reasons. First, I simply wanted to have something quick, and get used to the data. And second, I did not know about ALTO and METS files enough to truly make something out of them. The problem of disregarding the METS file is that I only had a big dump of words, and did not know which words came from which article, or ad in this case.
</p>
<p>
In the <a href="../posts/2019-01-13-newspapers_mets_alto.html">second blog post</a>), I extracted data from the <em>L‚ÄôUnion</em> newspaper, this time by using the metadata from the METS files too. By combining the data from the ALTO files with the metadata from the METS files, I know which words came from which article, which would make further analysis much more interesting.
</p>
<p>
In the <a href="https://www.brodrigues.co/blog/2019-01-31-newspapers_shiny_app/">third blog post</a> of this series, I built a Shiny app which makes it easy to explore the 10 years of publications of <em>L‚ÄôUnion</em>. In this blog post, I will explain in great detail how I created this app.
</p>
</section>
<section id="part-1-getting-the-data-ready-for-the-shiny-app" class="level2">
<h2 class="anchored" data-anchor-id="part-1-getting-the-data-ready-for-the-shiny-app">
Part 1: Getting the data ready for the Shiny app
</h2>
<section id="step-1-extracting-the-needed-data" class="level3">
<h3 class="anchored" data-anchor-id="step-1-extracting-the-needed-data">
Step 1: Extracting the needed data
</h3>
<p>
If you want to follow along with a dataset from a single publication, you can download the following archive on <a href="https://www.dropbox.com/s/56ttqetz4cirsja/1533660_newspaper_lunion_1860-11-14.zip?dl=0">dropbox</a>. Extract this archive, and you will find the data exactly as you would get it from the the big archive you can download from the website of the National Library of Luxembourg. However, to keep the size of the archive small, I removed the .pdf and .jpeg scans.
</p>
<p>
In the <a href="https://www.brodrigues.co/blog/2019-01-13-newspapers_mets_alto/">second blog post</a>) I wrote some functions that made extracting the needed data from the files easy. However, after I wrote the article, I noticed that in some cases these functions were not working exactly as intended. I rewrote them a little bit to overcome these issues. You can find the code I used right below. I won‚Äôt explain it too much, because you can read the details in the previous blog post. However, should something be unclear, just drop me an email or a tweet!
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code># This functions will be used within the next functions to extract the relevant pieces

extractor &lt;- function(string, regex, all = FALSE){
    if(all) {
        string %&gt;%
            str_extract_all(regex) %&gt;%
            flatten_chr() %&gt;%
            str_remove_all("=|\\\"") %&gt;%
            #str_extract_all("[:alnum:]+|.|,|\\?|!", simplify = FALSE) %&gt;%
            map(paste, collapse = "") %&gt;%
            flatten_chr()
    } else {
        string %&gt;%
            str_extract(regex) %&gt;%
            str_remove_all("=|\\\"") %&gt;%
            #str_extract_all("[:alnum:]+|.|,|\\?|!", simplify = TRUE) %&gt;%
            paste(collapse = " ") %&gt;%
            tolower()
    }
}

# This function extracts the data from the METS files, and returns a tibble:

extract_mets &lt;- function(article){
    id &lt;- article %&gt;%
        extractor("(?&lt;=ID)(.*?)(?=LABEL)")

    label &lt;- article %&gt;%
        extractor("(?&lt;=LABEL)(.*?)(?=TYPE)")

    type &lt;- article %&gt;%
        extractor("(?&lt;=TYPE)(.*?)(?=&gt;)")

    begins &lt;- article %&gt;%
        extractor("(?&lt;=BEGIN)(.*?)(?=BETYPE)", all = TRUE)

    tibble::tribble(~label, ~type, ~begins, ~id,
                    label, type, begins, id) %&gt;%
        unnest()
}

# This function extracts the data from the ALTO files, and also returns a tibble:

extract_alto &lt;- function(article){
    begins &lt;- article[1] %&gt;%
        extractor("(?&lt;=^ID)(.*?)(?=HPOS)", all = TRUE)

    content &lt;- article %&gt;%
        extractor("(?&lt;=CONTENT)(.*?)(?=WC)", all = TRUE)

    tibble::tribble(~begins, ~content,
                    begins, content) %&gt;%
        unnest()
}

# This function takes the path to a page as an argument, and extracts the data from 
# each article using the function defined above. It then writes a flat CSV to disk.

alto_csv &lt;- function(page_path){

    page &lt;- read_file(page_path)

    doc_name &lt;- str_extract(page_path, "(?&lt;=/text/).*")

    alto_articles &lt;- page %&gt;%
        str_split("TextBlock ") %&gt;%
        flatten_chr()

    alto_df &lt;- map_df(alto_articles, extract_alto)

    alto_df &lt;- alto_df %&gt;%
        mutate(document = doc_name)

    write_csv(alto_df, paste0(page_path, ".csv"))
}

# Same as above, but for the METS file:

mets_csv &lt;- function(page_path){

    page &lt;- read_file(page_path)

    doc_name &lt;- str_extract(page_path, "(?&lt;=/).*")

    mets_articles &lt;- page %&gt;%
        str_split("DMDID") %&gt;%
        flatten_chr()

    mets_df &lt;- map_df(mets_articles, extract_mets)

    mets_df &lt;- mets_df %&gt;%
        mutate(document = doc_name)

    write_csv(mets_df, paste0(page_path, ".csv"))
}

# Time to use the above defined functions. First, let's save the path of all the ALTO files
# into a list:

pages_alto &lt;- str_match(list.files(path = "./", all.files = TRUE, recursive = TRUE), ".*/text/.*.xml") %&gt;%
    discard(is.na)

# I use the {furrr} library to do the extraction in parallel, using 8 cores:

library(furrr)

plan(multiprocess, workers = 8)

tic &lt;- Sys.time()
future_map(pages_alto, alto_csv)
toc &lt;- Sys.time()

toc - tic

#Time difference of 18.64776 mins


# Same for the METS files:

pages_mets &lt;- str_match(list.files(path = "./", all.files = TRUE, recursive = TRUE), ".*mets.xml") %&gt;%
    discard(is.na)


library(furrr)

plan(multiprocess, workers = 8)

tic &lt;- Sys.time()
future_map(pages_mets, mets_csv)
toc &lt;- Sys.time()

toc - tic

#Time difference of 18.64776 mins</code></pre>
</details>
<p>
If you want to try the above code for one ALTO and METS files, you can use the following lines (use the download link in the beginning of the blog post to get the required data):
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code>mets &lt;- read_file("1533660_newspaper_lunion_1860-11-14/1533660_newspaper_lunion_1860-11-14-mets.xml")

mets_articles2 &lt;- mets %&gt;%
    str_split("DMDID") %&gt;%
    flatten_chr()


alto &lt;- read_file("1533660_newspaper_lunion_1860-11-14/text/1860-11-14_01-00001.xml")

alto_articles &lt;- alto %&gt;%
    str_split("TextBlock ") %&gt;%
    flatten_chr()

mets_df2 &lt;- mets_articles2 %&gt;%
    map_df(extract_mets)

# Same exercice for ALTO

alto_df &lt;- alto_articles %&gt;%
    map_df(extract_alto)</code></pre>
</details>
</section>
<section id="step-2-joining-the-data-and-the-metadata" class="level3">
<h3 class="anchored" data-anchor-id="step-2-joining-the-data-and-the-metadata">
Step 2: Joining the data and the metadata
</h3>
<p>
Now that I extracted the data from the ALTO files, and the metadata from the METS files, I still need to join both data sets and do some cleaning. What is the goal of joining these two sources? Remember, by doing this I will know which words come from which article, which will make things much easier later on. I explain how the code works as comments in the code block below:
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code>library(tidyverse)
library(udpipe)
library(textrank)
library(tidytext)

# First, I need the path to each folder that contains the ALTO and METS files. Each newspaper
# data is inside its own folder, one folder per publication. Inside, there's `text` folder that
# contains the ALTO and METS files. This is also where I saved the .csv files from before.

pathdirs &lt;- list.dirs(recursive = FALSE) %&gt;%
    str_match(".*lunion.*") %&gt;%
    discard(is.na)

# The following function imports the METS and the ALTO csv files, joins them, and does some 
# basic cleaning. I used a trick to detect German articles (even though L'Union is a French publication
# some articles are in German) and then remove them.

tidy_papers &lt;- function(path){
    mets_path &lt;- paste0(path, "/", list.files(path, ".*.xml.csv"))
    mets_csv &lt;- data.table::fread(mets_path)

    alto_path &lt;- paste0(path, "/text/", list.files(paste0(path, "/text/"), ".*.csv"))
    alto_csv &lt;- map_dfr(alto_path, data.table::fread)

    final &lt;- full_join(alto_csv, mets_csv, by = "begins") %&gt;%
        mutate(content = tolower(content)) %&gt;%
        mutate(content = if_else(str_detect(content, "hyppart1"), str_extract_all(content, "(?&lt;=CONTENT_).*", simplify = TRUE), content)) %&gt;%
        mutate(content = if_else(str_detect(content, "hyppart2"), NA_character_, content)) %&gt;%
        # When words are separated by a hyphen and split over two lines, it looks like this in the data.
        # ex SUBS_TYPEHypPart1 SUBS_CONTENTexcept√©e
        # cept√©e SUBS_TYPEHypPart2 SUBS_CONTENTexcept√©e
        # Here, the word `except√©e` is split over two lines, so using a regular expression, I keep
        # the string `except√©e`, which comes after the string `CONTENT`,  from the first line and 
        # replace the second line by an NA_character_
        mutate(content = if_else(str_detect(content, "superscript"), NA_character_, content)) %&gt;%
        mutate(content = if_else(str_detect(content, "subscript"), NA_character_, content)) %&gt;%
        filter(!is.na(content)) %&gt;%
        filter(type == "article") %&gt;%
        group_by(id) %&gt;%
        nest %&gt;%
        # Below I create a list column with all the content of the article in a single string.
        mutate(article_text = map(data, ~paste(.$content, collapse = " "))) %&gt;%
        mutate(article_text = as.character(article_text)) %&gt;%
        # Detecting and removing german articles
        mutate(german = str_detect(article_text, "wenn|wird|und")) %&gt;%
        filter(german == FALSE) %&gt;%
        select(-german) %&gt;%
        # Finally, creating the label of the article (the title), and removing things that are 
        # not articles, such as the daily feuilleton.
        mutate(label = map(data, ~`[`(.$label, 1))) %&gt;%
        filter(!str_detect(label, "embranchement|ligne|bourse|abonn√©s|feuilleton")) %&gt;%
        filter(label != "na")

    # Save the data in the rds format, as it is not a flat file
    saveRDS(final, paste0(path, "/", str_sub(path, 11, -1), ".rds"))
}

# Here again, I do this in parallel

library(furrr)

plan(multiprocess, workers = 8)

future_map(pathdirs, tidy_papers)</code></pre>
</details>
<p>
This is how one of these files looks like, after passing through this function:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/articles_rds.png"><!-- -->
</p>
<p>
One line is one article. The first column is the id of the article, the second column contains a data frame, the text of the article and finally the title of the article. Let‚Äôs take a look at the content of the first element of the <em>data</em> column:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/merged_alto_mets.png"><!-- -->
</p>
<p>
This is the result of the merger of the METS and ALTO csv files. The first column is the id of the article, the second column contains each individual word of the article, the <em>label</em> column the label, or title of the article.
</p>
</section>
<section id="step-3-part-of-speech-annotation" class="level3">
<h3 class="anchored" data-anchor-id="step-3-part-of-speech-annotation">
Step 3: Part-of-speech annotation
</h3>
<p>
Part-of-speech annotation is a technique with the aim of assigning to each word its part of speech. Basically, Pos annotation tells us whether a word is a verb, a noun, an adjective‚Ä¶ This will be quite useful for the analysis. To perform Pos annotation, you need to install the <code>{udpipe}</code> package, and download the pre-trained model for the language you want to annotate, in my case French:
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code># Only run this once. This downloads the model for French
udpipe_download_model(language = "french")

# Load the model
udmodel_french &lt;- udpipe_load_model(file = 'french-gsd-ud-2.3-181115.udpipe')

# Save the path of the files to annotate in a list:
pathrds &lt;- list.files(path = "./", all.files = TRUE, recursive = TRUE) %&gt;% 
  str_match(".*.rds") %&gt;%
  discard(is.na)

annotate_rds &lt;- function(path, udmodel){

    newspaper &lt;- readRDS(path)

    s &lt;- udpipe_annotate(udmodel, newspaper$article_text, doc_id = newspaper$label)
    x &lt;- data.frame(s)

    saveRDS(x, str_replace(path, ".rds", "_annotated.rds"))
}

library(furrr)
plan(multiprocess, workers = 8)
tic &lt;- Sys.time()
future_map(pathrds, annotate_rds, udmodel = udmodel_french)
toc &lt;- Sys.time()
toc - tic</code></pre>
</details>
<p>
And here is the result:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/pos_article.png"><!-- -->
</p>
<p>
The <em>upos</em> column contains the tags. Now I know which words are nouns, verbs, adjectives, stopwords‚Ä¶ Meaning that I can easily focus on the type of words that interest me. Plus, as an added benefit, I can focus on the lemma of the words. For example, the word <em>viennent</em>, is the <a href="https://en.wikipedia.org/wiki/French_conjugation">conjugated</a> form of the verb <em>venir</em>. <em>venir</em> is thus the lemma of <em>viennent</em>. This means that I can focus my analysis on lemmata. This is useful, because if I compute the frequency of words, <em>viennent</em> would be different from <em>venir</em>, which is not really what we want.
</p>
</section>
<section id="step-4-tf-idf" class="level3">
<h3 class="anchored" data-anchor-id="step-4-tf-idf">
Step 4: tf-idf
</h3>
<p>
Just like what I did in my <a href="https://www.brodrigues.co/blog/2019-01-04-newspapers/">first blog post</a>, I compute the tf-idf of words. The difference, is that here the ‚Äúdocument‚Äù is the article. This means that I will get the most frequent words inside each article, but who are at the same time rare in the other articles. Doing this ensures that I will only get very relevant words for each article.
</p>
<p>
In the lines below, I prepare the data to then make the plots. The files that are created using the code below are available in the following <a href="https://github.com/b-rodrigues/newspapers_shinyapp/tree/master/tf_idf_data">Github link</a>.
</p>
<p>
In the Shiny app, I read the data directly from the repo. This way, I can keep the app small in size.
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code>path_annotatedrds &lt;- list.files(path = "./", all.files = TRUE, recursive = TRUE) %&gt;% str_match(".*_annotated.rds") %&gt;%
    discard(is.na)

prepare_tf_idf &lt;- function(path){

    annotated_newspaper &lt;- readRDS(path)

    tf_idf_data &lt;- annotated_newspaper %&gt;%
        filter(upos %in% c("NOUN", "VERB", "ADJ", "PROPN")) %&gt;%
        filter(nchar(lemma) &gt; 3) %&gt;%
        count(doc_id, lemma) %&gt;%
        bind_tf_idf(lemma, doc_id, n) %&gt;%
        arrange(desc(tf_idf)) %&gt;%
        group_by(doc_id)

    name_tf_idf_data &lt;- str_split(path, "/", simplify = 1)[1] %&gt;%
        paste0("_tf_idf_data.rds")  %&gt;%
        str_sub(start = 9, -1)

    saveRDS(tf_idf_data, paste0("tf_idf_data/", name_tf_idf_data))
}

library(furrr)
plan(multiprocess, workers = 8)

future_map(path_annotatedrds, prepare_tf_idf)</code></pre>
</details>
</section>
<section id="step-5-summarizing-articles-by-extracting-the-most-relevant-sentences-using-textrank" class="level3">
<h3 class="anchored" data-anchor-id="step-5-summarizing-articles-by-extracting-the-most-relevant-sentences-using-textrank">
Step 5: Summarizing articles by extracting the most relevant sentences, using <code>{textrank}</code>
</h3>
<p>
The last step in data preparation is to extract the most relevant sentences of each articles, using the <code>{textrank}</code> package. This packages implements the <em>PageRank</em> algorithm developed by Larry Page and Sergey Brin in 1995. This algorithm ranks pages by the number of links that point to the pages; the most popular and important pages are also the ones with more links to them. A similar approach is used by the implementation of <code>{textrank}</code>. The algorithm is explained in detail in the following <a href="https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf">paper</a>.
</p>
<p>
However, I cannot simply apply <code>{textrank}</code> to the annotated data frame as it is. Because I have several articles, I have to run the <code>textrank_sentences()</code> function, which extracts the relevant sentences, article by article. For this I still need to transform the data set and also need to prepare the data in a way that makes it digestible by the function. I will not explain the code below line by line, since the documentation of the package is quite straightforward. However, keep in mind that I have to run the <code>textrank_sentences()</code> function for each article, which explains that as some point I use the following:
</p>
<pre class="r"><code>group_by(doc_id) %&gt;%
    nest() %&gt;%</code></pre>
<p>
which then makes it easy to work by article (<em>doc_id</em> is the id of the articles). This part is definitely the most complex, so if you‚Äôre interested in the methodology described here, really take your time to understand this function. Let me know if I can clarify things!
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre class="r"><code>library(textrank)
library(brotools)

path_annotatedrds &lt;- list.files(path = "./", all.files = TRUE, recursive = TRUE) %&gt;% str_match(".*_annotated.rds") %&gt;%
    discard(is.na)

prepare_textrank &lt;- function(path){

    annotated_newspaper &lt;- readRDS(path)

    # sentences summary
    x_text_rank &lt;- annotated_newspaper %&gt;%
        group_by(doc_id) %&gt;%
        nest() %&gt;%
        mutate(textrank_id = map(data, ~unique_identifier(., c("paragraph_id", "sentence_id")))) %&gt;%
        mutate(cleaned = map2(.x = data, .y = textrank_id, ~cbind(.x, "textrank_id" = .y))) %&gt;%
        select(doc_id, cleaned)

    x_text_rank2 &lt;- x_text_rank %&gt;%
        mutate(sentences = map(cleaned, ~select(., textrank_id, sentence))) %&gt;%
        # one_row() is a function from my own package, which eliminates duplicates rows
        # from a data frame
        mutate(sentences = map(sentences, ~one_row(., c("textrank_id", "sentence"))))

    x_terminology &lt;- x_text_rank %&gt;%
        mutate(terminology = map(cleaned, ~filter(., upos %in% c("NOUN", "ADJ")))) %&gt;%
        mutate(terminology = map(terminology, ~select(., textrank_id, "lemma"))) %&gt;%
        select(terminology)

    x_final &lt;- bind_cols(x_text_rank2, x_terminology)

    possibly_textrank_sentences &lt;- possibly(textrank_sentences, otherwise = NULL)

    x_final &lt;- x_final %&gt;%
        mutate(summary = map2(sentences, terminology, possibly_textrank_sentences)) %&gt;%
        select(doc_id, summary)

    name_textrank_data &lt;- str_split(path, "/", simplify = 1)[1] %&gt;%
        paste0("_textrank_data.rds") %&gt;%
        str_sub(start = 9, -1)

    saveRDS(x_final, paste0("textrank_data/", name_textrank_data))
}

library(furrr)
plan(multiprocess, workers = 8)

future_map(path_annotatedrds, prepare_textrank)</code></pre>
</details>
<p>
You can download the annotated data sets from the following <a href="https://github.com/b-rodrigues/newspapers_shinyapp/tree/master/textrank_data">link</a>. This is how the data looks like:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/textrank_df.png"><!-- -->
</p>
<p>
Using the <code>summary()</code> function on an element of the <em>summary</em> column returns the 5 most relevant sentences as extracted by <code>{textrank}</code>.
</p>
</section>
</section>
<section id="part-2-building-the-shiny-app" class="level2">
<h2 class="anchored" data-anchor-id="part-2-building-the-shiny-app">
Part 2: Building the shiny app
</h2>
<p>
The most difficult parts are behind us! Building a dashboard is quite easy thanks to the <code>{flexdashboard}</code> package. You need to know Markdown and some Shiny, but it‚Äôs way easier than building a complete Shiny app. First of all, install the <code>{fleshdashboard}</code> package, and start from a template, or from <a href="https://rmarkdown.rstudio.com/flexdashboard/layouts.html">this list of layouts</a>.
</p>
<p>
I think that the only trick worth mentioning is that I put the data in a Github repo, and read it directly from the Shiny app. Users choose a date, which I save in a reactive variable. I then build the right url that points towards the right data set, and read it:
</p>
<pre class="r"><code>path_tf_idf &lt;- reactive({
    paste0("https://raw.githubusercontent.com/b-rodrigues/newspapers_shinyapp/master/tf_idf_data/newspaper_lunion_", as.character(input$date2), "_tf_idf_data.rds")
})

dfInput &lt;- reactive({
        read_rds(url(path_tf_idf())) %&gt;%
        top_n(as.numeric(input$tf_df_words), tf_idf) %&gt;%
        mutate(word = reorder(lemma, tf_idf)) 
})</code></pre>
<p>
Because I did all the computations beforehand, the app simply reads the data and creates the bar plots for the tf-idf data, or prints the sentences for the textrank data. To print the sentences correcly, I had to use some html tags, using the <code>{htmltools}</code> package. Below you can find the source code of the app:
</p>
<details>
<p>
</p><summary>
Click if you want to see the code
</summary>
<p></p>
<pre><code>---
title: "Exploring 10 years of daily publications of the Luxembourguish newspaper, *L'Union*"
output: 
  flexdashboard::flex_dashboard:
    theme: yeti
    orientation: columns
    vertical_layout: fill
runtime: shiny

---

`` `{r setup, include=FALSE}
library(flexdashboard)
library(shiny)
library(tidyverse)
library(textrank)
library(tidytext)
library(udpipe)
library(plotly)
library(ggthemes)
`` `

Sidebar {.sidebar}
=====================================

`` `{r}
dateInput('date2',
      label = paste('Select date'),
      value = as.character(as.Date("1860-11-14")),
      min = as.Date("1860-11-12"), max = as.Date("1869-12-31"),
      format = "yyyy/mm/dd",
      startview = 'year', language = 'en-GB', weekstart = 1
    )
selectInput(inputId = "tf_df_words", 
            label = "Select number of unique words for tf-idf", 
            choices = seq(1:10),
            selected = 5)
selectInput(inputId = "textrank_n_sentences", 
            label = "Select the number of sentences for the summary of the article", 
            choices = seq(1:20), 
            selected = 5)
`` `

*The BnL has digitised over 800.000 pages of Luxembourg newspapers. From those, more than 700.000 
pages have rich metadata using international XML standards such as METS and ALTO. 
Multiple datasets are available for download. Each one is of different size and contains different
newspapers. All the digitised material can also be found on our search platform a-z.lu 
(Make sure to filter by ‚Äúeluxemburgensia‚Äù). All datasets contain XML (METS + ALTO), PDF, original 
TIFF and PNG files for every newspaper issue.* 
Source: https://data.bnl.lu/data/historical-newspapers/

This Shiny app allows you to get summaries of the 10 years of daily issues of the "L'Union" newspaper.
In the first tab, a simple word frequency per article is shown, using the tf-idf method. In the 
second tab, summary sentences have been extracted using the `{textrank}` package.


Word frequency per article
===================================== 
Row
-----------------------------------------------------------------------

### Note: there might be days without any publication. In case of an error, select another date.
    
`` `{r}
path_tf_idf &lt;- reactive({
    paste0("https://raw.githubusercontent.com/b-rodrigues/newspapers_shinyapp/master/tf_idf_data/newspaper_lunion_", as.character(input$date2), "_tf_idf_data.rds")
})
dfInput &lt;- reactive({
        read_rds(url(path_tf_idf())) %&gt;%
        top_n(as.numeric(input$tf_df_words), tf_idf) %&gt;%
        mutate(word = reorder(lemma, tf_idf)) 
})
renderPlotly({
    df_tf_idf &lt;- dfInput()
    p1 &lt;- ggplot(df_tf_idf,
                 aes(word, tf_idf)) +
                 geom_col(show.legend = FALSE, fill = "#82518c") +
                 labs(x = NULL, y = "tf-doc_idf") +
                 facet_wrap(~doc_id, ncol = 2, scales = "free") +
                 coord_flip() +
                 theme_dark()
    ggplotly(p1)
})
`` `

Summary of articles {data-orientation=rows}
===================================== 
Row 
-----------------------------------------------------------------------

### The sentence in bold is the title of the article. You can show more sentences in the summary by using the input in the sidebar.
    
`` `{r}
print_summary_textrank &lt;- function(doc_id, summary, n_sentences){
    htmltools::HTML(paste0("&lt;b&gt;", doc_id, "&lt;/b&gt;"), paste("&lt;p&gt;", summary(summary, n_sentences), sep = "", collapse = "&lt;br/&gt;"), "&lt;/p&gt;")
}
path_textrank &lt;- reactive({
    paste0("https://raw.githubusercontent.com/b-rodrigues/newspapers_shinyapp/master/textrank_data/newspaper_lunion_", as.character(input$date2), "_textrank_data.rds")
})
dfInput2 &lt;- reactive({
        read_rds(url(path_textrank()))
})
renderUI({
    df_textrank &lt;- dfInput2()
    
df_textrank &lt;- df_textrank %&gt;% 
    mutate(to_print = map2(doc_id, summary, print_summary_textrank, n_sentences = as.numeric(input$textrank_n_sentences)))
df_textrank$to_print
})
`` `
</code></pre>
</details>
<p>
I host the app on Shinyapps.io, which is really easy to do from within Rstudio.
</p>
<p>
That was quite long, I‚Äôm not sure that anyone will read this blog post completely, but oh well. Better to put the code online, might help someone one day, that leave it to rot on my hard drive.
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-02-04-newspapers_shiny_app_tutorial.html</guid>
  <pubDate>Mon, 04 Feb 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Using Data Science to read 10 years of Luxembourguish newspapers from the 19th century</title>
  <link>https://b-rodrigues.github.io/posts/2019-01-31-newspapers_shiny_app.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://brodriguesco.shinyapps.io/newspapers_app/"> <img src="https://b-rodrigues.github.io/assets/img/tf_idf.png" title="Click here to go the app"></a>
</p>
</div>
<p>
I have been playing around with historical newspaper data (see <a href="../posts/2019-01-04-newspapers.html">here</a> and <a href="../posts/2019-01-13-newspapers_mets_alto.html">here</a>). I have extracted the data from the largest archive available, as described in the previous blog post, and now created a shiny dashboard where it is possible to visualize the most common words per article, as well as read a summary of each article. The summary was made using a method called <em>textrank</em>, using the <code>{textrank}</code> package, which extracts relevant sentences using the Pagerank (developed by Google) algorithm. You can read the scientific paper <a href="https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf">here</a> for more info.
</p>
<p>
You can play around with the app by clicking <a href="https://brodriguesco.shinyapps.io/newspapers_app/">here</a>. In the next blog post, I will explain how I created the app, step by step. It‚Äôs going to be a long blog post!
</p>
<p>
Using the app, I noticed that some war happened around November 1860 in China, which turned out to be the <a href="https://en.wikipedia.org/wiki/Second_Opium_War">Second Opium War</a>. The war actually ended in October 1860, but apparently the news took several months to travel to Europe.
</p>
<p>
I also learned that already in the 1861, there was public transportation between some Luxembourguish villages, and French villages that were by the border (see the publication from the 17th of December 1861).
</p>
<p>
Let me know if you find about historical events using my app!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-01-31-newspapers_shiny_app.html</guid>
  <pubDate>Thu, 31 Jan 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Making sense of the METS and ALTO XML standards</title>
  <link>https://b-rodrigues.github.io/posts/2019-01-13-newspapers_mets_alto.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=V1qpvpH26fo"> <img src="https://b-rodrigues.github.io/assets/img/union.png" title="The 19th century was a tough place"></a>
</p>
</div>
<p>
Last week I wrote a <a href="https://www.brodrigues.co/blog/2019-01-04-newspapers/">blog post</a> where I analyzed one year of newspapers ads from 19th century newspapers. The data is made available by the <a href="https://data.bnl.lu/data/historical-newspapers/">national library of Luxembourg</a>. In this blog post, which is part 1 of a 2 part series, I extract data from the 257gb archive, which contains 10 years of publications of the <em>L‚ÄôUnion</em>, another 19th century Luxembourguish newspaper written in French. As I explained in the previous post, to make life easier to data scientists, the national library also included ALTO and METS files (which are a XML files used to describe the layout and contents of physical text sources, such as pages of a book or newspaper) which can be easily parsed by R.
</p>
<p>
This is how a ALTO file looks like:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/alto.png"><!-- -->
</p>
<p>
Each page of the newspaper of a given day has one ALTO file. This is how a METS file looks like:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/mets.png"><!-- -->
</p>
<p>
For each daily issue of the newspaper, there is a METS file. So 1 METS file for 4 ALTO files.
</p>
<p>
In my last blog post, I only extracted the words from the ALTO file (red rectangles of the first screenshot) and did not touch the METS file. The problem of doing this is that I get all the words for each page, without knowing which come from the same article. If I want to know which words come from the same article, I need to use the info from the METS file. From the METS file I have the ID of the article, and some other metadata, such as the title of the article and the type of the article (which can be <em>article</em>, <em>advertisement</em>, etc). The information highlighted with the green rectangles in the METS file can be linked to the green rectangles from the ALTO files. My goal is to get the following data frame from the METS file:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/mets_df.png"><!-- -->
</p>
<p>
and this data frame from the ALTO files:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/alto_df.png"><!-- -->
</p>
<p>
As you can see, by combining both data frames I can know which words come from the same article, which will be helpful for further analysis. <a href="https://en.wikipedia.org/wiki/1860s">A lot of things happened in the 1860s.</a> I am really curious to see if and how these events where reported in a Luxembourguish newspaper. I am particularly curious about how long it took to report certain news from far away, such as the assassination of Abraham Lincoln. But before that I need to extract the data!
</p>
<p>
I will only focus on the METS file. The logic for the ALTO file is the same. All the source code will be in the appendix of this blog post.
</p>
<p>
First, let‚Äôs take a look at a METS file:
</p>
<pre class="r"><code>library(tidyverse)
mets &lt;- read_file("1533660_newspaper_lunion_1860-11-14/1533660_newspaper_lunion_1860-11-14-mets.xml")</code></pre>
<p>
This is how it looks like:
</p>
<pre><code>"&lt;?xml version='1.0' encoding='utf-8'?&gt;\r\n&lt;mets xmlns=\"http://www.loc.gov/METS/\" xmlns:mix=\"http://www.loc.gov/mix/v20\" xmlns:mods=\"http://www.loc.gov/mods/v3\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" LABEL=\"L'UNION. 1860-11-14_01\" OBJID=\"https://persist.lu/ark:/70795/m62fcm\" TYPE=\"Newspaper\" xsi:schemaLocation=\"http://www.loc.gov/METS/ http://www.loc.gov/standards/mets/mets.xsd http://www.loc.gov/mods/v3 http://www.loc.gov/standards/mods/v3/mods-3-6.xsd http://www.loc.gov/mix/v20 http://www.loc.gov/standards/mix/mix.xsd\"&gt;\r\n  &lt;metsHdr CREATEDATE=\"2010-12-03T20:35:05\" LASTMODDATE=\"2018-05-09T05:35:51Z\"&gt;\r\n    &lt;agent OTHERTYPE=\"SOFTWARE\" ROLE=\"CREATOR\" TYPE=\"OTHER\"&gt;\r\n      &lt;name&gt;CCS docWORKS/METAe Version 6.4-3&lt;/name&gt;\r\n      &lt;note&gt;docWORKS-ID: 101636&lt;/note&gt;\r\n    &lt;/agent&gt;\r\n  &lt;/metsHdr&gt;\r\n  &lt;dmdSec ID=\"MODSMD_COLLECTION\"&gt;\r\n    &lt;mdWrap LABEL=\"Bibliographic meta-data of the collection\" MDTYPE=\"MODS\" MIMETYPE=\"text/xml\"&gt;\r\n      &lt;xmlData&gt;\r\n        &lt;mods:mods&gt;\r\n          &lt;mods:identifier type=\"local\"&gt;lunion&lt;/mods:identifier&gt;\r\n          &lt;mods:titleInfo ID=\"MODSMD_COLLECTION_TI1\" xml:lang=\"fr\"&gt;\r\n            &lt;mods:title&gt;L'UNION.&lt;/mods:title&gt;\r\n          &lt;/mods:titleInfo&gt;\r\n        &lt;/mods:mods&gt;\r\n      &lt;/xmlData&gt;\r\n    &lt;/mdWrap&gt;\r\n  &lt;/dmdSec&gt;\r\n  &lt;dmdSec ID=\"MODSMD_SECTION1\"&gt;\r\n    &lt;mdWrap MDTYPE=\"MODS\" MIMETYPE=\"text/xml\"&gt;\r\n      &lt;xmlData&gt;\r\n        &lt;mods:mods&gt;\r\n          &lt;mods:titleInfo ID=\"MODSMD_SECTION1_TI1\" xml:lang=\"fr\"&gt;\r\n            &lt;mods:title&gt;Chemins de fer. ‚Äî Service d'hiver.&lt;/mods:title&gt;\r\n          &lt;/mods:titleInfo&gt;\r\n          &lt;mods:language&gt;\r\n            &lt;mods:languageTerm authority=\"rfc3066\" type=\"code\"&gt;fr&lt;/mods:languageTerm&gt;\r\n ...."</code></pre>
<p>
As usual when you import text files like this, it‚Äôs always a good idea to split the file. I will split at the <code>‚ÄúDMDID‚Äù</code> character. Take a look back at the second screenshot. The very first tag, first row, first word after <code>div</code> is <code>‚ÄúDMDID‚Äù</code>. By splitting at this level, I will get back a list, where each element is the content of this <code>div DMDID</code> block. This is exactly what I need, since this block contains the information from the green rectangles. So let‚Äôs split the <code>mets</code> variable at this level:
</p>
<pre class="r"><code>mets_articles &lt;- mets %&gt;%
    str_split("DMDID") %&gt;%
    flatten_chr()</code></pre>
<p>
Let‚Äôs take a look at <code>mets_articles</code>:
</p>
<pre class="r"><code>str(mets_articles)</code></pre>
<pre><code> chr [1:25] "&lt;?xml version='1.0' encoding='utf-8'?&gt;\r\n&lt;mets xmlns=\"http://www.loc.gov/METS/\" xmlns:mix=\"http://www.loc.g"| __truncated__ ...</code></pre>
<p>
Doesn‚Äôt seem to be very helpful, but actually it is. We can see that <code>mets_articles</code> is a now a list of 25 elements.
</p>
<p>
This means that for each element of <code>mets_articles</code>, I need to get the identifier, the label, the type (the red rectangles from the screenshot), but also the information from the <code>‚ÄúBEGIN‚Äù</code> element (the green rectangle).
</p>
<p>
To do this, I‚Äôll be using regular expressions. In general, I start by experimenting in the console, and then when things start looking good, I write a function. Here is this function:
</p>
<pre class="r"><code>extractor &lt;- function(string, regex, all = FALSE){
    if(all) {
        string %&gt;%
            str_extract_all(regex) %&gt;%
            flatten_chr() %&gt;%
            str_extract_all("[:alnum:]+", simplify = FALSE) %&gt;%
            map(paste, collapse = "_") %&gt;%
            flatten_chr()
    } else {
        string %&gt;%
            str_extract(regex) %&gt;%
            str_extract_all("[:alnum:]+", simplify = TRUE) %&gt;%
            paste(collapse = " ") %&gt;%
            tolower()
    }
}</code></pre>
<p>
This function may seem complicated, but it simply encapsulates some pretty standard steps to get the data I need. I had to consider two cases. The first case is when I need to extract all the elements with <code>str_extract_all()</code>, or only the first occurrence, with <code>str_extract()</code>. Let‚Äôs test it on the first article of the <code>mets_articles</code> list:
</p>
<pre class="r"><code>mets_articles_1 &lt;- mets_articles[1]</code></pre>
<pre class="r"><code>extractor(mets_articles_1, "ID", all = FALSE)</code></pre>
<pre><code>## [1] "id"</code></pre>
<p>
Let‚Äôs see what happens with <code>all = TRUE</code>:
</p>
<pre class="r"><code>extractor(mets_articles_1, "ID", all = TRUE)</code></pre>
<pre><code>##   [1] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [15] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [29] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [43] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [57] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [71] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [85] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
##  [99] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
## [113] "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID" "ID"
## [127] "ID" "ID" "ID" "ID" "ID"</code></pre>
<p>
This seems to work as intended. Since I need to call this function several times, I‚Äôll be writing another function that extracts all I need:
</p>
<pre class="r"><code>extract_mets &lt;- function(article){

    id &lt;- article %&gt;%
        extractor("(?&lt;=ID)(.*?)(?=LABEL)")

    label &lt;- article %&gt;%
        extractor("(?&lt;=LABEL)(.*?)(?=TYPE)")

    type &lt;- article %&gt;%
        extractor("(?&lt;=TYPE)(.*?)(?=&gt;)")

    begins &lt;- article %&gt;%
        extractor("(?&lt;=BEGIN)(.*?)(?=BETYPE)", all = TRUE)

    tibble::tribble(~label, ~type, ~begins, ~id,
                    label, type, begins, id) %&gt;%
        unnest()
}</code></pre>
<p>
This function uses complex regular expressions to extract the strings I need, and then puts the result into a data frame, with the <code>tibble()</code> function. I then use <code>unnest()</code>, because <code>label</code>, <code>type</code>, <code>begins</code> and <code>id</code> are not the same length. <code>label</code>, <code>type</code> and <code>id</code> are of length 1, while <code>begins</code> is longer. This means that when I put them into a data frame it looks like this:
</p>
<pre class="r"><code>tribble(~a, ~b,
"a", rep("b", 4))</code></pre>
<pre><code>## # A tibble: 1 x 2
##   a     b        
##   &lt;chr&gt; &lt;list&gt;   
## 1 a     &lt;chr [4]&gt;</code></pre>
<p>
With <code>unnest()</code>, I get a nice data frame:
</p>
<pre class="r"><code>tribble(~a, ~b,
"a", rep("b", 4)) %&gt;% 
  unnest()</code></pre>
<pre><code>## # A tibble: 4 x 2
##   a     b    
##   &lt;chr&gt; &lt;chr&gt;
## 1 a     b    
## 2 a     b    
## 3 a     b    
## 4 a     b</code></pre>
<p>
Now, I simply need to map this function to all the files and that‚Äôs it! For this, I will write yet another helper function:
</p>
<pre class="r"><code>mets_csv &lt;- function(page_path){
    
    page &lt;- read_file(page_path)
    
    doc_name &lt;- str_extract(page_path, "(?&lt;=/).*")
    
    mets_articles &lt;- page %&gt;%
        str_split("DMDID") %&gt;%
        flatten_chr()
    
    mets_df &lt;- map_df(mets_articles, extract_mets)
    
    mets_df &lt;- mets_df %&gt;%
        mutate(document = doc_name)
    
    write_csv(mets_df, paste0(page_path, ".csv"))
}</code></pre>
<p>
This function takes the path to a METS file as input, and processes it using the steps I explained above. The only difference is that I add a column containing the name of the file that was processed, and write the resulting data frame directly to disk as a data frame. Finally, I can map this function to all the METS files:
</p>
<pre class="r"><code># Extract content from METS files

pages_mets &lt;- str_match(list.files(path = "./", all.files = TRUE, recursive = TRUE), ".*mets.xml") %&gt;%
    discard(is.na)

library(furrr)

plan(multiprocess, workers = 8)

tic &lt;- Sys.time()
future_map(pages_mets, mets_csv)
toc &lt;- Sys.time()

toc - tic</code></pre>
<p>
I use <code>{furrr}</code> to extract the data from all the files in parallel, by putting 8 cores of my CPU to work. This took around 3 minutes and 20 seconds to finish.
</p>
<p>
That‚Äôs it for now, stay tuned for part 2 where I will analyze this fresh data!
</p>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">
Appendix
</h2>
<pre class="r"><code>extract_alto &lt;- function(article){
    begins &lt;- article[1] %&gt;%
        extractor("(?&lt;=^ID)(.*?)(?=HPOS)", all = TRUE)

    content &lt;- article %&gt;%
        extractor("(?&lt;=CONTENT)(.*?)(?=WC)", all = TRUE)

    tibble::tribble(~begins, ~content,
                    begins, content) %&gt;%
        unnest()
}

alto_csv &lt;- function(page_path){

    page &lt;- read_file(page_path)

    doc_name &lt;- str_extract(page_path, "(?&lt;=/text/).*")

    alto_articles &lt;- page %&gt;%
        str_split("TextBlock ") %&gt;%
        flatten_chr()

    alto_df &lt;- map_df(alto_articles, extract_alto)

    alto_df &lt;- alto_df %&gt;%
        mutate(document = doc_name)

    write_csv(alto_df, paste0(page_path, ".csv"))
}


alto &lt;- read_file("1533660_newspaper_lunion_1860-11-14/text/1860-11-14_01-00001.xml")


# Extract content from alto files

pages_alto &lt;- str_match(list.files(path = "./", all.files = TRUE, recursive = TRUE), ".*/text/.*.xml") %&gt;%
    discard(is.na)


library(furrr)

plan(multiprocess, workers = 8)

tic &lt;- Sys.time()
future_map(pages_alto, alto_csv)
toc &lt;- Sys.time()

toc - tic

#Time difference of 18.64776 mins</code></pre>
</section>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-01-13-newspapers_mets_alto.html</guid>
  <pubDate>Sun, 13 Jan 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Looking into 19th century ads from a Luxembourguish newspaper with R</title>
  <link>https://b-rodrigues.github.io/posts/2019-01-04-newspapers.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=0xzN6FM5x_E"> <img src="https://b-rodrigues.github.io/assets/img/Wales.jpg" title="Sometimes ads are better than this. Especially if it's Flex Tape ¬Æ ads."></a>
</p>
</div>
<p>
The <a href="https://data.bnl.lu/data/historical-newspapers/">national library of Luxembourg</a> published some very interesting data sets; scans of historical newspapers! There are several data sets that you can download, from 250mb up to 257gb. I decided to take a look at the 32gb ‚ÄúML Starter Pack‚Äù. It contains high quality scans of one year of the <em>L‚Äôind√©pendence Luxembourgeoise</em> (Luxembourguish independence) from the year 1877. To make life easier to data scientists, the national library also included ALTO and METS files (which is a XML schema that is used to describe the layout and contents of physical text sources, such as pages of a book or newspaper) which can be easily parsed by R.
</p>
<p>
<em>L‚Äôind√©pendence Luxembourgeoise</em> is quite interesting in that it is a Luxembourguish newspaper written in French. Luxembourg always had 3 languages that were used in different situations, French, German and Luxembourguish. Luxembourguish is the language people used (and still use) for day to day life and to speak to their baker. Historically however, it was not used for the press or in politics. Instead it was German that was used for the press (or so I thought) and French in politics (only in <a href="http://legilux.public.lu/eli/etat/leg/loi/1984/02/24/n1/jo">1984</a> was Luxembourguish made an official Language of Luxembourg). It turns out however that <em>L‚Äôind√©pendence Luxembourgeoise</em>, a daily newspaper that does not exist anymore, was in French. This piqued my interest, and it also made analysis easier, for 2 reasons: I first started with the <em>Luxemburger Wort</em> (Luxembourg‚Äôs Word I guess would be a translation), which still exists today, but which is in German. And at that time, German was written using the Fraktur font, which makes it barely readable. Look at the alphabet in Fraktur:
</p>
<pre><code>ùï¨ ùï≠ ùïÆ ùïØ ùï∞ ùï± ùï≤ ùï≥ ùï¥ ùïµ ùï∂ ùï∑ ùï∏ ùïπ ùï∫ ùïª ùïº ùïΩ ùïæ ùïø ùñÄ ùñÅ ùñÇ ùñÉ ùñÑ ùñÖ
ùñÜ ùñá ùñà ùñâ ùñä ùñã ùñå ùñç ùñé ùñè ùñê ùñë ùñí ùñì ùñî ùñï ùññ ùñó ùñò ùñô ùñö ùñõ ùñú ùñù ùñû ùñü</code></pre>
<p>
It‚Äôs not like German is already hard enough, they had to invent the least readable font ever to write German in, to make extra sure it would be hell to decipher.
</p>
<p>
So basically I couldn‚Äôt be bothered to try to read a German newspaper in Fraktur. That‚Äôs when I noticed the <em>L‚Äôind√©pendence Luxembourgeoise</em>‚Ä¶ A Luxembourguish newspaper? Written in French? Sounds interesting.
</p>
<p>
And oh boy. Interesting it was.
</p>
<p>
19th century newspapers articles were something else. There‚Äôs this article for instance:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/pray for senators.png"><!-- -->
</p>
<p>
For those of you that do not read French, this article relates that in France, the ministry of justice required priests to include prayers on the Sunday that follows the start of the new season of parliamentary discussions, in order for God to provide senators his help.
</p>
<p>
There this gem too:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/tallest_soldier.jpg"><!-- -->
</p>
<p>
This article presents the tallest soldier of the German army, called Emhke, and nominated by the German Emperor himself to accompany him during his visit to Palestine. Emhke was 2.08 meters tall and weighted 236 pounds (apparently at the time Luxembourg was not fully sold on the metric system).
</p>
<p>
Anyway, I decided to take a look at ads. The last paper of this 4 page newspaper always contained ads and other announcements. For example, there‚Äôs this ad for a pharmacy:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/pharmacy.png"><!-- -->
</p>
<p>
that sells tea, and mineral water. Yes, tea and mineral water. In a pharmacy. Or this one:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/upside_down.png"><!-- -->
</p>
<p>
which is literally upside down in the newspaper (the one from the 10th of April 1877). I don‚Äôt know if it‚Äôs a mistake or if it‚Äôs a marketing ploy, but it did catch my attention, 140 years later, so <em>bravo</em>. This is an announcement made by a shop owner that wants to sell all his merchandise for cheap, perhaps to make space for new stuff coming in?
</p>
<p>
So I decided brush up on my natural language processing skills with R and do topic modeling on these ads. The challenge here is that a single document, the 4th page of the newspaper, contains a lot of ads. So it will probably be difficult to clearly isolate topics. But let‚Äôs try nonetheless. First of all, let‚Äôs load all the <code>.xml</code> files that contain the data. These files look like this:
</p>
<pre><code>&lt;TextLine ID="LINE6" STYLEREFS="TS11" HEIGHT="42" WIDTH="449" HPOS="165" VPOS="493"&gt;
                                    &lt;String ID="S16" CONTENT="l‚Äôapr√®s-midi," WC="0.638" CC="0803367024653" HEIGHT="42" WIDTH="208" HPOS="165" VPOS="493"/&gt;
                                    &lt;SP ID="SP11" WIDTH="24" HPOS="373" VPOS="493"/&gt;
                                    &lt;String ID="S17" CONTENT="le" WC="0.8" CC="40" HEIGHT="30" WIDTH="29" HPOS="397" VPOS="497"/&gt;
                                    &lt;SP ID="SP12" WIDTH="14" HPOS="426" VPOS="497"/&gt;
                                    &lt;String ID="S18" CONTENT="Gouverne" WC="0.638" CC="72370460" HEIGHT="31" WIDTH="161" HPOS="440" VPOS="496" SUBS_TYPE="HypPart1" SUBS_CONTENT="Gouvernement"/&gt;
                                    &lt;HYP CONTENT="-" WIDTH="11" HPOS="603" VPOS="514"/&gt;
                                  &lt;/TextLine&gt;
                        &lt;TextLine ID="LINE7" STYLEREFS="TS11" HEIGHT="41" WIDTH="449" HPOS="166" VPOS="541"&gt;
                                    &lt;String ID="S19" CONTENT="ment" WC="0.725" CC="0074" HEIGHT="26" WIDTH="81" HPOS="166" VPOS="545" SUBS_TYPE="HypPart2" SUBS_CONTENT="Gouvernement"/&gt;
                                    &lt;SP ID="SP13" WIDTH="24" HPOS="247" VPOS="545"/&gt;
                                    &lt;String ID="S20" CONTENT="Royal" WC="0.62" CC="74503" HEIGHT="41" WIDTH="100" HPOS="271" VPOS="541"/&gt;
                                    &lt;SP ID="SP14" WIDTH="26" HPOS="371" VPOS="541"/&gt;
                                    &lt;String ID="S21" CONTENT="Grand-Ducal" WC="0.682" CC="75260334005" HEIGHT="32" WIDTH="218" HPOS="397" VPOS="541"/&gt;
                                  &lt;/TextLine&gt;</code></pre>
<p>
I‚Äôm interested in the ‚ÄúCONTENT‚Äù tag, which contains the words. Let‚Äôs first get that into R.
</p>
<p>
Load the packages, and the files:
</p>
<pre class="r"><code>library(tidyverse)
library(tidytext)
library(topicmodels)
library(brotools)

ad_pages &lt;- str_match(list.files(path = "./", all.files = TRUE, recursive = TRUE), ".*4-alto.xml") %&gt;%
    discard(is.na)</code></pre>
<p>
I save the path of all the pages at once into the <code>ad_pages</code> variables. To understand how and why this works, you must take a look at the hierarchy of the folder:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/layout.png"><!-- -->
</p>
<p>
Inside each of these folder, there is a <code>text</code> folder, and inside this folder there are the <code>.xml</code> files. Because this structure is bit complex, I use the <code>list.files()</code> function with the <code>all.files</code> and <code>recursive</code> argument set to <code>TRUE</code> which allow me to dig deep into the folder structure and list every single file. I am only interested into the 4th page though, so that‚Äôs why I use <code>str_match()</code> to only keep the 4th page using the <code>‚Äú.*4-alto.xml‚Äù</code> regular expression. This is the right regular expression, because the files are named like so:
</p>
<pre><code>1877-12-29_01-00004-alto.xml</code></pre>
<p>
So in the end, <code>ad_pages</code> is a list of all the paths to these files. I then write a function to extract the contents of the ‚ÄúCONTENT‚Äù tag. Here is the function.
</p>
<pre class="r"><code>get_words &lt;- function(page_path){
    
    page &lt;- read_file(page_path)
    
    page_name &lt;- str_extract(page_path, "1.*(?=-0000)") 
    
    page %&gt;%  
        str_split("\n", simplify = TRUE) %&gt;% 
        keep(str_detect(., "CONTENT")) %&gt;% 
        str_extract("(?&lt;=CONTENT)(.*?)(?=WC)") %&gt;% 
        discard(is.na) %&gt;% 
        str_extract("[:alpha:]+") %&gt;% 
        tolower %&gt;% 
        as_tibble %&gt;% 
        rename(tokens = value) %&gt;% 
        mutate(page = page_name)
}</code></pre>
<p>
This function takes the path to a page as argument, and returns a tibble with the two columns: one containing the words, which I called <code>tokens</code> and the second the name of the document this word was found. I uploaded on <code>.xml</code> file <a href="https://gist.github.com/b-rodrigues/a22d2aa63dff01d88acc2916c003489d">here</a> so that you can try the function yourself. The difficult part is <code>str_extract(‚Äú(?&lt;=CONTENT)(.*?)(?=WC)‚Äú)</code> which is were the words inside the ‚ÄúCONTENT‚Äù tag get extracted.
</p>
<p>
I then map this function to all the pages, and get a nice tibble with all the words:
</p>
<pre class="r"><code>ad_words &lt;- map_dfr(ad_pages, get_words)</code></pre>
<pre class="r"><code>ad_words</code></pre>
<pre><code>## # A tibble: 1,114,662 x 2
##    tokens     page                            
##    &lt;chr&gt;      &lt;chr&gt;                           
##  1 afin       1877-01-05_01/text/1877-01-05_01
##  2 de         1877-01-05_01/text/1877-01-05_01
##  3 mettre     1877-01-05_01/text/1877-01-05_01
##  4 mes        1877-01-05_01/text/1877-01-05_01
##  5 honorables 1877-01-05_01/text/1877-01-05_01
##  6 clients    1877-01-05_01/text/1877-01-05_01
##  7 √†          1877-01-05_01/text/1877-01-05_01
##  8 m√™me       1877-01-05_01/text/1877-01-05_01
##  9 d          1877-01-05_01/text/1877-01-05_01
## 10 avantages  1877-01-05_01/text/1877-01-05_01
## # ‚Ä¶ with 1,114,652 more rows</code></pre>
<p>
I then do some further cleaning, removing stop words (French and German, because there are some ads in German) and a bunch of garbage characters and words, which are probably when the OCR failed. I also remove some German words from the few German ads that are in the paper, because they have a very high tf-idf (I‚Äôll explain below what that is). I also remove very common words in ads that were just like stopwords. Every ad of a shop mentioned their clients with <em>honorable client√®le</em>, or used the word <em>vente</em>, and so on. This is what you see below in the very long calls to <code>str_remove_all</code>. I also compute the <code>tf_idf</code> and I am grateful to ThinkR blog post on that, which you can read <a href="https://thinkr.fr/text-mining-et-topic-modeling-avec-r/">here</a>. It‚Äôs in French though, but the idea of the blog post is to present topic modeling with Wikipedia articles. You can also read the section on tf-idf from the Text Mining with R ebook, <a href="https://www.tidytextmining.com/tfidf.html">here</a>. tf-idf gives a measure of how common words are. Very common words, like stopwords, have a tf-idf of 0. So I use this to further remove very common words, by only keeping words with a tf-idf greater than 0.01. This is why I manually remove garbage words and German words below, because they are so uncommon that they have a very high tf-idf and mess up the rest of the analysis. To find these words I had to go back and forth between the tibble of cleaned words and my code, and manually add all these exceptions. It took some time, but definitely made the results of the next steps better.<br> I then use <code>cast_dtm</code> to cast the tibble into a DocumentTermMatrix object, which is needed for the <code>LDA()</code> function that does the topic modeling:
</p>
<pre class="r"><code>stopwords_fr &lt;- read_csv("https://raw.githubusercontent.com/stopwords-iso/stopwords-fr/master/stopwords-fr.txt",
                         col_names = FALSE)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   X1 = col_character()
## )</code></pre>
<pre class="r"><code>stopwords_de &lt;- read_csv("https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/stopwords-de.txt",
                         col_names = FALSE)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   X1 = col_character()
## )</code></pre>
<pre><code>## Warning: 1 parsing failure.
## row col  expected    actual                                                                                   file
## 157  -- 1 columns 2 columns 'https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/stopwords-de.txt'</code></pre>
<pre class="r"><code>ad_words2 &lt;- ad_words %&gt;% 
    filter(!is.na(tokens)) %&gt;% 
    mutate(tokens = str_remove_all(tokens, 
                                   '[|\\|!|"|#|$|%|&amp;|\\*|+|,|-|.|/|:|;|&lt;|=|&gt;|?|@|^|_|`|‚Äô|\'|‚Äò|(|)|\\||~|=|]|¬∞|&lt;|&gt;|¬´|¬ª|\\d{1,100}|¬©|¬Æ|‚Ä¢|‚Äî|‚Äû|‚Äú|-|¬¶\\\\|‚Äù')) %&gt;%
    mutate(tokens = str_remove_all(tokens,
                                   "j'|j‚Äô|m‚Äô|m'|n‚Äô|n'|c‚Äô|c'|qu‚Äô|qu'|s‚Äô|s'|t‚Äô|t'|l‚Äô|l'|d‚Äô|d'|luxembourg|honneur|rue|prix|maison|frs|ber|adresser|unb|mois|vente|informer|sann|neben|rbudj|artringen|salz|eingetragen|ort|ftofjenb|groifdjen|ort|boch|chem|jahrgang|uoa|genannt|neuwahl|wechsel|sittroe|yerlorenkost|beichsmark|tttr|slpril|ofto|rbudj|felben|acferft√ºcf|etr|eft|sbege|incl|estce|bes|franzosengrund|qne|nne|mme|qni|faire|id|kil")) %&gt;%
    anti_join(stopwords_de, by = c("tokens" = "X1")) %&gt;% 
    filter(!str_detect(tokens, "¬ß")) %&gt;% 
    mutate(tokens = ifelse(tokens == "in√©dite", "in√©dit", tokens)) %&gt;% 
    filter(tokens != "") %&gt;% 
    anti_join(stopwords_fr, by = c("tokens" = "X1")) %&gt;% 
    count(page, tokens) %&gt;% 
    bind_tf_idf(tokens, page, n) %&gt;% 
    arrange(desc(tf_idf))

dtm_long &lt;- ad_words2 %&gt;% 
    filter(tf_idf &gt; 0.01) %&gt;% 
    cast_dtm(page, tokens, n)</code></pre>
<p>
To read more details on this, I suggest you take a look at the following section of the Text Mining with R ebook: <a href="https://www.tidytextmining.com/topicmodeling.html#latent-dirichlet-allocation">Latent Dirichlet Allocation</a>.
</p>
<p>
I choose to model 10 topics (<code>k = 10</code>), and set the <code>alpha</code> parameter to 5. This hyperparamater controls how many topics are present in one document. Since my ads are all in one page (one document), I increased it. Let‚Äôs fit the model, and plot the results:
</p>
<pre class="r"><code>lda_model_long &lt;- LDA(dtm_long, k = 10, control = list(alpha = 5))</code></pre>
<p>
I plot the per-topic-per-word probabilities, the ‚Äúbeta‚Äù from the model and plot the 5 words that contribute the most to each topic:
</p>
<pre class="r"><code>result &lt;- tidy(lda_model_long, "beta")

result %&gt;%
    group_by(topic) %&gt;%
    top_n(5, beta) %&gt;%
    ungroup() %&gt;%
    arrange(topic, -beta) %&gt;% 
    mutate(term = reorder(term, beta)) %&gt;%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    coord_flip() +
    theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/newspapers-13-1.png" width="672">
</p>
<p>
So some topics seem clear to me, other not at all. For example topic 4 seems to be about shoes made out of leather. The word <code>semelle</code>, sole, also appears. Then there‚Äôs a lot of topics that reference either music, bals, or instruments. I guess these are ads for local music festivals, or similar events. There‚Äôs also an ad for what seems to be bundles of sticks, topic 3: <code>ch√™ne</code> is oak, <code>copeaux</code> is shavings and you know what <code>fagots</code> is. The first word <code>st√®re</code> which I did not know is a unit of volume equal to one cubic meter (see <a href="https://en.wikipedia.org/wiki/Stere">Wikipedia</a>). So they were likely selling bundle of oak sticks by the cubic meter. For the other topics, I either lack context or perhaps I just need to adjust <code>k</code>, the number of topics to model, and <code>alpha</code> to get better results. In the meantime, topic 1 is about shoes (<code>chaussures</code>), theatre, fuel (<code>combustible</code>) and farts (<code>pet</code>). Really wonder what they were selling in that shop.
</p>
<p>
In any case, this was quite an interesting project. I learned a lot about topic modeling and historical newspapers of my country! I do not know if I will continue exploring it myself, but I am really curious to see what others will do with it!
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2019-01-04-newspapers.html</guid>
  <pubDate>Fri, 04 Jan 2019 00:00:00 GMT</pubDate>
</item>
<item>
  <title>R or Python? Why not both? Using Anaconda Python within R with {reticulate}</title>
  <link>https://b-rodrigues.github.io/posts/2018-12-30-reticulate.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://youtu.be/I8vaCrVIR-Q?t=1h2m26s"> <img src="https://b-rodrigues.github.io/assets/img/why not both.png" title="This literally starts playing when you run both R and Python in the same session"></a>
</p>
</div>
<p>
This short blog post illustrates how easy it is to use R and Python in the same R Notebook thanks to the <code>{reticulate}</code> package. For this to work, you might need to upgrade RStudio to the <a href="https://www.rstudio.com/products/rstudio/download/preview/">current preview version</a>. Let‚Äôs start by importing <code>{reticulate}</code>:
</p>
<pre class="r"><code>library(reticulate)</code></pre>
<p>
<code>{reticulate}</code> is an RStudio package that provides ‚Äú<em>a comprehensive set of tools for interoperability between Python and R</em>‚Äù. With it, it is possible to call Python and use Python libraries within an R session, or define Python chunks in R markdown. I think that using R Notebooks is the best way to work with Python and R; when you want to use Python, you simply use a Python chunk:
</p>
<pre><code>```{python}
your python code here
```</code></pre>
<p>
There‚Äôs even autocompletion for Python object methods:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/autocompletion.png"><!-- -->
</p>
<p>
Fantastic!
</p>
<p>
However, if you wish to use Python interactively within your R session, you must start the Python REPL with the <code>repl_python()</code> function, which starts a Python REPL. You can then do whatever you want, even access objects from your R session, and then when you exit the REPL, any object you created in Python remains accessible in R. I think that using Python this way is a bit more involved and would advise using R Notebooks if you need to use both languages.
</p>
<p>
I installed the Anaconda Python distribution to have Python on my system. To use it with <code>{reticulate}</code> I must first use the <code>use_python()</code> function that allows me to set which version of Python I want to use:
</p>
<pre class="r"><code># This is an R chunk
use_python("~/miniconda3/bin/python")</code></pre>
<p>
I can now load a dataset, still using R:
</p>
<pre class="r"><code># This is an R chunk
data(mtcars)
head(mtcars)</code></pre>
<pre><code>##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1</code></pre>
<p>
and now, to access the <code>mtcars</code> data frame, I simply use the <code>r</code> object:
</p>
<pre class="python"><code># This is a Python chunk
print(r.mtcars.describe())</code></pre>
<pre><code>##              mpg        cyl        disp   ...            am       gear     carb
## count  32.000000  32.000000   32.000000   ...     32.000000  32.000000  32.0000
## mean   20.090625   6.187500  230.721875   ...      0.406250   3.687500   2.8125
## std     6.026948   1.785922  123.938694   ...      0.498991   0.737804   1.6152
## min    10.400000   4.000000   71.100000   ...      0.000000   3.000000   1.0000
## 25%    15.425000   4.000000  120.825000   ...      0.000000   3.000000   2.0000
## 50%    19.200000   6.000000  196.300000   ...      0.000000   4.000000   2.0000
## 75%    22.800000   8.000000  326.000000   ...      1.000000   4.000000   4.0000
## max    33.900000   8.000000  472.000000   ...      1.000000   5.000000   8.0000
## 
## [8 rows x 11 columns]</code></pre>
<p>
<code>.describe()</code> is a Python Pandas DataFrame method to get summary statistics of our data. This means that <code>mtcars</code> was automatically converted from a <code>tibble</code> object to a Pandas DataFrame! Let‚Äôs check its type:
</p>
<pre class="python"><code># This is a Python chunk
print(type(r.mtcars))</code></pre>
<pre><code>## &lt;class 'pandas.core.frame.DataFrame'&gt;</code></pre>
<p>
Let‚Äôs save the summary statistics in a variable:
</p>
<pre class="python"><code># This is a Python chunk
summary_mtcars = r.mtcars.describe()</code></pre>
<p>
Let‚Äôs access this from R, by using the <code>py</code> object:
</p>
<pre class="r"><code># This is an R chunk
class(py$summary_mtcars)</code></pre>
<pre><code>## [1] "data.frame"</code></pre>
<p>
Let‚Äôs try something more complex. Let‚Äôs first fit a linear model in Python, and see how R sees it:
</p>
<pre class="python"><code># This is a Python chunk
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
model = smf.ols('mpg ~ hp', data = r.mtcars).fit()
print(model.summary())</code></pre>
<pre><code>##                             OLS Regression Results                            
## ==============================================================================
## Dep. Variable:                    mpg   R-squared:                       0.602
## Model:                            OLS   Adj. R-squared:                  0.589
## Method:                 Least Squares   F-statistic:                     45.46
## Date:                Sun, 10 Feb 2019   Prob (F-statistic):           1.79e-07
## Time:                        00:25:51   Log-Likelihood:                -87.619
## No. Observations:                  32   AIC:                             179.2
## Df Residuals:                      30   BIC:                             182.2
## Df Model:                           1                                         
## Covariance Type:            nonrobust                                         
## ==============================================================================
##                  coef    std err          t      P&gt;|t|      [0.025      0.975]
## ------------------------------------------------------------------------------
## Intercept     30.0989      1.634     18.421      0.000      26.762      33.436
## hp            -0.0682      0.010     -6.742      0.000      -0.089      -0.048
## ==============================================================================
## Omnibus:                        3.692   Durbin-Watson:                   1.134
## Prob(Omnibus):                  0.158   Jarque-Bera (JB):                2.984
## Skew:                           0.747   Prob(JB):                        0.225
## Kurtosis:                       2.935   Cond. No.                         386.
## ==============================================================================
## 
## Warnings:
## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
<p>
Just for fun, I ran the linear regression with the Scikit-learn library too:
</p>
<pre class="python"><code># This is a Python chunk
import numpy as np
from sklearn.linear_model import LinearRegression  
regressor = LinearRegression()  
x = r.mtcars[["hp"]]
y = r.mtcars[["mpg"]]
model_scikit = regressor.fit(x, y)
print(model_scikit.intercept_)</code></pre>
<pre><code>## [30.09886054]</code></pre>
<pre class="python"><code>print(model_scikit.coef_)</code></pre>
<pre><code>## [[-0.06822828]]</code></pre>
<p>
Let‚Äôs access the <code>model</code> variable in R and see what type of object it is in R:
</p>
<pre class="r"><code># This is an R chunk
model_r &lt;- py$model
class(model_r)</code></pre>
<pre><code>## [1] "statsmodels.regression.linear_model.RegressionResultsWrapper"
## [2] "statsmodels.base.wrapper.ResultsWrapper"                     
## [3] "python.builtin.object"</code></pre>
<p>
So because this is a custom Python object, it does not get converted into the equivalent R object. This is described <a href="https://rstudio.github.io/reticulate/index.html">here</a>. However, you can still use Python methods from within an R chunk!
</p>
<pre class="r"><code># This is an R chunk
model_r$aic</code></pre>
<pre><code>## [1] 179.2386</code></pre>
<pre class="r"><code>model_r$params</code></pre>
<pre><code>##   Intercept          hp 
## 30.09886054 -0.06822828</code></pre>
<p>
I must say that I am very impressed with the <code>{reticulate}</code> package. I think that even if you are primarily a Python user, this is still very interesting to know in case you need a specific function from an R package. Just write all your script inside a Python Markdown chunk and then use the R function you need from an R chunk! Of course there is also a way to use R from Python, a Python library called <code>rpy2</code> but I am not very familiar with it. From what I read, it seems to be also quite simple to use.
</p>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-12-30-reticulate.html</guid>
  <pubDate>Sun, 30 Dec 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Some fun with {gganimate}</title>
  <link>https://b-rodrigues.github.io/posts/2018-12-27-fun_gganimate.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<video width="864" height="480" controls="">
<source src="../assets/img/wiid_gganimate.webm" type="video/webm">
Your browser does not support the video tag. </video>
</div>
<p>
In this short blog post I show you how you can use the <code>{gganimate}</code> package to create animations from <code>{ggplot2}</code> graphs with data from UNU-WIDER.
</p>
<section id="wiid-data" class="level2">
<h2 class="anchored" data-anchor-id="wiid-data">
WIID data
</h2>
<p>
Just before Christmas, UNU-WIDER released a new edition of their World Income Inequality Database:
</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
<em>NEW <a href="https://twitter.com/hashtag/DATA?src=hash&amp;ref_src=twsrc%5Etfw">#DATA</a></em><br>We‚Äôve just released a new version of the World Income Inequality Database.<br>WIID4 includes <a href="https://twitter.com/hashtag/data?src=hash&amp;ref_src=twsrc%5Etfw">#data</a> from 7 new countries, now totalling 189, and reaches the year 2017. All data is freely available for download on our website: <a href="https://t.co/XFxuLvyKTC">https://t.co/XFxuLvyKTC</a> <a href="https://t.co/rCf9eXN8D5">pic.twitter.com/rCf9eXN8D5</a>
</p>
‚Äî UNU-WIDER (<span class="citation" data-cites="UNUWIDER">@UNUWIDER</span>) <a href="https://twitter.com/UNUWIDER/status/1076001879556005888?ref_src=twsrc%5Etfw">December 21, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>
The data is available in Excel and STATA formats, and I thought it was a great opportunity to release it as an R package. You can install it with:
</p>
<pre class="r"><code>devtools::install_github("b-rodrigues/wiid4")</code></pre>
<p>
Here a short description of the data, taken from UNU-WIDER‚Äôs website:
</p>
<p>
<em>‚ÄúThe World Income Inequality Database (WIID) presents information on income inequality for developed, developing, and transition countries. It provides the most comprehensive set of income inequality statistics available and can be downloaded for free.</em>
</p>
<p>
<em>WIID4, released in December 2018, covers 189 countries (including historical entities), with over 11,000 data points in total. With the current version, the latest observations now reach the year 2017.‚Äù</em>
</p>
<p>
It was also a good opportunity to play around with the <code>{gganimate}</code> package. This package makes it possible to create animations and is an extension to <code>{ggplot2}</code>. Read more about it <a href="https://github.com/thomasp85/gganimate">here</a>.
</p>
</section>
<section id="preparing-the-data" class="level2">
<h2 class="anchored" data-anchor-id="preparing-the-data">
Preparing the data
</h2>
<p>
To create a smooth animation, I need to have a cylindrical panel data set; meaning that for each country in the data set, there are no missing years. I also chose to focus on certain variables only; net income, all the population of the country (instead of just focusing on the economically active for instance) as well as all the country itself (and not just the rural areas). On <a href="https://www.wider.unu.edu/sites/default/files/WIID/PDF/WIID4%20User%20Guide.pdf">this link</a> you can find a codebook (pdf warning), so you can understand the filters I defined below better.
</p>
<p>
Let‚Äôs first load the packages, data and perform the necessary transformations:
</p>
<pre class="r"><code>library(wiid4)
library(tidyverse)
library(ggrepel)
library(gganimate)
library(brotools)

small_wiid4 &lt;- wiid4 %&gt;%
    mutate(eu = as.character(eu)) %&gt;%
    mutate(eu = case_when(eu == "1" ~ "EU member state",
                          eu == "0" ~ "Non-EU member state")) %&gt;%
    filter(resource == 1, popcovr == 1, areacovr == 1, scale == 2) %&gt;%
    group_by(country) %&gt;%
    group_by(country, year) %&gt;%
    filter(quality_score == max(quality_score)) %&gt;%
    filter(source == min(source)) %&gt;%
    filter(!is.na(bottom5)) %&gt;%
    group_by(country) %&gt;%
    mutate(flag = ifelse(all(seq(2004, 2016) %in% year), 1, 0)) %&gt;%
    filter(flag == 1, year &gt; 2003) %&gt;%
    mutate(year = lubridate::ymd(paste0(year, "-01-01")))</code></pre>
<p>
For some country and some years, there are several sources of data with varying quality. I only keep the highest quality sources with:
</p>
<pre class="r"><code>    group_by(country, year) %&gt;%
    filter(quality_score == max(quality_score)) %&gt;%</code></pre>
<p>
If there are different sources of equal quality, I give priority to the sources that are the most comparable across country (Luxembourg Income Study, LIS data) to less comparable sources with (at least that‚Äôs my understanding of the <code>source</code> variable):
</p>
<pre class="r"><code>    filter(source == min(source)) %&gt;%</code></pre>
<p>
I then remove missing data with:
</p>
<pre class="r"><code>    filter(!is.na(bottom5)) %&gt;%</code></pre>
<p>
<code>bottom5</code> and <code>top5</code> give the share of income that is controlled by the bottom 5% and top 5% respectively. These are the variables that I want to plot.
</p>
<p>
Finally I keep the years 2004 to 2016, without any interruption with the following line:
</p>
<pre class="r"><code>    mutate(flag = ifelse(all(seq(2004, 2016) %in% year), 1, 0)) %&gt;%
    filter(flag == 1, year &gt; 2003) %&gt;%</code></pre>
<p>
<code>ifelse(all(seq(2004, 2016) %in% year), 1, 0))</code> creates a flag that equals <code>1</code> only if the years 2004 to 2016 are present in the data without any interruption. Then I only keep the data from 2004 on and only where the flag variable equals 1.
</p>
<p>
In the end, I ended up only with European countries. It would have been interesting to have countries from other continents, but apparently only European countries provide data in an annual basis.
</p>
</section>
<section id="creating-the-animation" class="level2">
<h2 class="anchored" data-anchor-id="creating-the-animation">
Creating the animation
</h2>
<p>
To create the animation I first started by creating a static ggplot showing what I wanted; a scatter plot of the income by bottom and top 5%. The size of the bubbles should be proportional to the GDP of the country (another variable provided in the data). Once the plot looked how I wanted I added the lines that are specific to <code>{gganimate}</code>:
</p>
<pre class="r"><code>    labs(title = 'Year: {frame_time}', x = 'Top 5', y = 'Bottom 5') +
    transition_time(year) +
    ease_aes('linear')</code></pre>
<p>
I took this from <code>{gganimate}</code>‚Äôs README.
</p>
<pre class="r"><code>animation &lt;- ggplot(small_wiid4) +
    geom_point(aes(y = bottom5, x = top5, colour = eu, size = log(gdp_ppp_pc_usd2011))) +
    xlim(c(10, 20)) +
    geom_label_repel(aes(y = bottom5, x = top5, label = country), hjust = 1, nudge_x = 20) +
    theme(legend.position = "bottom") +
    theme_blog() +
    scale_color_blog() +
    labs(title = 'Year: {frame_time}', x = 'Top 5', y = 'Bottom 5') +
    transition_time(year) +
    ease_aes('linear')</code></pre>
<p>
I use <code>geom_label_repel</code> to place the countries‚Äô labels on the right of the plot. If I don‚Äôt do this, the labels of the countries would be floating around and the animation would be unreadable.
</p>
<p>
I then spent some time trying to render a nice webm instead of a gif. It took some trial and error and I am still not entirely satisfied with the result, but here is the code to render the animation:
</p>
<pre class="r"><code>animate(animation, renderer = ffmpeg_renderer(options = list(s = "864x480", 
                                                             vcodec = "libvpx-vp9",
                                                             crf = "15",
                                                             b = "1600k", 
                                                             vf = "setpts=5*PTS")))</code></pre>
<p>
The option <code>vf = ‚Äúsetpts=5*PTS‚Äù</code> is important because it slows the video down, so we can actually see something. <code>crf = ‚Äú15‚Äù</code> is the quality of the video (lower is better), <code>b = ‚Äú1600k‚Äù</code> is the bitrate, and <code>vcodec = ‚Äúlibvpx-vp9‚Äù</code> is the codec I use. The video you saw at the top of this post is the result. You can also find the video <a href="https://raw.githubusercontent.com/rbind/b-rodrigues.github.com/master/static/img/wiid_gganimate.webm">here</a>, and here‚Äôs a gif if all else fails:
</p>
<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=3zXx0ReqOOI"> <img src="https://b-rodrigues.github.io/assets/img/wiid_gganimate_gif.gif" title="Click to listen to OST of this gif"></a>
</p>
</div>
<p>
I would have preferred if the video was smoother, which should be possible by creating more frames. I did not find such an option in <code>{gganimate}</code>, and perhaps there is none, at least for now.
</p>
<p>
In any case <code>{gganimate}</code> is pretty nice to play with, and I‚Äôll definitely use it more!
</p>
</section>
<section id="update" class="level3">
<h3 class="anchored" data-anchor-id="update">
Update
</h3>
<p>
Silly me! It turns out thate the <code>animate()</code> function has arguments that can control the number of frames and the duration, without needing to pass options to the renderer. I was looking at options for the renderer only, without having read the documentation of the <code>animate()</code> function. It turns out that you can pass several arguments to the <code>animate()</code> function; for example, here is how you can make a GIF that lasts for 20 seconds running and 20 frames per second, pausing for 5 frames at the end and then restarting:
</p>
<pre class="r"><code>animate(animation, nframes = 400, duration = 20, fps = 20, end_pause = 5, rewind = TRUE)</code></pre>
<p>
I guess that you should only pass options to the renderer if you really need fine-grained control.
</p>
<p>
This took around 2 minutes to finish. You can use the same options with the ffmpeg renderer too. Here is what the gif looks like:
</p>
<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=3zXx0ReqOOI"> <img src="https://b-rodrigues.github.io/assets/img/wiid_gganimate_gif_smooth.gif" title="Click to listen to OST of this gif"></a>
</p>
</div>
<p>
Much, much smoother!
</p>
</section>


 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-12-27-fun_gganimate.html</guid>
  <pubDate>Thu, 27 Dec 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Objects types and some useful R functions for beginners</title>
  <link>https://b-rodrigues.github.io/posts/2018-12-24-modern_objects.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=M-1nTwiHxic"> <img width="400" src="https://b-rodrigues.github.io/assets/img/santa_sanders.jpg" title="The frydiest time of the year"></a>
</p>
</div>
<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<p>
This blog post is an excerpt of my ebook <em>Modern R with the tidyverse</em> that you can read for free <a href="https://b-rodrigues.github.io/modern_R/">here</a>. This is taken from Chapter 2, which explains the different R objects you can manipulate as well as some functions to get you started.
</p>
<section id="objects-types-and-useful-r-functions-to-get-started" class="level2">
<h2 class="anchored" data-anchor-id="objects-types-and-useful-r-functions-to-get-started">
Objects, types and useful R functions to get started
</h2>
<p>
All objects in R have a given <em>type</em>. You already know most of them, as these types are also used in mathematics. Integers, floating point numbers, or floats, matrices, etc, are all objects you are already familiar with. But R has other, maybe lesser known data types (that you can find in a lot of other programming languages) that you need to become familiar with. But first, we need to learn how to assign a value to a variable. This can be done in two ways:
</p>
<pre class="r"><code>a &lt;- 3</code></pre>
<p>
or
</p>
<pre class="r"><code>a = 3</code></pre>
<p>
in very practical terms, there is no difference between the two. I prefer using <code>&lt;-</code> for assigning values to variables and reserve <code>=</code> for passing arguments to functions, for example:
</p>
<pre class="r"><code>spam &lt;- mean(x = c(1,2,3))</code></pre>
<p>
I think this is less confusing than:
</p>
<pre class="r"><code>spam = mean(x = c(1,2,3))</code></pre>
<p>
but as I explained above you can use whatever you feel most comfortable with.
</p>
<section id="the-numeric-class" class="level3">
<h3 class="anchored" data-anchor-id="the-numeric-class">
The <code>numeric</code> class
</h3>
<p>
To define single numbers, you can do the following:
</p>
<pre class="r"><code>a &lt;- 3</code></pre>
<p>
The <code>class()</code> function allows you to check the class of an object:
</p>
<pre class="r"><code>class(a)</code></pre>
<pre><code>## [1] "numeric"</code></pre>
<p>
Decimals are defined with the character <code>.</code>:
</p>
<pre class="r"><code>a &lt;- 3.14</code></pre>
<p>
R also supports integers. If you find yourself in a situation where you explicitly need an integer and not a floating point number, you can use the following:
</p>
<pre class="r"><code>a  &lt;- as.integer(3)
class(a)</code></pre>
<pre><code>## [1] "integer"</code></pre>
<p>
The <code>as.integer()</code> function is very useful, because it converts its argument into an integer. There is a whole family of <code>as.*()</code> functions. To convert <code>a</code> into a floating point number again:
</p>
<pre class="r"><code>class(as.numeric(a))</code></pre>
<pre><code>## [1] "numeric"</code></pre>
<p>
There is also <code>is.numeric()</code> which tests whether a number is of the <code>numeric</code> class:
</p>
<pre class="r"><code>is.numeric(a)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>
These functions are very useful, there is one for any of the supported types in R. Later, we are going to learn about the <code>{purrr}</code> package, which is a very powerful package for functional programming. This package includes further such functions.
</p>
</section>
<section id="the-character-class" class="level3">
<h3 class="anchored" data-anchor-id="the-character-class">
The <code>character</code> class
</h3>
<p>
Use <code>‚Äù ‚Äú</code> to define characters (called strings in other programming languages):
</p>
<pre class="r"><code>a &lt;- "this is a string"</code></pre>
<pre class="r"><code>class(a)</code></pre>
<pre><code>## [1] "character"</code></pre>
<p>
To convert something to a character you can use the <code>as.character()</code> function:
</p>
<pre class="r"><code>a &lt;- 4.392

class(a)</code></pre>
<pre><code>## [1] "numeric"</code></pre>
<pre class="r"><code>class(as.character(a))</code></pre>
<pre><code>## [1] "character"</code></pre>
<p>
It is also possible to convert a character to a numeric:
</p>
<pre class="r"><code>a &lt;- "4.392"

class(a)</code></pre>
<pre><code>## [1] "character"</code></pre>
<pre class="r"><code>class(as.numeric(a))</code></pre>
<pre><code>## [1] "numeric"</code></pre>
<p>
But this only works if it makes sense:
</p>
<pre class="r"><code>a &lt;- "this won't work, chief"

class(a)</code></pre>
<pre><code>## [1] "character"</code></pre>
<pre class="r"><code>as.numeric(a)</code></pre>
<pre><code>## Warning: NAs introduced by coercion</code></pre>
<pre><code>## [1] NA</code></pre>
<p>
A very nice package to work with characters is <code>{stringr}</code>, which is also part of the <code>{tidyverse}</code>.
</p>
</section>
<section id="the-factor-class" class="level3">
<h3 class="anchored" data-anchor-id="the-factor-class">
The <code>factor</code> class
</h3>
<p>
Factors look like characters, but are very different. They are the representation of categorical variables. A <code>{tidyverse}</code> package to work with factors is <code>{forcats}</code>. You would rarely use factor variables outside of datasets, so for now, it is enough to know that this class exists. We are going to learn more about factor variables in Chapter 4, by using the <code>{forcats}</code> package.
</p>
</section>
<section id="the-date-class" class="level3">
<h3 class="anchored" data-anchor-id="the-date-class">
The <code>Date</code> class
</h3>
<p>
Dates also look like characters, but are very different too:
</p>
<pre class="r"><code>as.Date("2019/03/19")</code></pre>
<pre><code>## [1] "2019-03-19"</code></pre>
<pre class="r"><code>class(as.Date("2019/03/19"))</code></pre>
<pre><code>## [1] "Date"</code></pre>
<p>
Manipulating dates and time can be tricky, but thankfully there‚Äôs a <code>{tidyverse}</code> package for that, called <code>{lubridate}</code>. We are going to go over this package in Chapter 4.
</p>
</section>
<section id="the-logical-class" class="level3">
<h3 class="anchored" data-anchor-id="the-logical-class">
The <code>logical</code> class
</h3>
<p>
This class is the result of logical comparisons, for example, if you type:
</p>
<pre class="r"><code>4 &gt; 3</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>
R returns <code>TRUE</code>, which is an object of class <code>logical</code>:
</p>
<pre class="r"><code>k &lt;- 4 &gt; 3
class(k)</code></pre>
<pre><code>## [1] "logical"</code></pre>
<p>
In other programming languages, <code>logical</code>s are often called <code>bool</code>s. A <code>logical</code> variable can only have two values, either <code>TRUE</code> or <code>FALSE</code>. You can test the truthiness of a variable with <code>isTRUE()</code>:
</p>
<pre class="r"><code>k &lt;- 4 &gt; 3
isTRUE(k)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>
How can you test if a variable is false? There is not a <code>isFALSE()</code> function (at least not without having to load a package containing this function), but there is way to do it:
</p>
<pre class="r"><code>k &lt;- 4 &gt; 3
!isTRUE(k)</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>
The <code>!</code> operator indicates negation, so the above expression could be translated as <em>is k not TRUE?</em>. There are other such operators, namely <code>&amp;, &amp;&amp;, |, ||</code>. <code>&amp;</code> means <em>and</em> and <code>|</code> stands for <em>or</em>. You might be wondering what the difference between <code>&amp;</code> and <code>&amp;&amp;</code> is? Or between <code>|</code> and <code>||</code>? <code>&amp;</code> and <code>|</code> work on vectors, doing pairwise comparisons:
</p>
<pre class="r"><code>one &lt;- c(TRUE, FALSE, TRUE, FALSE)
two &lt;- c(FALSE, TRUE, TRUE, TRUE)
one &amp; two</code></pre>
<pre><code>## [1] FALSE FALSE  TRUE FALSE</code></pre>
<p>
Compare this to the <code>&amp;&amp;</code> operator:
</p>
<pre class="r"><code>one &lt;- c(TRUE, FALSE, TRUE, FALSE)
two &lt;- c(FALSE, TRUE, TRUE, TRUE)
one &amp;&amp; two</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>
The <code>&amp;&amp;</code> and <code>||</code> operators only compare the first element of the vectors and stop as soon as a the return value can be safely determined. This is called short-circuiting. Consider the following:
</p>
<pre class="r"><code>one &lt;- c(TRUE, FALSE, TRUE, FALSE)
two &lt;- c(FALSE, TRUE, TRUE, TRUE)
three &lt;- c(TRUE, TRUE, FALSE, FALSE)
one &amp;&amp; two &amp;&amp; three</code></pre>
<pre><code>## [1] FALSE</code></pre>
<pre class="r"><code>one || two || three</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>
The <code>||</code> operator stops as soon it evaluates to <code>TRUE</code> whereas the <code>&amp;&amp;</code> stops as soon as it evaluates to <code>FALSE</code>. Personally, I rarely use <code>||</code> or <code>&amp;&amp;</code> because I get confused. I find using <code>|</code> or <code>&amp;</code> in combination with the <code>all()</code> or <code>any()</code> functions much more useful:
</p>
<pre class="r"><code>one &lt;- c(TRUE, FALSE, TRUE, FALSE)
two &lt;- c(FALSE, TRUE, TRUE, TRUE)
any(one &amp; two)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>all(one &amp; two)</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>
<code>any()</code> checks whether any of the vector‚Äôs elements are <code>TRUE</code> and <code>all()</code> checks if all elements of the vector are <code>TRUE</code>.
</p>
<p>
As a final note, you should know that is possible to use <code>T</code> for <code>TRUE</code> and <code>F</code> for <code>FALSE</code> but I would advise against doing this, because it is not very explicit.
</p>
</section>
<section id="vectors-and-matrices" class="level3">
<h3 class="anchored" data-anchor-id="vectors-and-matrices">
Vectors and matrices
</h3>
<p>
You can create a vector in different ways. But first of all, it is important to understand that a vector in most programming languages is nothing more than a list of things. These things can be numbers (either integers or floats), strings, or even other vectors. A vector in R can only contain elements of one single type. This is not the case for a list, which is much more flexible. We will talk about lists shortly, but let‚Äôs first focus on vectors and matrices.
</p>
<section id="the-c-function" class="level4">
<h4 class="anchored" data-anchor-id="the-c-function">
The <code>c()</code> function
</h4>
<p>
A very important function that allows you to build a vector is <code>c()</code>:
</p>
<pre class="r"><code>a &lt;- c(1,2,3,4,5)</code></pre>
<p>
This creates a vector with elements 1, 2, 3, 4, 5. If you check its class:
</p>
<pre class="r"><code>class(a)</code></pre>
<pre><code>## [1] "numeric"</code></pre>
<p>
This can be confusing: you where probably expecting a to be of class <em>vector</em> or something similar. This is not the case if you use <code>c()</code> to create the vector, because <code>c()</code> doesn‚Äôt build a vector in the mathematical sense, but a so-called atomic vector. Checking its dimension:
</p>
<pre class="r"><code>dim(a)</code></pre>
<pre><code>## NULL</code></pre>
<p>
returns <code>NULL</code> because an atomic vector doesn‚Äôt have a dimension. If you want to create a true vector, you need to use <code>cbind()</code> or <code>rbind()</code>.
</p>
<p>
But before continuing, be aware that atomic vectors can only contain elements of the same type:
</p>
<pre class="r"><code>c(1, 2, "3")</code></pre>
<pre><code>## [1] "1" "2" "3"</code></pre>
<p>
because ‚Äú3‚Äù is a character, all the other values get implicitly converted to characters. You have to be very careful about this, and if you use atomic vectors in your programming, you have to make absolutely sure that no characters or logicals or whatever else are going to convert your atomic vector to something you were not expecting.
</p>
</section>
<section id="cbind-and-rbind" class="level4">
<h4 class="anchored" data-anchor-id="cbind-and-rbind">
<code>cbind()</code> and <code>rbind()</code>
</h4>
<p>
You can create a <em>true</em> vector with <code>cbind()</code>:
</p>
<pre class="r"><code>a &lt;- cbind(1, 2, 3, 4, 5)</code></pre>
<p>
Check its class now:
</p>
<pre class="r"><code>class(a)</code></pre>
<pre><code>## [1] "matrix"</code></pre>
<p>
This is exactly what we expected. Let‚Äôs check its dimension:
</p>
<pre class="r"><code>dim(a)</code></pre>
<pre><code>## [1] 1 5</code></pre>
<p>
This returns the dimension of <code>a</code> using the LICO notation (number of LInes first, the number of COlumns).
</p>
<p>
It is also possible to bind vectors together to create a matrix.
</p>
<pre class="r"><code>b &lt;- cbind(6,7,8,9,10)</code></pre>
<p>
Now let‚Äôs put vector <code>a</code> and <code>b</code> into a matrix called <code>matrix_c</code> using <code>rbind()</code>. <code>rbind()</code> functions the same way as <code>cbind()</code> but glues the vectors together by rows and not by columns.
</p>
<pre class="r"><code>matrix_c &lt;- rbind(a,b)
print(matrix_c)</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    2    3    4    5
## [2,]    6    7    8    9   10</code></pre>
</section>
<section id="the-matrix-class" class="level4">
<h4 class="anchored" data-anchor-id="the-matrix-class">
The <code>matrix</code> class
</h4>
<p>
R also has support for matrices. For example, you can create a matrix of dimension (5,5) filled with 0‚Äôs with the <code>matrix()</code> function:
</p>
<pre class="r"><code>matrix_a &lt;- matrix(0, nrow = 5, ncol = 5)</code></pre>
<p>
If you want to create the following matrix:
</p>
<p>
<img src="https://latex.codecogs.com/png.latex?%5B%20B%20=%20(%0A%5C%5B%5Cbegin%7Barray%7D%7Bccc%7D%0A2%20&amp;amp;%204%20&amp;amp;%203%20%5C%5C%0A1%20&amp;amp;%205%20&amp;amp;%207%0A%5Cend%7Barray%7D%5C%5D%0A)%20%5D">
</p>
<p>
you would do it like this:
</p>
<pre class="r"><code>B &lt;- matrix(c(2, 4, 3, 1, 5, 7), nrow = 2, byrow = TRUE)</code></pre>
<p>
The option <code>byrow = TRUE</code> means that the rows of the matrix will be filled first.
</p>
<p>
You can access individual elements of <code>matrix_a</code> like so:
</p>
<pre class="r"><code>matrix_a[2, 3]</code></pre>
<pre><code>## [1] 0</code></pre>
<p>
and R returns its value, 0. We can assign a new value to this element if we want. Try:
</p>
<pre class="r"><code>matrix_a[2, 3] &lt;- 7</code></pre>
<p>
and now take a look at <code>matrix_a</code> again.
</p>
<pre class="r"><code>print(matrix_a)</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4] [,5]
## [1,]    0    0    0    0    0
## [2,]    0    0    7    0    0
## [3,]    0    0    0    0    0
## [4,]    0    0    0    0    0
## [5,]    0    0    0    0    0</code></pre>
<p>
Recall our vector <code>b</code>:
</p>
<pre class="r"><code>b &lt;- cbind(6,7,8,9,10)</code></pre>
<p>
To access its third element, you can simply write:
</p>
<pre class="r"><code>b[3]</code></pre>
<pre><code>## [1] 8</code></pre>
<p>
I have heard many people praising R for being a matrix based language. Matrices are indeed useful, and statisticians are used to working with them. However, I very rarely use matrices in my day to day work, and prefer an approach based on data frames (which will be discussed below). This is because working with data frames makes it easier to use R‚Äôs advanced functional programming language capabilities, and this is where R really shines in my opinion. Working with matrices almost automatically implies using loops and all the iterative programming techniques, <em>√† la Fortran</em>, which I personally believe are ill-suited for interactive statistical programming (as discussed in the introduction).
</p>
</section>
</section>
<section id="the-list-class" class="level3">
<h3 class="anchored" data-anchor-id="the-list-class">
The <code>list</code> class
</h3>
<p>
The <code>list</code> class is a very flexible class, and thus, very useful. You can put anything inside a list, such as numbers:
</p>
<pre class="r"><code>list1 &lt;- list(3, 2)</code></pre>
<p>
or other lists constructed with <code>c()</code>:
</p>
<pre class="r"><code>list2 &lt;- list(c(1, 2), c(3, 4))</code></pre>
<p>
you can also put objects of different classes in the same list:
</p>
<pre class="r"><code>list3 &lt;- list(3, c(1, 2), "lists are amazing!")</code></pre>
<p>
and of course create list of lists:
</p>
<pre class="r"><code>my_lists &lt;- list(list1, list2, list3)</code></pre>
<p>
To check the contents of a list, you can use the structure function <code>str()</code>:
</p>
<pre class="r"><code>str(my_lists)</code></pre>
<pre><code>## List of 3
##  $ :List of 2
##   ..$ : num 3
##   ..$ : num 2
##  $ :List of 2
##   ..$ : num [1:2] 1 2
##   ..$ : num [1:2] 3 4
##  $ :List of 3
##   ..$ : num 3
##   ..$ : num [1:2] 1 2
##   ..$ : chr "lists are amazing!"</code></pre>
<p>
or you can use RStudio‚Äôs <em>Environment</em> pane:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/rstudio_environment_list.gif"><!-- -->
</p>
<p>
You can also create named lists:
</p>
<pre class="r"><code>list4 &lt;- list("a" = 2, "b" = 8, "c" = "this is a named list")</code></pre>
<p>
and you can access the elements in two ways:
</p>
<pre class="r"><code>list4[[1]]</code></pre>
<pre><code>## [1] 2</code></pre>
<p>
or, for named lists:
</p>
<pre class="r"><code>list4$c</code></pre>
<pre><code>## [1] "this is a named list"</code></pre>
<p>
Lists are used extensively because they are so flexible. You can build lists of datasets and apply functions to all the datasets at once, build lists of models, lists of plots, etc‚Ä¶ In the later chapters we are going to learn all about them. Lists are central objects in a functional programming workflow for interactive statistical analysis.
</p>
</section>
<section id="the-data.frame-and-tibble-classes" class="level3">
<h3 class="anchored" data-anchor-id="the-data.frame-and-tibble-classes">
The <code>data.frame</code> and <code>tibble</code> classes
</h3>
<p>
In the next chapter we are going to learn how to import datasets into R. Once you import data, the resulting object is either a <code>data.frame</code> or a <code>tibble</code> depending on which package you used to import the data. <code>tibble</code>s extend <code>data.frame</code>s so if you know about <code>data.frame</code> objects already, working with <code>tibble</code>s will be very easy. <code>tibble</code>s have a better <code>print()</code> method, and some other niceties.
</p>
<p>
However, I want to stress that these objects are central to R and are thus very important; they are actually special cases of lists, discussed above. There are different ways to print a <code>data.frame</code> or a <code>tibble</code> if you wish to inspect it. You can use <code>View(my_data)</code> to show the <code>my_data</code> <code>data.frame</code> in the <em>View</em> pane of RStudio:
</p>
<p>
<img src="https://b-rodrigues.github.io/assets/img/rstudio_view_data.gif"><!-- -->
</p>
<p>
You can also use the <code>str()</code> function:
</p>
<pre class="r"><code>str(my_data)</code></pre>
<p>
And if you need to access an individual column, you can use the <code>$</code> sign, same as for a list:
</p>
<pre class="r"><code>my_data$col1</code></pre>
</section>
<section id="formulas" class="level3">
<h3 class="anchored" data-anchor-id="formulas">
Formulas
</h3>
<p>
We will learn more about formulas later, but because it is an important object, it is useful if you already know about them early on. A formula is defined in the following way:
</p>
<pre class="r"><code>my_formula &lt;- ~x

class(my_formula)</code></pre>
<pre><code>## [1] "formula"</code></pre>
<p>
Formula objects are defined using the <code>~</code> symbol. Formulas are useful to define statistical models, for example for a linear regression:
</p>
<pre class="r"><code>lm(y ~ x)</code></pre>
<p>
or also to define anonymous functions, but more on this later.
</p>
</section>
</section>
<section id="models" class="level2">
<h2 class="anchored" data-anchor-id="models">
Models
</h2>
<p>
A statistical model is an object like any other in R:
</p>
<pre class="r"><code>data(mtcars)

my_model &lt;- lm(mpg ~ hp, mtcars)

class(my_model)</code></pre>
<pre><code>## [1] "lm"</code></pre>
<p>
<code>my_model</code> is an object of class <code>lm</code>. You can apply different functions to a model object:
</p>
<pre class="r"><code>summary(my_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = mpg ~ hp, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.7121 -2.1122 -0.8854  1.5819  8.2360 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***
## hp          -0.06823    0.01012  -6.742 1.79e-07 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.863 on 30 degrees of freedom
## Multiple R-squared:  0.6024, Adjusted R-squared:  0.5892 
## F-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07</code></pre>
<p>
This class will be explored in later chapters.
</p>
<section id="null-na-and-nan" class="level3">
<h3 class="anchored" data-anchor-id="null-na-and-nan">
NULL, NA and NaN
</h3>
<p>
The <code>NULL</code>, <code>NA</code> and <code>NaN</code> classes are pretty special. <code>NULL</code> is returned when the result of function is undetermined. For example, consider <code>list4</code>:
</p>
<pre class="r"><code>list4</code></pre>
<pre><code>## $a
## [1] 2
## 
## $b
## [1] 8
## 
## $c
## [1] "this is a named list"</code></pre>
<p>
if you try to access an element that does not exist, such as <code>d</code>, you will get <code>NULL</code> back:
</p>
<pre class="r"><code>list4$d</code></pre>
<pre><code>## NULL</code></pre>
<p>
<code>NaN</code> means ‚ÄúNot a Number‚Äù and is returned when a function return something that is not a number:
</p>
<pre class="r"><code>sqrt(-1)</code></pre>
<pre><code>## Warning in sqrt(-1): NaNs produced</code></pre>
<pre><code>## [1] NaN</code></pre>
<p>
or:
</p>
<pre class="r"><code>0/0</code></pre>
<pre><code>## [1] NaN</code></pre>
<p>
Basically, numbers that cannot be represented as floating point numbers are <code>NaN</code>.
</p>
<p>
Finally, there‚Äôs <code>NA</code> which is closely related to <code>NaN</code> but is used for missing values. <code>NA</code> stands for <code>Not Available</code>. There are several types of <code>NA</code>s:
</p>
<ul>
<li>
<code>NA_integer_</code>
</li>
<li>
<code>NA_real_</code>
</li>
<li>
<code>NA_complex_</code>
</li>
<li>
<code>NA_character_</code>
</li>
</ul>
<p>
but these are in principle only used when you need to program your own functions and need to explicitly test for the missingness of, say, a character value.
</p>
<p>
To test whether a value is <code>NA</code>, use the <code>is.na()</code> function.
</p>
</section>
<section id="useful-functions-to-get-you-started" class="level3">
<h3 class="anchored" data-anchor-id="useful-functions-to-get-you-started">
Useful functions to get you started
</h3>
<p>
This section will list several basic R functions that are very useful and should be part of your toolbox.
</p>
<section id="sequences" class="level4">
<h4 class="anchored" data-anchor-id="sequences">
Sequences
</h4>
<p>
There are several functions that create sequences, <code>seq()</code>, <code>seq_along()</code> and <code>rep()</code>. <code>rep()</code> is easy enough:
</p>
<pre class="r"><code>rep(1, 10)</code></pre>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1</code></pre>
<p>
This simply repeats <code>1</code> 10 times. You can repeat other objects too:
</p>
<pre class="r"><code>rep("HAHA", 10)</code></pre>
<pre><code>##  [1] "HAHA" "HAHA" "HAHA" "HAHA" "HAHA" "HAHA" "HAHA" "HAHA" "HAHA" "HAHA"</code></pre>
<p>
To create a sequence, things are not as straightforward. There is <code>seq()</code>:
</p>
<pre class="r"><code>seq(1, 10)</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10</code></pre>
<pre class="r"><code>seq(70, 80)</code></pre>
<pre><code>##  [1] 70 71 72 73 74 75 76 77 78 79 80</code></pre>
<p>
It is also possible to provide a <code>by</code> argument:
</p>
<pre class="r"><code>seq(1, 10, by = 2)</code></pre>
<pre><code>## [1] 1 3 5 7 9</code></pre>
<p>
<code>seq_along()</code> behaves similarly, but returns the length of the object passed to it. So if you pass <code>list4</code> to <code>seq_along()</code>, it will return a sequence from 1 to 3:
</p>
<pre class="r"><code>seq_along(list4)</code></pre>
<pre><code>## [1] 1 2 3</code></pre>
<p>
which is also true for <code>seq()</code> actually:
</p>
<pre class="r"><code>seq(list4)</code></pre>
<pre><code>## [1] 1 2 3</code></pre>
<p>
but these two functions behave differently for arguments of length equal to 1:
</p>
<pre class="r"><code>seq(10)</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10</code></pre>
<pre class="r"><code>seq_along(10)</code></pre>
<pre><code>## [1] 1</code></pre>
<p>
So be quite careful about that. I would advise you do not use <code>seq()</code>, but only <code>seq_along()</code> and <code>seq_len()</code>. <code>seq_len()</code> only takes arguments of length 1:
</p>
<pre class="r"><code>seq_len(10)</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10</code></pre>
<pre class="r"><code>seq_along(10)</code></pre>
<pre><code>## [1] 1</code></pre>
<p>
The problem with <code>seq()</code> is that it is unpredictable; depending on its input, the output will either be an integer or a sequence. When programming, it is better to have function that are stricter and fail when confronted to special cases, instead of returning some result. This is a bit of a recurrent issue with R, and the functions from the <code>{tidyverse}</code> mitigate this issue by being stricter than their base R counterparts. For example, consider the <code>ifelse()</code> function from base R:
</p>
<pre class="r"><code>ifelse(3 &gt; 5, 1, "this is false")</code></pre>
<pre><code>## [1] "this is false"</code></pre>
<p>
and compare it to <code>{dplyr}</code>‚Äôs implementation, <code>if_else()</code>:
</p>
<pre class="r"><code>if_else(3 &gt; 5, 1, "this is false")
Error: `false` must be type double, not character
Call `rlang::last_error()` to see a backtrace</code></pre>
<p>
<code>if_else()</code> fails because the return value when <code>FALSE</code> is not a double (a real number) but a character. This might seem unnecessarily strict, but at least it is predictable. This makes debugging easier when used inside functions. In Chapter 8 we are going to learn how to write our own functions, and being strict makes programming easier.
</p>
</section>
<section id="basic-string-manipulation" class="level4">
<h4 class="anchored" data-anchor-id="basic-string-manipulation">
Basic string manipulation
</h4>
<p>
For now, we have not closely studied <code>character</code> objects, we only learned how to define them. Later, in Chapter 5 we will learn about the <code>{stringr}</code> package which provides useful function to work with strings. However, there are several base R functions that are very useful that you might want to know nonetheless, such as <code>paste()</code> and <code>paste0()</code>:
</p>
<pre class="r"><code>paste("Hello", "amigo")</code></pre>
<pre><code>## [1] "Hello amigo"</code></pre>
<p>
but you can also change the separator if needed:
</p>
<pre class="r"><code>paste("Hello", "amigo", sep = "--")</code></pre>
<pre><code>## [1] "Hello--amigo"</code></pre>
<p>
<code>paste0()</code> is the same as <code>paste()</code> but does not have any <code>sep</code> argument:
</p>
<pre class="r"><code>paste0("Hello", "amigo")</code></pre>
<pre><code>## [1] "Helloamigo"</code></pre>
<p>
If you provide a vector of characters, you can also use the <code>collapse</code> argument, which places whatever you provide for <code>collapse</code> between the characters of the vector:
</p>
<pre class="r"><code>paste0(c("Joseph", "Mary", "Jesus"), collapse = ", and ")</code></pre>
<pre><code>## [1] "Joseph, and Mary, and Jesus"</code></pre>
<p>
To change the case of characters, you can use <code>toupper()</code> and <code>tolower()</code>:
</p>
<pre class="r"><code>tolower("HAHAHAHAH")</code></pre>
<pre><code>## [1] "hahahahah"</code></pre>
<pre class="r"><code>toupper("hueuehuehuheuhe")</code></pre>
<pre><code>## [1] "HUEUEHUEHUHEUHE"</code></pre>
</section>
<section id="mathematical-functions" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-functions">
Mathematical functions
</h4>
<p>
Finally, there are the classical mathematical functions that you know and love:
</p>
<ul>
<li>
<code>sqrt()</code>
</li>
<li>
<code>exp()</code>
</li>
<li>
<code>log()</code>
</li>
<li>
<code>abs()</code>
</li>
<li>
<code>sin()</code>, <code>cos()</code>, <code>tan()</code>, and others
</li>
<li>
<code>sum()</code>, <code>cumsum()</code>, <code>prod()</code>, <code>cumprod()</code>
</li>
<li>
<code>max()</code>, <code>min()</code>
</li>
</ul>
<p>
and many others‚Ä¶
</p>
</section>
</section>
</section>



 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-12-24-modern_objects.html</guid>
  <pubDate>Mon, 24 Dec 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Using the tidyverse for more than data manipulation: estimating pi with Monte Carlo methods</title>
  <link>https://b-rodrigues.github.io/posts/2018-12-21-tidyverse_pi.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=kZJY15dyMig"> <img width="400" src="https://b-rodrigues.github.io/assets/img/casino.jpg" title="Audentes Fortuna Iuvat"></a>
</p>
</div>
<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<p>
This blog post is an excerpt of my ebook <em>Modern R with the tidyverse</em> that you can read for free <a href="https://b-rodrigues.github.io/modern_R/">here</a>. This is taken from Chapter 5, which presents the <code>{tidyverse}</code> packages and how to use them to compute descriptive statistics and manipulate data. In the text below, I show how you can use the <code>{tidyverse}</code> functions and principles for the estimation of <img src="https://latex.codecogs.com/png.latex?()"> using Monte Carlo simulation.
</p>
<section id="going-beyond-descriptive-statistics-and-data-manipulation" class="level2">
<h2 class="anchored" data-anchor-id="going-beyond-descriptive-statistics-and-data-manipulation">
Going beyond descriptive statistics and data manipulation
</h2>
<p>
The <code>{tidyverse}</code> collection of packages can do much more than simply data manipulation and descriptive statisics. You can use the principles we have covered and the functions you now know to do much more. For instance, you can use a few <code>{tidyverse}</code> functions to do Monte Carlo simulations, for example to estimate <img src="https://latex.codecogs.com/png.latex?()">.
</p>
<p>
Draw the unit circle inside the unit square, the ratio of the area of the circle to the area of the square will be <img src="https://latex.codecogs.com/png.latex?(/4)">. Then shot K arrows at the square; roughly <img src="https://latex.codecogs.com/png.latex?(K*/4)"> should have fallen inside the circle. So if now you shoot N arrows at the square, and M fall inside the circle, you have the following relationship <img src="https://latex.codecogs.com/png.latex?(M%20=%20N/4)"><em>. You can thus compute <img src="https://latex.codecogs.com/png.latex?()"> like so: <img src="https://latex.codecogs.com/png.latex?(=%204"></em>M/N).
</p>
<p>
The more arrows N you throw at the square, the better approximation of <img src="https://latex.codecogs.com/png.latex?()"> you‚Äôll have. Let‚Äôs try to do this with a tidy Monte Carlo simulation. First, let‚Äôs randomly pick some points inside the unit square:
</p>
<pre class="r"><code>library(tidyverse)
library(brotools)</code></pre>
<pre class="r"><code>n &lt;- 5000

set.seed(2019)
points &lt;- tibble("x" = runif(n), "y" = runif(n))</code></pre>
<p>
Now, to know if a point is inside the unit circle, we need to check wether <img src="https://latex.codecogs.com/png.latex?(x%5E2%20+%20y%5E2%20%3C%201)">. Let‚Äôs add a new column to the <code>points</code> tibble, called <code>inside</code> equal to 1 if the point is inside the unit circle and 0 if not:
</p>
<pre class="r"><code>points &lt;- points %&gt;% 
    mutate(inside = map2_dbl(.x = x, .y = y, ~ifelse(.x**2 + .y**2 &lt; 1, 1, 0))) %&gt;% 
    rowid_to_column("N")</code></pre>
<p>
Let‚Äôs take a look at <code>points</code>:
</p>
<pre class="r"><code>points</code></pre>
<pre><code>## # A tibble: 5,000 x 4
##        N       x      y inside
##    &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1     1 0.770   0.984       0
##  2     2 0.713   0.0107      1
##  3     3 0.303   0.133       1
##  4     4 0.618   0.0378      1
##  5     5 0.0505  0.677       1
##  6     6 0.0432  0.0846      1
##  7     7 0.820   0.727       0
##  8     8 0.00961 0.0758      1
##  9     9 0.102   0.373       1
## 10    10 0.609   0.676       1
## # ‚Ä¶ with 4,990 more rows</code></pre>
<p>
The <code>rowid_to_column()</code> function, from the <code>{tibble}</code> package, adds a new column to the data frame with an id, going from 1 to the number of rows in the data frame. Now, I can compute the estimation of <img src="https://latex.codecogs.com/png.latex?()"> at each row, by computing the cumulative sum of the 1‚Äôs in the <code>inside</code> column and dividing that by the current value of <code>N</code> column:
</p>
<pre class="r"><code>points &lt;- points %&gt;% 
    mutate(estimate = 4*cumsum(inside)/N)</code></pre>
<p>
<code>cumsum(inside)</code> is the <code>M</code> from the formula. Now, we can finish by plotting the result:
</p>
<pre class="r"><code>ggplot(points) + 
    geom_line(aes(y = estimate, x = N), colour = "#82518c") + 
    geom_hline(yintercept = pi) +
    theme_blog()</code></pre>
<p>
<img src="https://b-rodrigues.github.io/assets/img/tidyverse_pi-7-1.png" width="672">
</p>
<p>
In Chapter 6, we are going to learn all about <code>{ggplot2}</code>.
</p>
<p>
As the number of tries grows, the estimation of <img src="https://latex.codecogs.com/png.latex?()"> gets better.
</p>
<p>
Using a data frame as a structure to hold our simulated points and the results makes it very easy to avoid loops, and thus write code that is more concise and easier to follow. If you studied a quantitative field in u8niversity, you might have done a similar exercise at the time, very likely by defining a matrix to hold your points, and an empty vector to hold whether a particular point was inside the unit circle. Then you wrote a loop to compute whether a point was inside the unit circle, save this result in the before-defined empty vector and then compute the estimation of <img src="https://latex.codecogs.com/png.latex?()">. Again, I take this opportunity here to stress that there is nothing wrong with this approach per se, but R, with the <code>{tidyverse}</code> is better suited for a workflow where lists or data frames are the central objects and where the analyst operates over them with functional programming techniques.
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-12-21-tidyverse_pi.html</guid>
  <pubDate>Fri, 21 Dec 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Manipulate dates easily with {lubridate}</title>
  <link>https://b-rodrigues.github.io/posts/2018-12-15-lubridate_africa.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=FTQbiNvZqaY"> <img width="400" src="https://b-rodrigues.github.io/assets/img/africa.jpg" title="One of my favourite songs"></a>
</p>
</div>
<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<p>
This blog post is an excerpt of my ebook <em>Modern R with the tidyverse</em> that you can read for free <a href="https://b-rodrigues.github.io/modern_R/">here</a>. This is taken from Chapter 5, which presents the <code>{tidyverse}</code> packages and how to use them to compute descriptive statistics and manipulate data. In the text below, I scrape a table from Wikipedia, which shows when African countries gained independence from other countries. Then, using <code>{lubridate}</code> functions I show you how you can answers questions such as <em>Which countries gained independence before 1960?</em>.
</p>
<section id="set-up-scraping-some-data-from-wikipedia" class="level2">
<h2 class="anchored" data-anchor-id="set-up-scraping-some-data-from-wikipedia">
Set-up: scraping some data from Wikipedia
</h2>
<p>
<code>{lubridate}</code> is yet another tidyverse package, that makes dealing with dates or duration data (and intervals) as painless as possible. I do not use every function contained in the package daily, and as such will only focus on some of the functions. However, if you have to deal with dates often, you might want to explore the package thoroughly.
</p>
<p>
Let‚Äôs get some data from a Wikipedia table:
</p>
<pre class="r"><code>library(tidyverse)
library(rvest)</code></pre>
<pre class="r"><code>page &lt;- read_html("https://en.wikipedia.org/wiki/Decolonisation_of_Africa")

independence &lt;- page %&gt;%
    html_node(".wikitable") %&gt;%
    html_table(fill = TRUE)

independence &lt;- independence %&gt;%
    select(-Rank) %&gt;%
    map_df(~str_remove_all(., "\\[.*\\]")) %&gt;%
    rename(country = `Country[a]`,
           colonial_name = `Colonial name`,
           colonial_power = `Colonial power[b]`,
           independence_date = `Independence date[c]`,
           first_head_of_state = `First head of state[d]`,
           independence_won_through = `Independence won through`)</code></pre>
<p>
This dataset was scraped from the following Wikipedia <a href="https://en.wikipedia.org/wiki/Decolonisation_of_Africa#Timeline">table</a>. It shows when African countries gained independence from which colonial powers. In Chapter 11, I will show you how to scrape Wikipedia pages using R. For now, let‚Äôs take a look at the contents of the dataset:
</p>
<pre class="r"><code>independence</code></pre>
<pre><code>## # A tibble: 54 x 6
##    country colonial_name colonial_power independence_da‚Ä¶ first_head_of_s‚Ä¶
##    &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;            &lt;chr&gt;           
##  1 Liberia Liberia       United States  26 July 1847     Joseph Jenkins ‚Ä¶
##  2 South ‚Ä¶ Cape Colony ‚Ä¶ United Kingdom 31 May 1910      Louis Botha     
##  3 Egypt   Sultanate of‚Ä¶ United Kingdom 28 February 1922 Fuad I          
##  4 Eritrea Italian Erit‚Ä¶ Italy          10 February 1947 Haile Selassie  
##  5 Libya   British Mili‚Ä¶ United Kingdo‚Ä¶ 24 December 1951 Idris           
##  6 Sudan   Anglo-Egypti‚Ä¶ United Kingdo‚Ä¶ 1 January 1956   Ismail al-Azhari
##  7 Tunisia French Prote‚Ä¶ France         20 March 1956    Muhammad VIII a‚Ä¶
##  8 Morocco French Prote‚Ä¶ France&nbsp;Spain   2 March 19567 A‚Ä¶ Mohammed V      
##  9 Ghana   Gold Coast    United Kingdom 6 March 1957     Kwame Nkrumah   
## 10 Guinea  French West ‚Ä¶ France         2 October 1958   Ahmed S√©kou Tou‚Ä¶
## # ‚Ä¶ with 44 more rows, and 1 more variable: independence_won_through &lt;chr&gt;</code></pre>
<p>
as you can see, the date of independence is in a format that might make it difficult to answer questions such as <em>Which African countries gained independence before 1960 ?</em> for two reasons. First of all, the date uses the name of the month instead of the number of the month (well, this is not such a big deal, but still), and second of all the type of the independence day column is <em>character</em> and not ‚Äúdate‚Äù. So our first task is to correctly define the column as being of type date, while making sure that R understands that <em>January</em> is supposed to be ‚Äú01‚Äù, and so on.
</p>
</section>
<section id="using-lubridate" class="level2">
<h2 class="anchored" data-anchor-id="using-lubridate">
Using <code>{lubridate}</code>
</h2>
<p>
There are several helpful functions included in <code>{lubridate}</code> to convert columns to dates. For instance if the column you want to convert is of the form ‚Äú2012-11-21‚Äù, then you would use the function <code>ymd()</code>, for ‚Äúyear-month-day‚Äù. If, however the column is ‚Äú2012-21-11‚Äù, then you would use <code>ydm()</code>. There‚Äôs a few of these helper functions, and they can handle a lot of different formats for dates. In our case, having the name of the month instead of the number might seem quite problematic, but it turns out that this is a case that <code>{lubridate}</code> handles painfully:
</p>
<pre class="r"><code>library(lubridate)</code></pre>
<pre><code>## 
## Attaching package: 'lubridate'</code></pre>
<pre><code>## The following object is masked from 'package:base':
## 
##     date</code></pre>
<pre class="r"><code>independence &lt;- independence %&gt;%
  mutate(independence_date = dmy(independence_date))</code></pre>
<pre><code>## Warning: 5 failed to parse.</code></pre>
<p>
Some dates failed to parse, for instance for Morocco. This is because these countries have several independence dates; this means that the string to convert looks like:
</p>
<pre><code>"2 March 1956
7 April 1956
10 April 1958
4 January 1969"</code></pre>
<p>
which obviously cannot be converted by <code>{lubridate}</code> without further manipulation. I ignore these cases for simplicity‚Äôs sake.
</p>
<p>
Let‚Äôs take a look at the data now:
</p>
<pre class="r"><code>independence</code></pre>
<pre><code>## # A tibble: 54 x 6
##    country colonial_name colonial_power independence_da‚Ä¶ first_head_of_s‚Ä¶
##    &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;          &lt;date&gt;           &lt;chr&gt;           
##  1 Liberia Liberia       United States  1847-07-26       Joseph Jenkins ‚Ä¶
##  2 South ‚Ä¶ Cape Colony ‚Ä¶ United Kingdom 1910-05-31       Louis Botha     
##  3 Egypt   Sultanate of‚Ä¶ United Kingdom 1922-02-28       Fuad I          
##  4 Eritrea Italian Erit‚Ä¶ Italy          1947-02-10       Haile Selassie  
##  5 Libya   British Mili‚Ä¶ United Kingdo‚Ä¶ 1951-12-24       Idris           
##  6 Sudan   Anglo-Egypti‚Ä¶ United Kingdo‚Ä¶ 1956-01-01       Ismail al-Azhari
##  7 Tunisia French Prote‚Ä¶ France         1956-03-20       Muhammad VIII a‚Ä¶
##  8 Morocco French Prote‚Ä¶ France&nbsp;Spain   NA               Mohammed V      
##  9 Ghana   Gold Coast    United Kingdom 1957-03-06       Kwame Nkrumah   
## 10 Guinea  French West ‚Ä¶ France         1958-10-02       Ahmed S√©kou Tou‚Ä¶
## # ‚Ä¶ with 44 more rows, and 1 more variable: independence_won_through &lt;chr&gt;</code></pre>
<p>
As you can see, we now have a date column in the right format. We can now answer questions such as <em>Which countries gained independence before 1960?</em> quite easily, by using the functions <code>year()</code>, <code>month()</code> and <code>day()</code>. Let‚Äôs see which countries gained independence before 1960:
</p>
<pre class="r"><code>independence %&gt;%
  filter(year(independence_date) &lt;= 1960) %&gt;%
  pull(country)</code></pre>
<pre><code>##  [1] "Liberia"                          "South Africa"                    
##  [3] "Egypt"                            "Eritrea"                         
##  [5] "Libya"                            "Sudan"                           
##  [7] "Tunisia"                          "Ghana"                           
##  [9] "Guinea"                           "Cameroon"                        
## [11] "Togo"                             "Mali"                            
## [13] "Madagascar"                       "Democratic Republic of the Congo"
## [15] "Benin"                            "Niger"                           
## [17] "Burkina Faso"                     "Ivory Coast"                     
## [19] "Chad"                             "Central African Republic"        
## [21] "Republic of the Congo"            "Gabon"                           
## [23] "Mauritania"</code></pre>
<p>
You guessed it, <code>year()</code> extracts the year of the date column and converts it as a <em>numeric</em> so that we can work on it. This is the same for <code>month()</code> or <code>day()</code>. Let‚Äôs try to see if countries gained their independence on Christmas Eve:
</p>
<pre class="r"><code>independence %&gt;%
  filter(month(independence_date) == 12,
         day(independence_date) == 24) %&gt;%
  pull(country)</code></pre>
<pre><code>## [1] "Libya"</code></pre>
<p>
Seems like Libya was the only one! You can also operate on dates. For instance, let‚Äôs compute the difference between two dates, using the <code>interval()</code> column:
</p>
<pre class="r"><code>independence %&gt;%
  mutate(today = lubridate::today()) %&gt;%
  mutate(independent_since = interval(independence_date, today)) %&gt;%
  select(country, independent_since)</code></pre>
<pre><code>## # A tibble: 54 x 2
##    country      independent_since             
##    &lt;chr&gt;        &lt;S4: Interval&gt;                
##  1 Liberia      1847-07-26 UTC--2019-02-10 UTC
##  2 South Africa 1910-05-31 UTC--2019-02-10 UTC
##  3 Egypt        1922-02-28 UTC--2019-02-10 UTC
##  4 Eritrea      1947-02-10 UTC--2019-02-10 UTC
##  5 Libya        1951-12-24 UTC--2019-02-10 UTC
##  6 Sudan        1956-01-01 UTC--2019-02-10 UTC
##  7 Tunisia      1956-03-20 UTC--2019-02-10 UTC
##  8 Morocco      NA--NA                        
##  9 Ghana        1957-03-06 UTC--2019-02-10 UTC
## 10 Guinea       1958-10-02 UTC--2019-02-10 UTC
## # ‚Ä¶ with 44 more rows</code></pre>
<p>
The <code>independent_since</code> column now contains an <em>interval</em> object that we can convert to years:
</p>
<pre class="r"><code>independence %&gt;%
  mutate(today = lubridate::today()) %&gt;%
  mutate(independent_since = interval(independence_date, today)) %&gt;%
  select(country, independent_since) %&gt;%
  mutate(years_independent = as.numeric(independent_since, "years"))</code></pre>
<pre><code>## # A tibble: 54 x 3
##    country      independent_since              years_independent
##    &lt;chr&gt;        &lt;S4: Interval&gt;                             &lt;dbl&gt;
##  1 Liberia      1847-07-26 UTC--2019-02-10 UTC             172. 
##  2 South Africa 1910-05-31 UTC--2019-02-10 UTC             109. 
##  3 Egypt        1922-02-28 UTC--2019-02-10 UTC              97.0
##  4 Eritrea      1947-02-10 UTC--2019-02-10 UTC              72  
##  5 Libya        1951-12-24 UTC--2019-02-10 UTC              67.1
##  6 Sudan        1956-01-01 UTC--2019-02-10 UTC              63.1
##  7 Tunisia      1956-03-20 UTC--2019-02-10 UTC              62.9
##  8 Morocco      NA--NA                                      NA  
##  9 Ghana        1957-03-06 UTC--2019-02-10 UTC              61.9
## 10 Guinea       1958-10-02 UTC--2019-02-10 UTC              60.4
## # ‚Ä¶ with 44 more rows</code></pre>
<p>
We can now see for how long the last country to gain independence has been independent. Because the data is not tidy (in some cases, an African country was colonized by two powers, see Libya), I will only focus on 4 European colonial powers: Belgium, France, Portugal and the United Kingdom:
</p>
<pre class="r"><code>independence %&gt;%
  filter(colonial_power %in% c("Belgium", "France", "Portugal", "United Kingdom")) %&gt;%
  mutate(today = lubridate::today()) %&gt;%
  mutate(independent_since = interval(independence_date, today)) %&gt;%
  mutate(years_independent = as.numeric(independent_since, "years")) %&gt;%
  group_by(colonial_power) %&gt;%
  summarise(last_colony_independent_for = min(years_independent, na.rm = TRUE))</code></pre>
<pre><code>## # A tibble: 4 x 2
##   colonial_power last_colony_independent_for
##   &lt;chr&gt;                                &lt;dbl&gt;
## 1 Belgium                               56.6
## 2 France                                41.6
## 3 Portugal                              43.2
## 4 United Kingdom                        42.6</code></pre>
<p>
<code>{lubridate}</code> contains many more functions. If you often work with dates, duration or interval data, <code>{lubridate}</code> is a package that you have to master.
</p>


</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-12-15-lubridate_africa.html</guid>
  <pubDate>Sat, 15 Dec 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>What hyper-parameters are, and what to do with them; an illustration with ridge regression</title>
  <link>https://b-rodrigues.github.io/posts/2018-12-02-hyper-parameters.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<p>
<a href="https://www.youtube.com/watch?v=13Gd5kpLzsw"> <img width="400" src="https://b-rodrigues.github.io/assets/img/ridge.jpg" title="Gameboy ridge"></a>
</p>
</div>
<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<p>
This blog post is an excerpt of my ebook <em>Modern R with the tidyverse</em> that you can read for free <a href="https://b-rodrigues.github.io/modern_R/">here</a>. This is taken from Chapter 7, which deals with statistical models. In the text below, I explain what hyper-parameters are, and as an example I run a ridge regression using the <code>{glmnet}</code> package. The book is still being written, so comments are more than welcome!
</p>
<section id="hyper-parameters" class="level2">
<h2 class="anchored" data-anchor-id="hyper-parameters">
Hyper-parameters
</h2>
<p>
Hyper-parameters are parameters of the model that cannot be directly learned from the data. A linear regression does not have any hyper-parameters, but a random forest for instance has several. You might have heard of ridge regression, lasso and elasticnet. These are extensions to linear models that avoid over-fitting by penalizing <em>large</em> models. These extensions of the linear regression have hyper-parameters that the practitioner has to tune. There are several ways one can tune these parameters, for example, by doing a grid-search, or a random search over the grid or using more elaborate methods. To introduce hyper-parameters, let‚Äôs get to know ridge regression, also called Tikhonov regularization.
</p>
<section id="ridge-regression" class="level3">
<h3 class="anchored" data-anchor-id="ridge-regression">
Ridge regression
</h3>
<p>
Ridge regression is used when the data you are working with has a lot of explanatory variables, or when there is a risk that a simple linear regression might overfit to the training data, because, for example, your explanatory variables are collinear. If you are training a linear model and then you notice that it generalizes very badly to new, unseen data, it is very likely that the linear model you trained overfits the data. In this case, ridge regression might prove useful. The way ridge regression works might seem counter-intuititive; it boils down to fitting a <em>worse</em> model to the training data, but in return, this worse model will generalize better to new data.
</p>
<p>
The closed form solution of the ordinary least squares estimator is defined as:
</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cbeta%7D%20=%20(X'X)%5E%7B-1%7DX'Y"></p>
<p>
where <img src="https://latex.codecogs.com/png.latex?X"> is the design matrix (the matrix made up of the explanatory variables) and <img src="https://latex.codecogs.com/png.latex?Y"> is the dependent variable. For ridge regression, this closed form solution changes a little bit:
</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cbeta%7D%20=%20(X'X%20+%20%5Clambda%20I_p)%5E%7B-1%7DX'Y"></p>
<p>
where <img src="https://latex.codecogs.com/png.latex?lambda%20%5Cin%20%5Cmathbb%7BR%7D"> is an hyper-parameter and <img src="https://latex.codecogs.com/png.latex?I_p"> is the identity matrix of dimension <img src="https://latex.codecogs.com/png.latex?p"> (<img src="https://latex.codecogs.com/png.latex?p"> is the number of explanatory variables). This formula above is the closed form solution to the following optimisation program:
</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Csum_%7Bi=1%7D%5En%20%5Cleft(y_i%20-%20%5Csum_%7Bj=1%7D%5Epx_%7Bij%7D%5Cbeta_j%5Cright)%5E2%20"></p>
<p>
such that:
</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Csum_%7Bj=1%7D%5Ep(%5Cbeta_j)%5E2%20%3C%20c"></p>
<p>
for any strictly positive <img src="https://latex.codecogs.com/png.latex?(c)">.
</p>
<p>
The <code>glmnet()</code> function from the <code>{glmnet}</code> package can be used for ridge regression, by setting the <code>alpha</code> argument to 0 (setting it to 1 would do LASSO, and setting it to a number between 0 and 1 would do elasticnet). But in order to compare linear regression and ridge regression, let me first divide the data into a training set and a testing set. I will be using the <code>Housing</code> data from the <code>{Ecdat}</code> package:
</p>
<pre class="r"><code>library(tidyverse)
library(Ecdat)
library(glmnet)</code></pre>
<pre class="r"><code>index &lt;- 1:nrow(Housing)

set.seed(12345)
train_index &lt;- sample(index, round(0.90*nrow(Housing)), replace = FALSE)

test_index &lt;- setdiff(index, train_index)

train_x &lt;- Housing[train_index, ] %&gt;% 
    select(-price)

train_y &lt;- Housing[train_index, ] %&gt;% 
    pull(price)

test_x &lt;- Housing[test_index, ] %&gt;% 
    select(-price)

test_y &lt;- Housing[test_index, ] %&gt;% 
    pull(price)</code></pre>
<p>
I do the train/test split this way, because <code>glmnet()</code> requires a design matrix as input, and not a formula. Design matrices can be created using the <code>model.matrix()</code> function:
</p>
<pre class="r"><code>train_matrix &lt;- model.matrix(train_y ~ ., data = train_x)

test_matrix &lt;- model.matrix(test_y ~ ., data = test_x)</code></pre>
<p>
To run an unpenalized linear regression, we can set the penalty to 0:
</p>
<pre class="r"><code>model_lm_ridge &lt;- glmnet(y = train_y, x = train_matrix, alpha = 0, lambda = 0)</code></pre>
<p>
The model above provides the same result as a linear regression. Let‚Äôs compare the coefficients between the two:
</p>
<pre class="r"><code>coef(model_lm_ridge)</code></pre>
<pre><code>## 13 x 1 sparse Matrix of class "dgCMatrix"
##                       s0
## (Intercept) -3247.030393
## (Intercept)     .       
## lotsize         3.520283
## bedrooms     1745.211187
## bathrms     14337.551325
## stories      6736.679470
## drivewayyes  5687.132236
## recroomyes   5701.831289
## fullbaseyes  5708.978557
## gashwyes    12508.524241
## aircoyes    12592.435621
## garagepl     4438.918373
## prefareayes  9085.172469</code></pre>
<p>
and now the coefficients of the linear regression (because I provide a design matrix, I have to use <code>lm.fit()</code> instead of <code>lm()</code> which requires a formula, not a matrix.)
</p>
<pre class="r"><code>coef(lm.fit(x = train_matrix, y = train_y))</code></pre>
<pre><code>##  (Intercept)      lotsize     bedrooms      bathrms      stories 
## -3245.146665     3.520357  1744.983863 14336.336858  6737.000410 
##  drivewayyes   recroomyes  fullbaseyes     gashwyes     aircoyes 
##  5686.394123  5700.210775  5709.493884 12509.005265 12592.367268 
##     garagepl  prefareayes 
##  4439.029607  9085.409155</code></pre>
<p>
as you can see, the coefficients are the same. Let‚Äôs compute the RMSE for the unpenalized linear regression:
</p>
<pre class="r"><code>preds_lm &lt;- predict(model_lm_ridge, test_matrix)

rmse_lm &lt;- sqrt(mean((preds_lm - test_y)^2))</code></pre>
<p>
The RMSE for the linear unpenalized regression is equal to 14463.08.
</p>
<p>
Let‚Äôs now run a ridge regression, with <code>lambda</code> equal to 100, and see if the RMSE is smaller:
</p>
<pre class="r"><code>model_ridge &lt;- glmnet(y = train_y, x = train_matrix, alpha = 0, lambda = 100)</code></pre>
<p>
and let‚Äôs compute the RMSE again:
</p>
<pre class="r"><code>preds &lt;- predict(model_ridge, test_matrix)

rmse &lt;- sqrt(mean((preds - test_y)^2))</code></pre>
<p>
The RMSE for the linear penalized regression is equal to 14460.71, which is smaller than before. But which value of <code>lambda</code> gives smallest RMSE? To find out, one must run model over a grid of <code>lambda</code> values and pick the model with lowest RMSE. This procedure is available in the <code>cv.glmnet()</code> function, which picks the best value for <code>lambda</code>:
</p>
<pre class="r"><code>best_model &lt;- cv.glmnet(train_matrix, train_y)
# lambda that minimises the MSE
best_model$lambda.min</code></pre>
<pre><code>## [1] 66.07936</code></pre>
<p>
According to <code>cv.glmnet()</code> the best value for <code>lambda</code> is 66.0793576. In the next section, we will implement cross validation ourselves, in order to find the hyper-parameters of a random forest.
</p>


</section>
</section>

 ]]></description>
  <category>R</category>
  <category>data-science</category>
  <guid>https://b-rodrigues.github.io/posts/2018-12-02-hyper-parameters.html</guid>
  <pubDate>Sun, 02 Dec 2018 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
